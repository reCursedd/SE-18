{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/167347215", "html_url": "https://github.com/tensorflow/tensorflow/issues/537#issuecomment-167347215", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/537", "id": 167347215, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NzM0NzIxNQ==", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-26T17:26:57Z", "updated_at": "2015-12-26T18:29:55Z", "author_association": "NONE", "body_html": "<p>Hi again. I needed to make this more efficient, but my attempts haven't been working. I also realized that the previous solution I had built failed when there is more than one bucket (I had made it uni-bucket for testing). TF would throw an error about the encoder and decoder placeholders needing values. If I used a bucket of (10,12) and had set it up to also have a bucket of (20,25), then the error would be for encoders 10 through 20 and decoders 12 through 25.</p>\n<p>This was confusing because it seemed like I was building the graph similarly. What I have now is that I make the encoder_cell and the embedding in the Seq2Seq init:</p>\n<pre><code>    ...\n    # Create the internal multi-layer cell for our RNN.                                                                                                                                                     \n    single_cell = rnn_cell.GRUCell(layer_size)\n    if use_lstm:\n      single_cell = rnn_cell.BasicLSTMCell(layer_size)\n    cell = single_cell\n    if num_layers &gt; 1:\n      cell = rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n    encoder_cell = rnn_cell.EmbeddingWrapper(cell, total_vocab_size)\n    with vs.variable_scope('embedding_decoder_top_level'):\n        with ops.device(\"/cpu:0\"):\n            embedding = vs.get_variable(\"embedding\",\n                                        [vocab_size, cell.input_size])\n\n    # The seq2seq function: we use embedding for the input and attention.                                                                                                                                   \n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return seq2seq.embedding_attention_seq2seq(\n        encoder_inputs, decoder_inputs, cell, vocab_size_in,\n        vocab_size_out, output_projection=output_projection,\n        feed_previous=do_decode, encoder_cell=encoder_cell, \n        embedding=embedding)\n</code></pre>\n<p>I'm then passing them through into the embedding_attention_seq2seq:</p>\n<pre><code>    self.module_graph = {}\n\n    for num, module in enumerate(modules):\n        scope_name = 'module_%s' % language\n        with vs.variable_scope(scope_name):\n          if forward_only:\n            outputs, losses = seq2seq.model_with_buckets(...)\n          else:\n            outputs, losses = seq2seq.model_with_buckets(...)\n\n        self.module_graph[module] = [outputs, losses]\n        params = [param for param in tf.trainable_variables()\n                  if param.name.startswith(scope_name)]\n\n        if not forward_only:\n            gradient_norms = []\n            updates = []\n            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n            for b in xrange(len(buckets)):\n                gradients = tf.gradients(losses[b], params)\n                clipped_gradients, norm = tf.clip_by_global_norm(\n                    gradients, max_gradient_norm)\n            gradient_norms.append(norm)\n            updates.append(opt.apply_gradients(\n                zip(clipped_gradients, params), global_step=self.global_step))\n\n            self.module_graph[module].extend([gradient_norms, updates])\n</code></pre>\n<p>This throws a confusing error even before I can start training it:</p>\n<pre><code>  File \".../tensorflow/python/ops/nn.py\", line 835, in sampled_softmax_loss\n    name=name)\n  File \".../tensorflow/python/ops/nn.py\", line 654, in _compute_sampled_logits\n    true_logits += true_b\n  File \".../tensorflow/python/ops/math_ops.py\", line 425, in binary_op_wrapper\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\n  File \".../tensorflow/python/framework/ops.py\", line 528, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \".../tensorflow/python/framework/ops.py\", line 472, in _TensorTensorConversionFunction\n    % (dtype.name, t.dtype.name, str(t)))\n\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"module_a/model_with_buckets/module_a/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/Reshape_5:0\", shape=(?, 1), dtype=int32, device=/cpu:0)'\n</code></pre>\n<p>I also printed out the params (see below) to see if there was anything peculiar and I noticed the first value is <code>'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0'</code> (cell_output my own name for scope in rnn/rnn.py). This is the encoder_cell that's passed to the embedding_attention_seq2seq but it's under module_a's scope. Afaict, this means that it won't share training with module_b. Is that right?</p>\n<pre><code>[u'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Bias:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Bias:0']\n</code></pre>\n<p>I realize this might be out of scope of a Github issue now. Let me know if you'd rather take this offline. Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a></p>", "body_text": "Hi again. I needed to make this more efficient, but my attempts haven't been working. I also realized that the previous solution I had built failed when there is more than one bucket (I had made it uni-bucket for testing). TF would throw an error about the encoder and decoder placeholders needing values. If I used a bucket of (10,12) and had set it up to also have a bucket of (20,25), then the error would be for encoders 10 through 20 and decoders 12 through 25.\nThis was confusing because it seemed like I was building the graph similarly. What I have now is that I make the encoder_cell and the embedding in the Seq2Seq init:\n    ...\n    # Create the internal multi-layer cell for our RNN.                                                                                                                                                     \n    single_cell = rnn_cell.GRUCell(layer_size)\n    if use_lstm:\n      single_cell = rnn_cell.BasicLSTMCell(layer_size)\n    cell = single_cell\n    if num_layers > 1:\n      cell = rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n    encoder_cell = rnn_cell.EmbeddingWrapper(cell, total_vocab_size)\n    with vs.variable_scope('embedding_decoder_top_level'):\n        with ops.device(\"/cpu:0\"):\n            embedding = vs.get_variable(\"embedding\",\n                                        [vocab_size, cell.input_size])\n\n    # The seq2seq function: we use embedding for the input and attention.                                                                                                                                   \n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return seq2seq.embedding_attention_seq2seq(\n        encoder_inputs, decoder_inputs, cell, vocab_size_in,\n        vocab_size_out, output_projection=output_projection,\n        feed_previous=do_decode, encoder_cell=encoder_cell, \n        embedding=embedding)\n\nI'm then passing them through into the embedding_attention_seq2seq:\n    self.module_graph = {}\n\n    for num, module in enumerate(modules):\n        scope_name = 'module_%s' % language\n        with vs.variable_scope(scope_name):\n          if forward_only:\n            outputs, losses = seq2seq.model_with_buckets(...)\n          else:\n            outputs, losses = seq2seq.model_with_buckets(...)\n\n        self.module_graph[module] = [outputs, losses]\n        params = [param for param in tf.trainable_variables()\n                  if param.name.startswith(scope_name)]\n\n        if not forward_only:\n            gradient_norms = []\n            updates = []\n            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n            for b in xrange(len(buckets)):\n                gradients = tf.gradients(losses[b], params)\n                clipped_gradients, norm = tf.clip_by_global_norm(\n                    gradients, max_gradient_norm)\n            gradient_norms.append(norm)\n            updates.append(opt.apply_gradients(\n                zip(clipped_gradients, params), global_step=self.global_step))\n\n            self.module_graph[module].extend([gradient_norms, updates])\n\nThis throws a confusing error even before I can start training it:\n  File \".../tensorflow/python/ops/nn.py\", line 835, in sampled_softmax_loss\n    name=name)\n  File \".../tensorflow/python/ops/nn.py\", line 654, in _compute_sampled_logits\n    true_logits += true_b\n  File \".../tensorflow/python/ops/math_ops.py\", line 425, in binary_op_wrapper\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\n  File \".../tensorflow/python/framework/ops.py\", line 528, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \".../tensorflow/python/framework/ops.py\", line 472, in _TensorTensorConversionFunction\n    % (dtype.name, t.dtype.name, str(t)))\n\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"module_a/model_with_buckets/module_a/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/Reshape_5:0\", shape=(?, 1), dtype=int32, device=/cpu:0)'\n\nI also printed out the params (see below) to see if there was anything peculiar and I noticed the first value is 'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0' (cell_output my own name for scope in rnn/rnn.py). This is the encoder_cell that's passed to the embedding_attention_seq2seq but it's under module_a's scope. Afaict, this means that it won't share training with module_b. Is that right?\n[u'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Bias:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Bias:0']\n\nI realize this might be out of scope of a Github issue now. Let me know if you'd rather take this offline. Thanks @lukaszkaiser", "body": "Hi again. I needed to make this more efficient, but my attempts haven't been working. I also realized that the previous solution I had built failed when there is more than one bucket (I had made it uni-bucket for testing). TF would throw an error about the encoder and decoder placeholders needing values. If I used a bucket of (10,12) and had set it up to also have a bucket of (20,25), then the error would be for encoders 10 through 20 and decoders 12 through 25.\n\nThis was confusing because it seemed like I was building the graph similarly. What I have now is that I make the encoder_cell and the embedding in the Seq2Seq init:\n\n```\n    ...\n    # Create the internal multi-layer cell for our RNN.                                                                                                                                                     \n    single_cell = rnn_cell.GRUCell(layer_size)\n    if use_lstm:\n      single_cell = rnn_cell.BasicLSTMCell(layer_size)\n    cell = single_cell\n    if num_layers > 1:\n      cell = rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n    encoder_cell = rnn_cell.EmbeddingWrapper(cell, total_vocab_size)\n    with vs.variable_scope('embedding_decoder_top_level'):\n        with ops.device(\"/cpu:0\"):\n            embedding = vs.get_variable(\"embedding\",\n                                        [vocab_size, cell.input_size])\n\n    # The seq2seq function: we use embedding for the input and attention.                                                                                                                                   \n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return seq2seq.embedding_attention_seq2seq(\n        encoder_inputs, decoder_inputs, cell, vocab_size_in,\n        vocab_size_out, output_projection=output_projection,\n        feed_previous=do_decode, encoder_cell=encoder_cell, \n        embedding=embedding)\n```\n\nI'm then passing them through into the embedding_attention_seq2seq:\n\n```\n    self.module_graph = {}\n\n    for num, module in enumerate(modules):\n        scope_name = 'module_%s' % language\n        with vs.variable_scope(scope_name):\n          if forward_only:\n            outputs, losses = seq2seq.model_with_buckets(...)\n          else:\n            outputs, losses = seq2seq.model_with_buckets(...)\n\n        self.module_graph[module] = [outputs, losses]\n        params = [param for param in tf.trainable_variables()\n                  if param.name.startswith(scope_name)]\n\n        if not forward_only:\n            gradient_norms = []\n            updates = []\n            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n            for b in xrange(len(buckets)):\n                gradients = tf.gradients(losses[b], params)\n                clipped_gradients, norm = tf.clip_by_global_norm(\n                    gradients, max_gradient_norm)\n            gradient_norms.append(norm)\n            updates.append(opt.apply_gradients(\n                zip(clipped_gradients, params), global_step=self.global_step))\n\n            self.module_graph[module].extend([gradient_norms, updates])\n```\n\nThis throws a confusing error even before I can start training it: \n\n```\n  File \".../tensorflow/python/ops/nn.py\", line 835, in sampled_softmax_loss\n    name=name)\n  File \".../tensorflow/python/ops/nn.py\", line 654, in _compute_sampled_logits\n    true_logits += true_b\n  File \".../tensorflow/python/ops/math_ops.py\", line 425, in binary_op_wrapper\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\n  File \".../tensorflow/python/framework/ops.py\", line 528, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \".../tensorflow/python/framework/ops.py\", line 472, in _TensorTensorConversionFunction\n    % (dtype.name, t.dtype.name, str(t)))\n\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"module_a/model_with_buckets/module_a/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/Reshape_5:0\", shape=(?, 1), dtype=int32, device=/cpu:0)'\n```\n\nI also printed out the params (see below) to see if there was anything peculiar and I noticed the first value is `'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0'` (cell_output my own name for scope in rnn/rnn.py). This is the encoder_cell that's passed to the embedding_attention_seq2seq but it's under module_a's scope. Afaict, this means that it won't share training with module_b. Is that right?\n\n```\n[u'module_a/RNN/cell_output/EmbeddingWrapper/embedding:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Gates/Linear/Bias:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/RNN/cell_output/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnW_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnV_0:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Gates/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/GRUCell/Candidate/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/Attention_0/Linear/Bias:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Matrix:0', \nu'module_a/embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Bias:0']\n```\n\nI realize this might be out of scope of a Github issue now. Let me know if you'd rather take this offline. Thanks @lukaszkaiser \n"}