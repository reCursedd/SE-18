{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/166158728", "html_url": "https://github.com/tensorflow/tensorflow/issues/537#issuecomment-166158728", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/537", "id": 166158728, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NjE1ODcyOA==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-20T22:14:25Z", "updated_at": "2015-12-20T22:14:25Z", "author_association": "MEMBER", "body_html": "<p>Hi cinjon. I think I corrected the reuse leaking from model_with_buckets in a recent commit, so now you're hitting a different problem, this one.</p>\n<blockquote>\n<p>If I remove the reuse=True on that, then the error is again over-sharing, this time because<br>\nmodules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding already exists.</p>\n</blockquote>\n<p>I think removing reuse=True in your case is the right thing to do (you don't want to reuse across modules, right?). But then - do you want to share the encoder embedding across modules? If you do, then you should create a variable just for the embedding and pass it to EmbeddingWrapper, I think. If you don't, then maybe just put each module in it's own scope (e.g., with variable_scope(\"module\"  + str(num))) -- that will make all variables unique.</p>\n<p>Does that help? I'm not fully sure how exactly you want to share variables -- if every module is separate, then I think the best way is to have separate scope for each of them.</p>", "body_text": "Hi cinjon. I think I corrected the reuse leaking from model_with_buckets in a recent commit, so now you're hitting a different problem, this one.\n\nIf I remove the reuse=True on that, then the error is again over-sharing, this time because\nmodules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding already exists.\n\nI think removing reuse=True in your case is the right thing to do (you don't want to reuse across modules, right?). But then - do you want to share the encoder embedding across modules? If you do, then you should create a variable just for the embedding and pass it to EmbeddingWrapper, I think. If you don't, then maybe just put each module in it's own scope (e.g., with variable_scope(\"module\"  + str(num))) -- that will make all variables unique.\nDoes that help? I'm not fully sure how exactly you want to share variables -- if every module is separate, then I think the best way is to have separate scope for each of them.", "body": "Hi cinjon. I think I corrected the reuse leaking from model_with_buckets in a recent commit, so now you're hitting a different problem, this one.\n\n> If I remove the reuse=True on that, then the error is again over-sharing, this time because\n> modules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding already exists.\n\nI think removing reuse=True in your case is the right thing to do (you don't want to reuse across modules, right?). But then - do you want to share the encoder embedding across modules? If you do, then you should create a variable just for the embedding and pass it to EmbeddingWrapper, I think. If you don't, then maybe just put each module in it's own scope (e.g., with variable_scope(\"module\"  + str(num))) -- that will make all variables unique.\n\nDoes that help? I'm not fully sure how exactly you want to share variables -- if every module is separate, then I think the best way is to have separate scope for each of them.\n"}