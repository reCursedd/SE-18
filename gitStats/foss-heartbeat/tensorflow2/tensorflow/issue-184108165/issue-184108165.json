{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5083", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5083/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5083/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5083/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5083", "id": 184108165, "node_id": "MDU6SXNzdWUxODQxMDgxNjU=", "number": 5083, "title": "Strange memory usage with atrous_conv2d on 1D signal.", "user": {"login": "chasep255", "id": 15787797, "node_id": "MDQ6VXNlcjE1Nzg3Nzk3", "avatar_url": "https://avatars3.githubusercontent.com/u/15787797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chasep255", "html_url": "https://github.com/chasep255", "followers_url": "https://api.github.com/users/chasep255/followers", "following_url": "https://api.github.com/users/chasep255/following{/other_user}", "gists_url": "https://api.github.com/users/chasep255/gists{/gist_id}", "starred_url": "https://api.github.com/users/chasep255/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chasep255/subscriptions", "organizations_url": "https://api.github.com/users/chasep255/orgs", "repos_url": "https://api.github.com/users/chasep255/repos", "events_url": "https://api.github.com/users/chasep255/events{/privacy}", "received_events_url": "https://api.github.com/users/chasep255/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-10-20T00:34:26Z", "updated_at": "2016-10-28T18:21:10Z", "closed_at": "2016-10-28T18:21:10Z", "author_association": "NONE", "body_html": "<p>I am trying to build my own wavenet to generate sound.  For this I need to perform a dilated convolution on a 1D signal.  To accomplish this I simply added dummy dimension to my input and filter.</p>\n<pre><code>def dilated_causal_conv1d(x, filter, dialation):\n    padding = (tf.shape(filter)[0] - 1) * dialation\n    x = tf.pad(x, ((0, 0), (padding, 0), (0, 0)))\n    filter = tf.expand_dims(filter, 0)\n    x = tf.expand_dims(x, 0)\n    x = tf.nn.atrous_conv2d(x, filter, dialation, 'VALID')\n    x = tf.squeeze(x, (0,))\n    return x[:, padding:]\n</code></pre>\n<p>This appears to be working although I have not tried training it yet.  I have just managed to send a bunch of zeros through the network to make sure all of the shapes are in alignment.  When I ran it with an input of size (2, 16000) I got an error saying...</p>\n<blockquote>\n<p>W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.75GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.</p>\n</blockquote>\n<p>I did manage to successfully run even with this message so I increased the batch size and ran it again and it still worked.  I kept doing this and I was able to get my batch size up to 64 and it still has not failed to execute.  This is strange to me since I only ever get these warning when I am on the verge of running out of memory.  I remember reading somewhere that atrous_conv2d can be very inefficient in terms of memory on 1D signals, however I have not been able to find that again or an explanation of why.</p>\n<p>I am using the latest tensorflow on Ubuntu 16.04 with Cuda 8.  The size of the dilations range from 1 to 1024 in powers of 2 increments.</p>", "body_text": "I am trying to build my own wavenet to generate sound.  For this I need to perform a dilated convolution on a 1D signal.  To accomplish this I simply added dummy dimension to my input and filter.\ndef dilated_causal_conv1d(x, filter, dialation):\n    padding = (tf.shape(filter)[0] - 1) * dialation\n    x = tf.pad(x, ((0, 0), (padding, 0), (0, 0)))\n    filter = tf.expand_dims(filter, 0)\n    x = tf.expand_dims(x, 0)\n    x = tf.nn.atrous_conv2d(x, filter, dialation, 'VALID')\n    x = tf.squeeze(x, (0,))\n    return x[:, padding:]\n\nThis appears to be working although I have not tried training it yet.  I have just managed to send a bunch of zeros through the network to make sure all of the shapes are in alignment.  When I ran it with an input of size (2, 16000) I got an error saying...\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.75GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n\nI did manage to successfully run even with this message so I increased the batch size and ran it again and it still worked.  I kept doing this and I was able to get my batch size up to 64 and it still has not failed to execute.  This is strange to me since I only ever get these warning when I am on the verge of running out of memory.  I remember reading somewhere that atrous_conv2d can be very inefficient in terms of memory on 1D signals, however I have not been able to find that again or an explanation of why.\nI am using the latest tensorflow on Ubuntu 16.04 with Cuda 8.  The size of the dilations range from 1 to 1024 in powers of 2 increments.", "body": "I am trying to build my own wavenet to generate sound.  For this I need to perform a dilated convolution on a 1D signal.  To accomplish this I simply added dummy dimension to my input and filter.\n\n```\ndef dilated_causal_conv1d(x, filter, dialation):\n    padding = (tf.shape(filter)[0] - 1) * dialation\n    x = tf.pad(x, ((0, 0), (padding, 0), (0, 0)))\n    filter = tf.expand_dims(filter, 0)\n    x = tf.expand_dims(x, 0)\n    x = tf.nn.atrous_conv2d(x, filter, dialation, 'VALID')\n    x = tf.squeeze(x, (0,))\n    return x[:, padding:]\n```\n\nThis appears to be working although I have not tried training it yet.  I have just managed to send a bunch of zeros through the network to make sure all of the shapes are in alignment.  When I ran it with an input of size (2, 16000) I got an error saying...\n\n> W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.75GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n\nI did manage to successfully run even with this message so I increased the batch size and ran it again and it still worked.  I kept doing this and I was able to get my batch size up to 64 and it still has not failed to execute.  This is strange to me since I only ever get these warning when I am on the verge of running out of memory.  I remember reading somewhere that atrous_conv2d can be very inefficient in terms of memory on 1D signals, however I have not been able to find that again or an explanation of why.\n\nI am using the latest tensorflow on Ubuntu 16.04 with Cuda 8.  The size of the dilations range from 1 to 1024 in powers of 2 increments.\n"}