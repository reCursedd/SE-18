{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/232489489", "html_url": "https://github.com/tensorflow/tensorflow/issues/3299#issuecomment-232489489", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3299", "id": 232489489, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMjQ4OTQ4OQ==", "user": {"login": "changyun79", "id": 19846701, "node_id": "MDQ6VXNlcjE5ODQ2NzAx", "avatar_url": "https://avatars3.githubusercontent.com/u/19846701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/changyun79", "html_url": "https://github.com/changyun79", "followers_url": "https://api.github.com/users/changyun79/followers", "following_url": "https://api.github.com/users/changyun79/following{/other_user}", "gists_url": "https://api.github.com/users/changyun79/gists{/gist_id}", "starred_url": "https://api.github.com/users/changyun79/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/changyun79/subscriptions", "organizations_url": "https://api.github.com/users/changyun79/orgs", "repos_url": "https://api.github.com/users/changyun79/repos", "events_url": "https://api.github.com/users/changyun79/events{/privacy}", "received_events_url": "https://api.github.com/users/changyun79/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-13T21:17:12Z", "updated_at": "2016-07-13T21:20:49Z", "author_association": "NONE", "body_html": "<p>Thanks, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> . Any theory behind this design (too much volume cannot fit in I think?)  To me, make memory static is important for both performance and memory fragmentation and I am using SqueezeNet which can fit into my RAM so  I am basically looking to optimize the inference time for CPU only case and wondering whether it is doable on top of tensorflow. Thanks.</p>", "body_text": "Thanks, @zheng-xq . Any theory behind this design (too much volume cannot fit in I think?)  To me, make memory static is important for both performance and memory fragmentation and I am using SqueezeNet which can fit into my RAM so  I am basically looking to optimize the inference time for CPU only case and wondering whether it is doable on top of tensorflow. Thanks.", "body": "Thanks, @zheng-xq . Any theory behind this design (too much volume cannot fit in I think?)  To me, make memory static is important for both performance and memory fragmentation and I am using SqueezeNet which can fit into my RAM so  I am basically looking to optimize the inference time for CPU only case and wondering whether it is doable on top of tensorflow. Thanks. \n"}