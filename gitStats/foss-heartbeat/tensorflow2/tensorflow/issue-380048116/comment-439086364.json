{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439086364", "html_url": "https://github.com/tensorflow/tensorflow/issues/23698#issuecomment-439086364", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23698", "id": 439086364, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTA4NjM2NA==", "user": {"login": "AKI-github", "id": 16523781, "node_id": "MDQ6VXNlcjE2NTIzNzgx", "avatar_url": "https://avatars2.githubusercontent.com/u/16523781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AKI-github", "html_url": "https://github.com/AKI-github", "followers_url": "https://api.github.com/users/AKI-github/followers", "following_url": "https://api.github.com/users/AKI-github/following{/other_user}", "gists_url": "https://api.github.com/users/AKI-github/gists{/gist_id}", "starred_url": "https://api.github.com/users/AKI-github/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AKI-github/subscriptions", "organizations_url": "https://api.github.com/users/AKI-github/orgs", "repos_url": "https://api.github.com/users/AKI-github/repos", "events_url": "https://api.github.com/users/AKI-github/events{/privacy}", "received_events_url": "https://api.github.com/users/AKI-github/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-15T15:46:41Z", "updated_at": "2018-11-15T15:46:41Z", "author_association": "NONE", "body_html": "<p><strong>Your kindness is very much appreciated.</strong></p>\n<p>That means that the number of \"vocab_size = len(tokenizer.word_index)\" is too big, Am I saying this correctly now?<br>\nHowever, I used the same codes, and Dataset (MS COCO)<br>\nTherefore, should I change the setting or dataset or codes about \"tokenizer\" or \"vocab\"?</p>\n<p>Thank you for your continued support.</p>", "body_text": "Your kindness is very much appreciated.\nThat means that the number of \"vocab_size = len(tokenizer.word_index)\" is too big, Am I saying this correctly now?\nHowever, I used the same codes, and Dataset (MS COCO)\nTherefore, should I change the setting or dataset or codes about \"tokenizer\" or \"vocab\"?\nThank you for your continued support.", "body": "**Your kindness is very much appreciated.**\r\n\r\nThat means that the number of \"vocab_size = len(tokenizer.word_index)\" is too big, Am I saying this correctly now? \r\nHowever, I used the same codes, and Dataset (MS COCO)\r\nTherefore, should I change the setting or dataset or codes about \"tokenizer\" or \"vocab\"?\r\n\r\nThank you for your continued support.\r\n"}