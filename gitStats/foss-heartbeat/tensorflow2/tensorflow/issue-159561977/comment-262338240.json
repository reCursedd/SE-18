{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/262338240", "html_url": "https://github.com/tensorflow/tensorflow/issues/2776#issuecomment-262338240", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2776", "id": 262338240, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjMzODI0MA==", "user": {"login": "sirgogo", "id": 5591329, "node_id": "MDQ6VXNlcjU1OTEzMjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5591329?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sirgogo", "html_url": "https://github.com/sirgogo", "followers_url": "https://api.github.com/users/sirgogo/followers", "following_url": "https://api.github.com/users/sirgogo/following{/other_user}", "gists_url": "https://api.github.com/users/sirgogo/gists{/gist_id}", "starred_url": "https://api.github.com/users/sirgogo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sirgogo/subscriptions", "organizations_url": "https://api.github.com/users/sirgogo/orgs", "repos_url": "https://api.github.com/users/sirgogo/repos", "events_url": "https://api.github.com/users/sirgogo/events{/privacy}", "received_events_url": "https://api.github.com/users/sirgogo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-22T19:20:44Z", "updated_at": "2016-11-22T19:20:44Z", "author_association": "NONE", "body_html": "<p>Similar problem here, albeit different network. I'm not training on a GPU, but my machine has 64gb RAM. I have to adjust the batch size, or else I get a<code> Segmentation fault (core dumped)</code> error on the optimizer (backprop) step.</p>\n<p>However, monitoring the RAM usage (e.g. htop), it never crawls above 4GB. So I am inclined to believe this is a bug with tensorflow overestimating the memory required. Backprop should scale linearly with the batch size, and I am seeing this until it faults at an arbitrary point.</p>\n<p>Any ideas?! This is severely limiting my ability to train large models. Using a small batch size is just not going to cut it! -- the estimation of the gradient direction is very poor for large models if the batch size is too small.</p>", "body_text": "Similar problem here, albeit different network. I'm not training on a GPU, but my machine has 64gb RAM. I have to adjust the batch size, or else I get a Segmentation fault (core dumped) error on the optimizer (backprop) step.\nHowever, monitoring the RAM usage (e.g. htop), it never crawls above 4GB. So I am inclined to believe this is a bug with tensorflow overestimating the memory required. Backprop should scale linearly with the batch size, and I am seeing this until it faults at an arbitrary point.\nAny ideas?! This is severely limiting my ability to train large models. Using a small batch size is just not going to cut it! -- the estimation of the gradient direction is very poor for large models if the batch size is too small.", "body": "Similar problem here, albeit different network. I'm not training on a GPU, but my machine has 64gb RAM. I have to adjust the batch size, or else I get a` Segmentation fault (core dumped)` error on the optimizer (backprop) step.\r\n\r\nHowever, monitoring the RAM usage (e.g. htop), it never crawls above 4GB. So I am inclined to believe this is a bug with tensorflow overestimating the memory required. Backprop should scale linearly with the batch size, and I am seeing this until it faults at an arbitrary point.\r\n\r\nAny ideas?! This is severely limiting my ability to train large models. Using a small batch size is just not going to cut it! -- the estimation of the gradient direction is very poor for large models if the batch size is too small."}