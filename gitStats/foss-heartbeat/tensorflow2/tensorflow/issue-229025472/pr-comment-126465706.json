{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126465706", "pull_request_review_id": 48952099, "id": 126465706, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNjQ2NTcwNg==", "diff_hunk": "@@ -2095,3 +2095,293 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(c, m)\n     return m, new_state\n+\n+\n+class MultiplicativeIntegrationRNNCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Multiplicative Integration RNN cell.\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1606.06630\n+\n+    Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\n+    On Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\n+  \"\"\"\n+\n+  def __init__(self, num_units, bias_start=0.0, alpha_start=1.0, beta_start=1.0,\n+      activation=math_ops.tanh, reuse=None):\n+    \"\"\"Initialize the Multiplicative Integration RNN cell.\n+\n+    Args:\n+      num_units: int, The number of units in the RNN cell.\n+      bias_start: float. Starting value to initialize the bias, b.\n+        0.0 by default.\n+      alpha_start: float. Starting value to initialize the bias, alpha.\n+        1.0 by default.\n+      beta_start: float. Starting value to initialize the two biases, \n+        beta_1 and beta_2. 1.0 by default.\n+      activation: Activation function of the inner states.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+    \"\"\"\n+    super(MultiplicativeIntegrationRNNCell, self).__init__(_reuse=reuse)\n+    self._num_units = num_units\n+    self._bias_start = bias_start\n+    self._alpha_start = alpha_start\n+    self._beta_start = beta_start\n+    self._activation = activation\n+    self._reuse = reuse\n+\n+  @property\n+  def state_size(self):\n+    return self._num_units\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  def call(self, inputs, state):\n+    \"\"\"Run one step of Multiplicative Integration RNN.\n+\n+    Args:\n+      inputs: input Tensor, 2D, batch x input size.\n+      state: state Tensor, 2D, batch x num units.\n+\n+    Returns:\n+      new_output: batch x num units, Tensor representing the output of the UGRNN\n+        after reading `inputs` when previous state was `state`. Identical to\n+        `new_state`.\n+      new_state: batch x num units, Tensor representing the state of the UGRNN\n+        after reading `inputs` when previous state was `state`.\n+\n+    Raises:\n+      ValueError: If input size cannot be inferred from inputs via\n+        static shape inference.    \n+    \"\"\"\n+    output = self._activation(\n+        _multiplicative_integration(\n+          [inputs, state], [self._num_units], True,\n+          bias_start=self._bias_start, alpha_start=self._alpha_start,\n+          beta_start=self._beta_start))\n+    return output, output\n+\n+\n+class MultiplicativeIntegrationGRUCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Multiplicative Integration GRU(Gated Recurrent Unit) \n+  recurrent network cell.\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1606.06630\n+\n+    Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\n+    On Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\n+  \"\"\"\n+\n+  def __init__(self, num_units, bias_start=0.0, alpha_start=1.0, \n+                beta_start=1.0, activation=math_ops.tanh, reuse=None):\n+    \"\"\"Initialize the Multiplicative Integration GRU cell.\n+\n+    Args:\n+      num_units: int, The number of units in the GRU cell.\n+      bias_start: float. Starting value to initialize the bias, b.\n+        1.0 by default.\n+      alpha_start: float. Starting value to initialize the bias, alpha.\n+        1.0 by default.\n+      beta_start: float. Starting value to initialize the two biases, \n+        beta_1 and beta_2. 1.0 by default.\n+      activation: Activation function of the inner states.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+    \"\"\"\n+    super(MultiplicativeIntegrationGRUCell, self).__init__(_reuse=reuse)\n+    self._num_units = num_units\n+    self._bias_start = bias_start\n+    self._alpha_start = alpha_start\n+    self._beta_start = beta_start\n+    self._activation = activation\n+    self._reuse = reuse\n+\n+  @property\n+  def state_size(self):\n+    return self._num_units\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  def call(self, inputs, state):\n+    \"\"\"Run one step of Multiplicative Integration Gated Recurrent Unit(GRU).\"\"\"\n+    with vs.variable_scope(\"gates\"):\n+      r, u = array_ops.split(\n+          value=_multiplicative_integration(\n+            [inputs, state], [self._num_units, self._num_units],\n+            True, bias_start=self._bias_start, alpha_start=self._alpha_start,\n+            beta_start=self._beta_start),\n+          num_or_size_splits=2,\n+          axis=1)\n+      r, u = math_ops.sigmoid(r), math_ops.sigmoid(u)\n+    with vs.variable_scope(\"candidate\"):\n+      c = _multiplicative_integration([inputs, r * state], [self._num_units],\n+              True, bias_start=self._bias_start, alpha_start=self._alpha_start,\n+              beta_start=self._beta_start)\n+      c = self._activation(c)\n+    new_h = u * state + (1 - u) * c\n+    return new_h, new_h\n+\n+\n+class MultiplicativeIntegrationLSTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Multiplicative Integration LSTM(Long short-term memory cell)\n+  recurrent network cell.\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1606.06630\n+\n+    Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\n+    On Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\n+  \"\"\"\n+\n+  def __init__(self, num_units, forget_bias=0.0, bias_start=0.0, alpha_start=1.0, \n+      beta_start=1.0, state_is_tuple=True, activation=math_ops.tanh, reuse=None):", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 151, "commit_id": "51a91cfa79172476e3aa07cfcbb5bd415981d54b", "original_commit_id": "d60e060430f77dd4383819b32636773770e3159e", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "remove state_is_tuple argument; always assume it's True.", "created_at": "2017-07-10T16:03:06Z", "updated_at": "2017-07-12T13:09:48Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9933#discussion_r126465706", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9933", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126465706"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9933#discussion_r126465706"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9933"}}, "body_html": "<p>remove state_is_tuple argument; always assume it's True.</p>", "body_text": "remove state_is_tuple argument; always assume it's True."}