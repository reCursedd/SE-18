{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126466766", "pull_request_review_id": 48953328, "id": 126466766, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNjQ2Njc2Ng==", "diff_hunk": "@@ -2095,3 +2095,293 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(c, m)\n     return m, new_state\n+\n+\n+class MultiplicativeIntegrationRNNCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Multiplicative Integration RNN cell.\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1606.06630\n+\n+    Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\n+    On Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\n+  \"\"\"\n+\n+  def __init__(self, num_units, bias_start=0.0, alpha_start=1.0, beta_start=1.0,\n+      activation=math_ops.tanh, reuse=None):\n+    \"\"\"Initialize the Multiplicative Integration RNN cell.\n+\n+    Args:\n+      num_units: int, The number of units in the RNN cell.\n+      bias_start: float. Starting value to initialize the bias, b.\n+        0.0 by default.\n+      alpha_start: float. Starting value to initialize the bias, alpha.\n+        1.0 by default.\n+      beta_start: float. Starting value to initialize the two biases, \n+        beta_1 and beta_2. 1.0 by default.\n+      activation: Activation function of the inner states.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+    \"\"\"\n+    super(MultiplicativeIntegrationRNNCell, self).__init__(_reuse=reuse)\n+    self._num_units = num_units\n+    self._bias_start = bias_start\n+    self._alpha_start = alpha_start\n+    self._beta_start = beta_start\n+    self._activation = activation\n+    self._reuse = reuse\n+\n+  @property\n+  def state_size(self):\n+    return self._num_units\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  def call(self, inputs, state):\n+    \"\"\"Run one step of Multiplicative Integration RNN.\n+\n+    Args:\n+      inputs: input Tensor, 2D, batch x input size.\n+      state: state Tensor, 2D, batch x num units.\n+\n+    Returns:\n+      new_output: batch x num units, Tensor representing the output of the UGRNN\n+        after reading `inputs` when previous state was `state`. Identical to\n+        `new_state`.\n+      new_state: batch x num units, Tensor representing the state of the UGRNN\n+        after reading `inputs` when previous state was `state`.\n+\n+    Raises:\n+      ValueError: If input size cannot be inferred from inputs via\n+        static shape inference.    \n+    \"\"\"\n+    output = self._activation(\n+        _multiplicative_integration(\n+          [inputs, state], [self._num_units], True,\n+          bias_start=self._bias_start, alpha_start=self._alpha_start,\n+          beta_start=self._beta_start))\n+    return output, output\n+\n+\n+class MultiplicativeIntegrationGRUCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Multiplicative Integration GRU(Gated Recurrent Unit) \n+  recurrent network cell.\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1606.06630\n+\n+    Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\n+    On Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\n+  \"\"\"\n+\n+  def __init__(self, num_units, bias_start=0.0, alpha_start=1.0, \n+                beta_start=1.0, activation=math_ops.tanh, reuse=None):\n+    \"\"\"Initialize the Multiplicative Integration GRU cell.\n+\n+    Args:\n+      num_units: int, The number of units in the GRU cell.\n+      bias_start: float. Starting value to initialize the bias, b.\n+        1.0 by default.\n+      alpha_start: float. Starting value to initialize the bias, alpha.\n+        1.0 by default.\n+      beta_start: float. Starting value to initialize the two biases, \n+        beta_1 and beta_2. 1.0 by default.\n+      activation: Activation function of the inner states.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+    \"\"\"\n+    super(MultiplicativeIntegrationGRUCell, self).__init__(_reuse=reuse)\n+    self._num_units = num_units\n+    self._bias_start = bias_start\n+    self._alpha_start = alpha_start\n+    self._beta_start = beta_start\n+    self._activation = activation\n+    self._reuse = reuse\n+\n+  @property\n+  def state_size(self):\n+    return self._num_units\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  def call(self, inputs, state):\n+    \"\"\"Run one step of Multiplicative Integration Gated Recurrent Unit(GRU).\"\"\"\n+    with vs.variable_scope(\"gates\"):\n+      r, u = array_ops.split(\n+          value=_multiplicative_integration(\n+            [inputs, state], [self._num_units, self._num_units],\n+            True, bias_start=self._bias_start, alpha_start=self._alpha_start,\n+            beta_start=self._beta_start),\n+          num_or_size_splits=2,\n+          axis=1)\n+      r, u = math_ops.sigmoid(r), math_ops.sigmoid(u)\n+    with vs.variable_scope(\"candidate\"):\n+      c = _multiplicative_integration([inputs, r * state], [self._num_units],\n+              True, bias_start=self._bias_start, alpha_start=self._alpha_start,\n+              beta_start=self._beta_start)\n+      c = self._activation(c)\n+    new_h = u * state + (1 - u) * c\n+    return new_h, new_h\n+\n+\n+class MultiplicativeIntegrationLSTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Multiplicative Integration LSTM(Long short-term memory cell)\n+  recurrent network cell.\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1606.06630\n+\n+    Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov,\n+    On Multiplicative Integration with Recurrent Neural Networks. NIPS, 2016.\n+  \"\"\"\n+\n+  def __init__(self, num_units, forget_bias=0.0, bias_start=0.0, alpha_start=1.0, \n+      beta_start=1.0, state_is_tuple=True, activation=math_ops.tanh, reuse=None):\n+    \"\"\"Initialize the Multiplicative Integration LSTM cell.\n+\n+    Args:\n+      num_units: int, The number of units in the LSTM cell.\n+      forget_bias: float, The bias added to forget gates,\n+        0.0 by default.\n+      bias_start: float. Starting value to initialize the bias, b.\n+        1.0 by default.\n+      alpha_start: float. Starting value to initialize the bias, alpha.\n+        1.0 by default.\n+      beta_start: float. Starting value to initialize the two bias, \n+        beta_1 and beta_2. 1.0 by default.\n+      state_is_tuple: If True, accepted and returned states are 2-tuples of\n+        the `c_state` and `m_state`.  If False, they are concatenated\n+        along the column axis.  The latter behavior will soon be deprecated.\n+      activation: Activation function of the inner states.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+    \"\"\"\n+    super(MultiplicativeIntegrationLSTMCell, self).__init__(_reuse=reuse)\n+    if not state_is_tuple:\n+      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n+                   \"deprecated.  Use state_is_tuple=True.\", self)\n+    self._num_units = num_units\n+    self._forget_bias = forget_bias\n+    self._bias_start = bias_start\n+    self._alpha_start = alpha_start\n+    self._beta_start = beta_start\n+    self._state_is_tuple = state_is_tuple\n+    self._activation = activation\n+    self._reuse = reuse\n+\n+  @property\n+  def state_size(self):\n+    return (rnn_cell_impl.LSTMStateTuple(self._num_units, self._num_units) \n+        if self._state_is_tuple else 2 * self._num_units)\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  def call(self, inputs, state):\n+    \"\"\"Run one step of Multiplicative Integration Long short-term memory(LSTM).\"\"\"\n+    if self._state_is_tuple:\n+      c, h = state\n+    else:\n+      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n+    concat = _multiplicative_integration([inputs, h],\n+        [self._num_units, self._num_units, self._num_units, self._num_units],\n+        True, bias_start=self._bias_start, alpha_start=self._alpha_start,\n+        beta_start=self._beta_start)\n+\n+    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n+    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)\n+\n+    new_c = (c * math_ops.sigmoid(f + self._forget_bias)\n+                + math_ops.sigmoid(i) * self._activation(j))\n+    new_h = self._activation(new_c) * math_ops.sigmoid(o)\n+\n+    if self._state_is_tuple:\n+      new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)\n+    else:\n+      new_state = array_ops.concat([new_c, new_h], 1)\n+    return new_h, new_state\n+\n+\n+def _multiplicative_integration(args, output_sizes, bias, bias_start=0.0, \n+    alpha_start=1.0, beta_start=1.0):\n+  \"\"\"Multiplicative Integration: alpha * args[0] * W[0] + beta1 * args[1] * W[1] \n+            + beta1 * args[2] * W[2],\n+        where alpha, beta1, beta2 and W[i] are variables.\n+\n+  Args:\n+    args: a list of 2D, batch x n, Tensors.\n+    output_sizes: a list of second dimension of W[i], where list[i] is int.\n+    bias: boolean, whether to add a bias term or not.\n+    bias_start: float, starting value to initialize the bias; 0 by default.\n+    alpha_start: float, starting value to initialize the alpha; \n+      1 by default.\n+    beta_start: float, starting value to initialize the beta1 and beta2; \n+      1 by default.\n+\n+  Returns:\n+    A 2D Tensor with shape [batch x output_size] equal \n+      to the Multiplicative Integration above.\n+\n+  Raises:\n+    ValueError: if some of the arguments has unspecified or wrong shape.\n+  \"\"\"\n+  if args is None or (nest.is_sequence(args) and not args):\n+    raise ValueError(\"`args` must be specified\")\n+  if not nest.is_sequence(args):\n+    raise ValueError(\"`args` must be list\")\n+  if len(args) != 2:\n+    raise ValueError(\"`args` must contain 2 tensors\")\n+  if output_sizes is None or (nest.is_sequence(output_sizes) \n+      and not output_sizes):\n+    raise ValueError(\"`output_sizes` must be specified\")\n+  if not nest.is_sequence(output_sizes):\n+    raise ValueError(\"`output_sizes` must be list\")\n+  if not all(isinstance(x, int) for x in output_sizes):\n+    raise ValueError(\"`output_sizes` must contain integers only\")\n+\n+  total_output_size = sum(output_sizes)\n+  arg_sizes = []\n+  total_arg_size = 0\n+  shapes = [x.get_shape() for x in args]\n+  for shape in shapes:\n+    if shape.ndims != 2:\n+      raise ValueError(\"multiplicative_ntegration is expecting 2D arguments: %s\" \n+          % shapes)\n+    if shape[1].value is None:\n+      raise ValueError(\"multiplicative_ntegration expects shape[1] to be \"\n+             \"provided for shape %s, but saw %s\" % (shape, shape[1]))\n+    else:\n+      total_arg_size += shape[1].value\n+      arg_sizes.append(shape[1].value)\n+\n+  dtype = [a.dtype for a in args][0]\n+\n+  scope = vs.get_variable_scope()\n+  with vs.variable_scope(scope) as outer_scope:", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": 244, "original_position": 274, "commit_id": "51a91cfa79172476e3aa07cfcbb5bd415981d54b", "original_commit_id": "d60e060430f77dd4383819b32636773770e3159e", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "you don't need this \"with\" statement because you're not modifying outer_scope anywhere.", "created_at": "2017-07-10T16:07:09Z", "updated_at": "2017-07-12T13:09:48Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9933#discussion_r126466766", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9933", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126466766"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9933#discussion_r126466766"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9933"}}, "body_html": "<p>you don't need this \"with\" statement because you're not modifying outer_scope anywhere.</p>", "body_text": "you don't need this \"with\" statement because you're not modifying outer_scope anywhere."}