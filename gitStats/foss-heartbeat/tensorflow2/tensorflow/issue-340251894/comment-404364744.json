{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404364744", "html_url": "https://github.com/tensorflow/tensorflow/issues/20699#issuecomment-404364744", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20699", "id": 404364744, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDM2NDc0NA==", "user": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T01:55:40Z", "updated_at": "2018-07-12T01:55:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Interesting. This looks like memory increases as more of the dataset is read (in order to repeat the dataset has to hang onto all points), and then it levels off which presumably is when it is repeating. Based on my reading of .train_and_evaluate(), it doesn't seem to be equivalent to:</p>\n<pre><code>while True:\n  my_estimator.train(...)\n  my_estimator.evaluate(...)\n</code></pre>\n<p>One simple thing you can try is to put a <code>print</code> statement in the input_fn to see if it is called once or repeatedly. If it is just called once for training and once for eval, then this behavior makes sense. (I would expect a memory leak to be linearly increasing, not asymptotic.)</p>", "body_text": "Interesting. This looks like memory increases as more of the dataset is read (in order to repeat the dataset has to hang onto all points), and then it levels off which presumably is when it is repeating. Based on my reading of .train_and_evaluate(), it doesn't seem to be equivalent to:\nwhile True:\n  my_estimator.train(...)\n  my_estimator.evaluate(...)\n\nOne simple thing you can try is to put a print statement in the input_fn to see if it is called once or repeatedly. If it is just called once for training and once for eval, then this behavior makes sense. (I would expect a memory leak to be linearly increasing, not asymptotic.)", "body": "Interesting. This looks like memory increases as more of the dataset is read (in order to repeat the dataset has to hang onto all points), and then it levels off which presumably is when it is repeating. Based on my reading of .train_and_evaluate(), it doesn't seem to be equivalent to:\r\n```\r\nwhile True:\r\n  my_estimator.train(...)\r\n  my_estimator.evaluate(...)\r\n```\r\n\r\nOne simple thing you can try is to put a `print` statement in the input_fn to see if it is called once or repeatedly. If it is just called once for training and once for eval, then this behavior makes sense. (I would expect a memory leak to be linearly increasing, not asymptotic.)"}