{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20699", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20699/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20699/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20699/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20699", "id": 340251894, "node_id": "MDU6SXNzdWUzNDAyNTE4OTQ=", "number": 20699, "title": "Increasing memory use of Estimator with Dataset", "user": {"login": "The-Fonz", "id": 5527529, "node_id": "MDQ6VXNlcjU1Mjc1Mjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5527529?v=4", "gravatar_id": "", "url": "https://api.github.com/users/The-Fonz", "html_url": "https://github.com/The-Fonz", "followers_url": "https://api.github.com/users/The-Fonz/followers", "following_url": "https://api.github.com/users/The-Fonz/following{/other_user}", "gists_url": "https://api.github.com/users/The-Fonz/gists{/gist_id}", "starred_url": "https://api.github.com/users/The-Fonz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/The-Fonz/subscriptions", "organizations_url": "https://api.github.com/users/The-Fonz/orgs", "repos_url": "https://api.github.com/users/The-Fonz/repos", "events_url": "https://api.github.com/users/The-Fonz/events{/privacy}", "received_events_url": "https://api.github.com/users/The-Fonz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-11T13:52:13Z", "updated_at": "2018-09-25T03:22:26Z", "closed_at": "2018-07-14T16:16:12Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 18.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary (pip install)</li>\n<li><strong>TensorFlow version (use command below)</strong>: Tested on 1.8.0 and 1.10.0-dev20180620</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>: GTX1050</li>\n<li><strong>Exact command to reproduce</strong>: See below for minimal test case</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm experiencing a memory leak that leaves some allocated memory behind after each Estimator training/evaluation run. I noticed this when my ML Engine jobs started failing after 1.5 hours of training with an OOM error on big CSV files, using the standard Dataset input pipeline with some preprocessing, batching, prefetching etc. The memory graph shows slow but steady increase of memory use until total RAM is filled. I observe the same behavior on my local machine.</p>\n<p>I've distilled it down to a minimal test case with use of Estimator and Dataset input and tested on my local machine, where I also get slowly increasing memory use over time. It seems most pronounced when using a big shuffle buffer, but after all this time staring at it I'm not so sure of anything any more.</p>\n<p>Test script (first <code>pip install -U memory_usage</code>):</p>\n<pre><code>import tempfile, time, argparse, os\n\n# Install with pip install -U memory_usage\nfrom memory_profiler import memory_usage\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--apply-shuffle-repeat', action='store_true')\nparser.add_argument('--shuffle', action='store_true')\nparser.add_argument('--gpu', action='store_true')\nargs = parser.parse_args()\n\n# Turn on/off gpu\nif not args.gpu:\n    print(\"Turning off GPU\")\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n\nprint(f\"Tensorflow version: {tf.GIT_VERSION} {tf.VERSION}\")\n\nprint(f\"Devices: {device_lib.list_local_devices()}\")\n\ntmpdir = tempfile.mkdtemp()\n\n_input = [1] * 5000000\n_target = [1] * 5000000\n\ndef input_fn(batch_size=4096):\n    data = tf.data.Dataset.from_tensor_slices(\n        ({'input': _input}, _target))\n    if args.shuffle:\n        data = data.shuffle(1000000)\n    elif args.apply_shuffle_repeat:\n        data = data.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=1000000, count=1))\n    data = data.batch(batch_size)\n    return data\n\nestimator = tf.estimator.DNNRegressor(\n    [64],\n    [tf.feature_column.numeric_column(key='input')],\n    model_dir=tmpdir)\n\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn)\n\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\n\n# Train and evaluate forever\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n</code></pre>\n<p>Output of <code>mprof run tf_estimator_memoryleak_minimal.py --apply-shuffle-repeat </code> (so it runs on CPU, for GPU use <code>--gpu</code> option) and plotting with <code>mprof plot</code> shows memory usage slowly but surely creeping upward:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/5527529/42563920-b7986b62-84ff-11e8-9fa4-304f319927a0.png\"><img src=\"https://user-images.githubusercontent.com/5527529/42563920-b7986b62-84ff-11e8-9fa4-304f319927a0.png\" alt=\"tf-memoryuse\" style=\"max-width:100%;\"></a></p>\n<p>I would expect memory usage after every train/evaluate loop to return to the initial level.</p>\n<p>Am I using the Dataset in the wrong way? Should I convert to an Iterator first? Is slightly increasing memory use normal and should I restart my train script every once in a while?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\nTensorFlow installed from (source or binary): Binary (pip install)\nTensorFlow version (use command below): Tested on 1.8.0 and 1.10.0-dev20180620\nPython version: 3.6\nCUDA/cuDNN version: 9.0\nGPU model and memory: GTX1050\nExact command to reproduce: See below for minimal test case\n\nDescribe the problem\nI'm experiencing a memory leak that leaves some allocated memory behind after each Estimator training/evaluation run. I noticed this when my ML Engine jobs started failing after 1.5 hours of training with an OOM error on big CSV files, using the standard Dataset input pipeline with some preprocessing, batching, prefetching etc. The memory graph shows slow but steady increase of memory use until total RAM is filled. I observe the same behavior on my local machine.\nI've distilled it down to a minimal test case with use of Estimator and Dataset input and tested on my local machine, where I also get slowly increasing memory use over time. It seems most pronounced when using a big shuffle buffer, but after all this time staring at it I'm not so sure of anything any more.\nTest script (first pip install -U memory_usage):\nimport tempfile, time, argparse, os\n\n# Install with pip install -U memory_usage\nfrom memory_profiler import memory_usage\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--apply-shuffle-repeat', action='store_true')\nparser.add_argument('--shuffle', action='store_true')\nparser.add_argument('--gpu', action='store_true')\nargs = parser.parse_args()\n\n# Turn on/off gpu\nif not args.gpu:\n    print(\"Turning off GPU\")\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n\nprint(f\"Tensorflow version: {tf.GIT_VERSION} {tf.VERSION}\")\n\nprint(f\"Devices: {device_lib.list_local_devices()}\")\n\ntmpdir = tempfile.mkdtemp()\n\n_input = [1] * 5000000\n_target = [1] * 5000000\n\ndef input_fn(batch_size=4096):\n    data = tf.data.Dataset.from_tensor_slices(\n        ({'input': _input}, _target))\n    if args.shuffle:\n        data = data.shuffle(1000000)\n    elif args.apply_shuffle_repeat:\n        data = data.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=1000000, count=1))\n    data = data.batch(batch_size)\n    return data\n\nestimator = tf.estimator.DNNRegressor(\n    [64],\n    [tf.feature_column.numeric_column(key='input')],\n    model_dir=tmpdir)\n\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn)\n\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\n\n# Train and evaluate forever\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\nOutput of mprof run tf_estimator_memoryleak_minimal.py --apply-shuffle-repeat  (so it runs on CPU, for GPU use --gpu option) and plotting with mprof plot shows memory usage slowly but surely creeping upward:\n\nI would expect memory usage after every train/evaluate loop to return to the initial level.\nAm I using the Dataset in the wrong way? Should I convert to an Iterator first? Is slightly increasing memory use normal and should I restart my train script every once in a while?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: Binary (pip install)\r\n- **TensorFlow version (use command below)**: Tested on 1.8.0 and 1.10.0-dev20180620\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GTX1050\r\n- **Exact command to reproduce**: See below for minimal test case\r\n\r\n### Describe the problem\r\nI'm experiencing a memory leak that leaves some allocated memory behind after each Estimator training/evaluation run. I noticed this when my ML Engine jobs started failing after 1.5 hours of training with an OOM error on big CSV files, using the standard Dataset input pipeline with some preprocessing, batching, prefetching etc. The memory graph shows slow but steady increase of memory use until total RAM is filled. I observe the same behavior on my local machine.\r\n\r\nI've distilled it down to a minimal test case with use of Estimator and Dataset input and tested on my local machine, where I also get slowly increasing memory use over time. It seems most pronounced when using a big shuffle buffer, but after all this time staring at it I'm not so sure of anything any more.\r\n\r\nTest script (first `pip install -U memory_usage`):\r\n```\r\nimport tempfile, time, argparse, os\r\n\r\n# Install with pip install -U memory_usage\r\nfrom memory_profiler import memory_usage\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--apply-shuffle-repeat', action='store_true')\r\nparser.add_argument('--shuffle', action='store_true')\r\nparser.add_argument('--gpu', action='store_true')\r\nargs = parser.parse_args()\r\n\r\n# Turn on/off gpu\r\nif not args.gpu:\r\n    print(\"Turning off GPU\")\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\n\r\nprint(f\"Tensorflow version: {tf.GIT_VERSION} {tf.VERSION}\")\r\n\r\nprint(f\"Devices: {device_lib.list_local_devices()}\")\r\n\r\ntmpdir = tempfile.mkdtemp()\r\n\r\n_input = [1] * 5000000\r\n_target = [1] * 5000000\r\n\r\ndef input_fn(batch_size=4096):\r\n    data = tf.data.Dataset.from_tensor_slices(\r\n        ({'input': _input}, _target))\r\n    if args.shuffle:\r\n        data = data.shuffle(1000000)\r\n    elif args.apply_shuffle_repeat:\r\n        data = data.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=1000000, count=1))\r\n    data = data.batch(batch_size)\r\n    return data\r\n\r\nestimator = tf.estimator.DNNRegressor(\r\n    [64],\r\n    [tf.feature_column.numeric_column(key='input')],\r\n    model_dir=tmpdir)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn)\r\n\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\r\n\r\n# Train and evaluate forever\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nOutput of `mprof run tf_estimator_memoryleak_minimal.py --apply-shuffle-repeat\r\n` (so it runs on CPU, for GPU use `--gpu` option) and plotting with `mprof plot` shows memory usage slowly but surely creeping upward:\r\n![tf-memoryuse](https://user-images.githubusercontent.com/5527529/42563920-b7986b62-84ff-11e8-9fa4-304f319927a0.png)\r\n\r\nI would expect memory usage after every train/evaluate loop to return to the initial level.\r\n\r\nAm I using the Dataset in the wrong way? Should I convert to an Iterator first? Is slightly increasing memory use normal and should I restart my train script every once in a while?\r\n"}