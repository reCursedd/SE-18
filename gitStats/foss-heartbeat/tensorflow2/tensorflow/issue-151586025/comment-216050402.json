{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/216050402", "html_url": "https://github.com/tensorflow/tensorflow/issues/2145#issuecomment-216050402", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2145", "id": 216050402, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNjA1MDQwMg==", "user": {"login": "liumilan", "id": 5533901, "node_id": "MDQ6VXNlcjU1MzM5MDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5533901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liumilan", "html_url": "https://github.com/liumilan", "followers_url": "https://api.github.com/users/liumilan/followers", "following_url": "https://api.github.com/users/liumilan/following{/other_user}", "gists_url": "https://api.github.com/users/liumilan/gists{/gist_id}", "starred_url": "https://api.github.com/users/liumilan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liumilan/subscriptions", "organizations_url": "https://api.github.com/users/liumilan/orgs", "repos_url": "https://api.github.com/users/liumilan/repos", "events_url": "https://api.github.com/users/liumilan/events{/privacy}", "received_events_url": "https://api.github.com/users/liumilan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-01T15:31:06Z", "updated_at": "2016-05-01T15:31:06Z", "author_association": "NONE", "body_html": "<p>I have the same result after running simpleP2P<br>\n[@sjs_88_31 release]# ./simpleP2P<br>\n[./simpleP2P] - Starting...<br>\nChecking for multiple GPUs...<br>\nCUDA-capable device count: 4</p>\n<blockquote>\n<p>GPU0 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)<br>\nGPU1 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)<br>\nGPU2 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)<br>\nGPU3 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)</p>\n</blockquote>\n<p>Checking GPU(s) for support of peer to peer memory access...</p>\n<blockquote>\n<p>Peer access from Tesla K40m (GPU0) -&gt; Tesla K40m (GPU1) : Yes<br>\nPeer access from Tesla K40m (GPU0) -&gt; Tesla K40m (GPU2) : No<br>\nPeer access from Tesla K40m (GPU0) -&gt; Tesla K40m (GPU3) : No<br>\nPeer access from Tesla K40m (GPU1) -&gt; Tesla K40m (GPU0) : Yes<br>\nPeer access from Tesla K40m (GPU1) -&gt; Tesla K40m (GPU2) : No<br>\nPeer access from Tesla K40m (GPU1) -&gt; Tesla K40m (GPU3) : No<br>\nPeer access from Tesla K40m (GPU2) -&gt; Tesla K40m (GPU0) : No<br>\nPeer access from Tesla K40m (GPU2) -&gt; Tesla K40m (GPU1) : No<br>\nPeer access from Tesla K40m (GPU2) -&gt; Tesla K40m (GPU3) : Yes<br>\nPeer access from Tesla K40m (GPU3) -&gt; Tesla K40m (GPU0) : No<br>\nPeer access from Tesla K40m (GPU3) -&gt; Tesla K40m (GPU1) : No<br>\nPeer access from Tesla K40m (GPU3) -&gt; Tesla K40m (GPU2) : Yes<br>\nEnabling peer access between GPU0 and GPU1...<br>\nChecking GPU0 and GPU1 for UVA capabilities...<br>\nTesla K40m (GPU0) supports UVA: Yes<br>\nTesla K40m (GPU1) supports UVA: Yes<br>\nBoth GPUs can support UVA, enabling...<br>\nAllocating buffers (64MB on GPU0, GPU1 and CPU Host)...<br>\nCreating event handles...<br>\ncudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 9.52GB/s<br>\nPreparing host buffer and memcpy to GPU0...<br>\nRun kernel on GPU1, taking source data from GPU0 and writing to GPU1...<br>\nRun kernel on GPU0, taking source data from GPU1 and writing to GPU0...<br>\nCopy data back to host from GPU0 and verify results...<br>\nDisabling peer access...<br>\nShutting down...<br>\nTest passed</p>\n</blockquote>", "body_text": "I have the same result after running simpleP2P\n[@sjs_88_31 release]# ./simpleP2P\n[./simpleP2P] - Starting...\nChecking for multiple GPUs...\nCUDA-capable device count: 4\n\nGPU0 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\nGPU1 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\nGPU2 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\nGPU3 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n\nChecking GPU(s) for support of peer to peer memory access...\n\nPeer access from Tesla K40m (GPU0) -> Tesla K40m (GPU1) : Yes\nPeer access from Tesla K40m (GPU0) -> Tesla K40m (GPU2) : No\nPeer access from Tesla K40m (GPU0) -> Tesla K40m (GPU3) : No\nPeer access from Tesla K40m (GPU1) -> Tesla K40m (GPU0) : Yes\nPeer access from Tesla K40m (GPU1) -> Tesla K40m (GPU2) : No\nPeer access from Tesla K40m (GPU1) -> Tesla K40m (GPU3) : No\nPeer access from Tesla K40m (GPU2) -> Tesla K40m (GPU0) : No\nPeer access from Tesla K40m (GPU2) -> Tesla K40m (GPU1) : No\nPeer access from Tesla K40m (GPU2) -> Tesla K40m (GPU3) : Yes\nPeer access from Tesla K40m (GPU3) -> Tesla K40m (GPU0) : No\nPeer access from Tesla K40m (GPU3) -> Tesla K40m (GPU1) : No\nPeer access from Tesla K40m (GPU3) -> Tesla K40m (GPU2) : Yes\nEnabling peer access between GPU0 and GPU1...\nChecking GPU0 and GPU1 for UVA capabilities...\nTesla K40m (GPU0) supports UVA: Yes\nTesla K40m (GPU1) supports UVA: Yes\nBoth GPUs can support UVA, enabling...\nAllocating buffers (64MB on GPU0, GPU1 and CPU Host)...\nCreating event handles...\ncudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 9.52GB/s\nPreparing host buffer and memcpy to GPU0...\nRun kernel on GPU1, taking source data from GPU0 and writing to GPU1...\nRun kernel on GPU0, taking source data from GPU1 and writing to GPU0...\nCopy data back to host from GPU0 and verify results...\nDisabling peer access...\nShutting down...\nTest passed", "body": "I have the same result after running simpleP2P\n[@sjs_88_31 release]# ./simpleP2P \n[./simpleP2P] - Starting...\nChecking for multiple GPUs...\nCUDA-capable device count: 4\n\n> GPU0 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n> GPU1 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n> GPU2 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n> GPU3 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n\nChecking GPU(s) for support of peer to peer memory access...\n\n> Peer access from Tesla K40m (GPU0) -> Tesla K40m (GPU1) : Yes\n> Peer access from Tesla K40m (GPU0) -> Tesla K40m (GPU2) : No\n> Peer access from Tesla K40m (GPU0) -> Tesla K40m (GPU3) : No\n> Peer access from Tesla K40m (GPU1) -> Tesla K40m (GPU0) : Yes\n> Peer access from Tesla K40m (GPU1) -> Tesla K40m (GPU2) : No\n> Peer access from Tesla K40m (GPU1) -> Tesla K40m (GPU3) : No\n> Peer access from Tesla K40m (GPU2) -> Tesla K40m (GPU0) : No\n> Peer access from Tesla K40m (GPU2) -> Tesla K40m (GPU1) : No\n> Peer access from Tesla K40m (GPU2) -> Tesla K40m (GPU3) : Yes\n> Peer access from Tesla K40m (GPU3) -> Tesla K40m (GPU0) : No\n> Peer access from Tesla K40m (GPU3) -> Tesla K40m (GPU1) : No\n> Peer access from Tesla K40m (GPU3) -> Tesla K40m (GPU2) : Yes\n> Enabling peer access between GPU0 and GPU1...\n> Checking GPU0 and GPU1 for UVA capabilities...\n> Tesla K40m (GPU0) supports UVA: Yes\n> Tesla K40m (GPU1) supports UVA: Yes\n> Both GPUs can support UVA, enabling...\n> Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...\n> Creating event handles...\n> cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 9.52GB/s\n> Preparing host buffer and memcpy to GPU0...\n> Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...\n> Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...\n> Copy data back to host from GPU0 and verify results...\n> Disabling peer access...\n> Shutting down...\n> Test passed\n"}