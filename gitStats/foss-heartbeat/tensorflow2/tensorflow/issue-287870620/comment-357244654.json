{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357244654", "html_url": "https://github.com/tensorflow/tensorflow/issues/16045#issuecomment-357244654", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16045", "id": 357244654, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzI0NDY1NA==", "user": {"login": "joeyearsley", "id": 1836025, "node_id": "MDQ6VXNlcjE4MzYwMjU=", "avatar_url": "https://avatars2.githubusercontent.com/u/1836025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joeyearsley", "html_url": "https://github.com/joeyearsley", "followers_url": "https://api.github.com/users/joeyearsley/followers", "following_url": "https://api.github.com/users/joeyearsley/following{/other_user}", "gists_url": "https://api.github.com/users/joeyearsley/gists{/gist_id}", "starred_url": "https://api.github.com/users/joeyearsley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joeyearsley/subscriptions", "organizations_url": "https://api.github.com/users/joeyearsley/orgs", "repos_url": "https://api.github.com/users/joeyearsley/repos", "events_url": "https://api.github.com/users/joeyearsley/events{/privacy}", "received_events_url": "https://api.github.com/users/joeyearsley/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T13:57:13Z", "updated_at": "2018-01-12T13:59:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Having debugged <em>1</em> a bit more, it seems that Tensorflow (or underlying libraries) does not totally respect the <code>config.gpu_options.visible_device_list</code>.<br>\nThis results in a minor <code>10Mb</code> being taken when not using XLA and <code>500Mb</code> when using XLA on GPUs which are not in use in that process. I.e. Having an 8 Card system and wanting to run 4 jobs results in memory issues.</p>\n<p>I'm still debugging the <em>2nd</em> issue on my end, as our internal benchmarks are tuned towards our private data - I'm trying to replicate in tf_benchmarks to see if it is a container problem/benchmark code issue/XLA code issue.</p>", "body_text": "Having debugged 1 a bit more, it seems that Tensorflow (or underlying libraries) does not totally respect the config.gpu_options.visible_device_list.\nThis results in a minor 10Mb being taken when not using XLA and 500Mb when using XLA on GPUs which are not in use in that process. I.e. Having an 8 Card system and wanting to run 4 jobs results in memory issues.\nI'm still debugging the 2nd issue on my end, as our internal benchmarks are tuned towards our private data - I'm trying to replicate in tf_benchmarks to see if it is a container problem/benchmark code issue/XLA code issue.", "body": "Having debugged _1_ a bit more, it seems that Tensorflow (or underlying libraries) does not totally respect the `config.gpu_options.visible_device_list`.\r\nThis results in a minor `10Mb` being taken when not using XLA and `500Mb` when using XLA on GPUs which are not in use in that process. I.e. Having an 8 Card system and wanting to run 4 jobs results in memory issues.\r\n\r\nI'm still debugging the _2nd_ issue on my end, as our internal benchmarks are tuned towards our private data - I'm trying to replicate in tf_benchmarks to see if it is a container problem/benchmark code issue/XLA code issue. "}