{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7970", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7970/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7970/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7970/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7970", "id": 211054422, "node_id": "MDU6SXNzdWUyMTEwNTQ0MjI=", "number": 7970, "title": "SyncReplicasOptimizer race condition strange behavior?", "user": {"login": "volvador", "id": 15655730, "node_id": "MDQ6VXNlcjE1NjU1NzMw", "avatar_url": "https://avatars1.githubusercontent.com/u/15655730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/volvador", "html_url": "https://github.com/volvador", "followers_url": "https://api.github.com/users/volvador/followers", "following_url": "https://api.github.com/users/volvador/following{/other_user}", "gists_url": "https://api.github.com/users/volvador/gists{/gist_id}", "starred_url": "https://api.github.com/users/volvador/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/volvador/subscriptions", "organizations_url": "https://api.github.com/users/volvador/orgs", "repos_url": "https://api.github.com/users/volvador/repos", "events_url": "https://api.github.com/users/volvador/events{/privacy}", "received_events_url": "https://api.github.com/users/volvador/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 35, "created_at": "2017-03-01T11:30:04Z", "updated_at": "2018-08-15T08:17:57Z", "closed_at": "2018-02-23T18:49:40Z", "author_association": "NONE", "body_html": "<p>It seems there is a strange race condition in SyncReplicasOptimizer leading to strange behaviour. I include below an example code to reproduce what seems to be a bug (hopefully in my code) as well as the commands to reproduce it (pretty much the same code as in mnist_replica.py).</p>\n<p>I am trying to implement  synchronized SGD using SyncReplicasOptimizer, I also used the queue trick to make the parameter server stop gracefully when all workers are done. I have 4 workers and 1 parameter server. Worker 0 is the chief worker.</p>\n<p>Please bear with me for the long explanation of the different issues (they depend on the order in which processes are launched)</p>\n<p>**** First kind of issue ****</p>\n<p>launch the processes in this order</p>\n<pre><code>python test.py --job_name ps\npython test.py --job_name worker --taks_index 0\npython test.py --job_name worker --taks_index 1\npython test.py --job_name worker --taks_index 2\npython test.py --job_name worker --taks_index 3\n</code></pre>\n<p>The last worker throws the following error :</p>\n<pre><code>I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Unavailable: {\"created\":\"@1488366991.043859719\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\n</code></pre>\n<p>and quits, and it happens also that it hangs (not realising that the variable epoch is greater than 4, triggering the break from the training loop, and the enqueue operation to let the ps stop gracefully).</p>\n<p>It also happen that all is fine, and the execution terminates without any errors.</p>\n<p>**** Second kind of issue ****</p>\n<p>launch the processes in this order</p>\n<pre><code>python test.py --job_name ps\npython test.py --job_name worker --taks_index 3\npython test.py --job_name worker --taks_index 2\npython test.py --job_name worker --taks_index 1\npython test.py --job_name worker --taks_index 0\n</code></pre>\n<p>The chief here being launched at last.</p>\n<p>Strangely, the chief completes the loop and quits ( I thought with SyncReplicasOptimizer it had to wait for the other workers to complete each step).</p>\n<p>As for the other workers, I had all sort of results when doing the same experiment many times</p>\n<ol>\n<li>\n<p>Some workers simply hang and do not execute a single step in the <code>while true</code> training loop</p>\n</li>\n<li>\n<p>Some execute some steps, then simply hang, apparently they lose contact with the chief, and do not realise that the variable <code>epoch</code> is greater than 4, triggering the `break from the training loop.</p>\n</li>\n</ol>\n<p>Thank you for help with this issue.</p>\n<p>Below is the code of test.py</p>\n<pre><code>import os\nimport shutil\nimport tempfile\nimport numpy as np\nimport pandas as pd\nimport argparse\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.regularizers import l2\nimport tensorflow as tf\nimport keras\n\nnb_samples = 50\nnb_features = 5\nX_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\nY_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\n\ndef build_keras_model(input_dim):\n  hidden_dim = 10\n\n  model = Sequential()\n  model.add(Dense(input_dim = input_dim,\n                  output_dim=hidden_dim,\n                  activation='tanh'\n                  ))\n\n  model.add(Dense(output_dim=1, activation='linear'))\n\n  model.compile(loss='mse', optimizer='adam')\n  \n  return model\n\n\n\n\n################################################\n# DISTRIBUTE\n################################################\n\nparser = argparse.ArgumentParser(description='tensorflow')\nparser.add_argument('--job_name', dest='job_name')\nparser.add_argument('--task_index', dest='task_index', default=0)\nargs = parser.parse_args()\n\n\nps_hosts = ['localhost:2222']\nworker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\njob_name = args.job_name\ntask_index = int(args.task_index)\n\n# Create a cluster from the parameter server and worker hosts.\ncluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n  \nserver = tf.train.Server(cluster,\n                         job_name=job_name,\n                         task_index=task_index,\n                         config=tf.ConfigProto(log_device_placement=True,\n                                               inter_op_parallelism_threads=1,\n                                               intra_op_parallelism_threads=1))\n\n\nif job_name =='ps':\n  with tf.device(\"/job:ps/task:0\"):\n    queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n  sess = tf.Session(server.target)\n  # wait until all workers are done\n  for i in range(len(worker_hosts)):\n    sess.run(queue.dequeue())\nelse:\n  with tf.device(tf.train.replica_device_setter(\n                              worker_device=\"/job:worker/task:%d\" % task_index,\n                              cluster=cluster)):\n\n    keras.backend.set_learning_phase(1)\n    keras.backend.manual_variable_initialization(True)\n\n    model = build_keras_model(nb_features)\n    preds = model.output\n    targets = tf.placeholder(tf.float32, [None, 1])\n    total_loss = tf.reduce_mean(\n                        keras.objectives.mean_squared_error(targets, preds))\n\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n    # For early stopping management\n    epoch = tf.Variable(0, name=\"epoch\", trainable=False)\n    inc_epoch_op = tf.assign_add(epoch, 1)\n\n    is_chief=(task_index == 0)\n\n    opt = tf.train.AdamOptimizer()\n    num_workers = len(worker_hosts)\n    replicas_to_aggregate = num_workers\n    opt = tf.train.SyncReplicasOptimizer(\n                                         opt,\n                                         replicas_to_aggregate=replicas_to_aggregate,\n                                         total_num_replicas=num_workers,\n                                         name=\"sync_replicas\")\n\n    train_op = opt.minimize(total_loss, global_step=global_step)\n    local_init_op = opt.local_step_init_op\n    if is_chief:\n      local_init_op = opt.chief_init_op\n    ready_for_local_init_op = opt.ready_for_local_init_op\n\n    # Initial token and chief queue runners required by the sync_replicas mode\n    chief_queue_runner = opt.get_chief_queue_runner()\n    sync_init_op = opt.get_init_tokens_op()\n\n    init_op = tf.global_variables_initializer()\n    with tf.device(\"/job:ps/task:0\"):\n      queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n      enqueue_op = queue.enqueue(1)\n \n    train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\n    sv = tf.train.Supervisor(\n                             is_chief=is_chief,\n                             logdir=train_dir,\n                             init_op=init_op,\n                             local_init_op=local_init_op,\n                             ready_for_local_init_op=ready_for_local_init_op,\n                             recovery_wait_secs=1,\n                             global_step=global_step)\n    \n    print '######################################### ALL CREATED'\n    sess = sv.prepare_or_wait_for_session(server.target)\n    keras.backend.set_session(sess)\n    print '#######  SESSION OK ********'\n    if is_chief:\n      sess.run(sync_init_op)\n      sv.start_queue_runners(sess, [chief_queue_runner])\n    local_step = 0\n    while True:\n      train_feed = {model.input: X_train, targets: Y_train}\n\n      _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n      loss = sess.run(total_loss, feed_dict = train_feed)\n      if is_chief:\n        sess.run(inc_epoch_op)\n      local_step += 1\n      print '## epoch ', epoch.eval(sess)\n      if epoch.eval(sess) &gt; 4:\n        print '######################  TRYING TO LEAVE'\n        break\n\n    shutil.rmtree(train_dir)\n    print '######################  WHILE LOOP LEFT'\n    sess.run(enqueue_op)\n    print '## ENQUEUE OP DONE'\n</code></pre>", "body_text": "It seems there is a strange race condition in SyncReplicasOptimizer leading to strange behaviour. I include below an example code to reproduce what seems to be a bug (hopefully in my code) as well as the commands to reproduce it (pretty much the same code as in mnist_replica.py).\nI am trying to implement  synchronized SGD using SyncReplicasOptimizer, I also used the queue trick to make the parameter server stop gracefully when all workers are done. I have 4 workers and 1 parameter server. Worker 0 is the chief worker.\nPlease bear with me for the long explanation of the different issues (they depend on the order in which processes are launched)\n**** First kind of issue ****\nlaunch the processes in this order\npython test.py --job_name ps\npython test.py --job_name worker --taks_index 0\npython test.py --job_name worker --taks_index 1\npython test.py --job_name worker --taks_index 2\npython test.py --job_name worker --taks_index 3\n\nThe last worker throws the following error :\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Unavailable: {\"created\":\"@1488366991.043859719\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\n\nand quits, and it happens also that it hangs (not realising that the variable epoch is greater than 4, triggering the break from the training loop, and the enqueue operation to let the ps stop gracefully).\nIt also happen that all is fine, and the execution terminates without any errors.\n**** Second kind of issue ****\nlaunch the processes in this order\npython test.py --job_name ps\npython test.py --job_name worker --taks_index 3\npython test.py --job_name worker --taks_index 2\npython test.py --job_name worker --taks_index 1\npython test.py --job_name worker --taks_index 0\n\nThe chief here being launched at last.\nStrangely, the chief completes the loop and quits ( I thought with SyncReplicasOptimizer it had to wait for the other workers to complete each step).\nAs for the other workers, I had all sort of results when doing the same experiment many times\n\n\nSome workers simply hang and do not execute a single step in the while true training loop\n\n\nSome execute some steps, then simply hang, apparently they lose contact with the chief, and do not realise that the variable epoch is greater than 4, triggering the `break from the training loop.\n\n\nThank you for help with this issue.\nBelow is the code of test.py\nimport os\nimport shutil\nimport tempfile\nimport numpy as np\nimport pandas as pd\nimport argparse\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.regularizers import l2\nimport tensorflow as tf\nimport keras\n\nnb_samples = 50\nnb_features = 5\nX_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\nY_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\n\ndef build_keras_model(input_dim):\n  hidden_dim = 10\n\n  model = Sequential()\n  model.add(Dense(input_dim = input_dim,\n                  output_dim=hidden_dim,\n                  activation='tanh'\n                  ))\n\n  model.add(Dense(output_dim=1, activation='linear'))\n\n  model.compile(loss='mse', optimizer='adam')\n  \n  return model\n\n\n\n\n################################################\n# DISTRIBUTE\n################################################\n\nparser = argparse.ArgumentParser(description='tensorflow')\nparser.add_argument('--job_name', dest='job_name')\nparser.add_argument('--task_index', dest='task_index', default=0)\nargs = parser.parse_args()\n\n\nps_hosts = ['localhost:2222']\nworker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\njob_name = args.job_name\ntask_index = int(args.task_index)\n\n# Create a cluster from the parameter server and worker hosts.\ncluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n  \nserver = tf.train.Server(cluster,\n                         job_name=job_name,\n                         task_index=task_index,\n                         config=tf.ConfigProto(log_device_placement=True,\n                                               inter_op_parallelism_threads=1,\n                                               intra_op_parallelism_threads=1))\n\n\nif job_name =='ps':\n  with tf.device(\"/job:ps/task:0\"):\n    queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n  sess = tf.Session(server.target)\n  # wait until all workers are done\n  for i in range(len(worker_hosts)):\n    sess.run(queue.dequeue())\nelse:\n  with tf.device(tf.train.replica_device_setter(\n                              worker_device=\"/job:worker/task:%d\" % task_index,\n                              cluster=cluster)):\n\n    keras.backend.set_learning_phase(1)\n    keras.backend.manual_variable_initialization(True)\n\n    model = build_keras_model(nb_features)\n    preds = model.output\n    targets = tf.placeholder(tf.float32, [None, 1])\n    total_loss = tf.reduce_mean(\n                        keras.objectives.mean_squared_error(targets, preds))\n\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n    # For early stopping management\n    epoch = tf.Variable(0, name=\"epoch\", trainable=False)\n    inc_epoch_op = tf.assign_add(epoch, 1)\n\n    is_chief=(task_index == 0)\n\n    opt = tf.train.AdamOptimizer()\n    num_workers = len(worker_hosts)\n    replicas_to_aggregate = num_workers\n    opt = tf.train.SyncReplicasOptimizer(\n                                         opt,\n                                         replicas_to_aggregate=replicas_to_aggregate,\n                                         total_num_replicas=num_workers,\n                                         name=\"sync_replicas\")\n\n    train_op = opt.minimize(total_loss, global_step=global_step)\n    local_init_op = opt.local_step_init_op\n    if is_chief:\n      local_init_op = opt.chief_init_op\n    ready_for_local_init_op = opt.ready_for_local_init_op\n\n    # Initial token and chief queue runners required by the sync_replicas mode\n    chief_queue_runner = opt.get_chief_queue_runner()\n    sync_init_op = opt.get_init_tokens_op()\n\n    init_op = tf.global_variables_initializer()\n    with tf.device(\"/job:ps/task:0\"):\n      queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n      enqueue_op = queue.enqueue(1)\n \n    train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\n    sv = tf.train.Supervisor(\n                             is_chief=is_chief,\n                             logdir=train_dir,\n                             init_op=init_op,\n                             local_init_op=local_init_op,\n                             ready_for_local_init_op=ready_for_local_init_op,\n                             recovery_wait_secs=1,\n                             global_step=global_step)\n    \n    print '######################################### ALL CREATED'\n    sess = sv.prepare_or_wait_for_session(server.target)\n    keras.backend.set_session(sess)\n    print '#######  SESSION OK ********'\n    if is_chief:\n      sess.run(sync_init_op)\n      sv.start_queue_runners(sess, [chief_queue_runner])\n    local_step = 0\n    while True:\n      train_feed = {model.input: X_train, targets: Y_train}\n\n      _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n      loss = sess.run(total_loss, feed_dict = train_feed)\n      if is_chief:\n        sess.run(inc_epoch_op)\n      local_step += 1\n      print '## epoch ', epoch.eval(sess)\n      if epoch.eval(sess) > 4:\n        print '######################  TRYING TO LEAVE'\n        break\n\n    shutil.rmtree(train_dir)\n    print '######################  WHILE LOOP LEFT'\n    sess.run(enqueue_op)\n    print '## ENQUEUE OP DONE'", "body": "It seems there is a strange race condition in SyncReplicasOptimizer leading to strange behaviour. I include below an example code to reproduce what seems to be a bug (hopefully in my code) as well as the commands to reproduce it (pretty much the same code as in mnist_replica.py).\r\n\r\n\r\nI am trying to implement  synchronized SGD using SyncReplicasOptimizer, I also used the queue trick to make the parameter server stop gracefully when all workers are done. I have 4 workers and 1 parameter server. Worker 0 is the chief worker.\r\n\r\nPlease bear with me for the long explanation of the different issues (they depend on the order in which processes are launched)\r\n\r\n**** First kind of issue ****\r\n\r\nlaunch the processes in this order \r\n    \r\n    python test.py --job_name ps\r\n    python test.py --job_name worker --taks_index 0\r\n    python test.py --job_name worker --taks_index 1\r\n    python test.py --job_name worker --taks_index 2\r\n    python test.py --job_name worker --taks_index 3\r\n\r\nThe last worker throws the following error :\r\n\r\n    I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Unavailable: {\"created\":\"@1488366991.043859719\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\n\r\nand quits, and it happens also that it hangs (not realising that the variable epoch is greater than 4, triggering the break from the training loop, and the enqueue operation to let the ps stop gracefully).\r\n\r\nIt also happen that all is fine, and the execution terminates without any errors.\r\n\r\n\r\n**** Second kind of issue ****\r\n\r\nlaunch the processes in this order \r\n    \r\n    python test.py --job_name ps\r\n    python test.py --job_name worker --taks_index 3\r\n    python test.py --job_name worker --taks_index 2\r\n    python test.py --job_name worker --taks_index 1\r\n    python test.py --job_name worker --taks_index 0\r\n\r\n\r\nThe chief here being launched at last.\r\n\r\nStrangely, the chief completes the loop and quits ( I thought with SyncReplicasOptimizer it had to wait for the other workers to complete each step).\r\n\r\nAs for the other workers, I had all sort of results when doing the same experiment many times \r\n\r\n1) Some workers simply hang and do not execute a single step in the `while true` training loop\r\n\r\n2) Some execute some steps, then simply hang, apparently they lose contact with the chief, and do not realise that the variable `epoch` is greater than 4, triggering the `break from the training loop.\r\n\r\nThank you for help with this issue.\r\n\r\nBelow is the code of test.py\r\n\r\n    import os\r\n    import shutil\r\n    import tempfile\r\n    import numpy as np\r\n    import pandas as pd\r\n    import argparse\r\n    \r\n    from keras.models import Sequential\r\n    from keras.layers.core import Dense\r\n    from keras.regularizers import l2\r\n    import tensorflow as tf\r\n    import keras\r\n    \r\n    nb_samples = 50\r\n    nb_features = 5\r\n    X_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\r\n    Y_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\r\n    \r\n    def build_keras_model(input_dim):\r\n      hidden_dim = 10\r\n    \r\n      model = Sequential()\r\n      model.add(Dense(input_dim = input_dim,\r\n                      output_dim=hidden_dim,\r\n                      activation='tanh'\r\n                      ))\r\n    \r\n      model.add(Dense(output_dim=1, activation='linear'))\r\n    \r\n      model.compile(loss='mse', optimizer='adam')\r\n      \r\n      return model\r\n    \r\n    \r\n    \r\n    \r\n    ################################################\r\n    # DISTRIBUTE\r\n    ################################################\r\n    \r\n    parser = argparse.ArgumentParser(description='tensorflow')\r\n    parser.add_argument('--job_name', dest='job_name')\r\n    parser.add_argument('--task_index', dest='task_index', default=0)\r\n    args = parser.parse_args()\r\n    \r\n    \r\n    ps_hosts = ['localhost:2222']\r\n    worker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\r\n    job_name = args.job_name\r\n    task_index = int(args.task_index)\r\n    \r\n    # Create a cluster from the parameter server and worker hosts.\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n      \r\n    server = tf.train.Server(cluster,\r\n                             job_name=job_name,\r\n                             task_index=task_index,\r\n                             config=tf.ConfigProto(log_device_placement=True,\r\n                                                   inter_op_parallelism_threads=1,\r\n                                                   intra_op_parallelism_threads=1))\r\n    \r\n    \r\n    if job_name =='ps':\r\n      with tf.device(\"/job:ps/task:0\"):\r\n        queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n      sess = tf.Session(server.target)\r\n      # wait until all workers are done\r\n      for i in range(len(worker_hosts)):\r\n        sess.run(queue.dequeue())\r\n    else:\r\n      with tf.device(tf.train.replica_device_setter(\r\n                                  worker_device=\"/job:worker/task:%d\" % task_index,\r\n                                  cluster=cluster)):\r\n    \r\n        keras.backend.set_learning_phase(1)\r\n        keras.backend.manual_variable_initialization(True)\r\n    \r\n        model = build_keras_model(nb_features)\r\n        preds = model.output\r\n        targets = tf.placeholder(tf.float32, [None, 1])\r\n        total_loss = tf.reduce_mean(\r\n                            keras.objectives.mean_squared_error(targets, preds))\r\n    \r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n        # For early stopping management\r\n        epoch = tf.Variable(0, name=\"epoch\", trainable=False)\r\n        inc_epoch_op = tf.assign_add(epoch, 1)\r\n    \r\n        is_chief=(task_index == 0)\r\n    \r\n        opt = tf.train.AdamOptimizer()\r\n        num_workers = len(worker_hosts)\r\n        replicas_to_aggregate = num_workers\r\n        opt = tf.train.SyncReplicasOptimizer(\r\n                                             opt,\r\n                                             replicas_to_aggregate=replicas_to_aggregate,\r\n                                             total_num_replicas=num_workers,\r\n                                             name=\"sync_replicas\")\r\n    \r\n        train_op = opt.minimize(total_loss, global_step=global_step)\r\n        local_init_op = opt.local_step_init_op\r\n        if is_chief:\r\n          local_init_op = opt.chief_init_op\r\n        ready_for_local_init_op = opt.ready_for_local_init_op\r\n    \r\n        # Initial token and chief queue runners required by the sync_replicas mode\r\n        chief_queue_runner = opt.get_chief_queue_runner()\r\n        sync_init_op = opt.get_init_tokens_op()\r\n    \r\n        init_op = tf.global_variables_initializer()\r\n        with tf.device(\"/job:ps/task:0\"):\r\n          queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n          enqueue_op = queue.enqueue(1)\r\n     \r\n        train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\r\n        sv = tf.train.Supervisor(\r\n                                 is_chief=is_chief,\r\n                                 logdir=train_dir,\r\n                                 init_op=init_op,\r\n                                 local_init_op=local_init_op,\r\n                                 ready_for_local_init_op=ready_for_local_init_op,\r\n                                 recovery_wait_secs=1,\r\n                                 global_step=global_step)\r\n        \r\n        print '######################################### ALL CREATED'\r\n        sess = sv.prepare_or_wait_for_session(server.target)\r\n        keras.backend.set_session(sess)\r\n        print '#######  SESSION OK ********'\r\n        if is_chief:\r\n          sess.run(sync_init_op)\r\n          sv.start_queue_runners(sess, [chief_queue_runner])\r\n        local_step = 0\r\n        while True:\r\n          train_feed = {model.input: X_train, targets: Y_train}\r\n    \r\n          _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n          loss = sess.run(total_loss, feed_dict = train_feed)\r\n          if is_chief:\r\n            sess.run(inc_epoch_op)\r\n          local_step += 1\r\n          print '## epoch ', epoch.eval(sess)\r\n          if epoch.eval(sess) > 4:\r\n            print '######################  TRYING TO LEAVE'\r\n            break\r\n    \r\n        shutil.rmtree(train_dir)\r\n        print '######################  WHILE LOOP LEFT'\r\n        sess.run(enqueue_op)\r\n        print '## ENQUEUE OP DONE'\r\n    \r\n"}