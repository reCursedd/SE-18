{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285764640", "html_url": "https://github.com/tensorflow/tensorflow/issues/7970#issuecomment-285764640", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7970", "id": 285764640, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTc2NDY0MA==", "user": {"login": "volvador", "id": 15655730, "node_id": "MDQ6VXNlcjE1NjU1NzMw", "avatar_url": "https://avatars1.githubusercontent.com/u/15655730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/volvador", "html_url": "https://github.com/volvador", "followers_url": "https://api.github.com/users/volvador/followers", "following_url": "https://api.github.com/users/volvador/following{/other_user}", "gists_url": "https://api.github.com/users/volvador/gists{/gist_id}", "starred_url": "https://api.github.com/users/volvador/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/volvador/subscriptions", "organizations_url": "https://api.github.com/users/volvador/orgs", "repos_url": "https://api.github.com/users/volvador/repos", "events_url": "https://api.github.com/users/volvador/events{/privacy}", "received_events_url": "https://api.github.com/users/volvador/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-10T19:38:47Z", "updated_at": "2017-03-10T19:38:47Z", "author_association": "NONE", "body_html": "<p>Thanks. It still does not work. Only the chief executes the train loop (even if in the code replicas_to_aggregate=nb_workers), and it ends with an exception&gt; I put below the new code as well as the output of the chief worker</p>\n<pre><code>import os\nimport shutil\nimport tempfile\nimport numpy as np\nimport pandas as pd\nimport argparse\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.regularizers import l2\nimport tensorflow as tf\nimport keras\n\nnb_samples = 50\nnb_features = 5\nX_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\nY_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\n\ndef build_keras_model(input_dim):\n  hidden_dim = 10\n\n  model = Sequential()\n  model.add(Dense(input_dim = input_dim,\n                  output_dim=hidden_dim,\n                  activation='tanh'\n                  ))\n\n  model.add(Dense(output_dim=1, activation='linear'))\n\n  model.compile(loss='mse', optimizer='adam')\n  \n  return model\n\n\n\n\n################################################\n# DISTRIBUTE\n################################################\n\nparser = argparse.ArgumentParser(description='tensorflow')\nparser.add_argument('--job_name', dest='job_name')\nparser.add_argument('--task_index', dest='task_index', default=0)\nargs = parser.parse_args()\n\n\nps_hosts = ['localhost:2222']\nworker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\njob_name = args.job_name\ntask_index = int(args.task_index)\n\n# Create a cluster from the parameter server and worker hosts.\ncluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n  \nserver = tf.train.Server(cluster,\n                         job_name=job_name,\n                         task_index=task_index,\n                         config=tf.ConfigProto(log_device_placement=True,\n                                               inter_op_parallelism_threads=1,\n                                               intra_op_parallelism_threads=1))\n\n\nif job_name =='ps':\n  with tf.device(\"/job:ps/task:0\"):\n    queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n  sess = tf.Session(server.target)\n  # wait until all workers are done\n  for i in range(len(worker_hosts)):\n    sess.run(queue.dequeue())\nelse:\n  with tf.device(tf.train.replica_device_setter(\n                              worker_device=\"/job:worker/task:%d\" % task_index,\n                              cluster=cluster)):\n\n    keras.backend.set_learning_phase(1)\n    keras.backend.manual_variable_initialization(True)\n\n    model = build_keras_model(nb_features)\n    preds = model.output\n    targets = tf.placeholder(tf.float32, [None, 1])\n    total_loss = tf.reduce_mean(\n                        keras.objectives.mean_squared_error(targets, preds))\n\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n    # For stopping management\n    epoch = tf.Variable(0, name=\"epoch\", trainable=False)\n    inc_epoch_op = tf.assign_add(epoch, 1)\n\n    is_chief=(task_index == 0)\n\n    opt = tf.train.AdamOptimizer()\n    num_workers = len(worker_hosts)\n    replicas_to_aggregate = num_workers\n    opt = tf.train.SyncReplicasOptimizer(\n                                         opt,\n                                         replicas_to_aggregate=replicas_to_aggregate,\n                                         total_num_replicas=num_workers,\n                                         name=\"sync_replicas\")\n\n    train_op = opt.minimize(total_loss, global_step=global_step)\n    with tf.device(\"/job:ps/task:0\"):\n      queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n      enqueue_op = queue.enqueue(1)\n \n    train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\n    \n    print '######################################### ALL CREATED'\n    sync_replicas_hook = opt.make_session_run_hook(is_chief)\n    with tf.train.MonitoredTrainingSession(\n               master=server.target, is_chief=is_chief, checkpoint_dir=train_dir,\n                     hooks=[sync_replicas_hook]) as sess:\n      keras.backend.set_session(sess)\n      print '#######  SESSION OK ********'\n      local_step = 0\n      while True:\n        train_feed = {model.input: X_train, targets: Y_train}\n\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n        loss = sess.run(total_loss, feed_dict = train_feed)\n        if is_chief:\n          sess.run(inc_epoch_op)\n        local_step += 1\n        print '## epoch ', epoch.eval(sess)\n        if epoch.eval(sess) &gt; 4:\n          print '######################  TRYING TO LEAVE'\n          break\n\n    print '######################  WHILE LOOP LEFT'\n    sess.run(enqueue_op)\n    print '## ENQUEUE OP DONE'\n    shutil.rmtree(train_dir)\n</code></pre>\n<p>and this is the output of the chief worker</p>\n<pre><code>#######  SESSION OK ********\n## epoch  1\n## epoch  2\n## epoch  3\n## epoch  4\n## epoch  5\n######################  TRYING TO LEAVE\nException in thread Thread-2:\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/threading.py\", line 811, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib64/python2.7/threading.py\", line 764, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 250, in _run\n    coord.request_stop(e)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 211, in request_stop\n    six.reraise(*sys.exc_info())\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\nCancelledError: RunManyGraphs\n\nTraceback (most recent call last):\n  File \"test.py\", line 148, in &lt;module&gt;\n    break\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\n    self._close_internal(exception_type)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\n    self._sess.close()\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 739, in close\n    self._sess.close()\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 827, in close\n    self._coord.join()\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 390, in join\n    \" \".join(stragglers))\nRuntimeError: Coordinator stopped with threads still running: Thread-2\n</code></pre>", "body_text": "Thanks. It still does not work. Only the chief executes the train loop (even if in the code replicas_to_aggregate=nb_workers), and it ends with an exception> I put below the new code as well as the output of the chief worker\nimport os\nimport shutil\nimport tempfile\nimport numpy as np\nimport pandas as pd\nimport argparse\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.regularizers import l2\nimport tensorflow as tf\nimport keras\n\nnb_samples = 50\nnb_features = 5\nX_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\nY_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\n\ndef build_keras_model(input_dim):\n  hidden_dim = 10\n\n  model = Sequential()\n  model.add(Dense(input_dim = input_dim,\n                  output_dim=hidden_dim,\n                  activation='tanh'\n                  ))\n\n  model.add(Dense(output_dim=1, activation='linear'))\n\n  model.compile(loss='mse', optimizer='adam')\n  \n  return model\n\n\n\n\n################################################\n# DISTRIBUTE\n################################################\n\nparser = argparse.ArgumentParser(description='tensorflow')\nparser.add_argument('--job_name', dest='job_name')\nparser.add_argument('--task_index', dest='task_index', default=0)\nargs = parser.parse_args()\n\n\nps_hosts = ['localhost:2222']\nworker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\njob_name = args.job_name\ntask_index = int(args.task_index)\n\n# Create a cluster from the parameter server and worker hosts.\ncluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n  \nserver = tf.train.Server(cluster,\n                         job_name=job_name,\n                         task_index=task_index,\n                         config=tf.ConfigProto(log_device_placement=True,\n                                               inter_op_parallelism_threads=1,\n                                               intra_op_parallelism_threads=1))\n\n\nif job_name =='ps':\n  with tf.device(\"/job:ps/task:0\"):\n    queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n  sess = tf.Session(server.target)\n  # wait until all workers are done\n  for i in range(len(worker_hosts)):\n    sess.run(queue.dequeue())\nelse:\n  with tf.device(tf.train.replica_device_setter(\n                              worker_device=\"/job:worker/task:%d\" % task_index,\n                              cluster=cluster)):\n\n    keras.backend.set_learning_phase(1)\n    keras.backend.manual_variable_initialization(True)\n\n    model = build_keras_model(nb_features)\n    preds = model.output\n    targets = tf.placeholder(tf.float32, [None, 1])\n    total_loss = tf.reduce_mean(\n                        keras.objectives.mean_squared_error(targets, preds))\n\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n    # For stopping management\n    epoch = tf.Variable(0, name=\"epoch\", trainable=False)\n    inc_epoch_op = tf.assign_add(epoch, 1)\n\n    is_chief=(task_index == 0)\n\n    opt = tf.train.AdamOptimizer()\n    num_workers = len(worker_hosts)\n    replicas_to_aggregate = num_workers\n    opt = tf.train.SyncReplicasOptimizer(\n                                         opt,\n                                         replicas_to_aggregate=replicas_to_aggregate,\n                                         total_num_replicas=num_workers,\n                                         name=\"sync_replicas\")\n\n    train_op = opt.minimize(total_loss, global_step=global_step)\n    with tf.device(\"/job:ps/task:0\"):\n      queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\n      enqueue_op = queue.enqueue(1)\n \n    train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\n    \n    print '######################################### ALL CREATED'\n    sync_replicas_hook = opt.make_session_run_hook(is_chief)\n    with tf.train.MonitoredTrainingSession(\n               master=server.target, is_chief=is_chief, checkpoint_dir=train_dir,\n                     hooks=[sync_replicas_hook]) as sess:\n      keras.backend.set_session(sess)\n      print '#######  SESSION OK ********'\n      local_step = 0\n      while True:\n        train_feed = {model.input: X_train, targets: Y_train}\n\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n        loss = sess.run(total_loss, feed_dict = train_feed)\n        if is_chief:\n          sess.run(inc_epoch_op)\n        local_step += 1\n        print '## epoch ', epoch.eval(sess)\n        if epoch.eval(sess) > 4:\n          print '######################  TRYING TO LEAVE'\n          break\n\n    print '######################  WHILE LOOP LEFT'\n    sess.run(enqueue_op)\n    print '## ENQUEUE OP DONE'\n    shutil.rmtree(train_dir)\n\nand this is the output of the chief worker\n#######  SESSION OK ********\n## epoch  1\n## epoch  2\n## epoch  3\n## epoch  4\n## epoch  5\n######################  TRYING TO LEAVE\nException in thread Thread-2:\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/threading.py\", line 811, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib64/python2.7/threading.py\", line 764, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 250, in _run\n    coord.request_stop(e)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 211, in request_stop\n    six.reraise(*sys.exc_info())\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\nCancelledError: RunManyGraphs\n\nTraceback (most recent call last):\n  File \"test.py\", line 148, in <module>\n    break\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\n    self._close_internal(exception_type)\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\n    self._sess.close()\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 739, in close\n    self._sess.close()\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 827, in close\n    self._coord.join()\n  File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 390, in join\n    \" \".join(stragglers))\nRuntimeError: Coordinator stopped with threads still running: Thread-2", "body": "Thanks. It still does not work. Only the chief executes the train loop (even if in the code replicas_to_aggregate=nb_workers), and it ends with an exception> I put below the new code as well as the output of the chief worker \r\n\r\n    import os\r\n    import shutil\r\n    import tempfile\r\n    import numpy as np\r\n    import pandas as pd\r\n    import argparse\r\n    \r\n    from keras.models import Sequential\r\n    from keras.layers.core import Dense\r\n    from keras.regularizers import l2\r\n    import tensorflow as tf\r\n    import keras\r\n    \r\n    nb_samples = 50\r\n    nb_features = 5\r\n    X_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\r\n    Y_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\r\n    \r\n    def build_keras_model(input_dim):\r\n      hidden_dim = 10\r\n    \r\n      model = Sequential()\r\n      model.add(Dense(input_dim = input_dim,\r\n                      output_dim=hidden_dim,\r\n                      activation='tanh'\r\n                      ))\r\n    \r\n      model.add(Dense(output_dim=1, activation='linear'))\r\n    \r\n      model.compile(loss='mse', optimizer='adam')\r\n      \r\n      return model\r\n    \r\n    \r\n    \r\n    \r\n    ################################################\r\n    # DISTRIBUTE\r\n    ################################################\r\n    \r\n    parser = argparse.ArgumentParser(description='tensorflow')\r\n    parser.add_argument('--job_name', dest='job_name')\r\n    parser.add_argument('--task_index', dest='task_index', default=0)\r\n    args = parser.parse_args()\r\n    \r\n    \r\n    ps_hosts = ['localhost:2222']\r\n    worker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\r\n    job_name = args.job_name\r\n    task_index = int(args.task_index)\r\n    \r\n    # Create a cluster from the parameter server and worker hosts.\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n      \r\n    server = tf.train.Server(cluster,\r\n                             job_name=job_name,\r\n                             task_index=task_index,\r\n                             config=tf.ConfigProto(log_device_placement=True,\r\n                                                   inter_op_parallelism_threads=1,\r\n                                                   intra_op_parallelism_threads=1))\r\n    \r\n    \r\n    if job_name =='ps':\r\n      with tf.device(\"/job:ps/task:0\"):\r\n        queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n      sess = tf.Session(server.target)\r\n      # wait until all workers are done\r\n      for i in range(len(worker_hosts)):\r\n        sess.run(queue.dequeue())\r\n    else:\r\n      with tf.device(tf.train.replica_device_setter(\r\n                                  worker_device=\"/job:worker/task:%d\" % task_index,\r\n                                  cluster=cluster)):\r\n    \r\n        keras.backend.set_learning_phase(1)\r\n        keras.backend.manual_variable_initialization(True)\r\n    \r\n        model = build_keras_model(nb_features)\r\n        preds = model.output\r\n        targets = tf.placeholder(tf.float32, [None, 1])\r\n        total_loss = tf.reduce_mean(\r\n                            keras.objectives.mean_squared_error(targets, preds))\r\n    \r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n        # For stopping management\r\n        epoch = tf.Variable(0, name=\"epoch\", trainable=False)\r\n        inc_epoch_op = tf.assign_add(epoch, 1)\r\n    \r\n        is_chief=(task_index == 0)\r\n    \r\n        opt = tf.train.AdamOptimizer()\r\n        num_workers = len(worker_hosts)\r\n        replicas_to_aggregate = num_workers\r\n        opt = tf.train.SyncReplicasOptimizer(\r\n                                             opt,\r\n                                             replicas_to_aggregate=replicas_to_aggregate,\r\n                                             total_num_replicas=num_workers,\r\n                                             name=\"sync_replicas\")\r\n    \r\n        train_op = opt.minimize(total_loss, global_step=global_step)\r\n        with tf.device(\"/job:ps/task:0\"):\r\n          queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n          enqueue_op = queue.enqueue(1)\r\n     \r\n        train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\r\n        \r\n        print '######################################### ALL CREATED'\r\n        sync_replicas_hook = opt.make_session_run_hook(is_chief)\r\n        with tf.train.MonitoredTrainingSession(\r\n                   master=server.target, is_chief=is_chief, checkpoint_dir=train_dir,\r\n                         hooks=[sync_replicas_hook]) as sess:\r\n          keras.backend.set_session(sess)\r\n          print '#######  SESSION OK ********'\r\n          local_step = 0\r\n          while True:\r\n            train_feed = {model.input: X_train, targets: Y_train}\r\n    \r\n            _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n            loss = sess.run(total_loss, feed_dict = train_feed)\r\n            if is_chief:\r\n              sess.run(inc_epoch_op)\r\n            local_step += 1\r\n            print '## epoch ', epoch.eval(sess)\r\n            if epoch.eval(sess) > 4:\r\n              print '######################  TRYING TO LEAVE'\r\n              break\r\n    \r\n        print '######################  WHILE LOOP LEFT'\r\n        sess.run(enqueue_op)\r\n        print '## ENQUEUE OP DONE'\r\n        shutil.rmtree(train_dir)\r\n    \r\n\r\nand this is the output of the chief worker \r\n\r\n    #######  SESSION OK ********\r\n    ## epoch  1\r\n    ## epoch  2\r\n    ## epoch  3\r\n    ## epoch  4\r\n    ## epoch  5\r\n    ######################  TRYING TO LEAVE\r\n    Exception in thread Thread-2:\r\n    Traceback (most recent call last):\r\n      File \"/usr/lib64/python2.7/threading.py\", line 811, in __bootstrap_inner\r\n        self.run()\r\n      File \"/usr/lib64/python2.7/threading.py\", line 764, in run\r\n        self.__target(*self.__args, **self.__kwargs)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 250, in _run\r\n        coord.request_stop(e)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 211, in request_stop\r\n        six.reraise(*sys.exc_info())\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n        sess.run(enqueue_op)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 767, in run\r\n        run_metadata_ptr)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 965, in _run\r\n        feed_dict_string, options, run_metadata)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n        target_list, options, run_metadata)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    CancelledError: RunManyGraphs\r\n    \r\n    Traceback (most recent call last):\r\n      File \"test.py\", line 148, in <module>\r\n        break\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\r\n        self._close_internal(exception_type)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\r\n        self._sess.close()\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 739, in close\r\n        self._sess.close()\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 827, in close\r\n        self._coord.join()\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 390, in join\r\n        \" \".join(stragglers))\r\n    RuntimeError: Coordinator stopped with threads still running: Thread-2"}