{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296421579", "html_url": "https://github.com/tensorflow/tensorflow/issues/1588#issuecomment-296421579", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1588", "id": 296421579, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjQyMTU3OQ==", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-23T05:51:44Z", "updated_at": "2017-04-27T17:57:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a><br>\nI've read a related <a href=\"https://github.com/tensorflow/tensorflow/issues/342#issuecomment-160354041\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/342/hovercard\">comment</a> of yours and it helped me a lot.</p>\n<p>Is it possible to extract sparse ids and train sparse weights on the fly?</p>\n<p>My application is finetuning sparse embedding: Given a sequence of word ids, firstly use a cookbook to look up sparse indices and weights of each word, then use <code>tf.nn.embedding_lookup_sparse</code> to get their embeddings. Since I want to finetune the sparse weights (indices, i.e.: sparse structure of data, are fixed, of course),  directly feed them to placeholders are not adequate.</p>\n<p>To be more specific:</p>\n<ol>\n<li>How can I extract certain rows (maybe discrete and even have duplicate rows) out of a <code>SparseTensor</code>?</li>\n<li>If the first one is possible, can I perform training on sparse weights? (Words may duplicate in a sentence, so sparse weights may duplicate in a sentence, so the variables need to be shared to make sure gradient descent algorithms work properly. Simply pass pre-calculated weights to a placeholder does not work for this purpose)</li>\n</ol>\n<p>Currently is it possible to do this in TensorFlow? Or I need some work around (e.g. use sparse dense matrix multiplication or something else) to do this?</p>\n<hr>\n<h1>Update</h1>\n<p>I've worked this out. Just check <a href=\"http://stackoverflow.com/questions/43557785/how-can-i-select-a-row-from-a-sparsetensor-in-tensorflow/43615108\" rel=\"nofollow\">stackoverflow</a>.</p>", "body_text": "@ebrevdo\nI've read a related comment of yours and it helped me a lot.\nIs it possible to extract sparse ids and train sparse weights on the fly?\nMy application is finetuning sparse embedding: Given a sequence of word ids, firstly use a cookbook to look up sparse indices and weights of each word, then use tf.nn.embedding_lookup_sparse to get their embeddings. Since I want to finetune the sparse weights (indices, i.e.: sparse structure of data, are fixed, of course),  directly feed them to placeholders are not adequate.\nTo be more specific:\n\nHow can I extract certain rows (maybe discrete and even have duplicate rows) out of a SparseTensor?\nIf the first one is possible, can I perform training on sparse weights? (Words may duplicate in a sentence, so sparse weights may duplicate in a sentence, so the variables need to be shared to make sure gradient descent algorithms work properly. Simply pass pre-calculated weights to a placeholder does not work for this purpose)\n\nCurrently is it possible to do this in TensorFlow? Or I need some work around (e.g. use sparse dense matrix multiplication or something else) to do this?\n\nUpdate\nI've worked this out. Just check stackoverflow.", "body": "@ebrevdo \r\nI've read a related [comment](https://github.com/tensorflow/tensorflow/issues/342#issuecomment-160354041) of yours and it helped me a lot.\r\n\r\nIs it possible to extract sparse ids and train sparse weights on the fly?\r\n\r\nMy application is finetuning sparse embedding: Given a sequence of word ids, firstly use a cookbook to look up sparse indices and weights of each word, then use `tf.nn.embedding_lookup_sparse` to get their embeddings. Since I want to finetune the sparse weights (indices, i.e.: sparse structure of data, are fixed, of course),  directly feed them to placeholders are not adequate.\r\n\r\nTo be more specific:\r\n1. How can I extract certain rows (maybe discrete and even have duplicate rows) out of a `SparseTensor`?\r\n2. If the first one is possible, can I perform training on sparse weights? (Words may duplicate in a sentence, so sparse weights may duplicate in a sentence, so the variables need to be shared to make sure gradient descent algorithms work properly. Simply pass pre-calculated weights to a placeholder does not work for this purpose)\r\n\r\nCurrently is it possible to do this in TensorFlow? Or I need some work around (e.g. use sparse dense matrix multiplication or something else) to do this?\r\n\r\n---------------------------------\r\n\r\nUpdate\r\n============\r\n\r\nI've worked this out. Just check [stackoverflow](http://stackoverflow.com/questions/43557785/how-can-i-select-a-row-from-a-sparsetensor-in-tensorflow/43615108)."}