{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21904", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21904/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21904/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21904/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21904", "id": 354407496, "node_id": "MDU6SXNzdWUzNTQ0MDc0OTY=", "number": 21904, "title": "Simple memory placement optimizations not performed when targeting TPUs", "user": {"login": "Keno", "id": 1291671, "node_id": "MDQ6VXNlcjEyOTE2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Keno", "html_url": "https://github.com/Keno", "followers_url": "https://api.github.com/users/Keno/followers", "following_url": "https://api.github.com/users/Keno/following{/other_user}", "gists_url": "https://api.github.com/users/Keno/gists{/gist_id}", "starred_url": "https://api.github.com/users/Keno/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Keno/subscriptions", "organizations_url": "https://api.github.com/users/Keno/orgs", "repos_url": "https://api.github.com/users/Keno/repos", "events_url": "https://api.github.com/users/Keno/events{/privacy}", "received_events_url": "https://api.github.com/users/Keno/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jpienaar", "id": 706766, "node_id": "MDQ6VXNlcjcwNjc2Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/706766?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpienaar", "html_url": "https://github.com/jpienaar", "followers_url": "https://api.github.com/users/jpienaar/followers", "following_url": "https://api.github.com/users/jpienaar/following{/other_user}", "gists_url": "https://api.github.com/users/jpienaar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpienaar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpienaar/subscriptions", "organizations_url": "https://api.github.com/users/jpienaar/orgs", "repos_url": "https://api.github.com/users/jpienaar/repos", "events_url": "https://api.github.com/users/jpienaar/events{/privacy}", "received_events_url": "https://api.github.com/users/jpienaar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jpienaar", "id": 706766, "node_id": "MDQ6VXNlcjcwNjc2Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/706766?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpienaar", "html_url": "https://github.com/jpienaar", "followers_url": "https://api.github.com/users/jpienaar/followers", "following_url": "https://api.github.com/users/jpienaar/following{/other_user}", "gists_url": "https://api.github.com/users/jpienaar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpienaar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpienaar/subscriptions", "organizations_url": "https://api.github.com/users/jpienaar/orgs", "repos_url": "https://api.github.com/users/jpienaar/repos", "events_url": "https://api.github.com/users/jpienaar/events{/privacy}", "received_events_url": "https://api.github.com/users/jpienaar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-08-27T17:32:32Z", "updated_at": "2018-09-28T01:00:39Z", "closed_at": "2018-09-28T01:00:39Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 17.10</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: r1.9</li>\n<li><strong>Python version</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 7.2.0</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm playing with compiling Julia code to TF graphs for TPU offload. At the moment, I have the two functions:</p>\n<pre><code>f(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X)\ng(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X) * (X + c_9 * X)\n</code></pre>\n<p>(Note the <code>*</code> is matrix-multiply between two matrices). Now, when I run (the TF graph resulting from the compilation of) these two functions against a cloud TPU (Tensorflow version 1.9), with <code>X</code>,<code>Y</code> both 10,000 x 10,000 Float32 matrices, <code>c_i</code> Float32 constants, <code>f</code> runs fine, but <code>g</code> delivers the following out of memory error:</p>\n<pre><code>Tensorflow error: Status: Failed to allocate request for 385.74MiB (404480000B) on device ordinal 0\n\nTotal hbm usage &gt;= 7.88G:\n    reserved                 528.00M\n    persistent allocations     7.37G (7.4% fragmentation)\n    program                    33.0K\n\nPersistent allocations include some or all of:\n    arguments                771.48M (98.9% utilization)\n    output                   385.75M (98.9% utilization) [may share some memory with arguments]\n</code></pre>\n<p>This seems to indicate to me that it's trying to reserve space for all the intermediate matrix results in high bandwidth memory. I would have expected (and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7657273\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/learyg\">@learyg</a> mentioned that it should) that XLA would have optimized the memory placement in order to avoid this problem.</p>\n<p>Looking at the profile output for the <code>f</code> function, it seems to be doing all the broadcasts in order, followed by grouping all the matmuls (which would indeed blow the memory budget):<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1291671/44673466-a087b800-a9f9-11e8-9121-86311b5fe40f.png\"><img src=\"https://user-images.githubusercontent.com/1291671/44673466-a087b800-a9f9-11e8-9121-86311b5fe40f.png\" alt=\"screen shot 2018-08-27 at 1 02 37 pm\" style=\"max-width:100%;\"></a></p>\n<p>I also note that gap between the broadcasts and the matmuls, during which time it seems to be running XLA again. To me that indicates that XLA is not getting the full TF graph on the server side and thus cannot optimize the memory placement properly. I'm hoping that there's just some missing setting in the TF graph def that would allow it to do its job.</p>\n<h3>Source code / logs</h3>\n<p>I was able to reproduce this in raw Tensorflow.jl (which should have a more or less 1:1 API correspondence to python, though of course there may be a bug in the bindings also - cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=987837\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/malmaud\">@malmaud</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5127634\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oxinabox\">@oxinabox</a>), using code like this:</p>\n<pre><code>using TensorFlow\nusing TensorFlow: Device\n\nsess = Session(Graph(); target=\"grpc://localhost:8470\")\n\nwith_device(TensorFlow.NamedDevice(\"/job:tpu_worker/replica:0/task:0/device:TPU:0\")) do\n    global X,Y,Z\n    X = placeholder(Float32, shape=[10000, 10000])\n    Z = Y = placeholder(Float32, shape=[10000, 10000])\n    for i = 1:8 # Fails for 1:9\n        Z *= X + rand(Float32)*X\n    end\nend\n\nx, z = rand(Float32, 10000, 10000), rand(Float32, 10000, 10000)\nrun(sess, Z, Dict(X=&gt;x, Y=&gt;z))\n</code></pre>\n<p>A serialized GraphDef of the graph for <code>f</code> is at <a href=\"https://gist.github.com/Keno/9321e0f1f278b04fbf3b878e551b3ecb\">https://gist.github.com/Keno/9321e0f1f278b04fbf3b878e551b3ecb</a></p>\n<p>Is there something missing from this GraphDef that would enable XLA to perform the appropriate memory placement optimization on the device?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): r1.9\nPython version: N/A\nBazel version (if compiling from source): 0.11.1\nGCC/Compiler version (if compiling from source): 7.2.0\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nI'm playing with compiling Julia code to TF graphs for TPU offload. At the moment, I have the two functions:\nf(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X)\ng(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X) * (X + c_9 * X)\n\n(Note the * is matrix-multiply between two matrices). Now, when I run (the TF graph resulting from the compilation of) these two functions against a cloud TPU (Tensorflow version 1.9), with X,Y both 10,000 x 10,000 Float32 matrices, c_i Float32 constants, f runs fine, but g delivers the following out of memory error:\nTensorflow error: Status: Failed to allocate request for 385.74MiB (404480000B) on device ordinal 0\n\nTotal hbm usage >= 7.88G:\n    reserved                 528.00M\n    persistent allocations     7.37G (7.4% fragmentation)\n    program                    33.0K\n\nPersistent allocations include some or all of:\n    arguments                771.48M (98.9% utilization)\n    output                   385.75M (98.9% utilization) [may share some memory with arguments]\n\nThis seems to indicate to me that it's trying to reserve space for all the intermediate matrix results in high bandwidth memory. I would have expected (and @learyg mentioned that it should) that XLA would have optimized the memory placement in order to avoid this problem.\nLooking at the profile output for the f function, it seems to be doing all the broadcasts in order, followed by grouping all the matmuls (which would indeed blow the memory budget):\n\nI also note that gap between the broadcasts and the matmuls, during which time it seems to be running XLA again. To me that indicates that XLA is not getting the full TF graph on the server side and thus cannot optimize the memory placement properly. I'm hoping that there's just some missing setting in the TF graph def that would allow it to do its job.\nSource code / logs\nI was able to reproduce this in raw Tensorflow.jl (which should have a more or less 1:1 API correspondence to python, though of course there may be a bug in the bindings also - cc @malmaud @oxinabox), using code like this:\nusing TensorFlow\nusing TensorFlow: Device\n\nsess = Session(Graph(); target=\"grpc://localhost:8470\")\n\nwith_device(TensorFlow.NamedDevice(\"/job:tpu_worker/replica:0/task:0/device:TPU:0\")) do\n    global X,Y,Z\n    X = placeholder(Float32, shape=[10000, 10000])\n    Z = Y = placeholder(Float32, shape=[10000, 10000])\n    for i = 1:8 # Fails for 1:9\n        Z *= X + rand(Float32)*X\n    end\nend\n\nx, z = rand(Float32, 10000, 10000), rand(Float32, 10000, 10000)\nrun(sess, Z, Dict(X=>x, Y=>z))\n\nA serialized GraphDef of the graph for f is at https://gist.github.com/Keno/9321e0f1f278b04fbf3b878e551b3ecb\nIs there something missing from this GraphDef that would enable XLA to perform the appropriate memory placement optimization on the device?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.9\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI'm playing with compiling Julia code to TF graphs for TPU offload. At the moment, I have the two functions:\r\n```\r\nf(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X)\r\ng(X, Y) = Y * (X + c_1*X) * (X + c_2*X) * (X + c_3*X) * (X + c_4*X) * (X + c_5*X) * (X + c_6*X) * (X + c_7*X) * (X + c_8*X) * (X + c_9 * X)\r\n```\r\n(Note the `*` is matrix-multiply between two matrices). Now, when I run (the TF graph resulting from the compilation of) these two functions against a cloud TPU (Tensorflow version 1.9), with `X`,`Y` both 10,000 x 10,000 Float32 matrices, `c_i` Float32 constants, `f` runs fine, but `g` delivers the following out of memory error:\r\n\r\n```\r\nTensorflow error: Status: Failed to allocate request for 385.74MiB (404480000B) on device ordinal 0\r\n\r\nTotal hbm usage >= 7.88G:\r\n    reserved                 528.00M\r\n    persistent allocations     7.37G (7.4% fragmentation)\r\n    program                    33.0K\r\n\r\nPersistent allocations include some or all of:\r\n    arguments                771.48M (98.9% utilization)\r\n    output                   385.75M (98.9% utilization) [may share some memory with arguments]\r\n```\r\n\r\nThis seems to indicate to me that it's trying to reserve space for all the intermediate matrix results in high bandwidth memory. I would have expected (and @learyg mentioned that it should) that XLA would have optimized the memory placement in order to avoid this problem. \r\n\r\nLooking at the profile output for the `f` function, it seems to be doing all the broadcasts in order, followed by grouping all the matmuls (which would indeed blow the memory budget):\r\n![screen shot 2018-08-27 at 1 02 37 pm](https://user-images.githubusercontent.com/1291671/44673466-a087b800-a9f9-11e8-9121-86311b5fe40f.png)\r\n\r\nI also note that gap between the broadcasts and the matmuls, during which time it seems to be running XLA again. To me that indicates that XLA is not getting the full TF graph on the server side and thus cannot optimize the memory placement properly. I'm hoping that there's just some missing setting in the TF graph def that would allow it to do its job.\r\n\r\n### Source code / logs\r\nI was able to reproduce this in raw Tensorflow.jl (which should have a more or less 1:1 API correspondence to python, though of course there may be a bug in the bindings also - cc @malmaud @oxinabox), using code like this:\r\n```\r\nusing TensorFlow\r\nusing TensorFlow: Device\r\n\r\nsess = Session(Graph(); target=\"grpc://localhost:8470\")\r\n\r\nwith_device(TensorFlow.NamedDevice(\"/job:tpu_worker/replica:0/task:0/device:TPU:0\")) do\r\n    global X,Y,Z\r\n    X = placeholder(Float32, shape=[10000, 10000])\r\n    Z = Y = placeholder(Float32, shape=[10000, 10000])\r\n    for i = 1:8 # Fails for 1:9\r\n        Z *= X + rand(Float32)*X\r\n    end\r\nend\r\n\r\nx, z = rand(Float32, 10000, 10000), rand(Float32, 10000, 10000)\r\nrun(sess, Z, Dict(X=>x, Y=>z))\r\n```\r\nA serialized GraphDef of the graph for `f` is at https://gist.github.com/Keno/9321e0f1f278b04fbf3b878e551b3ecb\r\n\r\nIs there something missing from this GraphDef that would enable XLA to perform the appropriate memory placement optimization on the device?"}