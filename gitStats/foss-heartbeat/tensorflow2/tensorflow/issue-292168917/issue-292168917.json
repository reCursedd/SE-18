{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16507", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16507/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16507/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16507/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16507", "id": 292168917, "node_id": "MDU6SXNzdWUyOTIxNjg5MTc=", "number": 16507, "title": "ResourceExhaustedError, when running UNET", "user": {"login": "hiroshiperera", "id": 6210895, "node_id": "MDQ6VXNlcjYyMTA4OTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6210895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hiroshiperera", "html_url": "https://github.com/hiroshiperera", "followers_url": "https://api.github.com/users/hiroshiperera/followers", "following_url": "https://api.github.com/users/hiroshiperera/following{/other_user}", "gists_url": "https://api.github.com/users/hiroshiperera/gists{/gist_id}", "starred_url": "https://api.github.com/users/hiroshiperera/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hiroshiperera/subscriptions", "organizations_url": "https://api.github.com/users/hiroshiperera/orgs", "repos_url": "https://api.github.com/users/hiroshiperera/repos", "events_url": "https://api.github.com/users/hiroshiperera/events{/privacy}", "received_events_url": "https://api.github.com/users/hiroshiperera/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-28T01:43:47Z", "updated_at": "2018-01-29T19:47:54Z", "closed_at": "2018-01-29T19:47:54Z", "author_association": "NONE", "body_html": "<p>My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.</p>\n<pre><code>from __future__ import print_function\n\nimport numpy as np\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.layers import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as K\n\n\nK.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n\nimg_rows = 512\nimg_cols = 512\n\nsmooth = 1.\n\n\ndef dice_coef(y_true, y_pred):\n\ty_true_f = K.flatten(y_true)\n\ty_pred_f = K.flatten(y_pred)\n\tintersection = K.sum(y_true_f * y_pred_f)\n\treturn (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_coef_np(y_true,y_pred):\n\ty_true_f = y_true.flatten()\n\ty_pred_f = y_pred.flatten()\n\tintersection = np.sum(y_true_f * y_pred_f)\n\treturn (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n\treturn -dice_coef(y_true, y_pred)\n\n\ndef get_unet():\n\tinputs = Input((1,img_rows, img_cols))\n\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n\tpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n\tpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n\tpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n\tpool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n\t#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n\tup6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)\n\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n\t#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n\tup7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\n\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n\t#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n\tup8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)\n\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n\t#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n\tup9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)\n\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n\tconv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n\tmodel = Model(inputs=inputs, outputs=conv10)\n\n\tmodel.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\n\n\treturn model\n\n\ndef train_and_predict(use_existing):\n\tprint('-'*30)\n\tprint('Loading and preprocessing train data...')\n\tprint('-'*30)\n\timgs_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainImages.npy\").astype(np.float32)\n\timgs_mask_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainMasks.npy\").astype(np.float32)\n\n\timgs_test = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testImages.npy\").astype(np.float32)\n\timgs_mask_test_true = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testMasks.npy\").astype(np.float32)\n\t\n\tmean = np.mean(imgs_train)  # mean for data centering\n\tstd = np.std(imgs_train)  # std for data normalization\n\n\timgs_train -= mean  # images should already be standardized, but just in case\n\timgs_train /= std\n\n\tprint('-'*30)\n\tprint('Creating and compiling model...')\n\tprint('-'*30)\n\tmodel = get_unet()\n\t# Saving weights to unet.hdf5 at checkpoints\n\tmodel_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\n\t#\n\t# Should we load existing weights? \n\t# Set argument for call to train_and_predict to true at end of script\n\tif use_existing:\n\t\tmodel.load_weights('./unet.hdf5')\n\t\t\n\t# \n\t# The final results for this tutorial were produced using a multi-GPU\n\t# machine using TitanX's.\n\t# For a home GPU computation benchmark, on my home set up with a GTX970 \n\t# I was able to run 20 epochs with a training set size of 320 and \n\t# batch size of 2 in about an hour. I started getting reseasonable masks \n\t# after about 3 hours of training. \n\t#\n\tprint('-'*30)\n\tprint('Fitting model...')\n\tprint('-'*30)\n\tmodel.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,\n\t\t\t  callbacks=[model_checkpoint])\n\n\t# loading best weights from training session\n\tprint('-'*30)\n\tprint('Loading saved weights...')\n\tprint('-'*30)\n\tmodel.load_weights('./unet.hdf5')\n\n\tprint('-'*30)\n\tprint('Predicting masks on test data...')\n\tprint('-'*30)\n\tnum_test = len(imgs_test)\n\timgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)\n\tfor i in range(num_test):\n\t\timgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]\n\tnp.save('masksTestPredicted.npy', imgs_mask_test)\n\tmean = 0.0\n\tfor i in range(num_test):\n\t\tmean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])\n\tmean/=num_test\n\tprint(\"Mean Dice Coeff : \",mean)\n\nif __name__ == '__main__':\n\ttrain_and_predict(False)\n</code></pre>\n<p>But when running it using GPU I'm getting the following error.</p>\n<pre><code>Warning (from warnings module):\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\h5py\\__init__.py\", line 36\n\tfrom ._conv import register_converters as _register_converters\nFutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\nUsing TensorFlow backend.\n------------------------------\nLoading and preprocessing train data...\n------------------------------\n------------------------------\nCreating and compiling model...\n------------------------------\n------------------------------\nFitting model...\n------------------------------\nEpoch 1/10\nTraceback (most recent call last):\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\n\treturn fn(*args)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\n\tstatus, run_metadata)\n  File \"C:\\Research\\Python_installation\\lib\\contextlib.py\", line 66, in __exit__\n\tnext(self.gen)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n\tpywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\n\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\n\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in &lt;module&gt;\n\ttrain_and_predict(False)\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 127, in train_and_predict\n\tcallbacks=[model_checkpoint])\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1657, in fit\n\tvalidation_steps=validation_steps)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1213, in _fit_loop\n\touts = f(ins_batch)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2357, in __call__\n\t**self.session_kwargs)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\n\trun_metadata_ptr)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\n\tfeed_dict_tensor, options, run_metadata)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\n\toptions, run_metadata)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\n\traise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\n\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\n\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2d_1/convolution', defined at:\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 124, in main\n\tret = method(*args, **kwargs)\n  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 351, in runcode\n\texec(code, self.locals)\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in &lt;module&gt;\n\ttrain_and_predict(False)\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 106, in train_and_predict\n\tmodel = get_unet()\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 39, in get_unet\n\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n\toutput = self.call(inputs, **kwargs)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 164, in call\n\tdilation_rate=self.dilation_rate)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3195, in conv2d\n\tdata_format=tf_data_format)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 672, in convolution\n\top=op)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 338, in with_space_to_batch\n\treturn op(input, num_spatial_dims, padding)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 664, in op\n\tname=name)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 131, in _non_atrous_convolution\n\tname=name)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 397, in conv2d\n\tdata_format=data_format, name=name)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n\top_def=op_def)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n\toriginal_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n\tself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]\n\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\n\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<p>Can someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch</p>", "body_text": "My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.\nfrom __future__ import print_function\n\nimport numpy as np\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.layers import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as K\n\n\nK.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n\nimg_rows = 512\nimg_cols = 512\n\nsmooth = 1.\n\n\ndef dice_coef(y_true, y_pred):\n\ty_true_f = K.flatten(y_true)\n\ty_pred_f = K.flatten(y_pred)\n\tintersection = K.sum(y_true_f * y_pred_f)\n\treturn (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_coef_np(y_true,y_pred):\n\ty_true_f = y_true.flatten()\n\ty_pred_f = y_pred.flatten()\n\tintersection = np.sum(y_true_f * y_pred_f)\n\treturn (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n\treturn -dice_coef(y_true, y_pred)\n\n\ndef get_unet():\n\tinputs = Input((1,img_rows, img_cols))\n\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n\tpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n\tpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n\tpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n\tpool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n\t#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n\tup6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)\n\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n\t#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n\tup7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\n\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n\t#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n\tup8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)\n\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n\t#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n\tup9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)\n\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n\tconv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n\tmodel = Model(inputs=inputs, outputs=conv10)\n\n\tmodel.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\n\n\treturn model\n\n\ndef train_and_predict(use_existing):\n\tprint('-'*30)\n\tprint('Loading and preprocessing train data...')\n\tprint('-'*30)\n\timgs_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainImages.npy\").astype(np.float32)\n\timgs_mask_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainMasks.npy\").astype(np.float32)\n\n\timgs_test = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testImages.npy\").astype(np.float32)\n\timgs_mask_test_true = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testMasks.npy\").astype(np.float32)\n\t\n\tmean = np.mean(imgs_train)  # mean for data centering\n\tstd = np.std(imgs_train)  # std for data normalization\n\n\timgs_train -= mean  # images should already be standardized, but just in case\n\timgs_train /= std\n\n\tprint('-'*30)\n\tprint('Creating and compiling model...')\n\tprint('-'*30)\n\tmodel = get_unet()\n\t# Saving weights to unet.hdf5 at checkpoints\n\tmodel_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\n\t#\n\t# Should we load existing weights? \n\t# Set argument for call to train_and_predict to true at end of script\n\tif use_existing:\n\t\tmodel.load_weights('./unet.hdf5')\n\t\t\n\t# \n\t# The final results for this tutorial were produced using a multi-GPU\n\t# machine using TitanX's.\n\t# For a home GPU computation benchmark, on my home set up with a GTX970 \n\t# I was able to run 20 epochs with a training set size of 320 and \n\t# batch size of 2 in about an hour. I started getting reseasonable masks \n\t# after about 3 hours of training. \n\t#\n\tprint('-'*30)\n\tprint('Fitting model...')\n\tprint('-'*30)\n\tmodel.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,\n\t\t\t  callbacks=[model_checkpoint])\n\n\t# loading best weights from training session\n\tprint('-'*30)\n\tprint('Loading saved weights...')\n\tprint('-'*30)\n\tmodel.load_weights('./unet.hdf5')\n\n\tprint('-'*30)\n\tprint('Predicting masks on test data...')\n\tprint('-'*30)\n\tnum_test = len(imgs_test)\n\timgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)\n\tfor i in range(num_test):\n\t\timgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]\n\tnp.save('masksTestPredicted.npy', imgs_mask_test)\n\tmean = 0.0\n\tfor i in range(num_test):\n\t\tmean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])\n\tmean/=num_test\n\tprint(\"Mean Dice Coeff : \",mean)\n\nif __name__ == '__main__':\n\ttrain_and_predict(False)\n\nBut when running it using GPU I'm getting the following error.\nWarning (from warnings module):\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\h5py\\__init__.py\", line 36\n\tfrom ._conv import register_converters as _register_converters\nFutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\nUsing TensorFlow backend.\n------------------------------\nLoading and preprocessing train data...\n------------------------------\n------------------------------\nCreating and compiling model...\n------------------------------\n------------------------------\nFitting model...\n------------------------------\nEpoch 1/10\nTraceback (most recent call last):\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\n\treturn fn(*args)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\n\tstatus, run_metadata)\n  File \"C:\\Research\\Python_installation\\lib\\contextlib.py\", line 66, in __exit__\n\tnext(self.gen)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n\tpywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\n\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\n\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in <module>\n\ttrain_and_predict(False)\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 127, in train_and_predict\n\tcallbacks=[model_checkpoint])\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1657, in fit\n\tvalidation_steps=validation_steps)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1213, in _fit_loop\n\touts = f(ins_batch)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2357, in __call__\n\t**self.session_kwargs)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\n\trun_metadata_ptr)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\n\tfeed_dict_tensor, options, run_metadata)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\n\toptions, run_metadata)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\n\traise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\n\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\n\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2d_1/convolution', defined at:\n  File \"<string>\", line 1, in <module>\n  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 124, in main\n\tret = method(*args, **kwargs)\n  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 351, in runcode\n\texec(code, self.locals)\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in <module>\n\ttrain_and_predict(False)\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 106, in train_and_predict\n\tmodel = get_unet()\n  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 39, in get_unet\n\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n\toutput = self.call(inputs, **kwargs)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 164, in call\n\tdilation_rate=self.dilation_rate)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3195, in conv2d\n\tdata_format=tf_data_format)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 672, in convolution\n\top=op)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 338, in with_space_to_batch\n\treturn op(input, num_spatial_dims, padding)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 664, in op\n\tname=name)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 131, in _non_atrous_convolution\n\tname=name)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 397, in conv2d\n\tdata_format=data_format, name=name)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n\top_def=op_def)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n\toriginal_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n\tself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]\n\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\n\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCan someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch", "body": "My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.\r\n\r\n\tfrom __future__ import print_function\r\n\r\n\timport numpy as np\r\n\tfrom keras.models import Model\r\n\tfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\r\n\tfrom keras.layers import concatenate\r\n\tfrom keras.optimizers import Adam\r\n\tfrom keras.optimizers import SGD\r\n\tfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\n\tfrom keras import backend as K\r\n\r\n\r\n\tK.set_image_dim_ordering('th')  # Theano dimension ordering in this code\r\n\r\n\timg_rows = 512\r\n\timg_cols = 512\r\n\r\n\tsmooth = 1.\r\n\r\n\r\n\tdef dice_coef(y_true, y_pred):\r\n\t\ty_true_f = K.flatten(y_true)\r\n\t\ty_pred_f = K.flatten(y_pred)\r\n\t\tintersection = K.sum(y_true_f * y_pred_f)\r\n\t\treturn (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\r\n\r\n\tdef dice_coef_np(y_true,y_pred):\r\n\t\ty_true_f = y_true.flatten()\r\n\t\ty_pred_f = y_pred.flatten()\r\n\t\tintersection = np.sum(y_true_f * y_pred_f)\r\n\t\treturn (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\r\n\r\n\tdef dice_coef_loss(y_true, y_pred):\r\n\t\treturn -dice_coef(y_true, y_pred)\r\n\r\n\r\n\tdef get_unet():\r\n\t\tinputs = Input((1,img_rows, img_cols))\r\n\t\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\r\n\t\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\r\n\t\tpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n\r\n\t\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\r\n\t\tconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\r\n\t\tpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n\r\n\t\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\r\n\t\tconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\r\n\t\tpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n\r\n\t\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\r\n\t\tconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\r\n\t\tpool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\r\n\r\n\t\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\r\n\t\tconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\r\n\r\n\t\t#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\r\n\t\tup6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)\r\n\t\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\r\n\t\tconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\r\n\r\n\t\t#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\r\n\t\tup7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)\r\n\t\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\r\n\t\tconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\r\n\r\n\t\t#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\r\n\t\tup8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)\r\n\t\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\r\n\t\tconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\r\n\r\n\t\t#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\r\n\t\tup9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)\r\n\t\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\r\n\t\tconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\r\n\r\n\t\tconv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\r\n\r\n\t\tmodel = Model(inputs=inputs, outputs=conv10)\r\n\r\n\t\tmodel.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\r\n\r\n\t\treturn model\r\n\r\n\r\n\tdef train_and_predict(use_existing):\r\n\t\tprint('-'*30)\r\n\t\tprint('Loading and preprocessing train data...')\r\n\t\tprint('-'*30)\r\n\t\timgs_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainImages.npy\").astype(np.float32)\r\n\t\timgs_mask_train = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"trainMasks.npy\").astype(np.float32)\r\n\r\n\t\timgs_test = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testImages.npy\").astype(np.float32)\r\n\t\timgs_mask_test_true = np.load(\"C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/\"+\"testMasks.npy\").astype(np.float32)\r\n\t\t\r\n\t\tmean = np.mean(imgs_train)  # mean for data centering\r\n\t\tstd = np.std(imgs_train)  # std for data normalization\r\n\r\n\t\timgs_train -= mean  # images should already be standardized, but just in case\r\n\t\timgs_train /= std\r\n\r\n\t\tprint('-'*30)\r\n\t\tprint('Creating and compiling model...')\r\n\t\tprint('-'*30)\r\n\t\tmodel = get_unet()\r\n\t\t# Saving weights to unet.hdf5 at checkpoints\r\n\t\tmodel_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\r\n\t\t#\r\n\t\t# Should we load existing weights? \r\n\t\t# Set argument for call to train_and_predict to true at end of script\r\n\t\tif use_existing:\r\n\t\t\tmodel.load_weights('./unet.hdf5')\r\n\t\t\t\r\n\t\t# \r\n\t\t# The final results for this tutorial were produced using a multi-GPU\r\n\t\t# machine using TitanX's.\r\n\t\t# For a home GPU computation benchmark, on my home set up with a GTX970 \r\n\t\t# I was able to run 20 epochs with a training set size of 320 and \r\n\t\t# batch size of 2 in about an hour. I started getting reseasonable masks \r\n\t\t# after about 3 hours of training. \r\n\t\t#\r\n\t\tprint('-'*30)\r\n\t\tprint('Fitting model...')\r\n\t\tprint('-'*30)\r\n\t\tmodel.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,\r\n\t\t\t\t  callbacks=[model_checkpoint])\r\n\r\n\t\t# loading best weights from training session\r\n\t\tprint('-'*30)\r\n\t\tprint('Loading saved weights...')\r\n\t\tprint('-'*30)\r\n\t\tmodel.load_weights('./unet.hdf5')\r\n\r\n\t\tprint('-'*30)\r\n\t\tprint('Predicting masks on test data...')\r\n\t\tprint('-'*30)\r\n\t\tnum_test = len(imgs_test)\r\n\t\timgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)\r\n\t\tfor i in range(num_test):\r\n\t\t\timgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]\r\n\t\tnp.save('masksTestPredicted.npy', imgs_mask_test)\r\n\t\tmean = 0.0\r\n\t\tfor i in range(num_test):\r\n\t\t\tmean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])\r\n\t\tmean/=num_test\r\n\t\tprint(\"Mean Dice Coeff : \",mean)\r\n\r\n\tif __name__ == '__main__':\r\n\t\ttrain_and_predict(False)\r\n\t\t\r\nBut when running it using GPU I'm getting the following error.\r\n\r\n\tWarning (from warnings module):\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\h5py\\__init__.py\", line 36\r\n\t\tfrom ._conv import register_converters as _register_converters\r\n\tFutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n\tUsing TensorFlow backend.\r\n\t------------------------------\r\n\tLoading and preprocessing train data...\r\n\t------------------------------\r\n\t------------------------------\r\n\tCreating and compiling model...\r\n\t------------------------------\r\n\t------------------------------\r\n\tFitting model...\r\n\t------------------------------\r\n\tEpoch 1/10\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n\t\treturn fn(*args)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\r\n\t\tstatus, run_metadata)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\contextlib.py\", line 66, in __exit__\r\n\t\tnext(self.gen)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n\t\tpywrap_tensorflow.TF_GetCode(status))\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\r\n\t\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\r\n\t\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\tDuring handling of the above exception, another exception occurred:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in <module>\r\n\t\ttrain_and_predict(False)\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 127, in train_and_predict\r\n\t\tcallbacks=[model_checkpoint])\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1657, in fit\r\n\t\tvalidation_steps=validation_steps)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\training.py\", line 1213, in _fit_loop\r\n\t\touts = f(ins_batch)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2357, in __call__\r\n\t\t**self.session_kwargs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n\t\trun_metadata_ptr)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n\t\tfeed_dict_tensor, options, run_metadata)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n\t\toptions, run_metadata)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n\t\traise type(e)(node_def, op, message)\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]\r\n\t\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\r\n\t\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\tCaused by op 'conv2d_1/convolution', defined at:\r\n\t  File \"<string>\", line 1, in <module>\r\n\t  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 124, in main\r\n\t\tret = method(*args, **kwargs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\idlelib\\run.py\", line 351, in runcode\r\n\t\texec(code, self.locals)\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 150, in <module>\r\n\t\ttrain_and_predict(False)\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 106, in train_and_predict\r\n\t\tmodel = get_unet()\r\n\t  File \"C:\\Users\\hirplk\\Desktop\\unet\\DSB3Tutorial-master\\tutorial_code\\LUNA_train_unet.py\", line 39, in get_unet\r\n\t\tconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\r\n\t\toutput = self.call(inputs, **kwargs)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 164, in call\r\n\t\tdilation_rate=self.dilation_rate)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3195, in conv2d\r\n\t\tdata_format=tf_data_format)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 672, in convolution\r\n\t\top=op)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 338, in with_space_to_batch\r\n\t\treturn op(input, num_spatial_dims, padding)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 664, in op\r\n\t\tname=name)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 131, in _non_atrous_convolution\r\n\t\tname=name)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 397, in conv2d\r\n\t\tdata_format=data_format, name=name)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n\t\top_def=op_def)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n\t\toriginal_op=self._default_original_op, op_def=op_def)\r\n\t  File \"C:\\Research\\Python_installation\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n\t\tself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\tResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]\r\n\t\t [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]\r\n\t\t [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3022_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\r\nCan someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch"}