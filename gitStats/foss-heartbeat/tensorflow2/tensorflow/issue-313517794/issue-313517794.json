{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18437", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18437/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18437/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18437/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18437", "id": 313517794, "node_id": "MDU6SXNzdWUzMTM1MTc3OTQ=", "number": 18437, "title": "Converting TensorFlow frozen and inference model to lite fails with \"Check failed: array->has_shape\"", "user": {"login": "sehgal-abhishek", "id": 26176064, "node_id": "MDQ6VXNlcjI2MTc2MDY0", "avatar_url": "https://avatars1.githubusercontent.com/u/26176064?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sehgal-abhishek", "html_url": "https://github.com/sehgal-abhishek", "followers_url": "https://api.github.com/users/sehgal-abhishek/followers", "following_url": "https://api.github.com/users/sehgal-abhishek/following{/other_user}", "gists_url": "https://api.github.com/users/sehgal-abhishek/gists{/gist_id}", "starred_url": "https://api.github.com/users/sehgal-abhishek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sehgal-abhishek/subscriptions", "organizations_url": "https://api.github.com/users/sehgal-abhishek/orgs", "repos_url": "https://api.github.com/users/sehgal-abhishek/repos", "events_url": "https://api.github.com/users/sehgal-abhishek/events{/privacy}", "received_events_url": "https://api.github.com/users/sehgal-abhishek/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-04-11T23:06:55Z", "updated_at": "2018-08-02T22:23:15Z", "closed_at": "2018-08-02T22:23:15Z", "author_association": "NONE", "body_html": "<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</strong><br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nMac OS X High Sierra</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nSource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n'v1.7.0-rc1-1018-g7a0def60d4' 1.7.0-rc1</li>\n<li><strong>Python version</strong>:<br>\n3.5.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.10.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nApple LLVM version 9.1.0 (clang-902.0.39.1)</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nNA</li>\n<li><strong>GPU model and memory</strong>:<br>\nNA</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nbazel-bin/tensorflow/contrib/lite/toco/toco <br>\n--input_file=frozen.pb <br>\n--input_format=TENSORFLOW_GRAPHDEF <br>\n--output_format=TFLITE <br>\n--output_file=frozen.lite <br>\n--input_array=Inputs/Input <br>\n--output_array=Outputs/Prediction <br>\n--input_shape=1,28,28,1 <br>\n--inference_type=FLOAT<br>\n-<strong>Error Message</strong><br>\nF tensorflow/contrib/lite/toco/tooling_util.cc:831] Check failed: array-&gt;has_shape()</li>\n</ul>\n<p>I have created a TensorFlow model which classifies MNIST image data. The model is then frozen and optimized for inference. These models I can benchmark as well. But when converting to TensorFlow lite, the toco command fails with the above mentioned error message.</p>\n<p>I have summarized the graph as well for both frozen and inference model.<br>\nThe frozen model has the same size of (1,28,28,1) but the inference has a size of (None). I am using a placeholder when creating the model for the Input.</p>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMac OS X High Sierra\nTensorFlow installed from (source or binary):\nSource\nTensorFlow version (use command below):\n'v1.7.0-rc1-1018-g7a0def60d4' 1.7.0-rc1\nPython version:\n3.5.5\nBazel version (if compiling from source):\n0.10.1\nGCC/Compiler version (if compiling from source):\nApple LLVM version 9.1.0 (clang-902.0.39.1)\nCUDA/cuDNN version:\nNA\nGPU model and memory:\nNA\nExact command to reproduce:\nbazel-bin/tensorflow/contrib/lite/toco/toco \n--input_file=frozen.pb \n--input_format=TENSORFLOW_GRAPHDEF \n--output_format=TFLITE \n--output_file=frozen.lite \n--input_array=Inputs/Input \n--output_array=Outputs/Prediction \n--input_shape=1,28,28,1 \n--inference_type=FLOAT\n-Error Message\nF tensorflow/contrib/lite/toco/tooling_util.cc:831] Check failed: array->has_shape()\n\nI have created a TensorFlow model which classifies MNIST image data. The model is then frozen and optimized for inference. These models I can benchmark as well. But when converting to TensorFlow lite, the toco command fails with the above mentioned error message.\nI have summarized the graph as well for both frozen and inference model.\nThe frozen model has the same size of (1,28,28,1) but the inference has a size of (None). I am using a placeholder when creating the model for the Input.", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS X High Sierra\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n'v1.7.0-rc1-1018-g7a0def60d4' 1.7.0-rc1\r\n- **Python version**: \r\n3.5.5\r\n- **Bazel version (if compiling from source)**:\r\n0.10.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=frozen.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=frozen.lite \\\r\n--input_array=Inputs/Input \\\r\n--output_array=Outputs/Prediction \\\r\n--input_shape=1,28,28,1 \\\r\n--inference_type=FLOAT\r\n-**Error Message**\r\n F tensorflow/contrib/lite/toco/tooling_util.cc:831] Check failed: array->has_shape()\r\n\r\nI have created a TensorFlow model which classifies MNIST image data. The model is then frozen and optimized for inference. These models I can benchmark as well. But when converting to TensorFlow lite, the toco command fails with the above mentioned error message.\r\n\r\nI have summarized the graph as well for both frozen and inference model.\r\nThe frozen model has the same size of (1,28,28,1) but the inference has a size of (None). I am using a placeholder when creating the model for the Input.\r\n"}