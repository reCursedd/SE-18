{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357579534", "html_url": "https://github.com/tensorflow/tensorflow/issues/16106#issuecomment-357579534", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16106", "id": 357579534, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzU3OTUzNA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-15T04:18:42Z", "updated_at": "2018-01-15T04:18:42Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=93858\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kpot\">@kpot</a> : Thanks for the report.</p>\n<p>This is admittedly confusing, but is \"working as intended\".<br>\nThis is because <code>int32</code> tensors are placed in host memory. This hack comes from <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/eager/pywrap_tensor.cc#L287\">here</a>. You should not observe this behavior with other types.</p>\n<p>The general idea is that many (though not all) GPU kernels require the tensor to be in host memory when operating on int32 inputs  (you noticed this in the <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/kernels/reverse_op.cc#L331\"><code>Reverse</code></a> operation, and the same holds for many other operations). When working with TensorFlow graphs (as opposed to eager execution), this is still the case, but the runtime copies tensors between devices if needed. Setting <code>tfe.DEVICE_PLACEMENT_SILENT</code> will achieve the same effect with eager execution. We didn't make automatic copying between devices default for eager execution as that can often lead to hard to diagnose performance problems (and tracing the precise copies that become bottlenecks can be a little involved).</p>\n<p>This is something we need to clean up (CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15258583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/josh11b\">@josh11b</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19335798\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/agarwal-ashish\">@agarwal-ashish</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1994308\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/akshayka\">@akshayka</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3731025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/allenlavoie\">@allenlavoie</a> ). In the interim, the mental model to work with is that the int32 type is almost always in host memory.</p>\n<p>Mind if I ask how you ran into this error? Did you have a need to operate on int32 tensors on GPUs?</p>", "body_text": "@kpot : Thanks for the report.\nThis is admittedly confusing, but is \"working as intended\".\nThis is because int32 tensors are placed in host memory. This hack comes from here. You should not observe this behavior with other types.\nThe general idea is that many (though not all) GPU kernels require the tensor to be in host memory when operating on int32 inputs  (you noticed this in the Reverse operation, and the same holds for many other operations). When working with TensorFlow graphs (as opposed to eager execution), this is still the case, but the runtime copies tensors between devices if needed. Setting tfe.DEVICE_PLACEMENT_SILENT will achieve the same effect with eager execution. We didn't make automatic copying between devices default for eager execution as that can often lead to hard to diagnose performance problems (and tracing the precise copies that become bottlenecks can be a little involved).\nThis is something we need to clean up (CC @alextp @josh11b @agarwal-ashish @akshayka @allenlavoie ). In the interim, the mental model to work with is that the int32 type is almost always in host memory.\nMind if I ask how you ran into this error? Did you have a need to operate on int32 tensors on GPUs?", "body": "@kpot : Thanks for the report.\r\n\r\nThis is admittedly confusing, but is \"working as intended\".\r\nThis is because `int32` tensors are placed in host memory. This hack comes from [here](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/eager/pywrap_tensor.cc#L287). You should not observe this behavior with other types.\r\n\r\nThe general idea is that many (though not all) GPU kernels require the tensor to be in host memory when operating on int32 inputs  (you noticed this in the [`Reverse`](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/kernels/reverse_op.cc#L331) operation, and the same holds for many other operations). When working with TensorFlow graphs (as opposed to eager execution), this is still the case, but the runtime copies tensors between devices if needed. Setting `tfe.DEVICE_PLACEMENT_SILENT` will achieve the same effect with eager execution. We didn't make automatic copying between devices default for eager execution as that can often lead to hard to diagnose performance problems (and tracing the precise copies that become bottlenecks can be a little involved). \r\n\r\nThis is something we need to clean up (CC @alextp @josh11b @agarwal-ashish @akshayka @allenlavoie ). In the interim, the mental model to work with is that the int32 type is almost always in host memory.\r\n\r\nMind if I ask how you ran into this error? Did you have a need to operate on int32 tensors on GPUs?"}