{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357746031", "html_url": "https://github.com/tensorflow/tensorflow/issues/16106#issuecomment-357746031", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16106", "id": 357746031, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Nzc0NjAzMQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-15T17:29:54Z", "updated_at": "2018-01-15T17:29:54Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=93858\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kpot\">@kpot</a> - might I suggest alternatives for now?</p>\n<p>Two options:</p>\n<ul>\n<li>Use <code>int64</code> indices (which will be placed on GPU)</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\ntfe.enable_eager_execution()\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n  large_tensor <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  i64 <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> All operations run on the GPU and out will be placed on the GPU</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The next line could be placed inside the \"with device\" scope,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> but it can be placed outside as well - since all inputs are on</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> GPU, the op will execute on GPU</span>\nout <span class=\"pl-k\">=</span> tf.gather(large_tensor, tf.nn.top_k(i64, <span class=\"pl-v\">k</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>).values)</pre></div>\n<ul>\n<li>Pay the cost of copying the (hopefully small) indices tensor to GPU</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\ntfe.enable_eager_execution()\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n  large_tensor <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  i32 <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> top_k will run on the CPU, but gather on the GPU,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> so large_tensor will not move from GPU</span>\nout <span class=\"pl-k\">=</span> tf.gather(large_tensor, tf.nn.top_k(i32, <span class=\"pl-v\">k</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>).values.gpu())</pre></div>\n<p>Hope that helps.</p>", "body_text": "@kpot - might I suggest alternatives for now?\nTwo options:\n\nUse int64 indices (which will be placed on GPU)\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\ntfe.enable_eager_execution()\n\nwith tf.device('/gpu:0'):\n  large_tensor = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n  i64 = tf.constant([1, 2], dtype=tf.int64)\n# All operations run on the GPU and out will be placed on the GPU\n# The next line could be placed inside the \"with device\" scope,\n# but it can be placed outside as well - since all inputs are on\n# GPU, the op will execute on GPU\nout = tf.gather(large_tensor, tf.nn.top_k(i64, k=2).values)\n\nPay the cost of copying the (hopefully small) indices tensor to GPU\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\ntfe.enable_eager_execution()\n\nwith tf.device('/gpu:0'):\n  large_tensor = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n  i32 = tf.constant([1, 2], dtype=tf.int32)\n# top_k will run on the CPU, but gather on the GPU,\n# so large_tensor will not move from GPU\nout = tf.gather(large_tensor, tf.nn.top_k(i32, k=2).values.gpu())\nHope that helps.", "body": "@kpot - might I suggest alternatives for now?\r\n\r\nTwo options:\r\n\r\n- Use `int64` indices (which will be placed on GPU)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n  large_tensor = tf.constant([1, 2, 3, 4], dtype=tf.float32)\r\n  i64 = tf.constant([1, 2], dtype=tf.int64)\r\n# All operations run on the GPU and out will be placed on the GPU\r\n# The next line could be placed inside the \"with device\" scope,\r\n# but it can be placed outside as well - since all inputs are on\r\n# GPU, the op will execute on GPU\r\nout = tf.gather(large_tensor, tf.nn.top_k(i64, k=2).values)\r\n```\r\n\r\n- Pay the cost of copying the (hopefully small) indices tensor to GPU\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\nwith tf.device('/gpu:0'):\r\n  large_tensor = tf.constant([1, 2, 3, 4], dtype=tf.float32)\r\n  i32 = tf.constant([1, 2], dtype=tf.int32)\r\n# top_k will run on the CPU, but gather on the GPU,\r\n# so large_tensor will not move from GPU\r\nout = tf.gather(large_tensor, tf.nn.top_k(i32, k=2).values.gpu())\r\n```\r\n\r\nHope that helps."}