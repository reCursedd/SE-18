{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20812", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20812/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20812/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20812/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20812", "id": 341299688, "node_id": "MDU6SXNzdWUzNDEyOTk2ODg=", "number": 20812, "title": "train_and_evaluate method of tf.estimator doesn't accept a function as value for parameter \"Optimizer\" whereas fit method of tf.contrib.learn does", "user": {"login": "g-hrafiq", "id": 38344712, "node_id": "MDQ6VXNlcjM4MzQ0NzEy", "avatar_url": "https://avatars3.githubusercontent.com/u/38344712?v=4", "gravatar_id": "", "url": "https://api.github.com/users/g-hrafiq", "html_url": "https://github.com/g-hrafiq", "followers_url": "https://api.github.com/users/g-hrafiq/followers", "following_url": "https://api.github.com/users/g-hrafiq/following{/other_user}", "gists_url": "https://api.github.com/users/g-hrafiq/gists{/gist_id}", "starred_url": "https://api.github.com/users/g-hrafiq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/g-hrafiq/subscriptions", "organizations_url": "https://api.github.com/users/g-hrafiq/orgs", "repos_url": "https://api.github.com/users/g-hrafiq/repos", "events_url": "https://api.github.com/users/g-hrafiq/events{/privacy}", "received_events_url": "https://api.github.com/users/g-hrafiq/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-15T07:22:41Z", "updated_at": "2018-11-10T18:49:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>:  3.5.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>`</p>\n<h1>Code example of train method with tf.contrib.learn ( Which works )</h1>\n<pre><code>def get_stepw_decay_optimizer():\n  def step_decay(global_step):\n    return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\n                                     # use customized decay function in learning_rate\n  return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\n\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_cols,\n                                      n_classes=2,\n                                      hidden_units=[32,64,64,64,64,64,64,64,64,64,64,64,64,\n                                                    64,64,64,64,64,32],\n                                      dropout = 0.1,\n                                      optimizer=get_stepw_decay_optimizer)\n\nclassifier.fit(input_fn=get_input_fn(training_set), steps=125000)\n</code></pre>\n<p><code></code></p>\n<h1>Same code example of train_and_evaluate with tf.estimator ( Which fails )</h1>\n<pre><code>def get_stepw_decay_optimizer():\n  def step_decay(global_step):\n    return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\n                                     # use customized decay function in learning_rate\n  return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\n\nestimator = tf.estimator.DNNClassifier(feature_columns=create_feature_cols(),\n                                      n_classes=2,\n                                      hidden_units=[32,64,64,64,64,64,\n                                                    64,64,64,64,64,64,\n                                                    64,64,64,64,64,64,\n                                                    32],\n                                      dropout = 0.1,\n                                      optimizer=get_stepw_decay_optimizer)\n\ntrain_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n                                  max_steps = num_train_steps)\n\nexp = tf.estimator.LatestExporter(\"decision\", serving_fn)\n\neval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n                                steps = None, \n                                exporters = exp,\n                                start_delay_secs = 1, # start evaluating after N seconds, \n                                throttle_secs = 40)  # evaluate every N seconds\n\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n</code></pre>\n<p>`</p>\n<h3>Describe the problem</h3>\n<p><strong>While trying to implement learning rate decay in canned estimator, I encountered the below issue.</strong></p>\n<p>When using the canned DNNClassifier from <strong>tf.contrib.learn</strong> it is possible to pass a function as input to parameter \"optimizer\" when using \"fit\" method but errors out when using \"train_and_evaluate\" of estimator from class <strong>tf.estimator.DNNClassifier</strong>, <strong>The given object is not an Optimizer instance. Given: &lt;function get_stepw_decay_optimizer at 0x000002076BACBF28&gt;</strong>. I believe, both the methods should work the same way and not raise any errors.</p>\n<h3>Source code / logs</h3>\n<h2>Error Logs<br>\n`INFO:tensorflow:Using default config.<br>\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmpa1rpeahr<br>\nINFO:tensorflow:Using config: {'_session_config': None, '_log_step_count_steps': 100, '_is_chief': True, '_service': None, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_model_dir': 'C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmpa1rpeahr', '_num_ps_replicas': 0, '_task_type': 'worker', '_master': '', '_task_id': 0, '_save_checkpoints_secs': 600, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x000002076BFCBC88&gt;}<br>\nINFO:tensorflow:Running training and evaluation locally (non-distributed).<br>\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 40 secs (eval_spec.throttle_secs) or training is finished.</h2>\n<p>ValueError                                Traceback (most recent call last)<br>\n in ()<br>\n----&gt; 1 train_and_evaluate(None, num_train_steps=200000)</p>\n<p> in train_and_evaluate(output_dir, num_train_steps)<br>\n20                                     start_delay_secs = 1, # start evaluating after N seconds,<br>\n21                                     throttle_secs = 40)  # evaluate every N seconds<br>\n---&gt; 22     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in train_and_evaluate(estimator, train_spec, eval_spec)<br>\n428       config.task_type != run_config_lib.TaskType.EVALUATOR):<br>\n429     logging.info('Running training and evaluation locally (non-distributed).')<br>\n--&gt; 430     executor.run_local()<br>\n431     return<br>\n432</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in run_local(self)<br>\n607           input_fn=self._train_spec.input_fn,<br>\n608           max_steps=self._train_spec.max_steps,<br>\n--&gt; 609           hooks=train_hooks)<br>\n610<br>\n611       # Final export signal: For any eval result with global_step &gt;= train</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)<br>\n300<br>\n301     saving_listeners = _check_listeners_type(saving_listeners)<br>\n--&gt; 302     loss = self._train_model(input_fn, hooks, saving_listeners)<br>\n303     logging.info('Loss for final step: %s.', loss)<br>\n304     return self</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _train_model(self, input_fn, hooks, saving_listeners)<br>\n709       with ops.control_dependencies([global_step_read_tensor]):<br>\n710         estimator_spec = self._call_model_fn(<br>\n--&gt; 711             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)<br>\n712       # Check if the user created a loss summary, and add one if they didn't.<br>\n713       # We assume here that the summary is called 'loss'. If it is not, we will</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _call_model_fn(self, features, labels, mode, config)<br>\n692     if 'config' in model_fn_args:<br>\n693       kwargs['config'] = config<br>\n--&gt; 694     model_fn_results = self._model_fn(features=features, **kwargs)<br>\n695<br>\n696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _model_fn(features, labels, mode, config)<br>\n332           dropout=dropout,<br>\n333           input_layer_partitioner=input_layer_partitioner,<br>\n--&gt; 334           config=config)<br>\n335     super(DNNClassifier, self).<strong>init</strong>(<br>\n336         model_fn=_model_fn, model_dir=model_dir, config=config)</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)<br>\n167                      'Given type: {}'.format(type(features)))<br>\n168   optimizer = optimizers.get_optimizer_instance(<br>\n--&gt; 169       optimizer, learning_rate=_LEARNING_RATE)<br>\n170   num_ps_replicas = config.num_ps_replicas if config else 0<br>\n171</p>\n<p>c:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\optimizers.py in get_optimizer_instance(opt, learning_rate)<br>\n75   if not isinstance(opt, optimizer_lib.Optimizer):<br>\n76     raise ValueError(<br>\n---&gt; 77         'The given object is not an Optimizer instance. Given: {}'.format(opt))<br>\n78   return opt</p>\n<p>ValueError: The given object is not an Optimizer instance. Given: &lt;function get_stepw_decay_optimizer at 0x000002076BACBF28&gt;<br>\n`</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.8\nPython version:  3.5.4\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce:\n\n`\nCode example of train method with tf.contrib.learn ( Which works )\ndef get_stepw_decay_optimizer():\n  def step_decay(global_step):\n    return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\n                                     # use customized decay function in learning_rate\n  return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\n\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_cols,\n                                      n_classes=2,\n                                      hidden_units=[32,64,64,64,64,64,64,64,64,64,64,64,64,\n                                                    64,64,64,64,64,32],\n                                      dropout = 0.1,\n                                      optimizer=get_stepw_decay_optimizer)\n\nclassifier.fit(input_fn=get_input_fn(training_set), steps=125000)\n\n\nSame code example of train_and_evaluate with tf.estimator ( Which fails )\ndef get_stepw_decay_optimizer():\n  def step_decay(global_step):\n    return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\n                                     # use customized decay function in learning_rate\n  return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\n\nestimator = tf.estimator.DNNClassifier(feature_columns=create_feature_cols(),\n                                      n_classes=2,\n                                      hidden_units=[32,64,64,64,64,64,\n                                                    64,64,64,64,64,64,\n                                                    64,64,64,64,64,64,\n                                                    32],\n                                      dropout = 0.1,\n                                      optimizer=get_stepw_decay_optimizer)\n\ntrain_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n                                  max_steps = num_train_steps)\n\nexp = tf.estimator.LatestExporter(\"decision\", serving_fn)\n\neval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n                                steps = None, \n                                exporters = exp,\n                                start_delay_secs = 1, # start evaluating after N seconds, \n                                throttle_secs = 40)  # evaluate every N seconds\n\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n`\nDescribe the problem\nWhile trying to implement learning rate decay in canned estimator, I encountered the below issue.\nWhen using the canned DNNClassifier from tf.contrib.learn it is possible to pass a function as input to parameter \"optimizer\" when using \"fit\" method but errors out when using \"train_and_evaluate\" of estimator from class tf.estimator.DNNClassifier, The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>. I believe, both the methods should work the same way and not raise any errors.\nSource code / logs\nError Logs\n`INFO:tensorflow:Using default config.\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmpa1rpeahr\nINFO:tensorflow:Using config: {'_session_config': None, '_log_step_count_steps': 100, '_is_chief': True, '_service': None, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_model_dir': 'C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmpa1rpeahr', '_num_ps_replicas': 0, '_task_type': 'worker', '_master': '', '_task_id': 0, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002076BFCBC88>}\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 40 secs (eval_spec.throttle_secs) or training is finished.\nValueError                                Traceback (most recent call last)\n in ()\n----> 1 train_and_evaluate(None, num_train_steps=200000)\n in train_and_evaluate(output_dir, num_train_steps)\n20                                     start_delay_secs = 1, # start evaluating after N seconds,\n21                                     throttle_secs = 40)  # evaluate every N seconds\n---> 22     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in train_and_evaluate(estimator, train_spec, eval_spec)\n428       config.task_type != run_config_lib.TaskType.EVALUATOR):\n429     logging.info('Running training and evaluation locally (non-distributed).')\n--> 430     executor.run_local()\n431     return\n432\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in run_local(self)\n607           input_fn=self._train_spec.input_fn,\n608           max_steps=self._train_spec.max_steps,\n--> 609           hooks=train_hooks)\n610\n611       # Final export signal: For any eval result with global_step >= train\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n300\n301     saving_listeners = _check_listeners_type(saving_listeners)\n--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)\n303     logging.info('Loss for final step: %s.', loss)\n304     return self\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\n709       with ops.control_dependencies([global_step_read_tensor]):\n710         estimator_spec = self._call_model_fn(\n--> 711             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n712       # Check if the user created a loss summary, and add one if they didn't.\n713       # We assume here that the summary is called 'loss'. If it is not, we will\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _call_model_fn(self, features, labels, mode, config)\n692     if 'config' in model_fn_args:\n693       kwargs['config'] = config\n--> 694     model_fn_results = self._model_fn(features=features, **kwargs)\n695\n696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _model_fn(features, labels, mode, config)\n332           dropout=dropout,\n333           input_layer_partitioner=input_layer_partitioner,\n--> 334           config=config)\n335     super(DNNClassifier, self).init(\n336         model_fn=_model_fn, model_dir=model_dir, config=config)\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)\n167                      'Given type: {}'.format(type(features)))\n168   optimizer = optimizers.get_optimizer_instance(\n--> 169       optimizer, learning_rate=_LEARNING_RATE)\n170   num_ps_replicas = config.num_ps_replicas if config else 0\n171\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\optimizers.py in get_optimizer_instance(opt, learning_rate)\n75   if not isinstance(opt, optimizer_lib.Optimizer):\n76     raise ValueError(\n---> 77         'The given object is not an Optimizer instance. Given: {}'.format(opt))\n78   return opt\nValueError: The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>\n`", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.5.4\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: \r\n\r\n`\r\n# Code example of train method with tf.contrib.learn ( Which works )\r\n    def get_stepw_decay_optimizer():\r\n      def step_decay(global_step):\r\n        return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\r\n                                         # use customized decay function in learning_rate\r\n      return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\r\n\r\n    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_cols,\r\n                                          n_classes=2,\r\n                                          hidden_units=[32,64,64,64,64,64,64,64,64,64,64,64,64,\r\n                                                        64,64,64,64,64,32],\r\n                                          dropout = 0.1,\r\n                                          optimizer=get_stepw_decay_optimizer)\r\n\r\n    classifier.fit(input_fn=get_input_fn(training_set), steps=125000)\r\n`\r\n`\r\n# Same code example of train_and_evaluate with tf.estimator ( Which fails )\r\n    def get_stepw_decay_optimizer():\r\n      def step_decay(global_step):\r\n        return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\r\n                                         # use customized decay function in learning_rate\r\n      return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\r\n\r\n    estimator = tf.estimator.DNNClassifier(feature_columns=create_feature_cols(),\r\n                                          n_classes=2,\r\n                                          hidden_units=[32,64,64,64,64,64,\r\n                                                        64,64,64,64,64,64,\r\n                                                        64,64,64,64,64,64,\r\n                                                        32],\r\n                                          dropout = 0.1,\r\n                                          optimizer=get_stepw_decay_optimizer)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \r\n                                      max_steps = num_train_steps)\r\n\r\n    exp = tf.estimator.LatestExporter(\"decision\", serving_fn)\r\n\r\n    eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \r\n                                    steps = None, \r\n                                    exporters = exp,\r\n                                    start_delay_secs = 1, # start evaluating after N seconds, \r\n                                    throttle_secs = 40)  # evaluate every N seconds\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n`\r\n\r\n### Describe the problem\r\n**While trying to implement learning rate decay in canned estimator, I encountered the below issue.**\r\n\r\nWhen using the canned DNNClassifier from **tf.contrib.learn** it is possible to pass a function as input to parameter \"optimizer\" when using \"fit\" method but errors out when using \"train_and_evaluate\" of estimator from class **tf.estimator.DNNClassifier**, **The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>**. I believe, both the methods should work the same way and not raise any errors.\r\n\r\n### Source code / logs\r\nError Logs\r\n`INFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmpa1rpeahr\r\nINFO:tensorflow:Using config: {'_session_config': None, '_log_step_count_steps': 100, '_is_chief': True, '_service': None, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_model_dir': 'C:\\\\Users\\\\hrafiq\\\\AppData\\\\Local\\\\Temp\\\\tmpa1rpeahr', '_num_ps_replicas': 0, '_task_type': 'worker', '_master': '', '_task_id': 0, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002076BFCBC88>}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 40 secs (eval_spec.throttle_secs) or training is finished.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-28-53806042763d> in <module>()\r\n----> 1 train_and_evaluate(None, num_train_steps=200000)\r\n\r\n<ipython-input-27-182baf3372f9> in train_and_evaluate(output_dir, num_train_steps)\r\n     20                                     start_delay_secs = 1, # start evaluating after N seconds,\r\n     21                                     throttle_secs = 40)  # evaluate every N seconds\r\n---> 22     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    428       config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    429     logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 430     executor.run_local()\r\n    431     return\r\n    432 \r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in run_local(self)\r\n    607           input_fn=self._train_spec.input_fn,\r\n    608           max_steps=self._train_spec.max_steps,\r\n--> 609           hooks=train_hooks)\r\n    610 \r\n    611       # Final export signal: For any eval result with global_step >= train\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    300 \r\n    301     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    303     logging.info('Loss for final step: %s.', loss)\r\n    304     return self\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    709       with ops.control_dependencies([global_step_read_tensor]):\r\n    710         estimator_spec = self._call_model_fn(\r\n--> 711             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n    712       # Check if the user created a loss summary, and add one if they didn't.\r\n    713       # We assume here that the summary is called 'loss'. If it is not, we will\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n    692     if 'config' in model_fn_args:\r\n    693       kwargs['config'] = config\r\n--> 694     model_fn_results = self._model_fn(features=features, **kwargs)\r\n    695 \r\n    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _model_fn(features, labels, mode, config)\r\n    332           dropout=dropout,\r\n    333           input_layer_partitioner=input_layer_partitioner,\r\n--> 334           config=config)\r\n    335     super(DNNClassifier, self).__init__(\r\n    336         model_fn=_model_fn, model_dir=model_dir, config=config)\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)\r\n    167                      'Given type: {}'.format(type(features)))\r\n    168   optimizer = optimizers.get_optimizer_instance(\r\n--> 169       optimizer, learning_rate=_LEARNING_RATE)\r\n    170   num_ps_replicas = config.num_ps_replicas if config else 0\r\n    171 \r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\optimizers.py in get_optimizer_instance(opt, learning_rate)\r\n     75   if not isinstance(opt, optimizer_lib.Optimizer):\r\n     76     raise ValueError(\r\n---> 77         'The given object is not an Optimizer instance. Given: {}'.format(opt))\r\n     78   return opt\r\n\r\nValueError: The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>\r\n`\r\n"}