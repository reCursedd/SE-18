{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17121", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17121/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17121/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17121/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17121", "id": 298172027, "node_id": "MDU6SXNzdWUyOTgxNzIwMjc=", "number": 17121, "title": "Feature Request for the back-propagated errors in intermediate layers", "user": {"login": "glhfgg1024", "id": 30264480, "node_id": "MDQ6VXNlcjMwMjY0NDgw", "avatar_url": "https://avatars3.githubusercontent.com/u/30264480?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glhfgg1024", "html_url": "https://github.com/glhfgg1024", "followers_url": "https://api.github.com/users/glhfgg1024/followers", "following_url": "https://api.github.com/users/glhfgg1024/following{/other_user}", "gists_url": "https://api.github.com/users/glhfgg1024/gists{/gist_id}", "starred_url": "https://api.github.com/users/glhfgg1024/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glhfgg1024/subscriptions", "organizations_url": "https://api.github.com/users/glhfgg1024/orgs", "repos_url": "https://api.github.com/users/glhfgg1024/repos", "events_url": "https://api.github.com/users/glhfgg1024/events{/privacy}", "received_events_url": "https://api.github.com/users/glhfgg1024/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-02-19T06:22:19Z", "updated_at": "2018-02-21T15:42:28Z", "closed_at": "2018-02-19T06:25:40Z", "author_association": "NONE", "body_html": "<p>After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:</p>\n<pre><code> I-&gt;(W1)-&gt;C1-&gt;(W2)-&gt;C2-&gt;(W3)-&gt;O\n</code></pre>\n<p><code>I</code> is the input, <code>O</code> is the output, <code>W1,W2,W3</code> is the weights for 3 layers. <code>C1</code> and <code>C2</code> are the outputs for the first two layers. With <code>O</code> and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to <code>C1</code> and <code>C2</code>?</p>\n<p>I know we could get the parameter operators as follows:</p>\n<pre><code>W1_op = tf.get_default_graph().get_tensor_by_name('W1')\nW1_op = ...\n</code></pre>\n<p>My final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).</p>\n<p>I know that we could use the <code>tf.test.check_gradient</code> to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.</p>\n<p>In the Caffe framework, it seems those <code>errors</code> were saved in <code>diff</code> memory for each layer. I want to get these back-propagated <code>errors</code> in each layer. Does anybody know how to get that?</p>", "body_text": "After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:\n I->(W1)->C1->(W2)->C2->(W3)->O\n\nI is the input, O is the output, W1,W2,W3 is the weights for 3 layers. C1 and C2 are the outputs for the first two layers. With O and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to C1 and C2?\nI know we could get the parameter operators as follows:\nW1_op = tf.get_default_graph().get_tensor_by_name('W1')\nW1_op = ...\n\nMy final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).\nI know that we could use the tf.test.check_gradient to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.\nIn the Caffe framework, it seems those errors were saved in diff memory for each layer. I want to get these back-propagated errors in each layer. Does anybody know how to get that?", "body": "After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:\r\n\r\n     I->(W1)->C1->(W2)->C2->(W3)->O\r\n\r\n`I` is the input, `O` is the output, `W1,W2,W3` is the weights for 3 layers. `C1` and `C2` are the outputs for the first two layers. With `O` and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to `C1` and `C2`?  \r\n\r\nI know we could get the parameter operators as follows:\r\n\r\n    W1_op = tf.get_default_graph().get_tensor_by_name('W1')\r\n    W1_op = ...\r\n\r\nMy final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).\r\n\r\nI know that we could use the `tf.test.check_gradient` to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.\r\n\r\nIn the Caffe framework, it seems those `errors` were saved in `diff` memory for each layer. I want to get these back-propagated `errors` in each layer. Does anybody know how to get that?"}