{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264262711", "html_url": "https://github.com/tensorflow/tensorflow/issues/5981#issuecomment-264262711", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5981", "id": 264262711, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDI2MjcxMQ==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-01T19:04:39Z", "updated_at": "2016-12-01T19:05:26Z", "author_association": "MEMBER", "body_html": "<p>From reading your code, there are two likely reasons for this, one fairly obvious, the other less so:</p>\n<ol>\n<li>Your benchmark measures the overheads of retrieving a large trace protobuf via the <code>session.run</code> API.  This includes the cost of serializing trace data into a protobuf and copying back to Python.</li>\n<li><code>BasicLSTMCell</code> includes quite a large number of tiny elementwise ops, plus one large(ish) MatMul.  Tracing overheads are typically order 1us per op at execution time.  Most of the tiny ops in an LSTM cell execute in less time than this on a GPU.</li>\n</ol>\n<p>When I run on my desktop with a K40, I get 1.3s without profiling and 4.0s with profiling.  So basically the same ratio you reported.</p>\n<p>If you examine the timeline generated from the computation I expect the time spent <em>computing</em> is not much different to your baseline measurement....</p>\n<p>I added code to write out the profile and generate a chrome trace viewer timeline:</p>\n<pre><code>print(len(run_metadata.SerializeToString()))\n# Create the Timeline object, and write it to a json\ntl = timeline.Timeline(run_metadata.step_stats)\nctf = tl.generate_chrome_trace_format()\nwith open('timeline.json', 'w') as f:\n  f.write(ctf)\n</code></pre>\n<p>When you look at the captured profiler data you can see that the computation still executes in approx 1.3s.  This is basically identical to the time without profiling (which includes feeding input tensors from Python)<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/11547801/20807274/178c91f4-b7b3-11e6-9c65-7fdf77dda742.png\"><img src=\"https://cloud.githubusercontent.com/assets/11547801/20807274/178c91f4-b7b3-11e6-9c65-7fdf77dda742.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>So tracing isn't actually slowing down execution by very much.  Retrieving the trace seems to be expensive in this case.</p>\n<p>The size of the <code>run_metadata</code> is about 5MB.... larger than it <em>could</em> be, but I agree that it seems ridiculous that fetching back the extra 5MB takes over 2 seconds.  It's probably worth running under a CPU profiler to work out what's going on here!</p>", "body_text": "From reading your code, there are two likely reasons for this, one fairly obvious, the other less so:\n\nYour benchmark measures the overheads of retrieving a large trace protobuf via the session.run API.  This includes the cost of serializing trace data into a protobuf and copying back to Python.\nBasicLSTMCell includes quite a large number of tiny elementwise ops, plus one large(ish) MatMul.  Tracing overheads are typically order 1us per op at execution time.  Most of the tiny ops in an LSTM cell execute in less time than this on a GPU.\n\nWhen I run on my desktop with a K40, I get 1.3s without profiling and 4.0s with profiling.  So basically the same ratio you reported.\nIf you examine the timeline generated from the computation I expect the time spent computing is not much different to your baseline measurement....\nI added code to write out the profile and generate a chrome trace viewer timeline:\nprint(len(run_metadata.SerializeToString()))\n# Create the Timeline object, and write it to a json\ntl = timeline.Timeline(run_metadata.step_stats)\nctf = tl.generate_chrome_trace_format()\nwith open('timeline.json', 'w') as f:\n  f.write(ctf)\n\nWhen you look at the captured profiler data you can see that the computation still executes in approx 1.3s.  This is basically identical to the time without profiling (which includes feeding input tensors from Python)\n\nSo tracing isn't actually slowing down execution by very much.  Retrieving the trace seems to be expensive in this case.\nThe size of the run_metadata is about 5MB.... larger than it could be, but I agree that it seems ridiculous that fetching back the extra 5MB takes over 2 seconds.  It's probably worth running under a CPU profiler to work out what's going on here!", "body": "From reading your code, there are two likely reasons for this, one fairly obvious, the other less so:\r\n1) Your benchmark measures the overheads of retrieving a large trace protobuf via the `session.run` API.  This includes the cost of serializing trace data into a protobuf and copying back to Python.\r\n2) `BasicLSTMCell` includes quite a large number of tiny elementwise ops, plus one large(ish) MatMul.  Tracing overheads are typically order 1us per op at execution time.  Most of the tiny ops in an LSTM cell execute in less time than this on a GPU.\r\n\r\nWhen I run on my desktop with a K40, I get 1.3s without profiling and 4.0s with profiling.  So basically the same ratio you reported.\r\n\r\nIf you examine the timeline generated from the computation I expect the time spent _computing_ is not much different to your baseline measurement....\r\n\r\nI added code to write out the profile and generate a chrome trace viewer timeline:  \r\n```\r\nprint(len(run_metadata.SerializeToString()))\r\n# Create the Timeline object, and write it to a json\r\ntl = timeline.Timeline(run_metadata.step_stats)\r\nctf = tl.generate_chrome_trace_format()\r\nwith open('timeline.json', 'w') as f:\r\n  f.write(ctf)\r\n```\r\nWhen you look at the captured profiler data you can see that the computation still executes in approx 1.3s.  This is basically identical to the time without profiling (which includes feeding input tensors from Python)\r\n![image](https://cloud.githubusercontent.com/assets/11547801/20807274/178c91f4-b7b3-11e6-9c65-7fdf77dda742.png)\r\n\r\nSo tracing isn't actually slowing down execution by very much.  Retrieving the trace seems to be expensive in this case.\r\n\r\nThe size of the `run_metadata` is about 5MB.... larger than it _could_ be, but I agree that it seems ridiculous that fetching back the extra 5MB takes over 2 seconds.  It's probably worth running under a CPU profiler to work out what's going on here! \r\n"}