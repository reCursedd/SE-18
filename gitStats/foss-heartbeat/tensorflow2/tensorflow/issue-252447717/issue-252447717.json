{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12541", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12541/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12541/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12541/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12541", "id": 252447717, "node_id": "MDU6SXNzdWUyNTI0NDc3MTc=", "number": 12541, "title": "Feature Request - Dynamic Dispatch of optimized functions for deployment builds", "user": {"login": "vade", "id": 65011, "node_id": "MDQ6VXNlcjY1MDEx", "avatar_url": "https://avatars1.githubusercontent.com/u/65011?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vade", "html_url": "https://github.com/vade", "followers_url": "https://api.github.com/users/vade/followers", "following_url": "https://api.github.com/users/vade/following{/other_user}", "gists_url": "https://api.github.com/users/vade/gists{/gist_id}", "starred_url": "https://api.github.com/users/vade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vade/subscriptions", "organizations_url": "https://api.github.com/users/vade/orgs", "repos_url": "https://api.github.com/users/vade/repos", "events_url": "https://api.github.com/users/vade/events{/privacy}", "received_events_url": "https://api.github.com/users/vade/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-08-23T23:44:40Z", "updated_at": "2017-08-30T17:40:21Z", "closed_at": "2017-08-25T18:14:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>P</p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNA</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nMac OS X 10.12.16</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nN/A</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nN/A</li>\n<li><strong>Python version</strong>:<br>\nN/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nN/A</li>\n<li><strong>GPU model and memory</strong>:<br>\nN/A</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nN/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hello. Thanks for TF.</p>\n<p>Recently OpenCV has integrated dynamic dispatch allowing OpenCV library to leverage optimal code paths for supported hardware acceleration (sse, fma, avx, avx2 etc), from a single compiled binary. This is super useful for deployment builds when you are unsure what CPU family/features will potentially run your binary.</p>\n<p>I understand TF's execution model and potentially XLA may add complications, not even to say GPU support - but for CPU ops, or CPU only deployments I imagine something equivalent would be well received.</p>\n<p>While we are speaking of deployment builds, are folks just shipping non-optimized-for-client-hardware binaries, or compiling different builds for different hardware features for end products?</p>\n<p>Thank you.</p>", "body_text": "P\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNA\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMac OS X 10.12.16\nTensorFlow installed from (source or binary):\nN/A\nTensorFlow version (use command below):\nN/A\nPython version:\nN/A\nBazel version (if compiling from source):\nN/A\nCUDA/cuDNN version:\nN/A\nGPU model and memory:\nN/A\nExact command to reproduce:\nN/A\n\nDescribe the problem\nHello. Thanks for TF.\nRecently OpenCV has integrated dynamic dispatch allowing OpenCV library to leverage optimal code paths for supported hardware acceleration (sse, fma, avx, avx2 etc), from a single compiled binary. This is super useful for deployment builds when you are unsure what CPU family/features will potentially run your binary.\nI understand TF's execution model and potentially XLA may add complications, not even to say GPU support - but for CPU ops, or CPU only deployments I imagine something equivalent would be well received.\nWhile we are speaking of deployment builds, are folks just shipping non-optimized-for-client-hardware binaries, or compiling different builds for different hardware features for end products?\nThank you.", "body": "P\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS X 10.12.16 \r\n- **TensorFlow installed from (source or binary)**:\r\nN/A\r\n- **TensorFlow version (use command below)**:\r\nN/A\r\n- **Python version**: \r\nN/A\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\n\r\nHello. Thanks for TF.\r\n\r\nRecently OpenCV has integrated dynamic dispatch allowing OpenCV library to leverage optimal code paths for supported hardware acceleration (sse, fma, avx, avx2 etc), from a single compiled binary. This is super useful for deployment builds when you are unsure what CPU family/features will potentially run your binary. \r\n\r\nI understand TF's execution model and potentially XLA may add complications, not even to say GPU support - but for CPU ops, or CPU only deployments I imagine something equivalent would be well received.\r\n\r\nWhile we are speaking of deployment builds, are folks just shipping non-optimized-for-client-hardware binaries, or compiling different builds for different hardware features for end products?\r\n\r\nThank you. \r\n"}