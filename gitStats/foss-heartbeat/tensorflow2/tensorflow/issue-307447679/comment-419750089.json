{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419750089", "html_url": "https://github.com/tensorflow/tensorflow/issues/17910#issuecomment-419750089", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17910", "id": 419750089, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTc1MDA4OQ==", "user": {"login": "xilenteyex", "id": 10864603, "node_id": "MDQ6VXNlcjEwODY0NjAz", "avatar_url": "https://avatars1.githubusercontent.com/u/10864603?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xilenteyex", "html_url": "https://github.com/xilenteyex", "followers_url": "https://api.github.com/users/xilenteyex/followers", "following_url": "https://api.github.com/users/xilenteyex/following{/other_user}", "gists_url": "https://api.github.com/users/xilenteyex/gists{/gist_id}", "starred_url": "https://api.github.com/users/xilenteyex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xilenteyex/subscriptions", "organizations_url": "https://api.github.com/users/xilenteyex/orgs", "repos_url": "https://api.github.com/users/xilenteyex/repos", "events_url": "https://api.github.com/users/xilenteyex/events{/privacy}", "received_events_url": "https://api.github.com/users/xilenteyex/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-09T22:36:54Z", "updated_at": "2018-09-17T22:34:51Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a> ,<br>\nThanks for replying.</p>\n<p>I am trying to understand logging in tensorflow in depth. I think cifar is slightly a bigger model to start with. Now, I am using a MNIST example. Script that I ran can be found <a href=\"https://gist.github.com/xilenteyex/f652be2306573020eec9152a87915324\">(here)</a>. Following is the list of issues:</p>\n<ul>\n<li>\n<p>I made sure that run_metadata is called separately for every sess.run call. I am still seeing multiple entries for the same operation in the same thread . For example, op named \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" is logged three times in pid = '11'. Does this really mean that my model is designed in such a way that it requires \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" op to be executed three times for each sess.run call ? (step_stats can be found <a href=\"https://gist.github.com/xilenteyex/aef5dd189fe210acd4d8720c804b43a8\">here</a> and timline can be found <a href=\"https://gist.github.com/xilenteyex/6ac4f81d767bc6a2cab89822a6d38584\">here</a>)</p>\n</li>\n<li>\n<p>According to my understanding (please correct me, if I am wrong). There are 4 threads running on GPU, one is responsible for MEMCPYHtoD, one for MEMCPYDtoH, one for MEMCPYPtoP and one thread for execution of compute nodes. But, I see a lot of GPU streams in the timeline, is this just an artifact of timeline to enable it to be visualized in the timeline or it has some other significance ?</p>\n</li>\n<li>\n<p>Also, reading ur comments, on <a href=\"https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-244251867\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1824/hovercard\">other issues</a>, I understand that the pid named : \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" just logs the time it takes to launch the kernel on the GPU. Actual execution time is logged in the streams named \"/device:GPU:0/stream:* Compute\". But after looking at the timestamps and the duration, it looks like \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" is actually logging the total time (kernel launch + actual compute node execution time on GPU) e.g.<br>\nfor the op \"train/Adam/update_layer2/weights/Variable/ApplyAdam\", there is one entry in \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" (pid : 9) which starts at timestamp : 1536530947160930 and ends at 1536530947161023, but there are three entries in the pid : 11, named : \"/device:GPU:0/stream:22 Compute\". These three entries start and end with timestamps as follows:<br>\n(start, end)<br>\n(1536530947160982, 1536530947160984)<br>\n(1536530947160999, 1536530947161001)<br>\n(1536530947161016, 1536530947161021)<br>\nAll three of these have starting time after the start timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" and ends before the end timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\". What does this really mean ? What does these gaps in these three timeslots show? Also, I expect actual execution to start after the kernel has been queued. These timestamps show otherwise. Am I missing something ? Also, if they are executed three times, I expect three kernels to be queued, but \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" process has only one entry.<br>\nAlso, what is the difference between two GPU streams \"/device:GPU:0/stream:22 Compute\" (pid: 11) and \"/device:GPU:0/stream:all Compute\" (pid: 7)</p>\n</li>\n<li>\n<p>In pid: \"5\" named \"/device:GPU:0/memcpy Compute\", I assumed that it will only contain memcpy nvidia cuda operations. But, there is an op of type \"Assign\" in this stream. Is this the expected behavior ?</p>\n</li>\n<li>\n<p>In order to log the time it takes to send a tensor from one one device to another, is it enough to look at the duration and timestamps of memcpy ops ? or do i need to enable logging of send/recv op nodes using the hack as discussed in this <a href=\"https://github.com/tensorflow/tensorflow/issues/4809\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4809/hovercard\">github issue</a> ? If those memcpy ops report the correct times and we don't really need the hack to enable the send/recv ops. Is there a way to map MEMCPY ops to tensors ?</p>\n</li>\n<li>\n<p>Also, whats the purpose of \"/device:GPU:0/stream:* Tensors\" and \"/job:localhost/replica:0/task:0/device:GPU:0 Tensors\" processes ?</p>\n</li>\n<li>\n<p>Overall, for a given tensorflow graph, I am interested in creating a timeline programmatically (not UI, raw values), using which I can find the time at which an op (compute as well as communication op) started and the time it took to complete. Do you think tensorflow.timeline is the right way to go about doing this ?</p>\n</li>\n</ul>\n<p>P.S. : I am sorry, I know this is not a discussion forum and lack of documentation about usage and what each field represents about logging, I am unable to figure out how to use this exactly. I tried asking the same thing over stackoverflow a few times, but my questions remained unanswered. I am stuck at this for almost 3 months now. If you can clarify all these things, that wil be awesome!</p>\n<p>Thanks a lot for looking into this.</p>", "body_text": "Hi @prb12 ,\nThanks for replying.\nI am trying to understand logging in tensorflow in depth. I think cifar is slightly a bigger model to start with. Now, I am using a MNIST example. Script that I ran can be found (here). Following is the list of issues:\n\n\nI made sure that run_metadata is called separately for every sess.run call. I am still seeing multiple entries for the same operation in the same thread . For example, op named \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" is logged three times in pid = '11'. Does this really mean that my model is designed in such a way that it requires \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" op to be executed three times for each sess.run call ? (step_stats can be found here and timline can be found here)\n\n\nAccording to my understanding (please correct me, if I am wrong). There are 4 threads running on GPU, one is responsible for MEMCPYHtoD, one for MEMCPYDtoH, one for MEMCPYPtoP and one thread for execution of compute nodes. But, I see a lot of GPU streams in the timeline, is this just an artifact of timeline to enable it to be visualized in the timeline or it has some other significance ?\n\n\nAlso, reading ur comments, on other issues, I understand that the pid named : \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" just logs the time it takes to launch the kernel on the GPU. Actual execution time is logged in the streams named \"/device:GPU:0/stream:* Compute\". But after looking at the timestamps and the duration, it looks like \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" is actually logging the total time (kernel launch + actual compute node execution time on GPU) e.g.\nfor the op \"train/Adam/update_layer2/weights/Variable/ApplyAdam\", there is one entry in \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" (pid : 9) which starts at timestamp : 1536530947160930 and ends at 1536530947161023, but there are three entries in the pid : 11, named : \"/device:GPU:0/stream:22 Compute\". These three entries start and end with timestamps as follows:\n(start, end)\n(1536530947160982, 1536530947160984)\n(1536530947160999, 1536530947161001)\n(1536530947161016, 1536530947161021)\nAll three of these have starting time after the start timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" and ends before the end timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\". What does this really mean ? What does these gaps in these three timeslots show? Also, I expect actual execution to start after the kernel has been queued. These timestamps show otherwise. Am I missing something ? Also, if they are executed three times, I expect three kernels to be queued, but \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" process has only one entry.\nAlso, what is the difference between two GPU streams \"/device:GPU:0/stream:22 Compute\" (pid: 11) and \"/device:GPU:0/stream:all Compute\" (pid: 7)\n\n\nIn pid: \"5\" named \"/device:GPU:0/memcpy Compute\", I assumed that it will only contain memcpy nvidia cuda operations. But, there is an op of type \"Assign\" in this stream. Is this the expected behavior ?\n\n\nIn order to log the time it takes to send a tensor from one one device to another, is it enough to look at the duration and timestamps of memcpy ops ? or do i need to enable logging of send/recv op nodes using the hack as discussed in this github issue ? If those memcpy ops report the correct times and we don't really need the hack to enable the send/recv ops. Is there a way to map MEMCPY ops to tensors ?\n\n\nAlso, whats the purpose of \"/device:GPU:0/stream:* Tensors\" and \"/job:localhost/replica:0/task:0/device:GPU:0 Tensors\" processes ?\n\n\nOverall, for a given tensorflow graph, I am interested in creating a timeline programmatically (not UI, raw values), using which I can find the time at which an op (compute as well as communication op) started and the time it took to complete. Do you think tensorflow.timeline is the right way to go about doing this ?\n\n\nP.S. : I am sorry, I know this is not a discussion forum and lack of documentation about usage and what each field represents about logging, I am unable to figure out how to use this exactly. I tried asking the same thing over stackoverflow a few times, but my questions remained unanswered. I am stuck at this for almost 3 months now. If you can clarify all these things, that wil be awesome!\nThanks a lot for looking into this.", "body": "Hi @prb12 ,\r\nThanks for replying.\r\n\r\nI am trying to understand logging in tensorflow in depth. I think cifar is slightly a bigger model to start with. Now, I am using a MNIST example. Script that I ran can be found [(here)](https://gist.github.com/xilenteyex/f652be2306573020eec9152a87915324). Following is the list of issues:\r\n\r\n- I made sure that run_metadata is called separately for every sess.run call. I am still seeing multiple entries for the same operation in the same thread . For example, op named \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" is logged three times in pid = '11'. Does this really mean that my model is designed in such a way that it requires \"train/Adam/update_layer2/weights/Variable/ApplyAdam\" op to be executed three times for each sess.run call ? (step_stats can be found [here](https://gist.github.com/xilenteyex/aef5dd189fe210acd4d8720c804b43a8) and timline can be found [here](https://gist.github.com/xilenteyex/6ac4f81d767bc6a2cab89822a6d38584))\r\n\r\n- According to my understanding (please correct me, if I am wrong). There are 4 threads running on GPU, one is responsible for MEMCPYHtoD, one for MEMCPYDtoH, one for MEMCPYPtoP and one thread for execution of compute nodes. But, I see a lot of GPU streams in the timeline, is this just an artifact of timeline to enable it to be visualized in the timeline or it has some other significance ?\r\n\r\n- Also, reading ur comments, on [other issues](https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-244251867), I understand that the pid named : \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" just logs the time it takes to launch the kernel on the GPU. Actual execution time is logged in the streams named \"/device:GPU:0/stream:* Compute\". But after looking at the timestamps and the duration, it looks like \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" is actually logging the total time (kernel launch + actual compute node execution time on GPU) e.g. \r\nfor the op \"train/Adam/update_layer2/weights/Variable/ApplyAdam\", there is one entry in \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" (pid : 9) which starts at timestamp : 1536530947160930 and ends at 1536530947161023, but there are three entries in the pid : 11, named : \"/device:GPU:0/stream:22 Compute\". These three entries start and end with timestamps as follows:\r\n(start, end)\r\n(1536530947160982, 1536530947160984)\r\n(1536530947160999, 1536530947161001)\r\n(1536530947161016, 1536530947161021)\r\nAll three of these have starting time after the start timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" and ends before the end timestamp of \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\". What does this really mean ? What does these gaps in these three timeslots show? Also, I expect actual execution to start after the kernel has been queued. These timestamps show otherwise. Am I missing something ? Also, if they are executed three times, I expect three kernels to be queued, but \"/job:localhost/replica:0/task:0/device:GPU:0 Compute\" process has only one entry.\r\nAlso, what is the difference between two GPU streams \"/device:GPU:0/stream:22 Compute\" (pid: 11) and \"/device:GPU:0/stream:all Compute\" (pid: 7)\r\n\r\n- In pid: \"5\" named \"/device:GPU:0/memcpy Compute\", I assumed that it will only contain memcpy nvidia cuda operations. But, there is an op of type \"Assign\" in this stream. Is this the expected behavior ?\r\n\r\n- In order to log the time it takes to send a tensor from one one device to another, is it enough to look at the duration and timestamps of memcpy ops ? or do i need to enable logging of send/recv op nodes using the hack as discussed in this [github issue](https://github.com/tensorflow/tensorflow/issues/4809) ? If those memcpy ops report the correct times and we don't really need the hack to enable the send/recv ops. Is there a way to map MEMCPY ops to tensors ?\r\n\r\n- Also, whats the purpose of \"/device:GPU:0/stream:* Tensors\" and \"/job:localhost/replica:0/task:0/device:GPU:0 Tensors\" processes ?\r\n\r\n- Overall, for a given tensorflow graph, I am interested in creating a timeline programmatically (not UI, raw values), using which I can find the time at which an op (compute as well as communication op) started and the time it took to complete. Do you think tensorflow.timeline is the right way to go about doing this ?\r\n\r\nP.S. : I am sorry, I know this is not a discussion forum and lack of documentation about usage and what each field represents about logging, I am unable to figure out how to use this exactly. I tried asking the same thing over stackoverflow a few times, but my questions remained unanswered. I am stuck at this for almost 3 months now. If you can clarify all these things, that wil be awesome! \r\n\r\nThanks a lot for looking into this."}