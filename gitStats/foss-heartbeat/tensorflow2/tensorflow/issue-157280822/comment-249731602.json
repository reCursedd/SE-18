{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/249731602", "html_url": "https://github.com/tensorflow/tensorflow/issues/2540#issuecomment-249731602", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2540", "id": 249731602, "node_id": "MDEyOklzc3VlQ29tbWVudDI0OTczMTYwMg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-26T23:59:12Z", "updated_at": "2016-09-26T23:59:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>BTW, it's not just the issue with select, code below also gives NaN.</p>\n<p>x = tf.placeholder(tf.float32)<br>\ny = tf.exp(x)<br>\nz = tf.exp(-y)<br>\ngrad = tf.gradients(z,[x])[0]<br>\nprint sess.run(grad, feed_dict={x: 1e10})</p>\n<p>The underlying cause is that  chain rule gives f' * g' as derivative of<br>\nf(g(x)) so with because 1e10 overflows we get infinity_0 in both cases.<br>\nOne could argue that limiting arguments that justify inf_0=NaN should not<br>\napply in case of tf.select, and substitute special gradient for tf.select<br>\nusing gradient_override_map during runtime</p>\n<p>On Mon, Sep 26, 2016 at 9:35 AM, Eric Martin <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>After reading some code and docs, I think I see the difficulty with select<br>\ngradient.</p>\n<p>The select gradient is here: <a href=\"https://github.com/tensorflow/\">https://github.com/tensorflow/</a><br>\ntensorflow/blob/4addf4b5806cd731949c6582a83f58<br>\n24599cd1ef/tensorflow/python/ops/math_grad.py#L688</p>\n<p>Gradients are only taken on the output of an op with respect to the input<br>\n(and then multiplied by the accumulated gradient from objective to output<br>\nof op), so in my example above the select gradient computes dw/dt=1 and<br>\ndw/de=0. This gives dL/dt=dL/dw and dL/de=0 for objective L.</p>\n<p>However, the graph traversal algorithm to compute gradient continues, and<br>\ncomputes dt/dx = 1, de/dx = -Infinity, which finally gives dL/dx = dL/dt</p>\n<ul>\n<li>dt/dx + dL/de * de/dx = dL/dt * 1 + 0 * Inf = dL/dt + NaN = NaN.</li>\n</ul>\n<p>The issue is the 0 * Inf = NaN term, and the fact that this computation<br>\ndoesn't happen in the select node (happens in the divide node in this case).</p>\n<p>I spent a few minutes thinking of possible solutions to this, and none of<br>\nthem are pretty or seem worth implementing.<br>\nOne option is to pass a mask of hard zero gradients around during gradient<br>\ncomputation. In this case, after each node computes it's gradients, the<br>\nhard zero gradients can be reset to 0. This isn't a very appealing option.<br>\nThe other option is to significantly change the gradient propagation<br>\nalgorithm. Rather than computing gradient of objective with respect to op<br>\ninput, compute gradient of op output with respect to op input for all ops<br>\nfirst. Then do a multiplicative accumulate pass. This has the downside of<br>\nsignificant framework rewrite, and is unworkable because gradient of op<br>\noutput wrt op input can be high order tensors (gradient of matrix multiply<br>\nwrt inputs is very large).<br>\nIn summary, there doesn't seem to be a good way to make Tensorflow compute<br>\nthe select gradient correctly. I wrote this largely for my own<br>\nunderstanding of why \"[NaN gradient] is inevitable\", hopefully this can be<br>\nuseful to someone else who happens across the issue.</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"157280822\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2540\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2540/hovercard?comment_id=249623937&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/2540#issuecomment-249623937\">#2540 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHE-b1wrlAAYk-694eHsOS3GT9xdXks5qt_RsgaJpZM4Io0ms\">https://github.com/notifications/unsubscribe-auth/AABaHE-b1wrlAAYk-694eHsOS3GT9xdXks5qt_RsgaJpZM4Io0ms</a><br>\n.</p>\n</blockquote>", "body_text": "BTW, it's not just the issue with select, code below also gives NaN.\nx = tf.placeholder(tf.float32)\ny = tf.exp(x)\nz = tf.exp(-y)\ngrad = tf.gradients(z,[x])[0]\nprint sess.run(grad, feed_dict={x: 1e10})\nThe underlying cause is that  chain rule gives f' * g' as derivative of\nf(g(x)) so with because 1e10 overflows we get infinity_0 in both cases.\nOne could argue that limiting arguments that justify inf_0=NaN should not\napply in case of tf.select, and substitute special gradient for tf.select\nusing gradient_override_map during runtime\nOn Mon, Sep 26, 2016 at 9:35 AM, Eric Martin notifications@github.com\nwrote:\n\nAfter reading some code and docs, I think I see the difficulty with select\ngradient.\nThe select gradient is here: https://github.com/tensorflow/\ntensorflow/blob/4addf4b5806cd731949c6582a83f58\n24599cd1ef/tensorflow/python/ops/math_grad.py#L688\nGradients are only taken on the output of an op with respect to the input\n(and then multiplied by the accumulated gradient from objective to output\nof op), so in my example above the select gradient computes dw/dt=1 and\ndw/de=0. This gives dL/dt=dL/dw and dL/de=0 for objective L.\nHowever, the graph traversal algorithm to compute gradient continues, and\ncomputes dt/dx = 1, de/dx = -Infinity, which finally gives dL/dx = dL/dt\n\ndt/dx + dL/de * de/dx = dL/dt * 1 + 0 * Inf = dL/dt + NaN = NaN.\n\nThe issue is the 0 * Inf = NaN term, and the fact that this computation\ndoesn't happen in the select node (happens in the divide node in this case).\nI spent a few minutes thinking of possible solutions to this, and none of\nthem are pretty or seem worth implementing.\nOne option is to pass a mask of hard zero gradients around during gradient\ncomputation. In this case, after each node computes it's gradients, the\nhard zero gradients can be reset to 0. This isn't a very appealing option.\nThe other option is to significantly change the gradient propagation\nalgorithm. Rather than computing gradient of objective with respect to op\ninput, compute gradient of op output with respect to op input for all ops\nfirst. Then do a multiplicative accumulate pass. This has the downside of\nsignificant framework rewrite, and is unworkable because gradient of op\noutput wrt op input can be high order tensors (gradient of matrix multiply\nwrt inputs is very large).\nIn summary, there doesn't seem to be a good way to make Tensorflow compute\nthe select gradient correctly. I wrote this largely for my own\nunderstanding of why \"[NaN gradient] is inevitable\", hopefully this can be\nuseful to someone else who happens across the issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#2540 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABaHE-b1wrlAAYk-694eHsOS3GT9xdXks5qt_RsgaJpZM4Io0ms\n.", "body": "BTW, it's not just the issue with select, code below also gives NaN.\n\nx = tf.placeholder(tf.float32)\ny = tf.exp(x)\nz = tf.exp(-y)\ngrad = tf.gradients(z,[x])[0]\nprint sess.run(grad, feed_dict={x: 1e10})\n\nThe underlying cause is that  chain rule gives f' \\* g' as derivative of\nf(g(x)) so with because 1e10 overflows we get infinity_0 in both cases.\nOne could argue that limiting arguments that justify inf_0=NaN should not\napply in case of tf.select, and substitute special gradient for tf.select\nusing gradient_override_map during runtime\n\nOn Mon, Sep 26, 2016 at 9:35 AM, Eric Martin notifications@github.com\nwrote:\n\n> After reading some code and docs, I think I see the difficulty with select\n> gradient.\n> \n> The select gradient is here: https://github.com/tensorflow/\n> tensorflow/blob/4addf4b5806cd731949c6582a83f58\n> 24599cd1ef/tensorflow/python/ops/math_grad.py#L688\n> \n> Gradients are only taken on the output of an op with respect to the input\n> (and then multiplied by the accumulated gradient from objective to output\n> of op), so in my example above the select gradient computes dw/dt=1 and\n> dw/de=0. This gives dL/dt=dL/dw and dL/de=0 for objective L.\n> \n> However, the graph traversal algorithm to compute gradient continues, and\n> computes dt/dx = 1, de/dx = -Infinity, which finally gives dL/dx = dL/dt\n> - dt/dx + dL/de \\* de/dx = dL/dt \\* 1 + 0 \\* Inf = dL/dt + NaN = NaN.\n> \n> The issue is the 0 \\* Inf = NaN term, and the fact that this computation\n> doesn't happen in the select node (happens in the divide node in this case).\n> \n> I spent a few minutes thinking of possible solutions to this, and none of\n> them are pretty or seem worth implementing.\n> One option is to pass a mask of hard zero gradients around during gradient\n> computation. In this case, after each node computes it's gradients, the\n> hard zero gradients can be reset to 0. This isn't a very appealing option.\n> The other option is to significantly change the gradient propagation\n> algorithm. Rather than computing gradient of objective with respect to op\n> input, compute gradient of op output with respect to op input for all ops\n> first. Then do a multiplicative accumulate pass. This has the downside of\n> significant framework rewrite, and is unworkable because gradient of op\n> output wrt op input can be high order tensors (gradient of matrix multiply\n> wrt inputs is very large).\n> In summary, there doesn't seem to be a good way to make Tensorflow compute\n> the select gradient correctly. I wrote this largely for my own\n> understanding of why \"[NaN gradient] is inevitable\", hopefully this can be\n> useful to someone else who happens across the issue.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2540#issuecomment-249623937,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHE-b1wrlAAYk-694eHsOS3GT9xdXks5qt_RsgaJpZM4Io0ms\n> .\n"}