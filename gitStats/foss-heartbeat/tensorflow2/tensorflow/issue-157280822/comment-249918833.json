{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/249918833", "html_url": "https://github.com/tensorflow/tensorflow/issues/2540#issuecomment-249918833", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2540", "id": 249918833, "node_id": "MDEyOklzc3VlQ29tbWVudDI0OTkxODgzMw==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-27T16:27:00Z", "updated_at": "2016-09-27T16:27:00Z", "author_association": "NONE", "body_html": "<p>The exponent example is a little different because a intermediate computation (<code>y</code> in your example) is infinity, so it's not too surprising that the gradient is also problematic. The <code>epsilon * Inf</code> is fundamental to the gradient computation here, while it's not obviously so in the <code>select</code> case.</p>\n<p>I completely agree that limiting arguments for <code>0 * Inf = NaN</code> do not apply for select, because the gradient in select is set to 0 to specify that the gradient with respect to the entire non-selected subgraph is exactly 0.<br>\nI wasn't familiar with <code>gradient_override_map</code>, but I just read the documentation and I don't see how it can fix the problem as I believe the problem comes from the combination of backprop (multiplicatively accumulating gradients) and that <code>0 * x != 0</code> for all <code>x</code>. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> , did you have a specific idea about how to fix <code>select</code> with <code>gradient_override_map</code>?</p>", "body_text": "The exponent example is a little different because a intermediate computation (y in your example) is infinity, so it's not too surprising that the gradient is also problematic. The epsilon * Inf is fundamental to the gradient computation here, while it's not obviously so in the select case.\nI completely agree that limiting arguments for 0 * Inf = NaN do not apply for select, because the gradient in select is set to 0 to specify that the gradient with respect to the entire non-selected subgraph is exactly 0.\nI wasn't familiar with gradient_override_map, but I just read the documentation and I don't see how it can fix the problem as I believe the problem comes from the combination of backprop (multiplicatively accumulating gradients) and that 0 * x != 0 for all x. @yaroslavvb , did you have a specific idea about how to fix select with gradient_override_map?", "body": "The exponent example is a little different because a intermediate computation (`y` in your example) is infinity, so it's not too surprising that the gradient is also problematic. The `epsilon * Inf` is fundamental to the gradient computation here, while it's not obviously so in the `select` case.\n\nI completely agree that limiting arguments for `0 * Inf = NaN` do not apply for select, because the gradient in select is set to 0 to specify that the gradient with respect to the entire non-selected subgraph is exactly 0.\nI wasn't familiar with `gradient_override_map`, but I just read the documentation and I don't see how it can fix the problem as I believe the problem comes from the combination of backprop (multiplicatively accumulating gradients) and that `0 * x != 0` for all `x`. @yaroslavvb , did you have a specific idea about how to fix `select` with `gradient_override_map`?\n"}