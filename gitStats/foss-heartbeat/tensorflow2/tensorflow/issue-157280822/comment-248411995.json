{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/248411995", "html_url": "https://github.com/tensorflow/tensorflow/issues/2540#issuecomment-248411995", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2540", "id": 248411995, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODQxMTk5NQ==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-20T19:43:50Z", "updated_at": "2016-09-20T19:57:19Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> It's not clear to me why the propagation of NaN is inevitable, and I think this issue should be re-opened.</p>\n<p>If <code>w = tf.select(c, t, e)</code>, then <code>dw/dx = select(c, dt/dx, de/dx)</code></p>\n<p>Tensorflow 0.9 (and likely 0.10, but have not tested) do not compute this correctly. For instance,<br>\n<code>d/dx[ select(True, t, e) ] != dt/dx</code> for some cases.</p>\n<p>See the following example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nx <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.0</span>)\n\nt <span class=\"pl-k\">=</span> x\ne <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>. <span class=\"pl-k\">/</span> x\n\nw <span class=\"pl-k\">=</span> tf.select(<span class=\"pl-c1\">True</span>, t, e)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>w:<span class=\"pl-pds\">'</span></span>, sess.run(w)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dw/dx:<span class=\"pl-pds\">'</span></span>, sess.run(tf.gradients(w, x)[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dt/dx:<span class=\"pl-pds\">'</span></span>, sess.run(tf.gradients(t, x)[<span class=\"pl-c1\">0</span>])</pre></div>\n<p>outputs (with Tensorflow 0.9)</p>\n<pre><code>w: 0.0\ndw/dx: nan\ndt/dx: 1.0\n</code></pre>\n<p>These NaN's cause NaN gradients, which causes NaN weights and unnecessarily breaks training a net.</p>", "body_text": "@girving It's not clear to me why the propagation of NaN is inevitable, and I think this issue should be re-opened.\nIf w = tf.select(c, t, e), then dw/dx = select(c, dt/dx, de/dx)\nTensorflow 0.9 (and likely 0.10, but have not tested) do not compute this correctly. For instance,\nd/dx[ select(True, t, e) ] != dt/dx for some cases.\nSee the following example:\nimport tensorflow as tf\n\nx = tf.constant(0.0)\n\nt = x\ne = 1. / x\n\nw = tf.select(True, t, e)\n\nwith tf.Session() as sess:\n    print 'w:', sess.run(w)\n    print 'dw/dx:', sess.run(tf.gradients(w, x)[0])\n    print 'dt/dx:', sess.run(tf.gradients(t, x)[0])\noutputs (with Tensorflow 0.9)\nw: 0.0\ndw/dx: nan\ndt/dx: 1.0\n\nThese NaN's cause NaN gradients, which causes NaN weights and unnecessarily breaks training a net.", "body": "@girving It's not clear to me why the propagation of NaN is inevitable, and I think this issue should be re-opened.\n\nIf `w = tf.select(c, t, e)`, then `dw/dx = select(c, dt/dx, de/dx)` \n\nTensorflow 0.9 (and likely 0.10, but have not tested) do not compute this correctly. For instance,\n`d/dx[ select(True, t, e) ] != dt/dx` for some cases.\n\nSee the following example:\n\n``` python\nimport tensorflow as tf\n\nx = tf.constant(0.0)\n\nt = x\ne = 1. / x\n\nw = tf.select(True, t, e)\n\nwith tf.Session() as sess:\n    print 'w:', sess.run(w)\n    print 'dw/dx:', sess.run(tf.gradients(w, x)[0])\n    print 'dt/dx:', sess.run(tf.gradients(t, x)[0])\n```\n\noutputs (with Tensorflow 0.9)\n\n```\nw: 0.0\ndw/dx: nan\ndt/dx: 1.0\n```\n\nThese NaN's cause NaN gradients, which causes NaN weights and unnecessarily breaks training a net.\n"}