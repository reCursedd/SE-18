{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11379", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11379/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11379/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11379/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11379", "id": 241457616, "node_id": "MDU6SXNzdWUyNDE0NTc2MTY=", "number": 11379, "title": "TFrecord reading time very high ", "user": {"login": "sujoyp", "id": 8893859, "node_id": "MDQ6VXNlcjg4OTM4NTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/8893859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sujoyp", "html_url": "https://github.com/sujoyp", "followers_url": "https://api.github.com/users/sujoyp/followers", "following_url": "https://api.github.com/users/sujoyp/following{/other_user}", "gists_url": "https://api.github.com/users/sujoyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/sujoyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sujoyp/subscriptions", "organizations_url": "https://api.github.com/users/sujoyp/orgs", "repos_url": "https://api.github.com/users/sujoyp/repos", "events_url": "https://api.github.com/users/sujoyp/events{/privacy}", "received_events_url": "https://api.github.com/users/sujoyp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-08T14:59:30Z", "updated_at": "2017-07-13T00:41:09Z", "closed_at": "2017-07-13T00:41:09Z", "author_association": "NONE", "body_html": "<p>#I have divided a dataset into 10 tfrecords files and I want to read 100 data points from each to create a batch of 10 sequence of 100 data points. I use the following function to do that. The data loading time from the tfrecords start off slow and then reaches to around 0.65s and after 100-200 sess.run calls it increases to around 10s. Can you please point out any mistake or suggestion which might help to reduce the read time ? Also, the behaviour I mentioned becomes more erratic sometimes.</p>\n<pre><code>def get_data(mini_batch_size):\n  data = []\n  for i in range(mini_batch_size):\n    filename_queue = tf.train.string_input_producer([data_path + 'Features' + str(i) + '.tfrecords'])\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    batch_serialized_example = tf.train.batch([serialized_example], batch_size=step_size, num_threads=8, capacity=step_size)\n    features = tf.parse_example(batch_serialized_example,features={'feature_raw': tf.VarLenFeature(dtype=tf.float32)})\n    feature = features['feature_raw'].values\n    feature = tf.reshape(feature,[step_size, ConvLSTM.H, ConvLSTM.W, ConvLSTM.Di])\n    data.append(feature)\n  return tf.stack(data)\n</code></pre>", "body_text": "#I have divided a dataset into 10 tfrecords files and I want to read 100 data points from each to create a batch of 10 sequence of 100 data points. I use the following function to do that. The data loading time from the tfrecords start off slow and then reaches to around 0.65s and after 100-200 sess.run calls it increases to around 10s. Can you please point out any mistake or suggestion which might help to reduce the read time ? Also, the behaviour I mentioned becomes more erratic sometimes.\ndef get_data(mini_batch_size):\n  data = []\n  for i in range(mini_batch_size):\n    filename_queue = tf.train.string_input_producer([data_path + 'Features' + str(i) + '.tfrecords'])\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    batch_serialized_example = tf.train.batch([serialized_example], batch_size=step_size, num_threads=8, capacity=step_size)\n    features = tf.parse_example(batch_serialized_example,features={'feature_raw': tf.VarLenFeature(dtype=tf.float32)})\n    feature = features['feature_raw'].values\n    feature = tf.reshape(feature,[step_size, ConvLSTM.H, ConvLSTM.W, ConvLSTM.Di])\n    data.append(feature)\n  return tf.stack(data)", "body": "#I have divided a dataset into 10 tfrecords files and I want to read 100 data points from each to create a batch of 10 sequence of 100 data points. I use the following function to do that. The data loading time from the tfrecords start off slow and then reaches to around 0.65s and after 100-200 sess.run calls it increases to around 10s. Can you please point out any mistake or suggestion which might help to reduce the read time ? Also, the behaviour I mentioned becomes more erratic sometimes.\r\n\r\n    def get_data(mini_batch_size):\r\n      data = []\r\n      for i in range(mini_batch_size):\r\n        filename_queue = tf.train.string_input_producer([data_path + 'Features' + str(i) + '.tfrecords'])\r\n        reader = tf.TFRecordReader()\r\n        _, serialized_example = reader.read(filename_queue)\r\n        batch_serialized_example = tf.train.batch([serialized_example], batch_size=step_size, num_threads=8, capacity=step_size)\r\n        features = tf.parse_example(batch_serialized_example,features={'feature_raw': tf.VarLenFeature(dtype=tf.float32)})\r\n        feature = features['feature_raw'].values\r\n        feature = tf.reshape(feature,[step_size, ConvLSTM.H, ConvLSTM.W, ConvLSTM.Di])\r\n        data.append(feature)\r\n      return tf.stack(data)\r\n"}