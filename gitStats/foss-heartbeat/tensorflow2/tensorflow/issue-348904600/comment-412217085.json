{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/412217085", "html_url": "https://github.com/tensorflow/tensorflow/issues/21492#issuecomment-412217085", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21492", "id": 412217085, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjIxNzA4NQ==", "user": {"login": "xiejw", "id": 1184671, "node_id": "MDQ6VXNlcjExODQ2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1184671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiejw", "html_url": "https://github.com/xiejw", "followers_url": "https://api.github.com/users/xiejw/followers", "following_url": "https://api.github.com/users/xiejw/following{/other_user}", "gists_url": "https://api.github.com/users/xiejw/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiejw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiejw/subscriptions", "organizations_url": "https://api.github.com/users/xiejw/orgs", "repos_url": "https://api.github.com/users/xiejw/repos", "events_url": "https://api.github.com/users/xiejw/events{/privacy}", "received_events_url": "https://api.github.com/users/xiejw/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-10T21:57:00Z", "updated_at": "2018-08-10T21:57:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Assume device_filter is not set. The precise sequence can be described as follows:</p>\n<ol>\n<li>\n<p>start_std_server. This opens the port and listens the grpc in background thread. All workers must do this first. Otherwise it might delay other workers to train the model.</p>\n</li>\n<li>\n<p>(optional) time.sleep for delay starting. Note that at this time the grpc port is open already.</p>\n</li>\n<li>\n<p>Estimator.train. This creates the graph and launches a new tf.Session to train the model. The session will try to talk to each other device (in this case other workers) and then run the TF graph. This is the part it will try to block until all devices are found. The code is <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master.cc#L215\">here</a>.</p>\n</li>\n</ol>\n<p>Say there is a worker in the time.sleep for 100000 secs (basically forever). As the port has been opened in step 1, so other workers can start train (step 3) without waiting.</p>\n<p>With device filter, only step 3 is slightly different where each worker only tries to find the devices in the list rather than all devices.</p>", "body_text": "Assume device_filter is not set. The precise sequence can be described as follows:\n\n\nstart_std_server. This opens the port and listens the grpc in background thread. All workers must do this first. Otherwise it might delay other workers to train the model.\n\n\n(optional) time.sleep for delay starting. Note that at this time the grpc port is open already.\n\n\nEstimator.train. This creates the graph and launches a new tf.Session to train the model. The session will try to talk to each other device (in this case other workers) and then run the TF graph. This is the part it will try to block until all devices are found. The code is here.\n\n\nSay there is a worker in the time.sleep for 100000 secs (basically forever). As the port has been opened in step 1, so other workers can start train (step 3) without waiting.\nWith device filter, only step 3 is slightly different where each worker only tries to find the devices in the list rather than all devices.", "body": "Assume device_filter is not set. The precise sequence can be described as follows:\r\n\r\n1. start_std_server. This opens the port and listens the grpc in background thread. All workers must do this first. Otherwise it might delay other workers to train the model.\r\n\r\n2. (optional) time.sleep for delay starting. Note that at this time the grpc port is open already.\r\n\r\n3. Estimator.train. This creates the graph and launches a new tf.Session to train the model. The session will try to talk to each other device (in this case other workers) and then run the TF graph. This is the part it will try to block until all devices are found. The code is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master.cc#L215).\r\n\r\nSay there is a worker in the time.sleep for 100000 secs (basically forever). As the port has been opened in step 1, so other workers can start train (step 3) without waiting. \r\n\r\nWith device filter, only step 3 is slightly different where each worker only tries to find the devices in the list rather than all devices. "}