{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18492", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18492/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18492/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18492/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18492", "id": 314093246, "node_id": "MDU6SXNzdWUzMTQwOTMyNDY=", "number": 18492, "title": "Worker is failing with InvalidArgumentError>, \"A session is not created yet\"", "user": {"login": "deepak-2017", "id": 32203263, "node_id": "MDQ6VXNlcjMyMjAzMjYz", "avatar_url": "https://avatars3.githubusercontent.com/u/32203263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepak-2017", "html_url": "https://github.com/deepak-2017", "followers_url": "https://api.github.com/users/deepak-2017/followers", "following_url": "https://api.github.com/users/deepak-2017/following{/other_user}", "gists_url": "https://api.github.com/users/deepak-2017/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepak-2017/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepak-2017/subscriptions", "organizations_url": "https://api.github.com/users/deepak-2017/orgs", "repos_url": "https://api.github.com/users/deepak-2017/repos", "events_url": "https://api.github.com/users/deepak-2017/events{/privacy}", "received_events_url": "https://api.github.com/users/deepak-2017/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-04-13T12:45:23Z", "updated_at": "2018-07-28T03:49:07Z", "closed_at": "2018-07-28T03:49:07Z", "author_association": "NONE", "body_html": "<h2>System information</h2>\n<ul>\n<li>Have I written custom code - Yes</li>\n<li>OS Platform CentOS - 7.2.1511</li>\n<li>TensorFlow installed from - Binary</li>\n<li>TensorFlow version - 1.3.0</li>\n<li>Python version - 2.7</li>\n<li>Bazel version - N/A</li>\n<li>CUDA/cuDNN version - N/A</li>\n<li>GPU model and memory - N/A</li>\n</ul>\n<h2>Source Code</h2>\n<p>Sharing my code snippet below,</p>\n<pre><code>cluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\nserver = tf.train.Server(\n    cluster,\n    job_name=FLAGS.job_name,\n    task_index=FLAGS.task_index)\n   \nif FLAGS.pipeline_id is None:\n    raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\n\n#print('job name '+FLAGS.job_name)\n#print('task index name ' + FLAGS.task_index)\nif FLAGS.job_name == \"ps\":\n    server.join()\nelif FLAGS.job_name == \"worker\":\n    print('only for worker')\n    # if not os.path.exists(FLAGS.train_dir):\n    #    os.mkdir(FLAGS.train_dir)\n    train_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/training'\n    eval_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/eval'\n    print 'Train logs path -- ', train_logs_path\n    print 'Eval logs path -- ', eval_logs_path\n\n    tf.logging.set_verbosity(tf.logging.INFO)  # Set the verbosity to INFO level\n\n    # First create the dataset and load one batch\n\n    ##\n    ## replace flowers with FLAGS.pipeline_id in get_split()\n    ##\n    ##\n    labels_file = FLAGS.tf_records_dir + '/labels.txt'\n\n    dataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\n    print 'num of classes -------------------&gt;', dataset.num_classes\n    images, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\n                                   width=FLAGS.image_resize, is_training=True)\n\n    # Get number of steps to decay\n    num_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\n    num_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\n    decay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\n\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)):\n   \n\n\n\n\n\n\n        #======================= INITIATE TRAINING  =========================\n\n\n\n\n\n        #Create the model inference\n        with slim.arg_scope(inception_v3_arg_scope()):\n            logits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\n\n\n\n        exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\n        #exclude = get_variables_to_exclude()\n        for i in exclude:\n           print \"var to exclude -&gt; \",i\n        variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n\n        #Perform one-hot-encoding of the labels\n        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n\n        #Calculate loss\n        loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n        total_loss = tf.losses.get_total_loss()\n\n        #Create the global step\n        global_step = get_or_create_global_step()\n\n\n        #Define your exponentially decaying learning rate\n        lr = tf.train.exponential_decay(\n            learning_rate = FLAGS.initial_learning_rate,\n            global_step = global_step,\n            decay_steps = decay_steps,\n            decay_rate = FLAGS.learning_rate_decay_factor,\n            staircase = True)\n\n        #Get optimizer as configured by user\n        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n        #optimizer = getOptimizer(learning_rate = lr)\n\n        #Create the train_op.\n        variables_to_train = get_variables_to_train()\n        #for j in variables_to_train:\n          #print \"var to train \",j\n        #vn = tf.trainable_variables()\n        train_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\n\n\n        predictions = tf.argmax(end_points['Predictions'], 1)\n        probabilities = end_points['Predictions']\n        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n        metrics_op = tf.group(accuracy_update, probabilities)\n\n\n        #create all the summaries you need to monitor\n        tf.summary.scalar('losses/Total_Loss', total_loss)\n        tf.summary.scalar('accuracy', accuracy)\n        tf.summary.scalar('learning_rate', lr)\n        my_summary_op = tf.summary.merge_all()\n\n        #Define train step to run training operation\n        def train_step(sess, train_op, global_step):\n\n            start_time = time.time()\n            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n            time_elapsed = time.time() - start_time\n\n\n            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n\n            return total_loss, global_step_count\n\n    #Restore variables from checkpoint\n    saver = tf.train.Saver(variables_to_restore)\n    def restore_fn(sess):\n        return saver.restore(sess, FLAGS.checkpoint_path)\n\n    #Create supervisor\n    writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\n                             summary_writer=writer,\n                             summary_op = my_summary_op, init_fn = restore_fn)\n\n\n    #Run the managed session\n    with sv.prepare_or_wait_for_session(server.target) as sess:\n        #writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\n        for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\n            #Log info at each epoch:\n            if step % num_batches_per_epoch == 0:\n                logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\n                learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n                logging.info('Current Learning Rate: %s', learning_rate_value)\n                logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\n\n\n                logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n                print 'logits: \\n', logits_value\n                print 'Probabilities: \\n', probabilities_value\n                print 'predictions: \\n', predictions_value\n                print 'Labels:\\n:', labels_value\n\n            #Log the summaries every 10 step.\n            if step % FLAGS.steps_update_frequency == 0 and step != 0:\n                loss, gs = train_step(sess, train_op, sv.global_step)\n                summaries = sess.run(my_summary_op)\n                if FLAGS.task_index == 0:\n                    print('Compute Summaries--------------')\n                    sv.summary_computed(sess, summaries)\n                    #writer.add_summary(summaries, gs)\n                    print \"**** SAVE THE MODEL ****\"\n                    print 'Train logs path -- ', train_logs_path\n                    sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\n                    #saver.save(sess, train_logs_path+'/model', global_step=gs)\n                    ##\n\n                checkpoint_path = tf.train.latest_checkpoint(train_logs_path)\n                training_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\n                current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                print 'training_json -- ', training_json\n                print 'sv.save_path', sv.save_path\n                #dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\n\n                ##\n                print('*************** Running eval at step = {0} *********************'.format(step))\n                #cmd = 'python ./eval_image_classifier.py --pipeline_id='+FLAGS.pipeline_id+'  --eval_recording_step='+str(gs) + '  --tf_records_dir='+FLAGS.tf_records_dir + '  --checkpoint_dir='+train_logs_path + '  --eval_log_dir='+eval_logs_path + '  --dataset_name='+FLAGS.pipeline_id + '  --num_classes='+str(FLAGS.num_classes)  + '  --eval_num_epochs='+str(FLAGS.eval_num_epochs) + '  --eval_batch_size='+str(FLAGS.eval_batch_size) + '  --image_resize_method='+FLAGS.image_resize_method + '  --color_ordering='+str(FLAGS.color_ordering) + '  --saturation_contrast_lower_bound='+str(FLAGS.saturation_contrast_lower_bound) + '  --saturation_contrast_upper_bound='+str(FLAGS.saturation_contrast_upper_bound) + '  --brightness_max_delta='+str(FLAGS.brightness_max_delta) + '  --hue_max_delta='+str(FLAGS.hue_max_delta)\n                #p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n                #out, err = p.communicate()\n                print('***************Eval finished for step = {0}*********************'.format(step))\n\n\n            else:\n                loss, _ = train_step(sess, train_op, sv.global_step)\n\n        #We log the final training loss and accuracy\n        logging.info('Final Loss: %s', loss)\n        logging.info('Final Training Accuracy: %s', sess.run(accuracy))\n\n        #Save the model on completing the training\n        logging.info('Finished training! Saving model to disk now.')\n        if FLAGS.task_index == 0:\n          sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n</code></pre>\n<h2>Problem Description</h2>\n<p>I am having three nodes Tensorflow cluster with one parameter server, two workers.<br>\nOn machine A, I am running a parameter server and chief worker.<br>\nOn machine B, I am running 2nd worker.<br>\nMy tensorflow script expects one of the input parameter as the path to the directory containing tfrecord file.</p>\n<h2>Issue 1</h2>\n<p>Parameter server and chief worker are running fine but 2nd worker fails with below error.<br>\nI am running 2nd worker on machine B and input path to the tfrecords directory exists there.But I don't have any clue why it is looking for data under ps device=\"/job:ps/replica:0/task:0/cpu:0\" which is actually running on machine A where this path doesn't exist.</p>\n<pre><code>INFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framework.errors_impl.NotFoundError'&gt;, /home/mapr/mano/slim_data/flowers/slim_data_dir/flowers_train_00002-of-00005.tfrecord\n         [[Node: parallel_read/ReaderReadV2 = ReaderReadV2[_device=\"/job:ps/replica:0/task:0/cpu:0\"](parallel_read/TFRecordReaderV2, parallel_read/filenames)]]\n</code></pre>\n<h2>Issue 2</h2>\n<p>Just for my curiosity, I copied input tfrecord directory to machine A with the same path as on machine B.<br>\nThen the previous error gone but now I am facing different error.<br>\nI also checked no code has run within <strong>sv.prepare_or_wait_for_session(server.target)</strong>  block.<br>\nIt seems to me 2nd worker waits for chief to initialize the session but somehow chief doesn't return session and thus worker fails.</p>\n<pre><code>INFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'&gt;, A session is not created yet....\n</code></pre>\n<p>Plz help me here.</p>", "body_text": "System information\n\nHave I written custom code - Yes\nOS Platform CentOS - 7.2.1511\nTensorFlow installed from - Binary\nTensorFlow version - 1.3.0\nPython version - 2.7\nBazel version - N/A\nCUDA/cuDNN version - N/A\nGPU model and memory - N/A\n\nSource Code\nSharing my code snippet below,\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\nserver = tf.train.Server(\n    cluster,\n    job_name=FLAGS.job_name,\n    task_index=FLAGS.task_index)\n   \nif FLAGS.pipeline_id is None:\n    raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\n\n#print('job name '+FLAGS.job_name)\n#print('task index name ' + FLAGS.task_index)\nif FLAGS.job_name == \"ps\":\n    server.join()\nelif FLAGS.job_name == \"worker\":\n    print('only for worker')\n    # if not os.path.exists(FLAGS.train_dir):\n    #    os.mkdir(FLAGS.train_dir)\n    train_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/training'\n    eval_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/eval'\n    print 'Train logs path -- ', train_logs_path\n    print 'Eval logs path -- ', eval_logs_path\n\n    tf.logging.set_verbosity(tf.logging.INFO)  # Set the verbosity to INFO level\n\n    # First create the dataset and load one batch\n\n    ##\n    ## replace flowers with FLAGS.pipeline_id in get_split()\n    ##\n    ##\n    labels_file = FLAGS.tf_records_dir + '/labels.txt'\n\n    dataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\n    print 'num of classes ------------------->', dataset.num_classes\n    images, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\n                                   width=FLAGS.image_resize, is_training=True)\n\n    # Get number of steps to decay\n    num_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\n    num_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\n    decay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\n\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)):\n   \n\n\n\n\n\n\n        #======================= INITIATE TRAINING  =========================\n\n\n\n\n\n        #Create the model inference\n        with slim.arg_scope(inception_v3_arg_scope()):\n            logits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\n\n\n\n        exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\n        #exclude = get_variables_to_exclude()\n        for i in exclude:\n           print \"var to exclude -> \",i\n        variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n\n        #Perform one-hot-encoding of the labels\n        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n\n        #Calculate loss\n        loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n        total_loss = tf.losses.get_total_loss()\n\n        #Create the global step\n        global_step = get_or_create_global_step()\n\n\n        #Define your exponentially decaying learning rate\n        lr = tf.train.exponential_decay(\n            learning_rate = FLAGS.initial_learning_rate,\n            global_step = global_step,\n            decay_steps = decay_steps,\n            decay_rate = FLAGS.learning_rate_decay_factor,\n            staircase = True)\n\n        #Get optimizer as configured by user\n        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n        #optimizer = getOptimizer(learning_rate = lr)\n\n        #Create the train_op.\n        variables_to_train = get_variables_to_train()\n        #for j in variables_to_train:\n          #print \"var to train \",j\n        #vn = tf.trainable_variables()\n        train_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\n\n\n        predictions = tf.argmax(end_points['Predictions'], 1)\n        probabilities = end_points['Predictions']\n        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n        metrics_op = tf.group(accuracy_update, probabilities)\n\n\n        #create all the summaries you need to monitor\n        tf.summary.scalar('losses/Total_Loss', total_loss)\n        tf.summary.scalar('accuracy', accuracy)\n        tf.summary.scalar('learning_rate', lr)\n        my_summary_op = tf.summary.merge_all()\n\n        #Define train step to run training operation\n        def train_step(sess, train_op, global_step):\n\n            start_time = time.time()\n            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n            time_elapsed = time.time() - start_time\n\n\n            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n\n            return total_loss, global_step_count\n\n    #Restore variables from checkpoint\n    saver = tf.train.Saver(variables_to_restore)\n    def restore_fn(sess):\n        return saver.restore(sess, FLAGS.checkpoint_path)\n\n    #Create supervisor\n    writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\n                             summary_writer=writer,\n                             summary_op = my_summary_op, init_fn = restore_fn)\n\n\n    #Run the managed session\n    with sv.prepare_or_wait_for_session(server.target) as sess:\n        #writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\n        for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\n            #Log info at each epoch:\n            if step % num_batches_per_epoch == 0:\n                logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\n                learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n                logging.info('Current Learning Rate: %s', learning_rate_value)\n                logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\n\n\n                logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n                print 'logits: \\n', logits_value\n                print 'Probabilities: \\n', probabilities_value\n                print 'predictions: \\n', predictions_value\n                print 'Labels:\\n:', labels_value\n\n            #Log the summaries every 10 step.\n            if step % FLAGS.steps_update_frequency == 0 and step != 0:\n                loss, gs = train_step(sess, train_op, sv.global_step)\n                summaries = sess.run(my_summary_op)\n                if FLAGS.task_index == 0:\n                    print('Compute Summaries--------------')\n                    sv.summary_computed(sess, summaries)\n                    #writer.add_summary(summaries, gs)\n                    print \"**** SAVE THE MODEL ****\"\n                    print 'Train logs path -- ', train_logs_path\n                    sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\n                    #saver.save(sess, train_logs_path+'/model', global_step=gs)\n                    ##\n\n                checkpoint_path = tf.train.latest_checkpoint(train_logs_path)\n                training_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\n                current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                print 'training_json -- ', training_json\n                print 'sv.save_path', sv.save_path\n                #dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\n\n                ##\n                print('*************** Running eval at step = {0} *********************'.format(step))\n                #cmd = 'python ./eval_image_classifier.py --pipeline_id='+FLAGS.pipeline_id+'  --eval_recording_step='+str(gs) + '  --tf_records_dir='+FLAGS.tf_records_dir + '  --checkpoint_dir='+train_logs_path + '  --eval_log_dir='+eval_logs_path + '  --dataset_name='+FLAGS.pipeline_id + '  --num_classes='+str(FLAGS.num_classes)  + '  --eval_num_epochs='+str(FLAGS.eval_num_epochs) + '  --eval_batch_size='+str(FLAGS.eval_batch_size) + '  --image_resize_method='+FLAGS.image_resize_method + '  --color_ordering='+str(FLAGS.color_ordering) + '  --saturation_contrast_lower_bound='+str(FLAGS.saturation_contrast_lower_bound) + '  --saturation_contrast_upper_bound='+str(FLAGS.saturation_contrast_upper_bound) + '  --brightness_max_delta='+str(FLAGS.brightness_max_delta) + '  --hue_max_delta='+str(FLAGS.hue_max_delta)\n                #p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n                #out, err = p.communicate()\n                print('***************Eval finished for step = {0}*********************'.format(step))\n\n\n            else:\n                loss, _ = train_step(sess, train_op, sv.global_step)\n\n        #We log the final training loss and accuracy\n        logging.info('Final Loss: %s', loss)\n        logging.info('Final Training Accuracy: %s', sess.run(accuracy))\n\n        #Save the model on completing the training\n        logging.info('Finished training! Saving model to disk now.')\n        if FLAGS.task_index == 0:\n          sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n\nProblem Description\nI am having three nodes Tensorflow cluster with one parameter server, two workers.\nOn machine A, I am running a parameter server and chief worker.\nOn machine B, I am running 2nd worker.\nMy tensorflow script expects one of the input parameter as the path to the directory containing tfrecord file.\nIssue 1\nParameter server and chief worker are running fine but 2nd worker fails with below error.\nI am running 2nd worker on machine B and input path to the tfrecords directory exists there.But I don't have any clue why it is looking for data under ps device=\"/job:ps/replica:0/task:0/cpu:0\" which is actually running on machine A where this path doesn't exist.\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, /home/mapr/mano/slim_data/flowers/slim_data_dir/flowers_train_00002-of-00005.tfrecord\n         [[Node: parallel_read/ReaderReadV2 = ReaderReadV2[_device=\"/job:ps/replica:0/task:0/cpu:0\"](parallel_read/TFRecordReaderV2, parallel_read/filenames)]]\n\nIssue 2\nJust for my curiosity, I copied input tfrecord directory to machine A with the same path as on machine B.\nThen the previous error gone but now I am facing different error.\nI also checked no code has run within sv.prepare_or_wait_for_session(server.target)  block.\nIt seems to me 2nd worker waits for chief to initialize the session but somehow chief doesn't return session and thus worker fails.\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, A session is not created yet....\n\nPlz help me here.", "body": "System information\r\n------------------\r\n\r\n - Have I written custom code - Yes\r\n - OS Platform CentOS - 7.2.1511\r\n - TensorFlow installed from - Binary\r\n - TensorFlow version - 1.3.0\r\n - Python version - 2.7\r\n - Bazel version - N/A\r\n - CUDA/cuDNN version - N/A\r\n - GPU model and memory - N/A\r\n\r\nSource Code\r\n-----------\r\nSharing my code snippet below,\r\n\r\n    cluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n    \r\n    server = tf.train.Server(\r\n        cluster,\r\n        job_name=FLAGS.job_name,\r\n        task_index=FLAGS.task_index)\r\n       \r\n    if FLAGS.pipeline_id is None:\r\n        raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\r\n    \r\n    #print('job name '+FLAGS.job_name)\r\n    #print('task index name ' + FLAGS.task_index)\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    elif FLAGS.job_name == \"worker\":\r\n        print('only for worker')\r\n        # if not os.path.exists(FLAGS.train_dir):\r\n        #    os.mkdir(FLAGS.train_dir)\r\n        train_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/training'\r\n        eval_logs_path = FLAGS.logs_dir + '/' + FLAGS.pipeline_id + '/eval'\r\n        print 'Train logs path -- ', train_logs_path\r\n        print 'Eval logs path -- ', eval_logs_path\r\n    \r\n        tf.logging.set_verbosity(tf.logging.INFO)  # Set the verbosity to INFO level\r\n    \r\n        # First create the dataset and load one batch\r\n    \r\n        ##\r\n        ## replace flowers with FLAGS.pipeline_id in get_split()\r\n        ##\r\n        ##\r\n        labels_file = FLAGS.tf_records_dir + '/labels.txt'\r\n    \r\n        dataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\r\n        print 'num of classes ------------------->', dataset.num_classes\r\n        images, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\r\n                                       width=FLAGS.image_resize, is_training=True)\r\n    \r\n        # Get number of steps to decay\r\n        num_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\r\n        num_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\r\n        decay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\r\n    \r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                cluster=cluster)):\r\n       \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n            #======================= INITIATE TRAINING  =========================\r\n    \r\n    \r\n    \r\n    \r\n    \r\n            #Create the model inference\r\n            with slim.arg_scope(inception_v3_arg_scope()):\r\n                logits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\r\n    \r\n    \r\n    \r\n            exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\r\n            #exclude = get_variables_to_exclude()\r\n            for i in exclude:\r\n               print \"var to exclude -> \",i\r\n            variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\r\n    \r\n            #Perform one-hot-encoding of the labels\r\n            one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\r\n    \r\n            #Calculate loss\r\n            loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\r\n            total_loss = tf.losses.get_total_loss()\r\n    \r\n            #Create the global step\r\n            global_step = get_or_create_global_step()\r\n    \r\n    \r\n            #Define your exponentially decaying learning rate\r\n            lr = tf.train.exponential_decay(\r\n                learning_rate = FLAGS.initial_learning_rate,\r\n                global_step = global_step,\r\n                decay_steps = decay_steps,\r\n                decay_rate = FLAGS.learning_rate_decay_factor,\r\n                staircase = True)\r\n    \r\n            #Get optimizer as configured by user\r\n            optimizer = tf.train.AdamOptimizer(learning_rate = lr)\r\n            #optimizer = getOptimizer(learning_rate = lr)\r\n    \r\n            #Create the train_op.\r\n            variables_to_train = get_variables_to_train()\r\n            #for j in variables_to_train:\r\n              #print \"var to train \",j\r\n            #vn = tf.trainable_variables()\r\n            train_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\r\n    \r\n    \r\n            predictions = tf.argmax(end_points['Predictions'], 1)\r\n            probabilities = end_points['Predictions']\r\n            accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\r\n            metrics_op = tf.group(accuracy_update, probabilities)\r\n    \r\n    \r\n            #create all the summaries you need to monitor\r\n            tf.summary.scalar('losses/Total_Loss', total_loss)\r\n            tf.summary.scalar('accuracy', accuracy)\r\n            tf.summary.scalar('learning_rate', lr)\r\n            my_summary_op = tf.summary.merge_all()\r\n    \r\n            #Define train step to run training operation\r\n            def train_step(sess, train_op, global_step):\r\n    \r\n                start_time = time.time()\r\n                total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\r\n                time_elapsed = time.time() - start_time\r\n    \r\n    \r\n                logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\r\n    \r\n                return total_loss, global_step_count\r\n    \r\n        #Restore variables from checkpoint\r\n        saver = tf.train.Saver(variables_to_restore)\r\n        def restore_fn(sess):\r\n            return saver.restore(sess, FLAGS.checkpoint_path)\r\n    \r\n        #Create supervisor\r\n        writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\r\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\r\n                                 summary_writer=writer,\r\n                                 summary_op = my_summary_op, init_fn = restore_fn)\r\n    \r\n    \r\n        #Run the managed session\r\n        with sv.prepare_or_wait_for_session(server.target) as sess:\r\n            #writer = tf.summary.FileWriter(train_logs_path, graph=tf.get_default_graph())\r\n            for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\r\n                #Log info at each epoch:\r\n                if step % num_batches_per_epoch == 0:\r\n                    logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\r\n                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])\r\n                    logging.info('Current Learning Rate: %s', learning_rate_value)\r\n                    logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\r\n    \r\n    \r\n                    logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\r\n                    print 'logits: \\n', logits_value\r\n                    print 'Probabilities: \\n', probabilities_value\r\n                    print 'predictions: \\n', predictions_value\r\n                    print 'Labels:\\n:', labels_value\r\n    \r\n                #Log the summaries every 10 step.\r\n                if step % FLAGS.steps_update_frequency == 0 and step != 0:\r\n                    loss, gs = train_step(sess, train_op, sv.global_step)\r\n                    summaries = sess.run(my_summary_op)\r\n                    if FLAGS.task_index == 0:\r\n                        print('Compute Summaries--------------')\r\n                        sv.summary_computed(sess, summaries)\r\n                        #writer.add_summary(summaries, gs)\r\n                        print \"**** SAVE THE MODEL ****\"\r\n                        print 'Train logs path -- ', train_logs_path\r\n                        sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\r\n                        #saver.save(sess, train_logs_path+'/model', global_step=gs)\r\n                        ##\r\n    \r\n                    checkpoint_path = tf.train.latest_checkpoint(train_logs_path)\r\n                    training_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\r\n                    current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\r\n                    print 'training_json -- ', training_json\r\n                    print 'sv.save_path', sv.save_path\r\n                    #dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\r\n    \r\n                    ##\r\n                    print('*************** Running eval at step = {0} *********************'.format(step))\r\n                    #cmd = 'python ./eval_image_classifier.py --pipeline_id='+FLAGS.pipeline_id+'  --eval_recording_step='+str(gs) + '  --tf_records_dir='+FLAGS.tf_records_dir + '  --checkpoint_dir='+train_logs_path + '  --eval_log_dir='+eval_logs_path + '  --dataset_name='+FLAGS.pipeline_id + '  --num_classes='+str(FLAGS.num_classes)  + '  --eval_num_epochs='+str(FLAGS.eval_num_epochs) + '  --eval_batch_size='+str(FLAGS.eval_batch_size) + '  --image_resize_method='+FLAGS.image_resize_method + '  --color_ordering='+str(FLAGS.color_ordering) + '  --saturation_contrast_lower_bound='+str(FLAGS.saturation_contrast_lower_bound) + '  --saturation_contrast_upper_bound='+str(FLAGS.saturation_contrast_upper_bound) + '  --brightness_max_delta='+str(FLAGS.brightness_max_delta) + '  --hue_max_delta='+str(FLAGS.hue_max_delta)\r\n                    #p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\r\n                    #out, err = p.communicate()\r\n                    print('***************Eval finished for step = {0}*********************'.format(step))\r\n    \r\n    \r\n                else:\r\n                    loss, _ = train_step(sess, train_op, sv.global_step)\r\n    \r\n            #We log the final training loss and accuracy\r\n            logging.info('Final Loss: %s', loss)\r\n            logging.info('Final Training Accuracy: %s', sess.run(accuracy))\r\n    \r\n            #Save the model on completing the training\r\n            logging.info('Finished training! Saving model to disk now.')\r\n            if FLAGS.task_index == 0:\r\n              sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\r\n\r\nProblem Description\r\n-------------------\r\n\r\nI am having three nodes Tensorflow cluster with one parameter server, two workers.\r\nOn machine A, I am running a parameter server and chief worker.\r\nOn machine B, I am running 2nd worker.\r\nMy tensorflow script expects one of the input parameter as the path to the directory containing tfrecord file.\r\n\r\nIssue 1\r\n-------\r\n\r\nParameter server and chief worker are running fine but 2nd worker fails with below error.\r\nI am running 2nd worker on machine B and input path to the tfrecords directory exists there.But I don't have any clue why it is looking for data under ps device=\"/job:ps/replica:0/task:0/cpu:0\" which is actually running on machine A where this path doesn't exist.\r\n\r\n    \r\n    INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, /home/mapr/mano/slim_data/flowers/slim_data_dir/flowers_train_00002-of-00005.tfrecord\r\n             [[Node: parallel_read/ReaderReadV2 = ReaderReadV2[_device=\"/job:ps/replica:0/task:0/cpu:0\"](parallel_read/TFRecordReaderV2, parallel_read/filenames)]]\r\n    \r\n\r\nIssue 2\r\n-------\r\n\r\nJust for my curiosity, I copied input tfrecord directory to machine A with the same path as on machine B.\r\nThen the previous error gone but now I am facing different error.\r\nI also checked no code has run within **sv.prepare_or_wait_for_session(server.target)**  block.\r\nIt seems to me 2nd worker waits for chief to initialize the session but somehow chief doesn't return session and thus worker fails.\r\n\r\n    INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, A session is not created yet....\r\n\r\nPlz help me here."}