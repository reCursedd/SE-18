{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21523", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21523/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21523/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21523/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21523", "id": 349239749, "node_id": "MDU6SXNzdWUzNDkyMzk3NDk=", "number": 21523, "title": "SyncReplicaOptimizer prints global step warning unnecessarily", "user": {"login": "haidark", "id": 466428, "node_id": "MDQ6VXNlcjQ2NjQyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/466428?v=4", "gravatar_id": "", "url": "https://api.github.com/users/haidark", "html_url": "https://github.com/haidark", "followers_url": "https://api.github.com/users/haidark/followers", "following_url": "https://api.github.com/users/haidark/following{/other_user}", "gists_url": "https://api.github.com/users/haidark/gists{/gist_id}", "starred_url": "https://api.github.com/users/haidark/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/haidark/subscriptions", "organizations_url": "https://api.github.com/users/haidark/orgs", "repos_url": "https://api.github.com/users/haidark/repos", "events_url": "https://api.github.com/users/haidark/events{/privacy}", "received_events_url": "https://api.github.com/users/haidark/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-09T18:28:12Z", "updated_at": "2018-11-14T19:25:05Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: NA</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.0</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>:  cuda-9.0/ cuDNN 7</li>\n<li><strong>GPU model and memory</strong>: TITAN X 12GB</li>\n<li><strong>Exact command to reproduce</strong>:  Run the code below to produce the warnings</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>This is less a bug and more a suggested improvement. The setup illustrated in the code above is common for reinforcement learning algorithms where the workers are used to collect experience by acting in an environment according to the current policy in a distributed fashion and then synchronizing to update the parameters of the shared networks (stored on the parameter server). There are two ops that are run (multiple times) in a single iteration of the training loop. One is an <code>act</code> op and the other is the <code>update</code> op. The <code>act</code> op has nothing to do with the optimizer while the <code>update</code> op does. As such, running the <code>act</code> op should not affect the <code>global_step</code> while the <code>update</code> op should increment <code>global_step</code>. The problem, in my estimation, is that tensorflow does not recognize this difference and complains (by printing a warning) that <code>global_step</code> is not incremented every time the <code>act</code> op is run.</p>\n<p>This is extremely annoying as thousands of warnings are printed and useful information is lost in the torrent of text. Turning off all warning is the workaround I am using right now, but this is clearly not a long-term solution.</p>\n<p>The exact warning is:</p>\n<pre><code>\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 835 vs previous value: 835. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n</code></pre>\n<h3>Source code / logs</h3>\n<pre><code>import os\nimport multiprocessing as mp\nimport tensorflow as tf\nimport numpy as np\n\nnum_workers=4\n\ndef model(images):\n    net = tf.layers.dense(images, 500, activation=tf.nn.relu)\n    net = tf.layers.dense(net, 500, activation=tf.nn.relu)\n    net = tf.layers.dense(net, 10, activation=None)\n    return net\n\ndef train_run_parallel(cluster, job, task, num_workers):\n    server = tf.train.Server(cluster, job_name=job, task_index=task)\n    if job == 'ps':\n        server.join()\n    else:\n        worker_device = \"/job:worker/task:{}\".format(task)\n        with tf.device(tf.train.replica_device_setter(cluster=cluster)):\n            def train_next_batch(batchsize):\n                imgs = np.random.rand(batchsize,784)\n                inds = np.random.randint(0, 10, (batchsize))\n                labels = np.eye(10)[inds]\n                return imgs,labels\n            \n            images = tf.placeholder(tf.float32, [None, 784])\n            labels = tf.placeholder(tf.int32, [None, 10])    \n\n            logits = model(images)\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n\n            hooks = []\n            global_step = tf.train.get_or_create_global_step()\n            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)\n            optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,\n                                        total_num_replicas=num_workers, )\n            \n            hooks.append(optimizer.make_session_run_hook(task==0, num_tokens=0))\n            train_op = optimizer.minimize(loss, global_step=global_step,\n                                          aggregation_method=tf.AggregationMethod.ADD_N)\n            \n            \n            mon_sess = tf.train.MonitoredTrainingSession(master=server.target,\n                                                   is_chief=(task == 0),\n                                                   checkpoint_dir=\"./test_checkpoint_dir\",\n                                                   hooks=hooks)\n\n            for e in range(1000):\n                # simulate roll-out for reinforcement learning\n                for _ in range(100):\n                    img_batch, label_batch = train_next_batch(32)\n                    pred_logits = mon_sess.run([logits], feed_dict={images:img_batch})\n                img_batch, label_batch = train_next_batch(32)\n                _, ls, step = mon_sess.run([train_op, loss, global_step],\n                                            feed_dict={images: img_batch, labels: label_batch})\n                if step % 10 == 0:\n                    print(\"Worker %d, Train step %d, loss: %f\" % (task, step, ls))\n            server.join()\n            \ndef main():    \n    cluster = tf.train.ClusterSpec({\n        'worker': ['localhost:'+str(30352+w) for w in range(num_workers)],\n        'ps': [\n            'localhost:30351'\n        ]\n    })\n\n    job_task_index_map = [('ps', 0)]\n    for w in range(num_workers): job_task_index_map.append(('worker', w))\n\n    procs = []\n\n    for job, task in job_task_index_map:\n        proc = mp.Process(target=train_run_parallel, args=(cluster, job, task, num_workers))\n        procs.append(proc)\n        proc.start()\n    \n    for proc in procs:\n        proc.join()\n\nif __name__ == '__main__':\n    main()\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.10.0\nPython version: 3.5\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version:  cuda-9.0/ cuDNN 7\nGPU model and memory: TITAN X 12GB\nExact command to reproduce:  Run the code below to produce the warnings\n\nDescribe the problem\nThis is less a bug and more a suggested improvement. The setup illustrated in the code above is common for reinforcement learning algorithms where the workers are used to collect experience by acting in an environment according to the current policy in a distributed fashion and then synchronizing to update the parameters of the shared networks (stored on the parameter server). There are two ops that are run (multiple times) in a single iteration of the training loop. One is an act op and the other is the update op. The act op has nothing to do with the optimizer while the update op does. As such, running the act op should not affect the global_step while the update op should increment global_step. The problem, in my estimation, is that tensorflow does not recognize this difference and complains (by printing a warning) that global_step is not incremented every time the act op is run.\nThis is extremely annoying as thousands of warnings are printed and useful information is lost in the torrent of text. Turning off all warning is the workaround I am using right now, but this is clearly not a long-term solution.\nThe exact warning is:\n\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 835 vs previous value: 835. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n\nSource code / logs\nimport os\nimport multiprocessing as mp\nimport tensorflow as tf\nimport numpy as np\n\nnum_workers=4\n\ndef model(images):\n    net = tf.layers.dense(images, 500, activation=tf.nn.relu)\n    net = tf.layers.dense(net, 500, activation=tf.nn.relu)\n    net = tf.layers.dense(net, 10, activation=None)\n    return net\n\ndef train_run_parallel(cluster, job, task, num_workers):\n    server = tf.train.Server(cluster, job_name=job, task_index=task)\n    if job == 'ps':\n        server.join()\n    else:\n        worker_device = \"/job:worker/task:{}\".format(task)\n        with tf.device(tf.train.replica_device_setter(cluster=cluster)):\n            def train_next_batch(batchsize):\n                imgs = np.random.rand(batchsize,784)\n                inds = np.random.randint(0, 10, (batchsize))\n                labels = np.eye(10)[inds]\n                return imgs,labels\n            \n            images = tf.placeholder(tf.float32, [None, 784])\n            labels = tf.placeholder(tf.int32, [None, 10])    \n\n            logits = model(images)\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n\n            hooks = []\n            global_step = tf.train.get_or_create_global_step()\n            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)\n            optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,\n                                        total_num_replicas=num_workers, )\n            \n            hooks.append(optimizer.make_session_run_hook(task==0, num_tokens=0))\n            train_op = optimizer.minimize(loss, global_step=global_step,\n                                          aggregation_method=tf.AggregationMethod.ADD_N)\n            \n            \n            mon_sess = tf.train.MonitoredTrainingSession(master=server.target,\n                                                   is_chief=(task == 0),\n                                                   checkpoint_dir=\"./test_checkpoint_dir\",\n                                                   hooks=hooks)\n\n            for e in range(1000):\n                # simulate roll-out for reinforcement learning\n                for _ in range(100):\n                    img_batch, label_batch = train_next_batch(32)\n                    pred_logits = mon_sess.run([logits], feed_dict={images:img_batch})\n                img_batch, label_batch = train_next_batch(32)\n                _, ls, step = mon_sess.run([train_op, loss, global_step],\n                                            feed_dict={images: img_batch, labels: label_batch})\n                if step % 10 == 0:\n                    print(\"Worker %d, Train step %d, loss: %f\" % (task, step, ls))\n            server.join()\n            \ndef main():    \n    cluster = tf.train.ClusterSpec({\n        'worker': ['localhost:'+str(30352+w) for w in range(num_workers)],\n        'ps': [\n            'localhost:30351'\n        ]\n    })\n\n    job_task_index_map = [('ps', 0)]\n    for w in range(num_workers): job_task_index_map.append(('worker', w))\n\n    procs = []\n\n    for job, task in job_task_index_map:\n        proc = mp.Process(target=train_run_parallel, args=(cluster, job, task, num_workers))\n        procs.append(proc)\n        proc.start()\n    \n    for proc in procs:\n        proc.join()\n\nif __name__ == '__main__':\n    main()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**:  cuda-9.0/ cuDNN 7\r\n- **GPU model and memory**: TITAN X 12GB\r\n- **Exact command to reproduce**:  Run the code below to produce the warnings\r\n\r\n### Describe the problem\r\nThis is less a bug and more a suggested improvement. The setup illustrated in the code above is common for reinforcement learning algorithms where the workers are used to collect experience by acting in an environment according to the current policy in a distributed fashion and then synchronizing to update the parameters of the shared networks (stored on the parameter server). There are two ops that are run (multiple times) in a single iteration of the training loop. One is an `act` op and the other is the `update` op. The `act` op has nothing to do with the optimizer while the `update` op does. As such, running the `act` op should not affect the `global_step` while the `update` op should increment `global_step`. The problem, in my estimation, is that tensorflow does not recognize this difference and complains (by printing a warning) that `global_step` is not incremented every time the `act` op is run.\r\n\r\nThis is extremely annoying as thousands of warnings are printed and useful information is lost in the torrent of text. Turning off all warning is the workaround I am using right now, but this is clearly not a long-term solution.\r\n\r\nThe exact warning is:\r\n\r\n``` \r\n\r\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 835 vs previous value: 835. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\n```\r\n\r\n### Source code / logs\r\n```\r\nimport os\r\nimport multiprocessing as mp\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnum_workers=4\r\n\r\ndef model(images):\r\n    net = tf.layers.dense(images, 500, activation=tf.nn.relu)\r\n    net = tf.layers.dense(net, 500, activation=tf.nn.relu)\r\n    net = tf.layers.dense(net, 10, activation=None)\r\n    return net\r\n\r\ndef train_run_parallel(cluster, job, task, num_workers):\r\n    server = tf.train.Server(cluster, job_name=job, task_index=task)\r\n    if job == 'ps':\r\n        server.join()\r\n    else:\r\n        worker_device = \"/job:worker/task:{}\".format(task)\r\n        with tf.device(tf.train.replica_device_setter(cluster=cluster)):\r\n            def train_next_batch(batchsize):\r\n                imgs = np.random.rand(batchsize,784)\r\n                inds = np.random.randint(0, 10, (batchsize))\r\n                labels = np.eye(10)[inds]\r\n                return imgs,labels\r\n            \r\n            images = tf.placeholder(tf.float32, [None, 784])\r\n            labels = tf.placeholder(tf.int32, [None, 10])    \r\n\r\n            logits = model(images)\r\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\r\n\r\n            hooks = []\r\n            global_step = tf.train.get_or_create_global_step()\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)\r\n            optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,\r\n                                        total_num_replicas=num_workers, )\r\n            \r\n            hooks.append(optimizer.make_session_run_hook(task==0, num_tokens=0))\r\n            train_op = optimizer.minimize(loss, global_step=global_step,\r\n                                          aggregation_method=tf.AggregationMethod.ADD_N)\r\n            \r\n            \r\n            mon_sess = tf.train.MonitoredTrainingSession(master=server.target,\r\n                                                   is_chief=(task == 0),\r\n                                                   checkpoint_dir=\"./test_checkpoint_dir\",\r\n                                                   hooks=hooks)\r\n\r\n            for e in range(1000):\r\n                # simulate roll-out for reinforcement learning\r\n                for _ in range(100):\r\n                    img_batch, label_batch = train_next_batch(32)\r\n                    pred_logits = mon_sess.run([logits], feed_dict={images:img_batch})\r\n                img_batch, label_batch = train_next_batch(32)\r\n                _, ls, step = mon_sess.run([train_op, loss, global_step],\r\n                                            feed_dict={images: img_batch, labels: label_batch})\r\n                if step % 10 == 0:\r\n                    print(\"Worker %d, Train step %d, loss: %f\" % (task, step, ls))\r\n            server.join()\r\n            \r\ndef main():    \r\n    cluster = tf.train.ClusterSpec({\r\n        'worker': ['localhost:'+str(30352+w) for w in range(num_workers)],\r\n        'ps': [\r\n            'localhost:30351'\r\n        ]\r\n    })\r\n\r\n    job_task_index_map = [('ps', 0)]\r\n    for w in range(num_workers): job_task_index_map.append(('worker', w))\r\n\r\n    procs = []\r\n\r\n    for job, task in job_task_index_map:\r\n        proc = mp.Process(target=train_run_parallel, args=(cluster, job, task, num_workers))\r\n        procs.append(proc)\r\n        proc.start()\r\n    \r\n    for proc in procs:\r\n        proc.join()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n"}