{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/83268471", "pull_request_review_id": 4123758, "id": 83268471, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzMjY4NDcx", "diff_hunk": "@@ -0,0 +1,145 @@\n+/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <string>\n+\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/kernel_def_builder.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/bcast.h\"\n+\n+namespace tensorflow {\n+\n+// Position/length can be 32 or 64-bit integers\n+template <typename T>\n+class SubstrOp : public OpKernel {\n+  public:\n+    using OpKernel::OpKernel;\n+\n+    void Compute(OpKernelContext* context) override {\n+      // Get inputs\n+      const Tensor& input_tensor = context->input(0);\n+      const Tensor& pos_tensor = context->input(1);\n+      const Tensor& len_tensor = context->input(2);\n+      const TensorShape input_shape = input_tensor.shape();\n+      const TensorShape pos_shape = pos_tensor.shape();\n+      const TensorShape len_shape = len_tensor.shape();\n+      \n+      // Validate size of tensors\n+      OP_REQUIRES(context, pos_tensor.shape() == len_tensor.shape(),\n+                  errors::InvalidArgument(\"pos and len shapes must match: \",\n+                                           pos_shape.DebugString(), \" vs. \",\n+                                           len_shape.DebugString()));\n+      \n+      if (TensorShapeUtils::IsScalar(pos_shape)) {\n+        // Perform Op with scalar pos/len\n+        auto input = input_tensor.flat<string>();\n+        T pos = pos_tensor.scalar<T>()();\n+        T len = len_tensor.scalar<T>()();\n+\n+        // Allocate output\n+        Tensor* output_tensor = nullptr;\n+        OP_REQUIRES_OK(context,\n+                       context->allocate_output(\"output\", input_tensor.shape(),\n+                                                &output_tensor));\n+        auto output = output_tensor->flat<string>();\n+\n+        // Set output to be substrings of input strings\n+        for (size_t i = 0; i < input_tensor.NumElements(); ++i) {\n+          OP_REQUIRES(context, pos >= 0 && pos < input(i).size(),\n+                      errors::InvalidArgument(\"pos \", pos, \n+                                              \" out of range for string b'\", \n+                                              input(i), \"' at index \", i));\n+          output(i) = input(i).substr(pos, len);\n+        }\n+      } else if (input_shape == pos_shape) {\n+        // Perform Op element-wise\n+        auto input = input_tensor.flat<string>();\n+        auto pos = pos_tensor.flat<T>();\n+        auto len = len_tensor.flat<T>();\n+\n+        // Allocate output\n+        Tensor* output_tensor = nullptr;\n+        OP_REQUIRES_OK(context,\n+                       context->allocate_output(\"output\", input_tensor.shape(),\n+                                                &output_tensor));\n+        auto output = output_tensor->flat<string>();\n+\n+        // Set output to be substrings of input strings\n+        for (size_t i = 0; i < input_tensor.NumElements(); ++i) {\n+          OP_REQUIRES(context, pos(i) >= 0 && pos(i) < input(i).size(),\n+                      errors::InvalidArgument(\"pos \", pos(i), \n+                                              \" out of range for string b'\", \n+                                              input(i), \"' at index \", i));\n+          output(i) = input(i).substr(pos(i), len(i));\n+        }\n+      } else {  \n+        // Attempt broadcasting this Op\n+        context->SetStatus(errors::Unimplemented(\n+                 \"Substr broadcast is not supported yet.\"));\n+\n+        // BCast bcast(BCast::FromShape(input_shape), BCast::FromShape(pos_shape));\n+        // OP_REQUIRES(context, bcast.IsValid(), \n+        //             errors::InvalidArgument(\"Incompatible shapes: \", \n+        //                                     input_shape.DebugString(), \" vs. \",\n+        //                                     pos_shape.DebugString()));\n+        // TensorShape output_shape = BCast::ToShape(bcast.result_shape());\n+        // int ndims = output_shape.dims();\n+        // Tensor* output_tensor = nullptr;\n+        // OP_REQUIRES_OK(context,\n+        //                context->allocate_output(\"output\", output_shape,\n+        //                                         &output_tensor));\n+        // switch (ndims) {\n+        //   case 2: {\n+        //     auto output = output_tensor->shaped<string, 2>(bcast.x_reshape());\n+        //     auto input_reshaped = input_tensor.shaped<string, 2>(bcast.x_reshape());\n+        //     auto pos_reshaped = pos_tensor.shaped<T, 2>(bcast.y_reshape());\n+        //     auto len_reshaped = len_tensor.shaped<T, 2>(bcast.y_reshape());\n+              \n+        //     typename TTypes<string, 2>::Tensor input = input_reshaped.broadcast(BCast::ToIndexArray<2>(bcast.x_bcast()));", "path": "tensorflow/core/kernels/substr_op.cc", "position": null, "original_position": 117, "commit_id": "711a3125a1b1d5ae8fdf8597839cc96721c1e6e5", "original_commit_id": "2021b0f0e0bd4cd7c1cf77fa7e63b005c8583c7e", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "Ah, good point.  Well, if @benoitsteiner doesn't get back to you soon, perhaps stick with the scalar implementation for now, since I suspect that's good enough for your usecase.\n\n1) Implement the shape spec using broadcastable shapes.  E.g., have the C++ shape spec use BroadcastBinaryOpShapeFn in common_shape_fns.h so that the \"spec\" allows broadcasting: it's probably sufficient to check that shape(pos) == shape(len) and then do BroadcastBinaryOpShapefn on input and pos.\n\n2) In your kernel implementation, check that the pos/len are scalar, and if not, return an errors::Unimplemented, so it's clear that it _should_ be implemented but is not yet.\n\nProbably also mention that true ternary broadcasting might be necessary to make it efficient, since the looping you're doing even for the scalar approach isn't parallel.\n", "created_at": "2016-10-13T17:28:22Z", "updated_at": "2016-11-03T18:48:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4338#discussion_r83268471", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4338", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/83268471"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4338#discussion_r83268471"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4338"}}, "body_html": "<p>Ah, good point.  Well, if <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a> doesn't get back to you soon, perhaps stick with the scalar implementation for now, since I suspect that's good enough for your usecase.</p>\n<ol>\n<li>\n<p>Implement the shape spec using broadcastable shapes.  E.g., have the C++ shape spec use BroadcastBinaryOpShapeFn in common_shape_fns.h so that the \"spec\" allows broadcasting: it's probably sufficient to check that shape(pos) == shape(len) and then do BroadcastBinaryOpShapefn on input and pos.</p>\n</li>\n<li>\n<p>In your kernel implementation, check that the pos/len are scalar, and if not, return an errors::Unimplemented, so it's clear that it <em>should</em> be implemented but is not yet.</p>\n</li>\n</ol>\n<p>Probably also mention that true ternary broadcasting might be necessary to make it efficient, since the looping you're doing even for the scalar approach isn't parallel.</p>", "body_text": "Ah, good point.  Well, if @benoitsteiner doesn't get back to you soon, perhaps stick with the scalar implementation for now, since I suspect that's good enough for your usecase.\n\n\nImplement the shape spec using broadcastable shapes.  E.g., have the C++ shape spec use BroadcastBinaryOpShapeFn in common_shape_fns.h so that the \"spec\" allows broadcasting: it's probably sufficient to check that shape(pos) == shape(len) and then do BroadcastBinaryOpShapefn on input and pos.\n\n\nIn your kernel implementation, check that the pos/len are scalar, and if not, return an errors::Unimplemented, so it's clear that it should be implemented but is not yet.\n\n\nProbably also mention that true ternary broadcasting might be necessary to make it efficient, since the looping you're doing even for the scalar approach isn't parallel.", "in_reply_to_id": 81062953}