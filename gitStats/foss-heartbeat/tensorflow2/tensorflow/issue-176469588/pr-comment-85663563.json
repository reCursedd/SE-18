{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/85663563", "pull_request_review_id": 6366914, "id": 85663563, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDg1NjYzNTYz", "diff_hunk": "@@ -0,0 +1,220 @@\n+/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <string>\n+\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/kernel_def_builder.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/bcast.h\"\n+\n+namespace tensorflow {\n+\n+// Position/length can be 32 or 64-bit integers\n+template <typename T>\n+class SubstrOp : public OpKernel {\n+  public:\n+    using OpKernel::OpKernel;\n+\n+    void Compute(OpKernelContext* context) override {\n+      // Get inputs\n+      const Tensor& input_tensor = context->input(0);\n+      const Tensor& pos_tensor = context->input(1);\n+      const Tensor& len_tensor = context->input(2);\n+      const TensorShape input_shape = input_tensor.shape();\n+      const TensorShape pos_shape = pos_tensor.shape();\n+      const TensorShape len_shape = len_tensor.shape();\n+\n+      bool is_scalar = TensorShapeUtils::IsScalar(pos_shape);\n+      \n+      if (is_scalar || input_shape == pos_shape) {\n+        // pos/len are either scalar or match the shape of input_tensor\n+        // Do not need to do broadcasting\n+\n+        // Reshape input \n+        auto input = input_tensor.flat<string>();\n+        // Allocate output\n+        Tensor* output_tensor = nullptr;\n+        OP_REQUIRES_OK(context,\n+                       context->allocate_output(\"output\", input_tensor.shape(),\n+                                                &output_tensor));\n+        auto output = output_tensor->flat<string>();\n+        if (is_scalar) {\n+          // Perform Op with scalar pos/len\n+          const T pos = tensorflow::internal::SubtleMustCopy(pos_tensor.scalar<T>()());\n+          const T len = tensorflow::internal::SubtleMustCopy(len_tensor.scalar<T>()());\n+          for (size_t i = 0; i < input_tensor.NumElements(); ++i) {\n+            // Make sure pos won't cause a runtime error\n+            OP_REQUIRES(context, pos >= 0 && pos < input(i).size(),\n+                        errors::InvalidArgument(\"pos \", pos, \n+                                                \" out of range for string b'\", \n+                                                input(i), \"' at index \", i));\n+            output(i) = input(i).substr(pos, len);\n+          }\n+        } else {\n+          // Perform Op element-wise with tensor pos/len\n+          auto pos_flat = pos_tensor.flat<T>();\n+          auto len_flat = len_tensor.flat<T>();\n+          // Use SubtleMustCopy on pos/len to prevent async attacks\n+          const size_t num_elements = pos_tensor.NumElements();\n+          std::vector<T> pos(num_elements);\n+          std::vector<T> len(num_elements);\n+          for (size_t i = 0; i < num_elements; ++i) {\n+            pos[i] = tensorflow::internal::SubtleMustCopy(pos_flat(i));\n+            len[i] = tensorflow::internal::SubtleMustCopy(len_flat(i));\n+          }\n+          for (size_t i = 0; i < input_tensor.NumElements(); ++i) {\n+            // Make sure pos won't cause a runtime error\n+            OP_REQUIRES(context, pos[i] >= 0 && pos[i] < input(i).size(),\n+                        errors::InvalidArgument(\"pos \", pos[i], \n+                                                \" out of range for string b'\", \n+                                                input(i), \"' at index \", i));\n+            output(i) = input(i).substr(pos[i], len[i]);\n+          }\n+        }\n+      } else {\n+        // Perform op with broadcasting\n+        // TODO: Use ternary broadcasting for once available in Eigen. Current\n+        //       implementation iterates through broadcasted ops element-wise;\n+        //       this should be parallelized.\n+\n+        // Create BCast helper with shape of input and pos/len\n+        BCast bcast(BCast::FromShape(input_shape), BCast::FromShape(pos_shape));\n+        OP_REQUIRES(context, bcast.IsValid(), \n+                    errors::InvalidArgument(\"Incompatible shapes: \", \n+                                            input_shape.DebugString(), \" vs. \",\n+                                            pos_shape.DebugString()));\n+        TensorShape output_shape = BCast::ToShape(bcast.result_shape());\n+        int ndims = output_shape.dims();\n+        Tensor* output_tensor = nullptr;\n+        OP_REQUIRES_OK(context,\n+                       context->allocate_output(\"output\", output_shape,\n+                                                &output_tensor));\n+        switch (ndims) {\n+          case 1: {\n+            // Reshape tensors according to BCast results\n+            auto input = input_tensor.shaped<string,1>(bcast.x_reshape());\n+            auto output = output_tensor->shaped<string,1>(bcast.result_shape());\n+            auto pos = pos_tensor.shaped<T,1>(bcast.y_reshape());\n+            auto len = len_tensor.shaped<T,1>(bcast.y_reshape());\n+            \n+            // Allocate temporary buffer for broadcasted input tensor\n+            Tensor input_buffer;\n+            OP_REQUIRES_OK(context, \n+                           context->allocate_temp(DT_STRING,\n+                                                  output_shape, \n+                                                  &input_buffer));\n+            typename TTypes<string,1>::Tensor input_bcast = \n+                            input_buffer.shaped<string,1>(bcast.result_shape());\n+            input_bcast = input.broadcast(\n+                                   BCast::ToIndexArray<1>(bcast.x_bcast()));\n+            \n+            // Allocate temporary buffer for broadcasted position tensor\n+            Tensor pos_buffer;\n+            OP_REQUIRES_OK(context,\n+                           context->allocate_temp(DataTypeToEnum<T>::v(),\n+                                                  output_shape,\n+                                                  &pos_buffer));\n+            typename TTypes<T,1>::Tensor pos_bcast = pos_buffer.shaped<T,1>(\n+                                                          bcast.result_shape());\n+            pos_bcast = pos.broadcast(BCast::ToIndexArray<1>(bcast.y_bcast()));\n+            \n+            // Allocate temporary buffer for broadcasted length tensor\n+            Tensor len_buffer;\n+            OP_REQUIRES_OK(context,\n+                           context->allocate_temp(DataTypeToEnum<T>::v(),\n+                                                  output_shape,\n+                                                  &len_buffer));\n+            typename TTypes<T,1>::Tensor len_bcast = len_buffer.shaped<T,1>(\n+                                                          bcast.result_shape());\n+            len_bcast = len.broadcast(BCast::ToIndexArray<1>(bcast.y_bcast()));\n+            \n+            // Iterate through broadcasted tensors and perform substr\n+            for (int i = 0; i < output_shape.dim_size(0); ++i) {              \n+              output(i) = input_bcast(i).substr(pos_bcast(i), len_bcast(i));", "path": "tensorflow/core/kernels/substr_op.cc", "position": null, "original_position": 153, "commit_id": "711a3125a1b1d5ae8fdf8597839cc96721c1e6e5", "original_commit_id": "9abfa3152f94b39bbacca73a1018ef583cb64b98", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "I think you can just copy them as you loop through -- that's apparently how it's used elsewhere in the codebase, so it should be okay!\n", "created_at": "2016-10-30T16:42:29Z", "updated_at": "2016-11-03T18:48:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4338#discussion_r85663563", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4338", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/85663563"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4338#discussion_r85663563"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4338"}}, "body_html": "<p>I think you can just copy them as you loop through -- that's apparently how it's used elsewhere in the codebase, so it should be okay!</p>", "body_text": "I think you can just copy them as you loop through -- that's apparently how it's used elsewhere in the codebase, so it should be okay!", "in_reply_to_id": 85663366}