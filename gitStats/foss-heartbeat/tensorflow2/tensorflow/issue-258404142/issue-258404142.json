{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13116", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13116/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13116/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13116/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13116", "id": 258404142, "node_id": "MDU6SXNzdWUyNTg0MDQxNDI=", "number": 13116, "title": "tensorflow.python.debug.cli.offline_analyzer failed to read debug data from HDFS filesys", "user": {"login": "luchensk", "id": 28526467, "node_id": "MDQ6VXNlcjI4NTI2NDY3", "avatar_url": "https://avatars1.githubusercontent.com/u/28526467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luchensk", "html_url": "https://github.com/luchensk", "followers_url": "https://api.github.com/users/luchensk/followers", "following_url": "https://api.github.com/users/luchensk/following{/other_user}", "gists_url": "https://api.github.com/users/luchensk/gists{/gist_id}", "starred_url": "https://api.github.com/users/luchensk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luchensk/subscriptions", "organizations_url": "https://api.github.com/users/luchensk/orgs", "repos_url": "https://api.github.com/users/luchensk/repos", "events_url": "https://api.github.com/users/luchensk/events{/privacy}", "received_events_url": "https://api.github.com/users/luchensk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-18T08:38:12Z", "updated_at": "2017-09-18T09:43:26Z", "closed_at": "2017-09-18T09:43:26Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04.2</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0 from master branch</li>\n<li><strong>Python version</strong>: Python 2.7.12 (default, Nov 19 2016, 06:48:10)</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: null</li>\n<li><strong>GPU model and memory</strong>: null</li>\n<li><strong>Exact command to reproduce</strong>: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://&lt;debug_data_dir&gt;</li>\n</ul>\n<h3>Issue description</h3>\n<p>I saved debug data by <code>DumpingDebugHook</code> into hdfs filesys and then it failed to read the data by <code>python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://&lt;debug_data_path&gt;</code>, but it works well with the local filesys by the same way.</p>\n<h4>Error info:</h4>\n<pre><code># python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\ntfdbg offline: FLAGS.dump_dir = hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nhdfsExists: invokeMethod((Lorg/apache/hadoop/fs/Path;)Z) error:\njava.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: \"G53.ad000000000427.et2/11.140.133.72\"; destination host is: \"ns1\":8020;\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\nCaused by: java.net.SocketException: Network is unreachable\n\tat sun.nio.ch.Net.connect0(Native Method)\n\tat sun.nio.ch.Net.connect(Net.java:454)\n\tat sun.nio.ch.Net.connect(Net.java:446)\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\t... 17 more\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 78, in &lt;module&gt;\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 41, in main\n    FLAGS.dump_dir, validate=FLAGS.validate_graph)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 682, in __init__\n    raise IOError(\"Dump root directory %s does not exist\" % dump_root)\nIOError: Dump root directory hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100 does not exist\n</code></pre>\n<p>Be sure that the above hdfs dir exists, which including debug data by <code>DumpingDebugHook</code>, as below:</p>\n<pre><code># hdfs dfs -ls -d hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\ndrwxr-xr-x   - root supergroup          0 2017-09-18 08:01 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0 from master branch\nPython version: Python 2.7.12 (default, Nov 19 2016, 06:48:10)\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: null\nGPU model and memory: null\nExact command to reproduce: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>\n\nIssue description\nI saved debug data by DumpingDebugHook into hdfs filesys and then it failed to read the data by python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path>, but it works well with the local filesys by the same way.\nError info:\n# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\ntfdbg offline: FLAGS.dump_dir = hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nhdfsExists: invokeMethod((Lorg/apache/hadoop/fs/Path;)Z) error:\njava.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: \"G53.ad000000000427.et2/11.140.133.72\"; destination host is: \"ns1\":8020;\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\nCaused by: java.net.SocketException: Network is unreachable\n\tat sun.nio.ch.Net.connect0(Native Method)\n\tat sun.nio.ch.Net.connect(Net.java:454)\n\tat sun.nio.ch.Net.connect(Net.java:446)\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\t... 17 more\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 78, in <module>\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 41, in main\n    FLAGS.dump_dir, validate=FLAGS.validate_graph)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 682, in __init__\n    raise IOError(\"Dump root directory %s does not exist\" % dump_root)\nIOError: Dump root directory hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100 does not exist\n\nBe sure that the above hdfs dir exists, which including debug data by DumpingDebugHook, as below:\n# hdfs dfs -ls -d hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\ndrwxr-xr-x   - root supergroup          0 2017-09-18 08:01 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0 from master branch\r\n- **Python version**: Python 2.7.12 (default, Nov 19 2016, 06:48:10)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: null\r\n- **GPU model and memory**: null\r\n- **Exact command to reproduce**: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>\r\n\r\n### Issue description\r\nI saved debug data by `DumpingDebugHook` into hdfs filesys and then it failed to read the data by `python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path>`, but it works well with the local filesys by the same way.\r\n\r\n#### Error info:\r\n```\r\n# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\r\ntfdbg offline: FLAGS.dump_dir = hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\nhdfsExists: invokeMethod((Lorg/apache/hadoop/fs/Path;)Z) error:\r\njava.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: \"G53.ad000000000427.et2/11.140.133.72\"; destination host is: \"ns1\":8020;\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\r\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\r\nCaused by: java.net.SocketException: Network is unreachable\r\n\tat sun.nio.ch.Net.connect0(Native Method)\r\n\tat sun.nio.ch.Net.connect(Net.java:454)\r\n\tat sun.nio.ch.Net.connect(Net.java:446)\r\n\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\r\n\t... 17 more\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 78, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 41, in main\r\n    FLAGS.dump_dir, validate=FLAGS.validate_graph)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 682, in __init__\r\n    raise IOError(\"Dump root directory %s does not exist\" % dump_root)\r\nIOError: Dump root directory hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100 does not exist\r\n```\r\n\r\nBe sure that the above hdfs dir exists, which including debug data by `DumpingDebugHook`, as below:\r\n\r\n```\r\n# hdfs dfs -ls -d hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 08:01 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505721619237931_100\r\n```"}