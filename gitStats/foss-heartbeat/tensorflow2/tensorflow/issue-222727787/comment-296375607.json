{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296375607", "html_url": "https://github.com/tensorflow/tensorflow/issues/9310#issuecomment-296375607", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9310", "id": 296375607, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjM3NTYwNw==", "user": {"login": "MaeThird", "id": 14851411, "node_id": "MDQ6VXNlcjE0ODUxNDEx", "avatar_url": "https://avatars2.githubusercontent.com/u/14851411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MaeThird", "html_url": "https://github.com/MaeThird", "followers_url": "https://api.github.com/users/MaeThird/followers", "following_url": "https://api.github.com/users/MaeThird/following{/other_user}", "gists_url": "https://api.github.com/users/MaeThird/gists{/gist_id}", "starred_url": "https://api.github.com/users/MaeThird/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MaeThird/subscriptions", "organizations_url": "https://api.github.com/users/MaeThird/orgs", "repos_url": "https://api.github.com/users/MaeThird/repos", "events_url": "https://api.github.com/users/MaeThird/events{/privacy}", "received_events_url": "https://api.github.com/users/MaeThird/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-22T14:08:29Z", "updated_at": "2017-04-22T14:08:29Z", "author_association": "NONE", "body_html": "<p>Hi,<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a><br>\nIf I use</p>\n<blockquote>\n</blockquote>\n<p>new_saver = tf.train.import_meta_graph(\"xxx.meta\")<br>\nnew_saver.restore(sess, \"xxx.ckpt-yyy\")</p>\n<blockquote>\n</blockquote>\n<p>to run forward pass the only cannot uninitialized value is \".../moments/moments_1/mean/ExponentialMovingAverage\" .This is \"batch_norm\" codes as</p>\n<blockquote>\n</blockquote>\n<pre><code>with tf.variable_scope(name):\n    phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n    n_out = int(x.get_shape()[3])\n    beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n                       name=name+'/beta', trainable=True, dtype=x.dtype)\n    gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n                        name=name+'/gamma', trainable=True, dtype=x.dtype)      \n    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n    def mean_var_with_update():\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        with tf.control_dependencies([ema_apply_op]):\n            return tf.identity(batch_mean), tf.identity(batch_var)\n    mean, var = control_flow_ops.cond(phase_train,\n                                      mean_var_with_update,\n                                      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n    normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\nreturn normed\n</code></pre>\n<blockquote>\n</blockquote>\n<p>If I use</p>\n<blockquote>\n</blockquote>\n<p>slim.arg_scope([slim.conv2d],<br>\nweights_initializer=tf.truncated_normal_initializer(stddev=0.1),<br>\nweights_regularizer=slim.l2_regularizer(weight_decay),<br>\nnormalizer_fn=slim.batch_norm,<br>\nnormalizer_params=batch_norm_params)\"</p>\n<blockquote>\n</blockquote>\n<p>to replace my code no issue occurred.<br>\nThank you anyway.</p>", "body_text": "Hi,@drpngx\nIf I use\n\n\nnew_saver = tf.train.import_meta_graph(\"xxx.meta\")\nnew_saver.restore(sess, \"xxx.ckpt-yyy\")\n\n\nto run forward pass the only cannot uninitialized value is \".../moments/moments_1/mean/ExponentialMovingAverage\" .This is \"batch_norm\" codes as\n\n\nwith tf.variable_scope(name):\n    phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n    n_out = int(x.get_shape()[3])\n    beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n                       name=name+'/beta', trainable=True, dtype=x.dtype)\n    gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n                        name=name+'/gamma', trainable=True, dtype=x.dtype)      \n    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n    def mean_var_with_update():\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        with tf.control_dependencies([ema_apply_op]):\n            return tf.identity(batch_mean), tf.identity(batch_var)\n    mean, var = control_flow_ops.cond(phase_train,\n                                      mean_var_with_update,\n                                      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n    normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\nreturn normed\n\n\n\nIf I use\n\n\nslim.arg_scope([slim.conv2d],\nweights_initializer=tf.truncated_normal_initializer(stddev=0.1),\nweights_regularizer=slim.l2_regularizer(weight_decay),\nnormalizer_fn=slim.batch_norm,\nnormalizer_params=batch_norm_params)\"\n\n\nto replace my code no issue occurred.\nThank you anyway.", "body": "Hi,@drpngx \r\nIf I use \r\n\r\n> \r\n\r\nnew_saver = tf.train.import_meta_graph(\"xxx.meta\")\r\nnew_saver.restore(sess, \"xxx.ckpt-yyy\")\r\n\r\n> \r\n\r\n to run forward pass the only cannot uninitialized value is \".../moments/moments_1/mean/ExponentialMovingAverage\" .This is \"batch_norm\" codes as \r\n\r\n> \r\n    with tf.variable_scope(name):\r\n        phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\r\n        n_out = int(x.get_shape()[3])\r\n        beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\r\n                           name=name+'/beta', trainable=True, dtype=x.dtype)\r\n        gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\r\n                            name=name+'/gamma', trainable=True, dtype=x.dtype)      \r\n        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\r\n        ema = tf.train.ExponentialMovingAverage(decay=0.9)\r\n        def mean_var_with_update():\r\n            ema_apply_op = ema.apply([batch_mean, batch_var])\r\n            with tf.control_dependencies([ema_apply_op]):\r\n                return tf.identity(batch_mean), tf.identity(batch_var)\r\n        mean, var = control_flow_ops.cond(phase_train,\r\n                                          mean_var_with_update,\r\n                                          lambda: (ema.average(batch_mean), ema.average(batch_var)))\r\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\r\n    return normed\r\n> \r\nIf I use \r\n> \r\n\r\n\r\nslim.arg_scope([slim.conv2d],\r\n                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\r\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\r\n                        normalizer_fn=slim.batch_norm,\r\n                        normalizer_params=batch_norm_params)\" \r\n\r\n\r\n> \r\n\r\nto replace my code no issue occurred.\r\nThank you anyway."}