{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341862970", "html_url": "https://github.com/tensorflow/tensorflow/issues/14190#issuecomment-341862970", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14190", "id": 341862970, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTg2Mjk3MA==", "user": {"login": "zhedongzheng", "id": 16261331, "node_id": "MDQ6VXNlcjE2MjYxMzMx", "avatar_url": "https://avatars2.githubusercontent.com/u/16261331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhedongzheng", "html_url": "https://github.com/zhedongzheng", "followers_url": "https://api.github.com/users/zhedongzheng/followers", "following_url": "https://api.github.com/users/zhedongzheng/following{/other_user}", "gists_url": "https://api.github.com/users/zhedongzheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhedongzheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhedongzheng/subscriptions", "organizations_url": "https://api.github.com/users/zhedongzheng/orgs", "repos_url": "https://api.github.com/users/zhedongzheng/repos", "events_url": "https://api.github.com/users/zhedongzheng/events{/privacy}", "received_events_url": "https://api.github.com/users/zhedongzheng/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-04T01:40:31Z", "updated_at": "2017-11-04T03:21:38Z", "author_association": "NONE", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1256036\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sshrdp\">@sshrdp</a>, i have successfully run the boosted_tree in 1.4<br>\n(though it is very slow building the graph)</p>\n<ul>\n<li>Python 3 throws error, because they use <code>xrange</code> in the source code</li>\n</ul>\n<p>For those who may be interested running GBT in TF, the minimal script is:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> functools\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.boosted_trees.estimator_batch.estimator <span class=\"pl-k\">import</span> GradientBoostedDecisionTreeEstimator\n<span class=\"pl-k\">from</span> tensorflow.contrib.boosted_trees.proto <span class=\"pl-k\">import</span> learner_pb2\n<span class=\"pl-k\">from</span> tensorflow.contrib.boosted_trees.estimator_batch <span class=\"pl-k\">import</span> custom_loss_head\n<span class=\"pl-k\">from</span> tensorflow.contrib.boosted_trees.python.utils <span class=\"pl-k\">import</span> losses\n<span class=\"pl-k\">from</span> tensorflow.contrib <span class=\"pl-k\">import</span> metrics <span class=\"pl-k\">as</span> metrics_lib\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> math_ops\n\n\ntf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--batch_size<span class=\"pl-pds\">\"</span></span>,<span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>,<span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--depth<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Maximum depth of weak learners.<span class=\"pl-pds\">\"</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--l2<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>l2 regularization per batch.<span class=\"pl-pds\">\"</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--learning_rate<span class=\"pl-pds\">\"</span></span>,<span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>,<span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--examples_per_layer<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--num_trees<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--num_classes<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\nargs <span class=\"pl-k\">=</span> parser.parse_args()\n\nlearner_config <span class=\"pl-k\">=</span> learner_pb2.LearnerConfig()\nlearner_config.learning_rate_tuner.fixed.learning_rate <span class=\"pl-k\">=</span> args.learning_rate\nlearner_config.num_classes <span class=\"pl-k\">=</span> args.num_classes\nlearner_config.regularization.l1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\nlearner_config.regularization.l2 <span class=\"pl-k\">=</span> args.l2 <span class=\"pl-k\">/</span> args.examples_per_layer\nlearner_config.constraints.max_tree_depth <span class=\"pl-k\">=</span> args.depth\n\nlearner_config.growing_mode <span class=\"pl-k\">=</span> learner_pb2.LearnerConfig.<span class=\"pl-c1\">LAYER_BY_LAYER</span>\nlearner_config.multi_class_strategy <span class=\"pl-k\">=</span> learner_pb2.LearnerConfig.<span class=\"pl-c1\">DIAGONAL_HESSIAN</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_multiclass_metrics</span>(<span class=\"pl-smi\">predictions</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">weights</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Prepares eval metrics for multiclass eval.<span class=\"pl-pds\">\"\"\"</span></span>\n    metrics <span class=\"pl-k\">=</span> <span class=\"pl-c1\">dict</span>()\n    logits <span class=\"pl-k\">=</span> predictions[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scores<span class=\"pl-pds\">\"</span></span>]\n    classes <span class=\"pl-k\">=</span> math_ops.argmax(logits, <span class=\"pl-c1\">1</span>)\n    metrics[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> metrics_lib.streaming_accuracy(\n        classes, labels, weights)\n    <span class=\"pl-k\">return</span> metrics\n\n\nhead <span class=\"pl-k\">=</span> custom_loss_head.CustomLossHead(\n      <span class=\"pl-v\">loss_fn</span><span class=\"pl-k\">=</span>functools.partial(losses.per_example_maxent_loss, <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span>args.num_classes),\n      <span class=\"pl-v\">link_fn</span><span class=\"pl-k\">=</span>tf.identity,\n      <span class=\"pl-v\">logit_dimension</span><span class=\"pl-k\">=</span>args.num_classes,\n      <span class=\"pl-v\">metrics_fn</span><span class=\"pl-k\">=</span>_multiclass_metrics)\n\nestimator <span class=\"pl-k\">=</span> GradientBoostedDecisionTreeEstimator(\n    <span class=\"pl-v\">learner_config</span><span class=\"pl-k\">=</span>learner_config,\n    <span class=\"pl-v\">head</span><span class=\"pl-k\">=</span>head,\n    <span class=\"pl-v\">examples_per_layer</span><span class=\"pl-k\">=</span>args.examples_per_layer,\n    <span class=\"pl-v\">num_trees</span><span class=\"pl-k\">=</span>args.num_trees,\n    <span class=\"pl-v\">center_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n(X_train, y_train), (X_test, y_test) <span class=\"pl-k\">=</span> tf.keras.datasets.mnist.load_data()\nX_train <span class=\"pl-k\">=</span> (X_train <span class=\"pl-k\">/</span> <span class=\"pl-c1\">255</span>.).reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">28</span>).astype(np.float32)\nX_test <span class=\"pl-k\">=</span> (X_test <span class=\"pl-k\">/</span> <span class=\"pl-c1\">255</span>.).reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">28</span>).astype(np.float32)\ny_train <span class=\"pl-k\">=</span> y_train.astype(np.int32)\ny_test <span class=\"pl-k\">=</span> y_test.astype(np.int32)\n\nestimator.fit(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>tf.estimator.inputs.numpy_input_fn(\n    <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>_<span class=\"pl-pds\">'</span></span>:X_train}, <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>y_train, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>args.batch_size, <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\nestimator.evaluate(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>tf.estimator.inputs.numpy_input_fn(\n    <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>_<span class=\"pl-pds\">'</span></span>:X_test}, <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>y_test, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>args.batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))</pre></div>", "body_text": "Thanks @sshrdp, i have successfully run the boosted_tree in 1.4\n(though it is very slow building the graph)\n\nPython 3 throws error, because they use xrange in the source code\n\nFor those who may be interested running GBT in TF, the minimal script is:\nimport argparse\nimport functools\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeEstimator\nfrom tensorflow.contrib.boosted_trees.proto import learner_pb2\nfrom tensorflow.contrib.boosted_trees.estimator_batch import custom_loss_head\nfrom tensorflow.contrib.boosted_trees.python.utils import losses\nfrom tensorflow.contrib import metrics as metrics_lib\nfrom tensorflow.python.ops import math_ops\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--batch_size\",type=int,default=1000)\nparser.add_argument(\"--depth\", type=int, default=4, help=\"Maximum depth of weak learners.\")\nparser.add_argument(\"--l2\", type=float, default=1.0, help=\"l2 regularization per batch.\")\nparser.add_argument(\"--learning_rate\",type=float,default=0.1)\nparser.add_argument(\"--examples_per_layer\", type=int, default=1000)\nparser.add_argument(\"--num_trees\", type=int, default=100)\nparser.add_argument(\"--num_classes\", type=int, default=10)\nargs = parser.parse_args()\n\nlearner_config = learner_pb2.LearnerConfig()\nlearner_config.learning_rate_tuner.fixed.learning_rate = args.learning_rate\nlearner_config.num_classes = args.num_classes\nlearner_config.regularization.l1 = 0.0\nlearner_config.regularization.l2 = args.l2 / args.examples_per_layer\nlearner_config.constraints.max_tree_depth = args.depth\n\nlearner_config.growing_mode = learner_pb2.LearnerConfig.LAYER_BY_LAYER\nlearner_config.multi_class_strategy = learner_pb2.LearnerConfig.DIAGONAL_HESSIAN\n\n\ndef _multiclass_metrics(predictions, labels, weights):\n    \"\"\"Prepares eval metrics for multiclass eval.\"\"\"\n    metrics = dict()\n    logits = predictions[\"scores\"]\n    classes = math_ops.argmax(logits, 1)\n    metrics[\"accuracy\"] = metrics_lib.streaming_accuracy(\n        classes, labels, weights)\n    return metrics\n\n\nhead = custom_loss_head.CustomLossHead(\n      loss_fn=functools.partial(losses.per_example_maxent_loss, num_classes=args.num_classes),\n      link_fn=tf.identity,\n      logit_dimension=args.num_classes,\n      metrics_fn=_multiclass_metrics)\n\nestimator = GradientBoostedDecisionTreeEstimator(\n    learner_config=learner_config,\n    head=head,\n    examples_per_layer=args.examples_per_layer,\n    num_trees=args.num_trees,\n    center_bias=False)\n\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = (X_train / 255.).reshape(-1, 28*28).astype(np.float32)\nX_test = (X_test / 255.).reshape(-1, 28*28).astype(np.float32)\ny_train = y_train.astype(np.int32)\ny_test = y_test.astype(np.int32)\n\nestimator.fit(input_fn=tf.estimator.inputs.numpy_input_fn(\n    x={'_':X_train}, y=y_train, batch_size=args.batch_size, num_epochs=1, shuffle=True))\n\nestimator.evaluate(input_fn=tf.estimator.inputs.numpy_input_fn(\n    x={'_':X_test}, y=y_test, batch_size=args.batch_size, shuffle=False))", "body": "Thanks @sshrdp, i have successfully run the boosted_tree in 1.4\r\n(though it is very slow building the graph)\r\n* Python 3 throws error, because they use ```xrange``` in the source code\r\n\r\nFor those who may be interested running GBT in TF, the minimal script is:\r\n```python\r\nimport argparse\r\nimport functools\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeEstimator\r\nfrom tensorflow.contrib.boosted_trees.proto import learner_pb2\r\nfrom tensorflow.contrib.boosted_trees.estimator_batch import custom_loss_head\r\nfrom tensorflow.contrib.boosted_trees.python.utils import losses\r\nfrom tensorflow.contrib import metrics as metrics_lib\r\nfrom tensorflow.python.ops import math_ops\r\n\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--batch_size\",type=int,default=1000)\r\nparser.add_argument(\"--depth\", type=int, default=4, help=\"Maximum depth of weak learners.\")\r\nparser.add_argument(\"--l2\", type=float, default=1.0, help=\"l2 regularization per batch.\")\r\nparser.add_argument(\"--learning_rate\",type=float,default=0.1)\r\nparser.add_argument(\"--examples_per_layer\", type=int, default=1000)\r\nparser.add_argument(\"--num_trees\", type=int, default=100)\r\nparser.add_argument(\"--num_classes\", type=int, default=10)\r\nargs = parser.parse_args()\r\n\r\nlearner_config = learner_pb2.LearnerConfig()\r\nlearner_config.learning_rate_tuner.fixed.learning_rate = args.learning_rate\r\nlearner_config.num_classes = args.num_classes\r\nlearner_config.regularization.l1 = 0.0\r\nlearner_config.regularization.l2 = args.l2 / args.examples_per_layer\r\nlearner_config.constraints.max_tree_depth = args.depth\r\n\r\nlearner_config.growing_mode = learner_pb2.LearnerConfig.LAYER_BY_LAYER\r\nlearner_config.multi_class_strategy = learner_pb2.LearnerConfig.DIAGONAL_HESSIAN\r\n\r\n\r\ndef _multiclass_metrics(predictions, labels, weights):\r\n    \"\"\"Prepares eval metrics for multiclass eval.\"\"\"\r\n    metrics = dict()\r\n    logits = predictions[\"scores\"]\r\n    classes = math_ops.argmax(logits, 1)\r\n    metrics[\"accuracy\"] = metrics_lib.streaming_accuracy(\r\n        classes, labels, weights)\r\n    return metrics\r\n\r\n\r\nhead = custom_loss_head.CustomLossHead(\r\n      loss_fn=functools.partial(losses.per_example_maxent_loss, num_classes=args.num_classes),\r\n      link_fn=tf.identity,\r\n      logit_dimension=args.num_classes,\r\n      metrics_fn=_multiclass_metrics)\r\n\r\nestimator = GradientBoostedDecisionTreeEstimator(\r\n    learner_config=learner_config,\r\n    head=head,\r\n    examples_per_layer=args.examples_per_layer,\r\n    num_trees=args.num_trees,\r\n    center_bias=False)\r\n\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nX_train = (X_train / 255.).reshape(-1, 28*28).astype(np.float32)\r\nX_test = (X_test / 255.).reshape(-1, 28*28).astype(np.float32)\r\ny_train = y_train.astype(np.int32)\r\ny_test = y_test.astype(np.int32)\r\n\r\nestimator.fit(input_fn=tf.estimator.inputs.numpy_input_fn(\r\n    x={'_':X_train}, y=y_train, batch_size=args.batch_size, num_epochs=1, shuffle=True))\r\n\r\nestimator.evaluate(input_fn=tf.estimator.inputs.numpy_input_fn(\r\n    x={'_':X_test}, y=y_test, batch_size=args.batch_size, shuffle=False))\r\n```"}