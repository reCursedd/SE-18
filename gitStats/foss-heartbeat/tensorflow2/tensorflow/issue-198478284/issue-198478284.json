{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6616", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6616/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6616/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6616/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6616", "id": 198478284, "node_id": "MDU6SXNzdWUxOTg0NzgyODQ=", "number": 6616, "title": "fake_quant_with_min_max_args has odd behavior", "user": {"login": "yoslber", "id": 1845494, "node_id": "MDQ6VXNlcjE4NDU0OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1845494?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yoslber", "html_url": "https://github.com/yoslber", "followers_url": "https://api.github.com/users/yoslber/followers", "following_url": "https://api.github.com/users/yoslber/following{/other_user}", "gists_url": "https://api.github.com/users/yoslber/gists{/gist_id}", "starred_url": "https://api.github.com/users/yoslber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yoslber/subscriptions", "organizations_url": "https://api.github.com/users/yoslber/orgs", "repos_url": "https://api.github.com/users/yoslber/repos", "events_url": "https://api.github.com/users/yoslber/events{/privacy}", "received_events_url": "https://api.github.com/users/yoslber/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-01-03T13:50:23Z", "updated_at": "2018-11-23T01:20:54Z", "closed_at": "2017-04-18T15:29:22Z", "author_association": "NONE", "body_html": "<h3>Description</h3>\n<p>On a simple linear regression example, <code>fake_quant_with_min_max_args</code> is not working. If I change to <code>fake_quant_with_min_max_vars</code> with trainable quantization min/max ranges, it works just fine.<br>\nThe min/max values are the same in both approaches.<br>\nReproducer included below.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04.5, Python 2.7</p>\n<p>Installed version of CUDA and cuDNN: Cuda 8, CuDNN 5.1</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>\n<p>A link to the pip package you installed: <a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl</a></p>\n</li>\n<li>\n<p>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>. 0.12.1</p>\n</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>Below is a small reproducer, adapted from the example at <a href=\"https://www.tensorflow.org/get_started/\" rel=\"nofollow\">https://www.tensorflow.org/get_started/</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9</span>\nx_data <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">10000</span>).astype(np.float32)\ny_data <span class=\"pl-k\">=</span> x_data <span class=\"pl-k\">*</span> <span class=\"pl-c1\">123.456</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">78.9</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Try to find values for W and b that compute y_data = W * x_data + b</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (We know that W should be 123.456 and b 78.9, but TensorFlow will</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> figure that out for us.)</span>\nW <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">255.0</span>))\nb <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">1</span>]))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Now we quantize the weights and bias </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The expected result after training should be </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> y = x * 123 + 79</span>\nW <span class=\"pl-k\">=</span> tf.fake_quant_with_min_max_args(W, <span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">max</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">255.0</span>)\nb <span class=\"pl-k\">=</span> tf.fake_quant_with_min_max_args(b, <span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">max</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">255.0</span>)\n\ny <span class=\"pl-k\">=</span> W <span class=\"pl-k\">*</span> x_data <span class=\"pl-k\">+</span> b <span class=\"pl-c\"><span class=\"pl-c\">#</span> Model</span>\n\nloss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.square(y <span class=\"pl-k\">-</span> y_data))\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.5</span>)\ntrain <span class=\"pl-k\">=</span> optimizer.minimize(loss)\n\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(init)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Fit the line.</span>\n<span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">201</span>):\n    sess.run(train)\n    <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">20</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-c1\">print</span>(step, sess.run(loss), sess.run(W), sess.run(b))</pre></div>\n<p>Output is a random rounded values for W and b, depending on run, but not the expected W=123, b=79, and the loss is large.</p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>If I instead use <code>fake_quant_with_min_max_vars</code> as illustrated below, it works fine (by printing, we have verified that the quantization ranges are [0,255] for each training iteration). The loss decreases and the values for W and b are as expected. The example is adapted from <a href=\"https://www.tensorflow.org/get_started/\" rel=\"nofollow\">https://www.tensorflow.org/get_started/</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9</span>\nx_data <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">10000</span>).astype(np.float32)\ny_data <span class=\"pl-k\">=</span> x_data <span class=\"pl-k\">*</span> <span class=\"pl-c1\">123.456</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">78.9</span>\n\nW <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">255.0</span>))\nb <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">1</span>]))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Now we quantize the weights and bias</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The expected result after training should be </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> y = x * 123 + 79   Note that we train the quantization ranges</span>\nqmin <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0.0</span>)\nqmax <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">255.0</span>)\nW <span class=\"pl-k\">=</span> tf.fake_quant_with_min_max_vars(W, <span class=\"pl-v\">min</span><span class=\"pl-k\">=</span>qmin, <span class=\"pl-v\">max</span><span class=\"pl-k\">=</span>qmax)\n\nqminb <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0.0</span>)\nqmaxb <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">255.0</span>)\nb <span class=\"pl-k\">=</span> tf.fake_quant_with_min_max_vars(b, <span class=\"pl-v\">min</span><span class=\"pl-k\">=</span>qminb, <span class=\"pl-v\">max</span><span class=\"pl-k\">=</span>qmaxb)\n\ny <span class=\"pl-k\">=</span> W <span class=\"pl-k\">*</span> x_data <span class=\"pl-k\">+</span> b <span class=\"pl-c\"><span class=\"pl-c\">#</span> Model</span>\n\nloss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.square(y <span class=\"pl-k\">-</span> y_data))\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.5</span>)\ntrain <span class=\"pl-k\">=</span> optimizer.minimize(loss)\n\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(init)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Fit the line.</span>\n<span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">201</span>):\n    sess.run(train)\n    <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">20</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-c1\">print</span>(step, sess.run(loss), sess.run(W), sess.run(b))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Quantization ranges<span class=\"pl-pds\">'</span></span>,  sess.run(qmin), sess.run(qmax), sess.run(qminb), sess.run(qmaxb))</pre></div>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>", "body_text": "Description\nOn a simple linear regression example, fake_quant_with_min_max_args is not working. If I change to fake_quant_with_min_max_vars with trainable quantization min/max ranges, it works just fine.\nThe min/max values are the same in both approaches.\nReproducer included below.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone\nEnvironment info\nOperating System: Ubuntu 14.04.5, Python 2.7\nInstalled version of CUDA and cuDNN: Cuda 8, CuDNN 5.1\nIf installed from binary pip package, provide:\n\n\nA link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl\n\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\". 0.12.1\n\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nBelow is a small reproducer, adapted from the example at https://www.tensorflow.org/get_started/\nimport tensorflow as tf\nimport numpy as np\n\n# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9\nx_data = np.random.rand(10000).astype(np.float32)\ny_data = x_data * 123.456 + 78.9\n\n# Try to find values for W and b that compute y_data = W * x_data + b\n# (We know that W should be 123.456 and b 78.9, but TensorFlow will\n# figure that out for us.)\nW = tf.Variable(tf.random_uniform([1], 0.0, 255.0))\nb = tf.Variable(tf.zeros([1]))\n\n# Now we quantize the weights and bias \n# The expected result after training should be \n# y = x * 123 + 79\nW = tf.fake_quant_with_min_max_args(W, min=0.0, max=255.0)\nb = tf.fake_quant_with_min_max_args(b, min=0.0, max=255.0)\n\ny = W * x_data + b # Model\n\nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.\nfor step in range(201):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(loss), sess.run(W), sess.run(b))\nOutput is a random rounded values for W and b, depending on run, but not the expected W=123, b=79, and the loss is large.\nWhat other attempted solutions have you tried?\nIf I instead use fake_quant_with_min_max_vars as illustrated below, it works fine (by printing, we have verified that the quantization ranges are [0,255] for each training iteration). The loss decreases and the values for W and b are as expected. The example is adapted from https://www.tensorflow.org/get_started/\nimport tensorflow as tf\nimport numpy as np\n\n# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9\nx_data = np.random.rand(10000).astype(np.float32)\ny_data = x_data * 123.456 + 78.9\n\nW = tf.Variable(tf.random_uniform([1], 0.0, 255.0))\nb = tf.Variable(tf.zeros([1]))\n\n# Now we quantize the weights and bias\n# The expected result after training should be \n# y = x * 123 + 79   Note that we train the quantization ranges\nqmin = tf.Variable(0.0)\nqmax = tf.Variable(255.0)\nW = tf.fake_quant_with_min_max_vars(W, min=qmin, max=qmax)\n\nqminb = tf.Variable(0.0)\nqmaxb = tf.Variable(255.0)\nb = tf.fake_quant_with_min_max_vars(b, min=qminb, max=qmaxb)\n\ny = W * x_data + b # Model\n\nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.\nfor step in range(201):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(loss), sess.run(W), sess.run(b))\n\nprint('Quantization ranges',  sess.run(qmin), sess.run(qmax), sess.run(qminb), sess.run(qmaxb))\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).", "body": "### Description\r\n\r\nOn a simple linear regression example, `fake_quant_with_min_max_args` is not working. If I change to `fake_quant_with_min_max_vars` with trainable quantization min/max ranges, it works just fine. \r\nThe min/max values are the same in both approaches. \r\nReproducer included below.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5, Python 2.7\r\n\r\nInstalled version of CUDA and cuDNN: Cuda 8, CuDNN 5.1\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.12.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nBelow is a small reproducer, adapted from the example at [https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9\r\nx_data = np.random.rand(10000).astype(np.float32)\r\ny_data = x_data * 123.456 + 78.9\r\n\r\n# Try to find values for W and b that compute y_data = W * x_data + b\r\n# (We know that W should be 123.456 and b 78.9, but TensorFlow will\r\n# figure that out for us.)\r\nW = tf.Variable(tf.random_uniform([1], 0.0, 255.0))\r\nb = tf.Variable(tf.zeros([1]))\r\n\r\n# Now we quantize the weights and bias \r\n# The expected result after training should be \r\n# y = x * 123 + 79\r\nW = tf.fake_quant_with_min_max_args(W, min=0.0, max=255.0)\r\nb = tf.fake_quant_with_min_max_args(b, min=0.0, max=255.0)\r\n\r\ny = W * x_data + b # Model\r\n\r\nloss = tf.reduce_mean(tf.square(y - y_data))\r\noptimizer = tf.train.GradientDescentOptimizer(0.5)\r\ntrain = optimizer.minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# Fit the line.\r\nfor step in range(201):\r\n    sess.run(train)\r\n    if step % 20 == 0:\r\n        print(step, sess.run(loss), sess.run(W), sess.run(b))\r\n```\r\nOutput is a random rounded values for W and b, depending on run, but not the expected W=123, b=79, and the loss is large.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nIf I instead use `fake_quant_with_min_max_vars` as illustrated below, it works fine (by printing, we have verified that the quantization ranges are [0,255] for each training iteration). The loss decreases and the values for W and b are as expected. The example is adapted from [https://www.tensorflow.org/get_started/](https://www.tensorflow.org/get_started/)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Create 10000 phony x, y data points in NumPy, y = x * 123.456 + 78.9\r\nx_data = np.random.rand(10000).astype(np.float32)\r\ny_data = x_data * 123.456 + 78.9\r\n\r\nW = tf.Variable(tf.random_uniform([1], 0.0, 255.0))\r\nb = tf.Variable(tf.zeros([1]))\r\n\r\n# Now we quantize the weights and bias\r\n# The expected result after training should be \r\n# y = x * 123 + 79   Note that we train the quantization ranges\r\nqmin = tf.Variable(0.0)\r\nqmax = tf.Variable(255.0)\r\nW = tf.fake_quant_with_min_max_vars(W, min=qmin, max=qmax)\r\n\r\nqminb = tf.Variable(0.0)\r\nqmaxb = tf.Variable(255.0)\r\nb = tf.fake_quant_with_min_max_vars(b, min=qminb, max=qmaxb)\r\n\r\ny = W * x_data + b # Model\r\n\r\nloss = tf.reduce_mean(tf.square(y - y_data))\r\noptimizer = tf.train.GradientDescentOptimizer(0.5)\r\ntrain = optimizer.minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# Fit the line.\r\nfor step in range(201):\r\n    sess.run(train)\r\n    if step % 20 == 0:\r\n        print(step, sess.run(loss), sess.run(W), sess.run(b))\r\n\r\nprint('Quantization ranges',  sess.run(qmin), sess.run(qmax), sess.run(qminb), sess.run(qmaxb))\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link)."}