{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428403642", "html_url": "https://github.com/tensorflow/tensorflow/issues/22162#issuecomment-428403642", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22162", "id": 428403642, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODQwMzY0Mg==", "user": {"login": "feihugis", "id": 5057740, "node_id": "MDQ6VXNlcjUwNTc3NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/5057740?v=4", "gravatar_id": "", "url": "https://api.github.com/users/feihugis", "html_url": "https://github.com/feihugis", "followers_url": "https://api.github.com/users/feihugis/followers", "following_url": "https://api.github.com/users/feihugis/following{/other_user}", "gists_url": "https://api.github.com/users/feihugis/gists{/gist_id}", "starred_url": "https://api.github.com/users/feihugis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/feihugis/subscriptions", "organizations_url": "https://api.github.com/users/feihugis/orgs", "repos_url": "https://api.github.com/users/feihugis/repos", "events_url": "https://api.github.com/users/feihugis/events{/privacy}", "received_events_url": "https://api.github.com/users/feihugis/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T01:09:00Z", "updated_at": "2018-10-10T01:09:00Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5057740\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/feihugis\">@feihugis</a> no, this does not solve the problem, this merely corrects the minimal example I provided to demonstrate the problem to the way I initially intended it to be. The real world scenario where I am facing this problem is like this:<br>\nI am training to separate stages with the tensorflow object detection framework. Then I am connecting them together for inference (the second graph gets the cropped detections from the first stage as inputs). I import the two graphs with different name scopes, but still I get this error message unless I go into deep into the tensorflow code and add a name scope right to the beginning of \"shape_utils.static_or_dynamic_map_fn\", which I then have to change for each graph when running the provided export script.</p>\n<p>This can't be the way it's supposed to be. When I import the 2 graphs with different name scopes, I expect to be able to connect them. If I'd get a frozen graph from someone else, I couldn't go in and export it again with a different map_fn name scope like my current workaround. The error message is even stating <code>\"This is an internal bug, please file a bug report with instructions on how to reproduce the error.\"</code>. Please look into the provided example seriously:<br>\nThe rename_second= True in <code>connect_graphs(both_are_map_graphs=True, rename_second=True)</code><br>\ncauses the map_fn name scope to be renamed in the graph BUILD not when IMPORTING. <code>connect_graphs(both_are_map_graphs=True, rename_second=False)</code> still imports the graphs with two different names, but here the bug is caused.</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2621937\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/stefsietz\">@stefsietz</a> I add some logging code to the function <code>def connect_graphs()</code> as below. When <code>connect_graphs(both_are_map_graphs=True, rename_second=False)</code>, the output will be <code>Graph1 scope name: ;</code> and <code>Graph2 scope name: ;</code>. That means they have the same scope name. Is my understanding correct?</p>\n<p>Sure, you can submit a PR. The expert in TensorFlow group will help you.</p>\n<pre><code>def connect_graphs(both_are_map_graphs=False, rename_second=False):\n    test_data = np.asarray([[1.0], [2.0]], dtype=np.float32)\n\n    graph1 = build_map_graph()\n    g1_graph_def = graph1.as_graph_def()\n    ##################################################\n    print(\"Graph1 scope name: {};\".format(graph1.get_name_scope()))\n    ##################################################\n\n    g2_map_name_scope = 'graph2_map' if rename_second else ''\n    graph2 = build_map_graph(g2_map_name_scope) if both_are_map_graphs else build_simple_graph()\n    ##################################################\n    print(\"Graph2 scope name: {};\".format(graph2.get_name_scope()))\n    ##################################################\n\n    g2_graph_def = graph2.as_graph_def()\n\n    connected_graph = tf.Graph()\n\n    with connected_graph.as_default():\n        tf.import_graph_def(g1_graph_def, name='graph1')\n\n        g1_output_tensor = tf.get_default_graph().get_tensor_by_name('graph1/output:0')\n        g1_input_tensor = tf.get_default_graph().get_tensor_by_name('graph1/input:0')\n\n        tf.import_graph_def(g2_graph_def, name='graph2', input_map={'input': g1_output_tensor})\n        g2_output_tensor = tf.get_default_graph().get_tensor_by_name('graph2/output:0')\n\n        with tf.Session() as sess:\n            result = sess.run(g2_output_tensor, {g1_input_tensor: test_data})\n            print(result)\n</code></pre>", "body_text": "@feihugis no, this does not solve the problem, this merely corrects the minimal example I provided to demonstrate the problem to the way I initially intended it to be. The real world scenario where I am facing this problem is like this:\nI am training to separate stages with the tensorflow object detection framework. Then I am connecting them together for inference (the second graph gets the cropped detections from the first stage as inputs). I import the two graphs with different name scopes, but still I get this error message unless I go into deep into the tensorflow code and add a name scope right to the beginning of \"shape_utils.static_or_dynamic_map_fn\", which I then have to change for each graph when running the provided export script.\nThis can't be the way it's supposed to be. When I import the 2 graphs with different name scopes, I expect to be able to connect them. If I'd get a frozen graph from someone else, I couldn't go in and export it again with a different map_fn name scope like my current workaround. The error message is even stating \"This is an internal bug, please file a bug report with instructions on how to reproduce the error.\". Please look into the provided example seriously:\nThe rename_second= True in connect_graphs(both_are_map_graphs=True, rename_second=True)\ncauses the map_fn name scope to be renamed in the graph BUILD not when IMPORTING. connect_graphs(both_are_map_graphs=True, rename_second=False) still imports the graphs with two different names, but here the bug is caused.\n\n@stefsietz I add some logging code to the function def connect_graphs() as below. When connect_graphs(both_are_map_graphs=True, rename_second=False), the output will be Graph1 scope name: ; and Graph2 scope name: ;. That means they have the same scope name. Is my understanding correct?\nSure, you can submit a PR. The expert in TensorFlow group will help you.\ndef connect_graphs(both_are_map_graphs=False, rename_second=False):\n    test_data = np.asarray([[1.0], [2.0]], dtype=np.float32)\n\n    graph1 = build_map_graph()\n    g1_graph_def = graph1.as_graph_def()\n    ##################################################\n    print(\"Graph1 scope name: {};\".format(graph1.get_name_scope()))\n    ##################################################\n\n    g2_map_name_scope = 'graph2_map' if rename_second else ''\n    graph2 = build_map_graph(g2_map_name_scope) if both_are_map_graphs else build_simple_graph()\n    ##################################################\n    print(\"Graph2 scope name: {};\".format(graph2.get_name_scope()))\n    ##################################################\n\n    g2_graph_def = graph2.as_graph_def()\n\n    connected_graph = tf.Graph()\n\n    with connected_graph.as_default():\n        tf.import_graph_def(g1_graph_def, name='graph1')\n\n        g1_output_tensor = tf.get_default_graph().get_tensor_by_name('graph1/output:0')\n        g1_input_tensor = tf.get_default_graph().get_tensor_by_name('graph1/input:0')\n\n        tf.import_graph_def(g2_graph_def, name='graph2', input_map={'input': g1_output_tensor})\n        g2_output_tensor = tf.get_default_graph().get_tensor_by_name('graph2/output:0')\n\n        with tf.Session() as sess:\n            result = sess.run(g2_output_tensor, {g1_input_tensor: test_data})\n            print(result)", "body": "> @feihugis no, this does not solve the problem, this merely corrects the minimal example I provided to demonstrate the problem to the way I initially intended it to be. The real world scenario where I am facing this problem is like this:\r\n> I am training to separate stages with the tensorflow object detection framework. Then I am connecting them together for inference (the second graph gets the cropped detections from the first stage as inputs). I import the two graphs with different name scopes, but still I get this error message unless I go into deep into the tensorflow code and add a name scope right to the beginning of \"shape_utils.static_or_dynamic_map_fn\", which I then have to change for each graph when running the provided export script.\r\n> \r\n> This can't be the way it's supposed to be. When I import the 2 graphs with different name scopes, I expect to be able to connect them. If I'd get a frozen graph from someone else, I couldn't go in and export it again with a different map_fn name scope like my current workaround. The error message is even stating `\"This is an internal bug, please file a bug report with instructions on how to reproduce the error.\"`. Please look into the provided example seriously:\r\n> The rename_second= True in `connect_graphs(both_are_map_graphs=True, rename_second=True)`\r\n> causes the map_fn name scope to be renamed in the graph BUILD not when IMPORTING. `connect_graphs(both_are_map_graphs=True, rename_second=False)` still imports the graphs with two different names, but here the bug is caused.\r\n\r\n@stefsietz I add some logging code to the function `def connect_graphs()` as below. When `connect_graphs(both_are_map_graphs=True, rename_second=False)`, the output will be `Graph1 scope name: ;` and `Graph2 scope name: ;`. That means they have the same scope name. Is my understanding correct?\r\n\r\nSure, you can submit a PR. The expert in TensorFlow group will help you.\r\n```\r\ndef connect_graphs(both_are_map_graphs=False, rename_second=False):\r\n    test_data = np.asarray([[1.0], [2.0]], dtype=np.float32)\r\n\r\n    graph1 = build_map_graph()\r\n    g1_graph_def = graph1.as_graph_def()\r\n    ##################################################\r\n    print(\"Graph1 scope name: {};\".format(graph1.get_name_scope()))\r\n    ##################################################\r\n\r\n    g2_map_name_scope = 'graph2_map' if rename_second else ''\r\n    graph2 = build_map_graph(g2_map_name_scope) if both_are_map_graphs else build_simple_graph()\r\n    ##################################################\r\n    print(\"Graph2 scope name: {};\".format(graph2.get_name_scope()))\r\n    ##################################################\r\n\r\n    g2_graph_def = graph2.as_graph_def()\r\n\r\n    connected_graph = tf.Graph()\r\n\r\n    with connected_graph.as_default():\r\n        tf.import_graph_def(g1_graph_def, name='graph1')\r\n\r\n        g1_output_tensor = tf.get_default_graph().get_tensor_by_name('graph1/output:0')\r\n        g1_input_tensor = tf.get_default_graph().get_tensor_by_name('graph1/input:0')\r\n\r\n        tf.import_graph_def(g2_graph_def, name='graph2', input_map={'input': g1_output_tensor})\r\n        g2_output_tensor = tf.get_default_graph().get_tensor_by_name('graph2/output:0')\r\n\r\n        with tf.Session() as sess:\r\n            result = sess.run(g2_output_tensor, {g1_input_tensor: test_data})\r\n            print(result)\r\n```\r\n"}