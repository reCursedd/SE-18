{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15977", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15977/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15977/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15977/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15977", "id": 287145709, "node_id": "MDU6SXNzdWUyODcxNDU3MDk=", "number": 15977, "title": "Improve video input pipeline (using TFRecord files)", "user": {"login": "tomrunia", "id": 5536129, "node_id": "MDQ6VXNlcjU1MzYxMjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5536129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomrunia", "html_url": "https://github.com/tomrunia", "followers_url": "https://api.github.com/users/tomrunia/followers", "following_url": "https://api.github.com/users/tomrunia/following{/other_user}", "gists_url": "https://api.github.com/users/tomrunia/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomrunia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomrunia/subscriptions", "organizations_url": "https://api.github.com/users/tomrunia/orgs", "repos_url": "https://api.github.com/users/tomrunia/repos", "events_url": "https://api.github.com/users/tomrunia/events{/privacy}", "received_events_url": "https://api.github.com/users/tomrunia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-01-09T16:17:52Z", "updated_at": "2018-05-23T08:40:02Z", "closed_at": "2018-02-22T23:46:23Z", "author_association": "NONE", "body_html": "<p>I am building a video input pipeline for DeepMind's <a href=\"https://deepmind.com/research/open-source/open-source-datasets/kinetics/\" rel=\"nofollow\">Kinetics dataset</a> using TFRecord files. Since the dataset is large (200k videos) my TFRecord files store the frames as compressed JPG images; otherwise it would require too much space on disk. Each <code>tf.train.Example</code> has the following structure:</p>\n<pre><code>Example {\n  'num_frames': tf.int64,\n  'label': tf.int64,\n  'frames/0001': tf.string,\n  'frames/0002': tf.string,\n  ...\n}\n</code></pre>\n<p>Where all the frames store a JPG image as compressed bytes. Using <code>tf.data.TFRecordDataset</code> and <code>tf.image.decode_jpg</code> I am able to load the images and decode from JPG into <code>tf.uint8</code> tensors (full code can be found <a href=\"https://github.com/tomrunia/TF_VideoInputPipeline/blob/master/kinetics/input_pipeline.py\">here</a>):</p>\n<pre><code>def decode(serialized_example):\n  \n    # Prepare feature list; read encoded JPG images as bytes\n    features = dict()\n    features[\"class_label\"] = tf.FixedLenFeature((), tf.int64)\n    for i in range(64):\n        features[\"frames/{:04d}\".format(i)] = tf.FixedLenFeature((), tf.string)\n\n    # Parse into tensors\n    parsed_features = tf.parse_single_example(serialized_example, features)\n\n    # Decode the encoded JPG images\n    images = []\n    for i in range(64):\n        images.append(tf.image.decode_jpeg(parsed_features[\"frames/{:04d}\".format(i)]))\n\n    # Pack the frames into one big tensor of shape (N,H,W,3)\n    images = tf.stack(images)\n    label  = tf.cast(parsed_features['class_label'], tf.int64)\n\n    return images, label\n</code></pre>\n<p>Two things currently seem impossible with the current features of TFRecord files:</p>\n<ol>\n<li>There seems to be no way to take a random sample of frames. The code example now takes the first 64 frames from the TFRecord, but what is often preferred is taking a random sample of consecutive frames. In one of my failed attempts I have tried to accomplish this along the lines of:</li>\n</ol>\n<pre><code>num_frames = tf.cast(parsed_features['num_frames'], tf.int64)\noffset = tf.random_uniform(shape=(), minval=0, maxval=label, dtype=tf.int64)\n</code></pre>\n<ol start=\"2\">\n<li>The number of frames in the video example seems impossible to access in TensorFlow. It can be obtained using <code> tf.train.Example.FromString</code> as given <a href=\"https://stackoverflow.com/a/42402484/3419427\" rel=\"nofollow\">here</a>, but that does not help me in this case. If this was possible I could just load all the video frames into a tensor (at increased cost...) and than use <code>tf.random_crop</code> to sample a random number of frames from the video.</li>\n</ol>\n<p>My overall question is whether the input pipeline for videos using TFRecord files can be improved? This needs to consider speed of reading data and compression options to limit file size for enormous  datasets. It would be convenient to directly use mp4 streams with TFRecord files, however decoding this is problably much slower than decoding JPG images (<strong>EDIT</strong>: this pull request is related: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"259908240\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13242\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/13242/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/13242\">#13242</a>)</p>\n<p>Note that there are many ways to setup the data pipeline for videos. I have described some of them in this post on StackOverflow and motivated why I chose for TFRecord files. This post also describes the problem described here, so it may be informative: <a href=\"https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file\" rel=\"nofollow\">https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file</a></p>\n<p>Have I written custom code: N/A<br>\nOS Platform and Distribution: N/A<br>\nTensorFlow installed from: N/A<br>\nTensorFlow version: N/A<br>\nBazel version: N/A<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: N/A<br>\nExact command to reproduce: N/A</p>", "body_text": "I am building a video input pipeline for DeepMind's Kinetics dataset using TFRecord files. Since the dataset is large (200k videos) my TFRecord files store the frames as compressed JPG images; otherwise it would require too much space on disk. Each tf.train.Example has the following structure:\nExample {\n  'num_frames': tf.int64,\n  'label': tf.int64,\n  'frames/0001': tf.string,\n  'frames/0002': tf.string,\n  ...\n}\n\nWhere all the frames store a JPG image as compressed bytes. Using tf.data.TFRecordDataset and tf.image.decode_jpg I am able to load the images and decode from JPG into tf.uint8 tensors (full code can be found here):\ndef decode(serialized_example):\n  \n    # Prepare feature list; read encoded JPG images as bytes\n    features = dict()\n    features[\"class_label\"] = tf.FixedLenFeature((), tf.int64)\n    for i in range(64):\n        features[\"frames/{:04d}\".format(i)] = tf.FixedLenFeature((), tf.string)\n\n    # Parse into tensors\n    parsed_features = tf.parse_single_example(serialized_example, features)\n\n    # Decode the encoded JPG images\n    images = []\n    for i in range(64):\n        images.append(tf.image.decode_jpeg(parsed_features[\"frames/{:04d}\".format(i)]))\n\n    # Pack the frames into one big tensor of shape (N,H,W,3)\n    images = tf.stack(images)\n    label  = tf.cast(parsed_features['class_label'], tf.int64)\n\n    return images, label\n\nTwo things currently seem impossible with the current features of TFRecord files:\n\nThere seems to be no way to take a random sample of frames. The code example now takes the first 64 frames from the TFRecord, but what is often preferred is taking a random sample of consecutive frames. In one of my failed attempts I have tried to accomplish this along the lines of:\n\nnum_frames = tf.cast(parsed_features['num_frames'], tf.int64)\noffset = tf.random_uniform(shape=(), minval=0, maxval=label, dtype=tf.int64)\n\n\nThe number of frames in the video example seems impossible to access in TensorFlow. It can be obtained using  tf.train.Example.FromString as given here, but that does not help me in this case. If this was possible I could just load all the video frames into a tensor (at increased cost...) and than use tf.random_crop to sample a random number of frames from the video.\n\nMy overall question is whether the input pipeline for videos using TFRecord files can be improved? This needs to consider speed of reading data and compression options to limit file size for enormous  datasets. It would be convenient to directly use mp4 streams with TFRecord files, however decoding this is problably much slower than decoding JPG images (EDIT: this pull request is related: #13242)\nNote that there are many ways to setup the data pipeline for videos. I have described some of them in this post on StackOverflow and motivated why I chose for TFRecord files. This post also describes the problem described here, so it may be informative: https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file\nHave I written custom code: N/A\nOS Platform and Distribution: N/A\nTensorFlow installed from: N/A\nTensorFlow version: N/A\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A", "body": "I am building a video input pipeline for DeepMind's [Kinetics dataset](https://deepmind.com/research/open-source/open-source-datasets/kinetics/) using TFRecord files. Since the dataset is large (200k videos) my TFRecord files store the frames as compressed JPG images; otherwise it would require too much space on disk. Each `tf.train.Example` has the following structure:\r\n\r\n```\r\nExample {\r\n  'num_frames': tf.int64,\r\n  'label': tf.int64,\r\n  'frames/0001': tf.string,\r\n  'frames/0002': tf.string,\r\n  ...\r\n}\r\n```\r\n\r\nWhere all the frames store a JPG image as compressed bytes. Using `tf.data.TFRecordDataset` and `tf.image.decode_jpg` I am able to load the images and decode from JPG into `tf.uint8` tensors (full code can be found [here](https://github.com/tomrunia/TF_VideoInputPipeline/blob/master/kinetics/input_pipeline.py)):\r\n\r\n```\r\ndef decode(serialized_example):\r\n  \r\n    # Prepare feature list; read encoded JPG images as bytes\r\n    features = dict()\r\n    features[\"class_label\"] = tf.FixedLenFeature((), tf.int64)\r\n    for i in range(64):\r\n        features[\"frames/{:04d}\".format(i)] = tf.FixedLenFeature((), tf.string)\r\n\r\n    # Parse into tensors\r\n    parsed_features = tf.parse_single_example(serialized_example, features)\r\n\r\n    # Decode the encoded JPG images\r\n    images = []\r\n    for i in range(64):\r\n        images.append(tf.image.decode_jpeg(parsed_features[\"frames/{:04d}\".format(i)]))\r\n\r\n    # Pack the frames into one big tensor of shape (N,H,W,3)\r\n    images = tf.stack(images)\r\n    label  = tf.cast(parsed_features['class_label'], tf.int64)\r\n\r\n    return images, label\r\n```\r\nTwo things currently seem impossible with the current features of TFRecord files:\r\n\r\n1. There seems to be no way to take a random sample of frames. The code example now takes the first 64 frames from the TFRecord, but what is often preferred is taking a random sample of consecutive frames. In one of my failed attempts I have tried to accomplish this along the lines of:\r\n\r\n```\r\nnum_frames = tf.cast(parsed_features['num_frames'], tf.int64)\r\noffset = tf.random_uniform(shape=(), minval=0, maxval=label, dtype=tf.int64)\r\n```\r\n\r\n2. The number of frames in the video example seems impossible to access in TensorFlow. It can be obtained using ` tf.train.Example.FromString` as given [here](https://stackoverflow.com/a/42402484/3419427), but that does not help me in this case. If this was possible I could just load all the video frames into a tensor (at increased cost...) and than use `tf.random_crop` to sample a random number of frames from the video. \r\n\r\nMy overall question is whether the input pipeline for videos using TFRecord files can be improved? This needs to consider speed of reading data and compression options to limit file size for enormous  datasets. It would be convenient to directly use mp4 streams with TFRecord files, however decoding this is problably much slower than decoding JPG images (**EDIT**: this pull request is related: https://github.com/tensorflow/tensorflow/pull/13242)\r\n\r\nNote that there are many ways to setup the data pipeline for videos. I have described some of them in this post on StackOverflow and motivated why I chose for TFRecord files. This post also describes the problem described here, so it may be informative: https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n  "}