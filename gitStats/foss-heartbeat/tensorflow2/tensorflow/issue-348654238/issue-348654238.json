{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21475", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21475/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21475/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21475/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21475", "id": 348654238, "node_id": "MDU6SXNzdWUzNDg2NTQyMzg=", "number": 21475, "title": "FusedBatchNorm problem during benchmark test", "user": {"login": "DavideCatto", "id": 20124934, "node_id": "MDQ6VXNlcjIwMTI0OTM0", "avatar_url": "https://avatars0.githubusercontent.com/u/20124934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavideCatto", "html_url": "https://github.com/DavideCatto", "followers_url": "https://api.github.com/users/DavideCatto/followers", "following_url": "https://api.github.com/users/DavideCatto/following{/other_user}", "gists_url": "https://api.github.com/users/DavideCatto/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavideCatto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavideCatto/subscriptions", "organizations_url": "https://api.github.com/users/DavideCatto/orgs", "repos_url": "https://api.github.com/users/DavideCatto/repos", "events_url": "https://api.github.com/users/DavideCatto/events{/privacy}", "received_events_url": "https://api.github.com/users/DavideCatto/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-08-08T09:51:54Z", "updated_at": "2018-11-21T19:01:14Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: None</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: 1.9</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.11.1 (for covert and benchmark the model)</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: None, installed from build</li>\n<li><strong>CUDA/cuDNN version</strong>: Not used</li>\n<li><strong>GPU model and memory</strong>: Not used</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Problem description</h3>\n<p>I'm trying to convert a piece of mobilenetV1 (using slim model) from the master repository <a href=\"url\">https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md</a>; In particular I have added 2 simple convolution after the <em>MobilenetV1/Conv2d_8_pointwise/Conv2D</em>. Obviously the number of parameters is more less than the original MobilenetV1. Without train the model, I have initialized all the variables and save them into a freezed model. Now If I visualize the model with tensorboard is the same of that downloaded from the link above until the <em>MobilenetV1/Conv2d_8_pointwise/Conv2D</em> (It compare the Conv2D and the FusedBatchNorm, Gamma, Beta, var and so on). The problems cames when I try to benchmark the model using bazel: the original one is more faster than mine <em>(FLOPs estimated 1.14B VS 3.31B)</em> and also in summaries of the original model the <em>FusedBatchNorm</em> Node type doesn't compare instead in mine model yes. Where am I doing wrong? And there is also the possibility of combining the conv2D and the batch norm layer?</p>\n<h3>logs</h3>\n<p>------------------------- ORIGINAL MOBILENETV1 ---------------------------------------------------------------</p>\n<blockquote>\n<p>...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]<br>\n...stat_summarizer.cc:468] \t                  Conv2D\t       15\t    52.027\t    56.168%\t    56.168%\t 12447.652\t       15<br>\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t       13\t    15.413\t    16.640%\t    72.808%\t  7905.664\t       13<br>\n...stat_summarizer.cc:468] \t                     <strong>Mul</strong>\t       27\t    12.018\t    12.975%\t    85.783%\t     0.000\t       27<br>\n...stat_summarizer.cc:468] \t                     <strong>Add</strong>\t       27\t    11.806\t    12.746%\t    98.529%\t     0.000\t       27<br>\n...stat_summarizer.cc:468] \t                   <strong>Relu6</strong>\t       27\t     1.163\t     1.256%\t    99.784%\t     0.000\t       27<br>\n...stat_summarizer.cc:468] \t                   Const\t       83\t     0.120\t     0.130%\t    99.914%\t     0.000\t       83<br>\n...stat_summarizer.cc:468] \t                 Softmax\t        1\t     0.027\t     0.029%\t    99.943%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                 AvgPool\t        1\t     0.022\t     0.024%\t    99.967%\t     4.096\t        1<br>\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.007\t     0.008%\t    99.974%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                 BiasAdd\t        1\t     0.007\t     0.008%\t    99.982%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                 Squeeze\t        1\t     0.006\t     0.006%\t    99.988%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.005%\t    99.994%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.004%\t    99.998%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                Identity\t        1\t     0.002\t     0.002%\t   100.000%\t     0.000\t        1</p>\n</blockquote>\n<hr>\n<p>---------------------------- MY MODEL ------------------------------------------------------------------------</p>\n<blockquote>\n<p>...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]<br>\n...stat_summarizer.cc:468] \t          <strong>FusedBatchNorm</strong>\t       18\t   151.477\t    48.975%\t    48.975%\t    72.704\t       18<br>\n...stat_summarizer.cc:468] \t                  Conv2D\t       11\t   138.036\t    44.630%\t    93.605%\t 15457.344\t       11<br>\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t        8\t    18.148\t     5.868%\t    99.472%\t  9300.352\t        8<br>\n...stat_summarizer.cc:468] \t                    Relu\t       18\t     1.581\t     0.511%\t    99.984%\t     0.000\t       18<br>\n...stat_summarizer.cc:468] \t                   Const\t       29\t     0.038\t     0.012%\t    99.996%\t     0.000\t       29<br>\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.002%\t    99.997%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.001%\t    99.999%\t     0.000\t        1<br>\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.004\t     0.001%\t   100.000%\t     0.000\t        1</p>\n</blockquote>\n<hr>\n<p>OK, I have noted the differen Relu activation but I do not think that's the problem for the fusedbatchnorm layer..</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\nTensorFlow installed from (source or binary): 1.9\nTensorFlow version (use command below): 1.9\nPython version: 3.6\nBazel version (if compiling from source): 0.11.1 (for covert and benchmark the model)\nGCC/Compiler version (if compiling from source): None, installed from build\nCUDA/cuDNN version: Not used\nGPU model and memory: Not used\nExact command to reproduce:\n\nProblem description\nI'm trying to convert a piece of mobilenetV1 (using slim model) from the master repository https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md; In particular I have added 2 simple convolution after the MobilenetV1/Conv2d_8_pointwise/Conv2D. Obviously the number of parameters is more less than the original MobilenetV1. Without train the model, I have initialized all the variables and save them into a freezed model. Now If I visualize the model with tensorboard is the same of that downloaded from the link above until the MobilenetV1/Conv2d_8_pointwise/Conv2D (It compare the Conv2D and the FusedBatchNorm, Gamma, Beta, var and so on). The problems cames when I try to benchmark the model using bazel: the original one is more faster than mine (FLOPs estimated 1.14B VS 3.31B) and also in summaries of the original model the FusedBatchNorm Node type doesn't compare instead in mine model yes. Where am I doing wrong? And there is also the possibility of combining the conv2D and the batch norm layer?\nlogs\n------------------------- ORIGINAL MOBILENETV1 ---------------------------------------------------------------\n\n...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\n...stat_summarizer.cc:468] \t                  Conv2D\t       15\t    52.027\t    56.168%\t    56.168%\t 12447.652\t       15\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t       13\t    15.413\t    16.640%\t    72.808%\t  7905.664\t       13\n...stat_summarizer.cc:468] \t                     Mul\t       27\t    12.018\t    12.975%\t    85.783%\t     0.000\t       27\n...stat_summarizer.cc:468] \t                     Add\t       27\t    11.806\t    12.746%\t    98.529%\t     0.000\t       27\n...stat_summarizer.cc:468] \t                   Relu6\t       27\t     1.163\t     1.256%\t    99.784%\t     0.000\t       27\n...stat_summarizer.cc:468] \t                   Const\t       83\t     0.120\t     0.130%\t    99.914%\t     0.000\t       83\n...stat_summarizer.cc:468] \t                 Softmax\t        1\t     0.027\t     0.029%\t    99.943%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                 AvgPool\t        1\t     0.022\t     0.024%\t    99.967%\t     4.096\t        1\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.007\t     0.008%\t    99.974%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                 BiasAdd\t        1\t     0.007\t     0.008%\t    99.982%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                 Squeeze\t        1\t     0.006\t     0.006%\t    99.988%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.005%\t    99.994%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.004%\t    99.998%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                Identity\t        1\t     0.002\t     0.002%\t   100.000%\t     0.000\t        1\n\n\n---------------------------- MY MODEL ------------------------------------------------------------------------\n\n...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\n...stat_summarizer.cc:468] \t          FusedBatchNorm\t       18\t   151.477\t    48.975%\t    48.975%\t    72.704\t       18\n...stat_summarizer.cc:468] \t                  Conv2D\t       11\t   138.036\t    44.630%\t    93.605%\t 15457.344\t       11\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t        8\t    18.148\t     5.868%\t    99.472%\t  9300.352\t        8\n...stat_summarizer.cc:468] \t                    Relu\t       18\t     1.581\t     0.511%\t    99.984%\t     0.000\t       18\n...stat_summarizer.cc:468] \t                   Const\t       29\t     0.038\t     0.012%\t    99.996%\t     0.000\t       29\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.002%\t    99.997%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.001%\t    99.999%\t     0.000\t        1\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.004\t     0.001%\t   100.000%\t     0.000\t        1\n\n\nOK, I have noted the differen Relu activation but I do not think that's the problem for the fusedbatchnorm layer..", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: 1.9\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.11.1 (for covert and benchmark the model)\r\n- **GCC/Compiler version (if compiling from source)**: None, installed from build\r\n- **CUDA/cuDNN version**: Not used\r\n- **GPU model and memory**: Not used\r\n- **Exact command to reproduce**:\r\n\r\n### Problem description\r\nI'm trying to convert a piece of mobilenetV1 (using slim model) from the master repository [https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md](url); In particular I have added 2 simple convolution after the _MobilenetV1/Conv2d_8_pointwise/Conv2D_. Obviously the number of parameters is more less than the original MobilenetV1. Without train the model, I have initialized all the variables and save them into a freezed model. Now If I visualize the model with tensorboard is the same of that downloaded from the link above until the _MobilenetV1/Conv2d_8_pointwise/Conv2D_ (It compare the Conv2D and the FusedBatchNorm, Gamma, Beta, var and so on). The problems cames when I try to benchmark the model using bazel: the original one is more faster than mine _(FLOPs estimated 1.14B VS 3.31B)_ and also in summaries of the original model the _FusedBatchNorm_ Node type doesn't compare instead in mine model yes. Where am I doing wrong? And there is also the possibility of combining the conv2D and the batch norm layer?\r\n\r\n### logs\r\n\r\n\r\n------------------------- ORIGINAL MOBILENETV1 ---------------------------------------------------------------\r\n\r\n>...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n...stat_summarizer.cc:468] \t                  Conv2D\t       15\t    52.027\t    56.168%\t    56.168%\t 12447.652\t       15\r\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t       13\t    15.413\t    16.640%\t    72.808%\t  7905.664\t       13\r\n...stat_summarizer.cc:468] \t                     **Mul**\t       27\t    12.018\t    12.975%\t    85.783%\t     0.000\t       27\r\n...stat_summarizer.cc:468] \t                     **Add**\t       27\t    11.806\t    12.746%\t    98.529%\t     0.000\t       27\r\n...stat_summarizer.cc:468] \t                   **Relu6**\t       27\t     1.163\t     1.256%\t    99.784%\t     0.000\t       27\r\n...stat_summarizer.cc:468] \t                   Const\t       83\t     0.120\t     0.130%\t    99.914%\t     0.000\t       83\r\n...stat_summarizer.cc:468] \t                 Softmax\t        1\t     0.027\t     0.029%\t    99.943%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 AvgPool\t        1\t     0.022\t     0.024%\t    99.967%\t     4.096\t        1\r\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.007\t     0.008%\t    99.974%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 BiasAdd\t        1\t     0.007\t     0.008%\t    99.982%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 Squeeze\t        1\t     0.006\t     0.006%\t    99.988%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.005%\t    99.994%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.004%\t    99.998%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                Identity\t        1\t     0.002\t     0.002%\t   100.000%\t     0.000\t        1\r\n--------------------------------------------------------------------------------------------------------------\r\n\r\n---------------------------- MY MODEL ------------------------------------------------------------------------\r\n>...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n...stat_summarizer.cc:468] \t          **FusedBatchNorm**\t       18\t   151.477\t    48.975%\t    48.975%\t    72.704\t       18\r\n...stat_summarizer.cc:468] \t                  Conv2D\t       11\t   138.036\t    44.630%\t    93.605%\t 15457.344\t       11\r\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t        8\t    18.148\t     5.868%\t    99.472%\t  9300.352\t        8\r\n...stat_summarizer.cc:468] \t                    Relu\t       18\t     1.581\t     0.511%\t    99.984%\t     0.000\t       18\r\n...stat_summarizer.cc:468] \t                   Const\t       29\t     0.038\t     0.012%\t    99.996%\t     0.000\t       29\r\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.002%\t    99.997%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.001%\t    99.999%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.004\t     0.001%\t   100.000%\t     0.000\t        1\r\n--------------------------------------------------------------------------------------------------------------\r\nOK, I have noted the differen Relu activation but I do not think that's the problem for the fusedbatchnorm layer.."}