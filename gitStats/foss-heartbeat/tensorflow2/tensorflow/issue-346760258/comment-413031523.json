{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413031523", "html_url": "https://github.com/tensorflow/tensorflow/issues/21317#issuecomment-413031523", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21317", "id": 413031523, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzAzMTUyMw==", "user": {"login": "rjpower", "id": 607207, "node_id": "MDQ6VXNlcjYwNzIwNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/607207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rjpower", "html_url": "https://github.com/rjpower", "followers_url": "https://api.github.com/users/rjpower/followers", "following_url": "https://api.github.com/users/rjpower/following{/other_user}", "gists_url": "https://api.github.com/users/rjpower/gists{/gist_id}", "starred_url": "https://api.github.com/users/rjpower/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rjpower/subscriptions", "organizations_url": "https://api.github.com/users/rjpower/orgs", "repos_url": "https://api.github.com/users/rjpower/repos", "events_url": "https://api.github.com/users/rjpower/events{/privacy}", "received_events_url": "https://api.github.com/users/rjpower/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-14T22:05:43Z", "updated_at": "2018-08-14T22:05:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p><code>bfloat16</code> support isn't complete for GPUs, as it's not supported natively by the devices.</p>\n<p>For performance you'll want to use float32 or float16 for GPU execution (though float16 can be difficult to train models with).  TPUs support bfloat16 for effectively all operations (but you currently have to migrate your model to work on the TPU).</p>", "body_text": "bfloat16 support isn't complete for GPUs, as it's not supported natively by the devices.\nFor performance you'll want to use float32 or float16 for GPU execution (though float16 can be difficult to train models with).  TPUs support bfloat16 for effectively all operations (but you currently have to migrate your model to work on the TPU).", "body": "`bfloat16` support isn't complete for GPUs, as it's not supported natively by the devices.\r\n\r\nFor performance you'll want to use float32 or float16 for GPU execution (though float16 can be difficult to train models with).  TPUs support bfloat16 for effectively all operations (but you currently have to migrate your model to work on the TPU)."}