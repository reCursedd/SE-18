{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17958", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17958/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17958/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17958/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17958", "id": 308119407, "node_id": "MDU6SXNzdWUzMDgxMTk0MDc=", "number": 17958, "title": "Shuffling slows down iterator at consumption", "user": {"login": "kvanhoey", "id": 22071874, "node_id": "MDQ6VXNlcjIyMDcxODc0", "avatar_url": "https://avatars2.githubusercontent.com/u/22071874?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kvanhoey", "html_url": "https://github.com/kvanhoey", "followers_url": "https://api.github.com/users/kvanhoey/followers", "following_url": "https://api.github.com/users/kvanhoey/following{/other_user}", "gists_url": "https://api.github.com/users/kvanhoey/gists{/gist_id}", "starred_url": "https://api.github.com/users/kvanhoey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kvanhoey/subscriptions", "organizations_url": "https://api.github.com/users/kvanhoey/orgs", "repos_url": "https://api.github.com/users/kvanhoey/repos", "events_url": "https://api.github.com/users/kvanhoey/events{/privacy}", "received_events_url": "https://api.github.com/users/kvanhoey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-03-23T17:34:04Z", "updated_at": "2018-03-27T17:58:48Z", "closed_at": "2018-03-27T17:58:48Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 17.10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.1 (v1.4.1-9-gc646af1957)</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 6.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>: NVidia GeForce GTX 1060 6GB</li>\n<li><strong>Exact command to reproduce</strong>: python3 minimal_shuffle_bug.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Adding shuffling during the definition of a <code>tf.data.Dataset</code> Graph results in a progressive slowdown during the iterations that consume the data, up to the start of a new epoch (when restarting an iteration over the full dataset). I believe this is a bug, or if it's a normal state of things given internal memory usage in the dataset pipeline, then I'd like to request this to be documented in the documentation, e.g., in the <a href=\"https://www.tensorflow.org/versions/master/performance/datasets_performance\" rel=\"nofollow\">Input Pipeline Performance Guide</a>.</p>\n<h3>Source code / logs</h3>\n<p>A minimal working code example follows at the end of this post. The code does the same experiment twice, once with and once without dataset shuffling. Each experiment consumes 3 epochs of a dataset, within which one can notice slower and slower iterations (when shuffling is enabled), with a speed increase at each new start of epoch.<br>\nMy command-line output is the following:</p>\n<blockquote>\n<p>$ python3 minimal_shuffle_bug.py</p>\n</blockquote>\n<blockquote>\n<p>=== START TEST WITH SHUFFLING DISABLED ===<br>\n2018-03-23 18:15:47.136029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:<br>\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715<br>\npciBusID: 0000:01:00.0<br>\ntotalMemory: 5.93GiB freeMemory: 4.91GiB<br>\n2018-03-23 18:15:47.136056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)<br>\niteration 99: last 100 iterations took 2.96ms<br>\niteration 199: last 100 iterations took 2.49ms<br>\niteration 299: last 100 iterations took 2.47ms<br>\niteration 399: last 100 iterations took 2.47ms<br>\niteration 499: last 100 iterations took 2.33ms<br>\niteration 599: last 100 iterations took 2.32ms<br>\niteration 699: last 100 iterations took 2.06ms<br>\niteration 799: last 100 iterations took 2.06ms<br>\niteration 899: last 100 iterations took 2.06ms<br>\niteration 999: last 100 iterations took 2.01ms<br>\nStart new epoch<br>\niteration 1099: last 100 iterations took 2.08ms<br>\niteration 1199: last 100 iterations took 2.07ms<br>\niteration 1299: last 100 iterations took 2.07ms<br>\niteration 1399: last 100 iterations took 2.07ms<br>\niteration 1499: last 100 iterations took 2.07ms<br>\niteration 1599: last 100 iterations took 2.08ms<br>\niteration 1699: last 100 iterations took 2.04ms<br>\niteration 1799: last 100 iterations took 2.08ms<br>\niteration 1899: last 100 iterations took 2.07ms<br>\niteration 1999: last 100 iterations took 2.07ms<br>\nStart new epoch<br>\niteration 2099: last 100 iterations took 2.06ms<br>\niteration 2199: last 100 iterations took 2.07ms<br>\niteration 2299: last 100 iterations took 2.07ms<br>\niteration 2399: last 100 iterations took 2.01ms<br>\niteration 2499: last 100 iterations took 2.01ms<br>\niteration 2599: last 100 iterations took 2.01ms<br>\niteration 2699: last 100 iterations took 2.05ms<br>\niteration 2799: last 100 iterations took 2.01ms<br>\niteration 2899: last 100 iterations took 2.01ms<br>\niteration 2999: last 100 iterations took 2.01ms</p>\n</blockquote>\n<blockquote>\n<p>=== START TEST WITH SHUFFLING ENABLED ===<br>\n2018-03-23 18:15:53.605928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)<br>\niteration 99: last 100 iterations took 12.77ms<br>\niteration 199: last 100 iterations took 15.31ms<br>\niteration 299: last 100 iterations took 17.66ms<br>\niteration 399: last 100 iterations took 19.23ms<br>\niteration 499: last 100 iterations took 20.99ms<br>\niteration 599: last 100 iterations took 24.01ms<br>\niteration 699: last 100 iterations took 25.48ms<br>\niteration 799: last 100 iterations took 27.70ms<br>\niteration 899: last 100 iterations took 28.04ms<br>\niteration 999: last 100 iterations took 16.38ms<br>\nStart new epoch<br>\niteration 1099: last 100 iterations took 13.95ms<br>\niteration 1199: last 100 iterations took 16.74ms<br>\niteration 1299: last 100 iterations took 16.82ms<br>\niteration 1399: last 100 iterations took 20.15ms<br>\niteration 1499: last 100 iterations took 22.53ms<br>\niteration 1599: last 100 iterations took 23.19ms<br>\niteration 1699: last 100 iterations took 25.38ms<br>\niteration 1799: last 100 iterations took 25.96ms<br>\niteration 1899: last 100 iterations took 26.20ms<br>\niteration 1999: last 100 iterations took 18.47ms<br>\nStart new epoch<br>\niteration 2099: last 100 iterations took 12.99ms<br>\niteration 2199: last 100 iterations took 14.26ms<br>\niteration 2299: last 100 iterations took 17.93ms<br>\niteration 2399: last 100 iterations took 18.95ms<br>\niteration 2499: last 100 iterations took 20.89ms<br>\niteration 2599: last 100 iterations took 23.01ms<br>\niteration 2699: last 100 iterations took 25.17ms<br>\niteration 2799: last 100 iterations took 27.40ms<br>\niteration 2899: last 100 iterations took 26.44ms<br>\niteration 2999: last 100 iterations took 15.94ms</p>\n</blockquote>\n<p>And the code to reproduce this experiment:</p>\n<pre lang=\"import\" data-meta=\"tensorflow as tf\"><code>import time\n\nnum_data = 5000000\nnum_epoch = 3\nbatch_size = 5000\nnum_iters = num_data*num_epoch/batch_size\n\ndef test_shuffle(enable_shuffling):\n    # Define dataset\n    dataset = tf.data.Dataset.range(num_data)\n    if (enable_shuffling):\n        dataset = dataset.shuffle(num_data)\n    dataset = dataset.batch(batch_size)\n    iterator = dataset.make_initializable_iterator()\n    # Define next element op\n    x = iterator.get_next()\n\n    # Launch session\n    sess = tf.InteractiveSession()\n    sess.run(iterator.initializer)\n\n    # Consume iterator and time\n    for i in range(int(num_iters)):\n        try:\n            t1 = time.time()\n            res = sess.run(x)\n\n            if i%100 == 99:\n                t2 = time.time()\n                print ('iteration {0:d}: last 100 iterations took {1:0.2f}ms'.format(i,1000*(t2-t1)))\n        except tf.errors.OutOfRangeError:\n            sess.run(iterator.initializer)\n            print('Start new epoch')\n\nprint ('   === START TEST WITH SHUFFLING DISABLED === ')\ntest_shuffle(False)\nprint()\n\nprint ('   === START TEST WITH SHUFFLING ENABLED === ')\ntest_shuffle(True)\nprint()\n</code></pre>\n<p>Note: this bug report follows comments and discussion on bug report <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"243863096\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11591\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11591/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/11591\">#11591</a><br>\nI initially experienced this issue with the MNIST dataset (smaller dataset than the example attached, i.e., 60K iterations ;  but with more operations for dataset loading).</p>\n<p>Final note: I also tried the variant in which I do not capture the <code>OutOfRangeError</code> followed by initializing the iterator, but rather include a line <code>dataset = dataset.repeat(num_epoch)</code> in the dataset creation Graph. The results are similar.</p>\n<p>Thanks a lot.</p>\n<p>Kenneth</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.4.1 (v1.4.1-9-gc646af1957)\nPython version: 3.6.3\nBazel version (if compiling from source): 0.11.1\nGCC/Compiler version (if compiling from source): 6.4.0\nCUDA/cuDNN version: 9.0\nGPU model and memory: NVidia GeForce GTX 1060 6GB\nExact command to reproduce: python3 minimal_shuffle_bug.py\n\nDescribe the problem\nAdding shuffling during the definition of a tf.data.Dataset Graph results in a progressive slowdown during the iterations that consume the data, up to the start of a new epoch (when restarting an iteration over the full dataset). I believe this is a bug, or if it's a normal state of things given internal memory usage in the dataset pipeline, then I'd like to request this to be documented in the documentation, e.g., in the Input Pipeline Performance Guide.\nSource code / logs\nA minimal working code example follows at the end of this post. The code does the same experiment twice, once with and once without dataset shuffling. Each experiment consumes 3 epochs of a dataset, within which one can notice slower and slower iterations (when shuffling is enabled), with a speed increase at each new start of epoch.\nMy command-line output is the following:\n\n$ python3 minimal_shuffle_bug.py\n\n\n=== START TEST WITH SHUFFLING DISABLED ===\n2018-03-23 18:15:47.136029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715\npciBusID: 0000:01:00.0\ntotalMemory: 5.93GiB freeMemory: 4.91GiB\n2018-03-23 18:15:47.136056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\niteration 99: last 100 iterations took 2.96ms\niteration 199: last 100 iterations took 2.49ms\niteration 299: last 100 iterations took 2.47ms\niteration 399: last 100 iterations took 2.47ms\niteration 499: last 100 iterations took 2.33ms\niteration 599: last 100 iterations took 2.32ms\niteration 699: last 100 iterations took 2.06ms\niteration 799: last 100 iterations took 2.06ms\niteration 899: last 100 iterations took 2.06ms\niteration 999: last 100 iterations took 2.01ms\nStart new epoch\niteration 1099: last 100 iterations took 2.08ms\niteration 1199: last 100 iterations took 2.07ms\niteration 1299: last 100 iterations took 2.07ms\niteration 1399: last 100 iterations took 2.07ms\niteration 1499: last 100 iterations took 2.07ms\niteration 1599: last 100 iterations took 2.08ms\niteration 1699: last 100 iterations took 2.04ms\niteration 1799: last 100 iterations took 2.08ms\niteration 1899: last 100 iterations took 2.07ms\niteration 1999: last 100 iterations took 2.07ms\nStart new epoch\niteration 2099: last 100 iterations took 2.06ms\niteration 2199: last 100 iterations took 2.07ms\niteration 2299: last 100 iterations took 2.07ms\niteration 2399: last 100 iterations took 2.01ms\niteration 2499: last 100 iterations took 2.01ms\niteration 2599: last 100 iterations took 2.01ms\niteration 2699: last 100 iterations took 2.05ms\niteration 2799: last 100 iterations took 2.01ms\niteration 2899: last 100 iterations took 2.01ms\niteration 2999: last 100 iterations took 2.01ms\n\n\n=== START TEST WITH SHUFFLING ENABLED ===\n2018-03-23 18:15:53.605928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\niteration 99: last 100 iterations took 12.77ms\niteration 199: last 100 iterations took 15.31ms\niteration 299: last 100 iterations took 17.66ms\niteration 399: last 100 iterations took 19.23ms\niteration 499: last 100 iterations took 20.99ms\niteration 599: last 100 iterations took 24.01ms\niteration 699: last 100 iterations took 25.48ms\niteration 799: last 100 iterations took 27.70ms\niteration 899: last 100 iterations took 28.04ms\niteration 999: last 100 iterations took 16.38ms\nStart new epoch\niteration 1099: last 100 iterations took 13.95ms\niteration 1199: last 100 iterations took 16.74ms\niteration 1299: last 100 iterations took 16.82ms\niteration 1399: last 100 iterations took 20.15ms\niteration 1499: last 100 iterations took 22.53ms\niteration 1599: last 100 iterations took 23.19ms\niteration 1699: last 100 iterations took 25.38ms\niteration 1799: last 100 iterations took 25.96ms\niteration 1899: last 100 iterations took 26.20ms\niteration 1999: last 100 iterations took 18.47ms\nStart new epoch\niteration 2099: last 100 iterations took 12.99ms\niteration 2199: last 100 iterations took 14.26ms\niteration 2299: last 100 iterations took 17.93ms\niteration 2399: last 100 iterations took 18.95ms\niteration 2499: last 100 iterations took 20.89ms\niteration 2599: last 100 iterations took 23.01ms\niteration 2699: last 100 iterations took 25.17ms\niteration 2799: last 100 iterations took 27.40ms\niteration 2899: last 100 iterations took 26.44ms\niteration 2999: last 100 iterations took 15.94ms\n\nAnd the code to reproduce this experiment:\nimport time\n\nnum_data = 5000000\nnum_epoch = 3\nbatch_size = 5000\nnum_iters = num_data*num_epoch/batch_size\n\ndef test_shuffle(enable_shuffling):\n    # Define dataset\n    dataset = tf.data.Dataset.range(num_data)\n    if (enable_shuffling):\n        dataset = dataset.shuffle(num_data)\n    dataset = dataset.batch(batch_size)\n    iterator = dataset.make_initializable_iterator()\n    # Define next element op\n    x = iterator.get_next()\n\n    # Launch session\n    sess = tf.InteractiveSession()\n    sess.run(iterator.initializer)\n\n    # Consume iterator and time\n    for i in range(int(num_iters)):\n        try:\n            t1 = time.time()\n            res = sess.run(x)\n\n            if i%100 == 99:\n                t2 = time.time()\n                print ('iteration {0:d}: last 100 iterations took {1:0.2f}ms'.format(i,1000*(t2-t1)))\n        except tf.errors.OutOfRangeError:\n            sess.run(iterator.initializer)\n            print('Start new epoch')\n\nprint ('   === START TEST WITH SHUFFLING DISABLED === ')\ntest_shuffle(False)\nprint()\n\nprint ('   === START TEST WITH SHUFFLING ENABLED === ')\ntest_shuffle(True)\nprint()\n\nNote: this bug report follows comments and discussion on bug report #11591\nI initially experienced this issue with the MNIST dataset (smaller dataset than the example attached, i.e., 60K iterations ;  but with more operations for dataset loading).\nFinal note: I also tried the variant in which I do not capture the OutOfRangeError followed by initializing the iterator, but rather include a line dataset = dataset.repeat(num_epoch) in the dataset creation Graph. The results are similar.\nThanks a lot.\nKenneth", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.1 (v1.4.1-9-gc646af1957)\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVidia GeForce GTX 1060 6GB\r\n- **Exact command to reproduce**: python3 minimal_shuffle_bug.py\r\n\r\n### Describe the problem\r\nAdding shuffling during the definition of a `tf.data.Dataset` Graph results in a progressive slowdown during the iterations that consume the data, up to the start of a new epoch (when restarting an iteration over the full dataset). I believe this is a bug, or if it's a normal state of things given internal memory usage in the dataset pipeline, then I'd like to request this to be documented in the documentation, e.g., in the [Input Pipeline Performance Guide](https://www.tensorflow.org/versions/master/performance/datasets_performance).\r\n\r\n### Source code / logs\r\nA minimal working code example follows at the end of this post. The code does the same experiment twice, once with and once without dataset shuffling. Each experiment consumes 3 epochs of a dataset, within which one can notice slower and slower iterations (when shuffling is enabled), with a speed increase at each new start of epoch.\r\nMy command-line output is the following:\r\n\r\n> $ python3 minimal_shuffle_bug.py\r\n\r\n>    === START TEST WITH SHUFFLING DISABLED === \r\n2018-03-23 18:15:47.136029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 4.91GiB\r\n2018-03-23 18:15:47.136056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\niteration 99: last 100 iterations took 2.96ms\r\niteration 199: last 100 iterations took 2.49ms\r\niteration 299: last 100 iterations took 2.47ms\r\niteration 399: last 100 iterations took 2.47ms\r\niteration 499: last 100 iterations took 2.33ms\r\niteration 599: last 100 iterations took 2.32ms\r\niteration 699: last 100 iterations took 2.06ms\r\niteration 799: last 100 iterations took 2.06ms\r\niteration 899: last 100 iterations took 2.06ms\r\niteration 999: last 100 iterations took 2.01ms\r\nStart new epoch\r\niteration 1099: last 100 iterations took 2.08ms\r\niteration 1199: last 100 iterations took 2.07ms\r\niteration 1299: last 100 iterations took 2.07ms\r\niteration 1399: last 100 iterations took 2.07ms\r\niteration 1499: last 100 iterations took 2.07ms\r\niteration 1599: last 100 iterations took 2.08ms\r\niteration 1699: last 100 iterations took 2.04ms\r\niteration 1799: last 100 iterations took 2.08ms\r\niteration 1899: last 100 iterations took 2.07ms\r\niteration 1999: last 100 iterations took 2.07ms\r\nStart new epoch\r\niteration 2099: last 100 iterations took 2.06ms\r\niteration 2199: last 100 iterations took 2.07ms\r\niteration 2299: last 100 iterations took 2.07ms\r\niteration 2399: last 100 iterations took 2.01ms\r\niteration 2499: last 100 iterations took 2.01ms\r\niteration 2599: last 100 iterations took 2.01ms\r\niteration 2699: last 100 iterations took 2.05ms\r\niteration 2799: last 100 iterations took 2.01ms\r\niteration 2899: last 100 iterations took 2.01ms\r\niteration 2999: last 100 iterations took 2.01ms\r\n\r\n>    === START TEST WITH SHUFFLING ENABLED === \r\n2018-03-23 18:15:53.605928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\niteration 99: last 100 iterations took 12.77ms\r\niteration 199: last 100 iterations took 15.31ms\r\niteration 299: last 100 iterations took 17.66ms\r\niteration 399: last 100 iterations took 19.23ms\r\niteration 499: last 100 iterations took 20.99ms\r\niteration 599: last 100 iterations took 24.01ms\r\niteration 699: last 100 iterations took 25.48ms\r\niteration 799: last 100 iterations took 27.70ms\r\niteration 899: last 100 iterations took 28.04ms\r\niteration 999: last 100 iterations took 16.38ms\r\nStart new epoch\r\niteration 1099: last 100 iterations took 13.95ms\r\niteration 1199: last 100 iterations took 16.74ms\r\niteration 1299: last 100 iterations took 16.82ms\r\niteration 1399: last 100 iterations took 20.15ms\r\niteration 1499: last 100 iterations took 22.53ms\r\niteration 1599: last 100 iterations took 23.19ms\r\niteration 1699: last 100 iterations took 25.38ms\r\niteration 1799: last 100 iterations took 25.96ms\r\niteration 1899: last 100 iterations took 26.20ms\r\niteration 1999: last 100 iterations took 18.47ms\r\nStart new epoch\r\niteration 2099: last 100 iterations took 12.99ms\r\niteration 2199: last 100 iterations took 14.26ms\r\niteration 2299: last 100 iterations took 17.93ms\r\niteration 2399: last 100 iterations took 18.95ms\r\niteration 2499: last 100 iterations took 20.89ms\r\niteration 2599: last 100 iterations took 23.01ms\r\niteration 2699: last 100 iterations took 25.17ms\r\niteration 2799: last 100 iterations took 27.40ms\r\niteration 2899: last 100 iterations took 26.44ms\r\niteration 2999: last 100 iterations took 15.94ms\r\n\r\nAnd the code to reproduce this experiment:\r\n```import tensorflow as tf\r\nimport time\r\n\r\nnum_data = 5000000\r\nnum_epoch = 3\r\nbatch_size = 5000\r\nnum_iters = num_data*num_epoch/batch_size\r\n\r\ndef test_shuffle(enable_shuffling):\r\n    # Define dataset\r\n    dataset = tf.data.Dataset.range(num_data)\r\n    if (enable_shuffling):\r\n        dataset = dataset.shuffle(num_data)\r\n    dataset = dataset.batch(batch_size)\r\n    iterator = dataset.make_initializable_iterator()\r\n    # Define next element op\r\n    x = iterator.get_next()\r\n\r\n    # Launch session\r\n    sess = tf.InteractiveSession()\r\n    sess.run(iterator.initializer)\r\n\r\n    # Consume iterator and time\r\n    for i in range(int(num_iters)):\r\n        try:\r\n            t1 = time.time()\r\n            res = sess.run(x)\r\n\r\n            if i%100 == 99:\r\n                t2 = time.time()\r\n                print ('iteration {0:d}: last 100 iterations took {1:0.2f}ms'.format(i,1000*(t2-t1)))\r\n        except tf.errors.OutOfRangeError:\r\n            sess.run(iterator.initializer)\r\n            print('Start new epoch')\r\n\r\nprint ('   === START TEST WITH SHUFFLING DISABLED === ')\r\ntest_shuffle(False)\r\nprint()\r\n\r\nprint ('   === START TEST WITH SHUFFLING ENABLED === ')\r\ntest_shuffle(True)\r\nprint()\r\n```\r\n\r\nNote: this bug report follows comments and discussion on bug report https://github.com/tensorflow/tensorflow/issues/11591\r\nI initially experienced this issue with the MNIST dataset (smaller dataset than the example attached, i.e., 60K iterations ;  but with more operations for dataset loading).\r\n\r\nFinal note: I also tried the variant in which I do not capture the `OutOfRangeError` followed by initializing the iterator, but rather include a line `dataset = dataset.repeat(num_epoch)` in the dataset creation Graph. The results are similar.\r\n\r\nThanks a lot.\r\n\r\nKenneth\r\n"}