{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22291", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22291/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22291/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22291/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22291", "id": 360522742, "node_id": "MDU6SXNzdWUzNjA1MjI3NDI=", "number": 22291, "title": "vgg16  transfer learning will be error \"TypeError: provided list of inputs contains objects other than 'EagerTensor'\"", "user": {"login": "Q82822", "id": 36806042, "node_id": "MDQ6VXNlcjM2ODA2MDQy", "avatar_url": "https://avatars2.githubusercontent.com/u/36806042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Q82822", "html_url": "https://github.com/Q82822", "followers_url": "https://api.github.com/users/Q82822/followers", "following_url": "https://api.github.com/users/Q82822/following{/other_user}", "gists_url": "https://api.github.com/users/Q82822/gists{/gist_id}", "starred_url": "https://api.github.com/users/Q82822/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Q82822/subscriptions", "organizations_url": "https://api.github.com/users/Q82822/orgs", "repos_url": "https://api.github.com/users/Q82822/repos", "events_url": "https://api.github.com/users/Q82822/events{/privacy}", "received_events_url": "https://api.github.com/users/Q82822/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 736653459, "node_id": "MDU6TGFiZWw3MzY2NTM0NTk=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:eager", "name": "comp:eager", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-09-15T09:07:29Z", "updated_at": "2018-11-20T07:56:11Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):<br>\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04<br>\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:<br>\nTensorFlow installed from (source or binary): source<br>\nTensorFlow version (use command below): 1.11.0 (use tf-night-gpu)<br>\nPython version:2.7<br>\nBazel version (if compiling from source):<br>\nGCC/Compiler version (if compiling from source):<br>\nCUDA/cuDNN version: 9.0 /7.1<br>\nGPU model and memory:<br>\nExact command to reproduce:</p>\n<h3>Describe the problem</h3>\n<p>vgg16  used to <code>transfer learning </code> and data <code>cocodataset 2014</code>  and it will be error <code>TypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor</code>  but use<code> InceptionV3</code> it is can work.</p>\n<h3>Source code / logs</h3>\n<p>this is my code.  the cocodataset need to download 3 hours,<br>\nif vgg16  change to InceptionV3, it is can work</p>\n<pre><code>import tensorflow as tf\ntf.enable_eager_execution()\n\n# We'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n#import inception_v4 \n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport time\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nfrom glob import glob\nfrom PIL import Image\nimport pickle\n\nannotation_zip = tf.keras.utils.get_file('captions.zip', \n                                          cache_subdir=os.path.abspath('.'),\n                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n                                          extract = True)\nannotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n\nname_of_zip = 'train2014.zip'\nif not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n    image_zip = tf.keras.utils.get_file(name_of_zip, \n                                      cache_subdir=os.path.abspath('.'),\n                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n                                      extract = True)\n    PATH = os.path.dirname(image_zip)+'/train2014/'\nelse:\n    PATH = os.path.abspath('.')+'/train2014/'\n\n\n# read the json file# read  \nwith open(annotation_file, 'r') as f:\n    annotations = json.load(f)\n\n# storing the captions and the image name in vectors\nall_captions = []\nall_img_name_vector = []\n\nfor annot in annotations['annotations']:\n    caption = '&lt;start&gt; ' + annot['caption'] + ' &lt;end&gt;'\n    image_id = annot['image_id']\n    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n    \n    all_img_name_vector.append(full_coco_image_path)\n    all_captions.append(caption)\n\ntrain_captions, img_name_vector = shuffle(all_captions,\n                                          all_img_name_vector,\n                                          random_state=1)\n\n# selecting the first 30000 captions from the shuffled set\nnum_examples = 30000\n\ndef load_image(image_path):\n    img = tf.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize_images(img, (224, 224))\n    img = tf.keras.applications.vgg16.preprocess_input(x = img)\n    return img, image_path\n\nimage_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n\nstartTime=time.time()\n# getting the unique images\nencode_train = sorted(set(img_name_vector))\n\n# feel free to change the batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(\n                                encode_train).map(load_image).batch(16)\n\nfor img, path in image_dataset:\n    batch_features_0 = image_features_extract_model(img)\n  \n    batch_features = tf.reshape(batch_features_0, (batch_features_0.shape[0], -1, batch_features_0.shape[3]))\n    Nan = np.any(np.isnan(batch_features))\n\n    for bf, p in zip(batch_features, path):\n\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        np.save(path_of_feature, bf.numpy())\n\n### test image\nimage = './COCO_val2014_000000000042.jpg'\nprint(load_image(image))\n</code></pre>\n<p>the error output</p>\n<pre><code>_FallbackException                        Traceback (most recent call last)\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\n   6211         _ctx._context_handle, _ctx._eager_context.device_name, \"Reshape\",\n-&gt; 6212         name, _ctx._post_execution_callbacks, tensor, shape)\n   6213       return _result\n\n_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-7-90810626227a&gt; in &lt;module&gt;()\n      1 image = './COCO_val2014_000000000042.jpg'\n----&gt; 2 print(type(load_image(image)))\n      3 print(len(load_image(image)))\n      4 print(load_image(image))\n\n&lt;ipython-input-4-34a6a48d497c&gt; in load_image(image_path)\n      8     #img = tf.keras.applications.inception_v3.preprocess_input(img)\n      9     #img = inception_v4.preprocess_input(img)\n---&gt; 10     img = tf.keras.applications.vgg16.preprocess_input(x = img)\n     11     return img, image_path\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in preprocess_input(x, data_format, mode)\n    197     return _preprocess_numpy_input(x, data_format=data_format, mode=mode)\n    198   else:\n--&gt; 199     return _preprocess_symbolic_input(x, data_format=data_format, mode=mode)\n    200 \n    201 \n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in _preprocess_symbolic_input(x, data_format, mode)\n    160     x = K.bias_add(x, math_ops.cast(_IMAGENET_MEAN, K.dtype(x)), data_format)\n    161   else:\n--&gt; 162     x = K.bias_add(x, _IMAGENET_MEAN, data_format)\n    163   if std is not None:\n    164     x /= std\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in bias_add(x, bias, data_format)\n   4480     elif data_format == 'channels_last':\n   4481       if len(bias_shape) == 1:\n-&gt; 4482         x = x + reshape(bias, (1, 1, bias_shape[0]))\n   4483       else:\n   4484         x = x + reshape(bias, (1,) + bias_shape)\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in reshape(x, shape)\n   2214       A tensor.\n   2215   \"\"\"\n-&gt; 2216   return array_ops.reshape(x, shape)\n   2217 \n   2218 \n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\n   6214     except _core._FallbackException:\n   6215       return reshape_eager_fallback(\n-&gt; 6216           tensor, shape, name=name, ctx=_ctx)\n   6217     except _core._NotOkStatusException as e:\n   6218       if name is not None:\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)\n   6233   _attrs = (\"T\", _attr_T, \"Tshape\", _attr_Tshape)\n   6234   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n-&gt; 6235                              ctx=_ctx, name=name)\n   6236   _execute.record_gradient(\n   6237       \"Reshape\", _inputs_flat, _attrs, _result, name)\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n     59                                                op_name, inputs, attrs,\n---&gt; 60                                                num_outputs)\n     61   except core._NotOkStatusException as e:\n     62     if name is not None:\n\nTypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor\n</code></pre>", "body_text": "System information\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.11.0 (use tf-night-gpu)\nPython version:2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0 /7.1\nGPU model and memory:\nExact command to reproduce:\nDescribe the problem\nvgg16  used to transfer learning  and data cocodataset 2014  and it will be error TypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor  but use InceptionV3 it is can work.\nSource code / logs\nthis is my code.  the cocodataset need to download 3 hours,\nif vgg16  change to InceptionV3, it is can work\nimport tensorflow as tf\ntf.enable_eager_execution()\n\n# We'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n#import inception_v4 \n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport time\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nfrom glob import glob\nfrom PIL import Image\nimport pickle\n\nannotation_zip = tf.keras.utils.get_file('captions.zip', \n                                          cache_subdir=os.path.abspath('.'),\n                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n                                          extract = True)\nannotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n\nname_of_zip = 'train2014.zip'\nif not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n    image_zip = tf.keras.utils.get_file(name_of_zip, \n                                      cache_subdir=os.path.abspath('.'),\n                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n                                      extract = True)\n    PATH = os.path.dirname(image_zip)+'/train2014/'\nelse:\n    PATH = os.path.abspath('.')+'/train2014/'\n\n\n# read the json file# read  \nwith open(annotation_file, 'r') as f:\n    annotations = json.load(f)\n\n# storing the captions and the image name in vectors\nall_captions = []\nall_img_name_vector = []\n\nfor annot in annotations['annotations']:\n    caption = '<start> ' + annot['caption'] + ' <end>'\n    image_id = annot['image_id']\n    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n    \n    all_img_name_vector.append(full_coco_image_path)\n    all_captions.append(caption)\n\ntrain_captions, img_name_vector = shuffle(all_captions,\n                                          all_img_name_vector,\n                                          random_state=1)\n\n# selecting the first 30000 captions from the shuffled set\nnum_examples = 30000\n\ndef load_image(image_path):\n    img = tf.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize_images(img, (224, 224))\n    img = tf.keras.applications.vgg16.preprocess_input(x = img)\n    return img, image_path\n\nimage_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n\nstartTime=time.time()\n# getting the unique images\nencode_train = sorted(set(img_name_vector))\n\n# feel free to change the batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(\n                                encode_train).map(load_image).batch(16)\n\nfor img, path in image_dataset:\n    batch_features_0 = image_features_extract_model(img)\n  \n    batch_features = tf.reshape(batch_features_0, (batch_features_0.shape[0], -1, batch_features_0.shape[3]))\n    Nan = np.any(np.isnan(batch_features))\n\n    for bf, p in zip(batch_features, path):\n\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        np.save(path_of_feature, bf.numpy())\n\n### test image\nimage = './COCO_val2014_000000000042.jpg'\nprint(load_image(image))\n\nthe error output\n_FallbackException                        Traceback (most recent call last)\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\n   6211         _ctx._context_handle, _ctx._eager_context.device_name, \"Reshape\",\n-> 6212         name, _ctx._post_execution_callbacks, tensor, shape)\n   6213       return _result\n\n_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\n<ipython-input-7-90810626227a> in <module>()\n      1 image = './COCO_val2014_000000000042.jpg'\n----> 2 print(type(load_image(image)))\n      3 print(len(load_image(image)))\n      4 print(load_image(image))\n\n<ipython-input-4-34a6a48d497c> in load_image(image_path)\n      8     #img = tf.keras.applications.inception_v3.preprocess_input(img)\n      9     #img = inception_v4.preprocess_input(img)\n---> 10     img = tf.keras.applications.vgg16.preprocess_input(x = img)\n     11     return img, image_path\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in preprocess_input(x, data_format, mode)\n    197     return _preprocess_numpy_input(x, data_format=data_format, mode=mode)\n    198   else:\n--> 199     return _preprocess_symbolic_input(x, data_format=data_format, mode=mode)\n    200 \n    201 \n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in _preprocess_symbolic_input(x, data_format, mode)\n    160     x = K.bias_add(x, math_ops.cast(_IMAGENET_MEAN, K.dtype(x)), data_format)\n    161   else:\n--> 162     x = K.bias_add(x, _IMAGENET_MEAN, data_format)\n    163   if std is not None:\n    164     x /= std\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in bias_add(x, bias, data_format)\n   4480     elif data_format == 'channels_last':\n   4481       if len(bias_shape) == 1:\n-> 4482         x = x + reshape(bias, (1, 1, bias_shape[0]))\n   4483       else:\n   4484         x = x + reshape(bias, (1,) + bias_shape)\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in reshape(x, shape)\n   2214       A tensor.\n   2215   \"\"\"\n-> 2216   return array_ops.reshape(x, shape)\n   2217 \n   2218 \n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\n   6214     except _core._FallbackException:\n   6215       return reshape_eager_fallback(\n-> 6216           tensor, shape, name=name, ctx=_ctx)\n   6217     except _core._NotOkStatusException as e:\n   6218       if name is not None:\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)\n   6233   _attrs = (\"T\", _attr_T, \"Tshape\", _attr_Tshape)\n   6234   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n-> 6235                              ctx=_ctx, name=name)\n   6236   _execute.record_gradient(\n   6237       \"Reshape\", _inputs_flat, _attrs, _result, name)\n\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n     59                                                op_name, inputs, attrs,\n---> 60                                                num_outputs)\n     61   except core._NotOkStatusException as e:\n     62     if name is not None:\n\nTypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor", "body": "\r\n### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.11.0 (use tf-night-gpu)\r\nPython version:2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 9.0 /7.1\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\n### Describe the problem\r\nvgg16  used to `transfer learning ` and data `cocodataset 2014`  and it will be error `TypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor`  but use` InceptionV3` it is can work. \r\n\r\n### Source code / logs\r\nthis is my code.  the cocodataset need to download 3 hours,  \r\nif vgg16  change to InceptionV3, it is can work\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\n# We'll generate plots of attention in order to see which parts of an image\r\n# our model focuses on during captioning\r\nimport matplotlib.pyplot as plt\r\n#import inception_v4 \r\n# Scikit-learn includes many helpful utilities\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.utils import shuffle\r\nimport time\r\nimport re\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport json\r\nfrom glob import glob\r\nfrom PIL import Image\r\nimport pickle\r\n\r\nannotation_zip = tf.keras.utils.get_file('captions.zip', \r\n                                          cache_subdir=os.path.abspath('.'),\r\n                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\r\n                                          extract = True)\r\nannotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\r\n\r\nname_of_zip = 'train2014.zip'\r\nif not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\r\n    image_zip = tf.keras.utils.get_file(name_of_zip, \r\n                                      cache_subdir=os.path.abspath('.'),\r\n                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\r\n                                      extract = True)\r\n    PATH = os.path.dirname(image_zip)+'/train2014/'\r\nelse:\r\n    PATH = os.path.abspath('.')+'/train2014/'\r\n\r\n\r\n# read the json file# read  \r\nwith open(annotation_file, 'r') as f:\r\n    annotations = json.load(f)\r\n\r\n# storing the captions and the image name in vectors\r\nall_captions = []\r\nall_img_name_vector = []\r\n\r\nfor annot in annotations['annotations']:\r\n    caption = '<start> ' + annot['caption'] + ' <end>'\r\n    image_id = annot['image_id']\r\n    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\r\n    \r\n    all_img_name_vector.append(full_coco_image_path)\r\n    all_captions.append(caption)\r\n\r\ntrain_captions, img_name_vector = shuffle(all_captions,\r\n                                          all_img_name_vector,\r\n                                          random_state=1)\r\n\r\n# selecting the first 30000 captions from the shuffled set\r\nnum_examples = 30000\r\n\r\ndef load_image(image_path):\r\n    img = tf.read_file(image_path)\r\n    img = tf.image.decode_jpeg(img, channels=3)\r\n    img = tf.image.resize_images(img, (224, 224))\r\n    img = tf.keras.applications.vgg16.preprocess_input(x = img)\r\n    return img, image_path\r\n\r\nimage_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\r\nnew_input = image_model.input\r\nhidden_layer = image_model.layers[-1].output\r\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\r\n\r\nstartTime=time.time()\r\n# getting the unique images\r\nencode_train = sorted(set(img_name_vector))\r\n\r\n# feel free to change the batch_size according to your system configuration\r\nimage_dataset = tf.data.Dataset.from_tensor_slices(\r\n                                encode_train).map(load_image).batch(16)\r\n\r\nfor img, path in image_dataset:\r\n    batch_features_0 = image_features_extract_model(img)\r\n  \r\n    batch_features = tf.reshape(batch_features_0, (batch_features_0.shape[0], -1, batch_features_0.shape[3]))\r\n    Nan = np.any(np.isnan(batch_features))\r\n\r\n    for bf, p in zip(batch_features, path):\r\n\r\n        path_of_feature = p.numpy().decode(\"utf-8\")\r\n        np.save(path_of_feature, bf.numpy())\r\n\r\n### test image\r\nimage = './COCO_val2014_000000000042.jpg'\r\nprint(load_image(image))\r\n```\r\nthe error output\r\n```\r\n_FallbackException                        Traceback (most recent call last)\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   6211         _ctx._context_handle, _ctx._eager_context.device_name, \"Reshape\",\r\n-> 6212         name, _ctx._post_execution_callbacks, tensor, shape)\r\n   6213       return _result\r\n\r\n_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-90810626227a> in <module>()\r\n      1 image = './COCO_val2014_000000000042.jpg'\r\n----> 2 print(type(load_image(image)))\r\n      3 print(len(load_image(image)))\r\n      4 print(load_image(image))\r\n\r\n<ipython-input-4-34a6a48d497c> in load_image(image_path)\r\n      8     #img = tf.keras.applications.inception_v3.preprocess_input(img)\r\n      9     #img = inception_v4.preprocess_input(img)\r\n---> 10     img = tf.keras.applications.vgg16.preprocess_input(x = img)\r\n     11     return img, image_path\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in preprocess_input(x, data_format, mode)\r\n    197     return _preprocess_numpy_input(x, data_format=data_format, mode=mode)\r\n    198   else:\r\n--> 199     return _preprocess_symbolic_input(x, data_format=data_format, mode=mode)\r\n    200 \r\n    201 \r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in _preprocess_symbolic_input(x, data_format, mode)\r\n    160     x = K.bias_add(x, math_ops.cast(_IMAGENET_MEAN, K.dtype(x)), data_format)\r\n    161   else:\r\n--> 162     x = K.bias_add(x, _IMAGENET_MEAN, data_format)\r\n    163   if std is not None:\r\n    164     x /= std\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in bias_add(x, bias, data_format)\r\n   4480     elif data_format == 'channels_last':\r\n   4481       if len(bias_shape) == 1:\r\n-> 4482         x = x + reshape(bias, (1, 1, bias_shape[0]))\r\n   4483       else:\r\n   4484         x = x + reshape(bias, (1,) + bias_shape)\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in reshape(x, shape)\r\n   2214       A tensor.\r\n   2215   \"\"\"\r\n-> 2216   return array_ops.reshape(x, shape)\r\n   2217 \r\n   2218 \r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   6214     except _core._FallbackException:\r\n   6215       return reshape_eager_fallback(\r\n-> 6216           tensor, shape, name=name, ctx=_ctx)\r\n   6217     except _core._NotOkStatusException as e:\r\n   6218       if name is not None:\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)\r\n   6233   _attrs = (\"T\", _attr_T, \"Tshape\", _attr_Tshape)\r\n   6234   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n-> 6235                              ctx=_ctx, name=name)\r\n   6236   _execute.record_gradient(\r\n   6237       \"Reshape\", _inputs_flat, _attrs, _result, name)\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     59                                                op_name, inputs, attrs,\r\n---> 60                                                num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nTypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor\r\n```\r\n"}