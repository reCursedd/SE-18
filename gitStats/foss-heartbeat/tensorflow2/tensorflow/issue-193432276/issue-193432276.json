{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6081", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6081/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6081/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6081/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6081", "id": 193432276, "node_id": "MDU6SXNzdWUxOTM0MzIyNzY=", "number": 6081, "title": "How to restore training when MonitoredTrainingSession is used", "user": {"login": "taochenshh", "id": 15166943, "node_id": "MDQ6VXNlcjE1MTY2OTQz", "avatar_url": "https://avatars3.githubusercontent.com/u/15166943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taochenshh", "html_url": "https://github.com/taochenshh", "followers_url": "https://api.github.com/users/taochenshh/followers", "following_url": "https://api.github.com/users/taochenshh/following{/other_user}", "gists_url": "https://api.github.com/users/taochenshh/gists{/gist_id}", "starred_url": "https://api.github.com/users/taochenshh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taochenshh/subscriptions", "organizations_url": "https://api.github.com/users/taochenshh/orgs", "repos_url": "https://api.github.com/users/taochenshh/repos", "events_url": "https://api.github.com/users/taochenshh/events{/privacy}", "received_events_url": "https://api.github.com/users/taochenshh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-12-05T07:54:05Z", "updated_at": "2017-10-20T01:16:41Z", "closed_at": "2016-12-06T01:20:26Z", "author_association": "NONE", "body_html": "<p>I ran cifar10_train.py from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_train.py\">master branch</a>. My  question is how to restore the checkpoint file in case that <code>MonitoredTrainingSession</code> is being used instead of <code>Session</code>. I have modified the code like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>():\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Train CIFAR-10 for a number of steps.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">with</span> tf.Graph().as_default():\n    global_step <span class=\"pl-k\">=</span> tf.contrib.framework.get_or_create_global_step()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get images and labels for CIFAR-10.</span>\n    images, labels <span class=\"pl-k\">=</span> cifar10.distorted_inputs()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Build a Graph that computes the logits predictions from the</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> inference model.</span>\n    logits <span class=\"pl-k\">=</span> cifar10.inference(images)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Calculate loss.</span>\n    loss <span class=\"pl-k\">=</span> cifar10.loss(logits, labels)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Build a Graph that trains the model with one batch of examples and</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> updates the model parameters.</span>\n    train_op <span class=\"pl-k\">=</span> cifar10.train(loss, global_step)\n\n    <span class=\"pl-k\">class</span> <span class=\"pl-en\">_LoggerHook</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">train</span>.<span class=\"pl-e\">SessionRunHook</span>):\n      <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Logs loss and runtime.<span class=\"pl-pds\">\"\"\"</span></span>\n\n      <span class=\"pl-k\">def</span> <span class=\"pl-en\">begin</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>._step <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>\n\n      <span class=\"pl-k\">def</span> <span class=\"pl-en\">before_run</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">run_context</span>):\n        <span class=\"pl-c1\">self</span>._step <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        <span class=\"pl-c1\">self</span>._start_time <span class=\"pl-k\">=</span> time.time()\n        <span class=\"pl-k\">return</span> tf.train.SessionRunArgs(loss)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Asks for loss value.</span>\n\n      <span class=\"pl-k\">def</span> <span class=\"pl-en\">after_run</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">run_context</span>, <span class=\"pl-smi\">run_values</span>):\n        duration <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>._start_time\n        loss_value <span class=\"pl-k\">=</span> run_values.results\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n          num_examples_per_step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.batch_size\n          examples_per_sec <span class=\"pl-k\">=</span> num_examples_per_step <span class=\"pl-k\">/</span> duration\n          sec_per_batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">float</span>(duration)\n\n          format_str <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span>: step <span class=\"pl-c1\">%d</span>, loss = <span class=\"pl-c1\">%.2f</span> (<span class=\"pl-c1\">%.1f</span> examples/sec; <span class=\"pl-c1\">%.3f</span> <span class=\"pl-pds\">'</span></span>\n                        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>sec/batch)<span class=\"pl-pds\">'</span></span>)\n          <span class=\"pl-c1\">print</span> (format_str <span class=\"pl-k\">%</span> (datetime.now(), <span class=\"pl-c1\">self</span>._step, loss_value,\n                               examples_per_sec, sec_per_batch))\n\n    saver <span class=\"pl-k\">=</span> tf.train.Saver()\n    <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(<span class=\"pl-v\">checkpoint_dir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.train_dir,\n                                           <span class=\"pl-v\">hooks</span><span class=\"pl-k\">=</span>[tf.train.StopAtStepHook(<span class=\"pl-v\">last_step</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.max_steps),\n                                                  tf.train.NanTensorHook(loss),\n                                                  _LoggerHook()],\n                                           <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.log_device_placement),\n                                           <span class=\"pl-v\">save_checkpoint_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">600</span>,\n                                           <span class=\"pl-v\">save_summaries_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>) <span class=\"pl-k\">as</span> mon_sess:\n        ckpt <span class=\"pl-k\">=</span> tf.train.get_checkpoint_state(<span class=\"pl-c1\">FLAGS</span>.train_dir)\n        <span class=\"pl-k\">if</span> ckpt <span class=\"pl-k\">and</span> ckpt.model_checkpoint_path:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Restores from checkpoint</span>\n            saver.restore(mon_sess, ckpt.model_checkpoint_path)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assuming model_checkpoint_path looks something like:</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>   /my-favorite-path/cifar10_train/model.ckpt-0,</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> extract global_step from it.</span>\n            global_step <span class=\"pl-k\">=</span> ckpt.model_checkpoint_path.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>].split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n        <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> mon_sess.should_stop():\n            mon_sess.run(train_op)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">argv</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=unused-argument</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> cifar10.maybe_download_and_extract()</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> if tf.gfile.Exists(FLAGS.train_dir):</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>   tf.gfile.DeleteRecursively(FLAGS.train_dir)</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> tf.gfile.MakeDirs(FLAGS.train_dir)</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> train()</span>\n  cifar10.maybe_download_and_extract()\n  <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> tf.gfile.Exists(<span class=\"pl-c1\">FLAGS</span>.train_dir):\n      tf.gfile.MakeDirs(<span class=\"pl-c1\">FLAGS</span>.train_dir)\n  train()</pre></div>\n<p>The training variables seem to be restored, but the step variable still started from 0, how to make the step increases from the restored checkpoint step?</p>", "body_text": "I ran cifar10_train.py from master branch. My  question is how to restore the checkpoint file in case that MonitoredTrainingSession is being used instead of Session. I have modified the code like this:\ndef train():\n  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n  with tf.Graph().as_default():\n    global_step = tf.contrib.framework.get_or_create_global_step()\n\n    # Get images and labels for CIFAR-10.\n    images, labels = cifar10.distorted_inputs()\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits = cifar10.inference(images)\n\n    # Calculate loss.\n    loss = cifar10.loss(logits, labels)\n\n    # Build a Graph that trains the model with one batch of examples and\n    # updates the model parameters.\n    train_op = cifar10.train(loss, global_step)\n\n    class _LoggerHook(tf.train.SessionRunHook):\n      \"\"\"Logs loss and runtime.\"\"\"\n\n      def begin(self):\n        self._step = -1\n\n      def before_run(self, run_context):\n        self._step += 1\n        self._start_time = time.time()\n        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n\n      def after_run(self, run_context, run_values):\n        duration = time.time() - self._start_time\n        loss_value = run_values.results\n        if self._step % 10 == 0:\n          num_examples_per_step = FLAGS.batch_size\n          examples_per_sec = num_examples_per_step / duration\n          sec_per_batch = float(duration)\n\n          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n                        'sec/batch)')\n          print (format_str % (datetime.now(), self._step, loss_value,\n                               examples_per_sec, sec_per_batch))\n\n    saver = tf.train.Saver()\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=FLAGS.train_dir,\n                                           hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n                                                  tf.train.NanTensorHook(loss),\n                                                  _LoggerHook()],\n                                           config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement),\n                                           save_checkpoint_secs=600,\n                                           save_summaries_steps=100) as mon_sess:\n        ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            # Restores from checkpoint\n            saver.restore(mon_sess, ckpt.model_checkpoint_path)\n            # Assuming model_checkpoint_path looks something like:\n            #   /my-favorite-path/cifar10_train/model.ckpt-0,\n            # extract global_step from it.\n            global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n        while not mon_sess.should_stop():\n            mon_sess.run(train_op)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  # cifar10.maybe_download_and_extract()\n  # if tf.gfile.Exists(FLAGS.train_dir):\n  #   tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  # tf.gfile.MakeDirs(FLAGS.train_dir)\n  # train()\n  cifar10.maybe_download_and_extract()\n  if not tf.gfile.Exists(FLAGS.train_dir):\n      tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\nThe training variables seem to be restored, but the step variable still started from 0, how to make the step increases from the restored checkpoint step?", "body": "I ran cifar10_train.py from [master branch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_train.py). My  question is how to restore the checkpoint file in case that `MonitoredTrainingSession` is being used instead of `Session`. I have modified the code like this:\r\n```python\r\ndef train():\r\n  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\r\n  with tf.Graph().as_default():\r\n    global_step = tf.contrib.framework.get_or_create_global_step()\r\n\r\n    # Get images and labels for CIFAR-10.\r\n    images, labels = cifar10.distorted_inputs()\r\n\r\n    # Build a Graph that computes the logits predictions from the\r\n    # inference model.\r\n    logits = cifar10.inference(images)\r\n\r\n    # Calculate loss.\r\n    loss = cifar10.loss(logits, labels)\r\n\r\n    # Build a Graph that trains the model with one batch of examples and\r\n    # updates the model parameters.\r\n    train_op = cifar10.train(loss, global_step)\r\n\r\n    class _LoggerHook(tf.train.SessionRunHook):\r\n      \"\"\"Logs loss and runtime.\"\"\"\r\n\r\n      def begin(self):\r\n        self._step = -1\r\n\r\n      def before_run(self, run_context):\r\n        self._step += 1\r\n        self._start_time = time.time()\r\n        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\r\n\r\n      def after_run(self, run_context, run_values):\r\n        duration = time.time() - self._start_time\r\n        loss_value = run_values.results\r\n        if self._step % 10 == 0:\r\n          num_examples_per_step = FLAGS.batch_size\r\n          examples_per_sec = num_examples_per_step / duration\r\n          sec_per_batch = float(duration)\r\n\r\n          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\r\n                        'sec/batch)')\r\n          print (format_str % (datetime.now(), self._step, loss_value,\r\n                               examples_per_sec, sec_per_batch))\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=FLAGS.train_dir,\r\n                                           hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\r\n                                                  tf.train.NanTensorHook(loss),\r\n                                                  _LoggerHook()],\r\n                                           config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement),\r\n                                           save_checkpoint_secs=600,\r\n                                           save_summaries_steps=100) as mon_sess:\r\n        ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            # Restores from checkpoint\r\n            saver.restore(mon_sess, ckpt.model_checkpoint_path)\r\n            # Assuming model_checkpoint_path looks something like:\r\n            #   /my-favorite-path/cifar10_train/model.ckpt-0,\r\n            # extract global_step from it.\r\n            global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\r\n        while not mon_sess.should_stop():\r\n            mon_sess.run(train_op)\r\n\r\n\r\ndef main(argv=None):  # pylint: disable=unused-argument\r\n  # cifar10.maybe_download_and_extract()\r\n  # if tf.gfile.Exists(FLAGS.train_dir):\r\n  #   tf.gfile.DeleteRecursively(FLAGS.train_dir)\r\n  # tf.gfile.MakeDirs(FLAGS.train_dir)\r\n  # train()\r\n  cifar10.maybe_download_and_extract()\r\n  if not tf.gfile.Exists(FLAGS.train_dir):\r\n      tf.gfile.MakeDirs(FLAGS.train_dir)\r\n  train()\r\n```\r\nThe training variables seem to be restored, but the step variable still started from 0, how to make the step increases from the restored checkpoint step?\r\n"}