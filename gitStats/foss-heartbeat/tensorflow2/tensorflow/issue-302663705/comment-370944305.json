{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/370944305", "html_url": "https://github.com/tensorflow/tensorflow/issues/17476#issuecomment-370944305", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17476", "id": 370944305, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDk0NDMwNQ==", "user": {"login": "christian-rauch", "id": 8226248, "node_id": "MDQ6VXNlcjgyMjYyNDg=", "avatar_url": "https://avatars0.githubusercontent.com/u/8226248?v=4", "gravatar_id": "", "url": "https://api.github.com/users/christian-rauch", "html_url": "https://github.com/christian-rauch", "followers_url": "https://api.github.com/users/christian-rauch/followers", "following_url": "https://api.github.com/users/christian-rauch/following{/other_user}", "gists_url": "https://api.github.com/users/christian-rauch/gists{/gist_id}", "starred_url": "https://api.github.com/users/christian-rauch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/christian-rauch/subscriptions", "organizations_url": "https://api.github.com/users/christian-rauch/orgs", "repos_url": "https://api.github.com/users/christian-rauch/repos", "events_url": "https://api.github.com/users/christian-rauch/events{/privacy}", "received_events_url": "https://api.github.com/users/christian-rauch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-06T22:00:49Z", "updated_at": "2018-03-06T22:00:49Z", "author_association": "NONE", "body_html": "<p>Thanks for you explanation.<br>\nIf a gradient is not defined, wouldn't it be better to provide no parameter updates at all (i.e. return a gradient of 0)? At the moment, the gradient becomes undefined at some circumstances (non-unique singular values) and propagates further, which breaks the whole training process without further warnings.</p>", "body_text": "Thanks for you explanation.\nIf a gradient is not defined, wouldn't it be better to provide no parameter updates at all (i.e. return a gradient of 0)? At the moment, the gradient becomes undefined at some circumstances (non-unique singular values) and propagates further, which breaks the whole training process without further warnings.", "body": "Thanks for you explanation.\r\nIf a gradient is not defined, wouldn't it be better to provide no parameter updates at all (i.e. return a gradient of 0)? At the moment, the gradient becomes undefined at some circumstances (non-unique singular values) and propagates further, which breaks the whole training process without further warnings."}