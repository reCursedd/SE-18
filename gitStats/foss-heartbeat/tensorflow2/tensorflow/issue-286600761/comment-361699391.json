{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361699391", "html_url": "https://github.com/tensorflow/tensorflow/issues/15933#issuecomment-361699391", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15933", "id": 361699391, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTY5OTM5MQ==", "user": {"login": "Gemesys", "id": 16905336, "node_id": "MDQ6VXNlcjE2OTA1MzM2", "avatar_url": "https://avatars1.githubusercontent.com/u/16905336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gemesys", "html_url": "https://github.com/Gemesys", "followers_url": "https://api.github.com/users/Gemesys/followers", "following_url": "https://api.github.com/users/Gemesys/following{/other_user}", "gists_url": "https://api.github.com/users/Gemesys/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gemesys/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gemesys/subscriptions", "organizations_url": "https://api.github.com/users/Gemesys/orgs", "repos_url": "https://api.github.com/users/Gemesys/repos", "events_url": "https://api.github.com/users/Gemesys/events{/privacy}", "received_events_url": "https://api.github.com/users/Gemesys/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T19:04:06Z", "updated_at": "2018-01-30T21:32:54Z", "author_association": "NONE", "body_html": "<p>Cracked it.  Got a solution (workaround, essentially), and a probable cause.  The MacBook running MacOS Yosemite (10.10.5) looks to be doing it's rounding and/or floating-point math wrong.  Based on Quaeler's results which shows the simulation running on MacOS Sierra 10.12.6 same as on Linux platforms (CentOS-7.4 and Ubuntu), it's clear the evolution of the Laplace PDE (partial differential equation) simulation to the complex moire-pattern is the correct evolution, and the MacOS Yosemite 10.10.5 has a flaw in how it is doing 32-bit floating math.  On GNU Linux systems, both 32-bit and 64-bit (CentOS-6.6 and CentOS-7.4 confirmed) it is possible to explicitly control precision, using a routine \"fesetprec\", which can be lifted from a handbook of code:  (see here)  <a href=\"https://books.google.ca/books?id=OjUyDwAAQBAJ&amp;pg=PA127&amp;lpg=PA127&amp;dq=ieee.c+++fesetprec+to+control+precision&amp;source=bl&amp;ots=VLtoiOfYfE&amp;sig=BfdtySalckBzIB-mbV_Uy4uXLL4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjD1r6BzujYAhUH94MKHTNhDowQ6AEIJzAA#v=onepage&amp;q=ieee.c%20%20%20fesetprec%20to%20control%20precision&amp;f=false\" rel=\"nofollow\">https://books.google.ca/books?id=OjUyDwAAQBAJ&amp;pg=PA127&amp;lpg=PA127&amp;dq=ieee.c+++fesetprec+to+control+precision&amp;source=bl&amp;ots=VLtoiOfYfE&amp;sig=BfdtySalckBzIB-mbV_Uy4uXLL4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjD1r6BzujYAhUH94MKHTNhDowQ6AEIJzAA#v=onepage&amp;q=ieee.c%20%20%20fesetprec%20to%20control%20precision&amp;f=false</a></p>\n<p>The \"fesetprec\" invokes a macro, called: \"_FPU_SETCW\", which generates some assembler code to set values in the Intel control word which, on IA-32 architectures, is used to explicitly control precision of floating point calculations.  The macro's _FPU_GETCW and _FPU_SETCW are available on GNU Linux systems in the \"fpu_control.h\" include file, in /usr/include.  The Intel spec for 8087 and 80387 FPU's allowed this.  The newer/newest MMX (and now SSE and SSE2 and SSE3 and such) architectures are indicated as not using this control word -but curiously, if you run test programs to explicity set precision, and then run precision-dependent code, you can see that on <em>both</em> architectures - IA-32, and the newer 64-bit Intel Core-i3 and Core-i5 chips, this control word can <em>still</em> be set, at least on Linux machines.  I've confirmed this.</p>\n<p>I downloaded and converted the UCBTest suite which exercises a bunch of floating-point calculations, and then I also pulled together a working example of a test program (from the Handbook previously mentioned), which sets the precison control-word.  Basically, you define your vars as \"long double\", but then you can tweak the control-word to run in float, double, or long-double.  The test program \"chkprec.c\" gives three different results, depending on the precision selected.  This program works the same on both CentOS 6.6 and CentOS 7.4.   Long-double on a 32-bit box is probably going to be compiled as REAL*10 (the 80-bit extended precision - which Intel chips do their math in - and which has been a fixture of Intel architecture since the first IBM P/C.)  I know a tiny bit about this, because my \"gDOSbox\" app (free, no ads, no tracking on Google Playstore) is special, because I know it has had its conversion math fixed so that mapping from 32-bit (\"float\"), and 64-bit (double precision) to 80-bit (extended precision), was not originally being done correctly, (in the open-source DOSbox code), but is being done correctly in gDOSbox.   Most public DOSbox's would not run high-precision Fortran or C progams correctly. The math in \"gDOSbox\" can be run correctly, and the APL interpreters and Fortran compilers that it supports (WF77 - Watcom Fortran is one of them), do their math correctly (I've checked).</p>\n<p>For NN (Neural Net) applications, matrix-math must be done correctly, and the TensorFlow Laplace sim was a great exercise of matrix math.  The routine used is called: \"tf.nn.depthwise_conv2d\" (if you import TensorFlow into Python as \"tf\").  It is documented here:<br>\n<a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d</a></p>\n<p>You might also want to look at this, if learning about tf.nn.depthwise_conv2d:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/nn_impl.py\">https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/nn_impl.py</a></p>\n<p>What I found using UCBTest and chkprec.c, was that the Macbook Pro, under Yosemite (MacOS 10.10.5) does not support the explicit-setting of precision in the same manner that Linux does.  There is no provision for setting the precision control-word on the Mac under Yosemite, or if there is, it is substantially different than is the case for Linux.  My version of chkprec.c on the MacBook is an ugly hack, with three different functions, one in each precision level (float, double, and long double), and it was only in this way that I could reproduce the results I was getting on Linux, where the precision control word could be set to the desired precision using fsetprec.  This means that math-calc code written for Linux which relies on the ability to explicitly control precision, probably won't run correctly on the Mac, unless low-level changes are made.  Also, there are differences in the \"UCBfail\"s that occur on the Macbook, as opposed to the Linux boxes.  What is interesting, is the 64-bit Linux runs the UCBTest suite better than the 32-bit Linux (CentOS 6.6) does.  And both the Linux 64-bit and the MacOS 64-bit pass the cpar_DP and cpar_SP (\"paranoid\") tests, as well as the DP and SP (double and single precision) tests for \"mul\" and \"div\".    But the \"cpi_DP\" test iterates differently on Mac versus Linux 64-bit, and only in a few places where the exponents are large negative (E-26, E-38, E-40, and such).</p>\n<p>I will post the various test programs to my github account.</p>\n<p>In order to get the Laplace PDE simulation to iterate-and-evolve correctly (so you get a funky image of a moire-pattern, and some dark-matter black-space, instead of just a blank screen, as the MacBook was doing), I changed all the 32-bit (\"float\" level precision, like REAL<em>4 in Fortran), into 64-bit doubles (like REAL</em>8 in Fortran).   The TensorFlow documentation indicates that the tf.nn.depthwise_conv2d is able to operate in either mode, and it does.  It appears that TensorFlow is operating correctly, and it appears the bug that shows up in MacOS only manifests itself if the Python (numpy) variables are 32-bit (or \"float).</p>\n<p>Change all the variables to 64-bit, and the simulation works the same on both machines.<br>\nI'll monitor this thread for a bit, and if the TensorFlow authors don't close it, I will, as it appears not to be a TensorFlow issue, but is a MacOS issue, which as Quaeler discovered, can be resolved by upgrading to MacOS 10.12.6 (Sierra), and probably the newer \"High Sierra\" as well.    If you have other reasons for wishing to maintain your MacBook at Yosemite, but still want to run TensorFlow (in CPU mode), then I would recommend doing your network math and back-propagations with 64-bit floats, not 32-bit values.  I ran the sim forward with 57500 iterations, and the exponents are up around E+132 and E+133... big numbers, but still calculating correctly.    The final image and numeric (U(eval) at row 20) images for MacBook and Linux (CentOS-7.4) provided below.</p>\n<p>Hope all this helps anyone else struggling with wrong floating-point number problems on their MacBooks using TensorFlow.   The problem likely resides somewhere in the Xcode 7.2.1 - Clang 700x compiler used to build Python and TensorFlow, and again, is most likely resolved by using later versions of the compiler where the floating-point flaws appear to have been corrected.  (But they aren't all fixed.  Some of the test programs we have tried still show variance between Linux and MacOS results. The \"PIRATS\" program (see above) still shows some different numbers on each platform.)</p>\n<p>In my work with Xerion, I found it was critical to set the TCL-precision variable to it's highest value (17 or 18 or some such thing), if I wanted to write the network-weights out to a file, for subsequent loading at a later time, if the file was an ascii (not binary) numeric represetation.  The default was 8 digits after the decimal, and if you wrote and re-read network weights at that precision, your network was destroyed.  Weight digits 10 or 12 past the decimal place made a difference to network operation, and it became clear a very high level of precision was essential.   If you are working with neural networks, you might want to read W. Kahan's paper from his IEEE 754 lecture.  <a href=\"https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF\" rel=\"nofollow\">https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF</a>    It illustrates some of the issues and edge-case conditions that floating-point calculations run up against, and the choices and trade-offs that were made to create a viable floating-point calculation standard.   It is a good paper, and it provides background on why floating-point calculations can fail to operate as expected.<br>\n(Images of Laptest_64 sim on each platform follow..)<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/16905336/35584502-34d92556-05c3-11e8-9485-b8629e501347.jpg\"><img src=\"https://user-images.githubusercontent.com/16905336/35584502-34d92556-05c3-11e8-9485-b8629e501347.jpg\" alt=\"laptest_64_700_centos74_screenshot_2018-01-29_10_32pm\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/16905336/35584523-46e5b3b8-05c3-11e8-92c3-8cf00b9bb8ab.jpg\"><img src=\"https://user-images.githubusercontent.com/16905336/35584523-46e5b3b8-05c3-11e8-92c3-8cf00b9bb8ab.jpg\" alt=\"laptest_64_700_57k_macos_screenshot_2018-01-30_1_24am\" style=\"max-width:100%;\"></a></p>", "body_text": "Cracked it.  Got a solution (workaround, essentially), and a probable cause.  The MacBook running MacOS Yosemite (10.10.5) looks to be doing it's rounding and/or floating-point math wrong.  Based on Quaeler's results which shows the simulation running on MacOS Sierra 10.12.6 same as on Linux platforms (CentOS-7.4 and Ubuntu), it's clear the evolution of the Laplace PDE (partial differential equation) simulation to the complex moire-pattern is the correct evolution, and the MacOS Yosemite 10.10.5 has a flaw in how it is doing 32-bit floating math.  On GNU Linux systems, both 32-bit and 64-bit (CentOS-6.6 and CentOS-7.4 confirmed) it is possible to explicitly control precision, using a routine \"fesetprec\", which can be lifted from a handbook of code:  (see here)  https://books.google.ca/books?id=OjUyDwAAQBAJ&pg=PA127&lpg=PA127&dq=ieee.c+++fesetprec+to+control+precision&source=bl&ots=VLtoiOfYfE&sig=BfdtySalckBzIB-mbV_Uy4uXLL4&hl=en&sa=X&ved=0ahUKEwjD1r6BzujYAhUH94MKHTNhDowQ6AEIJzAA#v=onepage&q=ieee.c%20%20%20fesetprec%20to%20control%20precision&f=false\nThe \"fesetprec\" invokes a macro, called: \"_FPU_SETCW\", which generates some assembler code to set values in the Intel control word which, on IA-32 architectures, is used to explicitly control precision of floating point calculations.  The macro's _FPU_GETCW and _FPU_SETCW are available on GNU Linux systems in the \"fpu_control.h\" include file, in /usr/include.  The Intel spec for 8087 and 80387 FPU's allowed this.  The newer/newest MMX (and now SSE and SSE2 and SSE3 and such) architectures are indicated as not using this control word -but curiously, if you run test programs to explicity set precision, and then run precision-dependent code, you can see that on both architectures - IA-32, and the newer 64-bit Intel Core-i3 and Core-i5 chips, this control word can still be set, at least on Linux machines.  I've confirmed this.\nI downloaded and converted the UCBTest suite which exercises a bunch of floating-point calculations, and then I also pulled together a working example of a test program (from the Handbook previously mentioned), which sets the precison control-word.  Basically, you define your vars as \"long double\", but then you can tweak the control-word to run in float, double, or long-double.  The test program \"chkprec.c\" gives three different results, depending on the precision selected.  This program works the same on both CentOS 6.6 and CentOS 7.4.   Long-double on a 32-bit box is probably going to be compiled as REAL*10 (the 80-bit extended precision - which Intel chips do their math in - and which has been a fixture of Intel architecture since the first IBM P/C.)  I know a tiny bit about this, because my \"gDOSbox\" app (free, no ads, no tracking on Google Playstore) is special, because I know it has had its conversion math fixed so that mapping from 32-bit (\"float\"), and 64-bit (double precision) to 80-bit (extended precision), was not originally being done correctly, (in the open-source DOSbox code), but is being done correctly in gDOSbox.   Most public DOSbox's would not run high-precision Fortran or C progams correctly. The math in \"gDOSbox\" can be run correctly, and the APL interpreters and Fortran compilers that it supports (WF77 - Watcom Fortran is one of them), do their math correctly (I've checked).\nFor NN (Neural Net) applications, matrix-math must be done correctly, and the TensorFlow Laplace sim was a great exercise of matrix math.  The routine used is called: \"tf.nn.depthwise_conv2d\" (if you import TensorFlow into Python as \"tf\").  It is documented here:\nhttps://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d\nYou might also want to look at this, if learning about tf.nn.depthwise_conv2d:\nhttps://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/nn_impl.py\nWhat I found using UCBTest and chkprec.c, was that the Macbook Pro, under Yosemite (MacOS 10.10.5) does not support the explicit-setting of precision in the same manner that Linux does.  There is no provision for setting the precision control-word on the Mac under Yosemite, or if there is, it is substantially different than is the case for Linux.  My version of chkprec.c on the MacBook is an ugly hack, with three different functions, one in each precision level (float, double, and long double), and it was only in this way that I could reproduce the results I was getting on Linux, where the precision control word could be set to the desired precision using fsetprec.  This means that math-calc code written for Linux which relies on the ability to explicitly control precision, probably won't run correctly on the Mac, unless low-level changes are made.  Also, there are differences in the \"UCBfail\"s that occur on the Macbook, as opposed to the Linux boxes.  What is interesting, is the 64-bit Linux runs the UCBTest suite better than the 32-bit Linux (CentOS 6.6) does.  And both the Linux 64-bit and the MacOS 64-bit pass the cpar_DP and cpar_SP (\"paranoid\") tests, as well as the DP and SP (double and single precision) tests for \"mul\" and \"div\".    But the \"cpi_DP\" test iterates differently on Mac versus Linux 64-bit, and only in a few places where the exponents are large negative (E-26, E-38, E-40, and such).\nI will post the various test programs to my github account.\nIn order to get the Laplace PDE simulation to iterate-and-evolve correctly (so you get a funky image of a moire-pattern, and some dark-matter black-space, instead of just a blank screen, as the MacBook was doing), I changed all the 32-bit (\"float\" level precision, like REAL4 in Fortran), into 64-bit doubles (like REAL8 in Fortran).   The TensorFlow documentation indicates that the tf.nn.depthwise_conv2d is able to operate in either mode, and it does.  It appears that TensorFlow is operating correctly, and it appears the bug that shows up in MacOS only manifests itself if the Python (numpy) variables are 32-bit (or \"float).\nChange all the variables to 64-bit, and the simulation works the same on both machines.\nI'll monitor this thread for a bit, and if the TensorFlow authors don't close it, I will, as it appears not to be a TensorFlow issue, but is a MacOS issue, which as Quaeler discovered, can be resolved by upgrading to MacOS 10.12.6 (Sierra), and probably the newer \"High Sierra\" as well.    If you have other reasons for wishing to maintain your MacBook at Yosemite, but still want to run TensorFlow (in CPU mode), then I would recommend doing your network math and back-propagations with 64-bit floats, not 32-bit values.  I ran the sim forward with 57500 iterations, and the exponents are up around E+132 and E+133... big numbers, but still calculating correctly.    The final image and numeric (U(eval) at row 20) images for MacBook and Linux (CentOS-7.4) provided below.\nHope all this helps anyone else struggling with wrong floating-point number problems on their MacBooks using TensorFlow.   The problem likely resides somewhere in the Xcode 7.2.1 - Clang 700x compiler used to build Python and TensorFlow, and again, is most likely resolved by using later versions of the compiler where the floating-point flaws appear to have been corrected.  (But they aren't all fixed.  Some of the test programs we have tried still show variance between Linux and MacOS results. The \"PIRATS\" program (see above) still shows some different numbers on each platform.)\nIn my work with Xerion, I found it was critical to set the TCL-precision variable to it's highest value (17 or 18 or some such thing), if I wanted to write the network-weights out to a file, for subsequent loading at a later time, if the file was an ascii (not binary) numeric represetation.  The default was 8 digits after the decimal, and if you wrote and re-read network weights at that precision, your network was destroyed.  Weight digits 10 or 12 past the decimal place made a difference to network operation, and it became clear a very high level of precision was essential.   If you are working with neural networks, you might want to read W. Kahan's paper from his IEEE 754 lecture.  https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF    It illustrates some of the issues and edge-case conditions that floating-point calculations run up against, and the choices and trade-offs that were made to create a viable floating-point calculation standard.   It is a good paper, and it provides background on why floating-point calculations can fail to operate as expected.\n(Images of Laptest_64 sim on each platform follow..)", "body": "Cracked it.  Got a solution (workaround, essentially), and a probable cause.  The MacBook running MacOS Yosemite (10.10.5) looks to be doing it's rounding and/or floating-point math wrong.  Based on Quaeler's results which shows the simulation running on MacOS Sierra 10.12.6 same as on Linux platforms (CentOS-7.4 and Ubuntu), it's clear the evolution of the Laplace PDE (partial differential equation) simulation to the complex moire-pattern is the correct evolution, and the MacOS Yosemite 10.10.5 has a flaw in how it is doing 32-bit floating math.  On GNU Linux systems, both 32-bit and 64-bit (CentOS-6.6 and CentOS-7.4 confirmed) it is possible to explicitly control precision, using a routine \"fesetprec\", which can be lifted from a handbook of code:  (see here)  https://books.google.ca/books?id=OjUyDwAAQBAJ&pg=PA127&lpg=PA127&dq=ieee.c+++fesetprec+to+control+precision&source=bl&ots=VLtoiOfYfE&sig=BfdtySalckBzIB-mbV_Uy4uXLL4&hl=en&sa=X&ved=0ahUKEwjD1r6BzujYAhUH94MKHTNhDowQ6AEIJzAA#v=onepage&q=ieee.c%20%20%20fesetprec%20to%20control%20precision&f=false\r\n\r\nThe \"fesetprec\" invokes a macro, called: \"_FPU_SETCW\", which generates some assembler code to set values in the Intel control word which, on IA-32 architectures, is used to explicitly control precision of floating point calculations.  The macro's _FPU_GETCW and _FPU_SETCW are available on GNU Linux systems in the \"fpu_control.h\" include file, in /usr/include.  The Intel spec for 8087 and 80387 FPU's allowed this.  The newer/newest MMX (and now SSE and SSE2 and SSE3 and such) architectures are indicated as not using this control word -but curiously, if you run test programs to explicity set precision, and then run precision-dependent code, you can see that on *both* architectures - IA-32, and the newer 64-bit Intel Core-i3 and Core-i5 chips, this control word can *still* be set, at least on Linux machines.  I've confirmed this.\r\n\r\nI downloaded and converted the UCBTest suite which exercises a bunch of floating-point calculations, and then I also pulled together a working example of a test program (from the Handbook previously mentioned), which sets the precison control-word.  Basically, you define your vars as \"long double\", but then you can tweak the control-word to run in float, double, or long-double.  The test program \"chkprec.c\" gives three different results, depending on the precision selected.  This program works the same on both CentOS 6.6 and CentOS 7.4.   Long-double on a 32-bit box is probably going to be compiled as REAL*10 (the 80-bit extended precision - which Intel chips do their math in - and which has been a fixture of Intel architecture since the first IBM P/C.)  I know a tiny bit about this, because my \"gDOSbox\" app (free, no ads, no tracking on Google Playstore) is special, because I know it has had its conversion math fixed so that mapping from 32-bit (\"float\"), and 64-bit (double precision) to 80-bit (extended precision), was not originally being done correctly, (in the open-source DOSbox code), but is being done correctly in gDOSbox.   Most public DOSbox's would not run high-precision Fortran or C progams correctly. The math in \"gDOSbox\" can be run correctly, and the APL interpreters and Fortran compilers that it supports (WF77 - Watcom Fortran is one of them), do their math correctly (I've checked).\r\n\r\nFor NN (Neural Net) applications, matrix-math must be done correctly, and the TensorFlow Laplace sim was a great exercise of matrix math.  The routine used is called: \"tf.nn.depthwise_conv2d\" (if you import TensorFlow into Python as \"tf\").  It is documented here: \r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d\r\n\r\nYou might also want to look at this, if learning about tf.nn.depthwise_conv2d:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/nn_impl.py\r\n\r\nWhat I found using UCBTest and chkprec.c, was that the Macbook Pro, under Yosemite (MacOS 10.10.5) does not support the explicit-setting of precision in the same manner that Linux does.  There is no provision for setting the precision control-word on the Mac under Yosemite, or if there is, it is substantially different than is the case for Linux.  My version of chkprec.c on the MacBook is an ugly hack, with three different functions, one in each precision level (float, double, and long double), and it was only in this way that I could reproduce the results I was getting on Linux, where the precision control word could be set to the desired precision using fsetprec.  This means that math-calc code written for Linux which relies on the ability to explicitly control precision, probably won't run correctly on the Mac, unless low-level changes are made.  Also, there are differences in the \"UCBfail\"s that occur on the Macbook, as opposed to the Linux boxes.  What is interesting, is the 64-bit Linux runs the UCBTest suite better than the 32-bit Linux (CentOS 6.6) does.  And both the Linux 64-bit and the MacOS 64-bit pass the cpar_DP and cpar_SP (\"paranoid\") tests, as well as the DP and SP (double and single precision) tests for \"mul\" and \"div\".    But the \"cpi_DP\" test iterates differently on Mac versus Linux 64-bit, and only in a few places where the exponents are large negative (E-26, E-38, E-40, and such).  \r\n\r\nI will post the various test programs to my github account.\r\n\r\nIn order to get the Laplace PDE simulation to iterate-and-evolve correctly (so you get a funky image of a moire-pattern, and some dark-matter black-space, instead of just a blank screen, as the MacBook was doing), I changed all the 32-bit (\"float\" level precision, like REAL*4 in Fortran), into 64-bit doubles (like REAL*8 in Fortran).   The TensorFlow documentation indicates that the tf.nn.depthwise_conv2d is able to operate in either mode, and it does.  It appears that TensorFlow is operating correctly, and it appears the bug that shows up in MacOS only manifests itself if the Python (numpy) variables are 32-bit (or \"float).  \r\n\r\nChange all the variables to 64-bit, and the simulation works the same on both machines.\r\nI'll monitor this thread for a bit, and if the TensorFlow authors don't close it, I will, as it appears not to be a TensorFlow issue, but is a MacOS issue, which as Quaeler discovered, can be resolved by upgrading to MacOS 10.12.6 (Sierra), and probably the newer \"High Sierra\" as well.    If you have other reasons for wishing to maintain your MacBook at Yosemite, but still want to run TensorFlow (in CPU mode), then I would recommend doing your network math and back-propagations with 64-bit floats, not 32-bit values.  I ran the sim forward with 57500 iterations, and the exponents are up around E+132 and E+133... big numbers, but still calculating correctly.    The final image and numeric (U(eval) at row 20) images for MacBook and Linux (CentOS-7.4) provided below.  \r\n\r\nHope all this helps anyone else struggling with wrong floating-point number problems on their MacBooks using TensorFlow.   The problem likely resides somewhere in the Xcode 7.2.1 - Clang 700x compiler used to build Python and TensorFlow, and again, is most likely resolved by using later versions of the compiler where the floating-point flaws appear to have been corrected.  (But they aren't all fixed.  Some of the test programs we have tried still show variance between Linux and MacOS results. The \"PIRATS\" program (see above) still shows some different numbers on each platform.)\r\n\r\nIn my work with Xerion, I found it was critical to set the TCL-precision variable to it's highest value (17 or 18 or some such thing), if I wanted to write the network-weights out to a file, for subsequent loading at a later time, if the file was an ascii (not binary) numeric represetation.  The default was 8 digits after the decimal, and if you wrote and re-read network weights at that precision, your network was destroyed.  Weight digits 10 or 12 past the decimal place made a difference to network operation, and it became clear a very high level of precision was essential.   If you are working with neural networks, you might want to read W. Kahan's paper from his IEEE 754 lecture.  https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF    It illustrates some of the issues and edge-case conditions that floating-point calculations run up against, and the choices and trade-offs that were made to create a viable floating-point calculation standard.   It is a good paper, and it provides background on why floating-point calculations can fail to operate as expected.\r\n(Images of Laptest_64 sim on each platform follow..)\r\n![laptest_64_700_centos74_screenshot_2018-01-29_10_32pm](https://user-images.githubusercontent.com/16905336/35584502-34d92556-05c3-11e8-9485-b8629e501347.jpg)\r\n![laptest_64_700_57k_macos_screenshot_2018-01-30_1_24am](https://user-images.githubusercontent.com/16905336/35584523-46e5b3b8-05c3-11e8-92c3-8cf00b9bb8ab.jpg)\r\n"}