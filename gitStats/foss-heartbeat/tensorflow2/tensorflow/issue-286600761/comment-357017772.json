{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357017772", "html_url": "https://github.com/tensorflow/tensorflow/issues/15933#issuecomment-357017772", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15933", "id": 357017772, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzAxNzc3Mg==", "user": {"login": "Gemesys", "id": 16905336, "node_id": "MDQ6VXNlcjE2OTA1MzM2", "avatar_url": "https://avatars1.githubusercontent.com/u/16905336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gemesys", "html_url": "https://github.com/Gemesys", "followers_url": "https://api.github.com/users/Gemesys/followers", "following_url": "https://api.github.com/users/Gemesys/following{/other_user}", "gists_url": "https://api.github.com/users/Gemesys/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gemesys/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gemesys/subscriptions", "organizations_url": "https://api.github.com/users/Gemesys/orgs", "repos_url": "https://api.github.com/users/Gemesys/repos", "events_url": "https://api.github.com/users/Gemesys/events{/privacy}", "received_events_url": "https://api.github.com/users/Gemesys/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T18:27:25Z", "updated_at": "2018-01-11T18:31:17Z", "author_association": "NONE", "body_html": "<p>In case anyone is following this, here is the protocol for switching back to your custom-built TF version, from a binary one.  Just go to the directory where you built TF, and run ./configure to make sure you are pointing to your local python (I have a binary version installed in the ..frameworks area, and a local built one in /usr/local/bin with the site packages in /usr/local/lib/python2.7/site-packages/... ).  Do a \"bazel build\", (runs real quick, nothing needs to be compiled...), and then run \"bazel-bin/tensorflow/tools/pip_package/build_pip_package  /tmp/tensorflow_pkg\"  and you will get the .whl file in ./tmp/tensorflow_pkg.  Uninstall the binary TensorFlow  in python with \"pip uninstall tensorflow\".  Then change the name of the built .whl file if you need to  (mine gets built as \"tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl, and I have to rename or copy it to: \"tensorflow-1.4.1-py2-none-any.whl\", and you can then use  \"pip install ... \" to install it successfully.  Start Python, and \"import tensorflow as tf\".  This was failing for me, until it re-did the Bazel build.  (Note: I am using Bazel 0.9.0, built from source.)</p>\n<p>This gets me back to using my local-built TensorFlow and Python 2.7, with the unicode=ucs4 (same as the Linux binary Tensorflow and its local-built Python 2.7. ).   Note, you can check which unicode level your Python is built with using : \"import sys\" and then \"print sys.maxunicode\".  If you get \"1114111\", you are unicode=ucs4, if you get \"65535\", you are unicode=ucs2.</p>\n<p>Once I rebuilt TensorFlow pointing to the right Python, and rebuilt the .whl file, and un-installed and then installed the new wheel file with pip install, I was again able to successfully import it, and run the test programs.</p>\n<p>What is really curious, is that this switching between versions made absolutely no difference in the behaviour of the Macbook's version of the simulation test.  On the Macbook with macos 10.10.5, the test floating-point values all go to large numbers, (and a pure white image)  whereas on the other three platforms, the results oscillate between large positive and large negative numbers (and generate the moire-pattern image.).  All processors are little-endian Intel multi-core devices, and their behaviour should not diverge this way.   I've seen strange results like this before, and it was due to a serious flaw in a very low-level routine that converted between large floating-point values, and a 10-byte extended precision format that was rarely (but occasionally) used.  Only when code used the 80-bit field, did the numbers go squirrelly  - and when they did go off, it was only by a small amount.  But the problem would show up on scientific graphs - the numbering on the axis would be \"1 2 3 4 5 6 7 8 9 9\" instead of \"1 2 3 4 5 6 7 8 9 10\".      The situation here looks similar.  Very large floating point numbers are being flipped between registers, pipelines, and/or memory, and somewhere, some are getting mis-converted or precision is being dropped incorrectly - or maybe added incorrectly?</p>\n<p>These type of errors are not uncommon.  In the current MySql 5.7 release notes, deep on page 99 of an almost 400 page document, there is a reference to a JSON parsing error of a floating point number with a large negative exponent that apparently could crash the server.  Image attached.  Full document is at:  <a href=\"https://downloads.mysql.com/docs/mysql-5.7-relnotes-en.pdf\" rel=\"nofollow\">https://downloads.mysql.com/docs/mysql-5.7-relnotes-en.pdf</a></p>\n<p>As I said earlier, I will try to construct a more simple test-case that will illustrate the problem, but there is enough here that it might bear looking into by someone who knows the internals of TensorFlow.   It is possible that the Macbook Yosemite operation is correct, and that the other three platforms are propagating erroneous results, perhaps due to flawed interprocessor communication or pipelining/prefetching errors.  Until this evident calculation issue is understood, it is realistic to expect TensorFlow to give different training results on different machines of similar architecture, which is a real concern.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/16905336/34840668-b0adfca0-f6d3-11e7-8a39-e5f96a57db35.jpg\"><img src=\"https://user-images.githubusercontent.com/16905336/34840668-b0adfca0-f6d3-11e7-8a39-e5f96a57db35.jpg\" alt=\"example_of_floating_point_exponent_parse_fail\" style=\"max-width:100%;\"></a></p>", "body_text": "In case anyone is following this, here is the protocol for switching back to your custom-built TF version, from a binary one.  Just go to the directory where you built TF, and run ./configure to make sure you are pointing to your local python (I have a binary version installed in the ..frameworks area, and a local built one in /usr/local/bin with the site packages in /usr/local/lib/python2.7/site-packages/... ).  Do a \"bazel build\", (runs real quick, nothing needs to be compiled...), and then run \"bazel-bin/tensorflow/tools/pip_package/build_pip_package  /tmp/tensorflow_pkg\"  and you will get the .whl file in ./tmp/tensorflow_pkg.  Uninstall the binary TensorFlow  in python with \"pip uninstall tensorflow\".  Then change the name of the built .whl file if you need to  (mine gets built as \"tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl, and I have to rename or copy it to: \"tensorflow-1.4.1-py2-none-any.whl\", and you can then use  \"pip install ... \" to install it successfully.  Start Python, and \"import tensorflow as tf\".  This was failing for me, until it re-did the Bazel build.  (Note: I am using Bazel 0.9.0, built from source.)\nThis gets me back to using my local-built TensorFlow and Python 2.7, with the unicode=ucs4 (same as the Linux binary Tensorflow and its local-built Python 2.7. ).   Note, you can check which unicode level your Python is built with using : \"import sys\" and then \"print sys.maxunicode\".  If you get \"1114111\", you are unicode=ucs4, if you get \"65535\", you are unicode=ucs2.\nOnce I rebuilt TensorFlow pointing to the right Python, and rebuilt the .whl file, and un-installed and then installed the new wheel file with pip install, I was again able to successfully import it, and run the test programs.\nWhat is really curious, is that this switching between versions made absolutely no difference in the behaviour of the Macbook's version of the simulation test.  On the Macbook with macos 10.10.5, the test floating-point values all go to large numbers, (and a pure white image)  whereas on the other three platforms, the results oscillate between large positive and large negative numbers (and generate the moire-pattern image.).  All processors are little-endian Intel multi-core devices, and their behaviour should not diverge this way.   I've seen strange results like this before, and it was due to a serious flaw in a very low-level routine that converted between large floating-point values, and a 10-byte extended precision format that was rarely (but occasionally) used.  Only when code used the 80-bit field, did the numbers go squirrelly  - and when they did go off, it was only by a small amount.  But the problem would show up on scientific graphs - the numbering on the axis would be \"1 2 3 4 5 6 7 8 9 9\" instead of \"1 2 3 4 5 6 7 8 9 10\".      The situation here looks similar.  Very large floating point numbers are being flipped between registers, pipelines, and/or memory, and somewhere, some are getting mis-converted or precision is being dropped incorrectly - or maybe added incorrectly?\nThese type of errors are not uncommon.  In the current MySql 5.7 release notes, deep on page 99 of an almost 400 page document, there is a reference to a JSON parsing error of a floating point number with a large negative exponent that apparently could crash the server.  Image attached.  Full document is at:  https://downloads.mysql.com/docs/mysql-5.7-relnotes-en.pdf\nAs I said earlier, I will try to construct a more simple test-case that will illustrate the problem, but there is enough here that it might bear looking into by someone who knows the internals of TensorFlow.   It is possible that the Macbook Yosemite operation is correct, and that the other three platforms are propagating erroneous results, perhaps due to flawed interprocessor communication or pipelining/prefetching errors.  Until this evident calculation issue is understood, it is realistic to expect TensorFlow to give different training results on different machines of similar architecture, which is a real concern.", "body": "In case anyone is following this, here is the protocol for switching back to your custom-built TF version, from a binary one.  Just go to the directory where you built TF, and run ./configure to make sure you are pointing to your local python (I have a binary version installed in the ..frameworks area, and a local built one in /usr/local/bin with the site packages in /usr/local/lib/python2.7/site-packages/... ).  Do a \"bazel build\", (runs real quick, nothing needs to be compiled...), and then run \"bazel-bin/tensorflow/tools/pip_package/build_pip_package  /tmp/tensorflow_pkg\"  and you will get the .whl file in ./tmp/tensorflow_pkg.  Uninstall the binary TensorFlow  in python with \"pip uninstall tensorflow\".  Then change the name of the built .whl file if you need to  (mine gets built as \"tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl, and I have to rename or copy it to: \"tensorflow-1.4.1-py2-none-any.whl\", and you can then use  \"pip install ... \" to install it successfully.  Start Python, and \"import tensorflow as tf\".  This was failing for me, until it re-did the Bazel build.  (Note: I am using Bazel 0.9.0, built from source.)\r\n\r\nThis gets me back to using my local-built TensorFlow and Python 2.7, with the unicode=ucs4 (same as the Linux binary Tensorflow and its local-built Python 2.7. ).   Note, you can check which unicode level your Python is built with using : \"import sys\" and then \"print sys.maxunicode\".  If you get \"1114111\", you are unicode=ucs4, if you get \"65535\", you are unicode=ucs2.\r\n\r\nOnce I rebuilt TensorFlow pointing to the right Python, and rebuilt the .whl file, and un-installed and then installed the new wheel file with pip install, I was again able to successfully import it, and run the test programs.\r\n\r\nWhat is really curious, is that this switching between versions made absolutely no difference in the behaviour of the Macbook's version of the simulation test.  On the Macbook with macos 10.10.5, the test floating-point values all go to large numbers, (and a pure white image)  whereas on the other three platforms, the results oscillate between large positive and large negative numbers (and generate the moire-pattern image.).  All processors are little-endian Intel multi-core devices, and their behaviour should not diverge this way.   I've seen strange results like this before, and it was due to a serious flaw in a very low-level routine that converted between large floating-point values, and a 10-byte extended precision format that was rarely (but occasionally) used.  Only when code used the 80-bit field, did the numbers go squirrelly  - and when they did go off, it was only by a small amount.  But the problem would show up on scientific graphs - the numbering on the axis would be \"1 2 3 4 5 6 7 8 9 9\" instead of \"1 2 3 4 5 6 7 8 9 10\".      The situation here looks similar.  Very large floating point numbers are being flipped between registers, pipelines, and/or memory, and somewhere, some are getting mis-converted or precision is being dropped incorrectly - or maybe added incorrectly?    \r\n\r\nThese type of errors are not uncommon.  In the current MySql 5.7 release notes, deep on page 99 of an almost 400 page document, there is a reference to a JSON parsing error of a floating point number with a large negative exponent that apparently could crash the server.  Image attached.  Full document is at:  https://downloads.mysql.com/docs/mysql-5.7-relnotes-en.pdf \r\n\r\nAs I said earlier, I will try to construct a more simple test-case that will illustrate the problem, but there is enough here that it might bear looking into by someone who knows the internals of TensorFlow.   It is possible that the Macbook Yosemite operation is correct, and that the other three platforms are propagating erroneous results, perhaps due to flawed interprocessor communication or pipelining/prefetching errors.  Until this evident calculation issue is understood, it is realistic to expect TensorFlow to give different training results on different machines of similar architecture, which is a real concern.   \r\n\r\n![example_of_floating_point_exponent_parse_fail](https://user-images.githubusercontent.com/16905336/34840668-b0adfca0-f6d3-11e7-8a39-e5f96a57db35.jpg)\r\n"}