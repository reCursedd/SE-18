{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/238745104", "html_url": "https://github.com/tensorflow/tensorflow/issues/3705#issuecomment-238745104", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3705", "id": 238745104, "node_id": "MDEyOklzc3VlQ29tbWVudDIzODc0NTEwNA==", "user": {"login": "DjangoPeng", "id": 16943353, "node_id": "MDQ6VXNlcjE2OTQzMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16943353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DjangoPeng", "html_url": "https://github.com/DjangoPeng", "followers_url": "https://api.github.com/users/DjangoPeng/followers", "following_url": "https://api.github.com/users/DjangoPeng/following{/other_user}", "gists_url": "https://api.github.com/users/DjangoPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/DjangoPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DjangoPeng/subscriptions", "organizations_url": "https://api.github.com/users/DjangoPeng/orgs", "repos_url": "https://api.github.com/users/DjangoPeng/repos", "events_url": "https://api.github.com/users/DjangoPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/DjangoPeng/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-10T01:55:24Z", "updated_at": "2016-08-10T02:08:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> Ok\uff0clet me be clear.<br>\nI add two <code>prints</code> between the <code>session.run</code>:</p>\n<pre><code># Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\nprint(\"------------------------------------------------------------------------------\")\nprint(\"------------------------------------------------------------------------------\")\n\nprint(\"start running session\")\noutputs = session.run(output_feed, input_feed)\nprint(\"finish running session\")\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n</code></pre>\n<p>And this is the standard ouput:</p>\n<blockquote>\n<p>reading data line 100000<br>\nreading data line 200000<br>\nfinish reading data<br>\nready to train<br>\nstart training loop:0<br>\nstart_time:1470835889.1598</p>\n<hr>\n<hr>\n<p>start running session<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7061 get requests, put_count=3604 evicted_count=1000 eviction_rate=0.277469 and unsatisfied allocation rate=0.645376<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110</p>\n</blockquote>\n<p>As you can see, the <strong>finish running session</strong> is not printed at the std.out, cause it's hanging inside the <code>sess.run()</code>.</p>", "body_text": "@girving Ok\uff0clet me be clear.\nI add two prints between the session.run:\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\nprint(\"------------------------------------------------------------------------------\")\nprint(\"------------------------------------------------------------------------------\")\n\nprint(\"start running session\")\noutputs = session.run(output_feed, input_feed)\nprint(\"finish running session\")\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\nAnd this is the standard ouput:\n\nreading data line 100000\nreading data line 200000\nfinish reading data\nready to train\nstart training loop:0\nstart_time:1470835889.1598\n\n\nstart running session\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7061 get requests, put_count=3604 evicted_count=1000 eviction_rate=0.277469 and unsatisfied allocation rate=0.645376\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n\nAs you can see, the finish running session is not printed at the std.out, cause it's hanging inside the sess.run().", "body": "@girving Ok\uff0clet me be clear.\nI add two `prints` between the `session.run`:\n\n```\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\nprint(\"------------------------------------------------------------------------------\")\nprint(\"------------------------------------------------------------------------------\")\n\nprint(\"start running session\")\noutputs = session.run(output_feed, input_feed)\nprint(\"finish running session\")\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n```\n\nAnd this is the standard ouput:\n\n> reading data line 100000\n>    reading data line 200000\n>    finish reading data\n>    ready to train\n>    start training loop:0\n>    start_time:1470835889.1598\n> \n> ---\n> \n> ---\n> \n>    start running session\n>    I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7061 get requests, put_count=3604 evicted_count=1000 eviction_rate=0.277469 and unsatisfied allocation rate=0.645376\n>    I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n\nAs you can see, the **finish running session** is not printed at the std.out, cause it's hanging inside the `sess.run()`.\n"}