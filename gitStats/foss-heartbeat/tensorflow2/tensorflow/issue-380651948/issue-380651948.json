{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23733", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23733/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23733/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23733/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23733", "id": 380651948, "node_id": "MDU6SXNzdWUzODA2NTE5NDg=", "number": 23733, "title": "Memory leak in tf.train.Example/tf.train.Features/tf.gfile", "user": {"login": "galeone", "id": 8427788, "node_id": "MDQ6VXNlcjg0Mjc3ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/8427788?v=4", "gravatar_id": "", "url": "https://api.github.com/users/galeone", "html_url": "https://github.com/galeone", "followers_url": "https://api.github.com/users/galeone/followers", "following_url": "https://api.github.com/users/galeone/following{/other_user}", "gists_url": "https://api.github.com/users/galeone/gists{/gist_id}", "starred_url": "https://api.github.com/users/galeone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/galeone/subscriptions", "organizations_url": "https://api.github.com/users/galeone/orgs", "repos_url": "https://api.github.com/users/galeone/repos", "events_url": "https://api.github.com/users/galeone/events{/privacy}", "received_events_url": "https://api.github.com/users/galeone/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097543484, "node_id": "MDU6TGFiZWwxMDk3NTQzNDg0", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:runtime", "name": "comp:runtime", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-11-14T11:18:42Z", "updated_at": "2018-11-21T01:04:07Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux</li>\n<li>TensorFlow installed from (source or binary): repository</li>\n<li>TensorFlow version (use command below): 1.12</li>\n<li>Python version: 3.7</li>\n<li>CUDA/cuDNN version: cuda 10, cudnn 7</li>\n<li>GPU model and memory: nvidia 1080ti</li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>The system goes OOM and I have 64 GiB of memory.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>I should be able to create a dataset converting files I read from Google Cloud to local tfrecords.</p>\n<p>I'm debugging the memory usage of python and as you can see below, the usage is around 670K - hence the memory allocation should be outside of python, and the only library I'm using is Tensorflow.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> subprocess\n<span class=\"pl-k\">import</span> multiprocessing\n<span class=\"pl-k\">from</span> glob <span class=\"pl-k\">import</span> glob\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow <span class=\"pl-k\">import</span> keras <span class=\"pl-k\">as</span> k\n<span class=\"pl-k\">import</span> json\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> logging\n<span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Pattern, List, Tuple, Callable\n<span class=\"pl-k\">import</span> linecache\n<span class=\"pl-k\">import</span> tracemalloc\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">display_top</span>(<span class=\"pl-smi\">snapshot</span>, <span class=\"pl-smi\">key_type</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lineno<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-smi\">limit</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>):\n    snapshot <span class=\"pl-k\">=</span> snapshot.filter_traces(\n        (\n            tracemalloc.Filter(<span class=\"pl-c1\">False</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;frozen importlib._bootstrap&gt;<span class=\"pl-pds\">\"</span></span>),\n            tracemalloc.Filter(<span class=\"pl-c1\">False</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;unknown&gt;<span class=\"pl-pds\">\"</span></span>),\n        )\n    )\n    top_stats <span class=\"pl-k\">=</span> snapshot.statistics(key_type)\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Top <span class=\"pl-c1\">%s</span> lines<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> limit)\n    <span class=\"pl-k\">for</span> index, stat <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(top_stats[:limit], <span class=\"pl-c1\">1</span>):\n        frame <span class=\"pl-k\">=</span> stat.traceback[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> replace \"/path/to/module/file.py\" with \"module/file.py\"</span>\n        filename <span class=\"pl-k\">=</span> os.sep.join(frame.filename.split(os.sep)[<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>:])\n        <span class=\"pl-c1\">print</span>(\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>#<span class=\"pl-c1\">%s</span>: <span class=\"pl-c1\">%s</span>:<span class=\"pl-c1\">%s</span>: <span class=\"pl-c1\">%.1f</span> KiB<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (index, filename, frame.lineno, stat.size <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1024</span>)\n        )\n        line <span class=\"pl-k\">=</span> linecache.getline(frame.filename, frame.lineno).strip()\n        <span class=\"pl-k\">if</span> line:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> line)\n\n    other <span class=\"pl-k\">=</span> top_stats[limit:]\n    <span class=\"pl-k\">if</span> other:\n        size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sum</span>(stat.size <span class=\"pl-k\">for</span> stat <span class=\"pl-k\">in</span> other)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%s</span> other: <span class=\"pl-c1\">%.1f</span> KiB<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (<span class=\"pl-c1\">len</span>(other), size <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1024</span>))\n    total <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sum</span>(stat.size <span class=\"pl-k\">for</span> stat <span class=\"pl-k\">in</span> top_stats)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Total allocated size: <span class=\"pl-c1\">%.1f</span> KiB<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (total <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1024</span>))\n\n\ntracemalloc.start()\n\n<span class=\"pl-c1\">CACHE</span> <span class=\"pl-k\">=</span> os.path.join(os.getcwd(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.data<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-c1\">ROWS_PER_RECORD</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n<span class=\"pl-c1\">SPLITS</span> <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>validation<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c1\">LOG</span> <span class=\"pl-k\">=</span> logging.Logger(<span class=\"pl-c1\">__name__</span>)\n<span class=\"pl-c1\">LOG</span>.setLevel(logging.<span class=\"pl-c1\">INFO</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_int64_feature</span>(<span class=\"pl-smi\">value</span>):\n    <span class=\"pl-k\">return</span> tf.train.Feature(<span class=\"pl-v\">int64_list</span><span class=\"pl-k\">=</span>tf.train.Int64List(<span class=\"pl-v\">value</span><span class=\"pl-k\">=</span>[value]))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_float_feature</span>(<span class=\"pl-smi\">value</span>):\n    <span class=\"pl-k\">return</span> tf.train.Feature(<span class=\"pl-v\">float_list</span><span class=\"pl-k\">=</span>tf.train.FloatList(<span class=\"pl-v\">value</span><span class=\"pl-k\">=</span>[value]))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_bytes_feature</span>(<span class=\"pl-smi\">value</span>):\n    <span class=\"pl-k\">return</span> tf.train.Feature(<span class=\"pl-v\">bytes_list</span><span class=\"pl-k\">=</span>tf.train.BytesList(<span class=\"pl-v\">value</span><span class=\"pl-k\">=</span>[value]))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">rows_in_tfrecord</span>(<span class=\"pl-smi\">path</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Count the number of elements in a tfrecord file.</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        path: The path of the TFRecord file</span>\n<span class=\"pl-s\">    Return:</span>\n<span class=\"pl-s\">        The number of examples in the TFRecord</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">sum</span>(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> tf.io.tf_record_iterator(path))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_is_dir</span>(<span class=\"pl-smi\">path</span>: <span class=\"pl-c1\">str</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Determines if path is a directory only looking at the filename.</span>\n<span class=\"pl-s\">    It's ugly, but is the only way to get a decent speed.</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        path: The input path</span>\n<span class=\"pl-s\">    Return:</span>\n<span class=\"pl-s\">        value: True if the path looks like a directory</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">return</span> path.endswith(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">or</span> <span class=\"pl-k\">not</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">in</span> path.split(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/<span class=\"pl-pds\">\"</span></span>)[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">list_dir</span>(<span class=\"pl-smi\">root</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Recursively list a directory and returns the list of all the files.</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        root: The path of the root directory</span>\n<span class=\"pl-s\">    Return:</span>\n<span class=\"pl-s\">        List of all the files found in this directory and its subfolders.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    ret <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> full_path <span class=\"pl-k\">in</span> tf.gfile.Glob(os.path.join(root, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*<span class=\"pl-pds\">\"</span></span>)):\n        <span class=\"pl-k\">if</span> _is_dir(full_path):\n            <span class=\"pl-k\">return</span> ret <span class=\"pl-k\">+</span> list_dir(full_path)\n        <span class=\"pl-k\">else</span>:\n            ret.append(full_path)\n    <span class=\"pl-k\">return</span> ret\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_gs2tfrecord</span>(<span class=\"pl-smi\">gs_paths</span>: List[<span class=\"pl-c1\">str</span>], <span class=\"pl-smi\">patterns</span>: List[Pattern], <span class=\"pl-smi\">cache</span>: <span class=\"pl-c1\">str</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">CACHE</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Fetch images from google cloud storage, conver to tf_record and</span>\n<span class=\"pl-s\">    properly handle cache.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        gs_paths: list of paths on google cloud storage</span>\n<span class=\"pl-s\">        patterns: list of compiled regex to fiter the data on each gs_paths</span>\n<span class=\"pl-s\">        cache: local cache/dataset folder</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    gcloud_auth <span class=\"pl-k\">=</span> os.path.join(\n        os.path.expanduser(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~<span class=\"pl-pds\">\"</span></span>),\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.config<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gcloud<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>application_default_credentials.json<span class=\"pl-pds\">\"</span></span>,\n    )\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(gcloud_auth):\n        subprocess.check_call([<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gcloud<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>auth<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>application-default<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>login<span class=\"pl-pds\">\"</span></span>])\n\n    meta_file <span class=\"pl-k\">=</span> os.path.join(cache, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>meta.json<span class=\"pl-pds\">\"</span></span>)\n    dataset_path <span class=\"pl-k\">=</span> os.path.join(cache, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dataset<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(cache):\n        os.makedirs(dataset_path)\n        <span class=\"pl-k\">for</span> split <span class=\"pl-k\">in</span> <span class=\"pl-c1\">SPLITS</span>:\n            os.makedirs(os.path.join(dataset_path, split))\n        meta <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>local_files<span class=\"pl-pds\">\"</span></span>: {path: [] <span class=\"pl-k\">for</span> path <span class=\"pl-k\">in</span> gs_paths}}\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(meta_file, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>r<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n            meta <span class=\"pl-k\">=</span> json.load(fp)\n\n    <span class=\"pl-k\">for</span> remote_path, pattern <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(gs_paths, patterns):\n        remote_files <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>()\n        <span class=\"pl-k\">for</span> name <span class=\"pl-k\">in</span> list_dir(remote_path):\n            <span class=\"pl-k\">if</span> pattern.search(name.replace(remote_path, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>)):\n                remote_files.add(name)\n\n        local_files <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>(meta[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>local_files<span class=\"pl-pds\">\"</span></span>][remote_path])\n\n        diff <span class=\"pl-k\">=</span> remote_files <span class=\"pl-k\">-</span> local_files\n        <span class=\"pl-c1\">LOG</span>.warn(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>New remote elements: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">len</span>(diff)))\n        <span class=\"pl-k\">if</span> diff:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Check the current tfrecords, find the last one created</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Fill the empty spaces in this one (rewriting it completely)</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> And create the other ones</span>\n            record_id <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sum</span>(\n                <span class=\"pl-c1\">len</span>(glob(os.path.join(dataset_path, split, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.tfrecord<span class=\"pl-pds\">\"</span></span>)))\n                <span class=\"pl-k\">for</span> split <span class=\"pl-k\">in</span> <span class=\"pl-c1\">SPLITS</span>\n            )\n\n            last_tf_record <span class=\"pl-k\">=</span> os.path.join(dataset_path, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1.tfrecord<span class=\"pl-pds\">\"</span></span>)\n            <span class=\"pl-k\">for</span> split <span class=\"pl-k\">in</span> <span class=\"pl-c1\">SPLITS</span>:\n                last_path <span class=\"pl-k\">=</span> os.path.join(dataset_path, split, <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{</span>record_id<span class=\"pl-c1\">}</span><span class=\"pl-s\">.tfrecord</span><span class=\"pl-pds\">\"</span>)\n                <span class=\"pl-k\">if</span> os.path.exists(last_path):\n                    last_tf_record <span class=\"pl-k\">=</span> last_path\n                    <span class=\"pl-k\">break</span>\n\n            examples <span class=\"pl-k\">=</span> []\n            <span class=\"pl-k\">if</span> os.path.exists(last_tf_record):\n                rows_in_last <span class=\"pl-k\">=</span> rows_in_tfrecord(last_tf_record)\n                <span class=\"pl-k\">if</span> rows_in_last <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">ROWS_PER_RECORD</span>:\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> read all rows in the last tfrecord</span>\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> we're going to add the last one</span>\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> and recreate it</span>\n                    <span class=\"pl-k\">for</span> row <span class=\"pl-k\">in</span> tf.io.tf_record_iterator(last_tf_record):\n                        example <span class=\"pl-k\">=</span> tf.train.Example()\n                        examples.append(example.ParseFromString(row))\n                <span class=\"pl-k\">else</span>:\n                    record_id <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> fetch new files and update the tfrecords</span>\n            tot_new_elements <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(diff)\n            <span class=\"pl-k\">for</span> idx, new_element <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(diff):\n                <span class=\"pl-c1\">LOG</span>.warn(<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">File: </span><span class=\"pl-c1\">{</span>new_element<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>)\n                <span class=\"pl-k\">with</span> tf.gfile.GFile(new_element, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rb<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n                    img <span class=\"pl-k\">=</span> fp.read()\n                remote_meta <span class=\"pl-k\">=</span> os.path.join(os.path.realpath(new_element), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>meta.json<span class=\"pl-pds\">\"</span></span>)\n                <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> tf.gfile.Exists(remote_meta):\n                    example_meta <span class=\"pl-k\">=</span> {\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>count<span class=\"pl-pds\">\"</span></span>: _int64_feature(<span class=\"pl-c1\">1</span>),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feature1<span class=\"pl-pds\">\"</span></span>: _float_feature(<span class=\"pl-c1\">1076</span>),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feature2<span class=\"pl-pds\">\"</span></span>: _float_feature(<span class=\"pl-c1\">100</span>),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f3<span class=\"pl-pds\">\"</span></span>: _float_feature(<span class=\"pl-c1\">1076</span>),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f4<span class=\"pl-pds\">\"</span></span>: _float_feature(<span class=\"pl-c1\">100</span>),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f5<span class=\"pl-pds\">\"</span></span>: _bytes_feature(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mango<span class=\"pl-pds\">\"</span></span>.encode(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>UTF-8<span class=\"pl-pds\">\"</span></span>)),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f7<span class=\"pl-pds\">\"</span></span>: _bytes_feature(\n                            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>banana<span class=\"pl-pds\">\"</span></span>.encode(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>UTF-8<span class=\"pl-pds\">\"</span></span>)\n                        ),\n                    }\n                <span class=\"pl-k\">else</span>:\n                    <span class=\"pl-k\">with</span> tf.gfile.GFile(remote_meta, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>r<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n                        example_meta <span class=\"pl-k\">=</span> json.loads(fp.read())\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>: check if image is an old style image and convert to new format</span>\n                example_meta[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>img<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> _bytes_feature(img)\n                examples.append(\n                    tf.train.Example(<span class=\"pl-v\">features</span><span class=\"pl-k\">=</span>tf.train.Features(<span class=\"pl-v\">feature</span><span class=\"pl-k\">=</span>example_meta))\n                )\n\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(examples) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">ROWS_PER_RECORD</span> <span class=\"pl-k\">or</span> idx <span class=\"pl-k\">==</span> tot_new_elements <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>:\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> write (always in the training dataset) and reset examples buffer</span>\n                    filename <span class=\"pl-k\">=</span> os.path.join(\n                        dataset_path, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{</span>record_id<span class=\"pl-c1\">}</span><span class=\"pl-s\">.tfrecord</span><span class=\"pl-pds\">\"</span>\n                    )\n                    <span class=\"pl-k\">with</span> tf.io.TFRecordWriter(filename) <span class=\"pl-k\">as</span> writer:\n                        <span class=\"pl-k\">for</span> example <span class=\"pl-k\">in</span> examples:\n                            writer.write(example.SerializeToString())\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reset example buffer</span>\n                    examples.clear()\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Increment tfrecord id</span>\n                    record_id <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n                    <span class=\"pl-c1\">LOG</span>.warn(<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">TFRecord: </span><span class=\"pl-c1\">{</span>filename<span class=\"pl-c1\">}</span><span class=\"pl-s\"> written</span><span class=\"pl-pds\">\"</span>)\n                    snapshot <span class=\"pl-k\">=</span> tracemalloc.take_snapshot()\n                    display_top(snapshot)\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> break</span>\n</pre></div>\n<p>Just use the <code>_gs2tfrecord</code> function. I'm using it to fetch data from Google Cloud Storage, but since I'm using <code>tf.gfile</code> a local path can be used too.</p>\n<p>My guess is that leak can be somewhere in <code>tf.train.Example</code> or in <code>tf.train.Feature</code> since those are the only 2 functions I call in a loop.</p>\n<p><strong>UPDATE</strong></p>\n<p>I've created another function that just download the images from Google Cloud and store them locally. It goes out of memory in the same way.</p>\n<p>Maybe the leak is in <code>tf.gfile</code>?</p>\n<p>Here's the function</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_gs2folder</span>(\n    <span class=\"pl-smi\">gs_paths</span>: List[<span class=\"pl-c1\">str</span>], <span class=\"pl-smi\">patterns</span>: List[Pattern], <span class=\"pl-smi\">names</span>: List[<span class=\"pl-c1\">str</span>], <span class=\"pl-smi\">cache</span>: <span class=\"pl-c1\">str</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">CACHE</span>\n):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Fetch images from google cloud storage, conver to tf_record and</span>\n<span class=\"pl-s\">    properly handle cache.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        gs_paths: list of paths on google cloud storage</span>\n<span class=\"pl-s\">        patterns: list of compiled regex to fiter the data on each gs_paths</span>\n<span class=\"pl-s\">        names: list of dataset name</span>\n<span class=\"pl-s\">        cache: local cache/dataset folder</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(gs_paths) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">len</span>(patterns) <span class=\"pl-k\">and</span> <span class=\"pl-c1\">len</span>(patterns) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">len</span>(names)\n\n    gcloud_auth <span class=\"pl-k\">=</span> os.path.join(\n        os.path.expanduser(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~<span class=\"pl-pds\">\"</span></span>),\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.config<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gcloud<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>application_default_credentials.json<span class=\"pl-pds\">\"</span></span>,\n    )\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(gcloud_auth):\n        subprocess.check_call([<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gcloud<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>auth<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>application-default<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>login<span class=\"pl-pds\">\"</span></span>])\n\n    <span class=\"pl-k\">for</span> remote_path, pattern, name <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(gs_paths, patterns, names):\n        meta_file <span class=\"pl-k\">=</span> os.path.join(cache, name, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>meta.json<span class=\"pl-pds\">\"</span></span>)\n        dataset_path <span class=\"pl-k\">=</span> os.path.join(cache, name, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>original<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(cache):\n            os.makedirs(dataset_path)\n            meta <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>local_files<span class=\"pl-pds\">\"</span></span>: {path: [] <span class=\"pl-k\">for</span> path <span class=\"pl-k\">in</span> gs_paths}}\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(meta_file, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>r<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n                meta <span class=\"pl-k\">=</span> json.load(fp)\n\n        remote_files <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>()\n        <span class=\"pl-k\">for</span> file_name <span class=\"pl-k\">in</span> list_dir(remote_path):\n            <span class=\"pl-k\">if</span> pattern.search(file_name.replace(remote_path, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>)):\n                remote_files.add(file_name)\n\n        local_files <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>(meta[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>local_files<span class=\"pl-pds\">\"</span></span>][remote_path])\n        diff <span class=\"pl-k\">=</span> remote_files <span class=\"pl-k\">-</span> local_files\n        <span class=\"pl-c1\">LOG</span>.warning(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>New remote elements: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">len</span>(diff)))\n        last_id <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(local_files) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n        <span class=\"pl-k\">if</span> diff:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> fetch new files and store them</span>\n            tot_new_elements <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(diff)\n\n            <span class=\"pl-k\">for</span> idx, new_element <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(diff):\n                <span class=\"pl-c1\">LOG</span>.warning(<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">File: </span><span class=\"pl-c1\">{</span>new_element<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>)\n                <span class=\"pl-k\">with</span> tf.gfile.GFile(new_element, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rb<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n                    img_bytes <span class=\"pl-k\">=</span> fp.read()\n\n                image <span class=\"pl-k\">=</span> Image.open(io.BytesIO(img_bytes))\n                image <span class=\"pl-k\">=</span> np.array(image)\n                <span class=\"pl-k\">if</span> image.shape[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">==</span> <span class=\"pl-c1\">4</span>:\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> remove transparency</span>\n                    image <span class=\"pl-k\">=</span> image[<span class=\"pl-c1\">...</span>, :<span class=\"pl-c1\">3</span>]\n                remote_meta <span class=\"pl-k\">=</span> os.path.join(os.path.realpath(new_element), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>meta.json<span class=\"pl-pds\">\"</span></span>)\n                <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> tf.gfile.Exists(remote_meta):\n                    example_meta <span class=\"pl-k\">=</span> {\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>count<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1</span>\n                    }\n                <span class=\"pl-k\">else</span>:\n                    <span class=\"pl-k\">with</span> tf.gfile.GFile(remote_meta, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>r<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n                        example_meta <span class=\"pl-k\">=</span> json.loads(fp.read())\n\n                image <span class=\"pl-k\">=</span> Image.fromarray(image)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Just save the image file with its meta info</span>\n                image.save(os.path.join(dataset_path, <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{</span>last_id<span class=\"pl-c1\">}</span><span class=\"pl-s\">.png</span><span class=\"pl-pds\">\"</span>))\n                <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(os.path.join(dataset_path, <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{</span>last_id<span class=\"pl-c1\">}</span><span class=\"pl-s\">.json</span><span class=\"pl-pds\">\"</span>), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>w<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n                    json.dump(example_meta, fp)\n                last_id <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n\n        meta[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>local_files<span class=\"pl-pds\">\"</span></span>][remote_path] <span class=\"pl-k\">=</span> remote_files\n\n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(meta, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>w<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> fp:\n        json.dump(meta, fp)</pre></div>\n<p><strong>Other info / logs</strong></p>\n<p>This is the output of a <code>display_top</code> invocation:</p>\n<pre><code>Top 10 lines\n#1: python3.7/linecache.py:137: 246.9 KiB\n    lines = fp.readlines()\n#2: util/compat.py:80: 150.0 KiB\n    return bytes_or_text.decode(encoding)\n#3: datasets/floorplans.py:143: 64.2 KiB\n    diff = remote_files - local_files\n#4: datasets/floorplans.py:139: 32.0 KiB\n    remote_files.add(name)\n#5: internal/python_message.py:475: 21.6 KiB\n    self._oneofs = {}\n#6: internal/python_message.py:425: 15.5 KiB\n    result = message_type._concrete_class()\n#7: python3.7/tracemalloc.py:185: 13.4 KiB\n    self._frames = tuple(reversed(frames))\n#8: internal/python_message.py:472: 12.0 KiB\n    self._fields = {}\n#9: internal/python_message.py:1402: 10.3 KiB\n    self._parent_message_weakref = weakref.proxy(parent_message)\n#10: internal/python_message.py:1155: 5.1 KiB\n    for field, value in list(self._fields.items()):  # dict can change size!\n228 other: 104.0 KiB\nTotal allocated size: 675.1 KiB\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\nTensorFlow installed from (source or binary): repository\nTensorFlow version (use command below): 1.12\nPython version: 3.7\nCUDA/cuDNN version: cuda 10, cudnn 7\nGPU model and memory: nvidia 1080ti\n\nDescribe the current behavior\nThe system goes OOM and I have 64 GiB of memory.\nDescribe the expected behavior\nI should be able to create a dataset converting files I read from Google Cloud to local tfrecords.\nI'm debugging the memory usage of python and as you can see below, the usage is around 670K - hence the memory allocation should be outside of python, and the only library I'm using is Tensorflow.\nCode to reproduce the issue\nimport subprocess\nimport multiprocessing\nfrom glob import glob\nimport tensorflow as tf\nfrom tensorflow import keras as k\nimport json\nimport os\nimport logging\nfrom typing import Pattern, List, Tuple, Callable\nimport linecache\nimport tracemalloc\n\n\ndef display_top(snapshot, key_type=\"lineno\", limit=10):\n    snapshot = snapshot.filter_traces(\n        (\n            tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n            tracemalloc.Filter(False, \"<unknown>\"),\n        )\n    )\n    top_stats = snapshot.statistics(key_type)\n\n    print(\"Top %s lines\" % limit)\n    for index, stat in enumerate(top_stats[:limit], 1):\n        frame = stat.traceback[0]\n        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n        print(\n            \"#%s: %s:%s: %.1f KiB\" % (index, filename, frame.lineno, stat.size / 1024)\n        )\n        line = linecache.getline(frame.filename, frame.lineno).strip()\n        if line:\n            print(\"    %s\" % line)\n\n    other = top_stats[limit:]\n    if other:\n        size = sum(stat.size for stat in other)\n        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n    total = sum(stat.size for stat in top_stats)\n    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n\n\ntracemalloc.start()\n\nCACHE = os.path.join(os.getcwd(), \".data\")\nROWS_PER_RECORD = 32\nSPLITS = (\"train\", \"validation\", \"test\")\n\nLOG = logging.Logger(__name__)\nLOG.setLevel(logging.INFO)\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _float_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef rows_in_tfrecord(path):\n    \"\"\"Count the number of elements in a tfrecord file.\n    Args:\n        path: The path of the TFRecord file\n    Return:\n        The number of examples in the TFRecord\n    \"\"\"\n    return sum(1 for _ in tf.io.tf_record_iterator(path))\n\n\ndef _is_dir(path: str):\n    \"\"\"Determines if path is a directory only looking at the filename.\n    It's ugly, but is the only way to get a decent speed.\n    Args:\n        path: The input path\n    Return:\n        value: True if the path looks like a directory\n    \"\"\"\n    return path.endswith(\"/\") or not \".\" in path.split(\"/\")[-1]\n\n\ndef list_dir(root):\n    \"\"\"Recursively list a directory and returns the list of all the files.\n    Args:\n        root: The path of the root directory\n    Return:\n        List of all the files found in this directory and its subfolders.\n    \"\"\"\n    ret = []\n    for full_path in tf.gfile.Glob(os.path.join(root, \"*\")):\n        if _is_dir(full_path):\n            return ret + list_dir(full_path)\n        else:\n            ret.append(full_path)\n    return ret\n\n\ndef _gs2tfrecord(gs_paths: List[str], patterns: List[Pattern], cache: str = CACHE):\n    \"\"\"Fetch images from google cloud storage, conver to tf_record and\n    properly handle cache.\n\n    Args:\n        gs_paths: list of paths on google cloud storage\n        patterns: list of compiled regex to fiter the data on each gs_paths\n        cache: local cache/dataset folder\n    \"\"\"\n    gcloud_auth = os.path.join(\n        os.path.expanduser(\"~\"),\n        \".config\",\n        \"gcloud\",\n        \"application_default_credentials.json\",\n    )\n    if not os.path.exists(gcloud_auth):\n        subprocess.check_call([\"gcloud\", \"auth\", \"application-default\", \"login\"])\n\n    meta_file = os.path.join(cache, \"meta.json\")\n    dataset_path = os.path.join(cache, \"dataset\")\n    if not os.path.exists(cache):\n        os.makedirs(dataset_path)\n        for split in SPLITS:\n            os.makedirs(os.path.join(dataset_path, split))\n        meta = {\"local_files\": {path: [] for path in gs_paths}}\n    else:\n        with open(meta_file, \"r\") as fp:\n            meta = json.load(fp)\n\n    for remote_path, pattern in zip(gs_paths, patterns):\n        remote_files = set()\n        for name in list_dir(remote_path):\n            if pattern.search(name.replace(remote_path, \"\")):\n                remote_files.add(name)\n\n        local_files = set(meta[\"local_files\"][remote_path])\n\n        diff = remote_files - local_files\n        LOG.warn(\"New remote elements: {}\".format(len(diff)))\n        if diff:\n            # Check the current tfrecords, find the last one created\n            # Fill the empty spaces in this one (rewriting it completely)\n            # And create the other ones\n            record_id = sum(\n                len(glob(os.path.join(dataset_path, split, \"*.tfrecord\")))\n                for split in SPLITS\n            )\n\n            last_tf_record = os.path.join(dataset_path, \"train\", \"1.tfrecord\")\n            for split in SPLITS:\n                last_path = os.path.join(dataset_path, split, f\"{record_id}.tfrecord\")\n                if os.path.exists(last_path):\n                    last_tf_record = last_path\n                    break\n\n            examples = []\n            if os.path.exists(last_tf_record):\n                rows_in_last = rows_in_tfrecord(last_tf_record)\n                if rows_in_last < ROWS_PER_RECORD:\n                    # read all rows in the last tfrecord\n                    # we're going to add the last one\n                    # and recreate it\n                    for row in tf.io.tf_record_iterator(last_tf_record):\n                        example = tf.train.Example()\n                        examples.append(example.ParseFromString(row))\n                else:\n                    record_id += 1\n\n            # fetch new files and update the tfrecords\n            tot_new_elements = len(diff)\n            for idx, new_element in enumerate(diff):\n                LOG.warn(f\"File: {new_element}\")\n                with tf.gfile.GFile(new_element, \"rb\") as fp:\n                    img = fp.read()\n                remote_meta = os.path.join(os.path.realpath(new_element), \"meta.json\")\n                if not tf.gfile.Exists(remote_meta):\n                    example_meta = {\n                        \"count\": _int64_feature(1),\n                        \"feature1\": _float_feature(1076),\n                        \"feature2\": _float_feature(100),\n                        \"f3\": _float_feature(1076),\n                        \"f4\": _float_feature(100),\n                        \"f5\": _bytes_feature(\"mango\".encode(\"UTF-8\")),\n                        \"f7\": _bytes_feature(\n                            \"banana\".encode(\"UTF-8\")\n                        ),\n                    }\n                else:\n                    with tf.gfile.GFile(remote_meta, \"r\") as fp:\n                        example_meta = json.loads(fp.read())\n\n                # TODO: check if image is an old style image and convert to new format\n                example_meta[\"img\"] = _bytes_feature(img)\n                examples.append(\n                    tf.train.Example(features=tf.train.Features(feature=example_meta))\n                )\n\n                if len(examples) == ROWS_PER_RECORD or idx == tot_new_elements - 1:\n                    # write (always in the training dataset) and reset examples buffer\n                    filename = os.path.join(\n                        dataset_path, \"train\", f\"{record_id}.tfrecord\"\n                    )\n                    with tf.io.TFRecordWriter(filename) as writer:\n                        for example in examples:\n                            writer.write(example.SerializeToString())\n                    # Reset example buffer\n                    examples.clear()\n                    # Increment tfrecord id\n                    record_id += 1\n                    LOG.warn(f\"TFRecord: {filename} written\")\n                    snapshot = tracemalloc.take_snapshot()\n                    display_top(snapshot)\n                    # break\n\nJust use the _gs2tfrecord function. I'm using it to fetch data from Google Cloud Storage, but since I'm using tf.gfile a local path can be used too.\nMy guess is that leak can be somewhere in tf.train.Example or in tf.train.Feature since those are the only 2 functions I call in a loop.\nUPDATE\nI've created another function that just download the images from Google Cloud and store them locally. It goes out of memory in the same way.\nMaybe the leak is in tf.gfile?\nHere's the function\ndef _gs2folder(\n    gs_paths: List[str], patterns: List[Pattern], names: List[str], cache: str = CACHE\n):\n    \"\"\"Fetch images from google cloud storage, conver to tf_record and\n    properly handle cache.\n\n    Args:\n        gs_paths: list of paths on google cloud storage\n        patterns: list of compiled regex to fiter the data on each gs_paths\n        names: list of dataset name\n        cache: local cache/dataset folder\n    \"\"\"\n    assert len(gs_paths) == len(patterns) and len(patterns) == len(names)\n\n    gcloud_auth = os.path.join(\n        os.path.expanduser(\"~\"),\n        \".config\",\n        \"gcloud\",\n        \"application_default_credentials.json\",\n    )\n    if not os.path.exists(gcloud_auth):\n        subprocess.check_call([\"gcloud\", \"auth\", \"application-default\", \"login\"])\n\n    for remote_path, pattern, name in zip(gs_paths, patterns, names):\n        meta_file = os.path.join(cache, name, \"meta.json\")\n        dataset_path = os.path.join(cache, name, \"original\")\n        if not os.path.exists(cache):\n            os.makedirs(dataset_path)\n            meta = {\"local_files\": {path: [] for path in gs_paths}}\n        else:\n            with open(meta_file, \"r\") as fp:\n                meta = json.load(fp)\n\n        remote_files = set()\n        for file_name in list_dir(remote_path):\n            if pattern.search(file_name.replace(remote_path, \"\")):\n                remote_files.add(file_name)\n\n        local_files = set(meta[\"local_files\"][remote_path])\n        diff = remote_files - local_files\n        LOG.warning(\"New remote elements: {}\".format(len(diff)))\n        last_id = len(local_files) + 1\n        if diff:\n            # fetch new files and store them\n            tot_new_elements = len(diff)\n\n            for idx, new_element in enumerate(diff):\n                LOG.warning(f\"File: {new_element}\")\n                with tf.gfile.GFile(new_element, \"rb\") as fp:\n                    img_bytes = fp.read()\n\n                image = Image.open(io.BytesIO(img_bytes))\n                image = np.array(image)\n                if image.shape[-1] == 4:\n                    # remove transparency\n                    image = image[..., :3]\n                remote_meta = os.path.join(os.path.realpath(new_element), \"meta.json\")\n                if not tf.gfile.Exists(remote_meta):\n                    example_meta = {\n                        \"count\": 1\n                    }\n                else:\n                    with tf.gfile.GFile(remote_meta, \"r\") as fp:\n                        example_meta = json.loads(fp.read())\n\n                image = Image.fromarray(image)\n                # Just save the image file with its meta info\n                image.save(os.path.join(dataset_path, f\"{last_id}.png\"))\n                with open(os.path.join(dataset_path, f\"{last_id}.json\"), \"w\") as fp:\n                    json.dump(example_meta, fp)\n                last_id += 1\n\n        meta[\"local_files\"][remote_path] = remote_files\n\n    with open(meta, \"w\") as fp:\n        json.dump(meta, fp)\nOther info / logs\nThis is the output of a display_top invocation:\nTop 10 lines\n#1: python3.7/linecache.py:137: 246.9 KiB\n    lines = fp.readlines()\n#2: util/compat.py:80: 150.0 KiB\n    return bytes_or_text.decode(encoding)\n#3: datasets/floorplans.py:143: 64.2 KiB\n    diff = remote_files - local_files\n#4: datasets/floorplans.py:139: 32.0 KiB\n    remote_files.add(name)\n#5: internal/python_message.py:475: 21.6 KiB\n    self._oneofs = {}\n#6: internal/python_message.py:425: 15.5 KiB\n    result = message_type._concrete_class()\n#7: python3.7/tracemalloc.py:185: 13.4 KiB\n    self._frames = tuple(reversed(frames))\n#8: internal/python_message.py:472: 12.0 KiB\n    self._fields = {}\n#9: internal/python_message.py:1402: 10.3 KiB\n    self._parent_message_weakref = weakref.proxy(parent_message)\n#10: internal/python_message.py:1155: 5.1 KiB\n    for field, value in list(self._fields.items()):  # dict can change size!\n228 other: 104.0 KiB\nTotal allocated size: 675.1 KiB", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): repository\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: cuda 10, cudnn 7\r\n- GPU model and memory: nvidia 1080ti\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe system goes OOM and I have 64 GiB of memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nI should be able to create a dataset converting files I read from Google Cloud to local tfrecords.\r\n\r\nI'm debugging the memory usage of python and as you can see below, the usage is around 670K - hence the memory allocation should be outside of python, and the only library I'm using is Tensorflow.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport subprocess\r\nimport multiprocessing\r\nfrom glob import glob\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as k\r\nimport json\r\nimport os\r\nimport logging\r\nfrom typing import Pattern, List, Tuple, Callable\r\nimport linecache\r\nimport tracemalloc\r\n\r\n\r\ndef display_top(snapshot, key_type=\"lineno\", limit=10):\r\n    snapshot = snapshot.filter_traces(\r\n        (\r\n            tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\r\n            tracemalloc.Filter(False, \"<unknown>\"),\r\n        )\r\n    )\r\n    top_stats = snapshot.statistics(key_type)\r\n\r\n    print(\"Top %s lines\" % limit)\r\n    for index, stat in enumerate(top_stats[:limit], 1):\r\n        frame = stat.traceback[0]\r\n        # replace \"/path/to/module/file.py\" with \"module/file.py\"\r\n        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\r\n        print(\r\n            \"#%s: %s:%s: %.1f KiB\" % (index, filename, frame.lineno, stat.size / 1024)\r\n        )\r\n        line = linecache.getline(frame.filename, frame.lineno).strip()\r\n        if line:\r\n            print(\"    %s\" % line)\r\n\r\n    other = top_stats[limit:]\r\n    if other:\r\n        size = sum(stat.size for stat in other)\r\n        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\r\n    total = sum(stat.size for stat in top_stats)\r\n    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\r\n\r\n\r\ntracemalloc.start()\r\n\r\nCACHE = os.path.join(os.getcwd(), \".data\")\r\nROWS_PER_RECORD = 32\r\nSPLITS = (\"train\", \"validation\", \"test\")\r\n\r\nLOG = logging.Logger(__name__)\r\nLOG.setLevel(logging.INFO)\r\n\r\n\r\ndef _int64_feature(value):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n\r\ndef _float_feature(value):\r\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n\r\n\r\ndef _bytes_feature(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n\r\ndef rows_in_tfrecord(path):\r\n    \"\"\"Count the number of elements in a tfrecord file.\r\n    Args:\r\n        path: The path of the TFRecord file\r\n    Return:\r\n        The number of examples in the TFRecord\r\n    \"\"\"\r\n    return sum(1 for _ in tf.io.tf_record_iterator(path))\r\n\r\n\r\ndef _is_dir(path: str):\r\n    \"\"\"Determines if path is a directory only looking at the filename.\r\n    It's ugly, but is the only way to get a decent speed.\r\n    Args:\r\n        path: The input path\r\n    Return:\r\n        value: True if the path looks like a directory\r\n    \"\"\"\r\n    return path.endswith(\"/\") or not \".\" in path.split(\"/\")[-1]\r\n\r\n\r\ndef list_dir(root):\r\n    \"\"\"Recursively list a directory and returns the list of all the files.\r\n    Args:\r\n        root: The path of the root directory\r\n    Return:\r\n        List of all the files found in this directory and its subfolders.\r\n    \"\"\"\r\n    ret = []\r\n    for full_path in tf.gfile.Glob(os.path.join(root, \"*\")):\r\n        if _is_dir(full_path):\r\n            return ret + list_dir(full_path)\r\n        else:\r\n            ret.append(full_path)\r\n    return ret\r\n\r\n\r\ndef _gs2tfrecord(gs_paths: List[str], patterns: List[Pattern], cache: str = CACHE):\r\n    \"\"\"Fetch images from google cloud storage, conver to tf_record and\r\n    properly handle cache.\r\n\r\n    Args:\r\n        gs_paths: list of paths on google cloud storage\r\n        patterns: list of compiled regex to fiter the data on each gs_paths\r\n        cache: local cache/dataset folder\r\n    \"\"\"\r\n    gcloud_auth = os.path.join(\r\n        os.path.expanduser(\"~\"),\r\n        \".config\",\r\n        \"gcloud\",\r\n        \"application_default_credentials.json\",\r\n    )\r\n    if not os.path.exists(gcloud_auth):\r\n        subprocess.check_call([\"gcloud\", \"auth\", \"application-default\", \"login\"])\r\n\r\n    meta_file = os.path.join(cache, \"meta.json\")\r\n    dataset_path = os.path.join(cache, \"dataset\")\r\n    if not os.path.exists(cache):\r\n        os.makedirs(dataset_path)\r\n        for split in SPLITS:\r\n            os.makedirs(os.path.join(dataset_path, split))\r\n        meta = {\"local_files\": {path: [] for path in gs_paths}}\r\n    else:\r\n        with open(meta_file, \"r\") as fp:\r\n            meta = json.load(fp)\r\n\r\n    for remote_path, pattern in zip(gs_paths, patterns):\r\n        remote_files = set()\r\n        for name in list_dir(remote_path):\r\n            if pattern.search(name.replace(remote_path, \"\")):\r\n                remote_files.add(name)\r\n\r\n        local_files = set(meta[\"local_files\"][remote_path])\r\n\r\n        diff = remote_files - local_files\r\n        LOG.warn(\"New remote elements: {}\".format(len(diff)))\r\n        if diff:\r\n            # Check the current tfrecords, find the last one created\r\n            # Fill the empty spaces in this one (rewriting it completely)\r\n            # And create the other ones\r\n            record_id = sum(\r\n                len(glob(os.path.join(dataset_path, split, \"*.tfrecord\")))\r\n                for split in SPLITS\r\n            )\r\n\r\n            last_tf_record = os.path.join(dataset_path, \"train\", \"1.tfrecord\")\r\n            for split in SPLITS:\r\n                last_path = os.path.join(dataset_path, split, f\"{record_id}.tfrecord\")\r\n                if os.path.exists(last_path):\r\n                    last_tf_record = last_path\r\n                    break\r\n\r\n            examples = []\r\n            if os.path.exists(last_tf_record):\r\n                rows_in_last = rows_in_tfrecord(last_tf_record)\r\n                if rows_in_last < ROWS_PER_RECORD:\r\n                    # read all rows in the last tfrecord\r\n                    # we're going to add the last one\r\n                    # and recreate it\r\n                    for row in tf.io.tf_record_iterator(last_tf_record):\r\n                        example = tf.train.Example()\r\n                        examples.append(example.ParseFromString(row))\r\n                else:\r\n                    record_id += 1\r\n\r\n            # fetch new files and update the tfrecords\r\n            tot_new_elements = len(diff)\r\n            for idx, new_element in enumerate(diff):\r\n                LOG.warn(f\"File: {new_element}\")\r\n                with tf.gfile.GFile(new_element, \"rb\") as fp:\r\n                    img = fp.read()\r\n                remote_meta = os.path.join(os.path.realpath(new_element), \"meta.json\")\r\n                if not tf.gfile.Exists(remote_meta):\r\n                    example_meta = {\r\n                        \"count\": _int64_feature(1),\r\n                        \"feature1\": _float_feature(1076),\r\n                        \"feature2\": _float_feature(100),\r\n                        \"f3\": _float_feature(1076),\r\n                        \"f4\": _float_feature(100),\r\n                        \"f5\": _bytes_feature(\"mango\".encode(\"UTF-8\")),\r\n                        \"f7\": _bytes_feature(\r\n                            \"banana\".encode(\"UTF-8\")\r\n                        ),\r\n                    }\r\n                else:\r\n                    with tf.gfile.GFile(remote_meta, \"r\") as fp:\r\n                        example_meta = json.loads(fp.read())\r\n\r\n                # TODO: check if image is an old style image and convert to new format\r\n                example_meta[\"img\"] = _bytes_feature(img)\r\n                examples.append(\r\n                    tf.train.Example(features=tf.train.Features(feature=example_meta))\r\n                )\r\n\r\n                if len(examples) == ROWS_PER_RECORD or idx == tot_new_elements - 1:\r\n                    # write (always in the training dataset) and reset examples buffer\r\n                    filename = os.path.join(\r\n                        dataset_path, \"train\", f\"{record_id}.tfrecord\"\r\n                    )\r\n                    with tf.io.TFRecordWriter(filename) as writer:\r\n                        for example in examples:\r\n                            writer.write(example.SerializeToString())\r\n                    # Reset example buffer\r\n                    examples.clear()\r\n                    # Increment tfrecord id\r\n                    record_id += 1\r\n                    LOG.warn(f\"TFRecord: {filename} written\")\r\n                    snapshot = tracemalloc.take_snapshot()\r\n                    display_top(snapshot)\r\n                    # break\r\n\r\n```\r\n\r\nJust use the `_gs2tfrecord` function. I'm using it to fetch data from Google Cloud Storage, but since I'm using `tf.gfile` a local path can be used too.\r\n\r\nMy guess is that leak can be somewhere in `tf.train.Example` or in `tf.train.Feature` since those are the only 2 functions I call in a loop.\r\n\r\n**UPDATE**\r\n\r\nI've created another function that just download the images from Google Cloud and store them locally. It goes out of memory in the same way.\r\n\r\nMaybe the leak is in `tf.gfile`?\r\n\r\nHere's the function\r\n```python\r\ndef _gs2folder(\r\n    gs_paths: List[str], patterns: List[Pattern], names: List[str], cache: str = CACHE\r\n):\r\n    \"\"\"Fetch images from google cloud storage, conver to tf_record and\r\n    properly handle cache.\r\n\r\n    Args:\r\n        gs_paths: list of paths on google cloud storage\r\n        patterns: list of compiled regex to fiter the data on each gs_paths\r\n        names: list of dataset name\r\n        cache: local cache/dataset folder\r\n    \"\"\"\r\n    assert len(gs_paths) == len(patterns) and len(patterns) == len(names)\r\n\r\n    gcloud_auth = os.path.join(\r\n        os.path.expanduser(\"~\"),\r\n        \".config\",\r\n        \"gcloud\",\r\n        \"application_default_credentials.json\",\r\n    )\r\n    if not os.path.exists(gcloud_auth):\r\n        subprocess.check_call([\"gcloud\", \"auth\", \"application-default\", \"login\"])\r\n\r\n    for remote_path, pattern, name in zip(gs_paths, patterns, names):\r\n        meta_file = os.path.join(cache, name, \"meta.json\")\r\n        dataset_path = os.path.join(cache, name, \"original\")\r\n        if not os.path.exists(cache):\r\n            os.makedirs(dataset_path)\r\n            meta = {\"local_files\": {path: [] for path in gs_paths}}\r\n        else:\r\n            with open(meta_file, \"r\") as fp:\r\n                meta = json.load(fp)\r\n\r\n        remote_files = set()\r\n        for file_name in list_dir(remote_path):\r\n            if pattern.search(file_name.replace(remote_path, \"\")):\r\n                remote_files.add(file_name)\r\n\r\n        local_files = set(meta[\"local_files\"][remote_path])\r\n        diff = remote_files - local_files\r\n        LOG.warning(\"New remote elements: {}\".format(len(diff)))\r\n        last_id = len(local_files) + 1\r\n        if diff:\r\n            # fetch new files and store them\r\n            tot_new_elements = len(diff)\r\n\r\n            for idx, new_element in enumerate(diff):\r\n                LOG.warning(f\"File: {new_element}\")\r\n                with tf.gfile.GFile(new_element, \"rb\") as fp:\r\n                    img_bytes = fp.read()\r\n\r\n                image = Image.open(io.BytesIO(img_bytes))\r\n                image = np.array(image)\r\n                if image.shape[-1] == 4:\r\n                    # remove transparency\r\n                    image = image[..., :3]\r\n                remote_meta = os.path.join(os.path.realpath(new_element), \"meta.json\")\r\n                if not tf.gfile.Exists(remote_meta):\r\n                    example_meta = {\r\n                        \"count\": 1\r\n                    }\r\n                else:\r\n                    with tf.gfile.GFile(remote_meta, \"r\") as fp:\r\n                        example_meta = json.loads(fp.read())\r\n\r\n                image = Image.fromarray(image)\r\n                # Just save the image file with its meta info\r\n                image.save(os.path.join(dataset_path, f\"{last_id}.png\"))\r\n                with open(os.path.join(dataset_path, f\"{last_id}.json\"), \"w\") as fp:\r\n                    json.dump(example_meta, fp)\r\n                last_id += 1\r\n\r\n        meta[\"local_files\"][remote_path] = remote_files\r\n\r\n    with open(meta, \"w\") as fp:\r\n        json.dump(meta, fp)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nThis is the output of a `display_top` invocation:\r\n\r\n```\r\nTop 10 lines\r\n#1: python3.7/linecache.py:137: 246.9 KiB\r\n    lines = fp.readlines()\r\n#2: util/compat.py:80: 150.0 KiB\r\n    return bytes_or_text.decode(encoding)\r\n#3: datasets/floorplans.py:143: 64.2 KiB\r\n    diff = remote_files - local_files\r\n#4: datasets/floorplans.py:139: 32.0 KiB\r\n    remote_files.add(name)\r\n#5: internal/python_message.py:475: 21.6 KiB\r\n    self._oneofs = {}\r\n#6: internal/python_message.py:425: 15.5 KiB\r\n    result = message_type._concrete_class()\r\n#7: python3.7/tracemalloc.py:185: 13.4 KiB\r\n    self._frames = tuple(reversed(frames))\r\n#8: internal/python_message.py:472: 12.0 KiB\r\n    self._fields = {}\r\n#9: internal/python_message.py:1402: 10.3 KiB\r\n    self._parent_message_weakref = weakref.proxy(parent_message)\r\n#10: internal/python_message.py:1155: 5.1 KiB\r\n    for field, value in list(self._fields.items()):  # dict can change size!\r\n228 other: 104.0 KiB\r\nTotal allocated size: 675.1 KiB\r\n```"}