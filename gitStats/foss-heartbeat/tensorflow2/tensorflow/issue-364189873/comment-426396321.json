{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/426396321", "html_url": "https://github.com/tensorflow/tensorflow/issues/22546#issuecomment-426396321", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22546", "id": 426396321, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjM5NjMyMQ==", "user": {"login": "priyankjain", "id": 4019056, "node_id": "MDQ6VXNlcjQwMTkwNTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/4019056?v=4", "gravatar_id": "", "url": "https://api.github.com/users/priyankjain", "html_url": "https://github.com/priyankjain", "followers_url": "https://api.github.com/users/priyankjain/followers", "following_url": "https://api.github.com/users/priyankjain/following{/other_user}", "gists_url": "https://api.github.com/users/priyankjain/gists{/gist_id}", "starred_url": "https://api.github.com/users/priyankjain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/priyankjain/subscriptions", "organizations_url": "https://api.github.com/users/priyankjain/orgs", "repos_url": "https://api.github.com/users/priyankjain/repos", "events_url": "https://api.github.com/users/priyankjain/events{/privacy}", "received_events_url": "https://api.github.com/users/priyankjain/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-02T19:14:26Z", "updated_at": "2018-10-02T19:15:39Z", "author_association": "NONE", "body_html": "<p>Indeed, adding an input to the custom OP works for all tf versions until 1.9. Unfortunately, not for 1.10.<br>\nHere is the modified C++ code</p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/op.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/shape_inference.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/op_kernel.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/common_shape_fns.h<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"pl-en\">tensorflow</span><span class=\"pl-k\">;</span>\n\n<span class=\"pl-en\">REGISTER_OP</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NumIntraOpThreads<span class=\"pl-pds\">\"</span></span>)\n.Input(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>placeholder: float32<span class=\"pl-pds\">\"</span></span>)\n.Output(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>num_intra_op_threads: int32<span class=\"pl-pds\">\"</span></span>)\n.SetShapeFn(tensorflow::shape_inference::ScalarShape)\n.Doc(<span class=\"pl-s\"><span class=\"pl-pds\">R\"doc(</span></span>\n<span class=\"pl-s\">A tensorflow OP that returns the number of threads in the intra_op_parallelism pool</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">)doc\"</span></span>);\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">NumIntraOpThreads</span> : <span class=\"pl-k\">public</span> <span class=\"pl-en\">OpKernel</span> {\n <span class=\"pl-k\">public:</span>\n  <span class=\"pl-k\">explicit</span> <span class=\"pl-en\">NumIntraOpThreads</span>(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">Compute</span>(OpKernelContext* context) <span class=\"pl-k\">override</span> {\n    <span class=\"pl-k\">int</span> num_intra_op_threads = context-&gt;<span class=\"pl-c1\">device</span>()-&gt;<span class=\"pl-c1\">tensorflow_cpu_worker_threads</span>()-&gt;<span class=\"pl-smi\">num_threads</span>;\n    Tensor* output_tensor = <span class=\"pl-c1\">NULL</span>;\n    <span class=\"pl-c1\">OP_REQUIRES_OK</span>(context, context-&gt;<span class=\"pl-c1\">allocate_output</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">TensorShape</span>({}), &amp;output_tensor));\n    <span class=\"pl-k\">auto</span> output_flat = output_tensor-&gt;<span class=\"pl-smi\">flat</span>&lt;int32&gt;();\n    <span class=\"pl-c1\">output_flat</span>(<span class=\"pl-c1\">0</span>) = num_intra_op_threads;\n    }\n};\n\n<span class=\"pl-en\">REGISTER_KERNEL_BUILDER</span>(Name(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NumIntraOpThreads<span class=\"pl-pds\">\"</span></span>).Device(DEVICE_CPU), NumIntraOpThreads);</pre></div>\n<p>The OP compilation code remains the same.<br>\nHere is the updated python code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Tensorflow version is <span class=\"pl-pds\">'</span></span>, tf.<span class=\"pl-c1\">__version__</span>)\nn_module <span class=\"pl-k\">=</span> tf.load_op_library(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>./num_intra_op_threads.so<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(tf.<span class=\"pl-c1\">__version__</span>)\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">intra_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">inter_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)):\n  <span class=\"pl-c1\">print</span>(n_module.num_intra_op_threads(tf.constant(<span class=\"pl-c1\">2.0</span>)).eval())</pre></div>\n<p>Output with TF 1.9:</p>\n<div class=\"highlight highlight-source-shell\"><pre>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Tensorflow version is <span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>1.9.0<span class=\"pl-pds\">'</span></span>)\n1.9.0\n2018-10-02 15:10:56.503960: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n10</pre></div>\n<p>Output with TF 1.10:</p>\n<div class=\"highlight highlight-source-shell\"><pre>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Tensorflow version is <span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>1.10.0<span class=\"pl-pds\">'</span></span>)\n1.10.0\n2018-10-02 15:11:47.141032: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n1</pre></div>", "body_text": "Indeed, adding an input to the custom OP works for all tf versions until 1.9. Unfortunately, not for 1.10.\nHere is the modified C++ code\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n\nusing namespace tensorflow;\n\nREGISTER_OP(\"NumIntraOpThreads\")\n.Input(\"placeholder: float32\")\n.Output(\"num_intra_op_threads: int32\")\n.SetShapeFn(tensorflow::shape_inference::ScalarShape)\n.Doc(R\"doc(\nA tensorflow OP that returns the number of threads in the intra_op_parallelism pool\n)doc\");\n\nclass NumIntraOpThreads : public OpKernel {\n public:\n  explicit NumIntraOpThreads(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    int num_intra_op_threads = context->device()->tensorflow_cpu_worker_threads()->num_threads;\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}), &output_tensor));\n    auto output_flat = output_tensor->flat<int32>();\n    output_flat(0) = num_intra_op_threads;\n    }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"NumIntraOpThreads\").Device(DEVICE_CPU), NumIntraOpThreads);\nThe OP compilation code remains the same.\nHere is the updated python code:\nimport tensorflow as tf\nprint('Tensorflow version is ', tf.__version__)\nn_module = tf.load_op_library('./num_intra_op_threads.so')\nprint(tf.__version__)\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)):\n  print(n_module.num_intra_op_threads(tf.constant(2.0)).eval())\nOutput with TF 1.9:\n('Tensorflow version is ', '1.9.0')\n1.9.0\n2018-10-02 15:10:56.503960: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n10\nOutput with TF 1.10:\n('Tensorflow version is ', '1.10.0')\n1.10.0\n2018-10-02 15:11:47.141032: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n1", "body": "Indeed, adding an input to the custom OP works for all tf versions until 1.9. Unfortunately, not for 1.10.\r\nHere is the modified C++ code\r\n```cpp\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/common_shape_fns.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"NumIntraOpThreads\")\r\n.Input(\"placeholder: float32\")\r\n.Output(\"num_intra_op_threads: int32\")\r\n.SetShapeFn(tensorflow::shape_inference::ScalarShape)\r\n.Doc(R\"doc(\r\nA tensorflow OP that returns the number of threads in the intra_op_parallelism pool\r\n)doc\");\r\n\r\nclass NumIntraOpThreads : public OpKernel {\r\n public:\r\n  explicit NumIntraOpThreads(OpKernelConstruction* context)\r\n      : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    int num_intra_op_threads = context->device()->tensorflow_cpu_worker_threads()->num_threads;\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}), &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n    output_flat(0) = num_intra_op_threads;\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"NumIntraOpThreads\").Device(DEVICE_CPU), NumIntraOpThreads);\r\n```\r\n\r\nThe OP compilation code remains the same.\r\nHere is the updated python code:\r\n```python\r\nimport tensorflow as tf\r\nprint('Tensorflow version is ', tf.__version__)\r\nn_module = tf.load_op_library('./num_intra_op_threads.so')\r\nprint(tf.__version__)\r\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)):\r\n  print(n_module.num_intra_op_threads(tf.constant(2.0)).eval())\r\n```\r\nOutput with TF 1.9:\r\n```bash\r\n('Tensorflow version is ', '1.9.0')\r\n1.9.0\r\n2018-10-02 15:10:56.503960: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n10\r\n```\r\n\r\nOutput with TF 1.10:\r\n```bash\r\n('Tensorflow version is ', '1.10.0')\r\n1.10.0\r\n2018-10-02 15:11:47.141032: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n1\r\n```"}