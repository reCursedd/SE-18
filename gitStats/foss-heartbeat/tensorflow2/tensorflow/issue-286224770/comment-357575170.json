{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357575170", "html_url": "https://github.com/tensorflow/tensorflow/issues/15871#issuecomment-357575170", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15871", "id": 357575170, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzU3NTE3MA==", "user": {"login": "qjivy", "id": 24410810, "node_id": "MDQ6VXNlcjI0NDEwODEw", "avatar_url": "https://avatars2.githubusercontent.com/u/24410810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qjivy", "html_url": "https://github.com/qjivy", "followers_url": "https://api.github.com/users/qjivy/followers", "following_url": "https://api.github.com/users/qjivy/following{/other_user}", "gists_url": "https://api.github.com/users/qjivy/gists{/gist_id}", "starred_url": "https://api.github.com/users/qjivy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qjivy/subscriptions", "organizations_url": "https://api.github.com/users/qjivy/orgs", "repos_url": "https://api.github.com/users/qjivy/repos", "events_url": "https://api.github.com/users/qjivy/events{/privacy}", "received_events_url": "https://api.github.com/users/qjivy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-15T03:35:07Z", "updated_at": "2018-01-15T03:35:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25754898\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrehentz\">@andrehentz</a> , thanks very much for your reply. We are eager to see more document about TFlite.</p>\n<p>May I ask a bit more about the TFlite demo apk's \"mobilenet_quant_v1_224.tflite\"?<br>\nWhen we add logs in the ./tensorflow/contrib/lite/kernels/conv.cc and  ./tensorflow/contrib/lite/kernels/depthwise_conv.cc, we get to know the operations are \"EvalQuantized\" version. They have uint8 input and int32 output. The Conv2D calls into gemmlowp and the depthwise_conv calls into the internal/optimized/depthwiseconv_uint8.h routines. They are indeed \"quantized nodes\" as expected.<br>\nSince there seems no tool at the moment to help to parse and analyze the tflite format model, I try to convert the \"mobilenet_quant_v1_224.tflite\" into protobuf first by the toco first according to <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md\">this page</a>.<br>\nThe command line is:<br>\n./toco --input_file=./mobilenet_quant_v1_224.tflite --output_file=./mobilenet_quant_v1_224.pb --input_format=TFLITE --output_format=TENSORFLOW_GRAPHDEF --input_shape=1,224,224,3 --input_array=Placeholder --output_array=MobilenetV1/Predictions/Softmax</p>\n<p>After succeed, I dig into the protobuf model by using the benchmark_model according to <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark\">this page</a>, along with the VLOG level set to 0.</p>\n<p>The command line is:</p>\n<p>bazel-bin/tensorflow/tools/benchmark/benchmark_model  --graph=./mobilenet_quant_v1_224.pb --show_flops --input_layer=Placeholder --input_layer_type=float --input_layer_shape=1,224,224,3 --output_layer=MobilenetV1/Predictions/Softmax --num_threads=1 --show_time=true --warmup_runs=1 --max_num_runs=1 --show_run_order=true --show_memory=true  --show_summary=true</p>\n<p>Checking into the  output log, I find there is no \"quantized operations\" at all in the converted protobuf model.<br>\nHere is some of the output log from benchmark_model summary:</p>\n<p>2018-01-15 03:19:05.459798: I tensorflow/core/util/stat_summarizer.cc:468] ============================== Summary by node type ==============================<br>\n2018-01-15 03:19:05.459801: I tensorflow/core/util/stat_summarizer.cc:468]                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]<br>\n2018-01-15 03:19:05.459804: I tensorflow/core/util/stat_summarizer.cc:468]                       BiasAdd               28         2747.066          52.212%         52.212%          0.000             28<br>\n2018-01-15 03:19:05.459806: I tensorflow/core/util/stat_summarizer.cc:468]                         Const               57         2425.916          46.108%         98.319%          0.000             57<br>\n2018-01-15 03:19:05.459809: I tensorflow/core/util/stat_summarizer.cc:468]                        Conv2D               15           57.005           1.083%         99.403%      12447.652             15<br>\n2018-01-15 03:19:05.459812: I tensorflow/core/util/stat_summarizer.cc:468]         DepthwiseConv2dNative               13           16.213           0.308%         99.711%       7905.664             13<br>\n2018-01-15 03:19:05.459815: I tensorflow/core/util/stat_summarizer.cc:468]       FakeQuantWithMinMaxArgs               31           14.897           0.283%         99.994%        602.112             31<br>\n2018-01-15 03:19:05.459817: I tensorflow/core/util/stat_summarizer.cc:468]                       AvgPool                1            0.086           0.002%         99.996%          4.096              1<br>\n2018-01-15 03:19:05.459820: I tensorflow/core/util/stat_summarizer.cc:468]                          NoOp                1            0.067           0.001%         99.997%          0.000              1<br>\n2018-01-15 03:19:05.459823: I tensorflow/core/util/stat_summarizer.cc:468]                          _Arg                1            0.055           0.001%         99.998%          0.000              1<br>\n2018-01-15 03:19:05.459825: I tensorflow/core/util/stat_summarizer.cc:468]                       Softmax                1            0.052           0.001%         99.999%          0.000              1<br>\n2018-01-15 03:19:05.459828: I tensorflow/core/util/stat_summarizer.cc:468]                       Reshape                1            0.032           0.001%        100.000%          0.000              1<br>\n2018-01-15 03:19:05.459831: I tensorflow/core/util/stat_summarizer.cc:468]                       _Retval                1            0.024           0.000%        100.000%          0.000              1</p>\n<p>Here also the log from the float point operations in the ~/tensorflow/core/kernel/*.cc:</p>\n<p>2018-01-15 03:19:16.119771: I tensorflow/core/kernels/conv_ops.cc:393] <strong>Conv2D</strong>: in_depth = 3, input_cols = 224, filter_cols = 3, input_rows = 224, filter_rows = 3, stride_rows = 2, stride_cols = 2, dilation_rows = 1, dilation_cols = 1, out_depth = 32</p>\n<p>2018-01-15 03:19:16.125776: I tensorflow/core/kernels/depthwise_conv_op.cc:360] <strong>DepthwiseConv2dNative</strong>:  Input: [1, 112, 112, 32]; Filter: [3, 3, 32, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [1, 112, 112, 32]</p>\n<p>To be short, when using the tflite quant_mobile, quantized operation are used (quan_conv2d and quant_depthwiseconv), when convert the tflite quant_mobile into protobuf by the toco and evaluate it by tensorflow's benchmark_model, it falls back into floating point version? Could you give me a little hint for it?</p>\n<p>Thank you!</p>", "body_text": "@andrehentz , thanks very much for your reply. We are eager to see more document about TFlite.\nMay I ask a bit more about the TFlite demo apk's \"mobilenet_quant_v1_224.tflite\"?\nWhen we add logs in the ./tensorflow/contrib/lite/kernels/conv.cc and  ./tensorflow/contrib/lite/kernels/depthwise_conv.cc, we get to know the operations are \"EvalQuantized\" version. They have uint8 input and int32 output. The Conv2D calls into gemmlowp and the depthwise_conv calls into the internal/optimized/depthwiseconv_uint8.h routines. They are indeed \"quantized nodes\" as expected.\nSince there seems no tool at the moment to help to parse and analyze the tflite format model, I try to convert the \"mobilenet_quant_v1_224.tflite\" into protobuf first by the toco first according to this page.\nThe command line is:\n./toco --input_file=./mobilenet_quant_v1_224.tflite --output_file=./mobilenet_quant_v1_224.pb --input_format=TFLITE --output_format=TENSORFLOW_GRAPHDEF --input_shape=1,224,224,3 --input_array=Placeholder --output_array=MobilenetV1/Predictions/Softmax\nAfter succeed, I dig into the protobuf model by using the benchmark_model according to this page, along with the VLOG level set to 0.\nThe command line is:\nbazel-bin/tensorflow/tools/benchmark/benchmark_model  --graph=./mobilenet_quant_v1_224.pb --show_flops --input_layer=Placeholder --input_layer_type=float --input_layer_shape=1,224,224,3 --output_layer=MobilenetV1/Predictions/Softmax --num_threads=1 --show_time=true --warmup_runs=1 --max_num_runs=1 --show_run_order=true --show_memory=true  --show_summary=true\nChecking into the  output log, I find there is no \"quantized operations\" at all in the converted protobuf model.\nHere is some of the output log from benchmark_model summary:\n2018-01-15 03:19:05.459798: I tensorflow/core/util/stat_summarizer.cc:468] ============================== Summary by node type ==============================\n2018-01-15 03:19:05.459801: I tensorflow/core/util/stat_summarizer.cc:468]                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\n2018-01-15 03:19:05.459804: I tensorflow/core/util/stat_summarizer.cc:468]                       BiasAdd               28         2747.066          52.212%         52.212%          0.000             28\n2018-01-15 03:19:05.459806: I tensorflow/core/util/stat_summarizer.cc:468]                         Const               57         2425.916          46.108%         98.319%          0.000             57\n2018-01-15 03:19:05.459809: I tensorflow/core/util/stat_summarizer.cc:468]                        Conv2D               15           57.005           1.083%         99.403%      12447.652             15\n2018-01-15 03:19:05.459812: I tensorflow/core/util/stat_summarizer.cc:468]         DepthwiseConv2dNative               13           16.213           0.308%         99.711%       7905.664             13\n2018-01-15 03:19:05.459815: I tensorflow/core/util/stat_summarizer.cc:468]       FakeQuantWithMinMaxArgs               31           14.897           0.283%         99.994%        602.112             31\n2018-01-15 03:19:05.459817: I tensorflow/core/util/stat_summarizer.cc:468]                       AvgPool                1            0.086           0.002%         99.996%          4.096              1\n2018-01-15 03:19:05.459820: I tensorflow/core/util/stat_summarizer.cc:468]                          NoOp                1            0.067           0.001%         99.997%          0.000              1\n2018-01-15 03:19:05.459823: I tensorflow/core/util/stat_summarizer.cc:468]                          _Arg                1            0.055           0.001%         99.998%          0.000              1\n2018-01-15 03:19:05.459825: I tensorflow/core/util/stat_summarizer.cc:468]                       Softmax                1            0.052           0.001%         99.999%          0.000              1\n2018-01-15 03:19:05.459828: I tensorflow/core/util/stat_summarizer.cc:468]                       Reshape                1            0.032           0.001%        100.000%          0.000              1\n2018-01-15 03:19:05.459831: I tensorflow/core/util/stat_summarizer.cc:468]                       _Retval                1            0.024           0.000%        100.000%          0.000              1\nHere also the log from the float point operations in the ~/tensorflow/core/kernel/*.cc:\n2018-01-15 03:19:16.119771: I tensorflow/core/kernels/conv_ops.cc:393] Conv2D: in_depth = 3, input_cols = 224, filter_cols = 3, input_rows = 224, filter_rows = 3, stride_rows = 2, stride_cols = 2, dilation_rows = 1, dilation_cols = 1, out_depth = 32\n2018-01-15 03:19:16.125776: I tensorflow/core/kernels/depthwise_conv_op.cc:360] DepthwiseConv2dNative:  Input: [1, 112, 112, 32]; Filter: [3, 3, 32, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [1, 112, 112, 32]\nTo be short, when using the tflite quant_mobile, quantized operation are used (quan_conv2d and quant_depthwiseconv), when convert the tflite quant_mobile into protobuf by the toco and evaluate it by tensorflow's benchmark_model, it falls back into floating point version? Could you give me a little hint for it?\nThank you!", "body": "@andrehentz , thanks very much for your reply. We are eager to see more document about TFlite.\r\n \r\nMay I ask a bit more about the TFlite demo apk's \"mobilenet_quant_v1_224.tflite\"?  \r\nWhen we add logs in the ./tensorflow/contrib/lite/kernels/conv.cc and  ./tensorflow/contrib/lite/kernels/depthwise_conv.cc, we get to know the operations are \"EvalQuantized\" version. They have uint8 input and int32 output. The Conv2D calls into gemmlowp and the depthwise_conv calls into the internal/optimized/depthwiseconv_uint8.h routines. They are indeed \"quantized nodes\" as expected.\r\nSince there seems no tool at the moment to help to parse and analyze the tflite format model, I try to convert the \"mobilenet_quant_v1_224.tflite\" into protobuf first by the toco first according to [this page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md).\r\nThe command line is:\r\n./toco --input_file=./mobilenet_quant_v1_224.tflite --output_file=./mobilenet_quant_v1_224.pb --input_format=TFLITE --output_format=TENSORFLOW_GRAPHDEF --input_shape=1,224,224,3 --input_array=Placeholder --output_array=MobilenetV1/Predictions/Softmax\r\n\r\nAfter succeed, I dig into the protobuf model by using the benchmark_model according to [this page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark), along with the VLOG level set to 0. \r\n\r\nThe command line is:\r\n\r\nbazel-bin/tensorflow/tools/benchmark/benchmark_model  --graph=./mobilenet_quant_v1_224.pb --show_flops --input_layer=Placeholder --input_layer_type=float --input_layer_shape=1,224,224,3 --output_layer=MobilenetV1/Predictions/Softmax --num_threads=1 --show_time=true --warmup_runs=1 --max_num_runs=1 --show_run_order=true --show_memory=true  --show_summary=true\r\n\r\nChecking into the  output log, I find there is no \"quantized operations\" at all in the converted protobuf model. \r\nHere is some of the output log from benchmark_model summary:\r\n\r\n\r\n2018-01-15 03:19:05.459798: I tensorflow/core/util/stat_summarizer.cc:468] ============================== Summary by node type ==============================\r\n2018-01-15 03:19:05.459801: I tensorflow/core/util/stat_summarizer.cc:468]                   [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n2018-01-15 03:19:05.459804: I tensorflow/core/util/stat_summarizer.cc:468]                       BiasAdd               28         2747.066          52.212%         52.212%          0.000             28\r\n2018-01-15 03:19:05.459806: I tensorflow/core/util/stat_summarizer.cc:468]                         Const               57         2425.916          46.108%         98.319%          0.000             57\r\n2018-01-15 03:19:05.459809: I tensorflow/core/util/stat_summarizer.cc:468]                        Conv2D               15           57.005           1.083%         99.403%      12447.652             15\r\n2018-01-15 03:19:05.459812: I tensorflow/core/util/stat_summarizer.cc:468]         DepthwiseConv2dNative               13           16.213           0.308%         99.711%       7905.664             13\r\n2018-01-15 03:19:05.459815: I tensorflow/core/util/stat_summarizer.cc:468]       FakeQuantWithMinMaxArgs               31           14.897           0.283%         99.994%        602.112             31\r\n2018-01-15 03:19:05.459817: I tensorflow/core/util/stat_summarizer.cc:468]                       AvgPool                1            0.086           0.002%         99.996%          4.096              1\r\n2018-01-15 03:19:05.459820: I tensorflow/core/util/stat_summarizer.cc:468]                          NoOp                1            0.067           0.001%         99.997%          0.000              1\r\n2018-01-15 03:19:05.459823: I tensorflow/core/util/stat_summarizer.cc:468]                          _Arg                1            0.055           0.001%         99.998%          0.000              1\r\n2018-01-15 03:19:05.459825: I tensorflow/core/util/stat_summarizer.cc:468]                       Softmax                1            0.052           0.001%         99.999%          0.000              1\r\n2018-01-15 03:19:05.459828: I tensorflow/core/util/stat_summarizer.cc:468]                       Reshape                1            0.032           0.001%        100.000%          0.000              1\r\n2018-01-15 03:19:05.459831: I tensorflow/core/util/stat_summarizer.cc:468]                       _Retval                1            0.024           0.000%        100.000%          0.000              1\r\n\r\n\r\nHere also the log from the float point operations in the ~/tensorflow/core/kernel/*.cc:\r\n\r\n2018-01-15 03:19:16.119771: I tensorflow/core/kernels/conv_ops.cc:393] **Conv2D**: in_depth = 3, input_cols = 224, filter_cols = 3, input_rows = 224, filter_rows = 3, stride_rows = 2, stride_cols = 2, dilation_rows = 1, dilation_cols = 1, out_depth = 32\r\n\r\n2018-01-15 03:19:16.125776: I tensorflow/core/kernels/depthwise_conv_op.cc:360] **DepthwiseConv2dNative**:  Input: [1, 112, 112, 32]; Filter: [3, 3, 32, 1]; stride = 1, pad_rows = 1, pad_cols = 1, output: [1, 112, 112, 32]\r\n\r\n\r\nTo be short, when using the tflite quant_mobile, quantized operation are used (quan_conv2d and quant_depthwiseconv), when convert the tflite quant_mobile into protobuf by the toco and evaluate it by tensorflow's benchmark_model, it falls back into floating point version? Could you give me a little hint for it?\r\n\r\nThank you! \r\n"}