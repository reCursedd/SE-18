{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318436724", "html_url": "https://github.com/tensorflow/tensorflow/issues/11442#issuecomment-318436724", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11442", "id": 318436724, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODQzNjcyNA==", "user": {"login": "MtDersvan", "id": 7069222, "node_id": "MDQ6VXNlcjcwNjkyMjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7069222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MtDersvan", "html_url": "https://github.com/MtDersvan", "followers_url": "https://api.github.com/users/MtDersvan/followers", "following_url": "https://api.github.com/users/MtDersvan/following{/other_user}", "gists_url": "https://api.github.com/users/MtDersvan/gists{/gist_id}", "starred_url": "https://api.github.com/users/MtDersvan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MtDersvan/subscriptions", "organizations_url": "https://api.github.com/users/MtDersvan/orgs", "repos_url": "https://api.github.com/users/MtDersvan/repos", "events_url": "https://api.github.com/users/MtDersvan/events{/privacy}", "received_events_url": "https://api.github.com/users/MtDersvan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T17:50:28Z", "updated_at": "2017-07-27T17:50:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> Is there a way then to harness the new AttentionWrapper (from tf.contrib.seq2seq) for sequence classification (attention reduction)? E.g. as a words-to-sentence reduction step.<br>\nI mostly refer to the <a href=\"https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\" rel=\"nofollow\">Hierarchical Attention Networks for Document Classification</a> paper. tensorflow/nmt tutorial mostly shows it in use with a decoder.<br>\nThanks.</p>", "body_text": "@ebrevdo Is there a way then to harness the new AttentionWrapper (from tf.contrib.seq2seq) for sequence classification (attention reduction)? E.g. as a words-to-sentence reduction step.\nI mostly refer to the Hierarchical Attention Networks for Document Classification paper. tensorflow/nmt tutorial mostly shows it in use with a decoder.\nThanks.", "body": "@ebrevdo Is there a way then to harness the new AttentionWrapper (from tf.contrib.seq2seq) for sequence classification (attention reduction)? E.g. as a words-to-sentence reduction step.\r\nI mostly refer to the [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf) paper. tensorflow/nmt tutorial mostly shows it in use with a decoder.\r\nThanks."}