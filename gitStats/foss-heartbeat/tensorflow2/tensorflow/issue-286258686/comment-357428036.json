{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357428036", "html_url": "https://github.com/tensorflow/tensorflow/issues/15876#issuecomment-357428036", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15876", "id": 357428036, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzQyODAzNg==", "user": {"login": "lissyx", "id": 1645737, "node_id": "MDQ6VXNlcjE2NDU3Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1645737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lissyx", "html_url": "https://github.com/lissyx", "followers_url": "https://api.github.com/users/lissyx/followers", "following_url": "https://api.github.com/users/lissyx/following{/other_user}", "gists_url": "https://api.github.com/users/lissyx/gists{/gist_id}", "starred_url": "https://api.github.com/users/lissyx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lissyx/subscriptions", "organizations_url": "https://api.github.com/users/lissyx/orgs", "repos_url": "https://api.github.com/users/lissyx/repos", "events_url": "https://api.github.com/users/lissyx/events{/privacy}", "received_events_url": "https://api.github.com/users/lissyx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-13T11:30:55Z", "updated_at": "2018-01-13T11:33:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=136291\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sanjoy\">@sanjoy</a> I do share your surprise :), here are more details, I hope it helps:</p>\n<pre><code>../../tensorflow//bazel-bin/native_client/libdeepspeech_model.so: undefined reference to `__xla_cpu_runtime_EigenMatMulF32'\n</code></pre>\n<p>And <code>libdeepspeech_model.so</code> is being built with <code>tf_library</code>. And inside the definition of <code>tf_library</code>, it does refers to <code>runtime_matmul</code>: <a href=\"https://github.com/mozilla/tensorflow/blob/1390dc180e25b5821be80b407ddc5fad73d4ef6a/tensorflow/compiler/aot/tfcompile.bzl#L197\">https://github.com/mozilla/tensorflow/blob/1390dc180e25b5821be80b407ddc5fad73d4ef6a/tensorflow/compiler/aot/tfcompile.bzl#L197</a></p>\n<p>However,</p>\n<pre><code>$ grep -c runtime_matmul bazel-out/local-opt/bin/native_client/libdeepspeech*.so-2.params \nbazel-out/local-opt/bin/native_client/libdeepspeech_model.so-2.params:0\nbazel-out/local-opt/bin/native_client/libdeepspeech.so-2.params:1\nbazel-out/local-opt/bin/native_client/libdeepspeech_utils.so-2.params:0\n</code></pre>\n<p>And:</p>\n<pre><code>$ grep runtime_matmul bazel-out/local-opt/bin/native_client/libdeepspeech.so-2.params\nbazel-out/local-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul.pic.o\n$ objdump -t bazel-out/local-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul.pic.o | grep MatMul\n0000000000000000 l    d  .text.__xla_cpu_runtime_EigenMatMulF32 0000000000000000 .text.__xla_cpu_runtime_EigenMatMulF32\n0000000000000000 l    d  .gcc_except_table.__xla_cpu_runtime_EigenMatMulF32     0000000000000000 .gcc_except_table.__xla_cpu_runtime_EigenMatMulF32\n0000000000000000 l    d  .text.__xla_cpu_runtime_EigenMatMulF64 0000000000000000 .text.__xla_cpu_runtime_EigenMatMulF64\n0000000000000000 l    d  .gcc_except_table.__xla_cpu_runtime_EigenMatMulF64     0000000000000000 .gcc_except_table.__xla_cpu_runtime_EigenMatMulF64\n0000000000000000 g     F .text.__xla_cpu_runtime_EigenMatMulF32 000000000000061e .hidden __xla_cpu_runtime_EigenMatMulF32\n0000000000000000 g     F .text.__xla_cpu_runtime_EigenMatMulF64 000000000000061e .hidden __xla_cpu_runtime_EigenMatMulF64\n</code></pre>\n<p>So, for some reason, <code>runtime_matmul.pic.o</code> is not being fed into <code>libdeepspeech_model.so</code>, only in <code>libdeepspeech.so</code>. With the symbol being hidden, it does account for the visibility because we are at DSO level. Maybe this is the real issue here, and maybe we should see <code>runtime_matmul.pic.o</code> in <code>bazel-out/local-opt/bin/native_client/libdeepspeech_model.so-2.params</code>.</p>\n<p>For reference, here is our definition of <code>libdeepspeech_model.so</code>:</p>\n<pre><code>\ntf_library(\n    name = \"deepspeech_model\",\n    cpp_class = \"DeepSpeech::nativeModel\",\n    # We don't need tests or benchmark binaries\n    gen_test=False, gen_benchmark=False,\n    # graph and config will be generated at build time thanks to the matching\n    # genrule.\n    graph = \"tfcompile.model.pb\",\n    config = \"tfcompile.config.pbtxt\",\n    # This depends on //tensorflow:rpi3 condition defined in mozilla/tensorflow\n    tfcompile_flags = select({\n        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\n        \"//conditions:default\": str('')\n    }),\n)\n</code></pre>\n<p>And <code>libdeepspeech.so</code>:</p>\n<pre><code>tf_cc_shared_object(\n    name = \"libdeepspeech.so\",\n    srcs = [\"deepspeech.cc\", \"deepspeech.h\", \"deepspeech_utils.h\", \"alphabet.h\", \"beam_search.h\", \"trie_node.h\"] +\n           if_native_model([\"deepspeech_model.h\"]) +\n           glob([\"kenlm/lm/*.cc\", \"kenlm/util/*.cc\", \"kenlm/util/double-conversion/*.cc\",\n                 \"kenlm/lm/*.hh\", \"kenlm/util/*.hh\", \"kenlm/util/double-conversion/*.h\"],\n                exclude = [\"kenlm/*/*test.cc\", \"kenlm/*/*main.cc\"]) +\n           glob([\"boost_locale/**/*.hpp\"]),\n    copts = [\"-std=c++11\", \"-Wno-sign-compare\"] + if_native_model([\n        \"-DDS_MODEL_TIMESTEPS=$(DS_MODEL_TIMESTEPS)\",\n        \"-DDS_NATIVE_MODEL=1\",\n    ]),\n    deps = [\n        \"//tensorflow/core:core_cpu\",\n        \"//tensorflow/core:direct_session\",\n        \"//tensorflow/core/kernels:constant_op\",        # Const\n        \"//tensorflow/core/kernels:identity_op\",        # Identity\n        \"//tensorflow/core/kernels:transpose_op\",       # Transpose\n        \"//tensorflow/core/kernels:reshape_op\",         # Reshape\n        \"//tensorflow/core/kernels:shape_ops\",          # Shape\n        \"//tensorflow/core/kernels:strided_slice_op\",   # StridedSlice\n        \"//tensorflow/core/kernels:pack_op\",            # Pack\n        \"//tensorflow/core/kernels:reverse_op\",         # ReverseV2\n        \"//tensorflow/core/kernels:concat_op\",          # ConcatV2\n        \"//tensorflow/core/kernels:split_op\",           # Split\n        \"//tensorflow/core/kernels:sparse_to_dense_op\", # SparseToDense\n        \"//tensorflow/core/kernels:relu_op\",            # Relu\n        \"//tensorflow/core/kernels:bias_op\",            # BiasAdd\n        \"//tensorflow/core/kernels:math\",               # Range, MatMul\n        \"//tensorflow/core/kernels:tensor_array_ops\",   # Placeholder, TensorArrayV3\n        \"//tensorflow/core/kernels:control_flow_ops\",   # Enter\n        \"//tensorflow/core/kernels:ctc_ops\",            # CTCBeamSearchDecoder\n        ### Needed by production model produced without \"--use_seq_length False\"\n        \"//tensorflow/core/kernels:logging_ops\",         # Assert\n        \"//tensorflow/core/kernels:reverse_sequence_op\", # ReverseSequence\n        # Classic deps\n        \"//tensorflow/core/util/ctc\",\n        \"//third_party/eigen3\",\n    ] + if_native_model([\n        \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\n        \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\n    ])\n      + if_cuda([\n        \"//tensorflow/core:core\",\n        \"//tensorflow/core/kernels:slice_op_gpu\",       # Slice GPU\n    ]),\n    includes = [\"kenlm\", \"boost_locale\"],\n    defines = [\"KENLM_MAX_ORDER=6\"],\n)\n</code></pre>", "body_text": "@sanjoy I do share your surprise :), here are more details, I hope it helps:\n../../tensorflow//bazel-bin/native_client/libdeepspeech_model.so: undefined reference to `__xla_cpu_runtime_EigenMatMulF32'\n\nAnd libdeepspeech_model.so is being built with tf_library. And inside the definition of tf_library, it does refers to runtime_matmul: https://github.com/mozilla/tensorflow/blob/1390dc180e25b5821be80b407ddc5fad73d4ef6a/tensorflow/compiler/aot/tfcompile.bzl#L197\nHowever,\n$ grep -c runtime_matmul bazel-out/local-opt/bin/native_client/libdeepspeech*.so-2.params \nbazel-out/local-opt/bin/native_client/libdeepspeech_model.so-2.params:0\nbazel-out/local-opt/bin/native_client/libdeepspeech.so-2.params:1\nbazel-out/local-opt/bin/native_client/libdeepspeech_utils.so-2.params:0\n\nAnd:\n$ grep runtime_matmul bazel-out/local-opt/bin/native_client/libdeepspeech.so-2.params\nbazel-out/local-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul.pic.o\n$ objdump -t bazel-out/local-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul.pic.o | grep MatMul\n0000000000000000 l    d  .text.__xla_cpu_runtime_EigenMatMulF32 0000000000000000 .text.__xla_cpu_runtime_EigenMatMulF32\n0000000000000000 l    d  .gcc_except_table.__xla_cpu_runtime_EigenMatMulF32     0000000000000000 .gcc_except_table.__xla_cpu_runtime_EigenMatMulF32\n0000000000000000 l    d  .text.__xla_cpu_runtime_EigenMatMulF64 0000000000000000 .text.__xla_cpu_runtime_EigenMatMulF64\n0000000000000000 l    d  .gcc_except_table.__xla_cpu_runtime_EigenMatMulF64     0000000000000000 .gcc_except_table.__xla_cpu_runtime_EigenMatMulF64\n0000000000000000 g     F .text.__xla_cpu_runtime_EigenMatMulF32 000000000000061e .hidden __xla_cpu_runtime_EigenMatMulF32\n0000000000000000 g     F .text.__xla_cpu_runtime_EigenMatMulF64 000000000000061e .hidden __xla_cpu_runtime_EigenMatMulF64\n\nSo, for some reason, runtime_matmul.pic.o is not being fed into libdeepspeech_model.so, only in libdeepspeech.so. With the symbol being hidden, it does account for the visibility because we are at DSO level. Maybe this is the real issue here, and maybe we should see runtime_matmul.pic.o in bazel-out/local-opt/bin/native_client/libdeepspeech_model.so-2.params.\nFor reference, here is our definition of libdeepspeech_model.so:\n\ntf_library(\n    name = \"deepspeech_model\",\n    cpp_class = \"DeepSpeech::nativeModel\",\n    # We don't need tests or benchmark binaries\n    gen_test=False, gen_benchmark=False,\n    # graph and config will be generated at build time thanks to the matching\n    # genrule.\n    graph = \"tfcompile.model.pb\",\n    config = \"tfcompile.config.pbtxt\",\n    # This depends on //tensorflow:rpi3 condition defined in mozilla/tensorflow\n    tfcompile_flags = select({\n        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\n        \"//conditions:default\": str('')\n    }),\n)\n\nAnd libdeepspeech.so:\ntf_cc_shared_object(\n    name = \"libdeepspeech.so\",\n    srcs = [\"deepspeech.cc\", \"deepspeech.h\", \"deepspeech_utils.h\", \"alphabet.h\", \"beam_search.h\", \"trie_node.h\"] +\n           if_native_model([\"deepspeech_model.h\"]) +\n           glob([\"kenlm/lm/*.cc\", \"kenlm/util/*.cc\", \"kenlm/util/double-conversion/*.cc\",\n                 \"kenlm/lm/*.hh\", \"kenlm/util/*.hh\", \"kenlm/util/double-conversion/*.h\"],\n                exclude = [\"kenlm/*/*test.cc\", \"kenlm/*/*main.cc\"]) +\n           glob([\"boost_locale/**/*.hpp\"]),\n    copts = [\"-std=c++11\", \"-Wno-sign-compare\"] + if_native_model([\n        \"-DDS_MODEL_TIMESTEPS=$(DS_MODEL_TIMESTEPS)\",\n        \"-DDS_NATIVE_MODEL=1\",\n    ]),\n    deps = [\n        \"//tensorflow/core:core_cpu\",\n        \"//tensorflow/core:direct_session\",\n        \"//tensorflow/core/kernels:constant_op\",        # Const\n        \"//tensorflow/core/kernels:identity_op\",        # Identity\n        \"//tensorflow/core/kernels:transpose_op\",       # Transpose\n        \"//tensorflow/core/kernels:reshape_op\",         # Reshape\n        \"//tensorflow/core/kernels:shape_ops\",          # Shape\n        \"//tensorflow/core/kernels:strided_slice_op\",   # StridedSlice\n        \"//tensorflow/core/kernels:pack_op\",            # Pack\n        \"//tensorflow/core/kernels:reverse_op\",         # ReverseV2\n        \"//tensorflow/core/kernels:concat_op\",          # ConcatV2\n        \"//tensorflow/core/kernels:split_op\",           # Split\n        \"//tensorflow/core/kernels:sparse_to_dense_op\", # SparseToDense\n        \"//tensorflow/core/kernels:relu_op\",            # Relu\n        \"//tensorflow/core/kernels:bias_op\",            # BiasAdd\n        \"//tensorflow/core/kernels:math\",               # Range, MatMul\n        \"//tensorflow/core/kernels:tensor_array_ops\",   # Placeholder, TensorArrayV3\n        \"//tensorflow/core/kernels:control_flow_ops\",   # Enter\n        \"//tensorflow/core/kernels:ctc_ops\",            # CTCBeamSearchDecoder\n        ### Needed by production model produced without \"--use_seq_length False\"\n        \"//tensorflow/core/kernels:logging_ops\",         # Assert\n        \"//tensorflow/core/kernels:reverse_sequence_op\", # ReverseSequence\n        # Classic deps\n        \"//tensorflow/core/util/ctc\",\n        \"//third_party/eigen3\",\n    ] + if_native_model([\n        \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\n        \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\n    ])\n      + if_cuda([\n        \"//tensorflow/core:core\",\n        \"//tensorflow/core/kernels:slice_op_gpu\",       # Slice GPU\n    ]),\n    includes = [\"kenlm\", \"boost_locale\"],\n    defines = [\"KENLM_MAX_ORDER=6\"],\n)", "body": "@sanjoy I do share your surprise :), here are more details, I hope it helps:\r\n```\r\n../../tensorflow//bazel-bin/native_client/libdeepspeech_model.so: undefined reference to `__xla_cpu_runtime_EigenMatMulF32'\r\n```\r\n\r\nAnd `libdeepspeech_model.so` is being built with `tf_library`. And inside the definition of `tf_library`, it does refers to `runtime_matmul`: https://github.com/mozilla/tensorflow/blob/1390dc180e25b5821be80b407ddc5fad73d4ef6a/tensorflow/compiler/aot/tfcompile.bzl#L197\r\n\r\nHowever,\r\n```\r\n$ grep -c runtime_matmul bazel-out/local-opt/bin/native_client/libdeepspeech*.so-2.params \r\nbazel-out/local-opt/bin/native_client/libdeepspeech_model.so-2.params:0\r\nbazel-out/local-opt/bin/native_client/libdeepspeech.so-2.params:1\r\nbazel-out/local-opt/bin/native_client/libdeepspeech_utils.so-2.params:0\r\n```\r\n\r\nAnd:\r\n```\r\n$ grep runtime_matmul bazel-out/local-opt/bin/native_client/libdeepspeech.so-2.params\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul.pic.o\r\n$ objdump -t bazel-out/local-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul.pic.o | grep MatMul\r\n0000000000000000 l    d  .text.__xla_cpu_runtime_EigenMatMulF32 0000000000000000 .text.__xla_cpu_runtime_EigenMatMulF32\r\n0000000000000000 l    d  .gcc_except_table.__xla_cpu_runtime_EigenMatMulF32     0000000000000000 .gcc_except_table.__xla_cpu_runtime_EigenMatMulF32\r\n0000000000000000 l    d  .text.__xla_cpu_runtime_EigenMatMulF64 0000000000000000 .text.__xla_cpu_runtime_EigenMatMulF64\r\n0000000000000000 l    d  .gcc_except_table.__xla_cpu_runtime_EigenMatMulF64     0000000000000000 .gcc_except_table.__xla_cpu_runtime_EigenMatMulF64\r\n0000000000000000 g     F .text.__xla_cpu_runtime_EigenMatMulF32 000000000000061e .hidden __xla_cpu_runtime_EigenMatMulF32\r\n0000000000000000 g     F .text.__xla_cpu_runtime_EigenMatMulF64 000000000000061e .hidden __xla_cpu_runtime_EigenMatMulF64\r\n```\r\n\r\nSo, for some reason, `runtime_matmul.pic.o` is not being fed into `libdeepspeech_model.so`, only in `libdeepspeech.so`. With the symbol being hidden, it does account for the visibility because we are at DSO level. Maybe this is the real issue here, and maybe we should see `runtime_matmul.pic.o` in `bazel-out/local-opt/bin/native_client/libdeepspeech_model.so-2.params`.\r\n\r\nFor reference, here is our definition of `libdeepspeech_model.so`:\r\n```\r\n\r\ntf_library(\r\n    name = \"deepspeech_model\",\r\n    cpp_class = \"DeepSpeech::nativeModel\",\r\n    # We don't need tests or benchmark binaries\r\n    gen_test=False, gen_benchmark=False,\r\n    # graph and config will be generated at build time thanks to the matching\r\n    # genrule.\r\n    graph = \"tfcompile.model.pb\",\r\n    config = \"tfcompile.config.pbtxt\",\r\n    # This depends on //tensorflow:rpi3 condition defined in mozilla/tensorflow\r\n    tfcompile_flags = select({\r\n        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\r\n        \"//conditions:default\": str('')\r\n    }),\r\n)\r\n```\r\n\r\nAnd `libdeepspeech.so`:\r\n```\r\ntf_cc_shared_object(\r\n    name = \"libdeepspeech.so\",\r\n    srcs = [\"deepspeech.cc\", \"deepspeech.h\", \"deepspeech_utils.h\", \"alphabet.h\", \"beam_search.h\", \"trie_node.h\"] +\r\n           if_native_model([\"deepspeech_model.h\"]) +\r\n           glob([\"kenlm/lm/*.cc\", \"kenlm/util/*.cc\", \"kenlm/util/double-conversion/*.cc\",\r\n                 \"kenlm/lm/*.hh\", \"kenlm/util/*.hh\", \"kenlm/util/double-conversion/*.h\"],\r\n                exclude = [\"kenlm/*/*test.cc\", \"kenlm/*/*main.cc\"]) +\r\n           glob([\"boost_locale/**/*.hpp\"]),\r\n    copts = [\"-std=c++11\", \"-Wno-sign-compare\"] + if_native_model([\r\n        \"-DDS_MODEL_TIMESTEPS=$(DS_MODEL_TIMESTEPS)\",\r\n        \"-DDS_NATIVE_MODEL=1\",\r\n    ]),\r\n    deps = [\r\n        \"//tensorflow/core:core_cpu\",\r\n        \"//tensorflow/core:direct_session\",\r\n        \"//tensorflow/core/kernels:constant_op\",        # Const\r\n        \"//tensorflow/core/kernels:identity_op\",        # Identity\r\n        \"//tensorflow/core/kernels:transpose_op\",       # Transpose\r\n        \"//tensorflow/core/kernels:reshape_op\",         # Reshape\r\n        \"//tensorflow/core/kernels:shape_ops\",          # Shape\r\n        \"//tensorflow/core/kernels:strided_slice_op\",   # StridedSlice\r\n        \"//tensorflow/core/kernels:pack_op\",            # Pack\r\n        \"//tensorflow/core/kernels:reverse_op\",         # ReverseV2\r\n        \"//tensorflow/core/kernels:concat_op\",          # ConcatV2\r\n        \"//tensorflow/core/kernels:split_op\",           # Split\r\n        \"//tensorflow/core/kernels:sparse_to_dense_op\", # SparseToDense\r\n        \"//tensorflow/core/kernels:relu_op\",            # Relu\r\n        \"//tensorflow/core/kernels:bias_op\",            # BiasAdd\r\n        \"//tensorflow/core/kernels:math\",               # Range, MatMul\r\n        \"//tensorflow/core/kernels:tensor_array_ops\",   # Placeholder, TensorArrayV3\r\n        \"//tensorflow/core/kernels:control_flow_ops\",   # Enter\r\n        \"//tensorflow/core/kernels:ctc_ops\",            # CTCBeamSearchDecoder\r\n        ### Needed by production model produced without \"--use_seq_length False\"\r\n        \"//tensorflow/core/kernels:logging_ops\",         # Assert\r\n        \"//tensorflow/core/kernels:reverse_sequence_op\", # ReverseSequence\r\n        # Classic deps\r\n        \"//tensorflow/core/util/ctc\",\r\n        \"//third_party/eigen3\",\r\n    ] + if_native_model([\r\n        \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\r\n        \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n    ])\r\n      + if_cuda([\r\n        \"//tensorflow/core:core\",\r\n        \"//tensorflow/core/kernels:slice_op_gpu\",       # Slice GPU\r\n    ]),\r\n    includes = [\"kenlm\", \"boost_locale\"],\r\n    defines = [\"KENLM_MAX_ORDER=6\"],\r\n)\r\n```"}