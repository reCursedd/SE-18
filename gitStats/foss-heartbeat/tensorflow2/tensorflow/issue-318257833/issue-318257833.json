{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18913", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18913/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18913/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18913/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18913", "id": 318257833, "node_id": "MDU6SXNzdWUzMTgyNTc4MzM=", "number": 18913, "title": "Accessing CuDNN autotuner in built-in Keras", "user": {"login": "ASvyatkovskiy", "id": 4876874, "node_id": "MDQ6VXNlcjQ4NzY4NzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/4876874?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ASvyatkovskiy", "html_url": "https://github.com/ASvyatkovskiy", "followers_url": "https://api.github.com/users/ASvyatkovskiy/followers", "following_url": "https://api.github.com/users/ASvyatkovskiy/following{/other_user}", "gists_url": "https://api.github.com/users/ASvyatkovskiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/ASvyatkovskiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ASvyatkovskiy/subscriptions", "organizations_url": "https://api.github.com/users/ASvyatkovskiy/orgs", "repos_url": "https://api.github.com/users/ASvyatkovskiy/repos", "events_url": "https://api.github.com/users/ASvyatkovskiy/events{/privacy}", "received_events_url": "https://api.github.com/users/ASvyatkovskiy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-04-27T02:33:54Z", "updated_at": "2018-07-04T19:08:36Z", "closed_at": "2018-07-04T18:37:54Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre>$ uname -mrs\nLinux 4.14.0-49.el7a.ppc64le ppc64le</pre></div>\n<ul>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6.0</li>\n<li><strong>Python version</strong>: 2.7.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.1, CuDNN 7.0.5</li>\n<li><strong>GPU model and memory</strong>: Tesla V100:</li>\n</ul>\n<pre><code>== nvidia-smi ===================================================\nThu Apr 26 18:35:34 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 396.19                 Driver Version: 396.19                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  Off  | 00000004:04:00.0 Off |                    0 |\n| N/A   29C    P0    52W / 300W |      0MiB / 15360MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n</code></pre>\n<ul>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<p>We would like to be able to have access to the CuDNN autotuner in <code>tf.keras</code> module to access optimal algorithms for a given hardware (or, perhaps passing a custom convolutional algorithm from config). In TensorFlow, I can specify to use the CuDNN autotuner by setting:</p>\n<div class=\"highlight highlight-source-shell\"><pre>os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TF_CUDNN_USE_AUTOTUNE<span class=\"pl-pds\">'</span></span>] = <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span></pre></div>\n<p>(currently enabled by default), which improves performance significantly on Volta GPUs and especially with FP16.</p>\n<p>However, I am unable to access this performance improvement when running pure <code>tf.keras</code>, where setting this environmental variable does not have any effect.</p>\n<h3>Source code / logs</h3>\n<p>Following simple example could be used to reproduce the issue: <a href=\"https://gist.github.com/ASvyatkovskiy/8d1dd622e447d9d8de1ec4e238e0dbaa\">https://gist.github.com/ASvyatkovskiy/8d1dd622e447d9d8de1ec4e238e0dbaa</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\n$ uname -mrs\nLinux 4.14.0-49.el7a.ppc64le ppc64le\n\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.6.0\nPython version: 2.7.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: CUDA 9.1, CuDNN 7.0.5\nGPU model and memory: Tesla V100:\n\n== nvidia-smi ===================================================\nThu Apr 26 18:35:34 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 396.19                 Driver Version: 396.19                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  Off  | 00000004:04:00.0 Off |                    0 |\n| N/A   29C    P0    52W / 300W |      0MiB / 15360MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n\nExact command to reproduce: N/A\n\nWe would like to be able to have access to the CuDNN autotuner in tf.keras module to access optimal algorithms for a given hardware (or, perhaps passing a custom convolutional algorithm from config). In TensorFlow, I can specify to use the CuDNN autotuner by setting:\nos.environ['TF_CUDNN_USE_AUTOTUNE'] = \"1\"\n(currently enabled by default), which improves performance significantly on Volta GPUs and especially with FP16.\nHowever, I am unable to access this performance improvement when running pure tf.keras, where setting this environmental variable does not have any effect.\nSource code / logs\nFollowing simple example could be used to reproduce the issue: https://gist.github.com/ASvyatkovskiy/8d1dd622e447d9d8de1ec4e238e0dbaa", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n```bash\r\n$ uname -mrs\r\nLinux 4.14.0-49.el7a.ppc64le ppc64le\r\n```\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 9.1, CuDNN 7.0.5\r\n- **GPU model and memory**: Tesla V100:\r\n```\r\n== nvidia-smi ===================================================\r\nThu Apr 26 18:35:34 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.19                 Driver Version: 396.19                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000004:04:00.0 Off |                    0 |\r\n| N/A   29C    P0    52W / 300W |      0MiB / 15360MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n- **Exact command to reproduce**: N/A\r\n\r\nWe would like to be able to have access to the CuDNN autotuner in `tf.keras` module to access optimal algorithms for a given hardware (or, perhaps passing a custom convolutional algorithm from config). In TensorFlow, I can specify to use the CuDNN autotuner by setting: \r\n```bash\r\nos.environ['TF_CUDNN_USE_AUTOTUNE'] = \"1\"\r\n```\r\n(currently enabled by default), which improves performance significantly on Volta GPUs and especially with FP16.\r\n\r\nHowever, I am unable to access this performance improvement when running pure `tf.keras`, where setting this environmental variable does not have any effect. \r\n\r\n### Source code / logs\r\n\r\nFollowing simple example could be used to reproduce the issue: https://gist.github.com/ASvyatkovskiy/8d1dd622e447d9d8de1ec4e238e0dbaa"}