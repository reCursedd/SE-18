{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14590", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14590/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14590/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14590/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14590", "id": 274248277, "node_id": "MDU6SXNzdWUyNzQyNDgyNzc=", "number": 14590, "title": "Loading TF 1.1 model in TF 1.4", "user": {"login": "s0urcer", "id": 1191582, "node_id": "MDQ6VXNlcjExOTE1ODI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1191582?v=4", "gravatar_id": "", "url": "https://api.github.com/users/s0urcer", "html_url": "https://github.com/s0urcer", "followers_url": "https://api.github.com/users/s0urcer/followers", "following_url": "https://api.github.com/users/s0urcer/following{/other_user}", "gists_url": "https://api.github.com/users/s0urcer/gists{/gist_id}", "starred_url": "https://api.github.com/users/s0urcer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/s0urcer/subscriptions", "organizations_url": "https://api.github.com/users/s0urcer/orgs", "repos_url": "https://api.github.com/users/s0urcer/repos", "events_url": "https://api.github.com/users/s0urcer/events{/privacy}", "received_events_url": "https://api.github.com/users/s0urcer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-11-15T17:53:36Z", "updated_at": "2017-12-20T16:44:02Z", "closed_at": "2017-12-05T17:50:12Z", "author_association": "NONE", "body_html": "<p>There is a model which has been trained in TF 1.1 (it's a seq2seq model with bahdanau attention). It uses DynamicAttentionWrapper (which has been renamed to AttentionWrapper). After updating TF to version 1.4 and switching to renamed AttentionWrapper the model can't be loaded. I get multiple errors like the following:</p>\n<pre><code>2017-11-15 19:45:26.550430: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam not found in checkpoint\n2017-11-15 19:45:26.550440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam_1 not found in checkpoint\n2017-11-15 19:45:26.550481: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam not found in checkpoint\n2017-11-15 19:45:26.552201: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam not found in checkpoint\n2017-11-15 19:45:26.552300: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam_1 not found in checkpoint\n2017-11-15 19:45:26.552426: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel not found in checkpoint\n2017-11-15 19:45:26.552434: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam_1 not found in checkpoint\n2017-11-15 19:45:26.552440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias not found in checkpoint\n2017-11-15 19:45:26.552496: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam not found in checkpoint\n2017-11-15 19:45:26.553055: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam_1 not found in checkpoint\n</code></pre>\n<p>Setting name to dynamic_attention_wrapper in AttentionWrapper constructor parameters does not resolve the issue. Am I missing something? Taking into account that production models take a lot of resources to be trained it would be good for them to be compatible between TF versions.  Thank you.</p>", "body_text": "There is a model which has been trained in TF 1.1 (it's a seq2seq model with bahdanau attention). It uses DynamicAttentionWrapper (which has been renamed to AttentionWrapper). After updating TF to version 1.4 and switching to renamed AttentionWrapper the model can't be loaded. I get multiple errors like the following:\n2017-11-15 19:45:26.550430: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam not found in checkpoint\n2017-11-15 19:45:26.550440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam_1 not found in checkpoint\n2017-11-15 19:45:26.550481: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam not found in checkpoint\n2017-11-15 19:45:26.552201: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam not found in checkpoint\n2017-11-15 19:45:26.552300: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam_1 not found in checkpoint\n2017-11-15 19:45:26.552426: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel not found in checkpoint\n2017-11-15 19:45:26.552434: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam_1 not found in checkpoint\n2017-11-15 19:45:26.552440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias not found in checkpoint\n2017-11-15 19:45:26.552496: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam not found in checkpoint\n2017-11-15 19:45:26.553055: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam_1 not found in checkpoint\n\nSetting name to dynamic_attention_wrapper in AttentionWrapper constructor parameters does not resolve the issue. Am I missing something? Taking into account that production models take a lot of resources to be trained it would be good for them to be compatible between TF versions.  Thank you.", "body": "There is a model which has been trained in TF 1.1 (it's a seq2seq model with bahdanau attention). It uses DynamicAttentionWrapper (which has been renamed to AttentionWrapper). After updating TF to version 1.4 and switching to renamed AttentionWrapper the model can't be loaded. I get multiple errors like the following:\r\n\r\n```\r\n2017-11-15 19:45:26.550430: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam not found in checkpoint\r\n2017-11-15 19:45:26.550440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam_1 not found in checkpoint\r\n2017-11-15 19:45:26.550481: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam not found in checkpoint\r\n2017-11-15 19:45:26.552201: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam not found in checkpoint\r\n2017-11-15 19:45:26.552300: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam_1 not found in checkpoint\r\n2017-11-15 19:45:26.552426: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel not found in checkpoint\r\n2017-11-15 19:45:26.552434: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam_1 not found in checkpoint\r\n2017-11-15 19:45:26.552440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias not found in checkpoint\r\n2017-11-15 19:45:26.552496: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam not found in checkpoint\r\n2017-11-15 19:45:26.553055: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam_1 not found in checkpoint\r\n```\r\n\r\nSetting name to dynamic_attention_wrapper in AttentionWrapper constructor parameters does not resolve the issue. Am I missing something? Taking into account that production models take a lot of resources to be trained it would be good for them to be compatible between TF versions.  Thank you."}