{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/297829967", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-297829967", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 297829967, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzgyOTk2Nw==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-27T20:28:20Z", "updated_at": "2017-04-27T20:28:20Z", "author_association": "NONE", "body_html": "<p>I often try to use TensorFlow on very large inputs (potentially &gt;1GB minibatch) with relatively light computation on each minibatch. These inputs are in a HDF5 file or a Numpy array either on disk or in memory, so I typically feed with <code>feed_dict</code>, potentially asynchronously into a queue. When running with multiple GPUs, TensorFlow is not able to even saturate the PCI-e bandwidth to the GPUs because of the memcpy from the feed_dict to the CPU tensor.</p>\n<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> mentioned, the <code>feed_dict</code> memcpy (on a single CPU core?) can be a huge performance bottleneck, and I'd like to see this addressed in any refactor of TensorFlow's input handling.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=170179\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jhseu\">@jhseu</a> You mentioned that you consider removing the <code>feed_dict</code> copy as orthogonal to this issue. Do you know if there's any issue or work being done on removing the copy (at least in some cases, like row-major Numpy arrays with nice strides)?</p>", "body_text": "I often try to use TensorFlow on very large inputs (potentially >1GB minibatch) with relatively light computation on each minibatch. These inputs are in a HDF5 file or a Numpy array either on disk or in memory, so I typically feed with feed_dict, potentially asynchronously into a queue. When running with multiple GPUs, TensorFlow is not able to even saturate the PCI-e bandwidth to the GPUs because of the memcpy from the feed_dict to the CPU tensor.\nAs @yaroslavvb mentioned, the feed_dict memcpy (on a single CPU core?) can be a huge performance bottleneck, and I'd like to see this addressed in any refactor of TensorFlow's input handling.\n@jhseu You mentioned that you consider removing the feed_dict copy as orthogonal to this issue. Do you know if there's any issue or work being done on removing the copy (at least in some cases, like row-major Numpy arrays with nice strides)?", "body": "I often try to use TensorFlow on very large inputs (potentially >1GB minibatch) with relatively light computation on each minibatch. These inputs are in a HDF5 file or a Numpy array either on disk or in memory, so I typically feed with ``feed_dict``, potentially asynchronously into a queue. When running with multiple GPUs, TensorFlow is not able to even saturate the PCI-e bandwidth to the GPUs because of the memcpy from the feed_dict to the CPU tensor.\r\n\r\nAs @yaroslavvb mentioned, the ``feed_dict`` memcpy (on a single CPU core?) can be a huge performance bottleneck, and I'd like to see this addressed in any refactor of TensorFlow's input handling.\r\n\r\n@jhseu You mentioned that you consider removing the ``feed_dict`` copy as orthogonal to this issue. Do you know if there's any issue or work being done on removing the copy (at least in some cases, like row-major Numpy arrays with nice strides)?"}