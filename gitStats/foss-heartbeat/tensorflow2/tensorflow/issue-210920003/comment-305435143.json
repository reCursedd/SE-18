{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/305435143", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-305435143", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 305435143, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTQzNTE0Mw==", "user": {"login": "kratzert", "id": 13069767, "node_id": "MDQ6VXNlcjEzMDY5NzY3", "avatar_url": "https://avatars0.githubusercontent.com/u/13069767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kratzert", "html_url": "https://github.com/kratzert", "followers_url": "https://api.github.com/users/kratzert/followers", "following_url": "https://api.github.com/users/kratzert/following{/other_user}", "gists_url": "https://api.github.com/users/kratzert/gists{/gist_id}", "starred_url": "https://api.github.com/users/kratzert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kratzert/subscriptions", "organizations_url": "https://api.github.com/users/kratzert/orgs", "repos_url": "https://api.github.com/users/kratzert/repos", "events_url": "https://api.github.com/users/kratzert/events{/privacy}", "received_events_url": "https://api.github.com/users/kratzert/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-01T09:07:12Z", "updated_at": "2017-06-01T09:07:12Z", "author_association": "NONE", "body_html": "<p>In the last couple of days I was playing around quite a bit with the new Input API and I think the <code>Dataset</code> and <code>Iterator</code> classes improve highly the clearness and readability of code (compared to the old input queues). Also switching between e.g. training and validation dataset within one session works quite effortless.</p>\n<p>But I have also some questions and suggestions.<br>\nMaybe first a question: Is the <code>Dataset</code> class implemented based on queues? Because from the post here it doesn't get clear to me if or if not. In Tensorboard there is no additional information added with the new API about the status of any queue (how many objects are currently queued). Also observing my CPU/GPU resources/workload I can see, that the GPU workload drops to zero often (I guess in between batches).</p>\n<p>Then a suggestion:<br>\nI think the <code>dataset.shuffle()</code> could be improved, if shuffling is not done only on the <em>n</em> ( = buffer_size) samples in memory, but somehow on the whole list of inputs. For example: I'm storing path to images and labels in a text file. If shuffling is not done already in the text file you can often have thousands of lines after each other of the same class. If I now only work with <code>dataset.shuffle()</code> it can easily happen (depending of the buffer_size) that all elements that get shuffled are anyhow of the same class. The only. Maybe some toy example (ignoring labels and only working with image paths) to make my point clearer. For reasons of readability I work with a very small <code>buffer_size</code> and list of file names. But I guess everyone can imagine the same just with thousands of filenames in the list and a buffer_size e.g. of 5000.</p>\n<pre><code>import tensorflow as tf\n\nimage_paths = tf.constant(['train/class1/img1.png',\n                          'train/class1/img2.png',\n                          'train/class1/img3.png',\n                          'train/class1/img4.png',\n                          'train/class1/img5.png',\n                          'train/class1/img6.png',\n                          'train/class1/img7.png',\n                          'train/class2/img1.png', \n                          'train/class2/img2.png',\n                          'train/class2/img3.png',\n                          'train/class2/img4.png',\n                          'train/class2/img5.png',\n                          'train/class1/img6.png',\n                          'train/class1/img7.png'])\n\ndataset = tf.contrib.data.Dataset.from_tensor_slices(image_paths)\ndataset = dataset.shuffle(3)\niterator = dataset.make_one_shot_iterator()\nnext_element = iterator.get_next()\n\nwith tf.Session() as sess:\n    while True:\n        try:\n            path = sess.run(next_element)\n            print(path)\n        except tf.errors.OutOfRangeError:\n            break\n</code></pre>\n<p>This would print something like:</p>\n<pre><code>'train/class1/img1.png'\n'train/class1/img2.png'\n'train/class1/img4.png'\n'train/class1/img3.png'\n'train/class1/img5.png'\n'train/class1/img7.png'\n'train/class2/img1.png'\n'train/class1/img6.png'\n'train/class2/img3.png'\n'train/class2/img4.png'\n'train/class1/img6.png'\n'train/class2/img5.png'\n'train/class1/img7.png'\n'train/class2/img2.png'\n</code></pre>\n<p>So since there is only a shuffling between the 3 examples in the buffer, the first samples (same for batches) will all have samples only of one class. So unless the shuffling isn't done already in the list of filenames you'll have troubles training any network. And if the available dataset of images is huge, increasing the buffer_size is often not a solution.<br>\nAnother problem I see, is that like shuffling currently is implemented, there is no true shuffling of the entire dataset possible. The only workaround I found was pre-shuffling the filelist I read from the text file before creating the dataset. But once the dataset is created, it's only possible to shuffle in the range of the buffer_size.</p>", "body_text": "In the last couple of days I was playing around quite a bit with the new Input API and I think the Dataset and Iterator classes improve highly the clearness and readability of code (compared to the old input queues). Also switching between e.g. training and validation dataset within one session works quite effortless.\nBut I have also some questions and suggestions.\nMaybe first a question: Is the Dataset class implemented based on queues? Because from the post here it doesn't get clear to me if or if not. In Tensorboard there is no additional information added with the new API about the status of any queue (how many objects are currently queued). Also observing my CPU/GPU resources/workload I can see, that the GPU workload drops to zero often (I guess in between batches).\nThen a suggestion:\nI think the dataset.shuffle() could be improved, if shuffling is not done only on the n ( = buffer_size) samples in memory, but somehow on the whole list of inputs. For example: I'm storing path to images and labels in a text file. If shuffling is not done already in the text file you can often have thousands of lines after each other of the same class. If I now only work with dataset.shuffle() it can easily happen (depending of the buffer_size) that all elements that get shuffled are anyhow of the same class. The only. Maybe some toy example (ignoring labels and only working with image paths) to make my point clearer. For reasons of readability I work with a very small buffer_size and list of file names. But I guess everyone can imagine the same just with thousands of filenames in the list and a buffer_size e.g. of 5000.\nimport tensorflow as tf\n\nimage_paths = tf.constant(['train/class1/img1.png',\n                          'train/class1/img2.png',\n                          'train/class1/img3.png',\n                          'train/class1/img4.png',\n                          'train/class1/img5.png',\n                          'train/class1/img6.png',\n                          'train/class1/img7.png',\n                          'train/class2/img1.png', \n                          'train/class2/img2.png',\n                          'train/class2/img3.png',\n                          'train/class2/img4.png',\n                          'train/class2/img5.png',\n                          'train/class1/img6.png',\n                          'train/class1/img7.png'])\n\ndataset = tf.contrib.data.Dataset.from_tensor_slices(image_paths)\ndataset = dataset.shuffle(3)\niterator = dataset.make_one_shot_iterator()\nnext_element = iterator.get_next()\n\nwith tf.Session() as sess:\n    while True:\n        try:\n            path = sess.run(next_element)\n            print(path)\n        except tf.errors.OutOfRangeError:\n            break\n\nThis would print something like:\n'train/class1/img1.png'\n'train/class1/img2.png'\n'train/class1/img4.png'\n'train/class1/img3.png'\n'train/class1/img5.png'\n'train/class1/img7.png'\n'train/class2/img1.png'\n'train/class1/img6.png'\n'train/class2/img3.png'\n'train/class2/img4.png'\n'train/class1/img6.png'\n'train/class2/img5.png'\n'train/class1/img7.png'\n'train/class2/img2.png'\n\nSo since there is only a shuffling between the 3 examples in the buffer, the first samples (same for batches) will all have samples only of one class. So unless the shuffling isn't done already in the list of filenames you'll have troubles training any network. And if the available dataset of images is huge, increasing the buffer_size is often not a solution.\nAnother problem I see, is that like shuffling currently is implemented, there is no true shuffling of the entire dataset possible. The only workaround I found was pre-shuffling the filelist I read from the text file before creating the dataset. But once the dataset is created, it's only possible to shuffle in the range of the buffer_size.", "body": "In the last couple of days I was playing around quite a bit with the new Input API and I think the `Dataset` and `Iterator` classes improve highly the clearness and readability of code (compared to the old input queues). Also switching between e.g. training and validation dataset within one session works quite effortless. \r\n\r\nBut I have also some questions and suggestions.\r\nMaybe first a question: Is the `Dataset` class implemented based on queues? Because from the post here it doesn't get clear to me if or if not. In Tensorboard there is no additional information added with the new API about the status of any queue (how many objects are currently queued). Also observing my CPU/GPU resources/workload I can see, that the GPU workload drops to zero often (I guess in between batches). \r\n\r\nThen a suggestion:\r\nI think the `dataset.shuffle()` could be improved, if shuffling is not done only on the _n_ ( = buffer_size) samples in memory, but somehow on the whole list of inputs. For example: I'm storing path to images and labels in a text file. If shuffling is not done already in the text file you can often have thousands of lines after each other of the same class. If I now only work with `dataset.shuffle()` it can easily happen (depending of the buffer_size) that all elements that get shuffled are anyhow of the same class. The only. Maybe some toy example (ignoring labels and only working with image paths) to make my point clearer. For reasons of readability I work with a very small `buffer_size` and list of file names. But I guess everyone can imagine the same just with thousands of filenames in the list and a buffer_size e.g. of 5000.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimage_paths = tf.constant(['train/class1/img1.png',\r\n                          'train/class1/img2.png',\r\n                          'train/class1/img3.png',\r\n                          'train/class1/img4.png',\r\n                          'train/class1/img5.png',\r\n                          'train/class1/img6.png',\r\n                          'train/class1/img7.png',\r\n                          'train/class2/img1.png', \r\n                          'train/class2/img2.png',\r\n                          'train/class2/img3.png',\r\n                          'train/class2/img4.png',\r\n                          'train/class2/img5.png',\r\n                          'train/class1/img6.png',\r\n                          'train/class1/img7.png'])\r\n\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices(image_paths)\r\ndataset = dataset.shuffle(3)\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    while True:\r\n        try:\r\n            path = sess.run(next_element)\r\n            print(path)\r\n        except tf.errors.OutOfRangeError:\r\n            break\r\n```\r\nThis would print something like:\r\n```\r\n'train/class1/img1.png'\r\n'train/class1/img2.png'\r\n'train/class1/img4.png'\r\n'train/class1/img3.png'\r\n'train/class1/img5.png'\r\n'train/class1/img7.png'\r\n'train/class2/img1.png'\r\n'train/class1/img6.png'\r\n'train/class2/img3.png'\r\n'train/class2/img4.png'\r\n'train/class1/img6.png'\r\n'train/class2/img5.png'\r\n'train/class1/img7.png'\r\n'train/class2/img2.png'\r\n```\r\nSo since there is only a shuffling between the 3 examples in the buffer, the first samples (same for batches) will all have samples only of one class. So unless the shuffling isn't done already in the list of filenames you'll have troubles training any network. And if the available dataset of images is huge, increasing the buffer_size is often not a solution.\r\nAnother problem I see, is that like shuffling currently is implemented, there is no true shuffling of the entire dataset possible. The only workaround I found was pre-shuffling the filelist I read from the text file before creating the dataset. But once the dataset is created, it's only possible to shuffle in the range of the buffer_size."}