{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296187676", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-296187676", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 296187676, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjE4NzY3Ng==", "user": {"login": "kratzert", "id": 13069767, "node_id": "MDQ6VXNlcjEzMDY5NzY3", "avatar_url": "https://avatars0.githubusercontent.com/u/13069767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kratzert", "html_url": "https://github.com/kratzert", "followers_url": "https://api.github.com/users/kratzert/followers", "following_url": "https://api.github.com/users/kratzert/following{/other_user}", "gists_url": "https://api.github.com/users/kratzert/gists{/gist_id}", "starred_url": "https://api.github.com/users/kratzert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kratzert/subscriptions", "organizations_url": "https://api.github.com/users/kratzert/orgs", "repos_url": "https://api.github.com/users/kratzert/repos", "events_url": "https://api.github.com/users/kratzert/events{/privacy}", "received_events_url": "https://api.github.com/users/kratzert/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-21T13:16:38Z", "updated_at": "2017-04-21T13:16:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> correct me if I'm wrong but this isn't entirely right. Unless you haven't implemented some input pipeline using Pythons Queue library or something similar it will be additionally the time of loading data from disk into memory and eventually preprocess them.</p>\n<p>For Images and especially larger batch size, this might take quite a while. Here is where you can really speed things up using TFs input queues because they will load e.g. images into disk ( + preprocess ) on CPU while you are training/evaluating your network on GPU. When computations are done, you can directly grab the next batch on copy the data on the GPU, without waiting for native Python to load new data into memory.</p>", "body_text": "@yaroslavvb correct me if I'm wrong but this isn't entirely right. Unless you haven't implemented some input pipeline using Pythons Queue library or something similar it will be additionally the time of loading data from disk into memory and eventually preprocess them.\nFor Images and especially larger batch size, this might take quite a while. Here is where you can really speed things up using TFs input queues because they will load e.g. images into disk ( + preprocess ) on CPU while you are training/evaluating your network on GPU. When computations are done, you can directly grab the next batch on copy the data on the GPU, without waiting for native Python to load new data into memory.", "body": "@yaroslavvb correct me if I'm wrong but this isn't entirely right. Unless you haven't implemented some input pipeline using Pythons Queue library or something similar it will be additionally the time of loading data from disk into memory and eventually preprocess them.\r\n\r\nFor Images and especially larger batch size, this might take quite a while. Here is where you can really speed things up using TFs input queues because they will load e.g. images into disk ( + preprocess ) on CPU while you are training/evaluating your network on GPU. When computations are done, you can directly grab the next batch on copy the data on the GPU, without waiting for native Python to load new data into memory."}