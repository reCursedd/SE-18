{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/283624387", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-283624387", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 283624387, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzYyNDM4Nw==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-02T11:06:23Z", "updated_at": "2017-03-02T11:47:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For a lot of my use cases, my input data is either 1. not on the file system, or 2. require complex preprocessing unavailable in TensorFlow. For both cases the existing input pipeline cannot help, so I use an input thread with enqueue/feed_dict + a training thread with dequeue a lot.</p>\n<p>Let's <strong>assume</strong> that in most cases, you don't need to use the model itself to produce data (though sometimes it's not true). Then a solution I really like to see, is to be able to receive(similar to dequeue) tensors from a different process. (Like <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"181790801\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4836\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4836/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4836\">#4836</a>)<br>\nThe benefits are:</p>\n<ol>\n<li>Can use whatever tools/languages to produce data from any sources, as long as they're finally sent with certain message protocol.</li>\n<li>(theoretically) doesn't require an extra python thread in the training process.</li>\n<li>If the message protocol supports pub/sub, then (1) multiple training sessions can subcribe and reuse the same input data, which is very useful when trying new models. (2) data can be generated from different machines if the pre-processing is too heavy for a single CPU.</li>\n</ol>\n<p>These are the features I really missed from a private system I've been using.<br>\nOne disadvantage is that IPC/socket has smaller bandwidth than RAM but usually it's not a bottleneck.<br>\nI know this feature may be too far away, but I hope the new design could allow such possible future feature.</p>", "body_text": "For a lot of my use cases, my input data is either 1. not on the file system, or 2. require complex preprocessing unavailable in TensorFlow. For both cases the existing input pipeline cannot help, so I use an input thread with enqueue/feed_dict + a training thread with dequeue a lot.\nLet's assume that in most cases, you don't need to use the model itself to produce data (though sometimes it's not true). Then a solution I really like to see, is to be able to receive(similar to dequeue) tensors from a different process. (Like #4836)\nThe benefits are:\n\nCan use whatever tools/languages to produce data from any sources, as long as they're finally sent with certain message protocol.\n(theoretically) doesn't require an extra python thread in the training process.\nIf the message protocol supports pub/sub, then (1) multiple training sessions can subcribe and reuse the same input data, which is very useful when trying new models. (2) data can be generated from different machines if the pre-processing is too heavy for a single CPU.\n\nThese are the features I really missed from a private system I've been using.\nOne disadvantage is that IPC/socket has smaller bandwidth than RAM but usually it's not a bottleneck.\nI know this feature may be too far away, but I hope the new design could allow such possible future feature.", "body": "For a lot of my use cases, my input data is either 1. not on the file system, or 2. require complex preprocessing unavailable in TensorFlow. For both cases the existing input pipeline cannot help, so I use an input thread with enqueue/feed_dict + a training thread with dequeue a lot.\r\n\r\nLet's **assume** that in most cases, you don't need to use the model itself to produce data (though sometimes it's not true). Then a solution I really like to see, is to be able to receive(similar to dequeue) tensors from a different process. (Like #4836)\r\nThe benefits are:\r\n1. Can use whatever tools/languages to produce data from any sources, as long as they're finally sent with certain message protocol.\r\n2. (theoretically) doesn't require an extra python thread in the training process.\r\n3. If the message protocol supports pub/sub, then (1) multiple training sessions can subcribe and reuse the same input data, which is very useful when trying new models. (2) data can be generated from different machines if the pre-processing is too heavy for a single CPU.\r\n\r\nThese are the features I really missed from a private system I've been using.\r\nOne disadvantage is that IPC/socket has smaller bandwidth than RAM but usually it's not a bottleneck.\r\nI know this feature may be too far away, but I hope the new design could allow such possible future feature."}