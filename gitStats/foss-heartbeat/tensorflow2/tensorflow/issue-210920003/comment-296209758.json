{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296209758", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-296209758", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 296209758, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjIwOTc1OA==", "user": {"login": "kratzert", "id": 13069767, "node_id": "MDQ6VXNlcjEzMDY5NzY3", "avatar_url": "https://avatars0.githubusercontent.com/u/13069767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kratzert", "html_url": "https://github.com/kratzert", "followers_url": "https://api.github.com/users/kratzert/followers", "following_url": "https://api.github.com/users/kratzert/following{/other_user}", "gists_url": "https://api.github.com/users/kratzert/gists{/gist_id}", "starred_url": "https://api.github.com/users/kratzert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kratzert/subscriptions", "organizations_url": "https://api.github.com/users/kratzert/orgs", "repos_url": "https://api.github.com/users/kratzert/repos", "events_url": "https://api.github.com/users/kratzert/events{/privacy}", "received_events_url": "https://api.github.com/users/kratzert/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-21T14:41:43Z", "updated_at": "2017-04-21T14:41:43Z", "author_association": "NONE", "body_html": "<p>Is it? By no means I want to defend TFs input queues, but as I read the post of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> he states that the additional time comes (only) from passing memory between native Python and TF ( + GPU). The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle. I can't find any of this statement in the post of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3112159\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/taion\">@taion</a>, but could be a misunderstanding of my side as I'm not a English native speaker.</p>\n<p>And I know, that for many cases asynchronous preprocessing is not possible, but for the cases it is (simple training of image classification CNN) TFs input queues help quite a lot.</p>", "body_text": "Is it? By no means I want to defend TFs input queues, but as I read the post of @yaroslavvb he states that the additional time comes (only) from passing memory between native Python and TF ( + GPU). The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle. I can't find any of this statement in the post of @taion, but could be a misunderstanding of my side as I'm not a English native speaker.\nAnd I know, that for many cases asynchronous preprocessing is not possible, but for the cases it is (simple training of image classification CNN) TFs input queues help quite a lot.", "body": "Is it? By no means I want to defend TFs input queues, but as I read the post of @yaroslavvb he states that the additional time comes (only) from passing memory between native Python and TF ( + GPU). The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle. I can't find any of this statement in the post of @taion, but could be a misunderstanding of my side as I'm not a English native speaker.\r\n\r\nAnd I know, that for many cases asynchronous preprocessing is not possible, but for the cases it is (simple training of image classification CNN) TFs input queues help quite a lot. "}