{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285162353", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-285162353", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 285162353, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTE2MjM1Mw==", "user": {"login": "ErikGoldman", "id": 243490, "node_id": "MDQ6VXNlcjI0MzQ5MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/243490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ErikGoldman", "html_url": "https://github.com/ErikGoldman", "followers_url": "https://api.github.com/users/ErikGoldman/followers", "following_url": "https://api.github.com/users/ErikGoldman/following{/other_user}", "gists_url": "https://api.github.com/users/ErikGoldman/gists{/gist_id}", "starred_url": "https://api.github.com/users/ErikGoldman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ErikGoldman/subscriptions", "organizations_url": "https://api.github.com/users/ErikGoldman/orgs", "repos_url": "https://api.github.com/users/ErikGoldman/repos", "events_url": "https://api.github.com/users/ErikGoldman/events{/privacy}", "received_events_url": "https://api.github.com/users/ErikGoldman/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-08T20:40:52Z", "updated_at": "2017-03-08T20:40:52Z", "author_association": "NONE", "body_html": "<p>right now there are two very divergent paths to getting data into Tensorflow: feed_dict and queues. queues are wonderful until you don't have a way to manipulate your data natively -- for example, if you want to load a .wav file, chop it into parts, and convert it to a spectrogram. at that point, you have to write a C++ op (doable, but a context switch + it makes a very inflexible pipeline) or pop back into Python land (slower, but very easy and flexible).</p>\n<p>it seems like the best compromise between speed and flexibility is to create a TF queue and then make a bunch of Python threads that feed it with data. this allows you to do flexible data processing in Python (roughly parallelized on the CPU, apart from GIL issues) while maintaining some amount of speed benefit.</p>\n<p>what if you just formalized that? the interface would be: push_data, end_of_data (for signaling the end of an epoch), and a dequeue_batch function that feeds the model. then your code could just load data in Python and stuff it onto the queue in parallel, while the model sits totally separate from all of that.</p>", "body_text": "right now there are two very divergent paths to getting data into Tensorflow: feed_dict and queues. queues are wonderful until you don't have a way to manipulate your data natively -- for example, if you want to load a .wav file, chop it into parts, and convert it to a spectrogram. at that point, you have to write a C++ op (doable, but a context switch + it makes a very inflexible pipeline) or pop back into Python land (slower, but very easy and flexible).\nit seems like the best compromise between speed and flexibility is to create a TF queue and then make a bunch of Python threads that feed it with data. this allows you to do flexible data processing in Python (roughly parallelized on the CPU, apart from GIL issues) while maintaining some amount of speed benefit.\nwhat if you just formalized that? the interface would be: push_data, end_of_data (for signaling the end of an epoch), and a dequeue_batch function that feeds the model. then your code could just load data in Python and stuff it onto the queue in parallel, while the model sits totally separate from all of that.", "body": "right now there are two very divergent paths to getting data into Tensorflow: feed_dict and queues. queues are wonderful until you don't have a way to manipulate your data natively -- for example, if you want to load a .wav file, chop it into parts, and convert it to a spectrogram. at that point, you have to write a C++ op (doable, but a context switch + it makes a very inflexible pipeline) or pop back into Python land (slower, but very easy and flexible).\r\n\r\nit seems like the best compromise between speed and flexibility is to create a TF queue and then make a bunch of Python threads that feed it with data. this allows you to do flexible data processing in Python (roughly parallelized on the CPU, apart from GIL issues) while maintaining some amount of speed benefit.\r\n\r\nwhat if you just formalized that? the interface would be: push_data, end_of_data (for signaling the end of an epoch), and a dequeue_batch function that feeds the model. then your code could just load data in Python and stuff it onto the queue in parallel, while the model sits totally separate from all of that."}