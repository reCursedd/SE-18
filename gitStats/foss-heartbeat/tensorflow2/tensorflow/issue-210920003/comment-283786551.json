{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/283786551", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-283786551", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 283786551, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Mzc4NjU1MQ==", "user": {"login": "kmhofmann", "id": 7887138, "node_id": "MDQ6VXNlcjc4ODcxMzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/7887138?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kmhofmann", "html_url": "https://github.com/kmhofmann", "followers_url": "https://api.github.com/users/kmhofmann/followers", "following_url": "https://api.github.com/users/kmhofmann/following{/other_user}", "gists_url": "https://api.github.com/users/kmhofmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/kmhofmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kmhofmann/subscriptions", "organizations_url": "https://api.github.com/users/kmhofmann/orgs", "repos_url": "https://api.github.com/users/kmhofmann/repos", "events_url": "https://api.github.com/users/kmhofmann/events{/privacy}", "received_events_url": "https://api.github.com/users/kmhofmann/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-02T21:24:28Z", "updated_at": "2017-03-02T21:24:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> One \"data set\" can be composed of anything between ~500-30,000 dynamically generated samples. At the moment, we don't perform specific operations at the end of each data set, i.e. everything gets put into the same (large) random shuffle queue, to mix samples between data sets. But I could also imagine cases where separation of sets might be helpful.</p>", "body_text": "@mrry One \"data set\" can be composed of anything between ~500-30,000 dynamically generated samples. At the moment, we don't perform specific operations at the end of each data set, i.e. everything gets put into the same (large) random shuffle queue, to mix samples between data sets. But I could also imagine cases where separation of sets might be helpful.", "body": "@mrry One \"data set\" can be composed of anything between ~500-30,000 dynamically generated samples. At the moment, we don't perform specific operations at the end of each data set, i.e. everything gets put into the same (large) random shuffle queue, to mix samples between data sets. But I could also imagine cases where separation of sets might be helpful."}