{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307476700", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-307476700", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 307476700, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzQ3NjcwMA==", "user": {"login": "nirmalthacker", "id": 1928815, "node_id": "MDQ6VXNlcjE5Mjg4MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1928815?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nirmalthacker", "html_url": "https://github.com/nirmalthacker", "followers_url": "https://api.github.com/users/nirmalthacker/followers", "following_url": "https://api.github.com/users/nirmalthacker/following{/other_user}", "gists_url": "https://api.github.com/users/nirmalthacker/gists{/gist_id}", "starred_url": "https://api.github.com/users/nirmalthacker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nirmalthacker/subscriptions", "organizations_url": "https://api.github.com/users/nirmalthacker/orgs", "repos_url": "https://api.github.com/users/nirmalthacker/repos", "events_url": "https://api.github.com/users/nirmalthacker/events{/privacy}", "received_events_url": "https://api.github.com/users/nirmalthacker/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-09T19:17:47Z", "updated_at": "2017-06-09T19:17:47Z", "author_association": "NONE", "body_html": "<p>Hi, firstly thanks for this API, Im very keen on using it. Primarily I am interested in using it to switch between training and validation datasets in the same process.</p>\n<p>However I'm confused how one does that in this new paradigm. For instance I see no way to \"get an iterator\" in the middle of a dataset. As an example here is a piece of code that demonstrates what I'd like to do. Every few steps in an epoch, I'd like to run a validation op, but the output of this code shows that the iterator never advances ahead of item 0 in either dataset.  How does one do that?</p>\n<pre><code>training_dataset = tf.contrib.data.Dataset.range(100)\nvalidation_dataset = tf.contrib.data.Dataset.range(900,950)\niterator = tf.contrib.data.Iterator.from_structure(training_dataset.output_types,\n                                   training_dataset.output_shapes)\nnext_element = iterator.get_next()\n\ntraining_init_op = iterator.make_initializer(training_dataset)\nvalidation_init_op = iterator.make_initializer(validation_dataset)\n\n# Run 3 epochs with 10 steps each\nfor e in range(3):\n  print(\"Epoch: %d\" %e)\n  for step in range(11):\n    sess.run(training_init_op)\n    ne = sess.run(next_element)\n    print(\"Step: %d Training set: next_element: %d \" %(step, ne))\n    # Run validation every 5 steps only\n    if step % 5 == 0:\n      sess.run(validation_init_op)\n      ne = sess.run(next_element)\n      print(\"Step: %d Validation set: next_element: %d \" %(step, ne))\n</code></pre>\n<p>In the above, since we run an init each time to get an iterator pointing to its required dataset, we end up running a training and validation on the first item of each dataset, always. How does one modify this to get the updated position of the iterator in each dataset?</p>", "body_text": "Hi, firstly thanks for this API, Im very keen on using it. Primarily I am interested in using it to switch between training and validation datasets in the same process.\nHowever I'm confused how one does that in this new paradigm. For instance I see no way to \"get an iterator\" in the middle of a dataset. As an example here is a piece of code that demonstrates what I'd like to do. Every few steps in an epoch, I'd like to run a validation op, but the output of this code shows that the iterator never advances ahead of item 0 in either dataset.  How does one do that?\ntraining_dataset = tf.contrib.data.Dataset.range(100)\nvalidation_dataset = tf.contrib.data.Dataset.range(900,950)\niterator = tf.contrib.data.Iterator.from_structure(training_dataset.output_types,\n                                   training_dataset.output_shapes)\nnext_element = iterator.get_next()\n\ntraining_init_op = iterator.make_initializer(training_dataset)\nvalidation_init_op = iterator.make_initializer(validation_dataset)\n\n# Run 3 epochs with 10 steps each\nfor e in range(3):\n  print(\"Epoch: %d\" %e)\n  for step in range(11):\n    sess.run(training_init_op)\n    ne = sess.run(next_element)\n    print(\"Step: %d Training set: next_element: %d \" %(step, ne))\n    # Run validation every 5 steps only\n    if step % 5 == 0:\n      sess.run(validation_init_op)\n      ne = sess.run(next_element)\n      print(\"Step: %d Validation set: next_element: %d \" %(step, ne))\n\nIn the above, since we run an init each time to get an iterator pointing to its required dataset, we end up running a training and validation on the first item of each dataset, always. How does one modify this to get the updated position of the iterator in each dataset?", "body": "Hi, firstly thanks for this API, Im very keen on using it. Primarily I am interested in using it to switch between training and validation datasets in the same process.\r\n\r\nHowever I'm confused how one does that in this new paradigm. For instance I see no way to \"get an iterator\" in the middle of a dataset. As an example here is a piece of code that demonstrates what I'd like to do. Every few steps in an epoch, I'd like to run a validation op, but the output of this code shows that the iterator never advances ahead of item 0 in either dataset.  How does one do that?\r\n\r\n```\r\ntraining_dataset = tf.contrib.data.Dataset.range(100)\r\nvalidation_dataset = tf.contrib.data.Dataset.range(900,950)\r\niterator = tf.contrib.data.Iterator.from_structure(training_dataset.output_types,\r\n                                   training_dataset.output_shapes)\r\nnext_element = iterator.get_next()\r\n\r\ntraining_init_op = iterator.make_initializer(training_dataset)\r\nvalidation_init_op = iterator.make_initializer(validation_dataset)\r\n\r\n# Run 3 epochs with 10 steps each\r\nfor e in range(3):\r\n  print(\"Epoch: %d\" %e)\r\n  for step in range(11):\r\n    sess.run(training_init_op)\r\n    ne = sess.run(next_element)\r\n    print(\"Step: %d Training set: next_element: %d \" %(step, ne))\r\n    # Run validation every 5 steps only\r\n    if step % 5 == 0:\r\n      sess.run(validation_init_op)\r\n      ne = sess.run(next_element)\r\n      print(\"Step: %d Validation set: next_element: %d \" %(step, ne))\r\n```\r\n\r\nIn the above, since we run an init each time to get an iterator pointing to its required dataset, we end up running a training and validation on the first item of each dataset, always. How does one modify this to get the updated position of the iterator in each dataset?"}