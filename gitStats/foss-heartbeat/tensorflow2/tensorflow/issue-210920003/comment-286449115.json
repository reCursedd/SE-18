{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286449115", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-286449115", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 286449115, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjQ0OTExNQ==", "user": {"login": "lw394", "id": 15891975, "node_id": "MDQ6VXNlcjE1ODkxOTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/15891975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lw394", "html_url": "https://github.com/lw394", "followers_url": "https://api.github.com/users/lw394/followers", "following_url": "https://api.github.com/users/lw394/following{/other_user}", "gists_url": "https://api.github.com/users/lw394/gists{/gist_id}", "starred_url": "https://api.github.com/users/lw394/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lw394/subscriptions", "organizations_url": "https://api.github.com/users/lw394/orgs", "repos_url": "https://api.github.com/users/lw394/repos", "events_url": "https://api.github.com/users/lw394/events{/privacy}", "received_events_url": "https://api.github.com/users/lw394/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-14T15:03:01Z", "updated_at": "2017-03-14T15:03:01Z", "author_association": "NONE", "body_html": "<p>We primarily deal with time series data, and prefer to not have to batch preprocess the whole dataset prior to training every unique model input architecture.  In fact, the preprocessed dataset size for one input architecture variant can be easily an order of magnitude larger than the unprocessed file set size.</p>\n<p>We've been able to deal with this using the current system of queues,TF ops, batch_join/etc to enable multiple on-the-fly preprocessing threads with across-file example mixing.  I have to say it's really nice and flexible for hyperparameter tuning the input architecture, and I like that the entire pipeline lives in the graph, feeds data in response to sess.run(train_op) calls, and can be restored from a common checkpoint with the model.</p>\n<p>If you're planning to deprecate the current queues paradigm, I would like to know that the <code>Dataset</code> and <code>Iterator</code> would enable the same flexibility.  For my use case it seems like <code>Dataset</code> could represent a collection of time series, and the <code>Iterator</code> would behave like a python iterator/generator and could handle any preprocessing to form batches of examples?</p>", "body_text": "We primarily deal with time series data, and prefer to not have to batch preprocess the whole dataset prior to training every unique model input architecture.  In fact, the preprocessed dataset size for one input architecture variant can be easily an order of magnitude larger than the unprocessed file set size.\nWe've been able to deal with this using the current system of queues,TF ops, batch_join/etc to enable multiple on-the-fly preprocessing threads with across-file example mixing.  I have to say it's really nice and flexible for hyperparameter tuning the input architecture, and I like that the entire pipeline lives in the graph, feeds data in response to sess.run(train_op) calls, and can be restored from a common checkpoint with the model.\nIf you're planning to deprecate the current queues paradigm, I would like to know that the Dataset and Iterator would enable the same flexibility.  For my use case it seems like Dataset could represent a collection of time series, and the Iterator would behave like a python iterator/generator and could handle any preprocessing to form batches of examples?", "body": "We primarily deal with time series data, and prefer to not have to batch preprocess the whole dataset prior to training every unique model input architecture.  In fact, the preprocessed dataset size for one input architecture variant can be easily an order of magnitude larger than the unprocessed file set size.  \r\n\r\nWe've been able to deal with this using the current system of queues,TF ops, batch_join/etc to enable multiple on-the-fly preprocessing threads with across-file example mixing.  I have to say it's really nice and flexible for hyperparameter tuning the input architecture, and I like that the entire pipeline lives in the graph, feeds data in response to sess.run(train_op) calls, and can be restored from a common checkpoint with the model.  \r\n\r\nIf you're planning to deprecate the current queues paradigm, I would like to know that the `Dataset` and `Iterator` would enable the same flexibility.  For my use case it seems like `Dataset` could represent a collection of time series, and the `Iterator` would behave like a python iterator/generator and could handle any preprocessing to form batches of examples?\r\n     "}