{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285115251", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-285115251", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 285115251, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTExNTI1MQ==", "user": {"login": "MicaelCarvalho", "id": 17184992, "node_id": "MDQ6VXNlcjE3MTg0OTky", "avatar_url": "https://avatars3.githubusercontent.com/u/17184992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MicaelCarvalho", "html_url": "https://github.com/MicaelCarvalho", "followers_url": "https://api.github.com/users/MicaelCarvalho/followers", "following_url": "https://api.github.com/users/MicaelCarvalho/following{/other_user}", "gists_url": "https://api.github.com/users/MicaelCarvalho/gists{/gist_id}", "starred_url": "https://api.github.com/users/MicaelCarvalho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MicaelCarvalho/subscriptions", "organizations_url": "https://api.github.com/users/MicaelCarvalho/orgs", "repos_url": "https://api.github.com/users/MicaelCarvalho/repos", "events_url": "https://api.github.com/users/MicaelCarvalho/events{/privacy}", "received_events_url": "https://api.github.com/users/MicaelCarvalho/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-08T17:51:45Z", "updated_at": "2017-03-08T17:51:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't know TF as well as others here, so please take my comments with some skepticism:</p>\n<ul>\n<li>\n<p>With <code>tf.py_func</code> I was able to solve most of my input-related problems, like loading .mat files in a symbolic-ish manner. The one I'm currently struggling with is the integration of <code>tf.train.batch</code> with the ability of picking the source from which the input should come, for having train/val data in the same symbolic variable. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"212435463\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8168\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8168/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/8168\">#8168</a></p>\n<p>I understand these functions were initially thought for simple use cases, but it would be nice to have more control of the pipeline without the burden of managing <em>everything</em> (e.g. using <code>tf.QueueBase.from_list</code> but being forced to feed queues and manage threads kind of manually).</p>\n</li>\n<li>\n<p>I'm not sure if TensorFlow optimizes the dequeue operation under the hood but, if not, I think we could greatly benefit from a parallel dequeue operation that charges data (i.e. next batch) into the GPU memory while it processes the previous data (i.e. current batch).</p>\n</li>\n<li>\n<p>I think <code>feed_dict</code>-like solutions are not optimal for passing big chunks of data to the train function, like a batch of images, since they're basically a pause in the execution graph to force TF to interact with <em>python's runtime</em>. An in-graph solution sounds better, with pointers to guide the graph execution, like <code>feed_dict={is_training = True}</code> to indicate the input should come from the training pipeline, the model should set batchnorm and dropout (et al) to train mode etc. This way, TF could better optimize/parallelize the execution, and all solutions would scale.</p>\n</li>\n<li>\n<p>The standard functions for creating batches apparently do not provide an index to indicate which batch we are processing. For example, a <code>slice_input_producer</code> receives the number of epochs to be generated but there seems to be no way of knowing the epoch of one sample without counting how many we have already evaluated.</p>\n</li>\n</ul>", "body_text": "I don't know TF as well as others here, so please take my comments with some skepticism:\n\n\nWith tf.py_func I was able to solve most of my input-related problems, like loading .mat files in a symbolic-ish manner. The one I'm currently struggling with is the integration of tf.train.batch with the ability of picking the source from which the input should come, for having train/val data in the same symbolic variable. #8168\nI understand these functions were initially thought for simple use cases, but it would be nice to have more control of the pipeline without the burden of managing everything (e.g. using tf.QueueBase.from_list but being forced to feed queues and manage threads kind of manually).\n\n\nI'm not sure if TensorFlow optimizes the dequeue operation under the hood but, if not, I think we could greatly benefit from a parallel dequeue operation that charges data (i.e. next batch) into the GPU memory while it processes the previous data (i.e. current batch).\n\n\nI think feed_dict-like solutions are not optimal for passing big chunks of data to the train function, like a batch of images, since they're basically a pause in the execution graph to force TF to interact with python's runtime. An in-graph solution sounds better, with pointers to guide the graph execution, like feed_dict={is_training = True} to indicate the input should come from the training pipeline, the model should set batchnorm and dropout (et al) to train mode etc. This way, TF could better optimize/parallelize the execution, and all solutions would scale.\n\n\nThe standard functions for creating batches apparently do not provide an index to indicate which batch we are processing. For example, a slice_input_producer receives the number of epochs to be generated but there seems to be no way of knowing the epoch of one sample without counting how many we have already evaluated.", "body": "I don't know TF as well as others here, so please take my comments with some skepticism:\r\n\r\n- With `tf.py_func` I was able to solve most of my input-related problems, like loading .mat files in a symbolic-ish manner. The one I'm currently struggling with is the integration of `tf.train.batch` with the ability of picking the source from which the input should come, for having train/val data in the same symbolic variable. #8168 \r\n\r\n  I understand these functions were initially thought for simple use cases, but it would be nice to have more control of the pipeline without the burden of managing _everything_ (e.g. using `tf.QueueBase.from_list` but being forced to feed queues and manage threads kind of manually).\r\n\r\n- I'm not sure if TensorFlow optimizes the dequeue operation under the hood but, if not, I think we could greatly benefit from a parallel dequeue operation that charges data (i.e. next batch) into the GPU memory while it processes the previous data (i.e. current batch).\r\n\r\n- I think `feed_dict`-like solutions are not optimal for passing big chunks of data to the train function, like a batch of images, since they're basically a pause in the execution graph to force TF to interact with _python's runtime_. An in-graph solution sounds better, with pointers to guide the graph execution, like `feed_dict={is_training = True}` to indicate the input should come from the training pipeline, the model should set batchnorm and dropout (et al) to train mode etc. This way, TF could better optimize/parallelize the execution, and all solutions would scale.\r\n\r\n- The standard functions for creating batches apparently do not provide an index to indicate which batch we are processing. For example, a `slice_input_producer` receives the number of epochs to be generated but there seems to be no way of knowing the epoch of one sample without counting how many we have already evaluated."}