{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/326098305", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-326098305", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 326098305, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjA5ODMwNQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-30T19:45:09Z", "updated_at": "2017-08-30T19:45:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This issue thread is becoming a bit unwieldy and it's getting hard to keep track of the individual discussions, so I'm going to lock it after responding to a few of the recent comments. Please feel free to open a new issue about any specific topics of feature requests related to <code>tf.contrib.data</code> and we can continue the discussion there.</p>\n<p>In response to a few recent questions:</p>\n<ul>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4441724\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/GPhilo\">@GPhilo</a> (<a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-325698349\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">link</a>) and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13069767\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kratzert\">@kratzert</a> (<a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308845375\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">link</a>): The Dataset API includes methods for prefetching, so it shouldn't be necessary to add a queue here, and you can retain the other advantages of Datasets (like reinitialization etc.). Passing <code>output_buffer_size=100 * FLAGS.batch_size</code> to the <code>dataset.map()</code> call, and following that with <code>dataset.batch(FLAGS.batch_size)</code> will run your <code>preprocess_image</code> function in parallel and should decently increase the performance.</p>\n<div class=\"highlight highlight-source-python\"><pre>dataset <span class=\"pl-k\">=</span> tf.contrib.data.Dataset.list_files(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span>/*/*.jpg<span class=\"pl-pds\">'</span></span>.format(<span class=\"pl-c1\">FLAGS</span>.dataset_dir))\ndataset <span class=\"pl-k\">=</span> dataset.map(preprocess_image, <span class=\"pl-v\">num_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.num_threads,\n                      <span class=\"pl-v\">output_buffer_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">FLAGS</span>.batch_size)\ndataset <span class=\"pl-k\">=</span> datsaet.batch(<span class=\"pl-c1\">FLAGS</span>.batch_size)\niterator <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator()\nfilenames, images <span class=\"pl-k\">=</span> iterator.get_next()</pre></div>\n<p>Note that in TF 1.4 there will be a <code>Dataset.prefetch()</code> method that makes it easier to add prefetching at any point in the pipeline, not just after a <code>map()</code>. (You can try it by downloading the current nightly build.)</p>\n<p>In reponse to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13069767\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kratzert\">@kratzert</a>'s <a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308845375\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">specific question</a> about the implementation, the <code>Dataset</code> and <code>Iterator</code> classes don't use TensorFlow's previous producer/consumer queues (such as <code>tf.FIFOQueue</code> or <code>tf.RandomShuffleQueue</code>), but they do include simpler (and more efficient) implementations of the core ideas. For example, <code>Dataset.prefetch()</code> will start a background thread to populate a ordered buffer that <em>acts like</em> a <code>tf.FIFOQueue</code>, so that downstream pipeline stages need not block. However, the <a href=\"https://github.com/tensorflow/tensorflow/blob/d236d19f7753ae23f91d794701efa70ace1629da/tensorflow/core/kernels/prefetch_dataset_op.cc\"><code>prefetch()</code> implementation</a> is much simpler, because it doesn't need to support as many different concurrent operations as a <code>tf.FIFOQueue</code>.</p>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4680197\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vvekic\">@vvekic</a> (<a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-325357396\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">link</a>): I'd be curious to see your code before and after trying the Dataset API, and perhaps you could follow up by opening an issue describing the performance bottleneck. Compared to feeding or a (non-<code>StagingArea</code>) queue-based pipeline, the new API should be more efficient, and I'd be curious to know which parts aren't!</p>\n<p>At present, you're correct that the <code>StagingArea</code> functionality is not included in the Dataset API, and for peak performance in GPU workloads you will need to add a staging area manually. However, we are actively working on implementing Datasets that can span devices (see <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/19a55725af8102d72d4e081c5139f0e4bd5a4bb7/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/19a55725af8102d72d4e081c5139f0e4bd5a4bb7\"><tt>19a5572</tt></a> for some of the work in progress) and one of the first use cases for that is to support prefetching into GPU memory.</p>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15181821\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tengerye\">@tengerye</a> (<a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-323665406\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">link</a>): For dynamically feeding data into a Dataset, I'd suggest you try out the <code>Dataset.from_generator()</code> method that we're adding to TF 1.4 (and which is available in nightly builds already). I answered <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=59132\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albertz\">@albertz</a>'s Stack Overflow question about doing this <a href=\"https://stackoverflow.com/a/45928467/3574081\" rel=\"nofollow\">here</a>. (Supporting distributed pipelines will depend on the cross-device Dataset support that I mentioned in the last answer, and we'll be implementing that soon.) I think this will also work for <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=206013\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rasmusbergpalm\">@rasmusbergpalm</a>'s <a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-322407451\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">request</a>, because you can create concurrent generators, and for <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=966348\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tillahoffmann\">@tillahoffmann</a>'s <a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321976369\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">request</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4741099\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sirfz\">@sirfz</a>'s <a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321091444\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">request</a> as well. This API is very new though, so if you have any feedback, please let us know!</p>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1836763\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jasonkriss\">@jasonkriss</a> (<a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-303602263\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">link</a>) We've implemented something called \"feedable\" iterators, which let you switch the input for single graph between multiple iterators (e.g. one for training and one for testing). The programmers' guide has <a href=\"https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\" rel=\"nofollow\">more details</a> about how to use this feature.</p>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4805513\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/guillaumekln\">@guillaumekln</a> (<a href=\"https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308789560\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7951/hovercard\">link</a>) If you want to batch sequences with different lengths, you can use the <code>Dataset.group_by_window()</code> transformation. Have a look at <a href=\"https://github.com/tensorflow/nmt/blob/04c8c04a8b4e805f3d0a9c42b4d17c85f1324c55/nmt/utils/iterator_utils.py#L194\">how this is used in the NMT model code</a> for an example.</p>\n</li>\n</ul>\n<p>Thanks again to all of you for your continued interest in this part of TensorFlow!</p>", "body_text": "This issue thread is becoming a bit unwieldy and it's getting hard to keep track of the individual discussions, so I'm going to lock it after responding to a few of the recent comments. Please feel free to open a new issue about any specific topics of feature requests related to tf.contrib.data and we can continue the discussion there.\nIn response to a few recent questions:\n\n\n@GPhilo (link) and @kratzert (link): The Dataset API includes methods for prefetching, so it shouldn't be necessary to add a queue here, and you can retain the other advantages of Datasets (like reinitialization etc.). Passing output_buffer_size=100 * FLAGS.batch_size to the dataset.map() call, and following that with dataset.batch(FLAGS.batch_size) will run your preprocess_image function in parallel and should decently increase the performance.\ndataset = tf.contrib.data.Dataset.list_files('{}/*/*.jpg'.format(FLAGS.dataset_dir))\ndataset = dataset.map(preprocess_image, num_threads=FLAGS.num_threads,\n                      output_buffer_size=100*FLAGS.batch_size)\ndataset = datsaet.batch(FLAGS.batch_size)\niterator = dataset.make_one_shot_iterator()\nfilenames, images = iterator.get_next()\nNote that in TF 1.4 there will be a Dataset.prefetch() method that makes it easier to add prefetching at any point in the pipeline, not just after a map(). (You can try it by downloading the current nightly build.)\nIn reponse to @kratzert's specific question about the implementation, the Dataset and Iterator classes don't use TensorFlow's previous producer/consumer queues (such as tf.FIFOQueue or tf.RandomShuffleQueue), but they do include simpler (and more efficient) implementations of the core ideas. For example, Dataset.prefetch() will start a background thread to populate a ordered buffer that acts like a tf.FIFOQueue, so that downstream pipeline stages need not block. However, the prefetch() implementation is much simpler, because it doesn't need to support as many different concurrent operations as a tf.FIFOQueue.\n\n\n@vvekic (link): I'd be curious to see your code before and after trying the Dataset API, and perhaps you could follow up by opening an issue describing the performance bottleneck. Compared to feeding or a (non-StagingArea) queue-based pipeline, the new API should be more efficient, and I'd be curious to know which parts aren't!\nAt present, you're correct that the StagingArea functionality is not included in the Dataset API, and for peak performance in GPU workloads you will need to add a staging area manually. However, we are actively working on implementing Datasets that can span devices (see 19a5572 for some of the work in progress) and one of the first use cases for that is to support prefetching into GPU memory.\n\n\n@tengerye (link): For dynamically feeding data into a Dataset, I'd suggest you try out the Dataset.from_generator() method that we're adding to TF 1.4 (and which is available in nightly builds already). I answered @albertz's Stack Overflow question about doing this here. (Supporting distributed pipelines will depend on the cross-device Dataset support that I mentioned in the last answer, and we'll be implementing that soon.) I think this will also work for @rasmusbergpalm's request, because you can create concurrent generators, and for @tillahoffmann's request and @sirfz's request as well. This API is very new though, so if you have any feedback, please let us know!\n\n\n@jasonkriss (link) We've implemented something called \"feedable\" iterators, which let you switch the input for single graph between multiple iterators (e.g. one for training and one for testing). The programmers' guide has more details about how to use this feature.\n\n\n@guillaumekln (link) If you want to batch sequences with different lengths, you can use the Dataset.group_by_window() transformation. Have a look at how this is used in the NMT model code for an example.\n\n\nThanks again to all of you for your continued interest in this part of TensorFlow!", "body": "This issue thread is becoming a bit unwieldy and it's getting hard to keep track of the individual discussions, so I'm going to lock it after responding to a few of the recent comments. Please feel free to open a new issue about any specific topics of feature requests related to `tf.contrib.data` and we can continue the discussion there.\r\n\r\nIn response to a few recent questions:\r\n\r\n* @GPhilo ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-325698349)) and @kratzert ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308845375)): The Dataset API includes methods for prefetching, so it shouldn't be necessary to add a queue here, and you can retain the other advantages of Datasets (like reinitialization etc.). Passing `output_buffer_size=100 * FLAGS.batch_size` to the `dataset.map()` call, and following that with `dataset.batch(FLAGS.batch_size)` will run your `preprocess_image` function in parallel and should decently increase the performance.\r\n\r\n  ```python\r\n  dataset = tf.contrib.data.Dataset.list_files('{}/*/*.jpg'.format(FLAGS.dataset_dir))\r\n  dataset = dataset.map(preprocess_image, num_threads=FLAGS.num_threads,\r\n                        output_buffer_size=100*FLAGS.batch_size)\r\n  dataset = datsaet.batch(FLAGS.batch_size)\r\n  iterator = dataset.make_one_shot_iterator()\r\n  filenames, images = iterator.get_next()\r\n  ```\r\n\r\n  Note that in TF 1.4 there will be a `Dataset.prefetch()` method that makes it easier to add prefetching at any point in the pipeline, not just after a `map()`. (You can try it by downloading the current nightly build.)\r\n\r\n  In reponse to @kratzert's [specific question](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308845375) about the implementation, the `Dataset` and `Iterator` classes don't use TensorFlow's previous producer/consumer queues (such as `tf.FIFOQueue` or `tf.RandomShuffleQueue`), but they do include simpler (and more efficient) implementations of the core ideas. For example, `Dataset.prefetch()` will start a background thread to populate a ordered buffer that *acts like* a `tf.FIFOQueue`, so that downstream pipeline stages need not block. However, the [`prefetch()` implementation](https://github.com/tensorflow/tensorflow/blob/d236d19f7753ae23f91d794701efa70ace1629da/tensorflow/core/kernels/prefetch_dataset_op.cc) is much simpler, because it doesn't need to support as many different concurrent operations as a `tf.FIFOQueue`.\r\n\r\n* @vvekic ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-325357396)): I'd be curious to see your code before and after trying the Dataset API, and perhaps you could follow up by opening an issue describing the performance bottleneck. Compared to feeding or a (non-`StagingArea`) queue-based pipeline, the new API should be more efficient, and I'd be curious to know which parts aren't! \r\n\r\n  At present, you're correct that the `StagingArea` functionality is not included in the Dataset API, and for peak performance in GPU workloads you will need to add a staging area manually. However, we are actively working on implementing Datasets that can span devices (see 19a55725af8102d72d4e081c5139f0e4bd5a4bb7 for some of the work in progress) and one of the first use cases for that is to support prefetching into GPU memory.\r\n\r\n* @tengerye ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-323665406)): For dynamically feeding data into a Dataset, I'd suggest you try out the `Dataset.from_generator()` method that we're adding to TF 1.4 (and which is available in nightly builds already). I answered @albertz's Stack Overflow question about doing this [here](https://stackoverflow.com/a/45928467/3574081). (Supporting distributed pipelines will depend on the cross-device Dataset support that I mentioned in the last answer, and we'll be implementing that soon.) I think this will also work for @rasmusbergpalm's [request](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-322407451), because you can create concurrent generators, and for @tillahoffmann's [request](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321976369) and @sirfz's [request](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321091444) as well. This API is very new though, so if you have any feedback, please let us know!\r\n\r\n* @jasonkriss ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-303602263)) We've implemented something called \"feedable\" iterators, which let you switch the input for single graph between multiple iterators (e.g. one for training and one for testing). The programmers' guide has [more details](https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator) about how to use this feature.\r\n\r\n* @guillaumekln ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308789560)) If you want to batch sequences with different lengths, you can use the `Dataset.group_by_window()` transformation. Have a look at [how this is used in the NMT model code](https://github.com/tensorflow/nmt/blob/04c8c04a8b4e805f3d0a9c42b4d17c85f1324c55/nmt/utils/iterator_utils.py#L194) for an example.\r\n\r\nThanks again to all of you for your continued interest in this part of TensorFlow!"}