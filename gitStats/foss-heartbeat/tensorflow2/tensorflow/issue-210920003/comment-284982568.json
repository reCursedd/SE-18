{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284982568", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-284982568", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 284982568, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDk4MjU2OA==", "user": {"login": "nicolasdespres", "id": 614631, "node_id": "MDQ6VXNlcjYxNDYzMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/614631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nicolasdespres", "html_url": "https://github.com/nicolasdespres", "followers_url": "https://api.github.com/users/nicolasdespres/followers", "following_url": "https://api.github.com/users/nicolasdespres/following{/other_user}", "gists_url": "https://api.github.com/users/nicolasdespres/gists{/gist_id}", "starred_url": "https://api.github.com/users/nicolasdespres/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nicolasdespres/subscriptions", "organizations_url": "https://api.github.com/users/nicolasdespres/orgs", "repos_url": "https://api.github.com/users/nicolasdespres/repos", "events_url": "https://api.github.com/users/nicolasdespres/events{/privacy}", "received_events_url": "https://api.github.com/users/nicolasdespres/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-08T08:48:49Z", "updated_at": "2017-03-08T08:48:49Z", "author_association": "NONE", "body_html": "<p>I am glad to see this initiative. The input pipeline is definitely the<br>\nsteepest part of the learning curve.</p>\n<p>I'd like:</p>\n<ul>\n<li>A unified API to manage both in-memory (<code>feed_dict</code>) dataset and large one so that the same code scale and your model only have to talk to one API. Although, I have not use it yet, I liked what I read in the input pipeline documentation.</li>\n<li>More iterators! They are great. Asynchronous iterators would be even better (see <a href=\"https://www.python.org/dev/peps/pep-0492/\" rel=\"nofollow\">PEP492</a>). Iterators implementing <code>__len__</code> are great for progress report.</li>\n<li>multiprocessing rather than threading</li>\n<li>Please no <code>Dataset</code> class because, IMHO, the \"dataset\" concept is ill-defined. The <code>Dataset</code> class described in the original post already exists in Python: it is a list of tuples. And what is a \"dataset\", anyway? A collection of train/valid/test data or simply a collection of data? Is it just a file? directory? generator? Are each data item (input/target) couple? Is that always true? Is the dictionary part of the text dataset?<br>\nThe choice of the data container is driven by a lot of constrains depending on its size and the execution environment. Instead of a <code>Dataset</code> container, I would prefer to have a rich set of containers offering different trade-off with respect to memory/time complexity. In addition, I would like to have a rich set of iterators, splitters, loaders, dumpers, slicers, repeaters, servers, generators to actually work with data coming from various source.</li>\n<li>The epoch concept does not have a clear semantic either. In my experience, it is best defined by <code>epoch = global_step / steps_per_epoch</code> and <code>steps_per_epoch = dataset_size / batch_size</code>.</li>\n</ul>\n<p>Here my attempt to translate to small in-memory dataset some of the routines available in the TF's input pipeline for large dataset. Here some examples of what I would like to see available in TensorFlow:</p>\n<ul>\n<li><a href=\"https://gist.github.com/nicolasdespres/fed915518cfcdb5c4dad73409eec77ea\">cycle_range</a></li>\n<li><a href=\"https://gist.github.com/nicolasdespres/e07aa951ba62aeaea60e740238571664\">shuffle_iter</a></li>\n<li><a href=\"https://gist.github.com/nicolasdespres/bbc62cb43f1ffe9b81d971cb957650c2\">batch_iter</a></li>\n<li><a href=\"https://gist.github.com/nicolasdespres/d380cc43ebb1eb2d780b816fde294869\">iter_shuffle_batch_range</a></li>\n<li><a href=\"https://gist.github.com/nicolasdespres/418279e152dca606a78584fc99630738\">iter_tensors_slice</a></li>\n<li><a href=\"https://gist.github.com/nicolasdespres/04dd05d0a8ebf66205a687f1706748dc\">iter_shuffle_batch_tensors</a></li>\n<li><a href=\"https://gist.github.com/nicolasdespres/81689421f56b86a315a81f19d301508a\">iter_shuffle_batch_window</a></li>\n</ul>\n<p>These routines demonstrate how far you can go with just simple iterators over list of indices.</p>", "body_text": "I am glad to see this initiative. The input pipeline is definitely the\nsteepest part of the learning curve.\nI'd like:\n\nA unified API to manage both in-memory (feed_dict) dataset and large one so that the same code scale and your model only have to talk to one API. Although, I have not use it yet, I liked what I read in the input pipeline documentation.\nMore iterators! They are great. Asynchronous iterators would be even better (see PEP492). Iterators implementing __len__ are great for progress report.\nmultiprocessing rather than threading\nPlease no Dataset class because, IMHO, the \"dataset\" concept is ill-defined. The Dataset class described in the original post already exists in Python: it is a list of tuples. And what is a \"dataset\", anyway? A collection of train/valid/test data or simply a collection of data? Is it just a file? directory? generator? Are each data item (input/target) couple? Is that always true? Is the dictionary part of the text dataset?\nThe choice of the data container is driven by a lot of constrains depending on its size and the execution environment. Instead of a Dataset container, I would prefer to have a rich set of containers offering different trade-off with respect to memory/time complexity. In addition, I would like to have a rich set of iterators, splitters, loaders, dumpers, slicers, repeaters, servers, generators to actually work with data coming from various source.\nThe epoch concept does not have a clear semantic either. In my experience, it is best defined by epoch = global_step / steps_per_epoch and steps_per_epoch = dataset_size / batch_size.\n\nHere my attempt to translate to small in-memory dataset some of the routines available in the TF's input pipeline for large dataset. Here some examples of what I would like to see available in TensorFlow:\n\ncycle_range\nshuffle_iter\nbatch_iter\niter_shuffle_batch_range\niter_tensors_slice\niter_shuffle_batch_tensors\niter_shuffle_batch_window\n\nThese routines demonstrate how far you can go with just simple iterators over list of indices.", "body": "I am glad to see this initiative. The input pipeline is definitely the\r\nsteepest part of the learning curve.\r\n\r\nI'd like:\r\n* A unified API to manage both in-memory (`feed_dict`) dataset and large one so that the same code scale and your model only have to talk to one API. Although, I have not use it yet, I liked what I read in the input pipeline documentation.\r\n* More iterators! They are great. Asynchronous iterators would be even better (see [PEP492](https://www.python.org/dev/peps/pep-0492/)). Iterators implementing `__len__` are great for progress report.\r\n* multiprocessing rather than threading\r\n* Please no `Dataset` class because, IMHO, the \"dataset\" concept is ill-defined. The `Dataset` class described in the original post already exists in Python: it is a list of tuples. And what is a \"dataset\", anyway? A collection of train/valid/test data or simply a collection of data? Is it just a file? directory? generator? Are each data item (input/target) couple? Is that always true? Is the dictionary part of the text dataset?\r\nThe choice of the data container is driven by a lot of constrains depending on its size and the execution environment. Instead of a `Dataset` container, I would prefer to have a rich set of containers offering different trade-off with respect to memory/time complexity. In addition, I would like to have a rich set of iterators, splitters, loaders, dumpers, slicers, repeaters, servers, generators to actually work with data coming from various source.\r\n* The epoch concept does not have a clear semantic either. In my experience, it is best defined by `epoch = global_step / steps_per_epoch` and `steps_per_epoch = dataset_size / batch_size`.\r\n\r\nHere my attempt to translate to small in-memory dataset some of the routines available in the TF's input pipeline for large dataset. Here some examples of what I would like to see available in TensorFlow:\r\n* [cycle_range](https://gist.github.com/nicolasdespres/fed915518cfcdb5c4dad73409eec77ea)\r\n* [shuffle_iter](https://gist.github.com/nicolasdespres/e07aa951ba62aeaea60e740238571664)\r\n* [batch_iter](https://gist.github.com/nicolasdespres/bbc62cb43f1ffe9b81d971cb957650c2)\r\n* [iter_shuffle_batch_range](https://gist.github.com/nicolasdespres/d380cc43ebb1eb2d780b816fde294869)\r\n* [iter_tensors_slice](https://gist.github.com/nicolasdespres/418279e152dca606a78584fc99630738)\r\n* [iter_shuffle_batch_tensors](https://gist.github.com/nicolasdespres/04dd05d0a8ebf66205a687f1706748dc)\r\n* [iter_shuffle_batch_window](https://gist.github.com/nicolasdespres/81689421f56b86a315a81f19d301508a)\r\n\r\nThese routines demonstrate how far you can go with just simple iterators over list of indices.\r\n"}