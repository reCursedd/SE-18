{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303546037", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-303546037", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 303546037, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzU0NjAzNw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-23T22:05:26Z", "updated_at": "2017-05-23T22:33:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've had my head down for a while, so there's lots to respond to here:</p>\n<ul>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3530212\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sjperkins\">@sjperkins</a>: Adding <code>Dataset.chain()</code> and <code>Dataset.product()</code> iterators shouldn't be too hard. I'd like to understand your use case a little better. Do you imagine that most uses will combine exactly two datasets (and hence we might use method chaining to combine them, e.g. <code>ds1.chain(ds2)</code>, <code>ds1.product(ds2)</code>) or will it be more common to combine more datasets (and hence we'd take a similar approach to <code>Dataset.zip()</code>, e.g. <code>Dataset.chain([ds1, ds2])</code>, <code>Dataset.product([ds1, ds2])</code>)? Also note that, if you need <code>product()</code> in the short term, I think you can write <code>ds1.flat_map(lambda x: tf.contrib.data.Dataset.zip((tf.contrib.data.Dataset.from_tensors(x).repeat(), ds2)))</code>. You could also fake out <code>chain()</code> with <code>Dataset.flat_map()</code> and <code>tf.cond()</code> but that would be quite ugly :).</p>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=856316\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/snnn\">@snnn</a>: Thanks for kicking the tires! It's great to hear that the new API brought a speedup for your code... we've definitely favored flexibility over performance with the initial version of the API, but look out for improvements over the coming versions.</p>\n<p>The idea of supporting seekable file formats is very appealing, and we're figuring out a good API for that. Right now you can use <code>Dataset.skip()</code> and <code>Dataset.take()</code> to select a sub-dataset from the files, but it is not very efficient, because they materialize the skipped-over inputs before discarding them. I could imagine adding an <code>Iterator::Seek(size_t n)</code> method internally, which would allow iterators to specialize their behavior in this case (or fall back to using <code>GetNext()</code> in a loop). This also seems like it would be important for checkpointing iterators, which might be useful for fault tolerance.</p>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13662086\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/omoindrot\">@omoindrot</a>: I agree that the <code>try-except</code> construction is pretty ugly, and we should try to find ways to improve it. The current version is designed to be a drop-in replacement for the queues, which use <code>tf.errors.OutOfRangeError</code> to signal completion, and various other classes are designed to catch that exception. Exposing an <code>iterator.is_empty</code> property would be possible, but it would be tricky to make it work in the way you suggest, because (to avoid an exception being raised) you'd need to guard the training subgraph with a <code>tf.cond(iterator.is_empty, make_train_op, lambda: tf.no_op())</code>. Another possibility would be to change the Python API for iterators to use two ops: <code>Iterator.move_next()</code> and <code>Iterator.get_current()</code> (e.g. like the C++ <code>IEnumerator</code> protocol), but that would introduce an additional <code>sess.run()</code> call, and make it harder to share the iterator between threads.</p>\n<p>One possibility I've considered is to create a wrapper that turns an <code>Iterator</code>-consuming step into a Python iterator. e.g. Some straw-man code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">iterate_step</span>(<span class=\"pl-smi\">sess</span>, <span class=\"pl-smi\">fetches</span>):\n  cached_step <span class=\"pl-k\">=</span> sess.make_callable(fetches)\n  <span class=\"pl-k\">try</span>:\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n      <span class=\"pl-k\">yield</span> cached_step.run()\n  <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n    <span class=\"pl-k\">pass</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ...</span>\n<span class=\"pl-k\">for</span> _, step, loss <span class=\"pl-k\">in</span> iterate_step(sess, [train_op, global_step, loss]):\n  <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run periodic eval, e.g.</span></pre></div>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=226553\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jimfleming\">@jimfleming</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4926264\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vonclites\">@vonclites</a>: Thanks for looking into the <code>MonitoredSession</code> integration. It's great to hear that it \"just works\"! I think we'll still need to do something better for the more advanced cases where we might want to reinitialize an iterator in the same session.</p>\n</li>\n<li>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=55744\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ahundt\">@ahundt</a>: From our initial experiments, the <em>peak</em> performance of the benchmark input pipeline is still slightly higher than what you get from using the <code>tf.contrib.data</code> API. However, the peak performance is much higher than the throughput of actually using the data to train a model like Inception or ResNet, so you might not notice the difference in regular use. We're investigating how to close the gap, and it's very likely that we'll incorporate some of the ideas from the benchmark code into the <code>Dataset</code> implementation.</p>\n<p>In particular, one current limitation of the <code>Dataset</code> and <code>Iterator</code> implementation is that the entire pipeline runs on a single device, whereas the more explicit code in the benchmarks is able to split the processing across multiple CPU and GPU devices. For the best performance, it's going to be important to integrate optimizations like the <code>StagingArea</code> for prefetching data to the GPU before it is needed, and we're working on a way to do that more transparently. For now, you can manually pipeline the output of an <code>Iterator.get_next()</code> op with the <code>StagingArea.put()</code> op in a similar manner to the benchmark code.</p>\n</li>\n</ul>", "body_text": "I've had my head down for a while, so there's lots to respond to here:\n\n\n@sjperkins: Adding Dataset.chain() and Dataset.product() iterators shouldn't be too hard. I'd like to understand your use case a little better. Do you imagine that most uses will combine exactly two datasets (and hence we might use method chaining to combine them, e.g. ds1.chain(ds2), ds1.product(ds2)) or will it be more common to combine more datasets (and hence we'd take a similar approach to Dataset.zip(), e.g. Dataset.chain([ds1, ds2]), Dataset.product([ds1, ds2]))? Also note that, if you need product() in the short term, I think you can write ds1.flat_map(lambda x: tf.contrib.data.Dataset.zip((tf.contrib.data.Dataset.from_tensors(x).repeat(), ds2))). You could also fake out chain() with Dataset.flat_map() and tf.cond() but that would be quite ugly :).\n\n\n@snnn: Thanks for kicking the tires! It's great to hear that the new API brought a speedup for your code... we've definitely favored flexibility over performance with the initial version of the API, but look out for improvements over the coming versions.\nThe idea of supporting seekable file formats is very appealing, and we're figuring out a good API for that. Right now you can use Dataset.skip() and Dataset.take() to select a sub-dataset from the files, but it is not very efficient, because they materialize the skipped-over inputs before discarding them. I could imagine adding an Iterator::Seek(size_t n) method internally, which would allow iterators to specialize their behavior in this case (or fall back to using GetNext() in a loop). This also seems like it would be important for checkpointing iterators, which might be useful for fault tolerance.\n\n\n@omoindrot: I agree that the try-except construction is pretty ugly, and we should try to find ways to improve it. The current version is designed to be a drop-in replacement for the queues, which use tf.errors.OutOfRangeError to signal completion, and various other classes are designed to catch that exception. Exposing an iterator.is_empty property would be possible, but it would be tricky to make it work in the way you suggest, because (to avoid an exception being raised) you'd need to guard the training subgraph with a tf.cond(iterator.is_empty, make_train_op, lambda: tf.no_op()). Another possibility would be to change the Python API for iterators to use two ops: Iterator.move_next() and Iterator.get_current() (e.g. like the C++ IEnumerator protocol), but that would introduce an additional sess.run() call, and make it harder to share the iterator between threads.\nOne possibility I've considered is to create a wrapper that turns an Iterator-consuming step into a Python iterator. e.g. Some straw-man code:\ndef iterate_step(sess, fetches):\n  cached_step = sess.make_callable(fetches)\n  try:\n    while True:\n      yield cached_step.run()\n  except tf.errors.OutOfRangeError:\n    pass\n\n# ...\nfor _, step, loss in iterate_step(sess, [train_op, global_step, loss]):\n  if step % 100 == 0:\n    # Run periodic eval, e.g.\n\n\n@jimfleming and @vonclites: Thanks for looking into the MonitoredSession integration. It's great to hear that it \"just works\"! I think we'll still need to do something better for the more advanced cases where we might want to reinitialize an iterator in the same session.\n\n\n@ahundt: From our initial experiments, the peak performance of the benchmark input pipeline is still slightly higher than what you get from using the tf.contrib.data API. However, the peak performance is much higher than the throughput of actually using the data to train a model like Inception or ResNet, so you might not notice the difference in regular use. We're investigating how to close the gap, and it's very likely that we'll incorporate some of the ideas from the benchmark code into the Dataset implementation.\nIn particular, one current limitation of the Dataset and Iterator implementation is that the entire pipeline runs on a single device, whereas the more explicit code in the benchmarks is able to split the processing across multiple CPU and GPU devices. For the best performance, it's going to be important to integrate optimizations like the StagingArea for prefetching data to the GPU before it is needed, and we're working on a way to do that more transparently. For now, you can manually pipeline the output of an Iterator.get_next() op with the StagingArea.put() op in a similar manner to the benchmark code.", "body": "I've had my head down for a while, so there's lots to respond to here:\r\n\r\n* @sjperkins: Adding `Dataset.chain()` and `Dataset.product()` iterators shouldn't be too hard. I'd like to understand your use case a little better. Do you imagine that most uses will combine exactly two datasets (and hence we might use method chaining to combine them, e.g. `ds1.chain(ds2)`, `ds1.product(ds2)`) or will it be more common to combine more datasets (and hence we'd take a similar approach to `Dataset.zip()`, e.g. `Dataset.chain([ds1, ds2])`, `Dataset.product([ds1, ds2])`)? Also note that, if you need `product()` in the short term, I think you can write `ds1.flat_map(lambda x: tf.contrib.data.Dataset.zip((tf.contrib.data.Dataset.from_tensors(x).repeat(), ds2)))`. You could also fake out `chain()` with `Dataset.flat_map()` and `tf.cond()` but that would be quite ugly :).\r\n\r\n* @snnn: Thanks for kicking the tires! It's great to hear that the new API brought a speedup for your code... we've definitely favored flexibility over performance with the initial version of the API, but look out for improvements over the coming versions.\r\n\r\n   The idea of supporting seekable file formats is very appealing, and we're figuring out a good API for that. Right now you can use `Dataset.skip()` and `Dataset.take()` to select a sub-dataset from the files, but it is not very efficient, because they materialize the skipped-over inputs before discarding them. I could imagine adding an `Iterator::Seek(size_t n)` method internally, which would allow iterators to specialize their behavior in this case (or fall back to using `GetNext()` in a loop). This also seems like it would be important for checkpointing iterators, which might be useful for fault tolerance.\r\n\r\n* @omoindrot: I agree that the `try-except` construction is pretty ugly, and we should try to find ways to improve it. The current version is designed to be a drop-in replacement for the queues, which use `tf.errors.OutOfRangeError` to signal completion, and various other classes are designed to catch that exception. Exposing an `iterator.is_empty` property would be possible, but it would be tricky to make it work in the way you suggest, because (to avoid an exception being raised) you'd need to guard the training subgraph with a `tf.cond(iterator.is_empty, make_train_op, lambda: tf.no_op())`. Another possibility would be to change the Python API for iterators to use two ops: `Iterator.move_next()` and `Iterator.get_current()` (e.g. like the C++ `IEnumerator` protocol), but that would introduce an additional `sess.run()` call, and make it harder to share the iterator between threads.\r\n\r\n   One possibility I've considered is to create a wrapper that turns an `Iterator`-consuming step into a Python iterator. e.g. Some straw-man code:\r\n\r\n   ```python\r\n   def iterate_step(sess, fetches):\r\n     cached_step = sess.make_callable(fetches)\r\n     try:\r\n       while True:\r\n         yield cached_step.run()\r\n     except tf.errors.OutOfRangeError:\r\n       pass\r\n\r\n   # ...\r\n   for _, step, loss in iterate_step(sess, [train_op, global_step, loss]):\r\n     if step % 100 == 0:\r\n       # Run periodic eval, e.g.\r\n   ```\r\n\r\n* @jimfleming and @vonclites: Thanks for looking into the `MonitoredSession` integration. It's great to hear that it \"just works\"! I think we'll still need to do something better for the more advanced cases where we might want to reinitialize an iterator in the same session.\r\n\r\n* @ahundt: From our initial experiments, the *peak* performance of the benchmark input pipeline is still slightly higher than what you get from using the `tf.contrib.data` API. However, the peak performance is much higher than the throughput of actually using the data to train a model like Inception or ResNet, so you might not notice the difference in regular use. We're investigating how to close the gap, and it's very likely that we'll incorporate some of the ideas from the benchmark code into the `Dataset` implementation.\r\n\r\n   In particular, one current limitation of the `Dataset` and `Iterator` implementation is that the entire pipeline runs on a single device, whereas the more explicit code in the benchmarks is able to split the processing across multiple CPU and GPU devices. For the best performance, it's going to be important to integrate optimizations like the `StagingArea` for prefetching data to the GPU before it is needed, and we're working on a way to do that more transparently. For now, you can manually pipeline the output of an `Iterator.get_next()` op with the `StagingArea.put()` op in a similar manner to the benchmark code."}