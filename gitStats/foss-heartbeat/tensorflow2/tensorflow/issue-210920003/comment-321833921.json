{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321833921", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321833921", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 321833921, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTgzMzkyMQ==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T14:48:01Z", "updated_at": "2017-08-11T14:48:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> This is great work and definitely very useful for creating nice learning APIs on top of TensorFlow. However, I have a couple main concerns:</p>\n<ul>\n<li>I cannot see a way currently to \"unzip\" a dataset. Let's say we have a trainable model that has both a <code>train</code>/<code>fit</code> method and a <code>infer</code>/<code>predict</code> method. Let's call the type of the (potentially) nested structure of inputs to our model <code>I</code> and the type of training inputs, which are only needed when training (e.g., supervision labels), <code>TI</code>. In this case, we want the <code>train</code> method to accept datasets with elements of type <code>(I, TI)</code> (i.e., a tuple of <code>I</code> and <code>TI</code>) and the <code>predict</code> method to accept datasets with elements of type <code>I</code> or <code>(I, TI)</code> (in which case it would ignore the labels). We also want the model to only have one underlying graph, supporting all these types of input. The way I could see doing that was for the underlying model to construct two iterators (one with elements type <code>I</code> and one with type <code>TI</code>) and initialize them according to the provided datasets. However, if somebody provides a dataset with elements of type <code>(I, TI)</code> to the <code>train</code> method, there is no way to unzip this dataset and initialize both iterators. One has to use <code>Dataset.map</code> twice, which is not efficient (I think but please correct me if I'm wrong) and which may also not pull matching elements from the datasets (if each pull advances the current index in the original first dataset -- I'm not sure if that happens).</li>\n<li>It would be nice to support iterators over tensors defined in other languages as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4741099\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sirfz\">@sirfz</a> mentioned. I cannot see an efficient way to do that with the current API. Please correct me if I'm wrong but currently one would have to create a new <code>TensorDataset</code> per batch and re-initialize an existing iterator.<br>\nI think <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a> may be able to comment on my first point as currently my understanding is that they are thinking or creating an entirely new graph for training only, for such cases (third step described <a href=\"https://github.com/fchollet/keras/issues/7503\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/7503/hovercard\">here</a>).</li>\n</ul>\n<p>Also, if my description is terribly unclear, please let me know and I'll try to clarify.</p>", "body_text": "@mrry This is great work and definitely very useful for creating nice learning APIs on top of TensorFlow. However, I have a couple main concerns:\n\nI cannot see a way currently to \"unzip\" a dataset. Let's say we have a trainable model that has both a train/fit method and a infer/predict method. Let's call the type of the (potentially) nested structure of inputs to our model I and the type of training inputs, which are only needed when training (e.g., supervision labels), TI. In this case, we want the train method to accept datasets with elements of type (I, TI) (i.e., a tuple of I and TI) and the predict method to accept datasets with elements of type I or (I, TI) (in which case it would ignore the labels). We also want the model to only have one underlying graph, supporting all these types of input. The way I could see doing that was for the underlying model to construct two iterators (one with elements type I and one with type TI) and initialize them according to the provided datasets. However, if somebody provides a dataset with elements of type (I, TI) to the train method, there is no way to unzip this dataset and initialize both iterators. One has to use Dataset.map twice, which is not efficient (I think but please correct me if I'm wrong) and which may also not pull matching elements from the datasets (if each pull advances the current index in the original first dataset -- I'm not sure if that happens).\nIt would be nice to support iterators over tensors defined in other languages as @sirfz mentioned. I cannot see an efficient way to do that with the current API. Please correct me if I'm wrong but currently one would have to create a new TensorDataset per batch and re-initialize an existing iterator.\nI think @fchollet may be able to comment on my first point as currently my understanding is that they are thinking or creating an entirely new graph for training only, for such cases (third step described here).\n\nAlso, if my description is terribly unclear, please let me know and I'll try to clarify.", "body": "@mrry This is great work and definitely very useful for creating nice learning APIs on top of TensorFlow. However, I have a couple main concerns:\r\n- I cannot see a way currently to \"unzip\" a dataset. Let's say we have a trainable model that has both a `train`/`fit` method and a `infer`/`predict` method. Let's call the type of the (potentially) nested structure of inputs to our model `I` and the type of training inputs, which are only needed when training (e.g., supervision labels), `TI`. In this case, we want the `train` method to accept datasets with elements of type `(I, TI)` (i.e., a tuple of `I` and `TI`) and the `predict` method to accept datasets with elements of type `I` or `(I, TI)` (in which case it would ignore the labels). We also want the model to only have one underlying graph, supporting all these types of input. The way I could see doing that was for the underlying model to construct two iterators (one with elements type `I` and one with type `TI`) and initialize them according to the provided datasets. However, if somebody provides a dataset with elements of type `(I, TI)` to the `train` method, there is no way to unzip this dataset and initialize both iterators. One has to use `Dataset.map` twice, which is not efficient (I think but please correct me if I'm wrong) and which may also not pull matching elements from the datasets (if each pull advances the current index in the original first dataset -- I'm not sure if that happens).\r\n- It would be nice to support iterators over tensors defined in other languages as @sirfz mentioned. I cannot see an efficient way to do that with the current API. Please correct me if I'm wrong but currently one would have to create a new `TensorDataset` per batch and re-initialize an existing iterator.\r\nI think @fchollet may be able to comment on my first point as currently my understanding is that they are thinking or creating an entirely new graph for training only, for such cases (third step described [here](https://github.com/fchollet/keras/issues/7503)).\r\n\r\nAlso, if my description is terribly unclear, please let me know and I'll try to clarify."}