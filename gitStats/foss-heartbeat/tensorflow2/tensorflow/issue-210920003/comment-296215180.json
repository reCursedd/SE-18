{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296215180", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-296215180", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 296215180, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjIxNTE4MA==", "user": {"login": "carlthome", "id": 1595907, "node_id": "MDQ6VXNlcjE1OTU5MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1595907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlthome", "html_url": "https://github.com/carlthome", "followers_url": "https://api.github.com/users/carlthome/followers", "following_url": "https://api.github.com/users/carlthome/following{/other_user}", "gists_url": "https://api.github.com/users/carlthome/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlthome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlthome/subscriptions", "organizations_url": "https://api.github.com/users/carlthome/orgs", "repos_url": "https://api.github.com/users/carlthome/repos", "events_url": "https://api.github.com/users/carlthome/events{/privacy}", "received_events_url": "https://api.github.com/users/carlthome/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-21T15:00:34Z", "updated_at": "2017-04-21T15:00:34Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle.</p>\n</blockquote>\n<p>That also applies to <code>tf.train</code>. Data doesn't just appear in the graph without reading it from disk, regardless of whether you do it via Python or TensorFlow's execution engine.</p>\n<p>Anyway, I find memapped NumPy arrays via <a href=\"https://pythonhosted.org/joblib/memory.html\" rel=\"nofollow\">joblib.cache.Memory</a> and feed_dict to be quite performant (GPU load over 90% throughout a training session), despite the extra copy.</p>", "body_text": "The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle.\n\nThat also applies to tf.train. Data doesn't just appear in the graph without reading it from disk, regardless of whether you do it via Python or TensorFlow's execution engine.\nAnyway, I find memapped NumPy arrays via joblib.cache.Memory and feed_dict to be quite performant (GPU load over 90% throughout a training session), despite the extra copy.", "body": "> The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle.\r\n\r\nThat also applies to `tf.train`. Data doesn't just appear in the graph without reading it from disk, regardless of whether you do it via Python or TensorFlow's execution engine.\r\n\r\nAnyway, I find memapped NumPy arrays via [joblib.cache.Memory](https://pythonhosted.org/joblib/memory.html) and feed_dict to be quite performant (GPU load over 90% throughout a training session), despite the extra copy."}