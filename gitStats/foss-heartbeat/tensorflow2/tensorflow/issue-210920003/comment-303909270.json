{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303909270", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-303909270", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 303909270, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzkwOTI3MA==", "user": {"login": "jasonkriss", "id": 1836763, "node_id": "MDQ6VXNlcjE4MzY3NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1836763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jasonkriss", "html_url": "https://github.com/jasonkriss", "followers_url": "https://api.github.com/users/jasonkriss/followers", "following_url": "https://api.github.com/users/jasonkriss/following{/other_user}", "gists_url": "https://api.github.com/users/jasonkriss/gists{/gist_id}", "starred_url": "https://api.github.com/users/jasonkriss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jasonkriss/subscriptions", "organizations_url": "https://api.github.com/users/jasonkriss/orgs", "repos_url": "https://api.github.com/users/jasonkriss/repos", "events_url": "https://api.github.com/users/jasonkriss/events{/privacy}", "received_events_url": "https://api.github.com/users/jasonkriss/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-25T03:00:01Z", "updated_at": "2017-05-25T03:00:01Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=400331\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lhlmgr\">@lhlmgr</a> The way i understand that example is that iterator needs to be reinitialized every time you switch between train and validation. This isn't the end of the world but for large datasets where we are shuffling minibatches, we want a reasonable <code>buffer_size</code> which means each initialization is quite slow. I find the <code>tf.cond</code> approach with two separate datasets/iterators to work better/faster in that case. That way we can periodically run through validation data without losing our place int the training set.</p>", "body_text": "@lhlmgr The way i understand that example is that iterator needs to be reinitialized every time you switch between train and validation. This isn't the end of the world but for large datasets where we are shuffling minibatches, we want a reasonable buffer_size which means each initialization is quite slow. I find the tf.cond approach with two separate datasets/iterators to work better/faster in that case. That way we can periodically run through validation data without losing our place int the training set.", "body": "@lhlmgr The way i understand that example is that iterator needs to be reinitialized every time you switch between train and validation. This isn't the end of the world but for large datasets where we are shuffling minibatches, we want a reasonable `buffer_size` which means each initialization is quite slow. I find the `tf.cond` approach with two separate datasets/iterators to work better/faster in that case. That way we can periodically run through validation data without losing our place int the training set."}