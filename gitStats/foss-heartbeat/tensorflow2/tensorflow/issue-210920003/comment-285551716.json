{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285551716", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-285551716", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 285551716, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTU1MTcxNg==", "user": {"login": "taion", "id": 3112159, "node_id": "MDQ6VXNlcjMxMTIxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3112159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taion", "html_url": "https://github.com/taion", "followers_url": "https://api.github.com/users/taion/followers", "following_url": "https://api.github.com/users/taion/following{/other_user}", "gists_url": "https://api.github.com/users/taion/gists{/gist_id}", "starred_url": "https://api.github.com/users/taion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taion/subscriptions", "organizations_url": "https://api.github.com/users/taion/orgs", "repos_url": "https://api.github.com/users/taion/repos", "events_url": "https://api.github.com/users/taion/events{/privacy}", "received_events_url": "https://api.github.com/users/taion/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-10T02:06:20Z", "updated_at": "2017-03-10T02:06:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I second <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=618857\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lming\">@lming</a>'s sentiment above.</p>\n<p>Our biggest issue with the current data loading scheme is just that it's very complicated and involves a lot of new concepts.</p>\n<p>We don't find it spectacularly difficult to write a multithreaded data loader ourselves in Python, and generally we don't find it overly difficult to ensure that our data loading and preprocessing runs sufficiently quickly that it doesn't actually bottleneck training.</p>\n<p>Where we're stuck is that to optimally follow recommendations, we end up in an awkward situation, one of:</p>\n<ul>\n<li>Using <code>feed_dict</code> and suffering any relevant performance hits</li>\n<li>Feeding from a separate thread and dealing with some one-off queue boilerplate (except this didn't speed things up at all when we tried it)</li>\n<li>Reimplementing our data loading and transformation pipeline with TF primitives, perhaps with <code>py_func</code>, but still using the TF API for managing queue runners</li>\n</ul>\n<p>The Python threading API isn't perfect, but in general when we're doing mostly non-GIL-taking tasks in NumPy or whatever, the TF queue API seems more of a burden than a help.</p>", "body_text": "I second @lming's sentiment above.\nOur biggest issue with the current data loading scheme is just that it's very complicated and involves a lot of new concepts.\nWe don't find it spectacularly difficult to write a multithreaded data loader ourselves in Python, and generally we don't find it overly difficult to ensure that our data loading and preprocessing runs sufficiently quickly that it doesn't actually bottleneck training.\nWhere we're stuck is that to optimally follow recommendations, we end up in an awkward situation, one of:\n\nUsing feed_dict and suffering any relevant performance hits\nFeeding from a separate thread and dealing with some one-off queue boilerplate (except this didn't speed things up at all when we tried it)\nReimplementing our data loading and transformation pipeline with TF primitives, perhaps with py_func, but still using the TF API for managing queue runners\n\nThe Python threading API isn't perfect, but in general when we're doing mostly non-GIL-taking tasks in NumPy or whatever, the TF queue API seems more of a burden than a help.", "body": "I second @lming's sentiment above.\r\n\r\nOur biggest issue with the current data loading scheme is just that it's very complicated and involves a lot of new concepts.\r\n\r\nWe don't find it spectacularly difficult to write a multithreaded data loader ourselves in Python, and generally we don't find it overly difficult to ensure that our data loading and preprocessing runs sufficiently quickly that it doesn't actually bottleneck training.\r\n\r\nWhere we're stuck is that to optimally follow recommendations, we end up in an awkward situation, one of:\r\n- Using `feed_dict` and suffering any relevant performance hits\r\n- Feeding from a separate thread and dealing with some one-off queue boilerplate (except this didn't speed things up at all when we tried it)\r\n- Reimplementing our data loading and transformation pipeline with TF primitives, perhaps with `py_func`, but still using the TF API for managing queue runners\r\n\r\nThe Python threading API isn't perfect, but in general when we're doing mostly non-GIL-taking tasks in NumPy or whatever, the TF queue API seems more of a burden than a help."}