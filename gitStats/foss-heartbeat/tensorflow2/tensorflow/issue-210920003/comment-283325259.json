{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/283325259", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-283325259", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 283325259, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzMyNTI1OQ==", "user": {"login": "TimZaman", "id": 7721540, "node_id": "MDQ6VXNlcjc3MjE1NDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7721540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TimZaman", "html_url": "https://github.com/TimZaman", "followers_url": "https://api.github.com/users/TimZaman/followers", "following_url": "https://api.github.com/users/TimZaman/following{/other_user}", "gists_url": "https://api.github.com/users/TimZaman/gists{/gist_id}", "starred_url": "https://api.github.com/users/TimZaman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TimZaman/subscriptions", "organizations_url": "https://api.github.com/users/TimZaman/orgs", "repos_url": "https://api.github.com/users/TimZaman/repos", "events_url": "https://api.github.com/users/TimZaman/events{/privacy}", "received_events_url": "https://api.github.com/users/TimZaman/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-01T12:17:05Z", "updated_at": "2017-03-01T12:17:05Z", "author_association": "CONTRIBUTOR", "body_html": "<ul>\n<li>Most importantly we need to address the dequeueing overhead. I've seen dozens and dozens of cases where (in the profiler; iirc MEMCpyWhatever) was really slow. This was mostly an issue where the GPU would get the data from the CPU.</li>\n<li>It would be great if there is still a way to have an <strong>input feed that comes from multi-threaded or multi-processing python</strong>. The following is a great and reliable hack to do currently do that:<br>\n<code>session.run(enqueue_op, feed_dict=$some_numpy_batch_input)</code> Where you can asynchronously feed the queue from python.</li>\n<li>It would also be wow to have <strong>GPU resident queues</strong>.</li>\n</ul>", "body_text": "Most importantly we need to address the dequeueing overhead. I've seen dozens and dozens of cases where (in the profiler; iirc MEMCpyWhatever) was really slow. This was mostly an issue where the GPU would get the data from the CPU.\nIt would be great if there is still a way to have an input feed that comes from multi-threaded or multi-processing python. The following is a great and reliable hack to do currently do that:\nsession.run(enqueue_op, feed_dict=$some_numpy_batch_input) Where you can asynchronously feed the queue from python.\nIt would also be wow to have GPU resident queues.", "body": "- Most importantly we need to address the dequeueing overhead. I've seen dozens and dozens of cases where (in the profiler; iirc MEMCpyWhatever) was really slow. This was mostly an issue where the GPU would get the data from the CPU.\r\n- It would be great if there is still a way to have an **input feed that comes from multi-threaded or multi-processing python**. The following is a great and reliable hack to do currently do that:\r\n`session.run(enqueue_op, feed_dict=$some_numpy_batch_input)` Where you can asynchronously feed the queue from python.\r\n- It would also be wow to have **GPU resident queues**."}