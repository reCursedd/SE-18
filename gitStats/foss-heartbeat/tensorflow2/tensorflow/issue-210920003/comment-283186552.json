{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/283186552", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-283186552", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 283186552, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzE4NjU1Mg==", "user": {"login": "kmhofmann", "id": 7887138, "node_id": "MDQ6VXNlcjc4ODcxMzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/7887138?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kmhofmann", "html_url": "https://github.com/kmhofmann", "followers_url": "https://api.github.com/users/kmhofmann/followers", "following_url": "https://api.github.com/users/kmhofmann/following{/other_user}", "gists_url": "https://api.github.com/users/kmhofmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/kmhofmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kmhofmann/subscriptions", "organizations_url": "https://api.github.com/users/kmhofmann/orgs", "repos_url": "https://api.github.com/users/kmhofmann/repos", "events_url": "https://api.github.com/users/kmhofmann/events{/privacy}", "received_events_url": "https://api.github.com/users/kmhofmann/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-28T22:48:15Z", "updated_at": "2017-02-28T23:23:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A must-have for one of our use cases is ad-hoc creation of data elements via a callback function (which creates tensors on the fly, e.g. using py_func() or through some other means).</p>\n<p>More specifically, we currently have a use case where we employ two queues; an outer one, using a string_input_producer (with shuffling), where each string denotes/points to a \"data set\", and the inner queue is then produced by generating a variable amount of samples from each \"data set\". Which and how many samples are generated differs per epoch (potentially conditional on past training behavior). Actually, we don't even use the nomenclature of an epoch anymore, since the same data is never seen twice, and above mentioned generation/sampling goes beyond the usual data augmentation.</p>\n<p>Long story short: With a slightly out-of-the-ordinary use case, we've been hit by pretty much all of the problems you have mentioned above, and our workarounds have not been pretty. We'd be extremely happy to see a very flexible mechanism, where such cases are supported, and data generation doesn't have to be shoehorned into forced-upon concepts like epochs, finitely repeating queues, etc. (although they can be modeled by its primitives).<br>\nI am not sure how well the planned Dataset/Iterator API would support this.</p>\n<p>Edit: Things we still need of course, include multi-threaded data generation, and multi-threaded random shuffle producer-consumer queues. But without the bane of GIL -- maybe via easy C++/Eigen hooks and thread control on the native side? Back and forth, via pybind?</p>\n<p>Edit2: The new input pipeline should also take support for variable-sized tensors (i.e. different per example) into account, for both training and inference, e.g. in a fully-convolutional setting.</p>", "body_text": "A must-have for one of our use cases is ad-hoc creation of data elements via a callback function (which creates tensors on the fly, e.g. using py_func() or through some other means).\nMore specifically, we currently have a use case where we employ two queues; an outer one, using a string_input_producer (with shuffling), where each string denotes/points to a \"data set\", and the inner queue is then produced by generating a variable amount of samples from each \"data set\". Which and how many samples are generated differs per epoch (potentially conditional on past training behavior). Actually, we don't even use the nomenclature of an epoch anymore, since the same data is never seen twice, and above mentioned generation/sampling goes beyond the usual data augmentation.\nLong story short: With a slightly out-of-the-ordinary use case, we've been hit by pretty much all of the problems you have mentioned above, and our workarounds have not been pretty. We'd be extremely happy to see a very flexible mechanism, where such cases are supported, and data generation doesn't have to be shoehorned into forced-upon concepts like epochs, finitely repeating queues, etc. (although they can be modeled by its primitives).\nI am not sure how well the planned Dataset/Iterator API would support this.\nEdit: Things we still need of course, include multi-threaded data generation, and multi-threaded random shuffle producer-consumer queues. But without the bane of GIL -- maybe via easy C++/Eigen hooks and thread control on the native side? Back and forth, via pybind?\nEdit2: The new input pipeline should also take support for variable-sized tensors (i.e. different per example) into account, for both training and inference, e.g. in a fully-convolutional setting.", "body": "A must-have for one of our use cases is ad-hoc creation of data elements via a callback function (which creates tensors on the fly, e.g. using py_func() or through some other means).\r\n\r\nMore specifically, we currently have a use case where we employ two queues; an outer one, using a string_input_producer (with shuffling), where each string denotes/points to a \"data set\", and the inner queue is then produced by generating a variable amount of samples from each \"data set\". Which and how many samples are generated differs per epoch (potentially conditional on past training behavior). Actually, we don't even use the nomenclature of an epoch anymore, since the same data is never seen twice, and above mentioned generation/sampling goes beyond the usual data augmentation.\r\n\r\nLong story short: With a slightly out-of-the-ordinary use case, we've been hit by pretty much all of the problems you have mentioned above, and our workarounds have not been pretty. We'd be extremely happy to see a very flexible mechanism, where such cases are supported, and data generation doesn't have to be shoehorned into forced-upon concepts like epochs, finitely repeating queues, etc. (although they can be modeled by its primitives).\r\nI am not sure how well the planned Dataset/Iterator API would support this.\r\n\r\nEdit: Things we still need of course, include multi-threaded data generation, and multi-threaded random shuffle producer-consumer queues. But without the bane of GIL -- maybe via easy C++/Eigen hooks and thread control on the native side? Back and forth, via pybind?\r\n\r\nEdit2: The new input pipeline should also take support for variable-sized tensors (i.e. different per example) into account, for both training and inference, e.g. in a fully-convolutional setting."}