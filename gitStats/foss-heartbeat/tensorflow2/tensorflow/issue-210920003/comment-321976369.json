{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321976369", "html_url": "https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321976369", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7951", "id": 321976369, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTk3NjM2OQ==", "user": {"login": "tillahoffmann", "id": 966348, "node_id": "MDQ6VXNlcjk2NjM0OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/966348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tillahoffmann", "html_url": "https://github.com/tillahoffmann", "followers_url": "https://api.github.com/users/tillahoffmann/followers", "following_url": "https://api.github.com/users/tillahoffmann/following{/other_user}", "gists_url": "https://api.github.com/users/tillahoffmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/tillahoffmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tillahoffmann/subscriptions", "organizations_url": "https://api.github.com/users/tillahoffmann/orgs", "repos_url": "https://api.github.com/users/tillahoffmann/repos", "events_url": "https://api.github.com/users/tillahoffmann/events{/privacy}", "received_events_url": "https://api.github.com/users/tillahoffmann/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-12T11:50:12Z", "updated_at": "2017-08-12T11:50:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The new input pipelines are great! But unfortunately, we are unable to use them for large-scale training because our data preprocessing is quite costly and needs to be distributed across multiple machines--or we just haven't figured out the right way to do it.</p>\n<p>We have thus been using the old <code>FIFOQueue</code> interface in the following manner (pseudocode):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Set up queues</span>\nkwargs <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>capacity<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">...</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dtypes<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">...</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>names<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">...</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">...</span>}\ntrain_queue <span class=\"pl-k\">=</span> tf.FIFOQueue(<span class=\"pl-k\">**</span>kwargs)\nvalid_queue <span class=\"pl-k\">=</span> tf.FIFOQueue(<span class=\"pl-k\">**</span>kwargs)\nqueue_index <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nqueue <span class=\"pl-k\">=</span> tf.QueueBase.from_list(queue_index, [train_queue, valid_queue])\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">...</span>\nbatch <span class=\"pl-k\">=</span> queue.dequeue_many(batch_size)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Build model</span>\noutput <span class=\"pl-k\">=</span> build_model(batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>X<span class=\"pl-pds\">'</span></span>])\nloss <span class=\"pl-k\">=</span> evaluate_loss(output, batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Fill queues</span>\ntrain_data_stream <span class=\"pl-k\">=</span> some_iterable_for_training_data()\nvalidation_data_stream <span class=\"pl-k\">=</span> some_iterable_for_training_data()\nstart_filling_queues_in_background_thread(train_queue, train_data_stream)\nstart_filling_queues_in_background_thread(validation_queue, validation_data_stream)</pre></div>\n<p>Having two different queues with <code>from_list</code> allows us to switch between the training and validation queue by either setting the <code>queue_index</code> or feeding it in the <code>feed_dict</code>.</p>\n<p>The <code>some_interable_for_xxx_data</code> are usually generators that get data from a bunch of workers sitting behind a load balancer (e.g. using ZeroMQ, RabbitMQ, or PubSub). This approach works well (because the queues provide a buffer) but we don't have any way of telling when the iterator is exhausted. Some workarounds are</p>\n<ul>\n<li>closing the queue in the background thread such that a <code>tf.errors.OutOfRangeError</code> is raised when the queue is exhausted (but then we can't reopen it again <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"178652537\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4535\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4535/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4535\">#4535</a>)</li>\n<li>setting a timeout on the <code>session.run</code> of the training op and assuming that a timeout is due to the queue being exhausted (but the network connection might be down or our workers might be too slow)</li>\n<li>counting the number of items we've processed and comparing with the expected number of items in the iterator (but that's fiddly and sometimes we don't even know how long the iterator is)</li>\n<li>adding an <code>exhausted</code> field to the queue <code>names</code> and letting the background thread enqueue an item with <code>exhausted=True</code> together with an assertion around the dequeue operation (but using <code>dequeue_many</code> will dequeue elements from the next epoch if the number of items per epoch is not an integer multiple of the batch size, see also <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"157033373\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2514\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2514/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2514\">#2514</a>)</li>\n</ul>\n<p>None of these are satisfactory and it would be great to see either the ability to construct <code>Dataset</code>s from python iterator <em>with a queue for buffering</em> or fix <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"178652537\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4535\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4535/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4535\">#4535</a> (which will automatically fix <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"157033373\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2514\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2514/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2514\">#2514</a>).</p>\n<p>Looking forward to hear whether we've just not been using the datasets API right.</p>", "body_text": "The new input pipelines are great! But unfortunately, we are unable to use them for large-scale training because our data preprocessing is quite costly and needs to be distributed across multiple machines--or we just haven't figured out the right way to do it.\nWe have thus been using the old FIFOQueue interface in the following manner (pseudocode):\n# Set up queues\nkwargs = {'capacity': ..., 'dtypes': ..., 'names': ..., 'shapes': ...}\ntrain_queue = tf.FIFOQueue(**kwargs)\nvalid_queue = tf.FIFOQueue(**kwargs)\nqueue_index = tf.Variable(0, trainable=False)\nqueue = tf.QueueBase.from_list(queue_index, [train_queue, valid_queue])\nbatch_size = ...\nbatch = queue.dequeue_many(batch_size)\n\n# Build model\noutput = build_model(batch['X'])\nloss = evaluate_loss(output, batch['y'])\n\n# Fill queues\ntrain_data_stream = some_iterable_for_training_data()\nvalidation_data_stream = some_iterable_for_training_data()\nstart_filling_queues_in_background_thread(train_queue, train_data_stream)\nstart_filling_queues_in_background_thread(validation_queue, validation_data_stream)\nHaving two different queues with from_list allows us to switch between the training and validation queue by either setting the queue_index or feeding it in the feed_dict.\nThe some_interable_for_xxx_data are usually generators that get data from a bunch of workers sitting behind a load balancer (e.g. using ZeroMQ, RabbitMQ, or PubSub). This approach works well (because the queues provide a buffer) but we don't have any way of telling when the iterator is exhausted. Some workarounds are\n\nclosing the queue in the background thread such that a tf.errors.OutOfRangeError is raised when the queue is exhausted (but then we can't reopen it again #4535)\nsetting a timeout on the session.run of the training op and assuming that a timeout is due to the queue being exhausted (but the network connection might be down or our workers might be too slow)\ncounting the number of items we've processed and comparing with the expected number of items in the iterator (but that's fiddly and sometimes we don't even know how long the iterator is)\nadding an exhausted field to the queue names and letting the background thread enqueue an item with exhausted=True together with an assertion around the dequeue operation (but using dequeue_many will dequeue elements from the next epoch if the number of items per epoch is not an integer multiple of the batch size, see also #2514)\n\nNone of these are satisfactory and it would be great to see either the ability to construct Datasets from python iterator with a queue for buffering or fix #4535 (which will automatically fix #2514).\nLooking forward to hear whether we've just not been using the datasets API right.", "body": "The new input pipelines are great! But unfortunately, we are unable to use them for large-scale training because our data preprocessing is quite costly and needs to be distributed across multiple machines--or we just haven't figured out the right way to do it. \r\n\r\nWe have thus been using the old `FIFOQueue` interface in the following manner (pseudocode):\r\n\r\n```python\r\n# Set up queues\r\nkwargs = {'capacity': ..., 'dtypes': ..., 'names': ..., 'shapes': ...}\r\ntrain_queue = tf.FIFOQueue(**kwargs)\r\nvalid_queue = tf.FIFOQueue(**kwargs)\r\nqueue_index = tf.Variable(0, trainable=False)\r\nqueue = tf.QueueBase.from_list(queue_index, [train_queue, valid_queue])\r\nbatch_size = ...\r\nbatch = queue.dequeue_many(batch_size)\r\n\r\n# Build model\r\noutput = build_model(batch['X'])\r\nloss = evaluate_loss(output, batch['y'])\r\n\r\n# Fill queues\r\ntrain_data_stream = some_iterable_for_training_data()\r\nvalidation_data_stream = some_iterable_for_training_data()\r\nstart_filling_queues_in_background_thread(train_queue, train_data_stream)\r\nstart_filling_queues_in_background_thread(validation_queue, validation_data_stream)\r\n```\r\n\r\nHaving two different queues with `from_list` allows us to switch between the training and validation queue by either setting the `queue_index` or feeding it in the `feed_dict`.\r\n\r\nThe `some_interable_for_xxx_data` are usually generators that get data from a bunch of workers sitting behind a load balancer (e.g. using ZeroMQ, RabbitMQ, or PubSub). This approach works well (because the queues provide a buffer) but we don't have any way of telling when the iterator is exhausted. Some workarounds are\r\n\r\n* closing the queue in the background thread such that a `tf.errors.OutOfRangeError` is raised when the queue is exhausted (but then we can't reopen it again #4535)\r\n* setting a timeout on the `session.run` of the training op and assuming that a timeout is due to the queue being exhausted (but the network connection might be down or our workers might be too slow)\r\n* counting the number of items we've processed and comparing with the expected number of items in the iterator (but that's fiddly and sometimes we don't even know how long the iterator is)\r\n* adding an `exhausted` field to the queue `names` and letting the background thread enqueue an item with `exhausted=True` together with an assertion around the dequeue operation (but using `dequeue_many` will dequeue elements from the next epoch if the number of items per epoch is not an integer multiple of the batch size, see also #2514)\r\n\r\nNone of these are satisfactory and it would be great to see either the ability to construct `Dataset`s from python iterator *with a queue for buffering* or fix #4535 (which will automatically fix #2514).\r\n\r\nLooking forward to hear whether we've just not been using the datasets API right."}