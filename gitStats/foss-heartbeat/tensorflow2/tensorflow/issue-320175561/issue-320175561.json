{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19078", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19078/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19078/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19078/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19078", "id": 320175561, "node_id": "MDU6SXNzdWUzMjAxNzU1NjE=", "number": 19078, "title": "Getting an error while converting frozen inference graph (*.pb) to tflite graph with toco", "user": {"login": "MrMWalker", "id": 22365575, "node_id": "MDQ6VXNlcjIyMzY1NTc1", "avatar_url": "https://avatars0.githubusercontent.com/u/22365575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrMWalker", "html_url": "https://github.com/MrMWalker", "followers_url": "https://api.github.com/users/MrMWalker/followers", "following_url": "https://api.github.com/users/MrMWalker/following{/other_user}", "gists_url": "https://api.github.com/users/MrMWalker/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrMWalker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrMWalker/subscriptions", "organizations_url": "https://api.github.com/users/MrMWalker/orgs", "repos_url": "https://api.github.com/users/MrMWalker/repos", "events_url": "https://api.github.com/users/MrMWalker/events{/privacy}", "received_events_url": "https://api.github.com/users/MrMWalker/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "achowdhery", "id": 4723042, "node_id": "MDQ6VXNlcjQ3MjMwNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4723042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achowdhery", "html_url": "https://github.com/achowdhery", "followers_url": "https://api.github.com/users/achowdhery/followers", "following_url": "https://api.github.com/users/achowdhery/following{/other_user}", "gists_url": "https://api.github.com/users/achowdhery/gists{/gist_id}", "starred_url": "https://api.github.com/users/achowdhery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achowdhery/subscriptions", "organizations_url": "https://api.github.com/users/achowdhery/orgs", "repos_url": "https://api.github.com/users/achowdhery/repos", "events_url": "https://api.github.com/users/achowdhery/events{/privacy}", "received_events_url": "https://api.github.com/users/achowdhery/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "achowdhery", "id": 4723042, "node_id": "MDQ6VXNlcjQ3MjMwNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4723042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achowdhery", "html_url": "https://github.com/achowdhery", "followers_url": "https://api.github.com/users/achowdhery/followers", "following_url": "https://api.github.com/users/achowdhery/following{/other_user}", "gists_url": "https://api.github.com/users/achowdhery/gists{/gist_id}", "starred_url": "https://api.github.com/users/achowdhery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achowdhery/subscriptions", "organizations_url": "https://api.github.com/users/achowdhery/orgs", "repos_url": "https://api.github.com/users/achowdhery/repos", "events_url": "https://api.github.com/users/achowdhery/events{/privacy}", "received_events_url": "https://api.github.com/users/achowdhery/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-05-04T07:22:48Z", "updated_at": "2018-09-13T17:11:34Z", "closed_at": "2018-09-13T17:11:34Z", "author_association": "NONE", "body_html": "<p>I've trained the R-FCN model on my own training set. Now, I'd like to convert my frozen inference graph form the *.pb format to the *.tflite format to use it on an android mobile phone.</p>\n<p>After training, I exported the frozen inference graph with the following command:</p>\n<pre><code>python3 export_inference_graph.py \n--pipeline_config_path=\"training/ckpt/rfcn-69/pipeline.config\" \n--trained_checkpoint_prefix=\"training/ckpt/rfcn-69/model.ckpt-300000\" \n--output_directory=\"training/ckpt/rfcn-69/\"\n</code></pre>\n<p>Afterwards I run the transform_graph util to quantize the graph. I noticed that it doesn't matter wether I run the transform_graph or not. In the end I'm getting the same error with both of the graphs.</p>\n<pre><code>bazel run tensorflow/tools/graph_transforms/transform_graph -- \n--in_graph=\"/git/bda/frozen_graphs/rfcn-69/frozen_inference_graph.pb\" \n--out_graph=\"/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb\" \n--inputs=image_tensor \n--outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" \n--transforms='fold_old_batch_norms quantize_weights strip_unused_nodes sort_by_execution_order obfuscate_names merge_duplicate_nodes'\n</code></pre>\n<p>Finally, I try to convert the quantized graph to an tflite graph with the toco util.</p>\n<pre><code>bazel run --config=opt  tensorflow/contrib/lite/toco:toco -- \n--input_file=/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb \n--output_file=/git/bda/frozen_graphs/out.tflite --inference_type=FLOAT \n--input_shape=1,600,1024,3 \n--input_array=image_tensor \n--output_arrays=num_detections,detection_boxes,detection_scores,detection_classes\n</code></pre>\n<p>But, the operation fails with the following error.</p>\n<pre><code>2018-04-11 13:19:58.364591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\n2018-04-11 13:19:58.364606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\n2018-04-11 13:19:58.364656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayReadV3\n2018-04-11 13:19:58.364753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\n2018-04-11 13:19:58.364851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\n2018-04-11 13:19:58.364969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\n2018-04-11 13:19:58.365001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArraySizeV3\n2018-04-11 13:19:58.365022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\n2018-04-11 13:19:58.366115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\n2018-04-11 13:20:00.620101: F tensorflow/contrib/lite/toco/tooling_util.cc:821] Check failed: d &gt;= 1 (0 vs. 1)\n</code></pre>\n<p>I've got no clue what could be the problem. Any help would be highly appreciated.</p>\n<p>Thanks in advance.</p>", "body_text": "I've trained the R-FCN model on my own training set. Now, I'd like to convert my frozen inference graph form the *.pb format to the *.tflite format to use it on an android mobile phone.\nAfter training, I exported the frozen inference graph with the following command:\npython3 export_inference_graph.py \n--pipeline_config_path=\"training/ckpt/rfcn-69/pipeline.config\" \n--trained_checkpoint_prefix=\"training/ckpt/rfcn-69/model.ckpt-300000\" \n--output_directory=\"training/ckpt/rfcn-69/\"\n\nAfterwards I run the transform_graph util to quantize the graph. I noticed that it doesn't matter wether I run the transform_graph or not. In the end I'm getting the same error with both of the graphs.\nbazel run tensorflow/tools/graph_transforms/transform_graph -- \n--in_graph=\"/git/bda/frozen_graphs/rfcn-69/frozen_inference_graph.pb\" \n--out_graph=\"/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb\" \n--inputs=image_tensor \n--outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" \n--transforms='fold_old_batch_norms quantize_weights strip_unused_nodes sort_by_execution_order obfuscate_names merge_duplicate_nodes'\n\nFinally, I try to convert the quantized graph to an tflite graph with the toco util.\nbazel run --config=opt  tensorflow/contrib/lite/toco:toco -- \n--input_file=/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb \n--output_file=/git/bda/frozen_graphs/out.tflite --inference_type=FLOAT \n--input_shape=1,600,1024,3 \n--input_array=image_tensor \n--output_arrays=num_detections,detection_boxes,detection_scores,detection_classes\n\nBut, the operation fails with the following error.\n2018-04-11 13:19:58.364591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\n2018-04-11 13:19:58.364606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\n2018-04-11 13:19:58.364656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayReadV3\n2018-04-11 13:19:58.364753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\n2018-04-11 13:19:58.364851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\n2018-04-11 13:19:58.364969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\n2018-04-11 13:19:58.365001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArraySizeV3\n2018-04-11 13:19:58.365022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\n2018-04-11 13:19:58.366115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\n2018-04-11 13:20:00.620101: F tensorflow/contrib/lite/toco/tooling_util.cc:821] Check failed: d >= 1 (0 vs. 1)\n\nI've got no clue what could be the problem. Any help would be highly appreciated.\nThanks in advance.", "body": "I've trained the R-FCN model on my own training set. Now, I'd like to convert my frozen inference graph form the *.pb format to the *.tflite format to use it on an android mobile phone.\r\n\r\nAfter training, I exported the frozen inference graph with the following command:\r\n\r\n```\r\npython3 export_inference_graph.py \r\n--pipeline_config_path=\"training/ckpt/rfcn-69/pipeline.config\" \r\n--trained_checkpoint_prefix=\"training/ckpt/rfcn-69/model.ckpt-300000\" \r\n--output_directory=\"training/ckpt/rfcn-69/\"\r\n```\r\nAfterwards I run the transform_graph util to quantize the graph. I noticed that it doesn't matter wether I run the transform_graph or not. In the end I'm getting the same error with both of the graphs.\r\n\r\n```\r\nbazel run tensorflow/tools/graph_transforms/transform_graph -- \r\n--in_graph=\"/git/bda/frozen_graphs/rfcn-69/frozen_inference_graph.pb\" \r\n--out_graph=\"/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb\" \r\n--inputs=image_tensor \r\n--outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" \r\n--transforms='fold_old_batch_norms quantize_weights strip_unused_nodes sort_by_execution_order obfuscate_names merge_duplicate_nodes'\r\n```\r\nFinally, I try to convert the quantized graph to an tflite graph with the toco util.\r\n\r\n```\r\nbazel run --config=opt  tensorflow/contrib/lite/toco:toco -- \r\n--input_file=/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb \r\n--output_file=/git/bda/frozen_graphs/out.tflite --inference_type=FLOAT \r\n--input_shape=1,600,1024,3 \r\n--input_array=image_tensor \r\n--output_arrays=num_detections,detection_boxes,detection_scores,detection_classes\r\n```\r\nBut, the operation fails with the following error.\r\n\r\n```\r\n2018-04-11 13:19:58.364591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\r\n2018-04-11 13:19:58.364606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\r\n2018-04-11 13:19:58.364656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayReadV3\r\n2018-04-11 13:19:58.364753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\r\n2018-04-11 13:19:58.364851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\r\n2018-04-11 13:19:58.364969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\r\n2018-04-11 13:19:58.365001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArraySizeV3\r\n2018-04-11 13:19:58.365022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\r\n2018-04-11 13:19:58.366115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\r\n2018-04-11 13:20:00.620101: F tensorflow/contrib/lite/toco/tooling_util.cc:821] Check failed: d >= 1 (0 vs. 1)\r\n```\r\nI've got no clue what could be the problem. Any help would be highly appreciated.\r\n\r\nThanks in advance."}