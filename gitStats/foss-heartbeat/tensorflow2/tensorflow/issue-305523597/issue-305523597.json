{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17736", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17736/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17736/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17736/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17736", "id": 305523597, "node_id": "MDU6SXNzdWUzMDU1MjM1OTc=", "number": 17736, "title": "worker task crashed  in distributed training if ps task or other worker task not started", "user": {"login": "sandflee", "id": 5102100, "node_id": "MDQ6VXNlcjUxMDIxMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/5102100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sandflee", "html_url": "https://github.com/sandflee", "followers_url": "https://api.github.com/users/sandflee/followers", "following_url": "https://api.github.com/users/sandflee/following{/other_user}", "gists_url": "https://api.github.com/users/sandflee/gists{/gist_id}", "starred_url": "https://api.github.com/users/sandflee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sandflee/subscriptions", "organizations_url": "https://api.github.com/users/sandflee/orgs", "repos_url": "https://api.github.com/users/sandflee/repos", "events_url": "https://api.github.com/users/sandflee/events{/privacy}", "received_events_url": "https://api.github.com/users/sandflee/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-03-15T12:02:06Z", "updated_at": "2018-06-13T03:20:27Z", "closed_at": "2018-05-26T18:37:38Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: yes</li>\n<li>**OS Platform and Distribution **: CentOS Linux release 7.4.1708</li>\n<li>**TensorFlow installed from **: binary</li>\n<li>**TensorFlow version **: 'v1.6.0-0-gd2e24b6039', '1.6.0'</li>\n<li><strong>Python version</strong>:  2.7.5</li>\n<li><strong>CUDA/cuDNN version</strong>:   N/A</li>\n<li><strong>GPU model and memory</strong>:   N/A</li>\n<li><strong>Exact command to reproduce</strong>:  N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>worker task will crashed after 10s, if ps task or other worker task not started. not sure it's a bug or misused the api.</p>\n<h3>Source code / logs</h3>\n<p>logs:</p>\n<blockquote>\n<p>2018-03-15 19:59:32.309238: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br>\n2018-03-15 19:59:32.312834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; bjlt-h1269.sy:42557}<br>\n2018-03-15 19:59:32.312861: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:37060, 1 -&gt; bjlt-h1270.sy:57571}<br>\n2018-03-15 19:59:32.315443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:37060<br>\nVariables initialized ...<br>\nWARNING:tensorflow:From ./demo.py:75: <strong>init</strong> (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease switch to tf.train.MonitoredTrainingSession<br>\n2018-03-15 19:59:36.773767: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error<br>\nTraceback (most recent call last):<br>\nFile \"./demo.py\", line 173, in <br>\ntf.app.run(main=main)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run<br>\n_sys.exit(main(argv))<br>\nFile \"./demo.py\", line 76, in main<br>\nwith sv.prepare_or_wait_for_session(server.target, config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement = True, log_device_placement = True)) as sess:<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 726, in prepare_or_wait_for_session<br>\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 281, in prepare_session<br>\nsess.run(init_op, feed_dict=init_feed_dict)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run<br>\noptions, run_metadata)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error</p>\n</blockquote>\n<p>full code see <a href=\"https://github.com/Qihoo360/XLearning/blob/master/examples/tensorflow/demo.py\">https://github.com/Qihoo360/XLearning/blob/master/examples/tensorflow/demo.py</a><br>\ncode:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">_</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> cluster specification</span>\n  <span class=\"pl-c1\">FLAGS</span>.task_index <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TF_INDEX<span class=\"pl-pds\">\"</span></span>])\n  <span class=\"pl-c1\">FLAGS</span>.job_name <span class=\"pl-k\">=</span> os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TF_ROLE<span class=\"pl-pds\">\"</span></span>]\n  cluster_def <span class=\"pl-k\">=</span> json.loads(os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TF_CLUSTER_DEF<span class=\"pl-pds\">\"</span></span>])\n  cluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec(cluster_def)\n\n  <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ClusterSpec:<span class=\"pl-pds\">\"</span></span>, cluster_def)\n  <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>current task id:<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">FLAGS</span>.task_index, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> role:<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">FLAGS</span>.job_name)\n  \n  gpu_options <span class=\"pl-k\">=</span> tf.GPUOptions(<span class=\"pl-v\">allow_growth</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n  server <span class=\"pl-k\">=</span> tf.train.Server(cluster, <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.job_name,<span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.task_index,<span class=\"pl-v\">config</span> <span class=\"pl-k\">=</span> tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options,<span class=\"pl-v\">allow_soft_placement</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>))\n  \n  <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>:\n    server.join()\n  <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">FLAGS</span>.job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> set the train parameters</span>\n    learning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.learning_rate\n    training_epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.training_epochs\n    batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.batch_size\n    iterData <span class=\"pl-k\">=</span> trainData(<span class=\"pl-c1\">FLAGS</span>.data_path, batch_size)\n    \n    <span class=\"pl-k\">with</span> tf.device(tf.train.replica_device_setter(<span class=\"pl-v\">worker_device</span><span class=\"pl-k\">=</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/job:worker/task:<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(<span class=\"pl-c1\">FLAGS</span>.task_index)),<span class=\"pl-v\">cluster</span><span class=\"pl-k\">=</span>cluster)):\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> count the number of updates</span>\n      global_step <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_step<span class=\"pl-pds\">'</span></span>, [],<span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> tf.constant_initializer(<span class=\"pl-c1\">0</span>), <span class=\"pl-v\">trainable</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> input </span>\n      <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input<span class=\"pl-pds\">'</span></span>):\n        x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">100</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x-input<span class=\"pl-pds\">\"</span></span>)\n        y_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y-input<span class=\"pl-pds\">\"</span></span>)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> model parameters</span>\n      tf.set_random_seed(<span class=\"pl-c1\">1</span>)\n      <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>):\n        W1 <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">50</span>]))\n        W2 <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([<span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">2</span>]))\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> bias</span>\n      <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>biases<span class=\"pl-pds\">\"</span></span>):\n        b1 <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">50</span>]))\n        b2 <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">2</span>]))\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> implement model</span>\n      <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax<span class=\"pl-pds\">\"</span></span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> y is our prediction</span>\n        z1 <span class=\"pl-k\">=</span> tf.add(tf.matmul(x,W1),b1)\n        a1 <span class=\"pl-k\">=</span> tf.nn.softmax(z1)\n        z2 <span class=\"pl-k\">=</span> tf.add(tf.matmul(a1,W2),b2)\n        y <span class=\"pl-k\">=</span> tf.nn.softmax(z2)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> specify cost function</span>\n      <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cross_entropy<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> this is our cost</span>\n        cross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(<span class=\"pl-k\">-</span>tf.reduce_sum(y_ <span class=\"pl-k\">*</span> tf.log(y), <span class=\"pl-v\">reduction_indices</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>]))\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> specify optimizer</span>\n      <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> optimizer is an \"operation\" which we can execute in a session</span>\n        grad_op <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(learning_rate)\n        train_op <span class=\"pl-k\">=</span> grad_op.minimize(cross_entropy, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> init_op</span>\n      tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cross_entropy<span class=\"pl-pds\">'</span></span>, cross_entropy )\n      merged <span class=\"pl-k\">=</span> tf.summary.merge_all()\n      init_op <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n      saver <span class=\"pl-k\">=</span> tf.train.Saver()\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Variables initialized ...<span class=\"pl-pds\">\"</span></span>)\n    sv <span class=\"pl-k\">=</span> tf.train.Supervisor(<span class=\"pl-v\">is_chief</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">FLAGS</span>.task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>), <span class=\"pl-v\">global_step</span> <span class=\"pl-k\">=</span> global_step, <span class=\"pl-v\">init_op</span> <span class=\"pl-k\">=</span> init_op)\n    <span class=\"pl-k\">with</span> sv.prepare_or_wait_for_session(server.target, <span class=\"pl-v\">config</span> <span class=\"pl-k\">=</span> tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options, <span class=\"pl-v\">allow_soft_placement</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>, <span class=\"pl-v\">log_device_placement</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)) <span class=\"pl-k\">as</span> sess:\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> perform training cycles</span>\n      start_time <span class=\"pl-k\">=</span> time.time()\n      <span class=\"pl-k\">if</span>(<span class=\"pl-c1\">FLAGS</span>.task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>):\n        train_writer <span class=\"pl-k\">=</span> tf.summary.FileWriter(<span class=\"pl-c1\">FLAGS</span>.log_dir, sess.graph)\n      <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(training_epochs):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> number of batches in one epoch                </span>\n        sys.stderr.write(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>reporter progress:<span class=\"pl-c1\">%0.4f</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(<span class=\"pl-c1\">float</span>(epoch)<span class=\"pl-k\">/</span>(training_epochs)))\n        totalStep <span class=\"pl-k\">=</span> iterData.batchCount()\n        <span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(totalStep):\n          iterator_curr <span class=\"pl-k\">=</span> iterData.nextBatch()\n          flag <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n          <span class=\"pl-k\">for</span> <span class=\"pl-c1\">iter</span> <span class=\"pl-k\">in</span> iterator_curr:\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">==</span> flag:\n                train_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">iter</span>[<span class=\"pl-c1\">1</span>].reshape(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">100</span>)\n                train_y <span class=\"pl-k\">=</span> oneHot(<span class=\"pl-c1\">iter</span>[<span class=\"pl-c1\">0</span>]).reshape(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>)\n            <span class=\"pl-k\">else</span>:\n                train_x <span class=\"pl-k\">=</span> np.concatenate((train_x, <span class=\"pl-c1\">iter</span>[<span class=\"pl-c1\">1</span>].reshape(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">100</span>)))\n                train_y <span class=\"pl-k\">=</span> np.concatenate((train_y, oneHot(<span class=\"pl-c1\">iter</span>[<span class=\"pl-c1\">0</span>]).reshape(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>)))\n            flag <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n          _, summary, cost, gstep <span class=\"pl-k\">=</span> sess.run(\n                  [train_op, merged, cross_entropy, global_step],\n                  <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: train_x, y_: train_y})\n          elapsed_time <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> start_time\n          start_time <span class=\"pl-k\">=</span> time.time()\n          <span class=\"pl-k\">if</span>(<span class=\"pl-c1\">FLAGS</span>.task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>):\n            train_writer.add_summary(summary, gstep)\n          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Step: <span class=\"pl-c1\">%d</span>,<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (gstep),\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> Epoch: <span class=\"pl-c1\">%2d</span>,<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (epoch),                            \n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> Cost: <span class=\"pl-c1\">%.4f</span>,<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> cost,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> Time: <span class=\"pl-c1\">%3.2f</span>ms<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">float</span>(elapsed_time<span class=\"pl-k\">*</span><span class=\"pl-c1\">1000</span>))\n        sys.stderr.write(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>reporter progress:<span class=\"pl-c1\">%0.4f</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(<span class=\"pl-c1\">float</span>(epoch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">/</span>(training_epochs)))\n  \n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Train Completed.<span class=\"pl-pds\">\"</span></span>)\n      <span class=\"pl-k\">if</span>(<span class=\"pl-c1\">FLAGS</span>.task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>):\n        train_writer.close()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>saving model...<span class=\"pl-pds\">\"</span></span>)\n        saver.save(sess, <span class=\"pl-c1\">FLAGS</span>.save_path<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/model.ckpt<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>done<span class=\"pl-pds\">\"</span></span>)       </pre></div>", "body_text": "System information\n\nHave I written custom code: yes\n**OS Platform and Distribution **: CentOS Linux release 7.4.1708\n**TensorFlow installed from **: binary\n**TensorFlow version **: 'v1.6.0-0-gd2e24b6039', '1.6.0'\nPython version:  2.7.5\nCUDA/cuDNN version:   N/A\nGPU model and memory:   N/A\nExact command to reproduce:  N/A\n\nDescribe the problem\nworker task will crashed after 10s, if ps task or other worker task not started. not sure it's a bug or misused the api.\nSource code / logs\nlogs:\n\n2018-03-15 19:59:32.309238: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-03-15 19:59:32.312834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> bjlt-h1269.sy:42557}\n2018-03-15 19:59:32.312861: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:37060, 1 -> bjlt-h1270.sy:57571}\n2018-03-15 19:59:32.315443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:37060\nVariables initialized ...\nWARNING:tensorflow:From ./demo.py:75: init (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease switch to tf.train.MonitoredTrainingSession\n2018-03-15 19:59:36.773767: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\nTraceback (most recent call last):\nFile \"./demo.py\", line 173, in \ntf.app.run(main=main)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\n_sys.exit(main(argv))\nFile \"./demo.py\", line 76, in main\nwith sv.prepare_or_wait_for_session(server.target, config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement = True, log_device_placement = True)) as sess:\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 726, in prepare_or_wait_for_session\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 281, in prepare_session\nsess.run(init_op, feed_dict=init_feed_dict)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\nrun_metadata_ptr)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\noptions, run_metadata)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\n\nfull code see https://github.com/Qihoo360/XLearning/blob/master/examples/tensorflow/demo.py\ncode:\ndef main(_):\n  # cluster specification\n  FLAGS.task_index = int(os.environ[\"TF_INDEX\"])\n  FLAGS.job_name = os.environ[\"TF_ROLE\"]\n  cluster_def = json.loads(os.environ[\"TF_CLUSTER_DEF\"])\n  cluster = tf.train.ClusterSpec(cluster_def)\n\n  print(\"ClusterSpec:\", cluster_def)\n  print(\"current task id:\", FLAGS.task_index, \" role:\", FLAGS.job_name)\n  \n  gpu_options = tf.GPUOptions(allow_growth = True)\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name,task_index=FLAGS.task_index,config = tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement = True))\n  \n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    # set the train parameters\n    learning_rate = FLAGS.learning_rate\n    training_epochs = FLAGS.training_epochs\n    batch_size = FLAGS.batch_size\n    iterData = trainData(FLAGS.data_path, batch_size)\n    \n    with tf.device(tf.train.replica_device_setter(worker_device=(\"/job:worker/task:%d\"%(FLAGS.task_index)),cluster=cluster)):\n      # count the number of updates\n      global_step = tf.get_variable('global_step', [],initializer = tf.constant_initializer(0), trainable = False)\n      # input \n      with tf.name_scope('input'):\n        x = tf.placeholder(tf.float32, shape=[None, 100], name=\"x-input\")\n        y_ = tf.placeholder(tf.float32, shape=[None, 2], name=\"y-input\")\n      # model parameters\n      tf.set_random_seed(1)\n      with tf.name_scope(\"weights\"):\n        W1 = tf.Variable(tf.random_normal([100, 50]))\n        W2 = tf.Variable(tf.random_normal([50, 2]))\n      # bias\n      with tf.name_scope(\"biases\"):\n        b1 = tf.Variable(tf.zeros([50]))\n        b2 = tf.Variable(tf.zeros([2]))\n      # implement model\n      with tf.name_scope(\"softmax\"):\n        # y is our prediction\n        z1 = tf.add(tf.matmul(x,W1),b1)\n        a1 = tf.nn.softmax(z1)\n        z2 = tf.add(tf.matmul(a1,W2),b2)\n        y = tf.nn.softmax(z2)\n      # specify cost function\n      with tf.name_scope('cross_entropy'):\n        # this is our cost\n        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n      # specify optimizer\n      with tf.name_scope('train'):\n        # optimizer is an \"operation\" which we can execute in a session\n        grad_op = tf.train.GradientDescentOptimizer(learning_rate)\n        train_op = grad_op.minimize(cross_entropy, global_step=global_step)\n      # init_op\n      tf.summary.scalar('cross_entropy', cross_entropy )\n      merged = tf.summary.merge_all()\n      init_op = tf.global_variables_initializer()\n      saver = tf.train.Saver()\n      print(\"Variables initialized ...\")\n    sv = tf.train.Supervisor(is_chief = (FLAGS.task_index == 0), global_step = global_step, init_op = init_op)\n    with sv.prepare_or_wait_for_session(server.target, config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement = True, log_device_placement = True)) as sess:\n      # perform training cycles\n      start_time = time.time()\n      if(FLAGS.task_index == 0):\n        train_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n      for epoch in range(training_epochs):\n        # number of batches in one epoch                \n        sys.stderr.write(\"reporter progress:%0.4f\\n\"%(float(epoch)/(training_epochs)))\n        totalStep = iterData.batchCount()\n        for step in range(totalStep):\n          iterator_curr = iterData.nextBatch()\n          flag = 0\n          for iter in iterator_curr:\n            if 0 == flag:\n                train_x = iter[1].reshape(1,100)\n                train_y = oneHot(iter[0]).reshape(1,2)\n            else:\n                train_x = np.concatenate((train_x, iter[1].reshape(1,100)))\n                train_y = np.concatenate((train_y, oneHot(iter[0]).reshape(1,2)))\n            flag = 1\n          _, summary, cost, gstep = sess.run(\n                  [train_op, merged, cross_entropy, global_step],\n                  feed_dict={x: train_x, y_: train_y})\n          elapsed_time = time.time() - start_time\n          start_time = time.time()\n          if(FLAGS.task_index == 0):\n            train_writer.add_summary(summary, gstep)\n          print(\"Step: %d,\" % (gstep),\n                \" Epoch: %2d,\" % (epoch),                            \n                \" Cost: %.4f,\" % cost,\n                \" Time: %3.2fms\" % float(elapsed_time*1000))\n        sys.stderr.write(\"reporter progress:%0.4f\\n\"%(float(epoch+1)/(training_epochs)))\n  \n      print(\"Train Completed.\")\n      if(FLAGS.task_index == 0):\n        train_writer.close()\n        print(\"saving model...\")\n        saver.save(sess, FLAGS.save_path+\"/model.ckpt\")\n    print(\"done\")", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution **: CentOS Linux release 7.4.1708\r\n- **TensorFlow installed from **: binary\r\n- **TensorFlow version **: 'v1.6.0-0-gd2e24b6039', '1.6.0'\r\n- **Python version**:  2.7.5\r\n- **CUDA/cuDNN version**:   N/A\r\n- **GPU model and memory**:   N/A\r\n- **Exact command to reproduce**:  N/A\r\n\r\n### Describe the problem\r\nworker task will crashed after 10s, if ps task or other worker task not started. not sure it's a bug or misused the api.\r\n\r\n### Source code / logs\r\n\r\nlogs:\r\n>2018-03-15 19:59:32.309238: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-03-15 19:59:32.312834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> bjlt-h1269.sy:42557}\r\n2018-03-15 19:59:32.312861: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:37060, 1 -> bjlt-h1270.sy:57571}\r\n2018-03-15 19:59:32.315443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:37060\r\nVariables initialized ...\r\nWARNING:tensorflow:From ./demo.py:75: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2018-03-15 19:59:36.773767: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\r\nTraceback (most recent call last):\r\n  File \"./demo.py\", line 173, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"./demo.py\", line 76, in main\r\n    with sv.prepare_or_wait_for_session(server.target, config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement = True, log_device_placement = True)) as sess:\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 726, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 281, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n\r\nfull code see https://github.com/Qihoo360/XLearning/blob/master/examples/tensorflow/demo.py\r\ncode:\r\n``` python\r\ndef main(_):\r\n  # cluster specification\r\n  FLAGS.task_index = int(os.environ[\"TF_INDEX\"])\r\n  FLAGS.job_name = os.environ[\"TF_ROLE\"]\r\n  cluster_def = json.loads(os.environ[\"TF_CLUSTER_DEF\"])\r\n  cluster = tf.train.ClusterSpec(cluster_def)\r\n\r\n  print(\"ClusterSpec:\", cluster_def)\r\n  print(\"current task id:\", FLAGS.task_index, \" role:\", FLAGS.job_name)\r\n  \r\n  gpu_options = tf.GPUOptions(allow_growth = True)\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name,task_index=FLAGS.task_index,config = tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement = True))\r\n  \r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n    # set the train parameters\r\n    learning_rate = FLAGS.learning_rate\r\n    training_epochs = FLAGS.training_epochs\r\n    batch_size = FLAGS.batch_size\r\n    iterData = trainData(FLAGS.data_path, batch_size)\r\n    \r\n    with tf.device(tf.train.replica_device_setter(worker_device=(\"/job:worker/task:%d\"%(FLAGS.task_index)),cluster=cluster)):\r\n      # count the number of updates\r\n      global_step = tf.get_variable('global_step', [],initializer = tf.constant_initializer(0), trainable = False)\r\n      # input \r\n      with tf.name_scope('input'):\r\n        x = tf.placeholder(tf.float32, shape=[None, 100], name=\"x-input\")\r\n        y_ = tf.placeholder(tf.float32, shape=[None, 2], name=\"y-input\")\r\n      # model parameters\r\n      tf.set_random_seed(1)\r\n      with tf.name_scope(\"weights\"):\r\n        W1 = tf.Variable(tf.random_normal([100, 50]))\r\n        W2 = tf.Variable(tf.random_normal([50, 2]))\r\n      # bias\r\n      with tf.name_scope(\"biases\"):\r\n        b1 = tf.Variable(tf.zeros([50]))\r\n        b2 = tf.Variable(tf.zeros([2]))\r\n      # implement model\r\n      with tf.name_scope(\"softmax\"):\r\n        # y is our prediction\r\n        z1 = tf.add(tf.matmul(x,W1),b1)\r\n        a1 = tf.nn.softmax(z1)\r\n        z2 = tf.add(tf.matmul(a1,W2),b2)\r\n        y = tf.nn.softmax(z2)\r\n      # specify cost function\r\n      with tf.name_scope('cross_entropy'):\r\n        # this is our cost\r\n        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n      # specify optimizer\r\n      with tf.name_scope('train'):\r\n        # optimizer is an \"operation\" which we can execute in a session\r\n        grad_op = tf.train.GradientDescentOptimizer(learning_rate)\r\n        train_op = grad_op.minimize(cross_entropy, global_step=global_step)\r\n      # init_op\r\n      tf.summary.scalar('cross_entropy', cross_entropy )\r\n      merged = tf.summary.merge_all()\r\n      init_op = tf.global_variables_initializer()\r\n      saver = tf.train.Saver()\r\n      print(\"Variables initialized ...\")\r\n    sv = tf.train.Supervisor(is_chief = (FLAGS.task_index == 0), global_step = global_step, init_op = init_op)\r\n    with sv.prepare_or_wait_for_session(server.target, config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement = True, log_device_placement = True)) as sess:\r\n      # perform training cycles\r\n      start_time = time.time()\r\n      if(FLAGS.task_index == 0):\r\n        train_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\r\n      for epoch in range(training_epochs):\r\n        # number of batches in one epoch                \r\n        sys.stderr.write(\"reporter progress:%0.4f\\n\"%(float(epoch)/(training_epochs)))\r\n        totalStep = iterData.batchCount()\r\n        for step in range(totalStep):\r\n          iterator_curr = iterData.nextBatch()\r\n          flag = 0\r\n          for iter in iterator_curr:\r\n            if 0 == flag:\r\n                train_x = iter[1].reshape(1,100)\r\n                train_y = oneHot(iter[0]).reshape(1,2)\r\n            else:\r\n                train_x = np.concatenate((train_x, iter[1].reshape(1,100)))\r\n                train_y = np.concatenate((train_y, oneHot(iter[0]).reshape(1,2)))\r\n            flag = 1\r\n          _, summary, cost, gstep = sess.run(\r\n                  [train_op, merged, cross_entropy, global_step],\r\n                  feed_dict={x: train_x, y_: train_y})\r\n          elapsed_time = time.time() - start_time\r\n          start_time = time.time()\r\n          if(FLAGS.task_index == 0):\r\n            train_writer.add_summary(summary, gstep)\r\n          print(\"Step: %d,\" % (gstep),\r\n                \" Epoch: %2d,\" % (epoch),                            \r\n                \" Cost: %.4f,\" % cost,\r\n                \" Time: %3.2fms\" % float(elapsed_time*1000))\r\n        sys.stderr.write(\"reporter progress:%0.4f\\n\"%(float(epoch+1)/(training_epochs)))\r\n  \r\n      print(\"Train Completed.\")\r\n      if(FLAGS.task_index == 0):\r\n        train_writer.close()\r\n        print(\"saving model...\")\r\n        saver.save(sess, FLAGS.save_path+\"/model.ckpt\")\r\n    print(\"done\")       \r\n```\r\n"}