{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17344", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17344/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17344/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17344/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17344", "id": 301293455, "node_id": "MDU6SXNzdWUzMDEyOTM0NTU=", "number": 17344, "title": "Does the size of embedding have a huge influence on the training speed?", "user": {"login": "yananchen1989", "id": 26405281, "node_id": "MDQ6VXNlcjI2NDA1Mjgx", "avatar_url": "https://avatars2.githubusercontent.com/u/26405281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yananchen1989", "html_url": "https://github.com/yananchen1989", "followers_url": "https://api.github.com/users/yananchen1989/followers", "following_url": "https://api.github.com/users/yananchen1989/following{/other_user}", "gists_url": "https://api.github.com/users/yananchen1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/yananchen1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yananchen1989/subscriptions", "organizations_url": "https://api.github.com/users/yananchen1989/orgs", "repos_url": "https://api.github.com/users/yananchen1989/repos", "events_url": "https://api.github.com/users/yananchen1989/events{/privacy}", "received_events_url": "https://api.github.com/users/yananchen1989/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-01T06:36:00Z", "updated_at": "2018-03-01T19:36:28Z", "closed_at": "2018-03-01T19:36:28Z", "author_association": "NONE", "body_html": "<p>In my text classification task, I use vectors to represent the words. After I enlarge the size of vocabulary (word dimension is still 128), from 200000 to 1000000, the training step time arise from 0.1s to 0.4s. Here is the word embedding tensor creatation:</p>\n<p><code>tf.get_variable('embedding', shape, dtype=dtype, return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True)</code></p>\n<p>And the sentences are made of word ids and vectors will be looked up from the embedding tensor above.</p>\n<p><code>inputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)</code></p>\n<p>So what confused me is that why the size of vocabulary has a significant impact on the training speed. Is that normal ?</p>", "body_text": "In my text classification task, I use vectors to represent the words. After I enlarge the size of vocabulary (word dimension is still 128), from 200000 to 1000000, the training step time arise from 0.1s to 0.4s. Here is the word embedding tensor creatation:\ntf.get_variable('embedding', shape, dtype=dtype, return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True)\nAnd the sentences are made of word ids and vectors will be looked up from the embedding tensor above.\ninputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)\nSo what confused me is that why the size of vocabulary has a significant impact on the training speed. Is that normal ?", "body": "In my text classification task, I use vectors to represent the words. After I enlarge the size of vocabulary (word dimension is still 128), from 200000 to 1000000, the training step time arise from 0.1s to 0.4s. Here is the word embedding tensor creatation:\r\n\r\n`tf.get_variable('embedding', shape, dtype=dtype, return tf.get_variable('embedding', shape, dtype=dtype, initializer=tf.random_normal_initializer(stddev=0.1), trainable=True)`\r\n\r\nAnd the sentences are made of word ids and vectors will be looked up from the embedding tensor above.\r\n\r\n`inputs_embedding = tf.contrib.layers.embedding_lookup_unique(embedding, inputs)`\r\n\r\nSo what confused me is that why the size of vocabulary has a significant impact on the training speed. Is that normal ?"}