{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/392308854", "html_url": "https://github.com/tensorflow/tensorflow/issues/19159#issuecomment-392308854", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19159", "id": 392308854, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjMwODg1NA==", "user": {"login": "iamsimha", "id": 3105044, "node_id": "MDQ6VXNlcjMxMDUwNDQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/3105044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iamsimha", "html_url": "https://github.com/iamsimha", "followers_url": "https://api.github.com/users/iamsimha/followers", "following_url": "https://api.github.com/users/iamsimha/following{/other_user}", "gists_url": "https://api.github.com/users/iamsimha/gists{/gist_id}", "starred_url": "https://api.github.com/users/iamsimha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iamsimha/subscriptions", "organizations_url": "https://api.github.com/users/iamsimha/orgs", "repos_url": "https://api.github.com/users/iamsimha/repos", "events_url": "https://api.github.com/users/iamsimha/events{/privacy}", "received_events_url": "https://api.github.com/users/iamsimha/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-27T06:39:22Z", "updated_at": "2018-05-27T06:39:22Z", "author_association": "NONE", "body_html": "<p>The sampled_softmax_loss is different from CNTKs negative sampling loss in two ways.</p>\n<ol>\n<li>\n<p>The negative samples are generated by probabilistic sampling in TF, instead of deterministic strides.</p>\n</li>\n<li>\n<p>The weights parameter to sampled_softmax_loss can only be a tf.variable instead of an arbitrary op. This is because sampled_softmax_loss internally does a nn.embedding_lookup on weights parameter. According to my understanding embedding_lookup is efficient, only if the input is a variable or an op with sparse gradient support. If the input parameter is an arbitrary op, TF raises a sparse gradient warning.  So is it possible to support arbitrary ops for the weights parameter by removing dependency on embedding_lookup?</p>\n</li>\n</ol>", "body_text": "The sampled_softmax_loss is different from CNTKs negative sampling loss in two ways.\n\n\nThe negative samples are generated by probabilistic sampling in TF, instead of deterministic strides.\n\n\nThe weights parameter to sampled_softmax_loss can only be a tf.variable instead of an arbitrary op. This is because sampled_softmax_loss internally does a nn.embedding_lookup on weights parameter. According to my understanding embedding_lookup is efficient, only if the input is a variable or an op with sparse gradient support. If the input parameter is an arbitrary op, TF raises a sparse gradient warning.  So is it possible to support arbitrary ops for the weights parameter by removing dependency on embedding_lookup?", "body": "The sampled_softmax_loss is different from CNTKs negative sampling loss in two ways.\r\n\r\n1. The negative samples are generated by probabilistic sampling in TF, instead of deterministic strides.\r\n\r\n2. The weights parameter to sampled_softmax_loss can only be a tf.variable instead of an arbitrary op. This is because sampled_softmax_loss internally does a nn.embedding_lookup on weights parameter. According to my understanding embedding_lookup is efficient, only if the input is a variable or an op with sparse gradient support. If the input parameter is an arbitrary op, TF raises a sparse gradient warning.  So is it possible to support arbitrary ops for the weights parameter by removing dependency on embedding_lookup?"}