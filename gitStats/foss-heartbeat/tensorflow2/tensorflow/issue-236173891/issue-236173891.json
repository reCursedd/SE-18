{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10731", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10731/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10731/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10731/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10731", "id": 236173891, "node_id": "MDU6SXNzdWUyMzYxNzM4OTE=", "number": 10731, "title": "scope reuse problem variable not exist for using Dense(in contirb.seq2seq.attention_wrapper)", "user": {"login": "chenghuige", "id": 6323467, "node_id": "MDQ6VXNlcjYzMjM0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6323467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenghuige", "html_url": "https://github.com/chenghuige", "followers_url": "https://api.github.com/users/chenghuige/followers", "following_url": "https://api.github.com/users/chenghuige/following{/other_user}", "gists_url": "https://api.github.com/users/chenghuige/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenghuige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenghuige/subscriptions", "organizations_url": "https://api.github.com/users/chenghuige/orgs", "repos_url": "https://api.github.com/users/chenghuige/repos", "events_url": "https://api.github.com/users/chenghuige/events{/privacy}", "received_events_url": "https://api.github.com/users/chenghuige/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2017-06-15T12:26:18Z", "updated_at": "2018-06-02T07:17:26Z", "closed_at": "2018-06-02T07:04:53Z", "author_association": "NONE", "body_html": "<p>tf version '1.2.0-rc0'<br>\nin attention_wrapper.py(contirb.seq2seq.attention_wrapper), it use below</p>\n<pre><code>from tensorflow.python.layers import core as layers_core \nmemory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)  #line 416\n</code></pre>\n<p>but this usage will cause un expected result when trying to reuse memory_layer, see below</p>\n<pre><code>input = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\nwith tf.variable_scope('main') as scope:\n    memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\n    x = memory_layer(input)\n    scope.reuse_variables()\n    memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\n    y = memory_layer(input)\n</code></pre>\n<p>ValueError: Variable main/memory_layer_1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?</p>\n<p>One workaround is to change name=\"memory_layer\" to _scope=\"memory_layer\"</p>\n<pre><code>with tf.variable_scope('main') as scope:\n    memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\n    x = memory_layer(input)\n    scope.reuse_variables()\n    memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\n    y = memory_layer(input)\n</code></pre>\n<p>I think this is a bug for atttention_wrapper.py ? since we can not reuse memory_layer, then we can not train/evaluate in one graph when using attention cell wrapper.</p>", "body_text": "tf version '1.2.0-rc0'\nin attention_wrapper.py(contirb.seq2seq.attention_wrapper), it use below\nfrom tensorflow.python.layers import core as layers_core \nmemory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)  #line 416\n\nbut this usage will cause un expected result when trying to reuse memory_layer, see below\ninput = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\nwith tf.variable_scope('main') as scope:\n    memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\n    x = memory_layer(input)\n    scope.reuse_variables()\n    memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\n    y = memory_layer(input)\n\nValueError: Variable main/memory_layer_1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\nOne workaround is to change name=\"memory_layer\" to _scope=\"memory_layer\"\nwith tf.variable_scope('main') as scope:\n    memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\n    x = memory_layer(input)\n    scope.reuse_variables()\n    memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\n    y = memory_layer(input)\n\nI think this is a bug for atttention_wrapper.py ? since we can not reuse memory_layer, then we can not train/evaluate in one graph when using attention cell wrapper.", "body": "tf version '1.2.0-rc0' \r\nin attention_wrapper.py(contirb.seq2seq.attention_wrapper), it use below\r\n      \r\n    from tensorflow.python.layers import core as layers_core \r\n    memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)  #line 416\r\n\r\nbut this usage will cause un expected result when trying to reuse memory_layer, see below\r\n\r\n    input = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\r\n    with tf.variable_scope('main') as scope:\r\n        memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\r\n        x = memory_layer(input)\r\n        scope.reuse_variables()\r\n        memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\r\n        y = memory_layer(input)\r\n\r\nValueError: Variable main/memory_layer_1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\nOne workaround is to change name=\"memory_layer\" to _scope=\"memory_layer\"\r\n\r\n    with tf.variable_scope('main') as scope:\r\n        memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\r\n        x = memory_layer(input)\r\n        scope.reuse_variables()\r\n        memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\r\n        y = memory_layer(input)\r\n\r\nI think this is a bug for atttention_wrapper.py ? since we can not reuse memory_layer, then we can not train/evaluate in one graph when using attention cell wrapper.\r\n"}