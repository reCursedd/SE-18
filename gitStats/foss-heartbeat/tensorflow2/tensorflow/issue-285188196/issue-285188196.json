{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15727", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15727/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15727/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15727/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15727", "id": 285188196, "node_id": "MDU6SXNzdWUyODUxODgxOTY=", "number": 15727, "title": "using tf.layers.batch_normalization() gives erratic validation loss though implementation seems correct.", "user": {"login": "zaffnet", "id": 12458272, "node_id": "MDQ6VXNlcjEyNDU4Mjcy", "avatar_url": "https://avatars3.githubusercontent.com/u/12458272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zaffnet", "html_url": "https://github.com/zaffnet", "followers_url": "https://api.github.com/users/zaffnet/followers", "following_url": "https://api.github.com/users/zaffnet/following{/other_user}", "gists_url": "https://api.github.com/users/zaffnet/gists{/gist_id}", "starred_url": "https://api.github.com/users/zaffnet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zaffnet/subscriptions", "organizations_url": "https://api.github.com/users/zaffnet/orgs", "repos_url": "https://api.github.com/users/zaffnet/repos", "events_url": "https://api.github.com/users/zaffnet/events{/privacy}", "received_events_url": "https://api.github.com/users/zaffnet/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-30T07:00:14Z", "updated_at": "2017-12-30T09:12:21Z", "closed_at": "2017-12-30T09:12:21Z", "author_association": "NONE", "body_html": "<p>I am trying to use Batch Normalization using <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\" rel=\"nofollow\">tf.layers.batch_normalization()</a> and I have followed the documentation closely. My code looks like this:</p>\n\n<pre><code>def create_conv_exp_model(fingerprint_input, model_settings, is_training):\n  \n\n  # Dropout placeholder\n  if is_training:\n    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n\n  # Mode placeholder\n  mode_placeholder = tf.placeholder(tf.bool, name=\"mode_placeholder\")\n\n  he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\n\n  # Input Layer\n  input_frequency_size = model_settings['bins']\n  input_time_size = model_settings['spectrogram_length']\n  net = tf.reshape(fingerprint_input,\n                   [-1, input_time_size, input_frequency_size, 1],\n                   name=\"reshape\")\n  net = tf.layers.batch_normalization(net, \n                                      training=mode_placeholder,\n                                      name='bn_0')\n\n  for i in range(1, 6):\n    net = tf.layers.conv2d(inputs=net,\n                           filters=8*(2**i),\n                           kernel_size=[5, 5],\n                           padding='same',\n                           kernel_initializer=he_init,\n                           name=\"conv_%d\"%i)\n    net = tf.layers.batch_normalization(net,\n                                        training=mode_placeholder,\n                                        name='bn_%d'%i)\n    with tf.name_scope(\"relu_%d\"%i):\n      net = tf.nn.relu(net)\n    net = tf.layers.max_pooling2d(net, [2, 2], [2, 2], 'SAME', \n                                  name=\"maxpool_%d\"%i)\n\n  net_shape = net.get_shape().as_list()\n  net_height = net_shape[1]\n  net_width = net_shape[2]\n  net = tf.layers.conv2d( inputs=net,\n                          filters=1024,\n                          kernel_size=[net_height, net_width],\n                          strides=(net_height, net_width),\n                          padding='same',\n                          kernel_initializer=he_init,\n                          name=\"conv_f\")\n  net = tf.layers.batch_normalization( net, \n                                        training=mode_placeholder,\n                                        name='bn_f')\n  with tf.name_scope(\"relu_f\"):\n    net = tf.nn.relu(net)\n\n  net = tf.layers.conv2d( inputs=net,\n                          filters=model_settings['label_count'],\n                          kernel_size=[1, 1],\n                          padding='same',\n                          kernel_initializer=he_init,\n                          name=\"conv_l\")\n\n  ### Squeeze\n  squeezed = tf.squeeze(net, axis=[1, 2], name=\"squeezed\")\n\n  if is_training:\n    return squeezed, dropout_prob, mode_placeholder\n  else:\n    return squeezed, mode_placeholder\n</code></pre>\n<p>And my train step looks like this:</p>\n\n<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n  gvs = optimizer.compute_gradients(cross_entropy_mean)\n  capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n  train_step = optimizer.apply_gradients(gvs))\n</code></pre>\n<p>During training, I am feeding the graph with:</p>\n\n<pre><code>train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(\n    [\n        merged_summaries, evaluation_step, cross_entropy_mean, train_step,\n        increment_global_step\n    ],\n    feed_dict={\n        fingerprint_input: train_fingerprints,\n        ground_truth_input: train_ground_truth,\n        learning_rate_input: learning_rate_value,\n        dropout_prob: 0.5,\n        mode_placeholder: True\n    })\n</code></pre>\n<p>During validation,</p>\n\n<pre><code>validation_summary, validation_accuracy, conf_matrix = sess.run(\n                [merged_summaries, evaluation_step, confusion_matrix],\n                feed_dict={\n                    fingerprint_input: validation_fingerprints,\n                    ground_truth_input: validation_ground_truth,\n                    dropout_prob: 1.0,\n                    mode_placeholder: False\n                })\n</code></pre>\n<p>My loss and accuracy curves (orange is training, blue is validation):<br>\n<a href=\"https://i.stack.imgur.com/ZAqDw.png\" rel=\"nofollow\">Plot of loss vs number of iterations</a>,<br>\n<a href=\"https://i.stack.imgur.com/CYKJX.png\" rel=\"nofollow\">Plot of accuracy vs number of iterations</a></p>\n<p>The validation loss (and accuracy) seem very erratic. Is my implementation of Batch Normalization wrong? Or is this normal with Batch Normalization and I should wait for more iterations? Or maybe, moving statistics are not being saved and hence poor performance. I tried StackOverflow and found many people have the same problem and there is no definitive guide on how to resolve this.</p>", "body_text": "I am trying to use Batch Normalization using tf.layers.batch_normalization() and I have followed the documentation closely. My code looks like this:\n\ndef create_conv_exp_model(fingerprint_input, model_settings, is_training):\n  \n\n  # Dropout placeholder\n  if is_training:\n    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n\n  # Mode placeholder\n  mode_placeholder = tf.placeholder(tf.bool, name=\"mode_placeholder\")\n\n  he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\n\n  # Input Layer\n  input_frequency_size = model_settings['bins']\n  input_time_size = model_settings['spectrogram_length']\n  net = tf.reshape(fingerprint_input,\n                   [-1, input_time_size, input_frequency_size, 1],\n                   name=\"reshape\")\n  net = tf.layers.batch_normalization(net, \n                                      training=mode_placeholder,\n                                      name='bn_0')\n\n  for i in range(1, 6):\n    net = tf.layers.conv2d(inputs=net,\n                           filters=8*(2**i),\n                           kernel_size=[5, 5],\n                           padding='same',\n                           kernel_initializer=he_init,\n                           name=\"conv_%d\"%i)\n    net = tf.layers.batch_normalization(net,\n                                        training=mode_placeholder,\n                                        name='bn_%d'%i)\n    with tf.name_scope(\"relu_%d\"%i):\n      net = tf.nn.relu(net)\n    net = tf.layers.max_pooling2d(net, [2, 2], [2, 2], 'SAME', \n                                  name=\"maxpool_%d\"%i)\n\n  net_shape = net.get_shape().as_list()\n  net_height = net_shape[1]\n  net_width = net_shape[2]\n  net = tf.layers.conv2d( inputs=net,\n                          filters=1024,\n                          kernel_size=[net_height, net_width],\n                          strides=(net_height, net_width),\n                          padding='same',\n                          kernel_initializer=he_init,\n                          name=\"conv_f\")\n  net = tf.layers.batch_normalization( net, \n                                        training=mode_placeholder,\n                                        name='bn_f')\n  with tf.name_scope(\"relu_f\"):\n    net = tf.nn.relu(net)\n\n  net = tf.layers.conv2d( inputs=net,\n                          filters=model_settings['label_count'],\n                          kernel_size=[1, 1],\n                          padding='same',\n                          kernel_initializer=he_init,\n                          name=\"conv_l\")\n\n  ### Squeeze\n  squeezed = tf.squeeze(net, axis=[1, 2], name=\"squeezed\")\n\n  if is_training:\n    return squeezed, dropout_prob, mode_placeholder\n  else:\n    return squeezed, mode_placeholder\n\nAnd my train step looks like this:\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n  gvs = optimizer.compute_gradients(cross_entropy_mean)\n  capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n  train_step = optimizer.apply_gradients(gvs))\n\nDuring training, I am feeding the graph with:\n\ntrain_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(\n    [\n        merged_summaries, evaluation_step, cross_entropy_mean, train_step,\n        increment_global_step\n    ],\n    feed_dict={\n        fingerprint_input: train_fingerprints,\n        ground_truth_input: train_ground_truth,\n        learning_rate_input: learning_rate_value,\n        dropout_prob: 0.5,\n        mode_placeholder: True\n    })\n\nDuring validation,\n\nvalidation_summary, validation_accuracy, conf_matrix = sess.run(\n                [merged_summaries, evaluation_step, confusion_matrix],\n                feed_dict={\n                    fingerprint_input: validation_fingerprints,\n                    ground_truth_input: validation_ground_truth,\n                    dropout_prob: 1.0,\n                    mode_placeholder: False\n                })\n\nMy loss and accuracy curves (orange is training, blue is validation):\nPlot of loss vs number of iterations,\nPlot of accuracy vs number of iterations\nThe validation loss (and accuracy) seem very erratic. Is my implementation of Batch Normalization wrong? Or is this normal with Batch Normalization and I should wait for more iterations? Or maybe, moving statistics are not being saved and hence poor performance. I tried StackOverflow and found many people have the same problem and there is no definitive guide on how to resolve this.", "body": "I am trying to use Batch Normalization using [tf.layers.batch_normalization()][1] and I have followed the documentation closely. My code looks like this:\r\n\r\n<!-- language: python -->\r\n\r\n    def create_conv_exp_model(fingerprint_input, model_settings, is_training):\r\n      \r\n\r\n      # Dropout placeholder\r\n      if is_training:\r\n        dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\r\n\r\n      # Mode placeholder\r\n      mode_placeholder = tf.placeholder(tf.bool, name=\"mode_placeholder\")\r\n\r\n      he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\r\n\r\n      # Input Layer\r\n      input_frequency_size = model_settings['bins']\r\n      input_time_size = model_settings['spectrogram_length']\r\n      net = tf.reshape(fingerprint_input,\r\n                       [-1, input_time_size, input_frequency_size, 1],\r\n                       name=\"reshape\")\r\n      net = tf.layers.batch_normalization(net, \r\n                                          training=mode_placeholder,\r\n                                          name='bn_0')\r\n\r\n      for i in range(1, 6):\r\n        net = tf.layers.conv2d(inputs=net,\r\n                               filters=8*(2**i),\r\n                               kernel_size=[5, 5],\r\n                               padding='same',\r\n                               kernel_initializer=he_init,\r\n                               name=\"conv_%d\"%i)\r\n        net = tf.layers.batch_normalization(net,\r\n                                            training=mode_placeholder,\r\n                                            name='bn_%d'%i)\r\n        with tf.name_scope(\"relu_%d\"%i):\r\n          net = tf.nn.relu(net)\r\n        net = tf.layers.max_pooling2d(net, [2, 2], [2, 2], 'SAME', \r\n                                      name=\"maxpool_%d\"%i)\r\n\r\n      net_shape = net.get_shape().as_list()\r\n      net_height = net_shape[1]\r\n      net_width = net_shape[2]\r\n      net = tf.layers.conv2d( inputs=net,\r\n                              filters=1024,\r\n                              kernel_size=[net_height, net_width],\r\n                              strides=(net_height, net_width),\r\n                              padding='same',\r\n                              kernel_initializer=he_init,\r\n                              name=\"conv_f\")\r\n      net = tf.layers.batch_normalization( net, \r\n                                            training=mode_placeholder,\r\n                                            name='bn_f')\r\n      with tf.name_scope(\"relu_f\"):\r\n        net = tf.nn.relu(net)\r\n\r\n      net = tf.layers.conv2d( inputs=net,\r\n                              filters=model_settings['label_count'],\r\n                              kernel_size=[1, 1],\r\n                              padding='same',\r\n                              kernel_initializer=he_init,\r\n                              name=\"conv_l\")\r\n\r\n      ### Squeeze\r\n      squeezed = tf.squeeze(net, axis=[1, 2], name=\"squeezed\")\r\n\r\n      if is_training:\r\n        return squeezed, dropout_prob, mode_placeholder\r\n      else:\r\n        return squeezed, mode_placeholder\r\n\r\nAnd my train step looks like this:\r\n\r\n<!-- language: python -->\r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n      optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\r\n      gvs = optimizer.compute_gradients(cross_entropy_mean)\r\n      capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\r\n      train_step = optimizer.apply_gradients(gvs))\r\n\r\nDuring training, I am feeding the graph with:\r\n\r\n<!-- language: python -->\r\n\r\n    train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(\r\n        [\r\n            merged_summaries, evaluation_step, cross_entropy_mean, train_step,\r\n            increment_global_step\r\n        ],\r\n        feed_dict={\r\n            fingerprint_input: train_fingerprints,\r\n            ground_truth_input: train_ground_truth,\r\n            learning_rate_input: learning_rate_value,\r\n            dropout_prob: 0.5,\r\n            mode_placeholder: True\r\n        })\r\n\r\nDuring validation, \r\n\r\n<!-- language: python -->\r\n\r\n    validation_summary, validation_accuracy, conf_matrix = sess.run(\r\n                    [merged_summaries, evaluation_step, confusion_matrix],\r\n                    feed_dict={\r\n                        fingerprint_input: validation_fingerprints,\r\n                        ground_truth_input: validation_ground_truth,\r\n                        dropout_prob: 1.0,\r\n                        mode_placeholder: False\r\n                    })\r\n\r\nMy loss and accuracy curves (orange is training, blue is validation):\r\n[Plot of loss vs number of iterations][2],\r\n[Plot of accuracy vs number of iterations][3]\r\n\r\nThe validation loss (and accuracy) seem very erratic. Is my implementation of Batch Normalization wrong? Or is this normal with Batch Normalization and I should wait for more iterations? Or maybe, moving statistics are not being saved and hence poor performance. I tried StackOverflow and found many people have the same problem and there is no definitive guide on how to resolve this.\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\r\n  [2]: https://i.stack.imgur.com/ZAqDw.png\r\n  [3]: https://i.stack.imgur.com/CYKJX.png"}