{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9259", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9259/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9259/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9259/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9259", "id": 222075403, "node_id": "MDU6SXNzdWUyMjIwNzU0MDM=", "number": 9259, "title": "TensorFlow on ARMv7 seems to be slower", "user": {"login": "gunasekaran7", "id": 13835289, "node_id": "MDQ6VXNlcjEzODM1Mjg5", "avatar_url": "https://avatars2.githubusercontent.com/u/13835289?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gunasekaran7", "html_url": "https://github.com/gunasekaran7", "followers_url": "https://api.github.com/users/gunasekaran7/followers", "following_url": "https://api.github.com/users/gunasekaran7/following{/other_user}", "gists_url": "https://api.github.com/users/gunasekaran7/gists{/gist_id}", "starred_url": "https://api.github.com/users/gunasekaran7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gunasekaran7/subscriptions", "organizations_url": "https://api.github.com/users/gunasekaran7/orgs", "repos_url": "https://api.github.com/users/gunasekaran7/repos", "events_url": "https://api.github.com/users/gunasekaran7/events{/privacy}", "received_events_url": "https://api.github.com/users/gunasekaran7/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-04-17T06:30:04Z", "updated_at": "2017-10-20T04:34:04Z", "closed_at": "2017-04-18T00:43:42Z", "author_association": "NONE", "body_html": "<p>I have installed tensorflow on our custom board which is based on i.mx6 (ARMv7) processor. I have compiled the C++ module alone without python support and was able to run the \"tensorflow/pi_examples/label_image\"</p>\n<p>Command used:  <strong>make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI</strong></p>\n<p>The problem is that the application seems to be very slow compared to Laptop (Intel i5). The inference takes close to 60-70s while it takes only 0.6 - 0.7s in my Laptop.</p>\n<p>After that, I figured out that I missed to use compiler optimization flag to use Neon. I compiled with Neon support and now it takes around 10-17s for same example.</p>\n<p>Command used: <strong>make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI  OPTFLAGS=\"-Os -mfpu=neon -funsafe-math-optimizations -ftree-vectorize\"</strong></p>\n<p>Is there anything I am missing?</p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: No</li>\n<li><strong>OS Platform and Distribution</strong>: Yocto debian flavour</li>\n<li><strong>TensorFlow installed from</strong>:  source (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/27a98083a6c16f263d668271889863596efbeb84/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/27a98083a6c16f263d668271889863596efbeb84\"><tt>27a9808</tt></a>)</li>\n<li><strong>TensorFlow version</strong>: NA</li>\n<li><strong>Bazel version (if compiling from source)</strong>: Not used</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>: NA</li>\n</ul>", "body_text": "I have installed tensorflow on our custom board which is based on i.mx6 (ARMv7) processor. I have compiled the C++ module alone without python support and was able to run the \"tensorflow/pi_examples/label_image\"\nCommand used:  make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI\nThe problem is that the application seems to be very slow compared to Laptop (Intel i5). The inference takes close to 60-70s while it takes only 0.6 - 0.7s in my Laptop.\nAfter that, I figured out that I missed to use compiler optimization flag to use Neon. I compiled with Neon support and now it takes around 10-17s for same example.\nCommand used: make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI  OPTFLAGS=\"-Os -mfpu=neon -funsafe-math-optimizations -ftree-vectorize\"\nIs there anything I am missing?\nSystem information\n\nHave I written custom code: No\nOS Platform and Distribution: Yocto debian flavour\nTensorFlow installed from:  source (27a9808)\nTensorFlow version: NA\nBazel version (if compiling from source): Not used\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: NA", "body": "I have installed tensorflow on our custom board which is based on i.mx6 (ARMv7) processor. I have compiled the C++ module alone without python support and was able to run the \"tensorflow/pi_examples/label_image\"\r\n\r\nCommand used:  **make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI**\r\n\r\nThe problem is that the application seems to be very slow compared to Laptop (Intel i5). The inference takes close to 60-70s while it takes only 0.6 - 0.7s in my Laptop. \r\n\r\nAfter that, I figured out that I missed to use compiler optimization flag to use Neon. I compiled with Neon support and now it takes around 10-17s for same example.\r\n\r\nCommand used: **make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI  OPTFLAGS=\"-Os -mfpu=neon -funsafe-math-optimizations -ftree-vectorize\"**\r\n\r\nIs there anything I am missing?\r\n\r\n### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Yocto debian flavour\r\n- **TensorFlow installed from**:  source (27a9808)\r\n- **TensorFlow version**: NA\r\n- **Bazel version (if compiling from source)**: Not used\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n"}