{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19304", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19304/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19304/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19304/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19304", "id": 323383087, "node_id": "MDU6SXNzdWUzMjMzODMwODc=", "number": 19304, "title": "With TF C-API, got \"GPU device not registered\" and no-op for SessionRun.", "user": {"login": "thomasyoung", "id": 583132, "node_id": "MDQ6VXNlcjU4MzEzMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/583132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasyoung", "html_url": "https://github.com/thomasyoung", "followers_url": "https://api.github.com/users/thomasyoung/followers", "following_url": "https://api.github.com/users/thomasyoung/following{/other_user}", "gists_url": "https://api.github.com/users/thomasyoung/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasyoung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasyoung/subscriptions", "organizations_url": "https://api.github.com/users/thomasyoung/orgs", "repos_url": "https://api.github.com/users/thomasyoung/repos", "events_url": "https://api.github.com/users/thomasyoung/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasyoung/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-15T21:05:07Z", "updated_at": "2018-10-19T02:26:24Z", "closed_at": "2018-05-16T00:32:00Z", "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.7.0/1.8.0</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:n/a</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>:9.0/7.0</li>\n<li><strong>GPU model and memory</strong>:1080Ti</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a><br>\nOutput:</p>\n<pre lang=\"==\" data-meta=\"cat /etc/issue ===============================================\"><code>Linux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy                         1.14.3                \nprotobuf                      3.5.2.post1           \ntensorflow-gpu                1.8.0                 \n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.8.0\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\nSanity check: array([1], dtype=int32)\n/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64/:\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nTue May 15 13:54:31 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n| 27%   48C    P0    72W / 250W |   1489MiB / 11177MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1168      G   /usr/lib/xorg/Xorg                           870MiB |\n|    0      2504      G   compiz                                       437MiB |\n|    0      3240      G   ...-token=F3F41E0025601B32775ADF60FE0AE568   178MiB |\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\n</code></pre>\n<p>You can obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<pre><code>/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n('v1.8.0-0-g93bc2e2072', '1.8.0')\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I am trying to load and run graph model (mobilenet v2) with Tensorflow C-API. Python script downloaded from following link works on my desktop.<br>\n<code>https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py</code><br>\nThe c-api library and framework library are downloaded via Bazel:</p>\n<pre><code>new_http_archive(\n    name = \"libtensorflow\",\n    build_file = \"third_party/libtensorflow.BUILD\",\n    sha256 = \"05f5b543d5054f3c1ddc4f4caff7e3a6a96985579ef2d3dd9340ab94b1262f54\",\n    \n    url = \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.8.0.tar.gz\",\n    )\ncc_library(\n    name = \"libtensorflow\",\n    srcs = [\n        \"lib/libtensorflow.so\",\n        \"lib/libtensorflow_framework.so\",\n    ],\n    hdrs = [\n        \"include/tensorflow/c/c_api.h\",\n        \"include/tensorflow/c/c_api_experimental.h\",\n    ],\n    strip_include_prefix = \"include\",\n    visibility = [\"//visibility:public\"],\n)\n</code></pre>\n<p>With following code calling into C-API I see non-changed output buffer after SessionRun() and dubious console log:</p>\n<pre><code>2018-05-15 15:42:11.572574: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-05-15 15:42:11.695012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 9.26GiB\n2018-05-15 15:42:11.695035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n2018-05-15 15:42:12.104753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-05-15 15:42:12.104773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n2018-05-15 15:42:12.104777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n2018-05-15 15:42:12.104976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8958 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\nHello from TensorFlow C library version 1.8.0\ninf starting\n2018-05-15 15:42:12.195162: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\n\n</code></pre>\n<p>The model is downloaded here: <code>https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</code><br>\n<code>https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz</code></p>\n<p>The image is downloaded here:<br>\n<code>https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb</code><br>\nand processed with python code for loading in c++:</p>\n<pre><code>import numpy as np\nfrom PIL import Image\nimg = np.array(Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\nreimg=np.reshape(img, [224,672])\nnp.savetxt('panda.ndarray',reimg)\n</code></pre>\n<h3>Source code / logs</h3>\n<pre lang=\"#include\" data-meta=\"&lt;string.h&gt;\"><code>#include &lt;fstream&gt;\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;string&gt;\n\n#include \"gflags/gflags.h\"\n#include \"tensorflow/c/c_api.h\"\n\n#define ASSERT(expr, ...)      \\\n  if (!(expr)) {               \\\n    char buf[1024];            \\\n    sprintf(buf, __VA_ARGS__); \\\n    std::cerr &lt;&lt; buf;          \\\n    std::abort();              \\\n  }\n\nDEFINE_string(model_filename,\n              \"mobilenet_v2_1.4_224_frozen.pb\",\n              \"Filename for model to load\");\n\nDEFINE_int32(num_repeat, 10, \"Run inference many times.\");\n\nDEFINE_string(np_filename, \"panda.ndarray\",\n              \"Filename for (text) image file to load\");\n\nvoid free_buffer(void* data, size_t length) { free(data); }\n\nvoid Deallocator(void* data, size_t length, void* arg) { free(data); }\n\nTF_Buffer* read_file(const char* file) {\n  FILE* f = fopen(file, \"rb\");\n  ASSERT(f != nullptr, \"Model File Not Ready\");\n  fseek(f, 0, SEEK_END);\n  long fsize = ftell(f);\n  fseek(f, 0, SEEK_SET);  // same as rewind(f);\n\n  void* data = malloc(fsize);\n  std::ignore = fread(data, fsize, 1, f);\n  fclose(f);\n\n  TF_Buffer* buf = TF_NewBuffer();\n  buf-&gt;data = data;\n  buf-&gt;length = fsize;\n  buf-&gt;data_deallocator = free_buffer;\n  return buf;\n}\n\nint main(int argc, char** argv) {\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\n\n  // Creates buffer for graph def by reading pb file\n  std::unique_ptr&lt;TF_Buffer, std::function&lt;void(TF_Buffer*)&gt;&gt; p_model_buffer(\n      read_file(FLAGS_model_filename.c_str()), [](TF_Buffer* p) { TF_DeleteBuffer(p); });\n  ASSERT(\n      p_model_buffer != nullptr &amp;&amp; p_model_buffer-&gt;data != nullptr &amp;&amp; p_model_buffer-&gt;length != 0,\n      \"Reading Graph Model pb file Failure\");\n\n  // Creates error status for checking\n  std::unique_ptr&lt;TF_Status, std::function&lt;void(TF_Status*)&gt;&gt; p_status(\n      TF_NewStatus(), [](TF_Status* p) { TF_DeleteStatus(p); });\n  ASSERT(p_status != nullptr, \"Status Creation Failure\");\n\n  // Creates graph instance\n  std::unique_ptr&lt;TF_Graph, std::function&lt;void(TF_Graph*)&gt;&gt; p_graph(\n      TF_NewGraph(), [](TF_Graph* p) { TF_DeleteGraph(p); });\n  ASSERT(p_graph != nullptr, \"Graph Creation Failure\");\n\n  // Creates session option\n  std::unique_ptr&lt;TF_SessionOptions, std::function&lt;void(TF_SessionOptions*)&gt;&gt; p_session_opts(\n      TF_NewSessionOptions(), [](TF_SessionOptions* p) { TF_DeleteSessionOptions(p); });\n  ASSERT(p_session_opts != nullptr, \"Create SessionOptions Failure\");\n\n  // Creates options for importing graph_def\n  std::unique_ptr&lt;TF_ImportGraphDefOptions, std::function&lt;void(TF_ImportGraphDefOptions*)&gt;&gt;\n      p_import_graph_def_options(TF_NewImportGraphDefOptions(), [](TF_ImportGraphDefOptions* p) {\n        TF_DeleteImportGraphDefOptions(p);\n      });\n  ASSERT(p_import_graph_def_options != nullptr, \"Create Import Graph Def Option Failure\");\n\n  // Imports graph def into graph\n  TF_GraphImportGraphDef(p_graph.get(), p_model_buffer.get(), p_import_graph_def_options.get(),\n                         p_status.get());\n  ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Import Failure %s\",\n         TF_Message(p_status.get()));\n\n  // Grab output ops from importing result\n  TF_Operation* ops_in = TF_GraphOperationByName(p_graph.get(), \"input\");\n  ASSERT(ops_in != nullptr, \"Getting Input Operation Failure\");\n  TF_Operation* ops_out =\n      TF_GraphOperationByName(p_graph.get(), \"MobilenetV2/Predictions/Reshape_1\");\n  ASSERT(ops_out != nullptr, \"Getting Output Operation Failure\");\n  TF_Output tf_output_in{.oper = ops_in, .index = 0};\n  TF_Output tf_output_out{.oper = ops_out, .index = 0};\n\n  // Create Tensors for input and output\n  std::vector&lt;std::shared_ptr&lt;TF_Tensor&gt;&gt; input_tensor_vector;\n  std::vector&lt;std::shared_ptr&lt;TF_Tensor&gt;&gt; output_tensor_vector;\n  // Input tensor is fp32 image of 224x224 with bgr channels, as\n  // img = np.array(PIL.Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\n  int64_t input_dims[] = {1, 224, 224, 3};\n  ASSERT(sizeof(float) == 4, \"Float32 should be 4 bytes.\");\n  size_t input_num_bytes = 224 * 224 * 3 * sizeof(float);\n  // Output tensor is fp32 1001x1 one-hot for classes of objects\n  int64_t output_dims[] = {1, 1001};\n  size_t output_num_bytes = 1001 * sizeof(float);\n\n  float* input_buffer = static_cast&lt;float*&gt;(malloc(input_num_bytes));\n  ASSERT(input_buffer != nullptr, \"Input Tensor Memory Allocation Failure.\");\n  TF_Tensor* input_tensor =\n      TF_NewTensor(TF_FLOAT, input_dims, 4, input_buffer, input_num_bytes, &amp;Deallocator, 0);\n  ASSERT(input_tensor != nullptr, \"Input Tensor Creation Failure\");\n  input_tensor_vector.push_back(\n      std::shared_ptr&lt;TF_Tensor&gt;(input_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\n\n  float* output_buffer = static_cast&lt;float*&gt;(malloc(output_num_bytes));\n  ASSERT(output_buffer != nullptr, \"Output Tensor Memory Allocation Failure.\");\n  memset(output_buffer, 0, output_num_bytes);\n  TF_Tensor* output_tensor =\n      TF_NewTensor(TF_FLOAT, output_dims, 2, output_buffer, output_num_bytes, &amp;Deallocator, 0);\n  ASSERT(output_tensor != nullptr, \"Output Tensor Creation Failure\");\n  output_tensor_vector.push_back(\n      std::shared_ptr&lt;TF_Tensor&gt;(output_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\n\n  // Prepare input image data\n  {\n    std::ifstream img(FLAGS_np_filename);\n    ASSERT(img, \"Read input image tensor failure.\");\n    float* p_data = static_cast&lt;float*&gt;(TF_TensorData(input_tensor_vector[0].get()));\n    for (int i = 0; i &lt; 224; ++i) {\n      for (int j = 0; j &lt; 224 * 3; ++j) {\n        int offset = i * 224 * 3 + j;\n        img &gt;&gt; p_data[offset];\n      }\n    }\n  }\n  {\n    // Initializes config proto with magic numbers got from following python code:\n    // config = tf.ConfigProto(allow_soft_placement = True)\n    // serialized = config.SerializeToString() # '8\\x01'\n    char config_proto[] = {'8', 0x01};\n    TF_SetConfig(p_session_opts.get(), config_proto, 2, p_status.get());\n    ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Set Config Failure %s\",\n           TF_Message(p_status.get()));\n\n    // Creates session instance for loaded graph model\n    std::unique_ptr&lt;TF_Session, std::function&lt;void(TF_Session*)&gt;&gt; p_session(\n        TF_NewSession(p_graph.get(), p_session_opts.get(), p_status.get()), [&amp;](TF_Session* p) {\n          TF_CloseSession(p, p_status.get());\n          TF_DeleteSession(p, p_status.get());\n        });\n    ASSERT(p_session != nullptr, \"Session Creation Failure %s\", TF_Message(p_status.get()));\n\n    // Empty RunOptions for inference.\n    TF_Buffer* run_options = nullptr;\n    constexpr int ninputs = 1;\n    constexpr int noutputs = 1;\n    constexpr int ntargets = 0;\n    TF_Tensor* output_tensors[1] = {output_tensor_vector[0].get()};\n    TF_Tensor* const input_tensors[1] = {input_tensor_vector[0].get()};\n\n    std::cerr &lt;&lt; \"inf starting\" &lt;&lt; std::endl;\n    for (int i = 0; i &lt; FLAGS_num_repeat; ++i) {\n      TF_SessionRun(p_session.get(), run_options,\n                    // Input tensors\n                    &amp;tf_output_in, input_tensors, ninputs,\n                    // Output tensors\n                    &amp;tf_output_out, output_tensors, noutputs,\n                    // Target operations\n                    nullptr, ntargets,\n                    // RunMetadata\n                    nullptr,  // only valid run_metadata value\n                    p_status.get());\n      ASSERT(p_status != nullptr, \"Inference Failure %s\", TF_Message(p_status.get()));\n\n      // print out max prob and corresponding idx\n      const float* p_data = static_cast&lt;const float*&gt;(TF_TensorData(output_tensor_vector[0].get()));\n      float v_max = p_data[0];\n      int i_max = 0;\n      for (int i = 1; i &lt; 1001; ++i) {\n        if (p_data[i] &gt; v_max) {\n          v_max = p_data[i];\n          i_max = i;\n        }\n      }\n      std::cout &lt;&lt; \"Max value of \" &lt;&lt; v_max &lt;&lt; \" at \" &lt;&lt; i_max &lt;&lt; std::endl;\n    }\n  }\n\n  return 0;\n}\n</code></pre>", "body_text": "Please go to Stack Overflow for help and support:\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):1.7.0/1.8.0\nPython version: 2.7\nBazel version (if compiling from source):n/a\nGCC/Compiler version (if compiling from source):5.4.0\nCUDA/cuDNN version:9.0/7.0\nGPU model and memory:1080Ti\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nOutput:\nLinux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy                         1.14.3                \nprotobuf                      3.5.2.post1           \ntensorflow-gpu                1.8.0                 \n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.8.0\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\nSanity check: array([1], dtype=int32)\n/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64/:\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nTue May 15 13:54:31 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n| 27%   48C    P0    72W / 250W |   1489MiB / 11177MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1168      G   /usr/lib/xorg/Xorg                           870MiB |\n|    0      2504      G   compiz                                       437MiB |\n|    0      3240      G   ...-token=F3F41E0025601B32775ADF60FE0AE568   178MiB |\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\n\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n('v1.8.0-0-g93bc2e2072', '1.8.0')\n\nDescribe the problem\nI am trying to load and run graph model (mobilenet v2) with Tensorflow C-API. Python script downloaded from following link works on my desktop.\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py\nThe c-api library and framework library are downloaded via Bazel:\nnew_http_archive(\n    name = \"libtensorflow\",\n    build_file = \"third_party/libtensorflow.BUILD\",\n    sha256 = \"05f5b543d5054f3c1ddc4f4caff7e3a6a96985579ef2d3dd9340ab94b1262f54\",\n    \n    url = \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.8.0.tar.gz\",\n    )\ncc_library(\n    name = \"libtensorflow\",\n    srcs = [\n        \"lib/libtensorflow.so\",\n        \"lib/libtensorflow_framework.so\",\n    ],\n    hdrs = [\n        \"include/tensorflow/c/c_api.h\",\n        \"include/tensorflow/c/c_api_experimental.h\",\n    ],\n    strip_include_prefix = \"include\",\n    visibility = [\"//visibility:public\"],\n)\n\nWith following code calling into C-API I see non-changed output buffer after SessionRun() and dubious console log:\n2018-05-15 15:42:11.572574: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-05-15 15:42:11.695012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 9.26GiB\n2018-05-15 15:42:11.695035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n2018-05-15 15:42:12.104753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-05-15 15:42:12.104773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \n2018-05-15 15:42:12.104777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \n2018-05-15 15:42:12.104976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8958 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\nHello from TensorFlow C library version 1.8.0\ninf starting\n2018-05-15 15:42:12.195162: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\nMax value of 0 at 0\n\n\nThe model is downloaded here: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet\nhttps://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz\nThe image is downloaded here:\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb\nand processed with python code for loading in c++:\nimport numpy as np\nfrom PIL import Image\nimg = np.array(Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\nreimg=np.reshape(img, [224,672])\nnp.savetxt('panda.ndarray',reimg)\n\nSource code / logs\n#include <fstream>\n#include <iostream>\n#include <memory>\n#include <string>\n\n#include \"gflags/gflags.h\"\n#include \"tensorflow/c/c_api.h\"\n\n#define ASSERT(expr, ...)      \\\n  if (!(expr)) {               \\\n    char buf[1024];            \\\n    sprintf(buf, __VA_ARGS__); \\\n    std::cerr << buf;          \\\n    std::abort();              \\\n  }\n\nDEFINE_string(model_filename,\n              \"mobilenet_v2_1.4_224_frozen.pb\",\n              \"Filename for model to load\");\n\nDEFINE_int32(num_repeat, 10, \"Run inference many times.\");\n\nDEFINE_string(np_filename, \"panda.ndarray\",\n              \"Filename for (text) image file to load\");\n\nvoid free_buffer(void* data, size_t length) { free(data); }\n\nvoid Deallocator(void* data, size_t length, void* arg) { free(data); }\n\nTF_Buffer* read_file(const char* file) {\n  FILE* f = fopen(file, \"rb\");\n  ASSERT(f != nullptr, \"Model File Not Ready\");\n  fseek(f, 0, SEEK_END);\n  long fsize = ftell(f);\n  fseek(f, 0, SEEK_SET);  // same as rewind(f);\n\n  void* data = malloc(fsize);\n  std::ignore = fread(data, fsize, 1, f);\n  fclose(f);\n\n  TF_Buffer* buf = TF_NewBuffer();\n  buf->data = data;\n  buf->length = fsize;\n  buf->data_deallocator = free_buffer;\n  return buf;\n}\n\nint main(int argc, char** argv) {\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\n\n  // Creates buffer for graph def by reading pb file\n  std::unique_ptr<TF_Buffer, std::function<void(TF_Buffer*)>> p_model_buffer(\n      read_file(FLAGS_model_filename.c_str()), [](TF_Buffer* p) { TF_DeleteBuffer(p); });\n  ASSERT(\n      p_model_buffer != nullptr && p_model_buffer->data != nullptr && p_model_buffer->length != 0,\n      \"Reading Graph Model pb file Failure\");\n\n  // Creates error status for checking\n  std::unique_ptr<TF_Status, std::function<void(TF_Status*)>> p_status(\n      TF_NewStatus(), [](TF_Status* p) { TF_DeleteStatus(p); });\n  ASSERT(p_status != nullptr, \"Status Creation Failure\");\n\n  // Creates graph instance\n  std::unique_ptr<TF_Graph, std::function<void(TF_Graph*)>> p_graph(\n      TF_NewGraph(), [](TF_Graph* p) { TF_DeleteGraph(p); });\n  ASSERT(p_graph != nullptr, \"Graph Creation Failure\");\n\n  // Creates session option\n  std::unique_ptr<TF_SessionOptions, std::function<void(TF_SessionOptions*)>> p_session_opts(\n      TF_NewSessionOptions(), [](TF_SessionOptions* p) { TF_DeleteSessionOptions(p); });\n  ASSERT(p_session_opts != nullptr, \"Create SessionOptions Failure\");\n\n  // Creates options for importing graph_def\n  std::unique_ptr<TF_ImportGraphDefOptions, std::function<void(TF_ImportGraphDefOptions*)>>\n      p_import_graph_def_options(TF_NewImportGraphDefOptions(), [](TF_ImportGraphDefOptions* p) {\n        TF_DeleteImportGraphDefOptions(p);\n      });\n  ASSERT(p_import_graph_def_options != nullptr, \"Create Import Graph Def Option Failure\");\n\n  // Imports graph def into graph\n  TF_GraphImportGraphDef(p_graph.get(), p_model_buffer.get(), p_import_graph_def_options.get(),\n                         p_status.get());\n  ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Import Failure %s\",\n         TF_Message(p_status.get()));\n\n  // Grab output ops from importing result\n  TF_Operation* ops_in = TF_GraphOperationByName(p_graph.get(), \"input\");\n  ASSERT(ops_in != nullptr, \"Getting Input Operation Failure\");\n  TF_Operation* ops_out =\n      TF_GraphOperationByName(p_graph.get(), \"MobilenetV2/Predictions/Reshape_1\");\n  ASSERT(ops_out != nullptr, \"Getting Output Operation Failure\");\n  TF_Output tf_output_in{.oper = ops_in, .index = 0};\n  TF_Output tf_output_out{.oper = ops_out, .index = 0};\n\n  // Create Tensors for input and output\n  std::vector<std::shared_ptr<TF_Tensor>> input_tensor_vector;\n  std::vector<std::shared_ptr<TF_Tensor>> output_tensor_vector;\n  // Input tensor is fp32 image of 224x224 with bgr channels, as\n  // img = np.array(PIL.Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\n  int64_t input_dims[] = {1, 224, 224, 3};\n  ASSERT(sizeof(float) == 4, \"Float32 should be 4 bytes.\");\n  size_t input_num_bytes = 224 * 224 * 3 * sizeof(float);\n  // Output tensor is fp32 1001x1 one-hot for classes of objects\n  int64_t output_dims[] = {1, 1001};\n  size_t output_num_bytes = 1001 * sizeof(float);\n\n  float* input_buffer = static_cast<float*>(malloc(input_num_bytes));\n  ASSERT(input_buffer != nullptr, \"Input Tensor Memory Allocation Failure.\");\n  TF_Tensor* input_tensor =\n      TF_NewTensor(TF_FLOAT, input_dims, 4, input_buffer, input_num_bytes, &Deallocator, 0);\n  ASSERT(input_tensor != nullptr, \"Input Tensor Creation Failure\");\n  input_tensor_vector.push_back(\n      std::shared_ptr<TF_Tensor>(input_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\n\n  float* output_buffer = static_cast<float*>(malloc(output_num_bytes));\n  ASSERT(output_buffer != nullptr, \"Output Tensor Memory Allocation Failure.\");\n  memset(output_buffer, 0, output_num_bytes);\n  TF_Tensor* output_tensor =\n      TF_NewTensor(TF_FLOAT, output_dims, 2, output_buffer, output_num_bytes, &Deallocator, 0);\n  ASSERT(output_tensor != nullptr, \"Output Tensor Creation Failure\");\n  output_tensor_vector.push_back(\n      std::shared_ptr<TF_Tensor>(output_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\n\n  // Prepare input image data\n  {\n    std::ifstream img(FLAGS_np_filename);\n    ASSERT(img, \"Read input image tensor failure.\");\n    float* p_data = static_cast<float*>(TF_TensorData(input_tensor_vector[0].get()));\n    for (int i = 0; i < 224; ++i) {\n      for (int j = 0; j < 224 * 3; ++j) {\n        int offset = i * 224 * 3 + j;\n        img >> p_data[offset];\n      }\n    }\n  }\n  {\n    // Initializes config proto with magic numbers got from following python code:\n    // config = tf.ConfigProto(allow_soft_placement = True)\n    // serialized = config.SerializeToString() # '8\\x01'\n    char config_proto[] = {'8', 0x01};\n    TF_SetConfig(p_session_opts.get(), config_proto, 2, p_status.get());\n    ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Set Config Failure %s\",\n           TF_Message(p_status.get()));\n\n    // Creates session instance for loaded graph model\n    std::unique_ptr<TF_Session, std::function<void(TF_Session*)>> p_session(\n        TF_NewSession(p_graph.get(), p_session_opts.get(), p_status.get()), [&](TF_Session* p) {\n          TF_CloseSession(p, p_status.get());\n          TF_DeleteSession(p, p_status.get());\n        });\n    ASSERT(p_session != nullptr, \"Session Creation Failure %s\", TF_Message(p_status.get()));\n\n    // Empty RunOptions for inference.\n    TF_Buffer* run_options = nullptr;\n    constexpr int ninputs = 1;\n    constexpr int noutputs = 1;\n    constexpr int ntargets = 0;\n    TF_Tensor* output_tensors[1] = {output_tensor_vector[0].get()};\n    TF_Tensor* const input_tensors[1] = {input_tensor_vector[0].get()};\n\n    std::cerr << \"inf starting\" << std::endl;\n    for (int i = 0; i < FLAGS_num_repeat; ++i) {\n      TF_SessionRun(p_session.get(), run_options,\n                    // Input tensors\n                    &tf_output_in, input_tensors, ninputs,\n                    // Output tensors\n                    &tf_output_out, output_tensors, noutputs,\n                    // Target operations\n                    nullptr, ntargets,\n                    // RunMetadata\n                    nullptr,  // only valid run_metadata value\n                    p_status.get());\n      ASSERT(p_status != nullptr, \"Inference Failure %s\", TF_Message(p_status.get()));\n\n      // print out max prob and corresponding idx\n      const float* p_data = static_cast<const float*>(TF_TensorData(output_tensor_vector[0].get()));\n      float v_max = p_data[0];\n      int i_max = 0;\n      for (int i = 1; i < 1001; ++i) {\n        if (p_data[i] > v_max) {\n          v_max = p_data[i];\n          i_max = i;\n        }\n      }\n      std::cout << \"Max value of \" << v_max << \" at \" << i_max << std::endl;\n    }\n  }\n\n  return 0;\n}", "body": "Please go to Stack Overflow for help and support:\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.7.0/1.8.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:n/a\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**:1080Ti\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\nOutput:\r\n```== cat /etc/issue ===============================================\r\nLinux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                         1.14.3                \r\nprotobuf                      3.5.2.post1           \r\ntensorflow-gpu                1.8.0                 \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64/:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue May 15 13:54:31 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 27%   48C    P0    72W / 250W |   1489MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1168      G   /usr/lib/xorg/Xorg                           870MiB |\r\n|    0      2504      G   compiz                                       437MiB |\r\n|    0      3240      G   ...-token=F3F41E0025601B32775ADF60FE0AE568   178MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252\r\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n```\r\n\r\nYou can obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n```\r\n/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n('v1.8.0-0-g93bc2e2072', '1.8.0')\r\n```\r\n\r\n### Describe the problem\r\nI am trying to load and run graph model (mobilenet v2) with Tensorflow C-API. Python script downloaded from following link works on my desktop.\r\n`https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py`\r\nThe c-api library and framework library are downloaded via Bazel:\r\n```\r\nnew_http_archive(\r\n    name = \"libtensorflow\",\r\n    build_file = \"third_party/libtensorflow.BUILD\",\r\n    sha256 = \"05f5b543d5054f3c1ddc4f4caff7e3a6a96985579ef2d3dd9340ab94b1262f54\",\r\n    \r\n    url = \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.8.0.tar.gz\",\r\n    )\r\ncc_library(\r\n    name = \"libtensorflow\",\r\n    srcs = [\r\n        \"lib/libtensorflow.so\",\r\n        \"lib/libtensorflow_framework.so\",\r\n    ],\r\n    hdrs = [\r\n        \"include/tensorflow/c/c_api.h\",\r\n        \"include/tensorflow/c/c_api_experimental.h\",\r\n    ],\r\n    strip_include_prefix = \"include\",\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\nWith following code calling into C-API I see non-changed output buffer after SessionRun() and dubious console log:\r\n```\r\n2018-05-15 15:42:11.572574: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-05-15 15:42:11.695012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 9.26GiB\r\n2018-05-15 15:42:11.695035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-15 15:42:12.104753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-15 15:42:12.104773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-15 15:42:12.104777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-15 15:42:12.104976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8958 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nHello from TensorFlow C library version 1.8.0\r\ninf starting\r\n2018-05-15 15:42:12.195162: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\n\r\n```\r\n\r\nThe model is downloaded here: `https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet`\r\n`https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz`\r\n\r\nThe image is downloaded here:\r\n`https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb`\r\nand processed with python code for loading in c++:\r\n```\r\nimport numpy as np\r\nfrom PIL import Image\r\nimg = np.array(Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\r\nreimg=np.reshape(img, [224,672])\r\nnp.savetxt('panda.ndarray',reimg)\r\n```\r\n\r\n### Source code / logs\r\n```#include <string.h>\r\n#include <fstream>\r\n#include <iostream>\r\n#include <memory>\r\n#include <string>\r\n\r\n#include \"gflags/gflags.h\"\r\n#include \"tensorflow/c/c_api.h\"\r\n\r\n#define ASSERT(expr, ...)      \\\r\n  if (!(expr)) {               \\\r\n    char buf[1024];            \\\r\n    sprintf(buf, __VA_ARGS__); \\\r\n    std::cerr << buf;          \\\r\n    std::abort();              \\\r\n  }\r\n\r\nDEFINE_string(model_filename,\r\n              \"mobilenet_v2_1.4_224_frozen.pb\",\r\n              \"Filename for model to load\");\r\n\r\nDEFINE_int32(num_repeat, 10, \"Run inference many times.\");\r\n\r\nDEFINE_string(np_filename, \"panda.ndarray\",\r\n              \"Filename for (text) image file to load\");\r\n\r\nvoid free_buffer(void* data, size_t length) { free(data); }\r\n\r\nvoid Deallocator(void* data, size_t length, void* arg) { free(data); }\r\n\r\nTF_Buffer* read_file(const char* file) {\r\n  FILE* f = fopen(file, \"rb\");\r\n  ASSERT(f != nullptr, \"Model File Not Ready\");\r\n  fseek(f, 0, SEEK_END);\r\n  long fsize = ftell(f);\r\n  fseek(f, 0, SEEK_SET);  // same as rewind(f);\r\n\r\n  void* data = malloc(fsize);\r\n  std::ignore = fread(data, fsize, 1, f);\r\n  fclose(f);\r\n\r\n  TF_Buffer* buf = TF_NewBuffer();\r\n  buf->data = data;\r\n  buf->length = fsize;\r\n  buf->data_deallocator = free_buffer;\r\n  return buf;\r\n}\r\n\r\nint main(int argc, char** argv) {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n\r\n  // Creates buffer for graph def by reading pb file\r\n  std::unique_ptr<TF_Buffer, std::function<void(TF_Buffer*)>> p_model_buffer(\r\n      read_file(FLAGS_model_filename.c_str()), [](TF_Buffer* p) { TF_DeleteBuffer(p); });\r\n  ASSERT(\r\n      p_model_buffer != nullptr && p_model_buffer->data != nullptr && p_model_buffer->length != 0,\r\n      \"Reading Graph Model pb file Failure\");\r\n\r\n  // Creates error status for checking\r\n  std::unique_ptr<TF_Status, std::function<void(TF_Status*)>> p_status(\r\n      TF_NewStatus(), [](TF_Status* p) { TF_DeleteStatus(p); });\r\n  ASSERT(p_status != nullptr, \"Status Creation Failure\");\r\n\r\n  // Creates graph instance\r\n  std::unique_ptr<TF_Graph, std::function<void(TF_Graph*)>> p_graph(\r\n      TF_NewGraph(), [](TF_Graph* p) { TF_DeleteGraph(p); });\r\n  ASSERT(p_graph != nullptr, \"Graph Creation Failure\");\r\n\r\n  // Creates session option\r\n  std::unique_ptr<TF_SessionOptions, std::function<void(TF_SessionOptions*)>> p_session_opts(\r\n      TF_NewSessionOptions(), [](TF_SessionOptions* p) { TF_DeleteSessionOptions(p); });\r\n  ASSERT(p_session_opts != nullptr, \"Create SessionOptions Failure\");\r\n\r\n  // Creates options for importing graph_def\r\n  std::unique_ptr<TF_ImportGraphDefOptions, std::function<void(TF_ImportGraphDefOptions*)>>\r\n      p_import_graph_def_options(TF_NewImportGraphDefOptions(), [](TF_ImportGraphDefOptions* p) {\r\n        TF_DeleteImportGraphDefOptions(p);\r\n      });\r\n  ASSERT(p_import_graph_def_options != nullptr, \"Create Import Graph Def Option Failure\");\r\n\r\n  // Imports graph def into graph\r\n  TF_GraphImportGraphDef(p_graph.get(), p_model_buffer.get(), p_import_graph_def_options.get(),\r\n                         p_status.get());\r\n  ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Import Failure %s\",\r\n         TF_Message(p_status.get()));\r\n\r\n  // Grab output ops from importing result\r\n  TF_Operation* ops_in = TF_GraphOperationByName(p_graph.get(), \"input\");\r\n  ASSERT(ops_in != nullptr, \"Getting Input Operation Failure\");\r\n  TF_Operation* ops_out =\r\n      TF_GraphOperationByName(p_graph.get(), \"MobilenetV2/Predictions/Reshape_1\");\r\n  ASSERT(ops_out != nullptr, \"Getting Output Operation Failure\");\r\n  TF_Output tf_output_in{.oper = ops_in, .index = 0};\r\n  TF_Output tf_output_out{.oper = ops_out, .index = 0};\r\n\r\n  // Create Tensors for input and output\r\n  std::vector<std::shared_ptr<TF_Tensor>> input_tensor_vector;\r\n  std::vector<std::shared_ptr<TF_Tensor>> output_tensor_vector;\r\n  // Input tensor is fp32 image of 224x224 with bgr channels, as\r\n  // img = np.array(PIL.Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\r\n  int64_t input_dims[] = {1, 224, 224, 3};\r\n  ASSERT(sizeof(float) == 4, \"Float32 should be 4 bytes.\");\r\n  size_t input_num_bytes = 224 * 224 * 3 * sizeof(float);\r\n  // Output tensor is fp32 1001x1 one-hot for classes of objects\r\n  int64_t output_dims[] = {1, 1001};\r\n  size_t output_num_bytes = 1001 * sizeof(float);\r\n\r\n  float* input_buffer = static_cast<float*>(malloc(input_num_bytes));\r\n  ASSERT(input_buffer != nullptr, \"Input Tensor Memory Allocation Failure.\");\r\n  TF_Tensor* input_tensor =\r\n      TF_NewTensor(TF_FLOAT, input_dims, 4, input_buffer, input_num_bytes, &Deallocator, 0);\r\n  ASSERT(input_tensor != nullptr, \"Input Tensor Creation Failure\");\r\n  input_tensor_vector.push_back(\r\n      std::shared_ptr<TF_Tensor>(input_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\r\n\r\n  float* output_buffer = static_cast<float*>(malloc(output_num_bytes));\r\n  ASSERT(output_buffer != nullptr, \"Output Tensor Memory Allocation Failure.\");\r\n  memset(output_buffer, 0, output_num_bytes);\r\n  TF_Tensor* output_tensor =\r\n      TF_NewTensor(TF_FLOAT, output_dims, 2, output_buffer, output_num_bytes, &Deallocator, 0);\r\n  ASSERT(output_tensor != nullptr, \"Output Tensor Creation Failure\");\r\n  output_tensor_vector.push_back(\r\n      std::shared_ptr<TF_Tensor>(output_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\r\n\r\n  // Prepare input image data\r\n  {\r\n    std::ifstream img(FLAGS_np_filename);\r\n    ASSERT(img, \"Read input image tensor failure.\");\r\n    float* p_data = static_cast<float*>(TF_TensorData(input_tensor_vector[0].get()));\r\n    for (int i = 0; i < 224; ++i) {\r\n      for (int j = 0; j < 224 * 3; ++j) {\r\n        int offset = i * 224 * 3 + j;\r\n        img >> p_data[offset];\r\n      }\r\n    }\r\n  }\r\n  {\r\n    // Initializes config proto with magic numbers got from following python code:\r\n    // config = tf.ConfigProto(allow_soft_placement = True)\r\n    // serialized = config.SerializeToString() # '8\\x01'\r\n    char config_proto[] = {'8', 0x01};\r\n    TF_SetConfig(p_session_opts.get(), config_proto, 2, p_status.get());\r\n    ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Set Config Failure %s\",\r\n           TF_Message(p_status.get()));\r\n\r\n    // Creates session instance for loaded graph model\r\n    std::unique_ptr<TF_Session, std::function<void(TF_Session*)>> p_session(\r\n        TF_NewSession(p_graph.get(), p_session_opts.get(), p_status.get()), [&](TF_Session* p) {\r\n          TF_CloseSession(p, p_status.get());\r\n          TF_DeleteSession(p, p_status.get());\r\n        });\r\n    ASSERT(p_session != nullptr, \"Session Creation Failure %s\", TF_Message(p_status.get()));\r\n\r\n    // Empty RunOptions for inference.\r\n    TF_Buffer* run_options = nullptr;\r\n    constexpr int ninputs = 1;\r\n    constexpr int noutputs = 1;\r\n    constexpr int ntargets = 0;\r\n    TF_Tensor* output_tensors[1] = {output_tensor_vector[0].get()};\r\n    TF_Tensor* const input_tensors[1] = {input_tensor_vector[0].get()};\r\n\r\n    std::cerr << \"inf starting\" << std::endl;\r\n    for (int i = 0; i < FLAGS_num_repeat; ++i) {\r\n      TF_SessionRun(p_session.get(), run_options,\r\n                    // Input tensors\r\n                    &tf_output_in, input_tensors, ninputs,\r\n                    // Output tensors\r\n                    &tf_output_out, output_tensors, noutputs,\r\n                    // Target operations\r\n                    nullptr, ntargets,\r\n                    // RunMetadata\r\n                    nullptr,  // only valid run_metadata value\r\n                    p_status.get());\r\n      ASSERT(p_status != nullptr, \"Inference Failure %s\", TF_Message(p_status.get()));\r\n\r\n      // print out max prob and corresponding idx\r\n      const float* p_data = static_cast<const float*>(TF_TensorData(output_tensor_vector[0].get()));\r\n      float v_max = p_data[0];\r\n      int i_max = 0;\r\n      for (int i = 1; i < 1001; ++i) {\r\n        if (p_data[i] > v_max) {\r\n          v_max = p_data[i];\r\n          i_max = i;\r\n        }\r\n      }\r\n      std::cout << \"Max value of \" << v_max << \" at \" << i_max << std::endl;\r\n    }\r\n  }\r\n\r\n  return 0;\r\n}\r\n```"}