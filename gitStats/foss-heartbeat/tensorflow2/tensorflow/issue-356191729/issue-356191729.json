{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22005", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22005/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22005/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22005/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22005", "id": 356191729, "node_id": "MDU6SXNzdWUzNTYxOTE3Mjk=", "number": 22005, "title": "Running inference with TensorRT optimized protobuf using C++ API", "user": {"login": "fferroni", "id": 16327442, "node_id": "MDQ6VXNlcjE2MzI3NDQy", "avatar_url": "https://avatars1.githubusercontent.com/u/16327442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fferroni", "html_url": "https://github.com/fferroni", "followers_url": "https://api.github.com/users/fferroni/followers", "following_url": "https://api.github.com/users/fferroni/following{/other_user}", "gists_url": "https://api.github.com/users/fferroni/gists{/gist_id}", "starred_url": "https://api.github.com/users/fferroni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fferroni/subscriptions", "organizations_url": "https://api.github.com/users/fferroni/orgs", "repos_url": "https://api.github.com/users/fferroni/repos", "events_url": "https://api.github.com/users/fferroni/events{/privacy}", "received_events_url": "https://api.github.com/users/fferroni/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-09-01T10:27:09Z", "updated_at": "2018-11-13T09:31:38Z", "closed_at": "2018-09-19T16:58:16Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.1</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.15.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0 / 7.0</li>\n<li><strong>GPU model and memory</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I successfully build from source TF 1.10.1 with TensorRT 3.04 support. Using the Python <code>tensorflow.contrib.tensorrt</code> API, I can create a protobuf that contains some TRTEngineOp operations, giving me a substantial inference speed-up.<br>\nHowever, now I need to run this optimized protobuf via a C++ API. I build the required .so libraries for the C++ inference API (such as libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so). I can successfully load and run inference on the native TF protobuf, but not on the TRT optimized protobuf.<br>\nIt gives me an error saying that <code>TRTEngineOp not available.</code> Looking through Stackoverflow, I found some suggestions to use <code>TF_LoadLibrary</code> in the C API to load the <code>python/ops/_trt_engine_op.so</code> available in <code>tensorflow/contrib/tensorrt</code>. When I do this, the <code>TRTEngineOp</code> not available error disappears, but now it complains that it has no registered kernels. I noticed that the <code>python/ops/_trt_engine_op.so</code> Bazel target does not include any kernel definition, which explains why the TF_LoadLibrary doesn't find any kernels I guess. I am wondering how the Python API loads this. Can anyone help? Is it possible to run TensorRT optimized protobufs in C++ ?</p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Create an optimized protobuf in python, following for example this blog from <a href=\"https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html\" rel=\"nofollow\">Google</a></li>\n</ol>\n<pre><code># Reserve memory for TensorRT inference engine\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = number_between_0_and_1)\n...  \ntrt_graph = trt.create_inference_graph(\n                 input_graph_def = frozen_graph_def, \n                 outputs = output_node_name,\n                 max_batch_size=batch_size,\n                 max_workspace_size_bytes=workspace_size,\n                 precision_mode=precision)  # Get optimized graph\n</code></pre>\n<p>then save as protobuf file.</p>\n<ol start=\"2\">\n<li>Load optimized protobuf in C++, where we have access to the libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so precompiled.</li>\n</ol>\n<pre><code>#include &lt;tensorflow/core/graph/graph.h&gt;\n#include &lt;tensorflow/core/graph/default_device.h&gt;\n#include &lt;cuda_runtime.h&gt;\n#include tensorflow/c/c_api.h\n\nauto status = TF_NewStatus();\nTF_LoadLibrary(getPathToTrtEngineOpSo(), status);\n\ntensorflow::SessionOptions sessionOptions;\nsessionOptions.config.set_allow_soft_placement(getAllowSoftPlacement());\nsessionOptions.config.set_log_device_placement(getLogDevicePlacement());\nauto gpu_options = sessionOptions.config.mutable_gpu_options();\ngpu_options-&gt;set_allow_growth(getAllowMemoryGrowth());\ngpu_options-&gt;set_per_process_gpu_memory_fraction(getMemoryFraction());\n_session.reset(tensorflow::NewSession(sessionOptions));\n\ntensorflow::GraphDef graphDef;\ncheckStatusTF(tensorflow::ReadBinaryProto(tensorflow::Env::Default(),\n                                                  getModel(),\n                                                  &amp;graphDef),\n                      _logger);\ntensorflow::graph::SetDefaultDevice(getDevice(), &amp;graphDef);\ncheckStatusTF(_session-&gt;Create(graphDef), _logger);\n</code></pre>\n<p>followed by the appropriate run call.</p>\n<p>The libtensorflow_cc.so, libtensorflow_framework.so and libtensorflow.so are unmodified targets from Tensorflow repostory r1.10.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.10.1\nPython version: 3.5\nBazel version (if compiling from source): 0.15.1\nGCC/Compiler version (if compiling from source): 5\nCUDA/cuDNN version: 9.0 / 7.0\nGPU model and memory:\n\nDescribe the problem\nI successfully build from source TF 1.10.1 with TensorRT 3.04 support. Using the Python tensorflow.contrib.tensorrt API, I can create a protobuf that contains some TRTEngineOp operations, giving me a substantial inference speed-up.\nHowever, now I need to run this optimized protobuf via a C++ API. I build the required .so libraries for the C++ inference API (such as libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so). I can successfully load and run inference on the native TF protobuf, but not on the TRT optimized protobuf.\nIt gives me an error saying that TRTEngineOp not available. Looking through Stackoverflow, I found some suggestions to use TF_LoadLibrary in the C API to load the python/ops/_trt_engine_op.so available in tensorflow/contrib/tensorrt. When I do this, the TRTEngineOp not available error disappears, but now it complains that it has no registered kernels. I noticed that the python/ops/_trt_engine_op.so Bazel target does not include any kernel definition, which explains why the TF_LoadLibrary doesn't find any kernels I guess. I am wondering how the Python API loads this. Can anyone help? Is it possible to run TensorRT optimized protobufs in C++ ?\nSteps to reproduce\n\nCreate an optimized protobuf in python, following for example this blog from Google\n\n# Reserve memory for TensorRT inference engine\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = number_between_0_and_1)\n...  \ntrt_graph = trt.create_inference_graph(\n                 input_graph_def = frozen_graph_def, \n                 outputs = output_node_name,\n                 max_batch_size=batch_size,\n                 max_workspace_size_bytes=workspace_size,\n                 precision_mode=precision)  # Get optimized graph\n\nthen save as protobuf file.\n\nLoad optimized protobuf in C++, where we have access to the libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so precompiled.\n\n#include <tensorflow/core/graph/graph.h>\n#include <tensorflow/core/graph/default_device.h>\n#include <cuda_runtime.h>\n#include tensorflow/c/c_api.h\n\nauto status = TF_NewStatus();\nTF_LoadLibrary(getPathToTrtEngineOpSo(), status);\n\ntensorflow::SessionOptions sessionOptions;\nsessionOptions.config.set_allow_soft_placement(getAllowSoftPlacement());\nsessionOptions.config.set_log_device_placement(getLogDevicePlacement());\nauto gpu_options = sessionOptions.config.mutable_gpu_options();\ngpu_options->set_allow_growth(getAllowMemoryGrowth());\ngpu_options->set_per_process_gpu_memory_fraction(getMemoryFraction());\n_session.reset(tensorflow::NewSession(sessionOptions));\n\ntensorflow::GraphDef graphDef;\ncheckStatusTF(tensorflow::ReadBinaryProto(tensorflow::Env::Default(),\n                                                  getModel(),\n                                                  &graphDef),\n                      _logger);\ntensorflow::graph::SetDefaultDevice(getDevice(), &graphDef);\ncheckStatusTF(_session->Create(graphDef), _logger);\n\nfollowed by the appropriate run call.\nThe libtensorflow_cc.so, libtensorflow_framework.so and libtensorflow.so are unmodified targets from Tensorflow repostory r1.10.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.15.1\r\n- **GCC/Compiler version (if compiling from source)**: 5\r\n- **CUDA/cuDNN version**: 9.0 / 7.0\r\n- **GPU model and memory**:\r\n\r\n### Describe the problem\r\nI successfully build from source TF 1.10.1 with TensorRT 3.04 support. Using the Python `tensorflow.contrib.tensorrt` API, I can create a protobuf that contains some TRTEngineOp operations, giving me a substantial inference speed-up.\r\nHowever, now I need to run this optimized protobuf via a C++ API. I build the required .so libraries for the C++ inference API (such as libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so). I can successfully load and run inference on the native TF protobuf, but not on the TRT optimized protobuf. \r\nIt gives me an error saying that `TRTEngineOp not available.` Looking through Stackoverflow, I found some suggestions to use `TF_LoadLibrary` in the C API to load the `python/ops/_trt_engine_op.so` available in `tensorflow/contrib/tensorrt`. When I do this, the `TRTEngineOp` not available error disappears, but now it complains that it has no registered kernels. I noticed that the `python/ops/_trt_engine_op.so` Bazel target does not include any kernel definition, which explains why the TF_LoadLibrary doesn't find any kernels I guess. I am wondering how the Python API loads this. Can anyone help? Is it possible to run TensorRT optimized protobufs in C++ ?\r\n\r\n### Steps to reproduce\r\n1. Create an optimized protobuf in python, following for example this blog from [Google](https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html)\r\n\r\n```\r\n# Reserve memory for TensorRT inference engine\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = number_between_0_and_1)\r\n...  \r\ntrt_graph = trt.create_inference_graph(\r\n                 input_graph_def = frozen_graph_def, \r\n                 outputs = output_node_name,\r\n                 max_batch_size=batch_size,\r\n                 max_workspace_size_bytes=workspace_size,\r\n                 precision_mode=precision)  # Get optimized graph\r\n```\r\nthen save as protobuf file.\r\n\r\n2. Load optimized protobuf in C++, where we have access to the libtensorflow.so, libtensorflow_cc.so and libtensorflow_framework.so precompiled.\r\n```\r\n#include <tensorflow/core/graph/graph.h>\r\n#include <tensorflow/core/graph/default_device.h>\r\n#include <cuda_runtime.h>\r\n#include tensorflow/c/c_api.h\r\n\r\nauto status = TF_NewStatus();\r\nTF_LoadLibrary(getPathToTrtEngineOpSo(), status);\r\n\r\ntensorflow::SessionOptions sessionOptions;\r\nsessionOptions.config.set_allow_soft_placement(getAllowSoftPlacement());\r\nsessionOptions.config.set_log_device_placement(getLogDevicePlacement());\r\nauto gpu_options = sessionOptions.config.mutable_gpu_options();\r\ngpu_options->set_allow_growth(getAllowMemoryGrowth());\r\ngpu_options->set_per_process_gpu_memory_fraction(getMemoryFraction());\r\n_session.reset(tensorflow::NewSession(sessionOptions));\r\n\r\ntensorflow::GraphDef graphDef;\r\ncheckStatusTF(tensorflow::ReadBinaryProto(tensorflow::Env::Default(),\r\n                                                  getModel(),\r\n                                                  &graphDef),\r\n                      _logger);\r\ntensorflow::graph::SetDefaultDevice(getDevice(), &graphDef);\r\ncheckStatusTF(_session->Create(graphDef), _logger);\r\n```\r\nfollowed by the appropriate run call.\r\n\r\nThe libtensorflow_cc.so, libtensorflow_framework.so and libtensorflow.so are unmodified targets from Tensorflow repostory r1.10.\r\n\r\n"}