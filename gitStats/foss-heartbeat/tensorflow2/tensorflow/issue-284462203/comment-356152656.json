{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356152656", "html_url": "https://github.com/tensorflow/tensorflow/issues/15628#issuecomment-356152656", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15628", "id": 356152656, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjE1MjY1Ng==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-09T01:39:07Z", "updated_at": "2018-01-09T01:39:07Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3034091\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/denismakogon\">@denismakogon</a> I'd like to have a better understanding of what you'd like to be able to do.</p>\n<p>Currently TF's runtime execution component is pretty much entirely coded in C++.  Communication between distributed instances generally occurs at this level.  The TF level communication protocol is documented in protocol buffer declarations, e.g.:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/master.proto\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/master.proto</a></p>\n<p>Tasks communicate with one another via RPCs that follow these protocols.  The specifics of the RPC system and transport layer don't matter much (other than with respect to performance and reliability :-) ).  gRPC is the only official fully supported transport, but there is support in contrib for sending RecvTensor requests over other RPC interfaces including MPI and verbs.   The intent of the TensorFlow design is to make it relatively easy to substitute another RPC layer by implementing (parts of the) Worker and Master interfaces.  This is configurable at runtime by specifying the any of the available protocols (e.g. 'grpc' but could also be 'grpc+mpi' as <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mpi/mpi_server_lib.cc#L86\">here</a>).</p>\n<p>That said, what is it that you'd like to do, and what's missing from TF to make that happen?</p>", "body_text": "@denismakogon I'd like to have a better understanding of what you'd like to be able to do.\nCurrently TF's runtime execution component is pretty much entirely coded in C++.  Communication between distributed instances generally occurs at this level.  The TF level communication protocol is documented in protocol buffer declarations, e.g.:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/master.proto\nTasks communicate with one another via RPCs that follow these protocols.  The specifics of the RPC system and transport layer don't matter much (other than with respect to performance and reliability :-) ).  gRPC is the only official fully supported transport, but there is support in contrib for sending RecvTensor requests over other RPC interfaces including MPI and verbs.   The intent of the TensorFlow design is to make it relatively easy to substitute another RPC layer by implementing (parts of the) Worker and Master interfaces.  This is configurable at runtime by specifying the any of the available protocols (e.g. 'grpc' but could also be 'grpc+mpi' as here).\nThat said, what is it that you'd like to do, and what's missing from TF to make that happen?", "body": "@denismakogon I'd like to have a better understanding of what you'd like to be able to do.\r\n\r\nCurrently TF's runtime execution component is pretty much entirely coded in C++.  Communication between distributed instances generally occurs at this level.  The TF level communication protocol is documented in protocol buffer declarations, e.g.:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/master.proto\r\n\r\nTasks communicate with one another via RPCs that follow these protocols.  The specifics of the RPC system and transport layer don't matter much (other than with respect to performance and reliability :-) ).  gRPC is the only official fully supported transport, but there is support in contrib for sending RecvTensor requests over other RPC interfaces including MPI and verbs.   The intent of the TensorFlow design is to make it relatively easy to substitute another RPC layer by implementing (parts of the) Worker and Master interfaces.  This is configurable at runtime by specifying the any of the available protocols (e.g. 'grpc' but could also be 'grpc+mpi' as [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mpi/mpi_server_lib.cc#L86)).\r\n\r\nThat said, what is it that you'd like to do, and what's missing from TF to make that happen?\r\n\r\n \r\n\r\n\r\n"}