{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356288831", "html_url": "https://github.com/tensorflow/tensorflow/issues/15628#issuecomment-356288831", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15628", "id": 356288831, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjI4ODgzMQ==", "user": {"login": "denismakogon", "id": 3034091, "node_id": "MDQ6VXNlcjMwMzQwOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/3034091?v=4", "gravatar_id": "", "url": "https://api.github.com/users/denismakogon", "html_url": "https://github.com/denismakogon", "followers_url": "https://api.github.com/users/denismakogon/followers", "following_url": "https://api.github.com/users/denismakogon/following{/other_user}", "gists_url": "https://api.github.com/users/denismakogon/gists{/gist_id}", "starred_url": "https://api.github.com/users/denismakogon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/denismakogon/subscriptions", "organizations_url": "https://api.github.com/users/denismakogon/orgs", "repos_url": "https://api.github.com/users/denismakogon/repos", "events_url": "https://api.github.com/users/denismakogon/events{/privacy}", "received_events_url": "https://api.github.com/users/denismakogon/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-09T13:49:56Z", "updated_at": "2018-01-09T13:49:56Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a> thanks for the reply.</p>\n<p>So, here's what i would like to get from distributed mode:</p>\n<ol>\n<li>\n<p>Make transport truly pluggable. gRPC is the good thing, but i would like to have HTTP as well. So, i do see the need to give to developers a choice of transports as well as the way to implement transports without recompiling TensorFlow from the source (think of Python interface for transport).</p>\n</li>\n<li>\n<p>Provide task encoder that will accept computation task and return its byte representation to send over the transport. Or make task serializable so the transport layer will decide what to do with that.</p>\n</li>\n</ol>\n<p>So, the feature i would like to have is to let developers decide how would that run their computations within the distributed mode, for instance, i don't want to create the bunch of workers that I would have to maintain by my own processes but delegate it to any orchestration engine (serverless, containers, clear containers, etc.)</p>\n<h1>Side note</h1>\n<p>I've found quite interesting note <a href=\"https://www.tensorflow.org/deploy/distributed\" rel=\"nofollow\">here</a>:</p>\n<pre><code>Manually specifying these cluster specifications can be tedious, especially for large clusters. We are working on tools for launching tasks programmatically, e.g. using a cluster manager like Kubernetes. If there are particular cluster managers for which you'd like to see support, please raise a GitHub issue.\n</code></pre>\n<p>My particular area of the interest is serverless technologies, having Docker or Kubernetes as the backend for the cluster is a good thing, but imagine that get computations units (serverless functions) only when you have something to calculate. So, when i create a computation session (<code>with tf.Session(\"http://worker7.example.com:8080\") as sess:</code> where service that stands behind that URL nothing but serverless gateway)  that talks to a serverless function all computations are serialized and send within HTTP request(s) can be deserialized and executed within the serverless function.</p>", "body_text": "@poxvoculi thanks for the reply.\nSo, here's what i would like to get from distributed mode:\n\n\nMake transport truly pluggable. gRPC is the good thing, but i would like to have HTTP as well. So, i do see the need to give to developers a choice of transports as well as the way to implement transports without recompiling TensorFlow from the source (think of Python interface for transport).\n\n\nProvide task encoder that will accept computation task and return its byte representation to send over the transport. Or make task serializable so the transport layer will decide what to do with that.\n\n\nSo, the feature i would like to have is to let developers decide how would that run their computations within the distributed mode, for instance, i don't want to create the bunch of workers that I would have to maintain by my own processes but delegate it to any orchestration engine (serverless, containers, clear containers, etc.)\nSide note\nI've found quite interesting note here:\nManually specifying these cluster specifications can be tedious, especially for large clusters. We are working on tools for launching tasks programmatically, e.g. using a cluster manager like Kubernetes. If there are particular cluster managers for which you'd like to see support, please raise a GitHub issue.\n\nMy particular area of the interest is serverless technologies, having Docker or Kubernetes as the backend for the cluster is a good thing, but imagine that get computations units (serverless functions) only when you have something to calculate. So, when i create a computation session (with tf.Session(\"http://worker7.example.com:8080\") as sess: where service that stands behind that URL nothing but serverless gateway)  that talks to a serverless function all computations are serialized and send within HTTP request(s) can be deserialized and executed within the serverless function.", "body": "@poxvoculi thanks for the reply.\r\n\r\nSo, here's what i would like to get from distributed mode:\r\n\r\n1. Make transport truly pluggable. gRPC is the good thing, but i would like to have HTTP as well. So, i do see the need to give to developers a choice of transports as well as the way to implement transports without recompiling TensorFlow from the source (think of Python interface for transport).\r\n\r\n2. Provide task encoder that will accept computation task and return its byte representation to send over the transport. Or make task serializable so the transport layer will decide what to do with that.\r\n\r\nSo, the feature i would like to have is to let developers decide how would that run their computations within the distributed mode, for instance, i don't want to create the bunch of workers that I would have to maintain by my own processes but delegate it to any orchestration engine (serverless, containers, clear containers, etc.)\r\n\r\n\r\nSide note\r\n=======\r\nI've found quite interesting note [here](https://www.tensorflow.org/deploy/distributed):\r\n```\r\nManually specifying these cluster specifications can be tedious, especially for large clusters. We are working on tools for launching tasks programmatically, e.g. using a cluster manager like Kubernetes. If there are particular cluster managers for which you'd like to see support, please raise a GitHub issue.\r\n```\r\n\r\nMy particular area of the interest is serverless technologies, having Docker or Kubernetes as the backend for the cluster is a good thing, but imagine that get computations units (serverless functions) only when you have something to calculate. So, when i create a computation session (`with tf.Session(\"http://worker7.example.com:8080\") as sess:` where service that stands behind that URL nothing but serverless gateway)  that talks to a serverless function all computations are serialized and send within HTTP request(s) can be deserialized and executed within the serverless function.\r\n"}