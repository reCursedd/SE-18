{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/222872377", "html_url": "https://github.com/tensorflow/tensorflow/pull/2564#issuecomment-222872377", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2564", "id": 222872377, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMjg3MjM3Nw==", "user": {"login": "georgedahl", "id": 15904123, "node_id": "MDQ6VXNlcjE1OTA0MTIz", "avatar_url": "https://avatars2.githubusercontent.com/u/15904123?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgedahl", "html_url": "https://github.com/georgedahl", "followers_url": "https://api.github.com/users/georgedahl/followers", "following_url": "https://api.github.com/users/georgedahl/following{/other_user}", "gists_url": "https://api.github.com/users/georgedahl/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgedahl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgedahl/subscriptions", "organizations_url": "https://api.github.com/users/georgedahl/orgs", "repos_url": "https://api.github.com/users/georgedahl/repos", "events_url": "https://api.github.com/users/georgedahl/events{/privacy}", "received_events_url": "https://api.github.com/users/georgedahl/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-01T01:57:15Z", "updated_at": "2016-06-01T01:57:15Z", "author_association": "NONE", "body_html": "<p>I took a very quick glance and I believe this PR does the \"wrong\" thing efficiently, however it is a pretty reasonable \"wrong\" thing to do. What I mean is that for sparse updates, I don't think this will behave the same was as treating them as dense would. So doing the same optimization, but treating the updates as sparse will produce different results than treating them as dense, even if all the gradients are the same. This property needs to be documented, because one might somehow assume that using the sparse version would do the same thing as the dense one, just more efficiently when you have sparse updates.</p>\n<p>Essentially, RMSProp and similar algorithms generate an update for each weight at each step and the approach here discards the updates for weights that have a zero gradient on that step. This gives the rarely updated indices a lower learning rate than they would otherwise have in the dense version of the algorithm since \"correct\" RMSprop would divide by gradient statistics and amplify the updates for weights that had many zero gradients. However, that said, for sparse updates this might work ok sometimes and it isn't clear what the best thing to do is, so doing the easy and efficient thing could be worth implementing.</p>\n<p>I am pretty sure Adam in TF for sparse updates has the same problem as exhibited in this PR, and since this PR was for a feature request spawned by the inconsistency between ADAM supporting sparse updates and RMSProp not supporting them, it makes sense to have an analogous behavior. Of course it would be good to add something to the ADAM docs explaining that for sparse updates it only updates weights with non-zero gradient that step and makes no attempt at correcting for this.</p>\n<p>So to summarize, the basic approach of the PR seems OK, but I would like to see the exact algorithm documented better in the sparse case so users can be warned not to think it works the same way as the dense update algorithm. ADAM might also need a doc improvement.</p>", "body_text": "I took a very quick glance and I believe this PR does the \"wrong\" thing efficiently, however it is a pretty reasonable \"wrong\" thing to do. What I mean is that for sparse updates, I don't think this will behave the same was as treating them as dense would. So doing the same optimization, but treating the updates as sparse will produce different results than treating them as dense, even if all the gradients are the same. This property needs to be documented, because one might somehow assume that using the sparse version would do the same thing as the dense one, just more efficiently when you have sparse updates.\nEssentially, RMSProp and similar algorithms generate an update for each weight at each step and the approach here discards the updates for weights that have a zero gradient on that step. This gives the rarely updated indices a lower learning rate than they would otherwise have in the dense version of the algorithm since \"correct\" RMSprop would divide by gradient statistics and amplify the updates for weights that had many zero gradients. However, that said, for sparse updates this might work ok sometimes and it isn't clear what the best thing to do is, so doing the easy and efficient thing could be worth implementing.\nI am pretty sure Adam in TF for sparse updates has the same problem as exhibited in this PR, and since this PR was for a feature request spawned by the inconsistency between ADAM supporting sparse updates and RMSProp not supporting them, it makes sense to have an analogous behavior. Of course it would be good to add something to the ADAM docs explaining that for sparse updates it only updates weights with non-zero gradient that step and makes no attempt at correcting for this.\nSo to summarize, the basic approach of the PR seems OK, but I would like to see the exact algorithm documented better in the sparse case so users can be warned not to think it works the same way as the dense update algorithm. ADAM might also need a doc improvement.", "body": "I took a very quick glance and I believe this PR does the \"wrong\" thing efficiently, however it is a pretty reasonable \"wrong\" thing to do. What I mean is that for sparse updates, I don't think this will behave the same was as treating them as dense would. So doing the same optimization, but treating the updates as sparse will produce different results than treating them as dense, even if all the gradients are the same. This property needs to be documented, because one might somehow assume that using the sparse version would do the same thing as the dense one, just more efficiently when you have sparse updates.\n\nEssentially, RMSProp and similar algorithms generate an update for each weight at each step and the approach here discards the updates for weights that have a zero gradient on that step. This gives the rarely updated indices a lower learning rate than they would otherwise have in the dense version of the algorithm since \"correct\" RMSprop would divide by gradient statistics and amplify the updates for weights that had many zero gradients. However, that said, for sparse updates this might work ok sometimes and it isn't clear what the best thing to do is, so doing the easy and efficient thing could be worth implementing.\n\nI am pretty sure Adam in TF for sparse updates has the same problem as exhibited in this PR, and since this PR was for a feature request spawned by the inconsistency between ADAM supporting sparse updates and RMSProp not supporting them, it makes sense to have an analogous behavior. Of course it would be good to add something to the ADAM docs explaining that for sparse updates it only updates weights with non-zero gradient that step and makes no attempt at correcting for this.\n\nSo to summarize, the basic approach of the PR seems OK, but I would like to see the exact algorithm documented better in the sparse case so users can be warned not to think it works the same way as the dense update algorithm. ADAM might also need a doc improvement.\n"}