{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/379642627", "html_url": "https://github.com/tensorflow/tensorflow/issues/17464#issuecomment-379642627", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17464", "id": 379642627, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTY0MjYyNw==", "user": {"login": "HandsomeHans", "id": 26586545, "node_id": "MDQ6VXNlcjI2NTg2NTQ1", "avatar_url": "https://avatars2.githubusercontent.com/u/26586545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HandsomeHans", "html_url": "https://github.com/HandsomeHans", "followers_url": "https://api.github.com/users/HandsomeHans/followers", "following_url": "https://api.github.com/users/HandsomeHans/following{/other_user}", "gists_url": "https://api.github.com/users/HandsomeHans/gists{/gist_id}", "starred_url": "https://api.github.com/users/HandsomeHans/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HandsomeHans/subscriptions", "organizations_url": "https://api.github.com/users/HandsomeHans/orgs", "repos_url": "https://api.github.com/users/HandsomeHans/repos", "events_url": "https://api.github.com/users/HandsomeHans/events{/privacy}", "received_events_url": "https://api.github.com/users/HandsomeHans/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T06:03:26Z", "updated_at": "2018-04-09T06:09:28Z", "author_association": "NONE", "body_html": "<p>i got a same error when doing sync training in distribution environment.<br>\nSys: Linux Ubuntu 14.04<br>\nTF I install from: pip<br>\nTF version: 1.6.0<br>\npython version: 2.7.14</p>\n<pre><code>      Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\n      Traceback (most recent call last):\n        File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n          self.run()\n        File \"/usr/lib/python2.7/threading.py\", line 754, in run\n          self.__target(*self.__args, **self.__kwargs)\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\n          coord.request_stop(e)\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\n          six.reraise(*sys.exc_info())\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\n          enqueue_callable()\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1259, in _single_operation_run\n          None)\n       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in \n      __exit__\n        c_api.TF_GetCode(self.status.status))\n      CancelledError: Step was cancelled by an explicit call to Session::Close().\n</code></pre>\n<p><strong>and part of my codes is:</strong></p>\n<pre><code>      class _LoggerHook(tf.train.SessionRunHook):\n            def begin(self):\n                self._step = 0\n                self._start_time = time.time()\n                self._total_loss = 0\n        \n            def before_run(self, run_context):\n                self._step += 1\n                return tf.train.SessionRunArgs(loss)\n        \n            def after_run(self, run_context, run_values):\n                loss_value = run_values.results\n                self._total_loss += loss_value\n                if self._step % FLAGS.log_frequency == 0:\n                    current_time = time.time()\n                    duration = current_time - self._start_time\n                    self._start_time = current_time\n                    if self._step==0:\n                        avg_loss = loss_value\n                    else:\n                        avg_loss = self._total_loss/self._step\n                    eg_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n                    sec_per_batch = float(duration / FLAGS.log_frequency)\n                    print('%s: training step %d cur loss = %.4f avg loss = %.4f (%.1f images/sec %.3f sec/batch)'\n                          % (datetime.now(), self._step, loss_value, avg_loss, eg_per_sec, sec_per_batch))\n        \n        all_hooks=[tf.train.NanTensorHook(loss), tf.train.StopAtStepHook(last_step=FLAGS.max_steps), _LoggerHook()]\n        if FLAGS.issync:\n            all_hooks.append(sync_replicas_hook)\n        if FLAGS.debug:\n            all_hooks.append(tfdbg.LocalCLIDebugHook(ui_type='curses'))\n        if FLAGS.finetune:\n            print('Finetune from %s'%FLAGS.finetune)\n            saver = tf.train.Saver()\n        config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\n        config.gpu_options.allow_growth=True\n        with tf.train.MonitoredTrainingSession(\n                master=server.target,\n                is_chief=(FLAGS.task_index == 0),\n                checkpoint_dir=FLAGS.model_dir,\n                hooks=all_hooks,\n                config=config,\n                save_summaries_steps=100,\n                save_summaries_secs=None,\n                log_step_count_steps=None) as sess:\n            if FLAGS.finetune:\n                print('Load Pretrained model')\n                ckpt = tf.train.get_checkpoint_state(FLAGS.finetune)\n                if ckpt and ckpt.model_checkpoint_path:\n                    saver.restore(sess, ckpt.model_checkpoint_path)\n                print('-------------------------')\n            while not sess.should_stop():\n                sess.run(train_op)\n</code></pre>", "body_text": "i got a same error when doing sync training in distribution environment.\nSys: Linux Ubuntu 14.04\nTF I install from: pip\nTF version: 1.6.0\npython version: 2.7.14\n      Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\n      Traceback (most recent call last):\n        File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n          self.run()\n        File \"/usr/lib/python2.7/threading.py\", line 754, in run\n          self.__target(*self.__args, **self.__kwargs)\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\n          coord.request_stop(e)\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\n          six.reraise(*sys.exc_info())\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\n          enqueue_callable()\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1259, in _single_operation_run\n          None)\n       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in \n      __exit__\n        c_api.TF_GetCode(self.status.status))\n      CancelledError: Step was cancelled by an explicit call to Session::Close().\n\nand part of my codes is:\n      class _LoggerHook(tf.train.SessionRunHook):\n            def begin(self):\n                self._step = 0\n                self._start_time = time.time()\n                self._total_loss = 0\n        \n            def before_run(self, run_context):\n                self._step += 1\n                return tf.train.SessionRunArgs(loss)\n        \n            def after_run(self, run_context, run_values):\n                loss_value = run_values.results\n                self._total_loss += loss_value\n                if self._step % FLAGS.log_frequency == 0:\n                    current_time = time.time()\n                    duration = current_time - self._start_time\n                    self._start_time = current_time\n                    if self._step==0:\n                        avg_loss = loss_value\n                    else:\n                        avg_loss = self._total_loss/self._step\n                    eg_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n                    sec_per_batch = float(duration / FLAGS.log_frequency)\n                    print('%s: training step %d cur loss = %.4f avg loss = %.4f (%.1f images/sec %.3f sec/batch)'\n                          % (datetime.now(), self._step, loss_value, avg_loss, eg_per_sec, sec_per_batch))\n        \n        all_hooks=[tf.train.NanTensorHook(loss), tf.train.StopAtStepHook(last_step=FLAGS.max_steps), _LoggerHook()]\n        if FLAGS.issync:\n            all_hooks.append(sync_replicas_hook)\n        if FLAGS.debug:\n            all_hooks.append(tfdbg.LocalCLIDebugHook(ui_type='curses'))\n        if FLAGS.finetune:\n            print('Finetune from %s'%FLAGS.finetune)\n            saver = tf.train.Saver()\n        config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\n        config.gpu_options.allow_growth=True\n        with tf.train.MonitoredTrainingSession(\n                master=server.target,\n                is_chief=(FLAGS.task_index == 0),\n                checkpoint_dir=FLAGS.model_dir,\n                hooks=all_hooks,\n                config=config,\n                save_summaries_steps=100,\n                save_summaries_secs=None,\n                log_step_count_steps=None) as sess:\n            if FLAGS.finetune:\n                print('Load Pretrained model')\n                ckpt = tf.train.get_checkpoint_state(FLAGS.finetune)\n                if ckpt and ckpt.model_checkpoint_path:\n                    saver.restore(sess, ckpt.model_checkpoint_path)\n                print('-------------------------')\n            while not sess.should_stop():\n                sess.run(train_op)", "body": " i got a same error when doing sync training in distribution environment.\r\nSys: Linux Ubuntu 14.04\r\nTF I install from: pip\r\nTF version: 1.6.0\r\npython version: 2.7.14\r\n\r\n\r\n          Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\n          Traceback (most recent call last):\r\n            File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n              self.run()\r\n            File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n              self.__target(*self.__args, **self.__kwargs)\r\n            File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n              coord.request_stop(e)\r\n            File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n              six.reraise(*sys.exc_info())\r\n            File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n              enqueue_callable()\r\n            File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1259, in _single_operation_run\r\n              None)\r\n           File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in \r\n          __exit__\r\n            c_api.TF_GetCode(self.status.status))\r\n          CancelledError: Step was cancelled by an explicit call to Session::Close().\r\n\r\n\r\n**and part of my codes is:**\r\n\r\n          class _LoggerHook(tf.train.SessionRunHook):\r\n                def begin(self):\r\n                    self._step = 0\r\n                    self._start_time = time.time()\r\n                    self._total_loss = 0\r\n            \r\n                def before_run(self, run_context):\r\n                    self._step += 1\r\n                    return tf.train.SessionRunArgs(loss)\r\n            \r\n                def after_run(self, run_context, run_values):\r\n                    loss_value = run_values.results\r\n                    self._total_loss += loss_value\r\n                    if self._step % FLAGS.log_frequency == 0:\r\n                        current_time = time.time()\r\n                        duration = current_time - self._start_time\r\n                        self._start_time = current_time\r\n                        if self._step==0:\r\n                            avg_loss = loss_value\r\n                        else:\r\n                            avg_loss = self._total_loss/self._step\r\n                        eg_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\r\n                        sec_per_batch = float(duration / FLAGS.log_frequency)\r\n                        print('%s: training step %d cur loss = %.4f avg loss = %.4f (%.1f images/sec %.3f sec/batch)'\r\n                              % (datetime.now(), self._step, loss_value, avg_loss, eg_per_sec, sec_per_batch))\r\n            \r\n            all_hooks=[tf.train.NanTensorHook(loss), tf.train.StopAtStepHook(last_step=FLAGS.max_steps), _LoggerHook()]\r\n            if FLAGS.issync:\r\n                all_hooks.append(sync_replicas_hook)\r\n            if FLAGS.debug:\r\n                all_hooks.append(tfdbg.LocalCLIDebugHook(ui_type='curses'))\r\n            if FLAGS.finetune:\r\n                print('Finetune from %s'%FLAGS.finetune)\r\n                saver = tf.train.Saver()\r\n            config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\r\n            config.gpu_options.allow_growth=True\r\n            with tf.train.MonitoredTrainingSession(\r\n                    master=server.target,\r\n                    is_chief=(FLAGS.task_index == 0),\r\n                    checkpoint_dir=FLAGS.model_dir,\r\n                    hooks=all_hooks,\r\n                    config=config,\r\n                    save_summaries_steps=100,\r\n                    save_summaries_secs=None,\r\n                    log_step_count_steps=None) as sess:\r\n                if FLAGS.finetune:\r\n                    print('Load Pretrained model')\r\n                    ckpt = tf.train.get_checkpoint_state(FLAGS.finetune)\r\n                    if ckpt and ckpt.model_checkpoint_path:\r\n                        saver.restore(sess, ckpt.model_checkpoint_path)\r\n                    print('-------------------------')\r\n                while not sess.should_stop():\r\n                    sess.run(train_op)"}