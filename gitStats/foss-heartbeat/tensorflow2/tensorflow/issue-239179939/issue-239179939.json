{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11113", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11113/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11113/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11113/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11113", "id": 239179939, "node_id": "MDU6SXNzdWUyMzkxNzk5Mzk=", "number": 11113, "title": "Add cosine annealing for learning rate decay", "user": {"login": "adler-j", "id": 2202312, "node_id": "MDQ6VXNlcjIyMDIzMTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/2202312?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adler-j", "html_url": "https://github.com/adler-j", "followers_url": "https://api.github.com/users/adler-j/followers", "following_url": "https://api.github.com/users/adler-j/following{/other_user}", "gists_url": "https://api.github.com/users/adler-j/gists{/gist_id}", "starred_url": "https://api.github.com/users/adler-j/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adler-j/subscriptions", "organizations_url": "https://api.github.com/users/adler-j/orgs", "repos_url": "https://api.github.com/users/adler-j/repos", "events_url": "https://api.github.com/users/adler-j/events{/privacy}", "received_events_url": "https://api.github.com/users/adler-j/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-06-28T14:19:58Z", "updated_at": "2018-06-13T01:59:47Z", "closed_at": "2018-06-13T01:59:47Z", "author_association": "NONE", "body_html": "<p><a href=\"https://openreview.net/pdf?id=Skq89Scxx\" rel=\"nofollow\">SGDR: Stochastic Gradient Descent With Warm Restarts</a>, proposes decaying the learning rate according to</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2202312/27641761-1bf302c8-5c1d-11e7-8d4b-15988701ff3f.png\"><img src=\"https://user-images.githubusercontent.com/2202312/27641761-1bf302c8-5c1d-11e7-8d4b-15988701ff3f.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>where <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2202312/27641775-28b4e0a8-5c1d-11e7-8adc-1dcaea55d77c.png\"><img src=\"https://user-images.githubusercontent.com/2202312/27641775-28b4e0a8-5c1d-11e7-8adc-1dcaea55d77c.png\" alt=\"image\" style=\"max-width:100%;\"></a> is the minimum step length, <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2202312/27641798-3be6a80a-5c1d-11e7-9b41-d3c25b0b2b96.png\"><img src=\"https://user-images.githubusercontent.com/2202312/27641798-3be6a80a-5c1d-11e7-9b41-d3c25b0b2b96.png\" alt=\"image\" style=\"max-width:100%;\"></a> is the maximum step length, <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2202312/27641818-4686daf0-5c1d-11e7-8991-533ce8710e8f.png\"><img src=\"https://user-images.githubusercontent.com/2202312/27641818-4686daf0-5c1d-11e7-8991-533ce8710e8f.png\" alt=\"image\" style=\"max-width:100%;\"></a> is the global step and <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2202312/27641841-5594f2ac-5c1d-11e7-9f54-e3cc8ccd1566.png\"><img src=\"https://user-images.githubusercontent.com/2202312/27641841-5594f2ac-5c1d-11e7-9f54-e3cc8ccd1566.png\" alt=\"image\" style=\"max-width:100%;\"></a> is the maximum number of iterations.</p>\n<p>I've personally found this strategy to  be easy to use given that the number of hyperparameters is relatively small and results are good.</p>\n<p>Is this something we want added to tensorflow? Would you accept submissions?</p>", "body_text": "SGDR: Stochastic Gradient Descent With Warm Restarts, proposes decaying the learning rate according to\n\nwhere  is the minimum step length,  is the maximum step length,  is the global step and  is the maximum number of iterations.\nI've personally found this strategy to  be easy to use given that the number of hyperparameters is relatively small and results are good.\nIs this something we want added to tensorflow? Would you accept submissions?", "body": "[SGDR: Stochastic Gradient Descent With Warm Restarts](https://openreview.net/pdf?id=Skq89Scxx), proposes decaying the learning rate according to\r\n\r\n![image](https://user-images.githubusercontent.com/2202312/27641761-1bf302c8-5c1d-11e7-8d4b-15988701ff3f.png)\r\n\r\n\r\nwhere ![image](https://user-images.githubusercontent.com/2202312/27641775-28b4e0a8-5c1d-11e7-8adc-1dcaea55d77c.png) is the minimum step length, ![image](https://user-images.githubusercontent.com/2202312/27641798-3be6a80a-5c1d-11e7-9b41-d3c25b0b2b96.png) is the maximum step length, ![image](https://user-images.githubusercontent.com/2202312/27641818-4686daf0-5c1d-11e7-8991-533ce8710e8f.png) is the global step and ![image](https://user-images.githubusercontent.com/2202312/27641841-5594f2ac-5c1d-11e7-9f54-e3cc8ccd1566.png) is the maximum number of iterations.\r\n\r\nI've personally found this strategy to  be easy to use given that the number of hyperparameters is relatively small and results are good.\r\n\r\nIs this something we want added to tensorflow? Would you accept submissions?\r\n\r\n\r\n\r\n"}