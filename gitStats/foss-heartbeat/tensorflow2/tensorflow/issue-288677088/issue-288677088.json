{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16135", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16135/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16135/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16135/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16135", "id": 288677088, "node_id": "MDU6SXNzdWUyODg2NzcwODg=", "number": 16135, "title": "Distributed Tensorflow  using MPI", "user": {"login": "abidmalikwaterloo", "id": 447056, "node_id": "MDQ6VXNlcjQ0NzA1Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/447056?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abidmalikwaterloo", "html_url": "https://github.com/abidmalikwaterloo", "followers_url": "https://api.github.com/users/abidmalikwaterloo/followers", "following_url": "https://api.github.com/users/abidmalikwaterloo/following{/other_user}", "gists_url": "https://api.github.com/users/abidmalikwaterloo/gists{/gist_id}", "starred_url": "https://api.github.com/users/abidmalikwaterloo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abidmalikwaterloo/subscriptions", "organizations_url": "https://api.github.com/users/abidmalikwaterloo/orgs", "repos_url": "https://api.github.com/users/abidmalikwaterloo/repos", "events_url": "https://api.github.com/users/abidmalikwaterloo/events{/privacy}", "received_events_url": "https://api.github.com/users/abidmalikwaterloo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-15T18:05:52Z", "updated_at": "2018-08-24T08:01:15Z", "closed_at": "2018-01-23T20:26:01Z", "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<p>I have tried stackflow and Google group discussion forum but could  get any reply or comment</p>\n<ol>\n<li>It must be a bug or a feature request.</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nRedhat 7.4</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nfrom source with MPI</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\n1.41</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\n2.7.14</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nGCC 6.0</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\n8.0/6.5</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |<br>\n| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n</li>\n</ul>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID   Type   Process name                             Usage      |<br>\n|=============================================================================|<br>\n|  No running processes found                                                 |<br>\n+-----------------------------------------------------------------------------+</p>\n<ul>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.<br>\nI am using the following  script to launch distributed computing.</p>\n<p>#! /bin/bash</p>\n<p>module load openmpi/3.0.0-gnu</p>\n<p>host=$(hostname -s)<br>\nif [[ $host == \"node06\" ]]; then<br>\necho \"statring Node 6\"<br>\npython tf_dis_2.py --job_name=\"ps\" --task_index=0<br>\nelif [[ $host == \"node07\" ]]; then<br>\necho \"starting Node 7 as worker\"<br>\npython tf_dis_2.py --job_name=\"worker\" --task_index=0<br>\nelif [[ $host == \"node08\" ]]; then<br>\necho \"starting Node 8 as worker\"<br>\npython tf_dis_2.py --job_name=\"worker\" --task_index=1<br>\nfi</p>\n<hr>\n<p>I am running it on slurm  with three nodes.</p>\n<p>srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh</p>\n<p>I am using MPI instead of GPRC.</p>\n<p>I am getting the following message:</p>\n<hr>\n<p>srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh<br>\nstatring Node 6<br>\nstarting Node 8 as worker<br>\nstarting Node 7 as worker<br>\n2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:<br>\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732<br>\npciBusID: 0000:08:00.0<br>\ntotalMemory: 5.57GiB freeMemory: 5.49GiB<br>\n2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)<br>\nE0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22<br>\n2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; node06:2222}<br>\n2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; node07:2223, 1 -&gt; localhost:2224}<br>\n2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224<br>\n2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:<br>\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732<br>\npciBusID: 0000:08:00.0<br>\ntotalMemory: 5.57GiB freeMemory: 5.49GiB<br>\n2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)<br>\nE0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22<br>\n2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2222}<br>\n2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; node07:2223, 1 -&gt; node08:2224}<br>\n2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222<br>\n2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:<br>\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732<br>\npciBusID: 0000:08:00.0<br>\ntotalMemory: 5.57GiB freeMemory: 5.49GiB<br>\n2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)<br>\nE0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22<br>\n2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; node06:2222}<br>\n2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223, 1 -&gt; node08:2224}<br>\n2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223<br>\nAbid Malik<br>\nExtracting MNIST_data/train-images-idx3-ubyte.gz<br>\nExtracting MNIST_data/train-labels-idx1-ubyte.gz<br>\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz<br>\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz<br>\nVariables initialized ...<br>\nTraceback (most recent call last):<br>\nFile \"tf_dis_2.py\", line 102, in <br>\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 336, in <strong>init</strong><br>\nself._verify_setup()<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 885, in _verify_setup<br>\n\"their device set: %s\" % op)<br>\nValueError: When using replicas, all Variables must have their device set: name: \"weights/Variable\"<br>\nop: \"VariableV2\"<br>\nattr {<br>\nkey: \"container\"<br>\nvalue {<br>\ns: \"\"<br>\n}<br>\n}<br>\nattr {<br>\nkey: \"dtype\"<br>\nvalue {<br>\ntype: DT_FLOAT<br>\n}<br>\n}<br>\nattr {<br>\nkey: \"shape\"<br>\nvalue {<br>\nshape {<br>\ndim {<br>\nsize: 784<br>\n}<br>\ndim {<br>\nsize: 100<br>\n}<br>\n}<br>\n}<br>\n}<br>\nattr {<br>\nkey: \"shared_name\"<br>\nvalue {<br>\ns: \"\"<br>\n}<br>\n}</p>\n<h2>2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed<br>\nExtracting MNIST_data/train-images-idx3-ubyte.gz<br>\nExtracting MNIST_data/train-labels-idx1-ubyte.gz<br>\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz<br>\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz<br>\nVariables initialized ...<br>\nTraceback (most recent call last):<br>\nFile \"tf_dis_2.py\", line 114, in <br>\nwith sv.prepare_or_wait_for_session(server.target) as sess:<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 708, in prepare_or_wait_for_session<br>\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 273, in prepare_session<br>\nconfig=config)<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 205, in _restore_checkpoint<br>\nsaver.restore(sess, ckpt.model_checkpoint_path)<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1666, in restore<br>\n{self.saver_def.filename_tensor_name: save_path})<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run<br>\nrun_metadata_ptr)<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run<br>\noptions, run_metadata)<br>\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed<br>\nsrun: error: node08: task 2: Exited with exit code 1<br>\nsrun: error: node07: task 1: Exited with exit code 1</h2>\n<p>Why is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n</ul>\n<p>``<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import tensorflow as tf<br>\nimport sys<br>\nimport time</p>\n<p>print(\"Abid Malik\")</p>\n<p>parameter_servers = [\"node06:2222\"]<br>\nworkers = [\"node07:2223\",\"node08:2224\"]<br>\ncluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})</p>\n<p>tf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")<br>\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")<br>\nFLAGS = tf.app.flags.FLAGS</p>\n<p>server = tf.train.Server(<br>\ncluster,<br>\njob_name=FLAGS.job_name,<br>\ntask_index=FLAGS.task_index)</p>\n<p>batch_size = 100<br>\nlearning_rate = 0.0005<br>\ntraining_epochs = 20<br>\nlogs_path = \"/tmp/mnist/1\"</p>\n<p>from tensorflow.examples.tutorials.mnist import input_data<br>\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)</p>\n<p>if FLAGS.job_name == \"ps\":<br>\nserver.join()<br>\nelif FLAGS.job_name == \"worker\":</p>\n<pre><code>    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,cluster=cluster)):\n          \n            global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)\n\n          \n    with tf.name_scope('input'):\n          \n              x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\n           \n              y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n\n            \n    tf.set_random_seed(1)\n    with tf.name_scope(\"weights\"):\n                    W1 = tf.Variable(tf.random_normal([784, 100]))\n                    W2 = tf.Variable(tf.random_normal([100, 10]))\n\n           \n    with tf.name_scope(\"biases\"):\n                    b1 = tf.Variable(tf.zeros([100]))\n                    b2 = tf.Variable(tf.zeros([10]))\n\n           \n    with tf.name_scope(\"softmax\"):\n                    # y is our prediction\n                    z2 = tf.add(tf.matmul(x,W1),b1)\n                    a2 = tf.nn.sigmoid(z2)\n                    z3 = tf.add(tf.matmul(a2,W2),b2)\n                    y  = tf.nn.softmax(z3)\n\n           \n    with tf.name_scope('cross_entropy'):\n                    # this is our cost\n                    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n\n         \n    with tf.name_scope('train'):\n</code></pre>\n<p>grad_op = tf.train.GradientDescentOptimizer(learning_rate)<br>\ntrain_op = grad_op.minimize(cross_entropy, global_step=global_step)</p>\n<pre><code>    with tf.name_scope('Accuracy'):\n                    # accuracy\n                    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n    tf.summary.scalar(\"cost\", cross_entropy)\n    tf.summary.scalar(\"accuracy\", accuracy)\n\n    saver = tf.train.Saver()\n   \n    summary_op = tf.summary.merge_all()\n    init_op = tf.global_variables_initializer()\n    print(\"Variables initialized ...\")\n\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)\n\n\n    begin_time = time.time()\n    frequency = 100\n    with sv.prepare_or_wait_for_session(server.target) as sess:\n            # create log writer object (this will log on every machine)\n            writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n\n            # perform training cycles\n            start_time = time.time()\n            for epoch in range(training_epochs):\n\n                    # number of batches in one epoch\n                    batch_count = int(mnist.train.num_examples/batch_size)\n\n                    count = 0\n                    for i in range(batch_count):\n                            batch_x, batch_y = mnist.train.next_batch(batch_size)\n\n                            # perform the operations we defined earlier on batch\n                            _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})\n                            writer.add_summary(summary, step)\n\n                            count += 1\n                            if count % frequency == 0 or i+1 == batch_count:\n                                    elapsed_time = time.time() - start_time\n                                    start_time = time.time()\n                                    print(\"Step: %d,\" % (step+1),\n                                                            \" Epoch: %2d,\" % (epoch+1),\n                                                            \" Batch: %3d of %3d,\" % (i+1, batch_count),\n                                                            \" Cost: %.4f,\" % cost,\n                                                            \" AvgTime: %3.2fms\" % float(elapsed_time*1000/frequency))\n                                    count = 0\n\n\n            print(\"Test-Accuracy: %2.2f\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n            print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\n            print(\"Final Cost: %.4f\" % cost)\n\n    sv.stop()\n    print(\"done\")\n</code></pre>\n<p>``</p>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\nI have tried stackflow and Google group discussion forum but could  get any reply or comment\n\nIt must be a bug or a feature request.\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nRedhat 7.4\n\n\nTensorFlow installed from (source or binary):\nfrom source with MPI\n\n\nTensorFlow version (use command below):\n1.41\n\n\nPython version:\n2.7.14\n\n\nBazel version (if compiling from source):\n\n\nGCC/Compiler version (if compiling from source):\nGCC 6.0\n\n\nCUDA/cuDNN version:\n8.0/6.5\n\n\nGPU model and memory:\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |\n| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |\n+-------------------------------+----------------------+----------------------+\n\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nI am using the following  script to launch distributed computing.\n#! /bin/bash\nmodule load openmpi/3.0.0-gnu\nhost=$(hostname -s)\nif [[ $host == \"node06\" ]]; then\necho \"statring Node 6\"\npython tf_dis_2.py --job_name=\"ps\" --task_index=0\nelif [[ $host == \"node07\" ]]; then\necho \"starting Node 7 as worker\"\npython tf_dis_2.py --job_name=\"worker\" --task_index=0\nelif [[ $host == \"node08\" ]]; then\necho \"starting Node 8 as worker\"\npython tf_dis_2.py --job_name=\"worker\" --task_index=1\nfi\n\nI am running it on slurm  with three nodes.\nsrun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh\nI am using MPI instead of GPRC.\nI am getting the following message:\n\nsrun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh\nstatring Node 6\nstarting Node 8 as worker\nstarting Node 7 as worker\n2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\npciBusID: 0000:08:00.0\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\n2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\nE0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\n2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}\n2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}\n2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224\n2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\npciBusID: 0000:08:00.0\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\n2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\nE0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\n2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\n2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}\n2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\n2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\npciBusID: 0000:08:00.0\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\n2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\nE0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\n2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}\n2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}\n2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\nAbid Malik\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nVariables initialized ...\nTraceback (most recent call last):\nFile \"tf_dis_2.py\", line 102, in \nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 336, in init\nself._verify_setup()\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 885, in _verify_setup\n\"their device set: %s\" % op)\nValueError: When using replicas, all Variables must have their device set: name: \"weights/Variable\"\nop: \"VariableV2\"\nattr {\nkey: \"container\"\nvalue {\ns: \"\"\n}\n}\nattr {\nkey: \"dtype\"\nvalue {\ntype: DT_FLOAT\n}\n}\nattr {\nkey: \"shape\"\nvalue {\nshape {\ndim {\nsize: 784\n}\ndim {\nsize: 100\n}\n}\n}\n}\nattr {\nkey: \"shared_name\"\nvalue {\ns: \"\"\n}\n}\n2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nVariables initialized ...\nTraceback (most recent call last):\nFile \"tf_dis_2.py\", line 114, in \nwith sv.prepare_or_wait_for_session(server.target) as sess:\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 708, in prepare_or_wait_for_session\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 273, in prepare_session\nconfig=config)\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 205, in _restore_checkpoint\nsaver.restore(sess, ckpt.model_checkpoint_path)\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1666, in restore\n{self.saver_def.filename_tensor_name: save_path})\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\nrun_metadata_ptr)\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\noptions, run_metadata)\nFile \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed\nsrun: error: node08: task 2: Exited with exit code 1\nsrun: error: node07: task 1: Exited with exit code 1\nWhy is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\n``\nfrom future import print_function\nimport tensorflow as tf\nimport sys\nimport time\nprint(\"Abid Malik\")\nparameter_servers = [\"node06:2222\"]\nworkers = [\"node07:2223\",\"node08:2224\"]\ncluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\nFLAGS = tf.app.flags.FLAGS\nserver = tf.train.Server(\ncluster,\njob_name=FLAGS.job_name,\ntask_index=FLAGS.task_index)\nbatch_size = 100\nlearning_rate = 0.0005\ntraining_epochs = 20\nlogs_path = \"/tmp/mnist/1\"\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\nif FLAGS.job_name == \"ps\":\nserver.join()\nelif FLAGS.job_name == \"worker\":\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,cluster=cluster)):\n          \n            global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)\n\n          \n    with tf.name_scope('input'):\n          \n              x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\n           \n              y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n\n            \n    tf.set_random_seed(1)\n    with tf.name_scope(\"weights\"):\n                    W1 = tf.Variable(tf.random_normal([784, 100]))\n                    W2 = tf.Variable(tf.random_normal([100, 10]))\n\n           \n    with tf.name_scope(\"biases\"):\n                    b1 = tf.Variable(tf.zeros([100]))\n                    b2 = tf.Variable(tf.zeros([10]))\n\n           \n    with tf.name_scope(\"softmax\"):\n                    # y is our prediction\n                    z2 = tf.add(tf.matmul(x,W1),b1)\n                    a2 = tf.nn.sigmoid(z2)\n                    z3 = tf.add(tf.matmul(a2,W2),b2)\n                    y  = tf.nn.softmax(z3)\n\n           \n    with tf.name_scope('cross_entropy'):\n                    # this is our cost\n                    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n\n         \n    with tf.name_scope('train'):\n\ngrad_op = tf.train.GradientDescentOptimizer(learning_rate)\ntrain_op = grad_op.minimize(cross_entropy, global_step=global_step)\n    with tf.name_scope('Accuracy'):\n                    # accuracy\n                    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n    tf.summary.scalar(\"cost\", cross_entropy)\n    tf.summary.scalar(\"accuracy\", accuracy)\n\n    saver = tf.train.Saver()\n   \n    summary_op = tf.summary.merge_all()\n    init_op = tf.global_variables_initializer()\n    print(\"Variables initialized ...\")\n\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)\n\n\n    begin_time = time.time()\n    frequency = 100\n    with sv.prepare_or_wait_for_session(server.target) as sess:\n            # create log writer object (this will log on every machine)\n            writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n\n            # perform training cycles\n            start_time = time.time()\n            for epoch in range(training_epochs):\n\n                    # number of batches in one epoch\n                    batch_count = int(mnist.train.num_examples/batch_size)\n\n                    count = 0\n                    for i in range(batch_count):\n                            batch_x, batch_y = mnist.train.next_batch(batch_size)\n\n                            # perform the operations we defined earlier on batch\n                            _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})\n                            writer.add_summary(summary, step)\n\n                            count += 1\n                            if count % frequency == 0 or i+1 == batch_count:\n                                    elapsed_time = time.time() - start_time\n                                    start_time = time.time()\n                                    print(\"Step: %d,\" % (step+1),\n                                                            \" Epoch: %2d,\" % (epoch+1),\n                                                            \" Batch: %3d of %3d,\" % (i+1, batch_count),\n                                                            \" Cost: %.4f,\" % cost,\n                                                            \" AvgTime: %3.2fms\" % float(elapsed_time*1000/frequency))\n                                    count = 0\n\n\n            print(\"Test-Accuracy: %2.2f\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n            print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\n            print(\"Final Cost: %.4f\" % cost)\n\n    sv.stop()\n    print(\"done\")\n\n``", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\nI have tried stackflow and Google group discussion forum but could  get any reply or comment\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRedhat 7.4\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom source with MPI\r\n- **TensorFlow version (use command below)**:\r\n1.41\r\n- **Python version**: \r\n2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC 6.0\r\n- **CUDA/cuDNN version**:\r\n8.0/6.5\r\n- **GPU model and memory**:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |\r\n| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI am using the following  script to launch distributed computing.\r\n\r\n\r\n#! /bin/bash\r\n\r\nmodule load openmpi/3.0.0-gnu\r\n\r\nhost=$(hostname -s)\r\nif [[ $host == \"node06\" ]]; then\r\n        echo \"statring Node 6\"\r\n        python tf_dis_2.py --job_name=\"ps\" --task_index=0\r\nelif [[ $host == \"node07\" ]]; then\r\n        echo \"starting Node 7 as worker\"\r\n        python tf_dis_2.py --job_name=\"worker\" --task_index=0\r\nelif [[ $host == \"node08\" ]]; then\r\n        echo \"starting Node 8 as worker\"\r\n        python tf_dis_2.py --job_name=\"worker\" --task_index=1\r\nfi\r\n\r\n-----\r\n\r\nI am running it on slurm  with three nodes.\r\n\r\nsrun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh\r\n\r\nI am using MPI instead of GPRC.\r\n\r\nI am getting the following message:\r\n\r\n---------------------------------------------------\r\nsrun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh\r\nstatring Node 6\r\nstarting Node 8 as worker\r\nstarting Node 7 as worker\r\n2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\r\n2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\r\nE0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\r\n2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}\r\n2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}\r\n2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224\r\n2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\r\n2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\r\nE0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\r\n2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}\r\n2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\r\n2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 5.57GiB freeMemory: 5.49GiB\r\n2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)\r\nE0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22\r\n2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}\r\n2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}\r\n2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\r\nAbid Malik\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nVariables initialized ...\r\nTraceback (most recent call last):\r\n  File \"tf_dis_2.py\", line 102, in <module>\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 336, in __init__\r\n    self._verify_setup()\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 885, in _verify_setup\r\n    \"their device set: %s\" % op)\r\nValueError: When using replicas, all Variables must have their device set: name: \"weights/Variable\"\r\nop: \"VariableV2\"\r\nattr {\r\n  key: \"container\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"shape\"\r\n  value {\r\n    shape {\r\n      dim {\r\n        size: 784\r\n      }\r\n      dim {\r\n        size: 100\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"shared_name\"\r\n  value {\r\n    s: \"\"\r\n  }\r\n}\r\n\r\n2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nVariables initialized ...\r\nTraceback (most recent call last):\r\n  File \"tf_dis_2.py\", line 114, in <module>\r\n    with sv.prepare_or_wait_for_session(server.target) as sess:\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 708, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 273, in prepare_session\r\n    config=config)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 205, in _restore_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1666, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed\r\nsrun: error: node08: task 2: Exited with exit code 1\r\nsrun: error: node07: task 1: Exited with exit code 1\r\n---------------------------------------------------------------------------------\r\n\r\nWhy is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\n``\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport sys\r\nimport time\r\n\r\n\r\nprint(\"Abid Malik\")\r\n\r\n\r\nparameter_servers = [\"node06:2222\"]\r\nworkers = [\"node07:2223\",\"node08:2224\"]\r\ncluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\r\n\r\n\r\n\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n\r\n\r\n\r\n\r\nserver = tf.train.Server(\r\n    cluster,\r\n    job_name=FLAGS.job_name,\r\n    task_index=FLAGS.task_index)\r\n\r\n\r\nbatch_size = 100\r\nlearning_rate = 0.0005\r\ntraining_epochs = 20\r\nlogs_path = \"/tmp/mnist/1\"\r\n\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n\r\n        with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,cluster=cluster)):\r\n              \r\n                global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)\r\n\r\n              \r\n        with tf.name_scope('input'):\r\n              \r\n                  x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\r\n               \r\n                  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\r\n\r\n                \r\n        tf.set_random_seed(1)\r\n        with tf.name_scope(\"weights\"):\r\n                        W1 = tf.Variable(tf.random_normal([784, 100]))\r\n                        W2 = tf.Variable(tf.random_normal([100, 10]))\r\n\r\n               \r\n        with tf.name_scope(\"biases\"):\r\n                        b1 = tf.Variable(tf.zeros([100]))\r\n                        b2 = tf.Variable(tf.zeros([10]))\r\n\r\n               \r\n        with tf.name_scope(\"softmax\"):\r\n                        # y is our prediction\r\n                        z2 = tf.add(tf.matmul(x,W1),b1)\r\n                        a2 = tf.nn.sigmoid(z2)\r\n                        z3 = tf.add(tf.matmul(a2,W2),b2)\r\n                        y  = tf.nn.softmax(z3)\r\n\r\n               \r\n        with tf.name_scope('cross_entropy'):\r\n                        # this is our cost\r\n                        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n\r\n             \r\n        with tf.name_scope('train'):\r\n                       \r\n                                                                                                                                                                                                                                                                \r\n\r\ngrad_op = tf.train.GradientDescentOptimizer(learning_rate)\r\n                        train_op = grad_op.minimize(cross_entropy, global_step=global_step)\r\n\r\n\r\n        with tf.name_scope('Accuracy'):\r\n                        # accuracy\r\n                        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n   \r\n        tf.summary.scalar(\"cost\", cross_entropy)\r\n        tf.summary.scalar(\"accuracy\", accuracy)\r\n\r\n        saver = tf.train.Saver()\r\n       \r\n        summary_op = tf.summary.merge_all()\r\n        init_op = tf.global_variables_initializer()\r\n        print(\"Variables initialized ...\")\r\n\r\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=\"/tmp/train_logs\",global_step=global_step,init_op=init_op)\r\n\r\n\r\n        begin_time = time.time()\r\n        frequency = 100\r\n        with sv.prepare_or_wait_for_session(server.target) as sess:\r\n                # create log writer object (this will log on every machine)\r\n                writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\r\n\r\n                # perform training cycles\r\n                start_time = time.time()\r\n                for epoch in range(training_epochs):\r\n\r\n                        # number of batches in one epoch\r\n                        batch_count = int(mnist.train.num_examples/batch_size)\r\n\r\n                        count = 0\r\n                        for i in range(batch_count):\r\n                                batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n\r\n                                # perform the operations we defined earlier on batch\r\n                                _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})\r\n                                writer.add_summary(summary, step)\r\n\r\n                                count += 1\r\n                                if count % frequency == 0 or i+1 == batch_count:\r\n                                        elapsed_time = time.time() - start_time\r\n                                        start_time = time.time()\r\n                                        print(\"Step: %d,\" % (step+1),\r\n                                                                \" Epoch: %2d,\" % (epoch+1),\r\n                                                                \" Batch: %3d of %3d,\" % (i+1, batch_count),\r\n                                                                \" Cost: %.4f,\" % cost,\r\n                                                                \" AvgTime: %3.2fms\" % float(elapsed_time*1000/frequency))\r\n                                        count = 0\r\n\r\n\r\n                print(\"Test-Accuracy: %2.2f\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\r\n                print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\r\n                print(\"Final Cost: %.4f\" % cost)\r\n\r\n        sv.stop()\r\n        print(\"done\")\r\n                                                                                                                                                                                                                                                                 \r\n\r\n``"}