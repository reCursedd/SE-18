{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4095", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4095/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4095/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4095/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4095", "id": 173825407, "node_id": "MDU6SXNzdWUxNzM4MjU0MDc=", "number": 4095, "title": "How to use pre-trained word embeddings in seq2seq?", "user": {"login": "zhaopku", "id": 20232241, "node_id": "MDQ6VXNlcjIwMjMyMjQx", "avatar_url": "https://avatars1.githubusercontent.com/u/20232241?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhaopku", "html_url": "https://github.com/zhaopku", "followers_url": "https://api.github.com/users/zhaopku/followers", "following_url": "https://api.github.com/users/zhaopku/following{/other_user}", "gists_url": "https://api.github.com/users/zhaopku/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhaopku/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhaopku/subscriptions", "organizations_url": "https://api.github.com/users/zhaopku/orgs", "repos_url": "https://api.github.com/users/zhaopku/repos", "events_url": "https://api.github.com/users/zhaopku/events{/privacy}", "received_events_url": "https://api.github.com/users/zhaopku/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-08-29T17:21:16Z", "updated_at": "2018-07-22T10:29:41Z", "closed_at": "2016-08-31T00:02:25Z", "author_association": "NONE", "body_html": "<p>I am building a seq2seq model using functions in seq2seq.py, where they have a function like this:</p>\n<pre><code>embedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n                          num_encoder_symbols, num_decoder_symbols,\n                          embedding_size, output_projection=None,\n                          feed_previous=False, dtype=dtypes.float32,\n                          scope=None)\n</code></pre>\n<p>however, it seems that this function does not take pre-trained embeddings as input, are there any ways that I can take pre-trained word embeddings as input in this function?</p>", "body_text": "I am building a seq2seq model using functions in seq2seq.py, where they have a function like this:\nembedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n                          num_encoder_symbols, num_decoder_symbols,\n                          embedding_size, output_projection=None,\n                          feed_previous=False, dtype=dtypes.float32,\n                          scope=None)\n\nhowever, it seems that this function does not take pre-trained embeddings as input, are there any ways that I can take pre-trained word embeddings as input in this function?", "body": "I am building a seq2seq model using functions in seq2seq.py, where they have a function like this:\n\n```\nembedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n                          num_encoder_symbols, num_decoder_symbols,\n                          embedding_size, output_projection=None,\n                          feed_previous=False, dtype=dtypes.float32,\n                          scope=None)\n```\n\nhowever, it seems that this function does not take pre-trained embeddings as input, are there any ways that I can take pre-trained word embeddings as input in this function?\n"}