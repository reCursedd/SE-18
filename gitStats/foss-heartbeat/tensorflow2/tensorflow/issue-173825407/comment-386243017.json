{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/386243017", "html_url": "https://github.com/tensorflow/tensorflow/issues/4095#issuecomment-386243017", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4095", "id": 386243017, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjI0MzAxNw==", "user": {"login": "jkneng", "id": 5799741, "node_id": "MDQ6VXNlcjU3OTk3NDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5799741?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jkneng", "html_url": "https://github.com/jkneng", "followers_url": "https://api.github.com/users/jkneng/followers", "following_url": "https://api.github.com/users/jkneng/following{/other_user}", "gists_url": "https://api.github.com/users/jkneng/gists{/gist_id}", "starred_url": "https://api.github.com/users/jkneng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jkneng/subscriptions", "organizations_url": "https://api.github.com/users/jkneng/orgs", "repos_url": "https://api.github.com/users/jkneng/repos", "events_url": "https://api.github.com/users/jkneng/events{/privacy}", "received_events_url": "https://api.github.com/users/jkneng/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T09:46:37Z", "updated_at": "2018-05-03T09:50:06Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10519688\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewjylee\">@andrewjylee</a>, I have the same doubt with you. did you know how to align the word ids to ones in the pretrained embeddings?<br>\nDoes what <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> said means we have to align word ids when loading the pretrained embedding?<br>\nthanks.</p>", "body_text": "Hi @andrewjylee, I have the same doubt with you. did you know how to align the word ids to ones in the pretrained embeddings?\nDoes what @ebrevdo said means we have to align word ids when loading the pretrained embedding?\nthanks.", "body": "Hi @andrewjylee, I have the same doubt with you. did you know how to align the word ids to ones in the pretrained embeddings? \r\nDoes what @ebrevdo said means we have to align word ids when loading the pretrained embedding?\r\nthanks."}