{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/302969874", "html_url": "https://github.com/tensorflow/tensorflow/issues/4095#issuecomment-302969874", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4095", "id": 302969874, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjk2OTg3NA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-21T23:13:03Z", "updated_at": "2017-05-21T23:13:03Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">It doesn't, you have to have a map and reorder your loaded embedding to\nyour vocabulary.  This can be done with the vocabulary lookup table,\ntf.range, and tf.nn.top_k but is not obvious.  Alternatively manually\nreshuffle the embedding matrix to match your vocabulary.  We hope to have\nan example of how to do this in tf in the upcoming seq2seq tutorial with\nglove embedding files.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On May 21, 2017 3:37 PM, \"andrewjylee\" ***@***.***&gt; wrote:\n Hi <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; , quick question....\n What I don't understand is how to match the IDs in x with the IDs of your\n embedding.\n\n For instance, if my sentence is \"I like to run with dogs\", and that is\n vectorized as\n [5, 9, 2, 25, 16, 4], how will Tensorflow know that id 5 in the pretrained\n embedding matrix should correspond to 'I' and 9 to 'like', 2 to 'to', 25 to\n 'run', and so on?\n\n so maybe we have something like\n x=tf.placeholder(...) # [5, 9, 2, 25, 16, 4]\n W = tf.Variable... # What the attached stackoverflow answer says\n ...\n\n So when we do\n tf.nn.embedding_lookup(W, x), how will this know that the first element of\n x (5) and the 5th entry in W are mapping to the same underlying word?\n\n I hope my question made sense. Thanks in advance.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"173825407\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4095\" href=\"https://github.com/tensorflow/tensorflow/issues/4095#issuecomment-302968113\">#4095 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim1BZ9csSOmnlnFO_RpIdKM2trfEZks5r8LydgaJpZM4JvsZA\">https://github.com/notifications/unsubscribe-auth/ABtim1BZ9csSOmnlnFO_RpIdKM2trfEZks5r8LydgaJpZM4JvsZA</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "It doesn't, you have to have a map and reorder your loaded embedding to\nyour vocabulary.  This can be done with the vocabulary lookup table,\ntf.range, and tf.nn.top_k but is not obvious.  Alternatively manually\nreshuffle the embedding matrix to match your vocabulary.  We hope to have\nan example of how to do this in tf in the upcoming seq2seq tutorial with\nglove embedding files.\n\u2026\nOn May 21, 2017 3:37 PM, \"andrewjylee\" ***@***.***> wrote:\n Hi @ebrevdo <https://github.com/ebrevdo> , quick question....\n What I don't understand is how to match the IDs in x with the IDs of your\n embedding.\n\n For instance, if my sentence is \"I like to run with dogs\", and that is\n vectorized as\n [5, 9, 2, 25, 16, 4], how will Tensorflow know that id 5 in the pretrained\n embedding matrix should correspond to 'I' and 9 to 'like', 2 to 'to', 25 to\n 'run', and so on?\n\n so maybe we have something like\n x=tf.placeholder(...) # [5, 9, 2, 25, 16, 4]\n W = tf.Variable... # What the attached stackoverflow answer says\n ...\n\n So when we do\n tf.nn.embedding_lookup(W, x), how will this know that the first element of\n x (5) and the 5th entry in W are mapping to the same underlying word?\n\n I hope my question made sense. Thanks in advance.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#4095 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim1BZ9csSOmnlnFO_RpIdKM2trfEZks5r8LydgaJpZM4JvsZA>\n .", "body": "It doesn't, you have to have a map and reorder your loaded embedding to\nyour vocabulary.  This can be done with the vocabulary lookup table,\ntf.range, and tf.nn.top_k but is not obvious.  Alternatively manually\nreshuffle the embedding matrix to match your vocabulary.  We hope to have\nan example of how to do this in tf in the upcoming seq2seq tutorial with\nglove embedding files.\n\nOn May 21, 2017 3:37 PM, \"andrewjylee\" <notifications@github.com> wrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo> , quick question....\n> What I don't understand is how to match the IDs in x with the IDs of your\n> embedding.\n>\n> For instance, if my sentence is \"I like to run with dogs\", and that is\n> vectorized as\n> [5, 9, 2, 25, 16, 4], how will Tensorflow know that id 5 in the pretrained\n> embedding matrix should correspond to 'I' and 9 to 'like', 2 to 'to', 25 to\n> 'run', and so on?\n>\n> so maybe we have something like\n> x=tf.placeholder(...) # [5, 9, 2, 25, 16, 4]\n> W = tf.Variable... # What the attached stackoverflow answer says\n> ...\n>\n> So when we do\n> tf.nn.embedding_lookup(W, x), how will this know that the first element of\n> x (5) and the 5th entry in W are mapping to the same underlying word?\n>\n> I hope my question made sense. Thanks in advance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4095#issuecomment-302968113>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1BZ9csSOmnlnFO_RpIdKM2trfEZks5r8LydgaJpZM4JvsZA>\n> .\n>\n"}