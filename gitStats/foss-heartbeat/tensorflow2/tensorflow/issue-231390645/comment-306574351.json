{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/306574351", "html_url": "https://github.com/tensorflow/tensorflow/issues/10196#issuecomment-306574351", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10196", "id": 306574351, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjU3NDM1MQ==", "user": {"login": "VinceMarron", "id": 24817118, "node_id": "MDQ6VXNlcjI0ODE3MTE4", "avatar_url": "https://avatars3.githubusercontent.com/u/24817118?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VinceMarron", "html_url": "https://github.com/VinceMarron", "followers_url": "https://api.github.com/users/VinceMarron/followers", "following_url": "https://api.github.com/users/VinceMarron/following{/other_user}", "gists_url": "https://api.github.com/users/VinceMarron/gists{/gist_id}", "starred_url": "https://api.github.com/users/VinceMarron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VinceMarron/subscriptions", "organizations_url": "https://api.github.com/users/VinceMarron/orgs", "repos_url": "https://api.github.com/users/VinceMarron/repos", "events_url": "https://api.github.com/users/VinceMarron/events{/privacy}", "received_events_url": "https://api.github.com/users/VinceMarron/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-06T18:23:03Z", "updated_at": "2017-06-06T18:23:03Z", "author_association": "NONE", "body_html": "<p>As an aside, I eventually found that <code>tf.train.slice_input_producer</code> is unlike the other input producers and does not return a queue object so the above code may be misleading. I am still confused as to the best way to handle queue/process/batch for lists of path/label pairs but the below seems to be closer:</p>\n<p><code>with graph.as_default():</code></p>\n<pre><code>batch_size=32\nn_samples = 100\n\npathlist = tf.constant(paths, dtype=tf.string)[:n_samples]\nlabellist = tf.constant(labels, dtype=tf.int32)[:n_samples]\n\nrsq = tf.RandomShuffleQueue(100000000, 0, [tf.string, tf.int32], shapes=[[],[]])\ndo_enqueues = rsq.enqueue_many([pathlist, labellist])\n    \ngotpath, gotlabel = rsq.dequeue()\n\ny_ = tf.one_hot(gotlabel, 16, dtype=tf.float32)\n\nsignal = tf.contrib.ffmpeg.decode_audio(tf.read_file(gotpath), file_format='mp3', \n                                   samples_per_second=sr,  channel_count=1)[:450000,0]     \n\nbatch_sig, batch_y_ = tf.train.batch([signal, y_], batch_size=batch_size, \n                                     shapes=[(450000,), (16,)], \n                                     num_threads=1, capacity=32)\n\n\nwith tf.Session(config=tf.ConfigProto(operation_timeout_in_ms=5000)) as sess:\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n\n    \n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    sess.run(do_enqueues)\n    \n    i=0\n    \n    try:\n        while not coord.should_stop():\n            for step in range(n_samples//batch_size):\n                print(i, tf.reduce_max(batch_sig, axis=1).eval())\n                i=i+1\n            coord.request_stop()\n    except tf.errors.OutOfRangeError:\n        coord.request_stop()\n        print('Done training -- epoch limit reached')\n    finally:\n        coord.request_stop()\n        coord.join(threads)            \n</code></pre>\n<p>`</p>", "body_text": "As an aside, I eventually found that tf.train.slice_input_producer is unlike the other input producers and does not return a queue object so the above code may be misleading. I am still confused as to the best way to handle queue/process/batch for lists of path/label pairs but the below seems to be closer:\nwith graph.as_default():\nbatch_size=32\nn_samples = 100\n\npathlist = tf.constant(paths, dtype=tf.string)[:n_samples]\nlabellist = tf.constant(labels, dtype=tf.int32)[:n_samples]\n\nrsq = tf.RandomShuffleQueue(100000000, 0, [tf.string, tf.int32], shapes=[[],[]])\ndo_enqueues = rsq.enqueue_many([pathlist, labellist])\n    \ngotpath, gotlabel = rsq.dequeue()\n\ny_ = tf.one_hot(gotlabel, 16, dtype=tf.float32)\n\nsignal = tf.contrib.ffmpeg.decode_audio(tf.read_file(gotpath), file_format='mp3', \n                                   samples_per_second=sr,  channel_count=1)[:450000,0]     \n\nbatch_sig, batch_y_ = tf.train.batch([signal, y_], batch_size=batch_size, \n                                     shapes=[(450000,), (16,)], \n                                     num_threads=1, capacity=32)\n\n\nwith tf.Session(config=tf.ConfigProto(operation_timeout_in_ms=5000)) as sess:\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n\n    \n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    sess.run(do_enqueues)\n    \n    i=0\n    \n    try:\n        while not coord.should_stop():\n            for step in range(n_samples//batch_size):\n                print(i, tf.reduce_max(batch_sig, axis=1).eval())\n                i=i+1\n            coord.request_stop()\n    except tf.errors.OutOfRangeError:\n        coord.request_stop()\n        print('Done training -- epoch limit reached')\n    finally:\n        coord.request_stop()\n        coord.join(threads)            \n\n`", "body": "As an aside, I eventually found that `tf.train.slice_input_producer` is unlike the other input producers and does not return a queue object so the above code may be misleading. I am still confused as to the best way to handle queue/process/batch for lists of path/label pairs but the below seems to be closer:\r\n\r\n`with graph.as_default():`\r\n    \r\n    batch_size=32\r\n    n_samples = 100\r\n    \r\n    pathlist = tf.constant(paths, dtype=tf.string)[:n_samples]\r\n    labellist = tf.constant(labels, dtype=tf.int32)[:n_samples]\r\n\r\n    rsq = tf.RandomShuffleQueue(100000000, 0, [tf.string, tf.int32], shapes=[[],[]])\r\n    do_enqueues = rsq.enqueue_many([pathlist, labellist])\r\n        \r\n    gotpath, gotlabel = rsq.dequeue()\r\n\r\n    y_ = tf.one_hot(gotlabel, 16, dtype=tf.float32)\r\n\r\n    signal = tf.contrib.ffmpeg.decode_audio(tf.read_file(gotpath), file_format='mp3', \r\n                                       samples_per_second=sr,  channel_count=1)[:450000,0]     \r\n\r\n    batch_sig, batch_y_ = tf.train.batch([signal, y_], batch_size=batch_size, \r\n                                         shapes=[(450000,), (16,)], \r\n                                         num_threads=1, capacity=32)\r\n    \r\n    \r\n    with tf.Session(config=tf.ConfigProto(operation_timeout_in_ms=5000)) as sess:\r\n\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n\r\n        \r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        sess.run(do_enqueues)\r\n        \r\n        i=0\r\n        \r\n        try:\r\n            while not coord.should_stop():\r\n                for step in range(n_samples//batch_size):\r\n                    print(i, tf.reduce_max(batch_sig, axis=1).eval())\r\n                    i=i+1\r\n                coord.request_stop()\r\n        except tf.errors.OutOfRangeError:\r\n            coord.request_stop()\r\n            print('Done training -- epoch limit reached')\r\n        finally:\r\n            coord.request_stop()\r\n            coord.join(threads)            \r\n`"}