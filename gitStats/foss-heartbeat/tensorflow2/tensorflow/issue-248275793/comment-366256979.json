{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366256979", "html_url": "https://github.com/tensorflow/tensorflow/issues/12065#issuecomment-366256979", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12065", "id": 366256979, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjI1Njk3OQ==", "user": {"login": "fritzfitzpatrick", "id": 34604047, "node_id": "MDQ6VXNlcjM0NjA0MDQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/34604047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzfitzpatrick", "html_url": "https://github.com/fritzfitzpatrick", "followers_url": "https://api.github.com/users/fritzfitzpatrick/followers", "following_url": "https://api.github.com/users/fritzfitzpatrick/following{/other_user}", "gists_url": "https://api.github.com/users/fritzfitzpatrick/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzfitzpatrick/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzfitzpatrick/subscriptions", "organizations_url": "https://api.github.com/users/fritzfitzpatrick/orgs", "repos_url": "https://api.github.com/users/fritzfitzpatrick/repos", "events_url": "https://api.github.com/users/fritzfitzpatrick/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzfitzpatrick/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-16T14:54:30Z", "updated_at": "2018-02-16T14:55:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8942987\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/RylanSchaeffer\">@RylanSchaeffer</a> I have tried using a go token as well as the last value of the encoder input sequence as my start_inputs in the initializer_fn, and I have tried using the projection layer outputs as well as the original signal as the next_inputs in the next_inputs_fn.</p>\n<p>I am training my model on a linear time series, it is seeing a sequence [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6] and should learn to predict [0.7, 0.8]. During training and validation the l2 regulated loss goes down to 0.010 and the validation prediction using the training helper looks somewhat like [0.69, 0.79], so close enough.</p>\n<p>When I save that model, restore the variables to an identical inference model and run it, the output looks more like this [-0.14, 0.19] etc. I have no idea why it is off like that. Attached is the model in its current form, but as I said above, I have been through a few configurations. I can't seem to find the reason as to why the inference prediction is so vastly off the mark.<br>\n`</p>\n<pre><code># define training encoder\ninf_enc_out, inf_enc_state = tf.nn.dynamic_rnn(\n    enc_cell, \n    inf_enc_inp,\n    dtype = tf.float32,\n    sequence_length = seq_length_inp,\n    time_major = time_major)\n\nwith tf.variable_scope('projection_layer', reuse = tf.AUTO_REUSE):\n    from tensorflow.python.layers.core import Dense\n    projection_layer = Dense(features_dec_exp_out)\n\n# define inference custom helper\ndef initialize_fn():\n    finished = tf.tile([False], [batch_size])\n    enc_inp_end = inf_enc_inp[0, observation_length - 1, 0]\n    start_inputs = tf.reshape(enc_inp_end, shape=[1, 1]) \n    return (finished, start_inputs)\n\ndef sample_fn(time, outputs, state):\n    return tf.constant([0])\n\ndef next_inputs_fn(time, outputs, state, sample_ids):\n    finished = time &gt;= prediction_length\n    next_inputs = outputs\n    return (finished, next_inputs, state)\n\ninf_custom_helper = tf.contrib.seq2seq.CustomHelper(\n    initialize_fn = initialize_fn,\n    sample_fn = sample_fn,                      \n    next_inputs_fn = next_inputs_fn)\n\n# create inference decoder\ninf_decoder = tf.contrib.seq2seq.BasicDecoder(\n    dec_cell, \n    inf_custom_helper, \n    inf_enc_state,\n    projection_layer)\n\n# create inference dynamic decoding\ninf_dec_out, inf_dec_state, inf_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\n    inf_decoder, \n    output_time_major = time_major)\n\n# extract prediction from decoder output\ninf_output_dense = inf_dec_out.rnn_output`\n</code></pre>", "body_text": "@RylanSchaeffer I have tried using a go token as well as the last value of the encoder input sequence as my start_inputs in the initializer_fn, and I have tried using the projection layer outputs as well as the original signal as the next_inputs in the next_inputs_fn.\nI am training my model on a linear time series, it is seeing a sequence [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6] and should learn to predict [0.7, 0.8]. During training and validation the l2 regulated loss goes down to 0.010 and the validation prediction using the training helper looks somewhat like [0.69, 0.79], so close enough.\nWhen I save that model, restore the variables to an identical inference model and run it, the output looks more like this [-0.14, 0.19] etc. I have no idea why it is off like that. Attached is the model in its current form, but as I said above, I have been through a few configurations. I can't seem to find the reason as to why the inference prediction is so vastly off the mark.\n`\n# define training encoder\ninf_enc_out, inf_enc_state = tf.nn.dynamic_rnn(\n    enc_cell, \n    inf_enc_inp,\n    dtype = tf.float32,\n    sequence_length = seq_length_inp,\n    time_major = time_major)\n\nwith tf.variable_scope('projection_layer', reuse = tf.AUTO_REUSE):\n    from tensorflow.python.layers.core import Dense\n    projection_layer = Dense(features_dec_exp_out)\n\n# define inference custom helper\ndef initialize_fn():\n    finished = tf.tile([False], [batch_size])\n    enc_inp_end = inf_enc_inp[0, observation_length - 1, 0]\n    start_inputs = tf.reshape(enc_inp_end, shape=[1, 1]) \n    return (finished, start_inputs)\n\ndef sample_fn(time, outputs, state):\n    return tf.constant([0])\n\ndef next_inputs_fn(time, outputs, state, sample_ids):\n    finished = time >= prediction_length\n    next_inputs = outputs\n    return (finished, next_inputs, state)\n\ninf_custom_helper = tf.contrib.seq2seq.CustomHelper(\n    initialize_fn = initialize_fn,\n    sample_fn = sample_fn,                      \n    next_inputs_fn = next_inputs_fn)\n\n# create inference decoder\ninf_decoder = tf.contrib.seq2seq.BasicDecoder(\n    dec_cell, \n    inf_custom_helper, \n    inf_enc_state,\n    projection_layer)\n\n# create inference dynamic decoding\ninf_dec_out, inf_dec_state, inf_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\n    inf_decoder, \n    output_time_major = time_major)\n\n# extract prediction from decoder output\ninf_output_dense = inf_dec_out.rnn_output`", "body": "@RylanSchaeffer I have tried using a go token as well as the last value of the encoder input sequence as my start_inputs in the initializer_fn, and I have tried using the projection layer outputs as well as the original signal as the next_inputs in the next_inputs_fn. \r\n\r\nI am training my model on a linear time series, it is seeing a sequence [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6] and should learn to predict [0.7, 0.8]. During training and validation the l2 regulated loss goes down to 0.010 and the validation prediction using the training helper looks somewhat like [0.69, 0.79], so close enough.\r\n\r\nWhen I save that model, restore the variables to an identical inference model and run it, the output looks more like this [-0.14, 0.19] etc. I have no idea why it is off like that. Attached is the model in its current form, but as I said above, I have been through a few configurations. I can't seem to find the reason as to why the inference prediction is so vastly off the mark.\r\n`\r\n\r\n    # define training encoder\r\n    inf_enc_out, inf_enc_state = tf.nn.dynamic_rnn(\r\n        enc_cell, \r\n        inf_enc_inp,\r\n        dtype = tf.float32,\r\n        sequence_length = seq_length_inp,\r\n        time_major = time_major)\r\n    \r\n    with tf.variable_scope('projection_layer', reuse = tf.AUTO_REUSE):\r\n        from tensorflow.python.layers.core import Dense\r\n        projection_layer = Dense(features_dec_exp_out)\r\n\r\n    # define inference custom helper\r\n    def initialize_fn():\r\n        finished = tf.tile([False], [batch_size])\r\n        enc_inp_end = inf_enc_inp[0, observation_length - 1, 0]\r\n        start_inputs = tf.reshape(enc_inp_end, shape=[1, 1]) \r\n        return (finished, start_inputs)\r\n    \r\n    def sample_fn(time, outputs, state):\r\n        return tf.constant([0])\r\n\r\n    def next_inputs_fn(time, outputs, state, sample_ids):\r\n        finished = time >= prediction_length\r\n        next_inputs = outputs\r\n        return (finished, next_inputs, state)\r\n\r\n    inf_custom_helper = tf.contrib.seq2seq.CustomHelper(\r\n        initialize_fn = initialize_fn,\r\n        sample_fn = sample_fn,                      \r\n        next_inputs_fn = next_inputs_fn)\r\n\r\n    # create inference decoder\r\n    inf_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n        dec_cell, \r\n        inf_custom_helper, \r\n        inf_enc_state,\r\n        projection_layer)\r\n\r\n    # create inference dynamic decoding\r\n    inf_dec_out, inf_dec_state, inf_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\r\n        inf_decoder, \r\n        output_time_major = time_major)\r\n\r\n    # extract prediction from decoder output\r\n    inf_output_dense = inf_dec_out.rnn_output`\r\n\r\n"}