{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/403990602", "html_url": "https://github.com/tensorflow/tensorflow/issues/20630#issuecomment-403990602", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20630", "id": 403990602, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzk5MDYwMg==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-10T22:50:43Z", "updated_at": "2018-07-10T22:50:43Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=469371\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skeydan\">@skeydan</a> : Thanks for bringing this up.  The documentation you're pointing to is a bit misleading, we shouldn't override <code>predict</code> there, we should override <code>call</code> since <code>tf.keras.Model.predict</code> isn't meant to be overridden.</p>\n<p>I'll update the getting started guide. And as for other examples, there are a bunch in <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples</a>, and we'll be adding more to <a href=\"http://www.tensorflow.org\" rel=\"nofollow\">www.tensorflow.org</a> soon.</p>\n<p>As for an explanation, <code>tf.keras.Model.predict</code> returns a <code>numpy.ndarray</code>, not a <code>tf.Tensor</code>. And the TensorFlow libraries cannot differentiate through numpy conversions or operations. So, if you change your second example to use <code>model(x)</code> instead of <code>model.predict(x)</code>, it should work out fine:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\ntf.enable_eager_execution()\n\nn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nx <span class=\"pl-k\">=</span> tf.random_normal([n, <span class=\"pl-c1\">2</span>])\nnoise <span class=\"pl-k\">=</span> tf.random_normal([n, <span class=\"pl-c1\">2</span>])\n\ny <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">+</span> noise\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">keras</span>.<span class=\"pl-e\">Model</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.dense1 <span class=\"pl-k\">=</span> tf.keras.layers.Dense(<span class=\"pl-v\">units</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.dense1(x)\n\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\n\nmodel <span class=\"pl-k\">=</span> Model()\n<span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n  error <span class=\"pl-k\">=</span> model(x) <span class=\"pl-k\">-</span> y  <span class=\"pl-c\"><span class=\"pl-c\">#</span> NOT model.predict(x) - y</span>\n  loss_value <span class=\"pl-k\">=</span> tf.reduce_mean(tf.square(error))\ngradients <span class=\"pl-k\">=</span> tape.gradient(loss_value, model.variables)\n<span class=\"pl-c1\">print</span>(gradients)\noptimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(gradients, model.variables),\n                            <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_or_create_global_step())</pre></div>\n<p>And similarly, if we change the first example to override <code>call</code> instead of <code>predict</code>, it would work out fine too (which is the documentation fix to be made):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\ntf.enable_eager_execution()\ntfe <span class=\"pl-k\">=</span> tf.contrib.eager\nn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nx <span class=\"pl-k\">=</span> tf.random_normal([n, <span class=\"pl-c1\">2</span>])\nnoise <span class=\"pl-k\">=</span> tf.random_normal([n, <span class=\"pl-c1\">2</span>])\ny <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">+</span> noise\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">keras</span>.<span class=\"pl-e\">Model</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">super</span>(Model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.W <span class=\"pl-k\">=</span> tfe.Variable(<span class=\"pl-c1\">5</span>., <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">self</span>.B <span class=\"pl-k\">=</span> tfe.Variable(<span class=\"pl-c1\">10</span>., <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>)\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Overriding call not predict</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n    <span class=\"pl-k\">return</span> inputs <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.W <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.B\n\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\n\nmodel <span class=\"pl-k\">=</span> Model()\n<span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n  error <span class=\"pl-k\">=</span> model(x) <span class=\"pl-k\">-</span> y\n  loss_value <span class=\"pl-k\">=</span> tf.reduce_mean(tf.square(error))\ngradients <span class=\"pl-k\">=</span> tape.gradient(loss_value, model.variables)\n<span class=\"pl-c1\">print</span>(gradients)\noptimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(gradients, model.variables),\n                            <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_or_create_global_step())</pre></div>\n<p>FYI <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19695904\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/random-forests\">@random-forests</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13326758\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pavithrasv\">@pavithrasv</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=14262417\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yashk2810\">@yashk2810</a> - regarding documentation or other improvements that we might be able to make to reduce confusion between <code>tf.keras.Model.predict(x)</code> and <code>tf.keras.Model(x)</code>.</p>\n<p>Hope that helps.</p>", "body_text": "@skeydan : Thanks for bringing this up.  The documentation you're pointing to is a bit misleading, we shouldn't override predict there, we should override call since tf.keras.Model.predict isn't meant to be overridden.\nI'll update the getting started guide. And as for other examples, there are a bunch in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples, and we'll be adding more to www.tensorflow.org soon.\nAs for an explanation, tf.keras.Model.predict returns a numpy.ndarray, not a tf.Tensor. And the TensorFlow libraries cannot differentiate through numpy conversions or operations. So, if you change your second example to use model(x) instead of model.predict(x), it should work out fine:\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nn = 10\nx = tf.random_normal([n, 2])\nnoise = tf.random_normal([n, 2])\n\ny = x * 3 + 2 + noise\n\nclass Model(tf.keras.Model):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')\n    def call(self, inputs):\n        return self.dense1(x)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nmodel = Model()\nwith tf.GradientTape() as tape:\n  error = model(x) - y  # NOT model.predict(x) - y\n  loss_value = tf.reduce_mean(tf.square(error))\ngradients = tape.gradient(loss_value, model.variables)\nprint(gradients)\noptimizer.apply_gradients(zip(gradients, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\nAnd similarly, if we change the first example to override call instead of predict, it would work out fine too (which is the documentation fix to be made):\nimport tensorflow as tf\ntf.enable_eager_execution()\ntfe = tf.contrib.eager\nn = 10\nx = tf.random_normal([n, 2])\nnoise = tf.random_normal([n, 2])\ny = x * 3 + 2 + noise\n\nclass Model(tf.keras.Model):\n  def __init__(self):\n    super(Model, self).__init__()\n    self.W = tfe.Variable(5., name='weight')\n    self.B = tfe.Variable(10., name='bias')\n  # Overriding call not predict\n  def call(self, inputs):\n    return inputs * self.W + self.B\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nmodel = Model()\nwith tf.GradientTape() as tape:\n  error = model(x) - y\n  loss_value = tf.reduce_mean(tf.square(error))\ngradients = tape.gradient(loss_value, model.variables)\nprint(gradients)\noptimizer.apply_gradients(zip(gradients, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\nFYI @random-forests @fchollet @pavithrasv @yashk2810 - regarding documentation or other improvements that we might be able to make to reduce confusion between tf.keras.Model.predict(x) and tf.keras.Model(x).\nHope that helps.", "body": "@skeydan : Thanks for bringing this up.  The documentation you're pointing to is a bit misleading, we shouldn't override `predict` there, we should override `call` since `tf.keras.Model.predict` isn't meant to be overridden.\r\n\r\nI'll update the getting started guide. And as for other examples, there are a bunch in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples, and we'll be adding more to www.tensorflow.org soon.\r\n\r\nAs for an explanation, `tf.keras.Model.predict` returns a `numpy.ndarray`, not a `tf.Tensor`. And the TensorFlow libraries cannot differentiate through numpy conversions or operations. So, if you change your second example to use `model(x)` instead of `model.predict(x)`, it should work out fine:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\n\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')\r\n    def call(self, inputs):\r\n        return self.dense1(x)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model(x) - y  # NOT model.predict(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n```\r\n\r\nAnd similarly, if we change the first example to override `call` instead of `predict`, it would work out fine too (which is the documentation fix to be made):\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntfe = tf.contrib.eager\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.W = tfe.Variable(5., name='weight')\r\n    self.B = tfe.Variable(10., name='bias')\r\n  # Overriding call not predict\r\n  def call(self, inputs):\r\n    return inputs * self.W + self.B\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n```\r\n\r\nFYI @random-forests @fchollet @pavithrasv @yashk2810 - regarding documentation or other improvements that we might be able to make to reduce confusion between `tf.keras.Model.predict(x)` and `tf.keras.Model(x)`.\r\n\r\nHope that helps."}