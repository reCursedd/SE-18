{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20630", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20630/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20630/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20630/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20630", "id": 339239972, "node_id": "MDU6SXNzdWUzMzkyMzk5NzI=", "number": 20630, "title": "Eager execution guide: using GradientTape with keras.model and tf.keras.layer", "user": {"login": "skeydan", "id": 469371, "node_id": "MDQ6VXNlcjQ2OTM3MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/469371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skeydan", "html_url": "https://github.com/skeydan", "followers_url": "https://api.github.com/users/skeydan/followers", "following_url": "https://api.github.com/users/skeydan/following{/other_user}", "gists_url": "https://api.github.com/users/skeydan/gists{/gist_id}", "starred_url": "https://api.github.com/users/skeydan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skeydan/subscriptions", "organizations_url": "https://api.github.com/users/skeydan/orgs", "repos_url": "https://api.github.com/users/skeydan/repos", "events_url": "https://api.github.com/users/skeydan/events{/privacy}", "received_events_url": "https://api.github.com/users/skeydan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-08T16:47:32Z", "updated_at": "2018-07-13T01:04:23Z", "closed_at": "2018-07-11T02:18:26Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:  no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Fedora 28</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:  1.9</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The eager execution doc</p>\n<p><a href=\"https://www.tensorflow.org/programmers_guide/eager\" rel=\"nofollow\">https://www.tensorflow.org/programmers_guide/eager</a></p>\n<p>does not provide a simple complete example that shows how to use gradient tape optimization with keras layers (instead of tfe.Variables).<br>\nIt would be great if that could be added, as I seem to be getting gradients that are None when I try to replace tfe.Variables in the following, running, example (copied from the doc but even more simplified for ease of experimentation):</p>\n<pre><code>import tensorflow as tf\ntf.enable_eager_execution()\ntfe = tf.contrib.eager\nn = 10\nx = tf.random_normal([n, 2])\nnoise = tf.random_normal([n, 2])\ny = x * 3 + 2 + noise\n\nclass Model(tf.keras.Model):\n  def __init__(self):\n    super(Model, self).__init__()\n    self.W = tfe.Variable(5., name='weight')\n    self.B = tfe.Variable(10., name='bias')\n  def predict(self, inputs):\n    return inputs * self.W + self.B\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nmodel = Model()\nwith tf.GradientTape() as tape:\n  error = model.predict(x) - y\n  loss_value = tf.reduce_mean(tf.square(error))\ngradients = tape.gradient(loss_value, model.variables)\nprint(gradients)\noptimizer.apply_gradients(zip(gradients, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\n\n</code></pre>\n<p>by a Keras layer instead of weight and a bias (based on, but again simplified, the MNISTModel from the doc):</p>\n<pre><code>import tensorflow as tf\ntf.enable_eager_execution()\n\nn = 10\nx = tf.random_normal([n, 2])\nnoise = tf.random_normal([n, 2])\n\ny = x * 3 + 2 + noise\n\nclass Model(tf.keras.Model):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')\n    def call(self, inputs):\n        return self.dense1(x)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nmodel = Model()\nwith tf.GradientTape() as tape:\n  error = model.predict(x) - y\n  loss_value = tf.reduce_mean(tf.square(error))\ngradients = tape.gradient(loss_value, model.variables)\n# now gradients are None\nprint(gradients)\noptimizer.apply_gradients(zip(gradients, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\n                            \n</code></pre>\n<p>In this version, the gradients are None. Same if I use another construct from the doc (again, simplified):</p>\n<pre><code>model = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, input_shape=(2,))  # must declare input shape\n])\n</code></pre>\n<p>It would be great if the doc could be extended to show a complete tf.keras.layer example with GradientTape. Thank you!</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):  no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 28\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  1.9\nPython version:  3.6\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:N/A\n\nDescribe the problem\nThe eager execution doc\nhttps://www.tensorflow.org/programmers_guide/eager\ndoes not provide a simple complete example that shows how to use gradient tape optimization with keras layers (instead of tfe.Variables).\nIt would be great if that could be added, as I seem to be getting gradients that are None when I try to replace tfe.Variables in the following, running, example (copied from the doc but even more simplified for ease of experimentation):\nimport tensorflow as tf\ntf.enable_eager_execution()\ntfe = tf.contrib.eager\nn = 10\nx = tf.random_normal([n, 2])\nnoise = tf.random_normal([n, 2])\ny = x * 3 + 2 + noise\n\nclass Model(tf.keras.Model):\n  def __init__(self):\n    super(Model, self).__init__()\n    self.W = tfe.Variable(5., name='weight')\n    self.B = tfe.Variable(10., name='bias')\n  def predict(self, inputs):\n    return inputs * self.W + self.B\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nmodel = Model()\nwith tf.GradientTape() as tape:\n  error = model.predict(x) - y\n  loss_value = tf.reduce_mean(tf.square(error))\ngradients = tape.gradient(loss_value, model.variables)\nprint(gradients)\noptimizer.apply_gradients(zip(gradients, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\n\n\nby a Keras layer instead of weight and a bias (based on, but again simplified, the MNISTModel from the doc):\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nn = 10\nx = tf.random_normal([n, 2])\nnoise = tf.random_normal([n, 2])\n\ny = x * 3 + 2 + noise\n\nclass Model(tf.keras.Model):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')\n    def call(self, inputs):\n        return self.dense1(x)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\nmodel = Model()\nwith tf.GradientTape() as tape:\n  error = model.predict(x) - y\n  loss_value = tf.reduce_mean(tf.square(error))\ngradients = tape.gradient(loss_value, model.variables)\n# now gradients are None\nprint(gradients)\noptimizer.apply_gradients(zip(gradients, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\n                            \n\nIn this version, the gradients are None. Same if I use another construct from the doc (again, simplified):\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, input_shape=(2,))  # must declare input shape\n])\n\nIt would be great if the doc could be extended to show a complete tf.keras.layer example with GradientTape. Thank you!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 28\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  1.9\r\n- **Python version**:  3.6\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\n\r\nThe eager execution doc \r\n\r\nhttps://www.tensorflow.org/programmers_guide/eager \r\n\r\ndoes not provide a simple complete example that shows how to use gradient tape optimization with keras layers (instead of tfe.Variables). \r\nIt would be great if that could be added, as I seem to be getting gradients that are None when I try to replace tfe.Variables in the following, running, example (copied from the doc but even more simplified for ease of experimentation):\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntfe = tf.contrib.eager\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.W = tfe.Variable(5., name='weight')\r\n    self.B = tfe.Variable(10., name='bias')\r\n  def predict(self, inputs):\r\n    return inputs * self.W + self.B\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model.predict(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n\r\n```\r\n\r\nby a Keras layer instead of weight and a bias (based on, but again simplified, the MNISTModel from the doc):\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nn = 10\r\nx = tf.random_normal([n, 2])\r\nnoise = tf.random_normal([n, 2])\r\n\r\ny = x * 3 + 2 + noise\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')\r\n    def call(self, inputs):\r\n        return self.dense1(x)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\nmodel = Model()\r\nwith tf.GradientTape() as tape:\r\n  error = model.predict(x) - y\r\n  loss_value = tf.reduce_mean(tf.square(error))\r\ngradients = tape.gradient(loss_value, model.variables)\r\n# now gradients are None\r\nprint(gradients)\r\noptimizer.apply_gradients(zip(gradients, model.variables),\r\n                            global_step=tf.train.get_or_create_global_step())\r\n                            \r\n```\r\n\r\nIn this version, the gradients are None. Same if I use another construct from the doc (again, simplified):\r\n\r\n```\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(1, input_shape=(2,))  # must declare input shape\r\n])\r\n```\r\n\r\nIt would be great if the doc could be extended to show a complete tf.keras.layer example with GradientTape. Thank you!\r\n\r\n\r\n\r\n\r\n"}