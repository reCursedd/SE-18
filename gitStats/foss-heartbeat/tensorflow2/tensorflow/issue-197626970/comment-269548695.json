{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269548695", "html_url": "https://github.com/tensorflow/tensorflow/issues/6508#issuecomment-269548695", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6508", "id": 269548695, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTU0ODY5NQ==", "user": {"login": "mbz", "id": 1122127, "node_id": "MDQ6VXNlcjExMjIxMjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1122127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbz", "html_url": "https://github.com/mbz", "followers_url": "https://api.github.com/users/mbz/followers", "following_url": "https://api.github.com/users/mbz/following{/other_user}", "gists_url": "https://api.github.com/users/mbz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbz/subscriptions", "organizations_url": "https://api.github.com/users/mbz/orgs", "repos_url": "https://api.github.com/users/mbz/repos", "events_url": "https://api.github.com/users/mbz/events{/privacy}", "received_events_url": "https://api.github.com/users/mbz/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-28T22:06:41Z", "updated_at": "2016-12-28T22:07:14Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> thanks for all the info.</p>\n<p>adding <code>tf.ConfigProto(inter_op_parallelism_threads=64,intra_op_parallelism_threads=64)</code> to <code>tf.train.Server</code> solved the issue. CPU utilization is way higher and the memory is stable at ~20GB peaking at 30GB.</p>\n<p>I can dig more into why TF does not pick up the right number of cores if you folks are interested. It would be helpful if you point me to the right code though.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> this doesn't look like to be a memory leak. it's just high memory consumption due large number of received messages. Adding more threads to PS process (i.e. processing the messages faster) mitigates the memory issue, at least for 32 workers. The memory consumption still seems high though. Probably, because of gRPC overhead?</p>", "body_text": "@yaroslavvb thanks for all the info.\nadding tf.ConfigProto(inter_op_parallelism_threads=64,intra_op_parallelism_threads=64) to tf.train.Server solved the issue. CPU utilization is way higher and the memory is stable at ~20GB peaking at 30GB.\nI can dig more into why TF does not pick up the right number of cores if you folks are interested. It would be helpful if you point me to the right code though.\n@mrry this doesn't look like to be a memory leak. it's just high memory consumption due large number of received messages. Adding more threads to PS process (i.e. processing the messages faster) mitigates the memory issue, at least for 32 workers. The memory consumption still seems high though. Probably, because of gRPC overhead?", "body": "@yaroslavvb thanks for all the info. \r\n\r\nadding `tf.ConfigProto(inter_op_parallelism_threads=64,intra_op_parallelism_threads=64)` to `tf.train.Server` solved the issue. CPU utilization is way higher and the memory is stable at ~20GB peaking at 30GB. \r\n\r\nI can dig more into why TF does not pick up the right number of cores if you folks are interested. It would be helpful if you point me to the right code though.\r\n\r\n@mrry this doesn't look like to be a memory leak. it's just high memory consumption due large number of received messages. Adding more threads to PS process (i.e. processing the messages faster) mitigates the memory issue, at least for 32 workers. The memory consumption still seems high though. Probably, because of gRPC overhead?"}