{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6508", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6508/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6508/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6508/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6508", "id": 197626970, "node_id": "MDU6SXNzdWUxOTc2MjY5NzA=", "number": 6508, "title": "PS OOM in Distributed Inception", "user": {"login": "mbz", "id": 1122127, "node_id": "MDQ6VXNlcjExMjIxMjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1122127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbz", "html_url": "https://github.com/mbz", "followers_url": "https://api.github.com/users/mbz/followers", "following_url": "https://api.github.com/users/mbz/following{/other_user}", "gists_url": "https://api.github.com/users/mbz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbz/subscriptions", "organizations_url": "https://api.github.com/users/mbz/orgs", "repos_url": "https://api.github.com/users/mbz/repos", "events_url": "https://api.github.com/users/mbz/events{/privacy}", "received_events_url": "https://api.github.com/users/mbz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2016-12-26T23:33:52Z", "updated_at": "2017-01-06T00:17:56Z", "closed_at": "2017-01-06T00:17:56Z", "author_association": "NONE", "body_html": "<p>We are trying to train Inception model on ImageNet in a distributed setting following the guide <a href=\"https://github.com/tensorflow/models/tree/master/inception\">here</a>. TensorFlow successfully trains the model with 32 workers but with 64 worker, the PS dies with OOM killer.</p>\n<p>In all of these experiments, PS was running on a separate machine. Looking at the memory consumption of 32 workers, it seems that there is no memory issue and the memory becomes stable after a couple of iterations. With 64 workers, PS dies after allocating all the available memory (32GB) after only two iterations.</p>\n<p>Is this the expected behavior from PS or not? If not, please let us know if we can provide any other information that may help debugging this problem.</p>\n<h4>Environment info</h4>\n<p>TensorFlow 0.12<br>\nCUDA 8.0<br>\nCUDNN 5.1<br>\nRAM: 32 GB<br>\nGPU: K20</p>", "body_text": "We are trying to train Inception model on ImageNet in a distributed setting following the guide here. TensorFlow successfully trains the model with 32 workers but with 64 worker, the PS dies with OOM killer.\nIn all of these experiments, PS was running on a separate machine. Looking at the memory consumption of 32 workers, it seems that there is no memory issue and the memory becomes stable after a couple of iterations. With 64 workers, PS dies after allocating all the available memory (32GB) after only two iterations.\nIs this the expected behavior from PS or not? If not, please let us know if we can provide any other information that may help debugging this problem.\nEnvironment info\nTensorFlow 0.12\nCUDA 8.0\nCUDNN 5.1\nRAM: 32 GB\nGPU: K20", "body": "We are trying to train Inception model on ImageNet in a distributed setting following the guide [here](https://github.com/tensorflow/models/tree/master/inception). TensorFlow successfully trains the model with 32 workers but with 64 worker, the PS dies with OOM killer. \r\n\r\nIn all of these experiments, PS was running on a separate machine. Looking at the memory consumption of 32 workers, it seems that there is no memory issue and the memory becomes stable after a couple of iterations. With 64 workers, PS dies after allocating all the available memory (32GB) after only two iterations.\r\n\r\nIs this the expected behavior from PS or not? If not, please let us know if we can provide any other information that may help debugging this problem.\r\n\r\n#### Environment info\r\nTensorFlow 0.12\r\nCUDA 8.0\r\nCUDNN 5.1\r\nRAM: 32 GB\r\nGPU: K20\r\n\r\n"}