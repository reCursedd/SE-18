{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22987", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22987/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22987/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22987/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22987", "id": 370169055, "node_id": "MDU6SXNzdWUzNzAxNjkwNTU=", "number": 22987, "title": "Segmentation Fault (SIGSEGV) in middle of Training due to Runtime-statistics calculation ops.", "user": {"login": "BigBang0072", "id": 17550410, "node_id": "MDQ6VXNlcjE3NTUwNDEw", "avatar_url": "https://avatars1.githubusercontent.com/u/17550410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BigBang0072", "html_url": "https://github.com/BigBang0072", "followers_url": "https://api.github.com/users/BigBang0072/followers", "following_url": "https://api.github.com/users/BigBang0072/following{/other_user}", "gists_url": "https://api.github.com/users/BigBang0072/gists{/gist_id}", "starred_url": "https://api.github.com/users/BigBang0072/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BigBang0072/subscriptions", "organizations_url": "https://api.github.com/users/BigBang0072/orgs", "repos_url": "https://api.github.com/users/BigBang0072/repos", "events_url": "https://api.github.com/users/BigBang0072/events{/privacy}", "received_events_url": "https://api.github.com/users/BigBang0072/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547538, "node_id": "MDU6TGFiZWwxMDk3NTQ3NTM4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:gpu", "name": "comp:gpu", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-10-15T13:41:23Z", "updated_at": "2018-11-22T06:07:40Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nCustom code. The full code can be accessed <a href=\"https://github.com/grasseau/HAhRD/tree/master/GSOC18\">here at GitHub</a></li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nCentOS Linux release 7.5.1804 (Core)</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n('v1.8.0-0-g93bc2e2072', '1.8.0')</li>\n<li><strong>Python version</strong>:<br>\nPython 2.7.5 (default, Apr 11 2018, 07:36:10)</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\ncuda-9.0-cudnn-7</li>\n<li><strong>GPU model and memory</strong>:<br>\n2 Tesla V100 Nvidia GPUs, ~15 Gb each</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nPlease go through this instruction <a href=\"https://github.com/BigBang0072/HAhRD/wiki/GSOC-18-Work-Summary\">here</a> to run the code.<br>\nThe training runs without error on small dataset. The problem arises when number of minibatches are<br>\nmore per epoch keeping the batch size constant. (i.e more number of sess.run call per epoch)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The training crashes with the error <strong>Segmentation Fault (Core Dumped) in the middle of the training after around 14-18 epochs (with ~800 minibatches in each)</strong> even though I have sufficient RAM (~12-20% utilization holding steadily before crash).</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/17550410/46916113-c4b44e00-cfd3-11e8-842b-686a42b2c486.png\"><img src=\"https://user-images.githubusercontent.com/17550410/46916113-c4b44e00-cfd3-11e8-842b-686a42b2c486.png\" alt=\"screenshot from 2018-10-14 16-46-38\" style=\"max-width:100%;\"></a></p>\n<p><strong>Possible Memory Leak Checked</strong><br>\nInitially, I suspected a memory leak due to the addition of new ops after each iteration, so I finalized the graph in the training session. But this was not the source of the problem. No new nodes were added at each iteration.</p>\n<p><strong>Attempt 2: Locating the problem</strong><br>\nAfter that, I ran the training on gdb (see the stack trace in logs section) and along with some commenting of the code I have almost pinpointed the source of error.<br>\nCurrently every 30 minibatch training I am saving the summary here in the code. The full code can be found <a href=\"https://github.com/grasseau/HAhRD/blob/master/GSOC18/train_multi_gpu.py#L406\">here</a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/17550410/46916189-c29ebf00-cfd4-11e8-9a94-08f67c12198d.png\"><img src=\"https://user-images.githubusercontent.com/17550410/46916189-c29ebf00-cfd4-11e8-9a94-08f67c12198d.png\" alt=\"screenshot from 2018-10-14 17-13-59\" style=\"max-width:100%;\"></a></p>\n<p>The train_track_op in line number 415 is a list of which looks like this:<br>\n[gradient_update_op, loss_GPU1, loss_GPU2, merged_summary_op]</p>\n<p>If I comment out the section in lines 406-438 the training runs without error.</p>\n<p><strong>Attempt 3: Exact Location</strong><br>\nNow I comment out line 422 to 438,(the part where I save timeline and summary). I have checked there is no problem due to these lines.</p>\n<p>Now if, I run merged_summary op along with rest of the training op and <strong>comment out line 418 and 419,<br>\ni.e removing the run_options and run_metadata, the training goes without error</strong>. And if I leave these two lines uncommented the Segfault comes back.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/17550410/46951781-93588280-d0a6-11e8-8173-f52dfce1c3c6.png\"><img src=\"https://user-images.githubusercontent.com/17550410/46951781-93588280-d0a6-11e8-8173-f52dfce1c3c6.png\" alt=\"screenshot from 2018-10-15 17-41-41\" style=\"max-width:100%;\"></a></p>\n<p><strong>So, It seems some memory is being leaked when calculating the runtime statistics using  run_options and run_metadata when doing the full trace of the graph.</strong></p>\n<h3>Source code / logs</h3>\n<p><strong>Stack Trace</strong></p>\n<p>Program received signal SIGSEGV, Segmentation fault.<br>\n[Switching to Thread 0x7ff2d3fff700 (LWP 138819)]<br>\n0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0</p>\n<p>(gdb) backtrace<br>\n#0  0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a>  0x00007ff2b8b04fd0 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115894138\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2\">#2</a>  0x00007ff2b8d10c39 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115896656\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3\">#3</a>  0x00007ffff77fae25 in start_thread () from /lib64/libpthread.so.0<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115898449\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4\">#4</a>  0x00007ffff6e1bbad in clone () from /lib64/libc.so.6</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nCustom code. The full code can be accessed here at GitHub\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nCentOS Linux release 7.5.1804 (Core)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nBinary\nTensorFlow version (use command below):\n('v1.8.0-0-g93bc2e2072', '1.8.0')\nPython version:\nPython 2.7.5 (default, Apr 11 2018, 07:36:10)\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\ncuda-9.0-cudnn-7\nGPU model and memory:\n2 Tesla V100 Nvidia GPUs, ~15 Gb each\nExact command to reproduce:\nPlease go through this instruction here to run the code.\nThe training runs without error on small dataset. The problem arises when number of minibatches are\nmore per epoch keeping the batch size constant. (i.e more number of sess.run call per epoch)\n\nDescribe the problem\nThe training crashes with the error Segmentation Fault (Core Dumped) in the middle of the training after around 14-18 epochs (with ~800 minibatches in each) even though I have sufficient RAM (~12-20% utilization holding steadily before crash).\n\nPossible Memory Leak Checked\nInitially, I suspected a memory leak due to the addition of new ops after each iteration, so I finalized the graph in the training session. But this was not the source of the problem. No new nodes were added at each iteration.\nAttempt 2: Locating the problem\nAfter that, I ran the training on gdb (see the stack trace in logs section) and along with some commenting of the code I have almost pinpointed the source of error.\nCurrently every 30 minibatch training I am saving the summary here in the code. The full code can be found here\n\nThe train_track_op in line number 415 is a list of which looks like this:\n[gradient_update_op, loss_GPU1, loss_GPU2, merged_summary_op]\nIf I comment out the section in lines 406-438 the training runs without error.\nAttempt 3: Exact Location\nNow I comment out line 422 to 438,(the part where I save timeline and summary). I have checked there is no problem due to these lines.\nNow if, I run merged_summary op along with rest of the training op and comment out line 418 and 419,\ni.e removing the run_options and run_metadata, the training goes without error. And if I leave these two lines uncommented the Segfault comes back.\n\nSo, It seems some memory is being leaked when calculating the runtime statistics using  run_options and run_metadata when doing the full trace of the graph.\nSource code / logs\nStack Trace\nProgram received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7ff2d3fff700 (LWP 138819)]\n0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\n(gdb) backtrace\n#0  0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\n#1  0x00007ff2b8b04fd0 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\n#2  0x00007ff2b8d10c39 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\n#3  0x00007ffff77fae25 in start_thread () from /lib64/libpthread.so.0\n#4  0x00007ffff6e1bbad in clone () from /lib64/libc.so.6", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCustom code. The full code can be accessed [here at GitHub](https://github.com/grasseau/HAhRD/tree/master/GSOC18)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nCentOS Linux release 7.5.1804 (Core)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n('v1.8.0-0-g93bc2e2072', '1.8.0')\r\n- **Python version**:\r\nPython 2.7.5 (default, Apr 11 2018, 07:36:10)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\ncuda-9.0-cudnn-7\r\n- **GPU model and memory**:\r\n2 Tesla V100 Nvidia GPUs, ~15 Gb each\r\n- **Exact command to reproduce**:\r\nPlease go through this instruction [here](https://github.com/BigBang0072/HAhRD/wiki/GSOC-18-Work-Summary) to run the code.\r\nThe training runs without error on small dataset. The problem arises when number of minibatches are\r\nmore per epoch keeping the batch size constant. (i.e more number of sess.run call per epoch)\r\n\r\n### Describe the problem\r\nThe training crashes with the error **Segmentation Fault (Core Dumped) in the middle of the training after around 14-18 epochs (with ~800 minibatches in each)** even though I have sufficient RAM (~12-20% utilization holding steadily before crash).\r\n\r\n![screenshot from 2018-10-14 16-46-38](https://user-images.githubusercontent.com/17550410/46916113-c4b44e00-cfd3-11e8-842b-686a42b2c486.png)\r\n\r\n**Possible Memory Leak Checked**\r\nInitially, I suspected a memory leak due to the addition of new ops after each iteration, so I finalized the graph in the training session. But this was not the source of the problem. No new nodes were added at each iteration.\r\n\r\n**Attempt 2: Locating the problem**\r\nAfter that, I ran the training on gdb (see the stack trace in logs section) and along with some commenting of the code I have almost pinpointed the source of error.  \r\nCurrently every 30 minibatch training I am saving the summary here in the code. The full code can be found [here](https://github.com/grasseau/HAhRD/blob/master/GSOC18/train_multi_gpu.py#L406)\r\n![screenshot from 2018-10-14 17-13-59](https://user-images.githubusercontent.com/17550410/46916189-c29ebf00-cfd4-11e8-9a94-08f67c12198d.png)\r\n\r\n\r\nThe train_track_op in line number 415 is a list of which looks like this:  \r\n[gradient_update_op, loss_GPU1, loss_GPU2, merged_summary_op]\r\n\r\nIf I comment out the section in lines 406-438 the training runs without error.\r\n\r\n**Attempt 3: Exact Location** \r\nNow I comment out line 422 to 438,(the part where I save timeline and summary). I have checked there is no problem due to these lines.\r\n\r\nNow if, I run merged_summary op along with rest of the training op and **comment out line 418 and 419,\r\ni.e removing the run_options and run_metadata, the training goes without error**. And if I leave these two lines uncommented the Segfault comes back.  \r\n![screenshot from 2018-10-15 17-41-41](https://user-images.githubusercontent.com/17550410/46951781-93588280-d0a6-11e8-8173-f52dfce1c3c6.png)\r\n\r\n**So, It seems some memory is being leaked when calculating the runtime statistics using  run_options and run_metadata when doing the full trace of the graph.**\r\n\r\n\r\n### Source code / logs\r\n**Stack Trace**\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7ff2d3fff700 (LWP 138819)]\r\n0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\r\n\r\n(gdb) backtrace\r\n#0  0x00007ff2b8b00d6f in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\r\n#1  0x00007ff2b8b04fd0 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\r\n#2  0x00007ff2b8d10c39 in ?? () from /usr/local/cuda-9.0/extras/CUPTI/lib64/libcupti.so.9.0\r\n#3  0x00007ffff77fae25 in start_thread () from /lib64/libpthread.so.0\r\n#4  0x00007ffff6e1bbad in clone () from /lib64/libc.so.6\r\n\r\n"}