{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17150", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17150/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17150/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17150/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17150", "id": 298567973, "node_id": "MDU6SXNzdWUyOTg1Njc5NzM=", "number": 17150, "title": "Problem with Keras sparse_categorical_crossentropy", "user": {"login": "Hvass-Labs", "id": 13588114, "node_id": "MDQ6VXNlcjEzNTg4MTE0", "avatar_url": "https://avatars2.githubusercontent.com/u/13588114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hvass-Labs", "html_url": "https://github.com/Hvass-Labs", "followers_url": "https://api.github.com/users/Hvass-Labs/followers", "following_url": "https://api.github.com/users/Hvass-Labs/following{/other_user}", "gists_url": "https://api.github.com/users/Hvass-Labs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hvass-Labs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hvass-Labs/subscriptions", "organizations_url": "https://api.github.com/users/Hvass-Labs/orgs", "repos_url": "https://api.github.com/users/Hvass-Labs/repos", "events_url": "https://api.github.com/users/Hvass-Labs/events{/privacy}", "received_events_url": "https://api.github.com/users/Hvass-Labs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-02-20T11:58:34Z", "updated_at": "2018-11-10T18:48:41Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (pip install)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5.0 (Keras 2.1.2-tf)</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: Can't remember.</li>\n<li><strong>GPU model and memory</strong>: GTX 1070</li>\n<li><strong>Exact command to reproduce</strong>: See below.</li>\n</ul>\n<h3>Background</h3>\n<p>This issue seems to be specifically about Keras with TensorFlow so I have posted it here.</p>\n<p>I have a Keras model for doing Machine Translation of human languages. It has an encoder and decoder each of which use the <code>Embedding</code> and <code>GRU</code> layers from Keras. The output of the decoder is a one-hot encoded array.</p>\n<p>My data-set is from Europarl so it is very large already and converting the target-data from integer-tokens to one-hot-encoded labels would be extremely wasteful and take many GB of memory.</p>\n<p>One solution would be to write my own data-generator and only convert integer-tokens to one-hot-labels for a batch at a time. But that's not a very elegant solution.</p>\n<p>The correct solution is of course to use a sparse version of the crossentropy-loss which automatically converts the integer-tokens to a one-hot-encoded label for comparison to the model's output. Keras' has a built-in loss-function for doing exactly this called <code>sparse_categorical_crossentropy</code>. However, it doesn't seem to work as intended.</p>\n<h3>Error</h3>\n<p>The following shows the essential parts of the code.</p>\n<pre><code># (Omitted code for building neural network ...)\n\n# Output of the decoder-part of the neural network.\ndecoder_dense = Dense(num_words,\n                      activation='softmax',\n                      name='decoder_output')\ndecoder_output = decoder_dense(decoder_gru_output)\n\nmodel = Model(inputs=[encoder_input, decoder_input],\n              outputs=[decoder_output])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy')\n\nmodel.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\n</code></pre>\n<p>Everything runs fine except <code>model.fit()</code> at the end which gives this error:</p>\n<pre><code>ValueError: Error when checking target: expected decoder_output to have 3 dimensions, but got array with shape (20000, 67)\n</code></pre>\n<p>This is the shape of the model's output:</p>\n<pre><code>&gt;&gt;&gt; decoder_output.get_shape()\nTensorShape([Dimension(None), Dimension(None), Dimension(10000)])\n</code></pre>\n<p>This is the shape of the target-data, which is a 2-dim array of integer-values:</p>\n<pre><code>y_data['decoder_output'].shape\n&gt;&gt;&gt; (20000, 67)\n</code></pre>\n<p>Note that I only allow sequences of length 67 for the decoder's output.</p>\n<h3>Working Solution</h3>\n<p>We can use TensorFlow's implementation of sparse cross-entropy, which seems to work as intended.</p>\n<p>First we need to have a linear activation on the output of the decoder:</p>\n<pre><code>decoder_dense = Dense(num_words,\n                      activation='linear', # NOTE: changed from 'softmax'\n                      name='decoder_output')\n</code></pre>\n<p>Then we need a wrapper-function for the loss that is compatible with Keras:</p>\n<pre><code>def sparse_loss(y_true, y_pred):\n    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n                                                          logits=y_pred)\n</code></pre>\n<p>Then we need to create a placeholder variable for the batch of target-values. Once again I only allow sequences of length 67 (this is of course a variable in my own code).</p>\n<pre><code>decoder_target = tf.placeholder(dtype='int32', shape=(None, 67))\n\nmodel_train.compile(optimizer='adam,\n                    loss=sparse_loss,\n                    target_tensors=[decoder_target])\n</code></pre>\n<p>This works fine and we can train it by calling:</p>\n<pre><code>model.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\n</code></pre>\n<p>Maybe Keras should use TensorFlow's sparse-cross-entropy more directly, because it seems to handle higher-dim data better?</p>\n<h3>Documentation</h3>\n<p>Looking at the implementation of <code>sparse_categorical_crossentropy</code> in Keras there is actually some reshaping going on there, but the doc-string doesn't make clear what is assumed of the input/output dims and when/how reshaping is supposed to be done, so it's impossible to know whether it is a bug or a feature I am experiencing, and how to deal with it properly.</p>\n<p>The doc-string needs to be made more clear by someone who understands the intention of this code.</p>\n<p>Furthermore, the doc-string needs to be \"exported\" somehow to the online docs because it is not shown here: <a href=\"https://keras.io/losses/#sparse_categorical_crossentropy\" rel=\"nofollow\">https://keras.io/losses/#sparse_categorical_crossentropy</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary (pip install)\nTensorFlow version (use command below): 1.5.0 (Keras 2.1.2-tf)\nPython version: 3.6\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: Can't remember.\nGPU model and memory: GTX 1070\nExact command to reproduce: See below.\n\nBackground\nThis issue seems to be specifically about Keras with TensorFlow so I have posted it here.\nI have a Keras model for doing Machine Translation of human languages. It has an encoder and decoder each of which use the Embedding and GRU layers from Keras. The output of the decoder is a one-hot encoded array.\nMy data-set is from Europarl so it is very large already and converting the target-data from integer-tokens to one-hot-encoded labels would be extremely wasteful and take many GB of memory.\nOne solution would be to write my own data-generator and only convert integer-tokens to one-hot-labels for a batch at a time. But that's not a very elegant solution.\nThe correct solution is of course to use a sparse version of the crossentropy-loss which automatically converts the integer-tokens to a one-hot-encoded label for comparison to the model's output. Keras' has a built-in loss-function for doing exactly this called sparse_categorical_crossentropy. However, it doesn't seem to work as intended.\nError\nThe following shows the essential parts of the code.\n# (Omitted code for building neural network ...)\n\n# Output of the decoder-part of the neural network.\ndecoder_dense = Dense(num_words,\n                      activation='softmax',\n                      name='decoder_output')\ndecoder_output = decoder_dense(decoder_gru_output)\n\nmodel = Model(inputs=[encoder_input, decoder_input],\n              outputs=[decoder_output])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy')\n\nmodel.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\n\nEverything runs fine except model.fit() at the end which gives this error:\nValueError: Error when checking target: expected decoder_output to have 3 dimensions, but got array with shape (20000, 67)\n\nThis is the shape of the model's output:\n>>> decoder_output.get_shape()\nTensorShape([Dimension(None), Dimension(None), Dimension(10000)])\n\nThis is the shape of the target-data, which is a 2-dim array of integer-values:\ny_data['decoder_output'].shape\n>>> (20000, 67)\n\nNote that I only allow sequences of length 67 for the decoder's output.\nWorking Solution\nWe can use TensorFlow's implementation of sparse cross-entropy, which seems to work as intended.\nFirst we need to have a linear activation on the output of the decoder:\ndecoder_dense = Dense(num_words,\n                      activation='linear', # NOTE: changed from 'softmax'\n                      name='decoder_output')\n\nThen we need a wrapper-function for the loss that is compatible with Keras:\ndef sparse_loss(y_true, y_pred):\n    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n                                                          logits=y_pred)\n\nThen we need to create a placeholder variable for the batch of target-values. Once again I only allow sequences of length 67 (this is of course a variable in my own code).\ndecoder_target = tf.placeholder(dtype='int32', shape=(None, 67))\n\nmodel_train.compile(optimizer='adam,\n                    loss=sparse_loss,\n                    target_tensors=[decoder_target])\n\nThis works fine and we can train it by calling:\nmodel.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\n\nMaybe Keras should use TensorFlow's sparse-cross-entropy more directly, because it seems to handle higher-dim data better?\nDocumentation\nLooking at the implementation of sparse_categorical_crossentropy in Keras there is actually some reshaping going on there, but the doc-string doesn't make clear what is assumed of the input/output dims and when/how reshaping is supposed to be done, so it's impossible to know whether it is a bug or a feature I am experiencing, and how to deal with it properly.\nThe doc-string needs to be made more clear by someone who understands the intention of this code.\nFurthermore, the doc-string needs to be \"exported\" somehow to the online docs because it is not shown here: https://keras.io/losses/#sparse_categorical_crossentropy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (pip install)\r\n- **TensorFlow version (use command below)**: 1.5.0 (Keras 2.1.2-tf)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: Can't remember.\r\n- **GPU model and memory**: GTX 1070\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Background\r\n\r\nThis issue seems to be specifically about Keras with TensorFlow so I have posted it here.\r\n\r\nI have a Keras model for doing Machine Translation of human languages. It has an encoder and decoder each of which use the `Embedding` and `GRU` layers from Keras. The output of the decoder is a one-hot encoded array.\r\n\r\nMy data-set is from Europarl so it is very large already and converting the target-data from integer-tokens to one-hot-encoded labels would be extremely wasteful and take many GB of memory.\r\n\r\nOne solution would be to write my own data-generator and only convert integer-tokens to one-hot-labels for a batch at a time. But that's not a very elegant solution.\r\n\r\nThe correct solution is of course to use a sparse version of the crossentropy-loss which automatically converts the integer-tokens to a one-hot-encoded label for comparison to the model's output. Keras' has a built-in loss-function for doing exactly this called `sparse_categorical_crossentropy`. However, it doesn't seem to work as intended.\r\n\r\n### Error\r\n\r\nThe following shows the essential parts of the code.\r\n\r\n    # (Omitted code for building neural network ...)\r\n\r\n    # Output of the decoder-part of the neural network.\r\n    decoder_dense = Dense(num_words,\r\n                          activation='softmax',\r\n                          name='decoder_output')\r\n    decoder_output = decoder_dense(decoder_gru_output)\r\n\r\n    model = Model(inputs=[encoder_input, decoder_input],\r\n                  outputs=[decoder_output])\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy')\r\n\r\n    model.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\r\n\r\nEverything runs fine except `model.fit()` at the end which gives this error:\r\n\r\n    ValueError: Error when checking target: expected decoder_output to have 3 dimensions, but got array with shape (20000, 67)\r\n\r\nThis is the shape of the model's output:\r\n\r\n    >>> decoder_output.get_shape()\r\n    TensorShape([Dimension(None), Dimension(None), Dimension(10000)])\r\n\r\nThis is the shape of the target-data, which is a 2-dim array of integer-values:\r\n\r\n    y_data['decoder_output'].shape\r\n    >>> (20000, 67)\r\n\r\nNote that I only allow sequences of length 67 for the decoder's output.\r\n\r\n### Working Solution\r\n\r\nWe can use TensorFlow's implementation of sparse cross-entropy, which seems to work as intended.\r\n\r\nFirst we need to have a linear activation on the output of the decoder:\r\n\r\n    decoder_dense = Dense(num_words,\r\n                          activation='linear', # NOTE: changed from 'softmax'\r\n                          name='decoder_output')\r\n\r\nThen we need a wrapper-function for the loss that is compatible with Keras:\r\n\r\n    def sparse_loss(y_true, y_pred):\r\n        return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\r\n                                                              logits=y_pred)\r\n\r\nThen we need to create a placeholder variable for the batch of target-values. Once again I only allow sequences of length 67 (this is of course a variable in my own code).\r\n\r\n    decoder_target = tf.placeholder(dtype='int32', shape=(None, 67))\r\n\r\n    model_train.compile(optimizer='adam,\r\n                        loss=sparse_loss,\r\n                        target_tensors=[decoder_target])\r\n\r\nThis works fine and we can train it by calling:\r\n\r\n    model.fit(x=x_data, y=y_data, batch_size=128, epochs=3)\r\n\r\nMaybe Keras should use TensorFlow's sparse-cross-entropy more directly, because it seems to handle higher-dim data better?\r\n\r\n### Documentation\r\n\r\nLooking at the implementation of `sparse_categorical_crossentropy` in Keras there is actually some reshaping going on there, but the doc-string doesn't make clear what is assumed of the input/output dims and when/how reshaping is supposed to be done, so it's impossible to know whether it is a bug or a feature I am experiencing, and how to deal with it properly.\r\n\r\nThe doc-string needs to be made more clear by someone who understands the intention of this code.\r\n\r\nFurthermore, the doc-string needs to be \"exported\" somehow to the online docs because it is not shown here: https://keras.io/losses/#sparse_categorical_crossentropy\r\n"}