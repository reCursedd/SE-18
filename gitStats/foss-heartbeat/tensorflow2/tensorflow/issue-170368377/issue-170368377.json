{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3727", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3727/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3727/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3727/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3727", "id": 170368377, "node_id": "MDU6SXNzdWUxNzAzNjgzNzc=", "number": 3727, "title": "distributed seq2seq: too much device placement logs", "user": {"login": "DjangoPeng", "id": 16943353, "node_id": "MDQ6VXNlcjE2OTQzMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16943353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DjangoPeng", "html_url": "https://github.com/DjangoPeng", "followers_url": "https://api.github.com/users/DjangoPeng/followers", "following_url": "https://api.github.com/users/DjangoPeng/following{/other_user}", "gists_url": "https://api.github.com/users/DjangoPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/DjangoPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DjangoPeng/subscriptions", "organizations_url": "https://api.github.com/users/DjangoPeng/orgs", "repos_url": "https://api.github.com/users/DjangoPeng/repos", "events_url": "https://api.github.com/users/DjangoPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/DjangoPeng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-08-10T09:09:00Z", "updated_at": "2016-09-05T02:17:23Z", "closed_at": "2016-08-11T07:08:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm trying the distributed seq2seq model. But when I run the program, there are too much device placement logs. Just like this below:</p>\n<pre><code>sync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nsync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0\nVariable_1/initial_value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Variable_1/initial_value: /job:ps/replica:0/task:0/cpu:0\nmul/y: /job:worker/replica:0/task:1/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] mul/y: /job:worker/replica:0/task:1/gpu:0\nGradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0\nFill_3/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_3/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0\nFill_2/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_2/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0\nFill_1/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_1/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent/value: /job:ps/replica:0/task:0/cpu:0\nFill/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill/dims: /job:ps/replica:0/task:0/gpu:0\nVariable/initial_value: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Variable/initial_value: /job:ps/replica:0/task:0/gpu:0\n</code></pre>\n<p>While when I run the original seq2seq model in single machine, I don't have the problem. I locate the <code>simple_placer.cc:818</code>, finding they are really log ouputs.</p>\n<h2>Why does it output in the distributed model?</h2>\n<p>simple_placer.cc</p>\n<pre><code>void SimplePlacer::AssignAndLog(const string&amp; assigned_device,\n                                Node* node) const {\n  node-&gt;set_assigned_device_name(assigned_device);\n  // Log placement if log_device_placement is set.\n  if (options_ &amp;&amp; options_-&gt;config.log_device_placement()) {\n    printf(\"%s: %s\\n\", node-&gt;name().c_str(),\n           node-&gt;assigned_device_name().c_str());\n    LOG(INFO) &lt;&lt; node-&gt;name() &lt;&lt; \": \" &lt;&lt; node-&gt;assigned_device_name();\n  }\n}\n</code></pre>", "body_text": "I'm trying the distributed seq2seq model. But when I run the program, there are too much device placement logs. Just like this below:\nsync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nsync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0\nVariable_1/initial_value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Variable_1/initial_value: /job:ps/replica:0/task:0/cpu:0\nmul/y: /job:worker/replica:0/task:1/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] mul/y: /job:worker/replica:0/task:1/gpu:0\nGradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0\nFill_3/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_3/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0\nFill_2/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_2/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0\nFill_1/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_1/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent/value: /job:ps/replica:0/task:0/cpu:0\nFill/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill/dims: /job:ps/replica:0/task:0/gpu:0\nVariable/initial_value: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Variable/initial_value: /job:ps/replica:0/task:0/gpu:0\n\nWhile when I run the original seq2seq model in single machine, I don't have the problem. I locate the simple_placer.cc:818, finding they are really log ouputs.\nWhy does it output in the distributed model?\nsimple_placer.cc\nvoid SimplePlacer::AssignAndLog(const string& assigned_device,\n                                Node* node) const {\n  node->set_assigned_device_name(assigned_device);\n  // Log placement if log_device_placement is set.\n  if (options_ && options_->config.log_device_placement()) {\n    printf(\"%s: %s\\n\", node->name().c_str(),\n           node->assigned_device_name().c_str());\n    LOG(INFO) << node->name() << \": \" << node->assigned_device_name();\n  }\n}", "body": "I'm trying the distributed seq2seq model. But when I run the program, there are too much device placement logs. Just like this below:\n\n```\nsync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_4_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nsync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] sync_replicas/fifo_queue_2_enqueue/component_0: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_3/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_2/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate_1/indices: /job:ps/replica:0/task:0/cpu:0\nScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] ScatterUpdate/indices: /job:ps/replica:0/task:0/cpu:0\nVariable_1/initial_value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Variable_1/initial_value: /job:ps/replica:0/task:0/cpu:0\nmul/y: /job:worker/replica:0/task:1/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] mul/y: /job:worker/replica:0/task:1/gpu:0\nGradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_3/value: /job:ps/replica:0/task:0/cpu:0\nFill_3/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_3/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_2/value: /job:ps/replica:0/task:0/cpu:0\nFill_2/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_2/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent_1/value: /job:ps/replica:0/task:0/cpu:0\nFill_1/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill_1/dims: /job:ps/replica:0/task:0/gpu:0\nGradientDescent/value: /job:ps/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] GradientDescent/value: /job:ps/replica:0/task:0/cpu:0\nFill/dims: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Fill/dims: /job:ps/replica:0/task:0/gpu:0\nVariable/initial_value: /job:ps/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Variable/initial_value: /job:ps/replica:0/task:0/gpu:0\n```\n\nWhile when I run the original seq2seq model in single machine, I don't have the problem. I locate the `simple_placer.cc:818`, finding they are really log ouputs. \n## Why does it output in the distributed model?\n\nsimple_placer.cc\n\n```\nvoid SimplePlacer::AssignAndLog(const string& assigned_device,\n                                Node* node) const {\n  node->set_assigned_device_name(assigned_device);\n  // Log placement if log_device_placement is set.\n  if (options_ && options_->config.log_device_placement()) {\n    printf(\"%s: %s\\n\", node->name().c_str(),\n           node->assigned_device_name().c_str());\n    LOG(INFO) << node->name() << \": \" << node->assigned_device_name();\n  }\n}\n```\n"}