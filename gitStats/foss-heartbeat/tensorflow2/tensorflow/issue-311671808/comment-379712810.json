{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/379712810", "html_url": "https://github.com/tensorflow/tensorflow/issues/18266#issuecomment-379712810", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18266", "id": 379712810, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTcxMjgxMA==", "user": {"login": "bhack", "id": 1710528, "node_id": "MDQ6VXNlcjE3MTA1Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1710528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bhack", "html_url": "https://github.com/bhack", "followers_url": "https://api.github.com/users/bhack/followers", "following_url": "https://api.github.com/users/bhack/following{/other_user}", "gists_url": "https://api.github.com/users/bhack/gists{/gist_id}", "starred_url": "https://api.github.com/users/bhack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bhack/subscriptions", "organizations_url": "https://api.github.com/users/bhack/orgs", "repos_url": "https://api.github.com/users/bhack/repos", "events_url": "https://api.github.com/users/bhack/events{/privacy}", "received_events_url": "https://api.github.com/users/bhack/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T10:53:54Z", "updated_at": "2018-04-09T14:09:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> I think that there is a conceptual problem with <code>train_and_evaluate</code>.<br>\nMainly you control train/eval phases with <code>EvalSpec</code> (with a default throttle_secs=600) and/or check point in <code>RunConfig</code>.<br>\nIf the dataset is quite big you cannot evaluate until the cache is done cause a new TrainSpec call will rebuild the input/dataset pipeline and you will find a lock on the cache dir.<br>\nThere could be a not so trivial workaround with <code>RunConfig</code> <code>save_checkpoints_steps</code> to block throttle until you will not have covered the epoch  but in the distributed setup I suppose that the dataset cache is working locally per node so is it still plausible?</p>\n<p>Do you see any other solution (that I suppose still require documentation)?</p>", "body_text": "@mrry I think that there is a conceptual problem with train_and_evaluate.\nMainly you control train/eval phases with EvalSpec (with a default throttle_secs=600) and/or check point in RunConfig.\nIf the dataset is quite big you cannot evaluate until the cache is done cause a new TrainSpec call will rebuild the input/dataset pipeline and you will find a lock on the cache dir.\nThere could be a not so trivial workaround with RunConfig save_checkpoints_steps to block throttle until you will not have covered the epoch  but in the distributed setup I suppose that the dataset cache is working locally per node so is it still plausible?\nDo you see any other solution (that I suppose still require documentation)?", "body": "@mrry I think that there is a conceptual problem with `train_and_evaluate`. \r\nMainly you control train/eval phases with `EvalSpec` (with a default throttle_secs=600) and/or check point in `RunConfig`.\r\nIf the dataset is quite big you cannot evaluate until the cache is done cause a new TrainSpec call will rebuild the input/dataset pipeline and you will find a lock on the cache dir.\r\nThere could be a not so trivial workaround with `RunConfig` `save_checkpoints_steps` to block throttle until you will not have covered the epoch  but in the distributed setup I suppose that the dataset cache is working locally per node so is it still plausible?\r\n\r\nDo you see any other solution (that I suppose still require documentation)?\r\n  "}