{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18266", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18266/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18266/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18266/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18266", "id": 311671808, "node_id": "MDU6SXNzdWUzMTE2NzE4MDg=", "number": 18266, "title": "Cache lockfile already exists", "user": {"login": "lenlen", "id": 2477675, "node_id": "MDQ6VXNlcjI0Nzc2NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/2477675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lenlen", "html_url": "https://github.com/lenlen", "followers_url": "https://api.github.com/users/lenlen/followers", "following_url": "https://api.github.com/users/lenlen/following{/other_user}", "gists_url": "https://api.github.com/users/lenlen/gists{/gist_id}", "starred_url": "https://api.github.com/users/lenlen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lenlen/subscriptions", "organizations_url": "https://api.github.com/users/lenlen/orgs", "repos_url": "https://api.github.com/users/lenlen/repos", "events_url": "https://api.github.com/users/lenlen/events{/privacy}", "received_events_url": "https://api.github.com/users/lenlen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-04-05T16:01:18Z", "updated_at": "2018-11-14T19:16:40Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Using the train_and_evaluate method from estimator API and cache policy on filesystem, I get an error because the evaluation starts before that all cache is written on the filesystem. So when the train runs the second time, after the evaluation, I get the error because it finds the lock file.<br>\nIf the cache is written before the first evaluation I don't get the error.</p>\n<p>Here a snippet runnable on colab</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras as ks\nimport itertools\n\n\ndef batch_reshape(a, b):\n    a.set_shape([None, None, 3])\n    b.set_shape([None, None, 1])\n    return (a, b)\n\n\ndef gen(n):\n    a = np.array(np.random.rand(5, 5, 3).astype(np.float32))\n    b = np.array(np.random.rand(5, 5, 1).astype(np.float32))\n    return (a, b)\n\n\ndef my_input_fn_train():\n    ds = tf.data.Dataset.range(100000)\n    ds = ds.map(\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\n    ds = ds.map(batch_reshape)\n    ds = ds.cache(\"/tmp/mycache_train\")\n\n    ds = ds.repeat(3)\n\n    value = ds.make_one_shot_iterator().get_next()\n\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\n\n\ndef my_input_fn_eval():\n    ds = tf.data.Dataset.range(100000)\n    ds = ds.map(\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\n    ds = ds.map(batch_reshape)\n    ds = ds.cache(\"/tmp/mycache_eval\")\n\n    ds = ds.repeat(3)\n\n    value = ds.make_one_shot_iterator().get_next()\n\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\n\n\ndef main():\n    tf.logging.set_verbosity(tf.logging.INFO)\n    input_rgb = ks.layers.Input(shape=(1, 5, 5, 3), name=\"input_rgb\")\n    x = ks.layers.Dense(1, activation='relu', name=\"Dense_1\")(input_rgb)\n    x = ks.layers.Dense(1, activation='softmax', name=\"softmax\")(x)\n    model = ks.models.Model(inputs=[input_rgb], outputs=[x])\n    model.compile(\n        loss={'softmax': 'binary_crossentropy'},\n        optimizer=tf.keras.optimizers.Adam())\n\n    estimator = ks.estimator.model_to_estimator(keras_model=model)\n\n    train_spec = tf.estimator.TrainSpec(input_fn=my_input_fn_train)\n\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\n\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n\nif __name__ == \"__main__\":\n    main()\n\n</code></pre>\n<p>and here the output</p>\n<pre><code>INFO:tensorflow:Using the Keras model from memory.\nINFO:tensorflow:Using default config.\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpd1r865ry\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpd1r865ry', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb48f9ecf8&gt;, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 2 secs (eval_spec.throttle_secs) or training is finished.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/keras_model.ckpt\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpd1r865ry/model.ckpt.\nINFO:tensorflow:loss = 9.25762, step = 1\nINFO:tensorflow:global_step/sec: 612.893\nINFO:tensorflow:loss = 9.348354, step = 101 (0.166 sec)\nINFO:tensorflow:global_step/sec: 952.69\nINFO:tensorflow:loss = 8.319871, step = 201 (0.108 sec)\nINFO:tensorflow:global_step/sec: 838.883\nINFO:tensorflow:loss = 7.7207212, step = 301 (0.119 sec)\nINFO:tensorflow:global_step/sec: 917.55\nINFO:tensorflow:loss = 8.657611, step = 401 (0.110 sec)\nINFO:tensorflow:global_step/sec: 866.062\nINFO:tensorflow:loss = 8.567427, step = 501 (0.118 sec)\nINFO:tensorflow:global_step/sec: 862.789\nINFO:tensorflow:loss = 7.4227247, step = 601 (0.110 sec)\nINFO:tensorflow:global_step/sec: 847.577\nINFO:tensorflow:loss = 6.676866, step = 701 (0.118 sec)\nINFO:tensorflow:global_step/sec: 911.827\nINFO:tensorflow:loss = 7.4915752, step = 801 (0.112 sec)\nINFO:tensorflow:global_step/sec: 837.15\nINFO:tensorflow:loss = 7.7332606, step = 901 (0.120 sec)\nINFO:tensorflow:global_step/sec: 857.915\nINFO:tensorflow:loss = 7.283838, step = 1001 (0.116 sec)\nINFO:tensorflow:global_step/sec: 786.144\nINFO:tensorflow:loss = 8.61713, step = 1101 (0.127 sec)\nINFO:tensorflow:Saving checkpoints for 1150 into /tmp/tmpd1r865ry/model.ckpt.\nINFO:tensorflow:Loss for final step: 8.417924.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2018-04-05-15:36:08\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Evaluation [1/5]\nINFO:tensorflow:Evaluation [2/5]\nINFO:tensorflow:Evaluation [3/5]\nINFO:tensorflow:Evaluation [4/5]\nINFO:tensorflow:Evaluation [5/5]\nINFO:tensorflow:Finished evaluation at 2018-04-05-15:36:08\n\nINFO:tensorflow:Saving dict for global step 1150: global_step = 1150, loss = 8.261229\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\n\n---------------------------------------------------------------------------\nAlreadyExistsError                        Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\n   1360     try:\n-&gt; 1361       return fn(*args)\n   1362     except errors.OpError as e:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\n   1339           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n-&gt; 1340                                    target_list, status, run_metadata)\n   1341 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\n    515             compat.as_text(c_api.TF_Message(self.status.status)),\n--&gt; 516             c_api.TF_GetCode(self.status.status))\n    517     # Delete the underlying status object from memory otherwise it stays alive\n\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nDuring handling of the above exception, another exception occurred:\n\nAlreadyExistsError                        Traceback (most recent call last)\n&lt;ipython-input-5-9d8332f50ddd&gt; in &lt;module&gt;()\n     66 \n     67 if __name__ == \"__main__\":\n---&gt; 68     main()\n\n&lt;ipython-input-5-9d8332f50ddd&gt; in main()\n     62         input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\n     63 \n---&gt; 64     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n     65 \n     66 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\n    419         '(with task id 0).  Given task id {}'.format(config.task_id))\n    420 \n--&gt; 421   executor.run()\n    422 \n    423 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run(self)\n    492         config.task_type != run_config_lib.TaskType.EVALUATOR):\n    493       logging.info('Running training and evaluation locally (non-distributed).')\n--&gt; 494       self.run_local()\n    495       return\n    496 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run_local(self)\n    624           input_fn=self._train_spec.input_fn,\n    625           max_steps=self._train_spec.max_steps,\n--&gt; 626           hooks=train_hooks)\n    627 \n    628       # Final export signal: For any eval result with global_step &gt;= train\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    350 \n    351     saving_listeners = _check_listeners_type(saving_listeners)\n--&gt; 352     loss = self._train_model(input_fn, hooks, saving_listeners)\n    353     logging.info('Loss for final step: %s.', loss)\n    354     return self\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\n    889         loss = None\n    890         while not mon_sess.should_stop():\n--&gt; 891           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n    892       return loss\n    893 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\n    544                           feed_dict=feed_dict,\n    545                           options=options,\n--&gt; 546                           run_metadata=run_metadata)\n    547 \n    548   def run_step_fn(self, step_fn):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\n   1020                               feed_dict=feed_dict,\n   1021                               options=options,\n-&gt; 1022                               run_metadata=run_metadata)\n   1023       except _PREEMPTION_ERRORS as e:\n   1024         logging.info('An error was raised. This may be due to a preemption in '\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\n   1111         raise six.reraise(*original_exc_info)\n   1112       else:\n-&gt; 1113         raise six.reraise(*original_exc_info)\n   1114 \n   1115 \n\n/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)\n    691             if value.__traceback__ is not tb:\n    692                 raise value.with_traceback(tb)\n--&gt; 693             raise value\n    694         finally:\n    695             value = None\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\n   1096   def run(self, *args, **kwargs):\n   1097     try:\n-&gt; 1098       return self._sess.run(*args, **kwargs)\n   1099     except _PREEMPTION_ERRORS:\n   1100       raise\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\n   1168                                   feed_dict=feed_dict,\n   1169                                   options=options,\n-&gt; 1170                                   run_metadata=run_metadata)\n   1171 \n   1172     for hook in self._hooks:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\n    948 \n    949   def run(self, *args, **kwargs):\n--&gt; 950     return self._sess.run(*args, **kwargs)\n    951 \n    952   def run_step_fn(self, step_fn, raw_session, run_with_hooks):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    903     try:\n    904       result = self._run(None, fetches, feed_dict, options_ptr,\n--&gt; 905                          run_metadata_ptr)\n    906       if run_metadata:\n    907         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1135     if final_fetches or final_targets or (handle and feed_dict_tensor):\n   1136       results = self._do_run(handle, final_targets, final_fetches,\n-&gt; 1137                              feed_dict_tensor, options, run_metadata)\n   1138     else:\n   1139       results = []\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n   1353     if handle is None:\n   1354       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n-&gt; 1355                            options, run_metadata)\n   1356     else:\n   1357       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\n   1372         except KeyError:\n   1373           pass\n-&gt; 1374       raise type(e)(node_def, op, message)\n   1375 \n   1376   def _extend_graph(self):\n\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in &lt;module&gt;\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-5-9d8332f50ddd&gt;\", line 68, in &lt;module&gt;\n    main()\n  File \"&lt;ipython-input-5-9d8332f50ddd&gt;\", line 64, in main\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 421, in train_and_evaluate\n    executor.run()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 494, in run\n    self.run_local()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 626, in run_local\n    hooks=train_hooks)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 352, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 809, in _train_model\n    input_fn, model_fn_lib.ModeKeys.TRAIN))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 668, in _get_features_and_labels_from_input_fn\n    result = self._call_input_fn(input_fn, mode)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 760, in _call_input_fn\n    return input_fn(**kwargs)\n  File \"&lt;ipython-input-5-9d8332f50ddd&gt;\", line 28, in my_input_fn_train\n    value = ds.make_one_shot_iterator().get_next()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 330, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 866, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nAlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\n</code></pre>", "body_text": "Using the train_and_evaluate method from estimator API and cache policy on filesystem, I get an error because the evaluation starts before that all cache is written on the filesystem. So when the train runs the second time, after the evaluation, I get the error because it finds the lock file.\nIf the cache is written before the first evaluation I don't get the error.\nHere a snippet runnable on colab\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras as ks\nimport itertools\n\n\ndef batch_reshape(a, b):\n    a.set_shape([None, None, 3])\n    b.set_shape([None, None, 1])\n    return (a, b)\n\n\ndef gen(n):\n    a = np.array(np.random.rand(5, 5, 3).astype(np.float32))\n    b = np.array(np.random.rand(5, 5, 1).astype(np.float32))\n    return (a, b)\n\n\ndef my_input_fn_train():\n    ds = tf.data.Dataset.range(100000)\n    ds = ds.map(\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\n    ds = ds.map(batch_reshape)\n    ds = ds.cache(\"/tmp/mycache_train\")\n\n    ds = ds.repeat(3)\n\n    value = ds.make_one_shot_iterator().get_next()\n\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\n\n\ndef my_input_fn_eval():\n    ds = tf.data.Dataset.range(100000)\n    ds = ds.map(\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\n    ds = ds.map(batch_reshape)\n    ds = ds.cache(\"/tmp/mycache_eval\")\n\n    ds = ds.repeat(3)\n\n    value = ds.make_one_shot_iterator().get_next()\n\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\n\n\ndef main():\n    tf.logging.set_verbosity(tf.logging.INFO)\n    input_rgb = ks.layers.Input(shape=(1, 5, 5, 3), name=\"input_rgb\")\n    x = ks.layers.Dense(1, activation='relu', name=\"Dense_1\")(input_rgb)\n    x = ks.layers.Dense(1, activation='softmax', name=\"softmax\")(x)\n    model = ks.models.Model(inputs=[input_rgb], outputs=[x])\n    model.compile(\n        loss={'softmax': 'binary_crossentropy'},\n        optimizer=tf.keras.optimizers.Adam())\n\n    estimator = ks.estimator.model_to_estimator(keras_model=model)\n\n    train_spec = tf.estimator.TrainSpec(input_fn=my_input_fn_train)\n\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\n\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\nand here the output\nINFO:tensorflow:Using the Keras model from memory.\nINFO:tensorflow:Using default config.\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpd1r865ry\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpd1r865ry', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb48f9ecf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 2 secs (eval_spec.throttle_secs) or training is finished.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/keras_model.ckpt\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpd1r865ry/model.ckpt.\nINFO:tensorflow:loss = 9.25762, step = 1\nINFO:tensorflow:global_step/sec: 612.893\nINFO:tensorflow:loss = 9.348354, step = 101 (0.166 sec)\nINFO:tensorflow:global_step/sec: 952.69\nINFO:tensorflow:loss = 8.319871, step = 201 (0.108 sec)\nINFO:tensorflow:global_step/sec: 838.883\nINFO:tensorflow:loss = 7.7207212, step = 301 (0.119 sec)\nINFO:tensorflow:global_step/sec: 917.55\nINFO:tensorflow:loss = 8.657611, step = 401 (0.110 sec)\nINFO:tensorflow:global_step/sec: 866.062\nINFO:tensorflow:loss = 8.567427, step = 501 (0.118 sec)\nINFO:tensorflow:global_step/sec: 862.789\nINFO:tensorflow:loss = 7.4227247, step = 601 (0.110 sec)\nINFO:tensorflow:global_step/sec: 847.577\nINFO:tensorflow:loss = 6.676866, step = 701 (0.118 sec)\nINFO:tensorflow:global_step/sec: 911.827\nINFO:tensorflow:loss = 7.4915752, step = 801 (0.112 sec)\nINFO:tensorflow:global_step/sec: 837.15\nINFO:tensorflow:loss = 7.7332606, step = 901 (0.120 sec)\nINFO:tensorflow:global_step/sec: 857.915\nINFO:tensorflow:loss = 7.283838, step = 1001 (0.116 sec)\nINFO:tensorflow:global_step/sec: 786.144\nINFO:tensorflow:loss = 8.61713, step = 1101 (0.127 sec)\nINFO:tensorflow:Saving checkpoints for 1150 into /tmp/tmpd1r865ry/model.ckpt.\nINFO:tensorflow:Loss for final step: 8.417924.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2018-04-05-15:36:08\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Evaluation [1/5]\nINFO:tensorflow:Evaluation [2/5]\nINFO:tensorflow:Evaluation [3/5]\nINFO:tensorflow:Evaluation [4/5]\nINFO:tensorflow:Evaluation [5/5]\nINFO:tensorflow:Finished evaluation at 2018-04-05-15:36:08\n\nINFO:tensorflow:Saving dict for global step 1150: global_step = 1150, loss = 8.261229\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\n\n---------------------------------------------------------------------------\nAlreadyExistsError                        Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\n   1360     try:\n-> 1361       return fn(*args)\n   1362     except errors.OpError as e:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\n   1339           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n-> 1340                                    target_list, status, run_metadata)\n   1341 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\n    515             compat.as_text(c_api.TF_Message(self.status.status)),\n--> 516             c_api.TF_GetCode(self.status.status))\n    517     # Delete the underlying status object from memory otherwise it stays alive\n\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nDuring handling of the above exception, another exception occurred:\n\nAlreadyExistsError                        Traceback (most recent call last)\n<ipython-input-5-9d8332f50ddd> in <module>()\n     66 \n     67 if __name__ == \"__main__\":\n---> 68     main()\n\n<ipython-input-5-9d8332f50ddd> in main()\n     62         input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\n     63 \n---> 64     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n     65 \n     66 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\n    419         '(with task id 0).  Given task id {}'.format(config.task_id))\n    420 \n--> 421   executor.run()\n    422 \n    423 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run(self)\n    492         config.task_type != run_config_lib.TaskType.EVALUATOR):\n    493       logging.info('Running training and evaluation locally (non-distributed).')\n--> 494       self.run_local()\n    495       return\n    496 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run_local(self)\n    624           input_fn=self._train_spec.input_fn,\n    625           max_steps=self._train_spec.max_steps,\n--> 626           hooks=train_hooks)\n    627 \n    628       # Final export signal: For any eval result with global_step >= train\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    350 \n    351     saving_listeners = _check_listeners_type(saving_listeners)\n--> 352     loss = self._train_model(input_fn, hooks, saving_listeners)\n    353     logging.info('Loss for final step: %s.', loss)\n    354     return self\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\n    889         loss = None\n    890         while not mon_sess.should_stop():\n--> 891           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n    892       return loss\n    893 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\n    544                           feed_dict=feed_dict,\n    545                           options=options,\n--> 546                           run_metadata=run_metadata)\n    547 \n    548   def run_step_fn(self, step_fn):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\n   1020                               feed_dict=feed_dict,\n   1021                               options=options,\n-> 1022                               run_metadata=run_metadata)\n   1023       except _PREEMPTION_ERRORS as e:\n   1024         logging.info('An error was raised. This may be due to a preemption in '\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\n   1111         raise six.reraise(*original_exc_info)\n   1112       else:\n-> 1113         raise six.reraise(*original_exc_info)\n   1114 \n   1115 \n\n/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)\n    691             if value.__traceback__ is not tb:\n    692                 raise value.with_traceback(tb)\n--> 693             raise value\n    694         finally:\n    695             value = None\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\n   1096   def run(self, *args, **kwargs):\n   1097     try:\n-> 1098       return self._sess.run(*args, **kwargs)\n   1099     except _PREEMPTION_ERRORS:\n   1100       raise\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\n   1168                                   feed_dict=feed_dict,\n   1169                                   options=options,\n-> 1170                                   run_metadata=run_metadata)\n   1171 \n   1172     for hook in self._hooks:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\n    948 \n    949   def run(self, *args, **kwargs):\n--> 950     return self._sess.run(*args, **kwargs)\n    951 \n    952   def run_step_fn(self, step_fn, raw_session, run_with_hooks):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    903     try:\n    904       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 905                          run_metadata_ptr)\n    906       if run_metadata:\n    907         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1135     if final_fetches or final_targets or (handle and feed_dict_tensor):\n   1136       results = self._do_run(handle, final_targets, final_fetches,\n-> 1137                              feed_dict_tensor, options, run_metadata)\n   1138     else:\n   1139       results = []\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n   1353     if handle is None:\n   1354       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n-> 1355                            options, run_metadata)\n   1356     else:\n   1357       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\n   1372         except KeyError:\n   1373           pass\n-> 1374       raise type(e)(node_def, op, message)\n   1375 \n   1376   def _extend_graph(self):\n\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-9d8332f50ddd>\", line 68, in <module>\n    main()\n  File \"<ipython-input-5-9d8332f50ddd>\", line 64, in main\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 421, in train_and_evaluate\n    executor.run()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 494, in run\n    self.run_local()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 626, in run_local\n    hooks=train_hooks)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 352, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 809, in _train_model\n    input_fn, model_fn_lib.ModeKeys.TRAIN))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 668, in _get_features_and_labels_from_input_fn\n    result = self._call_input_fn(input_fn, mode)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 760, in _call_input_fn\n    return input_fn(**kwargs)\n  File \"<ipython-input-5-9d8332f50ddd>\", line 28, in my_input_fn_train\n    value = ds.make_one_shot_iterator().get_next()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 330, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 866, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nAlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]", "body": "Using the train_and_evaluate method from estimator API and cache policy on filesystem, I get an error because the evaluation starts before that all cache is written on the filesystem. So when the train runs the second time, after the evaluation, I get the error because it finds the lock file. \r\nIf the cache is written before the first evaluation I don't get the error.\r\n\r\nHere a snippet runnable on colab \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras as ks\r\nimport itertools\r\n\r\n\r\ndef batch_reshape(a, b):\r\n    a.set_shape([None, None, 3])\r\n    b.set_shape([None, None, 1])\r\n    return (a, b)\r\n\r\n\r\ndef gen(n):\r\n    a = np.array(np.random.rand(5, 5, 3).astype(np.float32))\r\n    b = np.array(np.random.rand(5, 5, 1).astype(np.float32))\r\n    return (a, b)\r\n\r\n\r\ndef my_input_fn_train():\r\n    ds = tf.data.Dataset.range(100000)\r\n    ds = ds.map(\r\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\r\n    ds = ds.map(batch_reshape)\r\n    ds = ds.cache(\"/tmp/mycache_train\")\r\n\r\n    ds = ds.repeat(3)\r\n\r\n    value = ds.make_one_shot_iterator().get_next()\r\n\r\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\r\n\r\n\r\ndef my_input_fn_eval():\r\n    ds = tf.data.Dataset.range(100000)\r\n    ds = ds.map(\r\n        lambda x: tf.py_func(func=gen, inp=[x], Tout=[tf.float32, tf.float32]))\r\n    ds = ds.map(batch_reshape)\r\n    ds = ds.cache(\"/tmp/mycache_eval\")\r\n\r\n    ds = ds.repeat(3)\r\n\r\n    value = ds.make_one_shot_iterator().get_next()\r\n\r\n    return {\"input_rgb\": value[0]}, {\"softmax\": value[1]}\r\n\r\n\r\ndef main():\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    input_rgb = ks.layers.Input(shape=(1, 5, 5, 3), name=\"input_rgb\")\r\n    x = ks.layers.Dense(1, activation='relu', name=\"Dense_1\")(input_rgb)\r\n    x = ks.layers.Dense(1, activation='softmax', name=\"softmax\")(x)\r\n    model = ks.models.Model(inputs=[input_rgb], outputs=[x])\r\n    model.compile(\r\n        loss={'softmax': 'binary_crossentropy'},\r\n        optimizer=tf.keras.optimizers.Adam())\r\n\r\n    estimator = ks.estimator.model_to_estimator(keras_model=model)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=my_input_fn_train)\r\n\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\nand here the output\r\n\r\n```\r\nINFO:tensorflow:Using the Keras model from memory.\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpd1r865ry\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpd1r865ry', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb48f9ecf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 2 secs (eval_spec.throttle_secs) or training is finished.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/keras_model.ckpt\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpd1r865ry/model.ckpt.\r\nINFO:tensorflow:loss = 9.25762, step = 1\r\nINFO:tensorflow:global_step/sec: 612.893\r\nINFO:tensorflow:loss = 9.348354, step = 101 (0.166 sec)\r\nINFO:tensorflow:global_step/sec: 952.69\r\nINFO:tensorflow:loss = 8.319871, step = 201 (0.108 sec)\r\nINFO:tensorflow:global_step/sec: 838.883\r\nINFO:tensorflow:loss = 7.7207212, step = 301 (0.119 sec)\r\nINFO:tensorflow:global_step/sec: 917.55\r\nINFO:tensorflow:loss = 8.657611, step = 401 (0.110 sec)\r\nINFO:tensorflow:global_step/sec: 866.062\r\nINFO:tensorflow:loss = 8.567427, step = 501 (0.118 sec)\r\nINFO:tensorflow:global_step/sec: 862.789\r\nINFO:tensorflow:loss = 7.4227247, step = 601 (0.110 sec)\r\nINFO:tensorflow:global_step/sec: 847.577\r\nINFO:tensorflow:loss = 6.676866, step = 701 (0.118 sec)\r\nINFO:tensorflow:global_step/sec: 911.827\r\nINFO:tensorflow:loss = 7.4915752, step = 801 (0.112 sec)\r\nINFO:tensorflow:global_step/sec: 837.15\r\nINFO:tensorflow:loss = 7.7332606, step = 901 (0.120 sec)\r\nINFO:tensorflow:global_step/sec: 857.915\r\nINFO:tensorflow:loss = 7.283838, step = 1001 (0.116 sec)\r\nINFO:tensorflow:global_step/sec: 786.144\r\nINFO:tensorflow:loss = 8.61713, step = 1101 (0.127 sec)\r\nINFO:tensorflow:Saving checkpoints for 1150 into /tmp/tmpd1r865ry/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 8.417924.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2018-04-05-15:36:08\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [1/5]\r\nINFO:tensorflow:Evaluation [2/5]\r\nINFO:tensorflow:Evaluation [3/5]\r\nINFO:tensorflow:Evaluation [4/5]\r\nINFO:tensorflow:Evaluation [5/5]\r\nINFO:tensorflow:Finished evaluation at 2018-04-05-15:36:08\r\n\r\nINFO:tensorflow:Saving dict for global step 1150: global_step = 1150, loss = 8.261229\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/tmpd1r865ry/model.ckpt-1150\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n\r\n---------------------------------------------------------------------------\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1360     try:\r\n-> 1361       return fn(*args)\r\n   1362     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1339           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\r\n-> 1340                                    target_list, status, run_metadata)\r\n   1341 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    515             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 516             c_api.TF_GetCode(self.status.status))\r\n    517     # Delete the underlying status object from memory otherwise it stays alive\r\n\r\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n<ipython-input-5-9d8332f50ddd> in <module>()\r\n     66 \r\n     67 if __name__ == \"__main__\":\r\n---> 68     main()\r\n\r\n<ipython-input-5-9d8332f50ddd> in main()\r\n     62         input_fn=my_input_fn_eval, steps=5, throttle_secs=2)\r\n     63 \r\n---> 64     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n     65 \r\n     66 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    419         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    420 \r\n--> 421   executor.run()\r\n    422 \r\n    423 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run(self)\r\n    492         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    493       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 494       self.run_local()\r\n    495       return\r\n    496 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py in run_local(self)\r\n    624           input_fn=self._train_spec.input_fn,\r\n    625           max_steps=self._train_spec.max_steps,\r\n--> 626           hooks=train_hooks)\r\n    627 \r\n    628       # Final export signal: For any eval result with global_step >= train\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    350 \r\n    351     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 352     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    353     logging.info('Loss for final step: %s.', loss)\r\n    354     return self\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    889         loss = None\r\n    890         while not mon_sess.should_stop():\r\n--> 891           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n    892       return loss\r\n    893 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    544                           feed_dict=feed_dict,\r\n    545                           options=options,\r\n--> 546                           run_metadata=run_metadata)\r\n    547 \r\n    548   def run_step_fn(self, step_fn):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1020                               feed_dict=feed_dict,\r\n   1021                               options=options,\r\n-> 1022                               run_metadata=run_metadata)\r\n   1023       except _PREEMPTION_ERRORS as e:\r\n   1024         logging.info('An error was raised. This may be due to a preemption in '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1111         raise six.reraise(*original_exc_info)\r\n   1112       else:\r\n-> 1113         raise six.reraise(*original_exc_info)\r\n   1114 \r\n   1115 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1096   def run(self, *args, **kwargs):\r\n   1097     try:\r\n-> 1098       return self._sess.run(*args, **kwargs)\r\n   1099     except _PREEMPTION_ERRORS:\r\n   1100       raise\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1168                                   feed_dict=feed_dict,\r\n   1169                                   options=options,\r\n-> 1170                                   run_metadata=run_metadata)\r\n   1171 \r\n   1172     for hook in self._hooks:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n    948 \r\n    949   def run(self, *args, **kwargs):\r\n--> 950     return self._sess.run(*args, **kwargs)\r\n    951 \r\n    952   def run_step_fn(self, step_fn, raw_session, run_with_hooks):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    903     try:\r\n    904       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 905                          run_metadata_ptr)\r\n    906       if run_metadata:\r\n    907         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1135     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1136       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1137                              feed_dict_tensor, options, run_metadata)\r\n   1138     else:\r\n   1139       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1353     if handle is None:\r\n   1354       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1355                            options, run_metadata)\r\n   1356     else:\r\n   1357       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1372         except KeyError:\r\n   1373           pass\r\n-> 1374       raise type(e)(node_def, op, message)\r\n   1375 \r\n   1376   def _extend_graph(self):\r\n\r\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-5-9d8332f50ddd>\", line 68, in <module>\r\n    main()\r\n  File \"<ipython-input-5-9d8332f50ddd>\", line 64, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 421, in train_and_evaluate\r\n    executor.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 494, in run\r\n    self.run_local()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\", line 626, in run_local\r\n    hooks=train_hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 352, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 809, in _train_model\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 668, in _get_features_and_labels_from_input_fn\r\n    result = self._call_input_fn(input_fn, mode)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 760, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"<ipython-input-5-9d8332f50ddd>\", line 28, in my_input_fn_train\r\n    value = ds.make_one_shot_iterator().get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 330, in get_next\r\n    name=name)), self._output_types,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 866, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nAlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/tmp/mycache_train.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1522942566\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,3], [?,?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\n```\r\n"}