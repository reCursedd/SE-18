{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419474643", "html_url": "https://github.com/tensorflow/tensorflow/issues/21135#issuecomment-419474643", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21135", "id": 419474643, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTQ3NDY0Mw==", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T15:23:35Z", "updated_at": "2018-09-07T15:23:35Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> Thank you for your response. Manually setting the checkpoint and writing into different files does work; however, I believe it should be processed by Tensorflow internally but not a user's extra effort. In previous Tensorflow versions, there is no such problem, but just since version 1.9, I started to encounter this conflict.</p>\n<p>One sample code could just be the Tensorflow's tutorial codes with a few extra lines as below:</p>\n<pre><code>mnist = tf.keras.datasets.mnist\ndef test_tesnorboard():\n  (x_train, y_train),(x_test, y_test) = mnist.load_data()\n  x_train, x_test = x_train / 255.0, x_test / 255.0\n\n  model = tf.keras.models.Sequential([\n\ttf.keras.layers.Flatten(),\n\ttf.keras.layers.Dense(512, activation=tf.nn.relu),\n\ttf.keras.layers.Dropout(0.2),\n\ttf.keras.layers.Dense(10, activation=tf.nn.softmax)\n  ])\n  model.compile(optimizer='adam',\n\t\t\t\tloss='sparse_categorical_crossentropy',\n\t\t\t\tmetrics=['accuracy'])\n\n  saver_callback = keras.callbacks.ModelCheckpoint(\n\t'test/test_save.ckpt', verbose=1, save_weights_only=True, period=1)\n\n  model.fit(x_train, y_train, epochs=1000000, callbacks=[saver_callback])\n  model.evaluate(x_test, y_test)\n</code></pre>\n<p>If we run the tensorboard at the same time during training, the training program will crash.</p>", "body_text": "@asimshankar Thank you for your response. Manually setting the checkpoint and writing into different files does work; however, I believe it should be processed by Tensorflow internally but not a user's extra effort. In previous Tensorflow versions, there is no such problem, but just since version 1.9, I started to encounter this conflict.\nOne sample code could just be the Tensorflow's tutorial codes with a few extra lines as below:\nmnist = tf.keras.datasets.mnist\ndef test_tesnorboard():\n  (x_train, y_train),(x_test, y_test) = mnist.load_data()\n  x_train, x_test = x_train / 255.0, x_test / 255.0\n\n  model = tf.keras.models.Sequential([\n\ttf.keras.layers.Flatten(),\n\ttf.keras.layers.Dense(512, activation=tf.nn.relu),\n\ttf.keras.layers.Dropout(0.2),\n\ttf.keras.layers.Dense(10, activation=tf.nn.softmax)\n  ])\n  model.compile(optimizer='adam',\n\t\t\t\tloss='sparse_categorical_crossentropy',\n\t\t\t\tmetrics=['accuracy'])\n\n  saver_callback = keras.callbacks.ModelCheckpoint(\n\t'test/test_save.ckpt', verbose=1, save_weights_only=True, period=1)\n\n  model.fit(x_train, y_train, epochs=1000000, callbacks=[saver_callback])\n  model.evaluate(x_test, y_test)\n\nIf we run the tensorboard at the same time during training, the training program will crash.", "body": "@asimshankar Thank you for your response. Manually setting the checkpoint and writing into different files does work; however, I believe it should be processed by Tensorflow internally but not a user's extra effort. In previous Tensorflow versions, there is no such problem, but just since version 1.9, I started to encounter this conflict.\r\n\r\nOne sample code could just be the Tensorflow's tutorial codes with a few extra lines as below:\r\n\r\n\tmnist = tf.keras.datasets.mnist\r\n\tdef test_tesnorboard():\r\n\t  (x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n\t  x_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n\t  model = tf.keras.models.Sequential([\r\n\t\ttf.keras.layers.Flatten(),\r\n\t\ttf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n\t\ttf.keras.layers.Dropout(0.2),\r\n\t\ttf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n\t  ])\r\n\t  model.compile(optimizer='adam',\r\n\t\t\t\t\tloss='sparse_categorical_crossentropy',\r\n\t\t\t\t\tmetrics=['accuracy'])\r\n\r\n\t  saver_callback = keras.callbacks.ModelCheckpoint(\r\n\t\t'test/test_save.ckpt', verbose=1, save_weights_only=True, period=1)\r\n\r\n\t  model.fit(x_train, y_train, epochs=1000000, callbacks=[saver_callback])\r\n\t  model.evaluate(x_test, y_test)\r\n\r\nIf we run the tensorboard at the same time during training, the training program will crash."}