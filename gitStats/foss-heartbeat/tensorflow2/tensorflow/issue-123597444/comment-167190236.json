{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/167190236", "html_url": "https://github.com/tensorflow/tensorflow/issues/593#issuecomment-167190236", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/593", "id": 167190236, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NzE5MDIzNg==", "user": {"login": "trubin", "id": 890936, "node_id": "MDQ6VXNlcjg5MDkzNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/890936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trubin", "html_url": "https://github.com/trubin", "followers_url": "https://api.github.com/users/trubin/followers", "following_url": "https://api.github.com/users/trubin/following{/other_user}", "gists_url": "https://api.github.com/users/trubin/gists{/gist_id}", "starred_url": "https://api.github.com/users/trubin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trubin/subscriptions", "organizations_url": "https://api.github.com/users/trubin/orgs", "repos_url": "https://api.github.com/users/trubin/repos", "events_url": "https://api.github.com/users/trubin/events{/privacy}", "received_events_url": "https://api.github.com/users/trubin/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-25T04:10:37Z", "updated_at": "2015-12-25T04:10:51Z", "author_association": "NONE", "body_html": "<p>I believe the bug is caused by the following logic:</p>\n<p>When computing a gradient (gradients.py::gradients() function), the has_control_flow bool is set to true when there is an \"Exit\" op anywhere in the subgraph under consideration (comes from _PendingCount()). Obviously this is true for a nested while. As we visit all the ops in the subgraph in reverse order [op being iterated in a queue of ops] , op is told that it is in a GradWhileContext through the function control_flow_ops.EnterGradWhileContext(op) (this is the only place in the code that function is called). EnterGradWhileContext creates a grad_ctxt for op's control_flow_context assuming that the control_flow_context is a WhileContext.</p>\n<p>So far so good. The issue is that in a nested While, presumably the output of the inner while will be used as the input to some node used in returning the output of the outer While, so the op associated with the op of the inner while output tensor will not have made it into the queue of ops before the tensor is needed as an input, and so will never have EnterGradWhileContext called on it.</p>\n<p>I propose modifying EnterGradWhileContext to search down the tree of input ops/output tensors so that a grad_ctxt is created and entered recursively for all ops to be used, and then similarly the whole tree exited when ExitGradWhileContext is called on an op.</p>\n<p>Not sure who might be able to let me know if this is a good idea or not?</p>", "body_text": "I believe the bug is caused by the following logic:\nWhen computing a gradient (gradients.py::gradients() function), the has_control_flow bool is set to true when there is an \"Exit\" op anywhere in the subgraph under consideration (comes from _PendingCount()). Obviously this is true for a nested while. As we visit all the ops in the subgraph in reverse order [op being iterated in a queue of ops] , op is told that it is in a GradWhileContext through the function control_flow_ops.EnterGradWhileContext(op) (this is the only place in the code that function is called). EnterGradWhileContext creates a grad_ctxt for op's control_flow_context assuming that the control_flow_context is a WhileContext.\nSo far so good. The issue is that in a nested While, presumably the output of the inner while will be used as the input to some node used in returning the output of the outer While, so the op associated with the op of the inner while output tensor will not have made it into the queue of ops before the tensor is needed as an input, and so will never have EnterGradWhileContext called on it.\nI propose modifying EnterGradWhileContext to search down the tree of input ops/output tensors so that a grad_ctxt is created and entered recursively for all ops to be used, and then similarly the whole tree exited when ExitGradWhileContext is called on an op.\nNot sure who might be able to let me know if this is a good idea or not?", "body": "I believe the bug is caused by the following logic:\n\nWhen computing a gradient (gradients.py::gradients() function), the has_control_flow bool is set to true when there is an \"Exit\" op anywhere in the subgraph under consideration (comes from _PendingCount()). Obviously this is true for a nested while. As we visit all the ops in the subgraph in reverse order [op being iterated in a queue of ops] , op is told that it is in a GradWhileContext through the function control_flow_ops.EnterGradWhileContext(op) (this is the only place in the code that function is called). EnterGradWhileContext creates a grad_ctxt for op's control_flow_context assuming that the control_flow_context is a WhileContext. \n\nSo far so good. The issue is that in a nested While, presumably the output of the inner while will be used as the input to some node used in returning the output of the outer While, so the op associated with the op of the inner while output tensor will not have made it into the queue of ops before the tensor is needed as an input, and so will never have EnterGradWhileContext called on it. \n\nI propose modifying EnterGradWhileContext to search down the tree of input ops/output tensors so that a grad_ctxt is created and entered recursively for all ops to be used, and then similarly the whole tree exited when ExitGradWhileContext is called on an op.\n\nNot sure who might be able to let me know if this is a good idea or not?\n"}