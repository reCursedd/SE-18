{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275302606", "html_url": "https://github.com/tensorflow/tensorflow/issues/2237#issuecomment-275302606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2237", "id": 275302606, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTMwMjYwNg==", "user": {"login": "kingtaurus", "id": 2761482, "node_id": "MDQ6VXNlcjI3NjE0ODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/2761482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kingtaurus", "html_url": "https://github.com/kingtaurus", "followers_url": "https://api.github.com/users/kingtaurus/followers", "following_url": "https://api.github.com/users/kingtaurus/following{/other_user}", "gists_url": "https://api.github.com/users/kingtaurus/gists{/gist_id}", "starred_url": "https://api.github.com/users/kingtaurus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kingtaurus/subscriptions", "organizations_url": "https://api.github.com/users/kingtaurus/orgs", "repos_url": "https://api.github.com/users/kingtaurus/repos", "events_url": "https://api.github.com/users/kingtaurus/events{/privacy}", "received_events_url": "https://api.github.com/users/kingtaurus/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-26T03:56:34Z", "updated_at": "2017-01-26T03:56:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a> I believe that this can now be solved using just <code>tf.while_loop</code> and then making sure your data is appropriately formatted. Using the idea from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3701985\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bogatyy\">@bogatyy</a> <a href=\"https://github.com/bogatyy/cs224d/tree/master/assignment3\">cs224d/repo</a> - I  modified it so that it doesn't require storage in a <code>tf.TensorArray</code> (the <code>while_loop</code> iteration is used for storage). Below is a rough sketch of a solution.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> private functions used to construct the graph.</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_embed_word</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">word_index</span>):\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Composition<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-k\">as</span> scope:\n            embedding <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-k\">return</span> tf.expand_dims(tf.gather(embedding, word_index), <span class=\"pl-c1\">0</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> private functions used to construct the graph.</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_combine_children</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">tensor_concat</span>, <span class=\"pl-smi\">left_idx</span>, <span class=\"pl-smi\">right_idx</span>):\n        left_tensor <span class=\"pl-k\">=</span> tf.expand_dims(tf.gather(tensor_concat, left_idx), <span class=\"pl-c1\">0</span>)\n        right_tensor <span class=\"pl-k\">=</span> tf.expand_dims(tf.gather(tensor_concat, right_idx), <span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Composition<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n            W1 <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>W1<span class=\"pl-pds\">'</span></span>)\n            b1 <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>b1<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">return</span> tf.nn.relu(tf.matmul(tf.concat(<span class=\"pl-c1\">1</span>, [left_tensor, right_tensor]), W1) <span class=\"pl-k\">+</span> b1)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_loop_over_tree</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">tensor_list</span>):\n        is_leaf <span class=\"pl-k\">=</span> tf.gather(<span class=\"pl-c1\">self</span>.is_a_leaf, i)\n        word_idx    <span class=\"pl-k\">=</span> tf.gather(<span class=\"pl-c1\">self</span>.word_index, i)\n        left_child  <span class=\"pl-k\">=</span> tf.gather(<span class=\"pl-c1\">self</span>.left_child, i)\n        right_child <span class=\"pl-k\">=</span> tf.gather(<span class=\"pl-c1\">self</span>.right_child, i)\n        node_tensor <span class=\"pl-k\">=</span> tf.cond(is_leaf, <span class=\"pl-k\">lambda</span> : <span class=\"pl-c1\">self</span>._embed_word(word_idx),\n                                       <span class=\"pl-k\">lambda</span> : <span class=\"pl-c1\">self</span>._combine_children(tensor_list, left_child, right_child))\n        tensor_list <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">0</span>, [tensor_list, node_tensor])\n        i <span class=\"pl-k\">=</span> tf.add(i,<span class=\"pl-c1\">1</span>)\n        <span class=\"pl-k\">return</span> i, tensor_list\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">construct_tensor_array</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        loop_condition <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">tensor_array</span>: \\\n                         tf.less(i, tf.squeeze(tf.shape(<span class=\"pl-c1\">self</span>.is_a_leaf)))\n        \n        left_most_element <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._embed_word(tf.gather(<span class=\"pl-c1\">self</span>.word_index, <span class=\"pl-c1\">0</span>))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>index should start @ 1</span>\n        i1 <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n\n        while_loop_op <span class=\"pl-k\">=</span> tf.while_loop(<span class=\"pl-v\">cond</span><span class=\"pl-k\">=</span>loop_condition,\n                                       <span class=\"pl-v\">body</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._loop_over_tree,\n                                       <span class=\"pl-v\">loop_vars</span><span class=\"pl-k\">=</span>[i1, left_most_element],\n                                       <span class=\"pl-v\">shape_invariants</span><span class=\"pl-k\">=</span>[i1.get_shape(), tf.TensorShape([<span class=\"pl-c1\">None</span>,<span class=\"pl-c1\">50</span>])])\n        \n        <span class=\"pl-k\">return</span> while_loop_op[<span class=\"pl-c1\">1</span>]</pre></div>\n<p>where,</p>\n<div class=\"highlight highlight-source-python\"><pre>        <span class=\"pl-c1\">self</span>.is_a_leaf   <span class=\"pl-k\">=</span> tf.placeholder(tf.bool, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>is_a_leaf<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.left_child  <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lchild<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.right_child <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rchild<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.word_index  <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>word_index<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.labelholder <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>labels_holder<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>and the feed dictionary is built as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">build_feed_dict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_node</span>):\n        nodes_list <span class=\"pl-k\">=</span> []\n        tr.leftTraverse(in_node, <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">node</span>, <span class=\"pl-smi\">args</span>: args.append(node), nodes_list)\n        node_to_index <span class=\"pl-k\">=</span> OrderedDict()\n        <span class=\"pl-k\">for</span> idx, i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(nodes_list):\n            node_to_index[i] <span class=\"pl-k\">=</span> idx\n\n        feed_dict <span class=\"pl-k\">=</span> {\n          <span class=\"pl-c1\">self</span>.is_a_leaf   : [ n.isLeaf <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> nodes_list ],\n          <span class=\"pl-c1\">self</span>.left_child  : [ node_to_index[n.left] <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> n.isLeaf <span class=\"pl-k\">else</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> nodes_list ],\n          <span class=\"pl-c1\">self</span>.right_child : [ node_to_index[n.right] <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> n.isLeaf <span class=\"pl-k\">else</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> nodes_list ],\n          <span class=\"pl-c1\">self</span>.word_index  : [ <span class=\"pl-c1\">self</span>.vocab.encode(n.word) <span class=\"pl-k\">if</span> n.word <span class=\"pl-k\">else</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> nodes_list ],\n          <span class=\"pl-c1\">self</span>.labelholder : [ n.label <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> nodes_list ]\n        }\n        <span class=\"pl-k\">return</span> feed_dict</pre></div>", "body_text": "@ibab I believe that this can now be solved using just tf.while_loop and then making sure your data is appropriately formatted. Using the idea from @bogatyy cs224d/repo - I  modified it so that it doesn't require storage in a tf.TensorArray (the while_loop iteration is used for storage). Below is a rough sketch of a solution.\n    # private functions used to construct the graph.\n    def _embed_word(self, word_index):\n        with tf.variable_scope(\"Composition\", reuse=True) as scope:\n            embedding = tf.get_variable(\"embedding\")\n        return tf.expand_dims(tf.gather(embedding, word_index), 0)\n\n    # private functions used to construct the graph.\n    def _combine_children(self, tensor_concat, left_idx, right_idx):\n        left_tensor = tf.expand_dims(tf.gather(tensor_concat, left_idx), 0)\n        right_tensor = tf.expand_dims(tf.gather(tensor_concat, right_idx), 0)\n        with tf.variable_scope('Composition', reuse=True):\n            W1 = tf.get_variable('W1')\n            b1 = tf.get_variable('b1')\n        return tf.nn.relu(tf.matmul(tf.concat(1, [left_tensor, right_tensor]), W1) + b1)\n\n    def _loop_over_tree(self, i, tensor_list):\n        is_leaf = tf.gather(self.is_a_leaf, i)\n        word_idx    = tf.gather(self.word_index, i)\n        left_child  = tf.gather(self.left_child, i)\n        right_child = tf.gather(self.right_child, i)\n        node_tensor = tf.cond(is_leaf, lambda : self._embed_word(word_idx),\n                                       lambda : self._combine_children(tensor_list, left_child, right_child))\n        tensor_list = tf.concat(0, [tensor_list, node_tensor])\n        i = tf.add(i,1)\n        return i, tensor_list\n\n    def construct_tensor_array(self):\n        loop_condition = lambda i, tensor_array: \\\n                         tf.less(i, tf.squeeze(tf.shape(self.is_a_leaf)))\n        \n        left_most_element = self._embed_word(tf.gather(self.word_index, 0))\n        #index should start @ 1\n        i1 = tf.constant(1, dtype=tf.int32)\n\n        while_loop_op = tf.while_loop(cond=loop_condition,\n                                       body=self._loop_over_tree,\n                                       loop_vars=[i1, left_most_element],\n                                       shape_invariants=[i1.get_shape(), tf.TensorShape([None,50])])\n        \n        return while_loop_op[1]\nwhere,\n        self.is_a_leaf   = tf.placeholder(tf.bool, [None], name=\"is_a_leaf\")\n        self.left_child  = tf.placeholder(tf.int32, [None], name=\"lchild\")\n        self.right_child = tf.placeholder(tf.int32, [None], name=\"rchild\")\n        self.word_index  = tf.placeholder(tf.int32, [None], name=\"word_index\")\n        self.labelholder = tf.placeholder(tf.int32, [None], name=\"labels_holder\")\nand the feed dictionary is built as follows:\n    def build_feed_dict(self, in_node):\n        nodes_list = []\n        tr.leftTraverse(in_node, lambda node, args: args.append(node), nodes_list)\n        node_to_index = OrderedDict()\n        for idx, i in enumerate(nodes_list):\n            node_to_index[i] = idx\n\n        feed_dict = {\n          self.is_a_leaf   : [ n.isLeaf for n in nodes_list ],\n          self.left_child  : [ node_to_index[n.left] if not n.isLeaf else -1 for n in nodes_list ],\n          self.right_child : [ node_to_index[n.right] if not n.isLeaf else -1 for n in nodes_list ],\n          self.word_index  : [ self.vocab.encode(n.word) if n.word else -1 for n in nodes_list ],\n          self.labelholder : [ n.label for n in nodes_list ]\n        }\n        return feed_dict", "body": "@ibab I believe that this can now be solved using just `tf.while_loop` and then making sure your data is appropriately formatted. Using the idea from @bogatyy [cs224d/repo](https://github.com/bogatyy/cs224d/tree/master/assignment3) - I  modified it so that it doesn't require storage in a `tf.TensorArray` (the `while_loop` iteration is used for storage). Below is a rough sketch of a solution.\r\n\r\n```python\r\n    # private functions used to construct the graph.\r\n    def _embed_word(self, word_index):\r\n        with tf.variable_scope(\"Composition\", reuse=True) as scope:\r\n            embedding = tf.get_variable(\"embedding\")\r\n        return tf.expand_dims(tf.gather(embedding, word_index), 0)\r\n\r\n    # private functions used to construct the graph.\r\n    def _combine_children(self, tensor_concat, left_idx, right_idx):\r\n        left_tensor = tf.expand_dims(tf.gather(tensor_concat, left_idx), 0)\r\n        right_tensor = tf.expand_dims(tf.gather(tensor_concat, right_idx), 0)\r\n        with tf.variable_scope('Composition', reuse=True):\r\n            W1 = tf.get_variable('W1')\r\n            b1 = tf.get_variable('b1')\r\n        return tf.nn.relu(tf.matmul(tf.concat(1, [left_tensor, right_tensor]), W1) + b1)\r\n\r\n    def _loop_over_tree(self, i, tensor_list):\r\n        is_leaf = tf.gather(self.is_a_leaf, i)\r\n        word_idx    = tf.gather(self.word_index, i)\r\n        left_child  = tf.gather(self.left_child, i)\r\n        right_child = tf.gather(self.right_child, i)\r\n        node_tensor = tf.cond(is_leaf, lambda : self._embed_word(word_idx),\r\n                                       lambda : self._combine_children(tensor_list, left_child, right_child))\r\n        tensor_list = tf.concat(0, [tensor_list, node_tensor])\r\n        i = tf.add(i,1)\r\n        return i, tensor_list\r\n\r\n    def construct_tensor_array(self):\r\n        loop_condition = lambda i, tensor_array: \\\r\n                         tf.less(i, tf.squeeze(tf.shape(self.is_a_leaf)))\r\n        \r\n        left_most_element = self._embed_word(tf.gather(self.word_index, 0))\r\n        #index should start @ 1\r\n        i1 = tf.constant(1, dtype=tf.int32)\r\n\r\n        while_loop_op = tf.while_loop(cond=loop_condition,\r\n                                       body=self._loop_over_tree,\r\n                                       loop_vars=[i1, left_most_element],\r\n                                       shape_invariants=[i1.get_shape(), tf.TensorShape([None,50])])\r\n        \r\n        return while_loop_op[1]\r\n```\r\n\r\nwhere,\r\n```python\r\n        self.is_a_leaf   = tf.placeholder(tf.bool, [None], name=\"is_a_leaf\")\r\n        self.left_child  = tf.placeholder(tf.int32, [None], name=\"lchild\")\r\n        self.right_child = tf.placeholder(tf.int32, [None], name=\"rchild\")\r\n        self.word_index  = tf.placeholder(tf.int32, [None], name=\"word_index\")\r\n        self.labelholder = tf.placeholder(tf.int32, [None], name=\"labels_holder\")\r\n```\r\n\r\nand the feed dictionary is built as follows:\r\n```python\r\n    def build_feed_dict(self, in_node):\r\n        nodes_list = []\r\n        tr.leftTraverse(in_node, lambda node, args: args.append(node), nodes_list)\r\n        node_to_index = OrderedDict()\r\n        for idx, i in enumerate(nodes_list):\r\n            node_to_index[i] = idx\r\n\r\n        feed_dict = {\r\n          self.is_a_leaf   : [ n.isLeaf for n in nodes_list ],\r\n          self.left_child  : [ node_to_index[n.left] if not n.isLeaf else -1 for n in nodes_list ],\r\n          self.right_child : [ node_to_index[n.right] if not n.isLeaf else -1 for n in nodes_list ],\r\n          self.word_index  : [ self.vocab.encode(n.word) if n.word else -1 for n in nodes_list ],\r\n          self.labelholder : [ n.label for n in nodes_list ]\r\n        }\r\n        return feed_dict\r\n```\r\n\r\n"}