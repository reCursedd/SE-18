{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/229832148", "html_url": "https://github.com/tensorflow/tensorflow/issues/2237#issuecomment-229832148", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2237", "id": 229832148, "node_id": "MDEyOklzc3VlQ29tbWVudDIyOTgzMjE0OA==", "user": {"login": "kingtaurus", "id": 2761482, "node_id": "MDQ6VXNlcjI3NjE0ODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/2761482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kingtaurus", "html_url": "https://github.com/kingtaurus", "followers_url": "https://api.github.com/users/kingtaurus/followers", "following_url": "https://api.github.com/users/kingtaurus/following{/other_user}", "gists_url": "https://api.github.com/users/kingtaurus/gists{/gist_id}", "starred_url": "https://api.github.com/users/kingtaurus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kingtaurus/subscriptions", "organizations_url": "https://api.github.com/users/kingtaurus/orgs", "repos_url": "https://api.github.com/users/kingtaurus/repos", "events_url": "https://api.github.com/users/kingtaurus/events{/privacy}", "received_events_url": "https://api.github.com/users/kingtaurus/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-01T01:25:50Z", "updated_at": "2016-07-01T01:25:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a> - Yup, you're correct about the per-tree basis. I believe that this gives a non-ideal starting point for how to implement recursive structure in tensorflow. Right now it's problematic because the graph state has to be cleared (since each tree gets a graph, some memory may be allocated, so this has to be dealt with) and then reloaded. I think trying to shoe-horn in the other flow-control operations might be bad idea (this really doesn't fall into any of the operations like <code>tf.map_fn</code>, <code>tf.scan</code>, or <code>tf.foldl</code> or <code>tf.foldr</code>). I guess you could try using <code>tf.case</code> and <code>tf.while_loop</code> but that looks like a pain.</p>\n<p>There are few things that need to be implemented:<br>\n(1) recursive descent graph construction (based upon 'node' type and 'loss' type). This become even more complex if you need things like attention. Ideally I would like something like,<br>\n<code>tf.walk( leaf_func, node_func, output_func, tree_storage, output_style=\"root_only | all \")</code></p>\n<p>(2) dynamic construction of and freeing of the computation graphs (you can't maintain all the possible allocated graphs in memory, but you can use flyweights and have an allocatation pool for storage).</p>\n<p>(3) tensorboard interface/display changes to handle this new 'dynamic' graph behavior.</p>", "body_text": "@ibab - Yup, you're correct about the per-tree basis. I believe that this gives a non-ideal starting point for how to implement recursive structure in tensorflow. Right now it's problematic because the graph state has to be cleared (since each tree gets a graph, some memory may be allocated, so this has to be dealt with) and then reloaded. I think trying to shoe-horn in the other flow-control operations might be bad idea (this really doesn't fall into any of the operations like tf.map_fn, tf.scan, or tf.foldl or tf.foldr). I guess you could try using tf.case and tf.while_loop but that looks like a pain.\nThere are few things that need to be implemented:\n(1) recursive descent graph construction (based upon 'node' type and 'loss' type). This become even more complex if you need things like attention. Ideally I would like something like,\ntf.walk( leaf_func, node_func, output_func, tree_storage, output_style=\"root_only | all \")\n(2) dynamic construction of and freeing of the computation graphs (you can't maintain all the possible allocated graphs in memory, but you can use flyweights and have an allocatation pool for storage).\n(3) tensorboard interface/display changes to handle this new 'dynamic' graph behavior.", "body": "@ibab - Yup, you're correct about the per-tree basis. I believe that this gives a non-ideal starting point for how to implement recursive structure in tensorflow. Right now it's problematic because the graph state has to be cleared (since each tree gets a graph, some memory may be allocated, so this has to be dealt with) and then reloaded. I think trying to shoe-horn in the other flow-control operations might be bad idea (this really doesn't fall into any of the operations like `tf.map_fn`, `tf.scan`, or `tf.foldl` or `tf.foldr`). I guess you could try using `tf.case` and `tf.while_loop` but that looks like a pain.\n\nThere are few things that need to be implemented:\n(1) recursive descent graph construction (based upon 'node' type and 'loss' type). This become even more complex if you need things like attention. Ideally I would like something like,\n`tf.walk( leaf_func, node_func, output_func, tree_storage, output_style=\"root_only | all \")`\n\n(2) dynamic construction of and freeing of the computation graphs (you can't maintain all the possible allocated graphs in memory, but you can use flyweights and have an allocatation pool for storage).\n\n(3) tensorboard interface/display changes to handle this new 'dynamic' graph behavior.\n"}