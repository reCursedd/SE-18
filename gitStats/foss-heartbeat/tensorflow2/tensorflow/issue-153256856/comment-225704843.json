{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225704843", "html_url": "https://github.com/tensorflow/tensorflow/issues/2237#issuecomment-225704843", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2237", "id": 225704843, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTcwNDg0Mw==", "user": {"login": "kingtaurus", "id": 2761482, "node_id": "MDQ6VXNlcjI3NjE0ODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/2761482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kingtaurus", "html_url": "https://github.com/kingtaurus", "followers_url": "https://api.github.com/users/kingtaurus/followers", "following_url": "https://api.github.com/users/kingtaurus/following{/other_user}", "gists_url": "https://api.github.com/users/kingtaurus/gists{/gist_id}", "starred_url": "https://api.github.com/users/kingtaurus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kingtaurus/subscriptions", "organizations_url": "https://api.github.com/users/kingtaurus/orgs", "repos_url": "https://api.github.com/users/kingtaurus/repos", "events_url": "https://api.github.com/users/kingtaurus/events{/privacy}", "received_events_url": "https://api.github.com/users/kingtaurus/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-13T20:52:36Z", "updated_at": "2016-06-13T21:13:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is an NLP style model (sentiment analysis) and is based upon assignment 3 from <a href=\"http://cs224d.stanford.edu/syllabus.html\" rel=\"nofollow\">Stanford CS224D</a>. The tree structure is created as a purely python structure (tree class and node class). The variable state is saved/restored on a per batch usage and the tree structure is created using recursive descent (with any leaf nodes using an embedding operation to generate a feature vector), the loss could be created in a similar manner.</p>\n<p>The tree class is made up of nodes (tree structure is created through a parser - I'm leaving the details out for now):</p>\n<pre><code>class Node:  # a node in the tree\n    def __init__(self, label, word=None):\n        self.label = label\n        self.word = word\n        self.parent = None  # reference to parent\n        self.left = None  # reference to left child\n        self.right = None  # reference to right child\n        # true if I am a leaf (could have probably derived this from if I have\n        # a word)\n        self.isLeaf = False\n        # true if we have finished performing fowardprop on this node (note,\n        # there are many ways to implement the recursion.. some might not\n        # require this flag)\n</code></pre>\n<p>The tensorflow contributions, there are few parts:<br>\n(1) Variable creation (<strong>NOTE</strong>: <code>saver.restore(sess, './weights/%s.temp'%self.config.model_name)</code> is used to reload state) :</p>\n<pre><code>    def add_model_vars(self):\n        '''\n        You model contains the following parameters:\n            embedding:  tensor(vocab_size, embed_size)\n            W1:         tensor(2* embed_size, embed_size)\n            b1:         tensor(1, embed_size)\n            U:          tensor(embed_size, output_size)\n            bs:         tensor(1, output_size)\n        Hint: Add the tensorflow variables to the graph here and *reuse* them while building\n                the compution graphs for composition and projection for each tree\n        Hint: Use a variable_scope \"Composition\" for the composition layer, and\n              \"Projection\") for the linear transformations preceding the softmax.\n        '''\n        with tf.variable_scope('Composition'):\n            ### YOUR CODE HERE\n            #initializer=initializer=tf.random_normal_initializer(0,3)\n            embedding = tf.get_variable(\"embedding\",\n                                        [self.vocab.total_words, self.config.embed_size])\n            W1 = tf.get_variable(\"W1\", [2 * self.config.embed_size, self.config.embed_size])\n            b1 = tf.get_variable(\"b1\", [1, self.config.embed_size])\n            ### END YOUR CODE\n        with tf.variable_scope('Projection'):\n            ### YOUR CODE HERE\n            U = tf.get_variable(\"U\", [self.config.embed_size, self.config.label_size])\n            bs = tf.get_variable(\"bs\", [1, self.config.label_size])\n            ### END YOUR CODE\n</code></pre>\n<p>(2) model construction:</p>\n<pre><code>def add_model(self, node):\n        \"\"\"Recursively build the model to compute the phrase embeddings in the tree\n\n        Hint: Refer to tree.py and vocab.py before you start. Refer to\n              the model's vocab with self.vocab\n        Hint: Reuse the \"Composition\" variable_scope here\n        Hint: Store a node's vector representation in node.tensor so it can be\n              used by it's parent\n        Hint: If node is a leaf node, it's vector representation is just that of the\n              word vector (see tf.gather()).\n        Args:\n            node: a Node object\n        Returns:\n            node_tensors: Dict: key = Node, value = tensor(1, embed_size)\n        \"\"\"\n        with tf.variable_scope('Composition', reuse=True):\n            ### YOUR CODE HERE\n            embedding = tf.get_variable(\"embedding\")\n            W1 = tf.get_variable(\"W1\")\n            b1 = tf.get_variable(\"b1\")\n            l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1)\n            tf.add_to_collection(name=\"l2_loss\", value=l2_loss)\n            ### END YOUR CODE\n\n        W_split = tf.split(0, 2, W1)\n        W_left  = W_split[0]\n        W_right = W_split[1]\n        node_tensors = OrderedDict()\n        curr_node_tensor = None\n        if node.isLeaf:\n            ### YOUR CODE HERE\n            word_id = self.vocab.encode(node.word)\n            curr_node_tensor = tf.expand_dims(tf.gather(embedding, word_id),0)\n            ### END YOUR CODE\n        else:\n            node_tensors.update(self.add_model(node.left))\n            node_tensors.update(self.add_model(node.right))\n            ### YOUR CODE HERE\n            #This operation could be done without the split call above\n            #child_tensor = tf.concat(1, [node_tensors[node.left], node_tensors[node.right]])\n            #curr_node_tensor = tf.nn.relu(tf.matmul(child_tensor, W1) + b1)\n            curr_node_tensor = tf.matmul(node_tensors[node.left], W_left) + tf.matmul(node_tensors[node.right], W_right) + b1\n            ### END YOUR CODE\n        node_tensors[node] = curr_node_tensor\n        return node_tensors\n</code></pre>\n<p>(3) the projection layer is used to construct the sentiment for the root node:</p>\n<pre><code>    def add_projections(self, node_tensors):\n        \"\"\"Add projections to the composition vectors to compute the raw sentiment scores\n\n        Hint: Reuse the \"Projection\" variable_scope here\n        Args:\n            node_tensors: tensor(?, embed_size)\n        Returns:\n            output: tensor(?, label_size)\n        \"\"\"\n        logits = None\n        ### YOUR CODE HERE\n        with tf.variable_scope(\"Projection\", reuse=True):\n            U = tf.get_variable(\"U\")\n            bs = tf.get_variable(\"bs\")\n        logits = tf.matmul(node_tensors, U) + bs\n        ### END YOUR CODE\n        return logits\n</code></pre>\n<p>(4) loss (note: that you can use all the nodes, or just the root node for inference):<br>\n<code>tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))</code>,<br>\nwhere <code>logits</code> is given by a call to <code>inference</code></p>\n<pre><code>    def inference(self, tree, predict_only_root=False):\n        \"\"\"For a given tree build the RNN models computation graph up to where it\n            may be used for inference.\n        Args:\n            tree: a Tree object on which to build the computation graph for the RNN\n        Returns:\n            softmax_linear: Output tensor with the computed logits.\n        \"\"\"\n        node_tensors = self.add_model(tree.root)\n        if predict_only_root:\n            node_tensors = node_tensors[tree.root]\n        else:\n            node_tensors = [tensor for node, tensor in node_tensors.items() if node.label!=2]\n            node_tensors = tf.concat(0, node_tensors)\n        return self.add_projections(node_tensors)\n</code></pre>", "body_text": "This is an NLP style model (sentiment analysis) and is based upon assignment 3 from Stanford CS224D. The tree structure is created as a purely python structure (tree class and node class). The variable state is saved/restored on a per batch usage and the tree structure is created using recursive descent (with any leaf nodes using an embedding operation to generate a feature vector), the loss could be created in a similar manner.\nThe tree class is made up of nodes (tree structure is created through a parser - I'm leaving the details out for now):\nclass Node:  # a node in the tree\n    def __init__(self, label, word=None):\n        self.label = label\n        self.word = word\n        self.parent = None  # reference to parent\n        self.left = None  # reference to left child\n        self.right = None  # reference to right child\n        # true if I am a leaf (could have probably derived this from if I have\n        # a word)\n        self.isLeaf = False\n        # true if we have finished performing fowardprop on this node (note,\n        # there are many ways to implement the recursion.. some might not\n        # require this flag)\n\nThe tensorflow contributions, there are few parts:\n(1) Variable creation (NOTE: saver.restore(sess, './weights/%s.temp'%self.config.model_name) is used to reload state) :\n    def add_model_vars(self):\n        '''\n        You model contains the following parameters:\n            embedding:  tensor(vocab_size, embed_size)\n            W1:         tensor(2* embed_size, embed_size)\n            b1:         tensor(1, embed_size)\n            U:          tensor(embed_size, output_size)\n            bs:         tensor(1, output_size)\n        Hint: Add the tensorflow variables to the graph here and *reuse* them while building\n                the compution graphs for composition and projection for each tree\n        Hint: Use a variable_scope \"Composition\" for the composition layer, and\n              \"Projection\") for the linear transformations preceding the softmax.\n        '''\n        with tf.variable_scope('Composition'):\n            ### YOUR CODE HERE\n            #initializer=initializer=tf.random_normal_initializer(0,3)\n            embedding = tf.get_variable(\"embedding\",\n                                        [self.vocab.total_words, self.config.embed_size])\n            W1 = tf.get_variable(\"W1\", [2 * self.config.embed_size, self.config.embed_size])\n            b1 = tf.get_variable(\"b1\", [1, self.config.embed_size])\n            ### END YOUR CODE\n        with tf.variable_scope('Projection'):\n            ### YOUR CODE HERE\n            U = tf.get_variable(\"U\", [self.config.embed_size, self.config.label_size])\n            bs = tf.get_variable(\"bs\", [1, self.config.label_size])\n            ### END YOUR CODE\n\n(2) model construction:\ndef add_model(self, node):\n        \"\"\"Recursively build the model to compute the phrase embeddings in the tree\n\n        Hint: Refer to tree.py and vocab.py before you start. Refer to\n              the model's vocab with self.vocab\n        Hint: Reuse the \"Composition\" variable_scope here\n        Hint: Store a node's vector representation in node.tensor so it can be\n              used by it's parent\n        Hint: If node is a leaf node, it's vector representation is just that of the\n              word vector (see tf.gather()).\n        Args:\n            node: a Node object\n        Returns:\n            node_tensors: Dict: key = Node, value = tensor(1, embed_size)\n        \"\"\"\n        with tf.variable_scope('Composition', reuse=True):\n            ### YOUR CODE HERE\n            embedding = tf.get_variable(\"embedding\")\n            W1 = tf.get_variable(\"W1\")\n            b1 = tf.get_variable(\"b1\")\n            l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1)\n            tf.add_to_collection(name=\"l2_loss\", value=l2_loss)\n            ### END YOUR CODE\n\n        W_split = tf.split(0, 2, W1)\n        W_left  = W_split[0]\n        W_right = W_split[1]\n        node_tensors = OrderedDict()\n        curr_node_tensor = None\n        if node.isLeaf:\n            ### YOUR CODE HERE\n            word_id = self.vocab.encode(node.word)\n            curr_node_tensor = tf.expand_dims(tf.gather(embedding, word_id),0)\n            ### END YOUR CODE\n        else:\n            node_tensors.update(self.add_model(node.left))\n            node_tensors.update(self.add_model(node.right))\n            ### YOUR CODE HERE\n            #This operation could be done without the split call above\n            #child_tensor = tf.concat(1, [node_tensors[node.left], node_tensors[node.right]])\n            #curr_node_tensor = tf.nn.relu(tf.matmul(child_tensor, W1) + b1)\n            curr_node_tensor = tf.matmul(node_tensors[node.left], W_left) + tf.matmul(node_tensors[node.right], W_right) + b1\n            ### END YOUR CODE\n        node_tensors[node] = curr_node_tensor\n        return node_tensors\n\n(3) the projection layer is used to construct the sentiment for the root node:\n    def add_projections(self, node_tensors):\n        \"\"\"Add projections to the composition vectors to compute the raw sentiment scores\n\n        Hint: Reuse the \"Projection\" variable_scope here\n        Args:\n            node_tensors: tensor(?, embed_size)\n        Returns:\n            output: tensor(?, label_size)\n        \"\"\"\n        logits = None\n        ### YOUR CODE HERE\n        with tf.variable_scope(\"Projection\", reuse=True):\n            U = tf.get_variable(\"U\")\n            bs = tf.get_variable(\"bs\")\n        logits = tf.matmul(node_tensors, U) + bs\n        ### END YOUR CODE\n        return logits\n\n(4) loss (note: that you can use all the nodes, or just the root node for inference):\ntf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)),\nwhere logits is given by a call to inference\n    def inference(self, tree, predict_only_root=False):\n        \"\"\"For a given tree build the RNN models computation graph up to where it\n            may be used for inference.\n        Args:\n            tree: a Tree object on which to build the computation graph for the RNN\n        Returns:\n            softmax_linear: Output tensor with the computed logits.\n        \"\"\"\n        node_tensors = self.add_model(tree.root)\n        if predict_only_root:\n            node_tensors = node_tensors[tree.root]\n        else:\n            node_tensors = [tensor for node, tensor in node_tensors.items() if node.label!=2]\n            node_tensors = tf.concat(0, node_tensors)\n        return self.add_projections(node_tensors)", "body": "This is an NLP style model (sentiment analysis) and is based upon assignment 3 from [Stanford CS224D](http://cs224d.stanford.edu/syllabus.html). The tree structure is created as a purely python structure (tree class and node class). The variable state is saved/restored on a per batch usage and the tree structure is created using recursive descent (with any leaf nodes using an embedding operation to generate a feature vector), the loss could be created in a similar manner.\n\nThe tree class is made up of nodes (tree structure is created through a parser - I'm leaving the details out for now):\n\n```\nclass Node:  # a node in the tree\n    def __init__(self, label, word=None):\n        self.label = label\n        self.word = word\n        self.parent = None  # reference to parent\n        self.left = None  # reference to left child\n        self.right = None  # reference to right child\n        # true if I am a leaf (could have probably derived this from if I have\n        # a word)\n        self.isLeaf = False\n        # true if we have finished performing fowardprop on this node (note,\n        # there are many ways to implement the recursion.. some might not\n        # require this flag)\n```\n\nThe tensorflow contributions, there are few parts:\n(1) Variable creation (**NOTE**: `saver.restore(sess, './weights/%s.temp'%self.config.model_name)` is used to reload state) :\n\n```\n    def add_model_vars(self):\n        '''\n        You model contains the following parameters:\n            embedding:  tensor(vocab_size, embed_size)\n            W1:         tensor(2* embed_size, embed_size)\n            b1:         tensor(1, embed_size)\n            U:          tensor(embed_size, output_size)\n            bs:         tensor(1, output_size)\n        Hint: Add the tensorflow variables to the graph here and *reuse* them while building\n                the compution graphs for composition and projection for each tree\n        Hint: Use a variable_scope \"Composition\" for the composition layer, and\n              \"Projection\") for the linear transformations preceding the softmax.\n        '''\n        with tf.variable_scope('Composition'):\n            ### YOUR CODE HERE\n            #initializer=initializer=tf.random_normal_initializer(0,3)\n            embedding = tf.get_variable(\"embedding\",\n                                        [self.vocab.total_words, self.config.embed_size])\n            W1 = tf.get_variable(\"W1\", [2 * self.config.embed_size, self.config.embed_size])\n            b1 = tf.get_variable(\"b1\", [1, self.config.embed_size])\n            ### END YOUR CODE\n        with tf.variable_scope('Projection'):\n            ### YOUR CODE HERE\n            U = tf.get_variable(\"U\", [self.config.embed_size, self.config.label_size])\n            bs = tf.get_variable(\"bs\", [1, self.config.label_size])\n            ### END YOUR CODE\n```\n\n(2) model construction:\n\n```\ndef add_model(self, node):\n        \"\"\"Recursively build the model to compute the phrase embeddings in the tree\n\n        Hint: Refer to tree.py and vocab.py before you start. Refer to\n              the model's vocab with self.vocab\n        Hint: Reuse the \"Composition\" variable_scope here\n        Hint: Store a node's vector representation in node.tensor so it can be\n              used by it's parent\n        Hint: If node is a leaf node, it's vector representation is just that of the\n              word vector (see tf.gather()).\n        Args:\n            node: a Node object\n        Returns:\n            node_tensors: Dict: key = Node, value = tensor(1, embed_size)\n        \"\"\"\n        with tf.variable_scope('Composition', reuse=True):\n            ### YOUR CODE HERE\n            embedding = tf.get_variable(\"embedding\")\n            W1 = tf.get_variable(\"W1\")\n            b1 = tf.get_variable(\"b1\")\n            l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1)\n            tf.add_to_collection(name=\"l2_loss\", value=l2_loss)\n            ### END YOUR CODE\n\n        W_split = tf.split(0, 2, W1)\n        W_left  = W_split[0]\n        W_right = W_split[1]\n        node_tensors = OrderedDict()\n        curr_node_tensor = None\n        if node.isLeaf:\n            ### YOUR CODE HERE\n            word_id = self.vocab.encode(node.word)\n            curr_node_tensor = tf.expand_dims(tf.gather(embedding, word_id),0)\n            ### END YOUR CODE\n        else:\n            node_tensors.update(self.add_model(node.left))\n            node_tensors.update(self.add_model(node.right))\n            ### YOUR CODE HERE\n            #This operation could be done without the split call above\n            #child_tensor = tf.concat(1, [node_tensors[node.left], node_tensors[node.right]])\n            #curr_node_tensor = tf.nn.relu(tf.matmul(child_tensor, W1) + b1)\n            curr_node_tensor = tf.matmul(node_tensors[node.left], W_left) + tf.matmul(node_tensors[node.right], W_right) + b1\n            ### END YOUR CODE\n        node_tensors[node] = curr_node_tensor\n        return node_tensors\n```\n\n(3) the projection layer is used to construct the sentiment for the root node:\n\n```\n    def add_projections(self, node_tensors):\n        \"\"\"Add projections to the composition vectors to compute the raw sentiment scores\n\n        Hint: Reuse the \"Projection\" variable_scope here\n        Args:\n            node_tensors: tensor(?, embed_size)\n        Returns:\n            output: tensor(?, label_size)\n        \"\"\"\n        logits = None\n        ### YOUR CODE HERE\n        with tf.variable_scope(\"Projection\", reuse=True):\n            U = tf.get_variable(\"U\")\n            bs = tf.get_variable(\"bs\")\n        logits = tf.matmul(node_tensors, U) + bs\n        ### END YOUR CODE\n        return logits\n```\n\n(4) loss (note: that you can use all the nodes, or just the root node for inference):\n `tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))`,\nwhere `logits` is given by a call to `inference`\n\n```\n    def inference(self, tree, predict_only_root=False):\n        \"\"\"For a given tree build the RNN models computation graph up to where it\n            may be used for inference.\n        Args:\n            tree: a Tree object on which to build the computation graph for the RNN\n        Returns:\n            softmax_linear: Output tensor with the computed logits.\n        \"\"\"\n        node_tensors = self.add_model(tree.root)\n        if predict_only_root:\n            node_tensors = node_tensors[tree.root]\n        else:\n            node_tensors = [tensor for node, tensor in node_tensors.items() if node.label!=2]\n            node_tensors = tf.concat(0, node_tensors)\n        return self.add_projections(node_tensors)\n```\n"}