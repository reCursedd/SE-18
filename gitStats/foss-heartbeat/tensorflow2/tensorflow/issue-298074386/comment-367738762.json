{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/367738762", "html_url": "https://github.com/tensorflow/tensorflow/issues/17106#issuecomment-367738762", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17106", "id": 367738762, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzczODc2Mg==", "user": {"login": "jeherr", "id": 17952097, "node_id": "MDQ6VXNlcjE3OTUyMDk3", "avatar_url": "https://avatars0.githubusercontent.com/u/17952097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeherr", "html_url": "https://github.com/jeherr", "followers_url": "https://api.github.com/users/jeherr/followers", "following_url": "https://api.github.com/users/jeherr/following{/other_user}", "gists_url": "https://api.github.com/users/jeherr/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeherr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeherr/subscriptions", "organizations_url": "https://api.github.com/users/jeherr/orgs", "repos_url": "https://api.github.com/users/jeherr/repos", "events_url": "https://api.github.com/users/jeherr/events{/privacy}", "received_events_url": "https://api.github.com/users/jeherr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T16:32:38Z", "updated_at": "2018-02-22T16:32:38Z", "author_association": "NONE", "body_html": "<p>I've also run into a problem with dynamic partition for TensorFlow 1.5. I'll try to illustrate it as best as I can in code below.</p>\n<pre><code>padding_mask = tf.where(tf.not_equal(Zs, 0))\ndxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\ndxyzs = tf.gather_nd(dxyzs, padding_mask)\ndist_tensor = tf.norm(dxyzs, axis=-1)\ngauss = tf_gauss(dist_tensor, gauss_params)\nharmonics = tf_spherical_harmonics(dxyzs, dist_tensor, l_max)\nchannel_scatter = tf.gather(tf.equal(tf.expand_dims(Zs, axis=-1), elements), padding_mask[:,0])\nchannel_scatter = tf.where(channel_scatter, tf.ones_like(channel_scatter)), tf.zeros_like(channel_scatter)))\nchannel_gauss = tf.expand_dims(gauss, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\nchannel_harmonics = tf.expand_dims(harmonics, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\nembeds = tf.reshape(tf.einsum('ijkg,ijkl-&gt;ikgl', channel_gauss, channel_harmonics), [tf.shape(padding_mask)[0], -1])\npartition_idx = tf.cast(tf.where(tf.equal(tf.expand_dims(tf.gather_nd(Zs, padding_mask), axis=-1), tf.expand_dims(elements, axis=0)))[:,1], tf.int32)\nembeds = tf.dynamic_partition(embeds, partition_idx, num_elements)\nmol_idx = tf.dynamic_partition(padding_mask, partition_idx, num_elements)\n</code></pre>\n<p>So dxyzs is gathered according to padding_mask on line 3 which gives them the same first dimension sizes. The rest of the code here preserves that same first dimension up through the einsum on line 11. Line 12 defines my partitioning indices for dynamic partition which is then used for both lines 12 and 13. So embeds and mol_idx should each be a list of tensors where every tensor in embeds has the same size first dimension as every corresponding tensor in mol_idx.</p>\n<p>The issue is that the last tensor in embeds and mol_idx do not always (but sometimes do) have the same first dimension. This never occurs for me with any of the other tensors in each list. I haven't figured out whether or not the one with the wrong dimension size is from embeds or mol_idx (or both) for sure since these sizes will vary based on the batch, but I very strongly suspect it is from mol_idx based on typical sizes when the error does not occur. One possible cause may be the difference in dtypes. embeds contains tensors of either float32 or float64, and mol_idx contains tensors of int32. I haven't checked whether this occurs if mol_idx is int64 yet.</p>\n<p>The same problem does not happen in Tensorflow &lt;=1.4, and putting the dynamic partition ops on the cpu manually does fix this issue for me. Hope this helps.</p>", "body_text": "I've also run into a problem with dynamic partition for TensorFlow 1.5. I'll try to illustrate it as best as I can in code below.\npadding_mask = tf.where(tf.not_equal(Zs, 0))\ndxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\ndxyzs = tf.gather_nd(dxyzs, padding_mask)\ndist_tensor = tf.norm(dxyzs, axis=-1)\ngauss = tf_gauss(dist_tensor, gauss_params)\nharmonics = tf_spherical_harmonics(dxyzs, dist_tensor, l_max)\nchannel_scatter = tf.gather(tf.equal(tf.expand_dims(Zs, axis=-1), elements), padding_mask[:,0])\nchannel_scatter = tf.where(channel_scatter, tf.ones_like(channel_scatter)), tf.zeros_like(channel_scatter)))\nchannel_gauss = tf.expand_dims(gauss, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\nchannel_harmonics = tf.expand_dims(harmonics, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\nembeds = tf.reshape(tf.einsum('ijkg,ijkl->ikgl', channel_gauss, channel_harmonics), [tf.shape(padding_mask)[0], -1])\npartition_idx = tf.cast(tf.where(tf.equal(tf.expand_dims(tf.gather_nd(Zs, padding_mask), axis=-1), tf.expand_dims(elements, axis=0)))[:,1], tf.int32)\nembeds = tf.dynamic_partition(embeds, partition_idx, num_elements)\nmol_idx = tf.dynamic_partition(padding_mask, partition_idx, num_elements)\n\nSo dxyzs is gathered according to padding_mask on line 3 which gives them the same first dimension sizes. The rest of the code here preserves that same first dimension up through the einsum on line 11. Line 12 defines my partitioning indices for dynamic partition which is then used for both lines 12 and 13. So embeds and mol_idx should each be a list of tensors where every tensor in embeds has the same size first dimension as every corresponding tensor in mol_idx.\nThe issue is that the last tensor in embeds and mol_idx do not always (but sometimes do) have the same first dimension. This never occurs for me with any of the other tensors in each list. I haven't figured out whether or not the one with the wrong dimension size is from embeds or mol_idx (or both) for sure since these sizes will vary based on the batch, but I very strongly suspect it is from mol_idx based on typical sizes when the error does not occur. One possible cause may be the difference in dtypes. embeds contains tensors of either float32 or float64, and mol_idx contains tensors of int32. I haven't checked whether this occurs if mol_idx is int64 yet.\nThe same problem does not happen in Tensorflow <=1.4, and putting the dynamic partition ops on the cpu manually does fix this issue for me. Hope this helps.", "body": "I've also run into a problem with dynamic partition for TensorFlow 1.5. I'll try to illustrate it as best as I can in code below.\r\n\r\n```\r\npadding_mask = tf.where(tf.not_equal(Zs, 0))\r\ndxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\r\ndxyzs = tf.gather_nd(dxyzs, padding_mask)\r\ndist_tensor = tf.norm(dxyzs, axis=-1)\r\ngauss = tf_gauss(dist_tensor, gauss_params)\r\nharmonics = tf_spherical_harmonics(dxyzs, dist_tensor, l_max)\r\nchannel_scatter = tf.gather(tf.equal(tf.expand_dims(Zs, axis=-1), elements), padding_mask[:,0])\r\nchannel_scatter = tf.where(channel_scatter, tf.ones_like(channel_scatter)), tf.zeros_like(channel_scatter)))\r\nchannel_gauss = tf.expand_dims(gauss, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\r\nchannel_harmonics = tf.expand_dims(harmonics, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\r\nembeds = tf.reshape(tf.einsum('ijkg,ijkl->ikgl', channel_gauss, channel_harmonics), [tf.shape(padding_mask)[0], -1])\r\npartition_idx = tf.cast(tf.where(tf.equal(tf.expand_dims(tf.gather_nd(Zs, padding_mask), axis=-1), tf.expand_dims(elements, axis=0)))[:,1], tf.int32)\r\nembeds = tf.dynamic_partition(embeds, partition_idx, num_elements)\r\nmol_idx = tf.dynamic_partition(padding_mask, partition_idx, num_elements)\r\n```\r\n\r\nSo dxyzs is gathered according to padding_mask on line 3 which gives them the same first dimension sizes. The rest of the code here preserves that same first dimension up through the einsum on line 11. Line 12 defines my partitioning indices for dynamic partition which is then used for both lines 12 and 13. So embeds and mol_idx should each be a list of tensors where every tensor in embeds has the same size first dimension as every corresponding tensor in mol_idx. \r\n\r\nThe issue is that the last tensor in embeds and mol_idx do not always (but sometimes do) have the same first dimension. This never occurs for me with any of the other tensors in each list. I haven't figured out whether or not the one with the wrong dimension size is from embeds or mol_idx (or both) for sure since these sizes will vary based on the batch, but I very strongly suspect it is from mol_idx based on typical sizes when the error does not occur. One possible cause may be the difference in dtypes. embeds contains tensors of either float32 or float64, and mol_idx contains tensors of int32. I haven't checked whether this occurs if mol_idx is int64 yet.\r\n\r\nThe same problem does not happen in Tensorflow <=1.4, and putting the dynamic partition ops on the cpu manually does fix this issue for me. Hope this helps.\r\n"}