{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21476", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21476/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21476/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21476/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21476", "id": 348691115, "node_id": "MDU6SXNzdWUzNDg2OTExMTU=", "number": 21476, "title": "Transforming a dataset pre-processing to an inference graph", "user": {"login": "baluyotraf", "id": 7478783, "node_id": "MDQ6VXNlcjc0Nzg3ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7478783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/baluyotraf", "html_url": "https://github.com/baluyotraf", "followers_url": "https://api.github.com/users/baluyotraf/followers", "following_url": "https://api.github.com/users/baluyotraf/following{/other_user}", "gists_url": "https://api.github.com/users/baluyotraf/gists{/gist_id}", "starred_url": "https://api.github.com/users/baluyotraf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/baluyotraf/subscriptions", "organizations_url": "https://api.github.com/users/baluyotraf/orgs", "repos_url": "https://api.github.com/users/baluyotraf/repos", "events_url": "https://api.github.com/users/baluyotraf/events{/privacy}", "received_events_url": "https://api.github.com/users/baluyotraf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-08-08T11:47:31Z", "updated_at": "2018-09-15T03:22:47Z", "closed_at": "2018-09-15T03:22:47Z", "author_association": "NONE", "body_html": "<p>If we are working with data with fixed dimensions we can easily load the data in a dataset using from_tensor_slices. Thus, when working on inference, we can use a placeholder that will give the dataset the numpy array.</p>\n<p>When working with data with varying dimensions (text and a like) most of the answers I've seen suggest using the from_generator function and it works. However there doesn't seem to be a way to make if work in inference.</p>\n<p>Several solutions I though of are running the dataset ops separately from the inference graph or serialize the data and have the inference graph read it.</p>\n<p>Is there a recommended way of doing this? I want to reuse the dataset preprocessing codes in the training rather than implement them again with in numpy so that I can feed it to the inference graph. If not I think there is a value in having it in tensorflow.</p>\n<p>Have I written custom code: N/A<br>\nOS Platform and Distribution: N/A<br>\nTensorFlow installed from: N/A<br>\nTensorFlow version: master branch<br>\nBazel version: N/A<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: N/A<br>\nExact command to reproduce: N/A<br>\nMobile device: N/A</p>", "body_text": "If we are working with data with fixed dimensions we can easily load the data in a dataset using from_tensor_slices. Thus, when working on inference, we can use a placeholder that will give the dataset the numpy array.\nWhen working with data with varying dimensions (text and a like) most of the answers I've seen suggest using the from_generator function and it works. However there doesn't seem to be a way to make if work in inference.\nSeveral solutions I though of are running the dataset ops separately from the inference graph or serialize the data and have the inference graph read it.\nIs there a recommended way of doing this? I want to reuse the dataset preprocessing codes in the training rather than implement them again with in numpy so that I can feed it to the inference graph. If not I think there is a value in having it in tensorflow.\nHave I written custom code: N/A\nOS Platform and Distribution: N/A\nTensorFlow installed from: N/A\nTensorFlow version: master branch\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\nMobile device: N/A", "body": "If we are working with data with fixed dimensions we can easily load the data in a dataset using from_tensor_slices. Thus, when working on inference, we can use a placeholder that will give the dataset the numpy array.\r\n\r\nWhen working with data with varying dimensions (text and a like) most of the answers I've seen suggest using the from_generator function and it works. However there doesn't seem to be a way to make if work in inference.\r\n\r\nSeveral solutions I though of are running the dataset ops separately from the inference graph or serialize the data and have the inference graph read it.\r\n\r\nIs there a recommended way of doing this? I want to reuse the dataset preprocessing codes in the training rather than implement them again with in numpy so that I can feed it to the inference graph. If not I think there is a value in having it in tensorflow.\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: master branch\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nMobile device: N/A"}