{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12954", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12954/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12954/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12954/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12954", "id": 256578178, "node_id": "MDU6SXNzdWUyNTY1NzgxNzg=", "number": 12954, "title": "Naming issue of tensorflow.python.layers.core.Dense", "user": {"login": "Yevgnen", "id": 16749790, "node_id": "MDQ6VXNlcjE2NzQ5Nzkw", "avatar_url": "https://avatars0.githubusercontent.com/u/16749790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yevgnen", "html_url": "https://github.com/Yevgnen", "followers_url": "https://api.github.com/users/Yevgnen/followers", "following_url": "https://api.github.com/users/Yevgnen/following{/other_user}", "gists_url": "https://api.github.com/users/Yevgnen/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yevgnen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yevgnen/subscriptions", "organizations_url": "https://api.github.com/users/Yevgnen/orgs", "repos_url": "https://api.github.com/users/Yevgnen/repos", "events_url": "https://api.github.com/users/Yevgnen/events{/privacy}", "received_events_url": "https://api.github.com/users/Yevgnen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-09-11T04:22:21Z", "updated_at": "2018-02-05T19:57:56Z", "closed_at": "2018-01-18T00:44:58Z", "author_association": "NONE", "body_html": "<p>Since I think this issue has nothing to do with the system information, I would temporarily ignore them.</p>\n<ul>\n<li><strong>TensorFlow version (use command below)</strong>:v1.2.1-4-g4acb96a 1.2.1</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The <code>Dense</code> layers defined in <code>tensorflow.python.layers.core</code> build <code>kernel</code> and <code>bias</code> in <code>build</code> function, while the <code>build</code> function is called in <code>__call__</code> function. The may cause that sometime one define a layer, but not call it immediately which causes an unexpected variable naming issue.</p>\n<h3>Source code / logs</h3>\n<p>For example, when I try to implement a toy seq2seq model, the following code</p>\n<pre><code>            with tf.variable_scope('output'):\n                self._output_layer = _core_layers.Dense(\n                    self._vocab_size, name='output_layer')\n\n            if targets is not None: # TRAINING\n                embedding_targets = tf.nn.embedding_lookup(\n                    self._embedding, targets)\n\n                outputs, final_state, = tf.nn.dynamic_rnn(\n                    cell=self._cell,\n                    inputs=embedding_targets,\n                    sequence_length=lengths,\n                    dtype=_FLOAT,\n                    time_major=True)\n\n                logits = self._output_layer(outputs)\n            else: # INFER, or sampling\n                _, batch_size, _ = tf.unstack(tf.shape(encoder_outputs))\n                eos_ids = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n                eos_step_embedded = tf.nn.embedding_lookup(\n                    self._embedding, eos_ids)\n\n                def loop_fn_transition(time, cell_output, cell_state,\n                                       loop_state):\n\n                    def get_input():\n                        output_logits = self._output_layer(cell_output)  #&lt;----- kernel/bias of dense defined here\n                        predictions = tf.argmax(output_logits, axis=1)\n                        next_input = tf.nn.embedding_lookup(\n                            self._embedding, predictions)\n                        return next_input\n\n                    elements_finished = (time &gt;= lengths)\n                    emit_output = cell_output\n                    cell_state = cell_state\n                    loop_state = None\n\n                    return (elements_finished, get_input(), cell_state,\n                            emit_output, loop_state)\n\n                def loop_fn(time, cell_output, cell_state, loop_state):\n                    if cell_state is None:\n                        elements_finished = (0 &gt;= lengths)\n                        next_input = eos_step_embedded\n                        cell_state = encoder_final_state\n                        emit_output = None\n                        loop_state = None\n\n                        return (elements_finished, next_input, cell_state,\n                                emit_output, loop_state)\n                    else:\n                        return loop_fn_transition(time, cell_output, cell_state,\n                                                  loop_state)\n\n                cell_outputs, final_state, _ = tf.nn.raw_rnn(\n                    self._cell, loop_fn)\n</code></pre>\n<p>I got (by inspecting the checkpoint file)</p>\n<pre><code>basic_seq2seq/decoder/output_layer/bias (DT_FLOAT) [1301]\nbasic_seq2seq/decoder/output_layer/kernel (DT_FLOAT) [200,1301]\n</code></pre>\n<p>in training mode while got</p>\n<pre><code>basic_seq2seq/decoder/rnn/output_layer/bias (DT_FLOAT) [1301]\nbasic_seq2seq/decoder/rnn/output_layer/kernel (DT_FLOAT) [200,1301]\n</code></pre>\n<p>in inference mode. So I can't restore a training checkpoint when inference due to <code>NotFoundError (see above for traceback): Key basic_seq2seq/decoder/rnn/output_layer/kernel not found in checkpoint</code></p>", "body_text": "Since I think this issue has nothing to do with the system information, I would temporarily ignore them.\n\nTensorFlow version (use command below):v1.2.1-4-g4acb96a 1.2.1\n\nDescribe the problem\nThe Dense layers defined in tensorflow.python.layers.core build kernel and bias in build function, while the build function is called in __call__ function. The may cause that sometime one define a layer, but not call it immediately which causes an unexpected variable naming issue.\nSource code / logs\nFor example, when I try to implement a toy seq2seq model, the following code\n            with tf.variable_scope('output'):\n                self._output_layer = _core_layers.Dense(\n                    self._vocab_size, name='output_layer')\n\n            if targets is not None: # TRAINING\n                embedding_targets = tf.nn.embedding_lookup(\n                    self._embedding, targets)\n\n                outputs, final_state, = tf.nn.dynamic_rnn(\n                    cell=self._cell,\n                    inputs=embedding_targets,\n                    sequence_length=lengths,\n                    dtype=_FLOAT,\n                    time_major=True)\n\n                logits = self._output_layer(outputs)\n            else: # INFER, or sampling\n                _, batch_size, _ = tf.unstack(tf.shape(encoder_outputs))\n                eos_ids = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n                eos_step_embedded = tf.nn.embedding_lookup(\n                    self._embedding, eos_ids)\n\n                def loop_fn_transition(time, cell_output, cell_state,\n                                       loop_state):\n\n                    def get_input():\n                        output_logits = self._output_layer(cell_output)  #<----- kernel/bias of dense defined here\n                        predictions = tf.argmax(output_logits, axis=1)\n                        next_input = tf.nn.embedding_lookup(\n                            self._embedding, predictions)\n                        return next_input\n\n                    elements_finished = (time >= lengths)\n                    emit_output = cell_output\n                    cell_state = cell_state\n                    loop_state = None\n\n                    return (elements_finished, get_input(), cell_state,\n                            emit_output, loop_state)\n\n                def loop_fn(time, cell_output, cell_state, loop_state):\n                    if cell_state is None:\n                        elements_finished = (0 >= lengths)\n                        next_input = eos_step_embedded\n                        cell_state = encoder_final_state\n                        emit_output = None\n                        loop_state = None\n\n                        return (elements_finished, next_input, cell_state,\n                                emit_output, loop_state)\n                    else:\n                        return loop_fn_transition(time, cell_output, cell_state,\n                                                  loop_state)\n\n                cell_outputs, final_state, _ = tf.nn.raw_rnn(\n                    self._cell, loop_fn)\n\nI got (by inspecting the checkpoint file)\nbasic_seq2seq/decoder/output_layer/bias (DT_FLOAT) [1301]\nbasic_seq2seq/decoder/output_layer/kernel (DT_FLOAT) [200,1301]\n\nin training mode while got\nbasic_seq2seq/decoder/rnn/output_layer/bias (DT_FLOAT) [1301]\nbasic_seq2seq/decoder/rnn/output_layer/kernel (DT_FLOAT) [200,1301]\n\nin inference mode. So I can't restore a training checkpoint when inference due to NotFoundError (see above for traceback): Key basic_seq2seq/decoder/rnn/output_layer/kernel not found in checkpoint", "body": "Since I think this issue has nothing to do with the system information, I would temporarily ignore them.\r\n\r\n- **TensorFlow version (use command below)**:v1.2.1-4-g4acb96a 1.2.1\r\n\r\n\r\n### Describe the problem\r\nThe `Dense` layers defined in `tensorflow.python.layers.core` build `kernel` and `bias` in `build` function, while the `build` function is called in `__call__` function. The may cause that sometime one define a layer, but not call it immediately which causes an unexpected variable naming issue.\r\n\r\n### Source code / logs\r\nFor example, when I try to implement a toy seq2seq model, the following code\r\n\r\n```\r\n            with tf.variable_scope('output'):\r\n                self._output_layer = _core_layers.Dense(\r\n                    self._vocab_size, name='output_layer')\r\n\r\n            if targets is not None: # TRAINING\r\n                embedding_targets = tf.nn.embedding_lookup(\r\n                    self._embedding, targets)\r\n\r\n                outputs, final_state, = tf.nn.dynamic_rnn(\r\n                    cell=self._cell,\r\n                    inputs=embedding_targets,\r\n                    sequence_length=lengths,\r\n                    dtype=_FLOAT,\r\n                    time_major=True)\r\n\r\n                logits = self._output_layer(outputs)\r\n            else: # INFER, or sampling\r\n                _, batch_size, _ = tf.unstack(tf.shape(encoder_outputs))\r\n                eos_ids = tf.ones([batch_size], dtype=tf.int32, name='EOS')\r\n                eos_step_embedded = tf.nn.embedding_lookup(\r\n                    self._embedding, eos_ids)\r\n\r\n                def loop_fn_transition(time, cell_output, cell_state,\r\n                                       loop_state):\r\n\r\n                    def get_input():\r\n                        output_logits = self._output_layer(cell_output)  #<----- kernel/bias of dense defined here\r\n                        predictions = tf.argmax(output_logits, axis=1)\r\n                        next_input = tf.nn.embedding_lookup(\r\n                            self._embedding, predictions)\r\n                        return next_input\r\n\r\n                    elements_finished = (time >= lengths)\r\n                    emit_output = cell_output\r\n                    cell_state = cell_state\r\n                    loop_state = None\r\n\r\n                    return (elements_finished, get_input(), cell_state,\r\n                            emit_output, loop_state)\r\n\r\n                def loop_fn(time, cell_output, cell_state, loop_state):\r\n                    if cell_state is None:\r\n                        elements_finished = (0 >= lengths)\r\n                        next_input = eos_step_embedded\r\n                        cell_state = encoder_final_state\r\n                        emit_output = None\r\n                        loop_state = None\r\n\r\n                        return (elements_finished, next_input, cell_state,\r\n                                emit_output, loop_state)\r\n                    else:\r\n                        return loop_fn_transition(time, cell_output, cell_state,\r\n                                                  loop_state)\r\n\r\n                cell_outputs, final_state, _ = tf.nn.raw_rnn(\r\n                    self._cell, loop_fn)\r\n```\r\n\r\nI got (by inspecting the checkpoint file)\r\n\r\n```\r\nbasic_seq2seq/decoder/output_layer/bias (DT_FLOAT) [1301]\r\nbasic_seq2seq/decoder/output_layer/kernel (DT_FLOAT) [200,1301]\r\n```\r\nin training mode while got \r\n\r\n```\r\nbasic_seq2seq/decoder/rnn/output_layer/bias (DT_FLOAT) [1301]\r\nbasic_seq2seq/decoder/rnn/output_layer/kernel (DT_FLOAT) [200,1301]\r\n```\r\nin inference mode. So I can't restore a training checkpoint when inference due to `NotFoundError (see above for traceback): Key basic_seq2seq/decoder/rnn/output_layer/kernel not found in checkpoint`\r\n"}