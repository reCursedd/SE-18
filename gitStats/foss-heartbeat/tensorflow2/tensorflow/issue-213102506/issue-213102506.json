{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8243", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8243/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8243/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8243/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8243", "id": 213102506, "node_id": "MDU6SXNzdWUyMTMxMDI1MDY=", "number": 8243, "title": "[Time Consuming] TF C++ Session->Run - Images for Real-time Inference", "user": {"login": "HossamAmer12", "id": 13424646, "node_id": "MDQ6VXNlcjEzNDI0NjQ2", "avatar_url": "https://avatars0.githubusercontent.com/u/13424646?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HossamAmer12", "html_url": "https://github.com/HossamAmer12", "followers_url": "https://api.github.com/users/HossamAmer12/followers", "following_url": "https://api.github.com/users/HossamAmer12/following{/other_user}", "gists_url": "https://api.github.com/users/HossamAmer12/gists{/gist_id}", "starred_url": "https://api.github.com/users/HossamAmer12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HossamAmer12/subscriptions", "organizations_url": "https://api.github.com/users/HossamAmer12/orgs", "repos_url": "https://api.github.com/users/HossamAmer12/repos", "events_url": "https://api.github.com/users/HossamAmer12/events{/privacy}", "received_events_url": "https://api.github.com/users/HossamAmer12/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-09T17:05:02Z", "updated_at": "2017-03-09T17:27:33Z", "closed_at": "2017-03-09T17:09:12Z", "author_association": "NONE", "body_html": "<p>[Tensorflow (TF) on CPU]<br>\nI am using the skeleton code provided for C++ TF inference [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc] in order to run a frozen model I have created in Python. This model is an FC NN with two hidden layers.</p>\n<p>In my current project's code, I run the inference for each single image (8x8 pixels). For each sample, each Session-&gt;Run call takes about 0.02 seconds, which is expensive in my application. However, when I send a batch of 1560 samples, the Session-&gt;Run call takes about 0.03 seconds.</p>\n<p>Are these time measurements normal for the Session-&gt;Run Call? From the C++ end, should I send my frozen model batches of images in one Tensor?  From the Python end, are there optimisation tricks to alleviate that bottleneck? Is there a way to concurrently do Session-Run calls in C++?</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"139518464\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1439\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1439/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1439\">#1439</a><br>\n<a href=\"http://stackoverflow.com/questions/39070708/why-sometimes-tensorflow-runs-slower-and-slower-with-the-process-of-training\" rel=\"nofollow\">http://stackoverflow.com/questions/39070708/why-sometimes-tensorflow-runs-slower-and-slower-with-the-process-of-training</a></p>\n<h3>Environment info</h3>\n<p>Operating System: Linux<br>\nInstalled version of CUDA and cuDNN: N/A</p>\n<h3>What other attempted solutions have you tried?</h3>\n<ol>\n<li>I installed TF using the optimised instruction set for the CPU, but it does not seem to give me the huge time saving mentioned here [http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions]</li>\n<li>Unified the session for the Graph I created.</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>", "body_text": "[Tensorflow (TF) on CPU]\nI am using the skeleton code provided for C++ TF inference [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc] in order to run a frozen model I have created in Python. This model is an FC NN with two hidden layers.\nIn my current project's code, I run the inference for each single image (8x8 pixels). For each sample, each Session->Run call takes about 0.02 seconds, which is expensive in my application. However, when I send a batch of 1560 samples, the Session->Run call takes about 0.03 seconds.\nAre these time measurements normal for the Session->Run Call? From the C++ end, should I send my frozen model batches of images in one Tensor?  From the Python end, are there optimisation tricks to alleviate that bottleneck? Is there a way to concurrently do Session-Run calls in C++?\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n#1439\nhttp://stackoverflow.com/questions/39070708/why-sometimes-tensorflow-runs-slower-and-slower-with-the-process-of-training\nEnvironment info\nOperating System: Linux\nInstalled version of CUDA and cuDNN: N/A\nWhat other attempted solutions have you tried?\n\nI installed TF using the optimised instruction set for the CPU, but it does not seem to give me the huge time saving mentioned here [http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions]\nUnified the session for the Graph I created.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).", "body": "[Tensorflow (TF) on CPU]\r\nI am using the skeleton code provided for C++ TF inference [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc] in order to run a frozen model I have created in Python. This model is an FC NN with two hidden layers.\r\n\r\nIn my current project's code, I run the inference for each single image (8x8 pixels). For each sample, each Session->Run call takes about 0.02 seconds, which is expensive in my application. However, when I send a batch of 1560 samples, the Session->Run call takes about 0.03 seconds. \r\n\r\nAre these time measurements normal for the Session->Run Call? From the C++ end, should I send my frozen model batches of images in one Tensor?  From the Python end, are there optimisation tricks to alleviate that bottleneck? Is there a way to concurrently do Session-Run calls in C++?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/1439\r\nhttp://stackoverflow.com/questions/39070708/why-sometimes-tensorflow-runs-slower-and-slower-with-the-process-of-training\r\n\r\n### Environment info\r\nOperating System: Linux\r\nInstalled version of CUDA and cuDNN: N/A\r\n\r\n### What other attempted solutions have you tried?\r\n1. I installed TF using the optimised instruction set for the CPU, but it does not seem to give me the huge time saving mentioned here [http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions]\r\n2. Unified the session for the Graph I created.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n"}