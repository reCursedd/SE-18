{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22137", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22137/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22137/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22137/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22137", "id": 357902847, "node_id": "MDU6SXNzdWUzNTc5MDI4NDc=", "number": 22137, "title": "ps0 will be OOM using MonitoredTrainingSession when workers too many", "user": {"login": "jackonan", "id": 9108860, "node_id": "MDQ6VXNlcjkxMDg4NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/9108860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jackonan", "html_url": "https://github.com/jackonan", "followers_url": "https://api.github.com/users/jackonan/followers", "following_url": "https://api.github.com/users/jackonan/following{/other_user}", "gists_url": "https://api.github.com/users/jackonan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jackonan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jackonan/subscriptions", "organizations_url": "https://api.github.com/users/jackonan/orgs", "repos_url": "https://api.github.com/users/jackonan/repos", "events_url": "https://api.github.com/users/jackonan/events{/privacy}", "received_events_url": "https://api.github.com/users/jackonan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-07T03:07:28Z", "updated_at": "2018-11-23T18:39:29Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: No</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 2.7.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 1.9</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.9.2</li>\n<li><strong>CUDA/cuDNN version</strong>: CPU</li>\n<li><strong>GPU model and memory</strong>: CPU</li>\n<li><strong>Exact command to reproduce</strong>: Using MonitoredTrainingSession and thousands of workers.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>My model is large and complex. Thousands of worker&amp;ps are needed and MonitoredTrainingSession is used. In the stage of initialization, the memory of ps0 will raise up to 100G and then OOM, but other ps work correctly. Op device placement is tf.train.replica_device_setter and no other special strategy.</p>\n<p>I open the log_device_placement and found that all ops related with report_uninitialized_xxx are placed on ps0. I think this is the root cause.</p>\n<p>I change the code as PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357898972\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/22136\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/22136/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/22136\">#22136</a> , and then it works correctly.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.4\nPython version: 2.7.5\nBazel version (if compiling from source): 1.9\nGCC/Compiler version (if compiling from source): 4.9.2\nCUDA/cuDNN version: CPU\nGPU model and memory: CPU\nExact command to reproduce: Using MonitoredTrainingSession and thousands of workers.\n\nDescribe the problem\nMy model is large and complex. Thousands of worker&ps are needed and MonitoredTrainingSession is used. In the stage of initialization, the memory of ps0 will raise up to 100G and then OOM, but other ps work correctly. Op device placement is tf.train.replica_device_setter and no other special strategy.\nI open the log_device_placement and found that all ops related with report_uninitialized_xxx are placed on ps0. I think this is the root cause.\nI change the code as PR #22136 , and then it works correctly.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: 1.9\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.2\r\n- **CUDA/cuDNN version**: CPU\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**: Using MonitoredTrainingSession and thousands of workers.\r\n\r\n\r\n### Describe the problem\r\nMy model is large and complex. Thousands of worker&ps are needed and MonitoredTrainingSession is used. In the stage of initialization, the memory of ps0 will raise up to 100G and then OOM, but other ps work correctly. Op device placement is tf.train.replica_device_setter and no other special strategy.\r\n\r\nI open the log_device_placement and found that all ops related with report_uninitialized_xxx are placed on ps0. I think this is the root cause.\r\n\r\nI change the code as PR #22136 , and then it works correctly.\r\n"}