{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/385443655", "html_url": "https://github.com/tensorflow/tensorflow/issues/18979#issuecomment-385443655", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18979", "id": 385443655, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTQ0MzY1NQ==", "user": {"login": "liquidscience", "id": 8677795, "node_id": "MDQ6VXNlcjg2Nzc3OTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8677795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liquidscience", "html_url": "https://github.com/liquidscience", "followers_url": "https://api.github.com/users/liquidscience/followers", "following_url": "https://api.github.com/users/liquidscience/following{/other_user}", "gists_url": "https://api.github.com/users/liquidscience/gists{/gist_id}", "starred_url": "https://api.github.com/users/liquidscience/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liquidscience/subscriptions", "organizations_url": "https://api.github.com/users/liquidscience/orgs", "repos_url": "https://api.github.com/users/liquidscience/repos", "events_url": "https://api.github.com/users/liquidscience/events{/privacy}", "received_events_url": "https://api.github.com/users/liquidscience/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-30T15:58:42Z", "updated_at": "2018-04-30T15:58:42Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=79535\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bjacob\">@bjacob</a> Thanks for the response,<br>\nYes , I have tried<br>\n<code>--output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars </code><br>\nBut the output i want should be directly coming from<code> Conv2d_13_pointwise/Conv2D_Fold</code> layer without the activation.</p>\n<p>I was thinking if we are performing quantized inference than quantization on a mobile device would be happening after each step and not after a certain number of operations like Convolution followed by Batchnormalization and RELU6, so it must be possible to get this intermediate quantized result?</p>\n<p>Or is it that at inference time some float operations are still being performed which get changed to 8bits after certain steps?</p>\n<p>I might be wrong on this , but thats what i understood.</p>", "body_text": "@bjacob Thanks for the response,\nYes , I have tried\n--output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars \nBut the output i want should be directly coming from Conv2d_13_pointwise/Conv2D_Fold layer without the activation.\nI was thinking if we are performing quantized inference than quantization on a mobile device would be happening after each step and not after a certain number of operations like Convolution followed by Batchnormalization and RELU6, so it must be possible to get this intermediate quantized result?\nOr is it that at inference time some float operations are still being performed which get changed to 8bits after certain steps?\nI might be wrong on this , but thats what i understood.", "body": "@bjacob Thanks for the response,\r\nYes , I have tried \r\n`--output_array=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars\r\n`\r\nBut the output i want should be directly coming from` Conv2d_13_pointwise/Conv2D_Fold` layer without the activation.\r\n\r\nI was thinking if we are performing quantized inference than quantization on a mobile device would be happening after each step and not after a certain number of operations like Convolution followed by Batchnormalization and RELU6, so it must be possible to get this intermediate quantized result?\r\n\r\nOr is it that at inference time some float operations are still being performed which get changed to 8bits after certain steps?\r\n\r\nI might be wrong on this , but thats what i understood."}