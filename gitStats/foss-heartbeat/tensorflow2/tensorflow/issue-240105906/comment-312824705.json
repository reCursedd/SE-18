{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312824705", "html_url": "https://github.com/tensorflow/tensorflow/issues/11239#issuecomment-312824705", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11239", "id": 312824705, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjgyNDcwNQ==", "user": {"login": "OlavHN", "id": 324645, "node_id": "MDQ6VXNlcjMyNDY0NQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/324645?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OlavHN", "html_url": "https://github.com/OlavHN", "followers_url": "https://api.github.com/users/OlavHN/followers", "following_url": "https://api.github.com/users/OlavHN/following{/other_user}", "gists_url": "https://api.github.com/users/OlavHN/gists{/gist_id}", "starred_url": "https://api.github.com/users/OlavHN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OlavHN/subscriptions", "organizations_url": "https://api.github.com/users/OlavHN/orgs", "repos_url": "https://api.github.com/users/OlavHN/repos", "events_url": "https://api.github.com/users/OlavHN/events{/privacy}", "received_events_url": "https://api.github.com/users/OlavHN/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-04T09:07:20Z", "updated_at": "2017-07-04T09:07:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As a \"workaround\" / hack it's possible to split the dataset into two, and reinitialize the second dataset from the first on each session run:</p>\n<pre><code>import tensorflow as tf\n\nv = tf.Variable(5, dtype=tf.int64)\n\ndataset1 = tf.contrib.data.Dataset.range(10)\niterator1 = dataset1.make_one_shot_iterator()\n\n# Out of dataset function scope so mutations possible\nelements = iterator1.get_next() + v.assign(v + 1)\n\ndataset2 = tf.contrib.data.Dataset.from_tensors(elements)\niterator2 = dataset2.make_initializable_iterator()\n\n# Crazy hack .. reinitialize second iterator from first for each run\nwith tf.control_dependencies([iterator2.initializer]):\n    next = iterator2.get_next()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(10):\n        res = sess.run(next)\n        print(res)\n</code></pre>\n<p>The reason for wanting this functionality is making oversampling part of the dataset pipeline by keeping a running average of the input distribution (i.e. calculate statistics on the original data, but output oversampled datapoints at the end of the pipeline).</p>", "body_text": "As a \"workaround\" / hack it's possible to split the dataset into two, and reinitialize the second dataset from the first on each session run:\nimport tensorflow as tf\n\nv = tf.Variable(5, dtype=tf.int64)\n\ndataset1 = tf.contrib.data.Dataset.range(10)\niterator1 = dataset1.make_one_shot_iterator()\n\n# Out of dataset function scope so mutations possible\nelements = iterator1.get_next() + v.assign(v + 1)\n\ndataset2 = tf.contrib.data.Dataset.from_tensors(elements)\niterator2 = dataset2.make_initializable_iterator()\n\n# Crazy hack .. reinitialize second iterator from first for each run\nwith tf.control_dependencies([iterator2.initializer]):\n    next = iterator2.get_next()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(10):\n        res = sess.run(next)\n        print(res)\n\nThe reason for wanting this functionality is making oversampling part of the dataset pipeline by keeping a running average of the input distribution (i.e. calculate statistics on the original data, but output oversampled datapoints at the end of the pipeline).", "body": "As a \"workaround\" / hack it's possible to split the dataset into two, and reinitialize the second dataset from the first on each session run:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.Variable(5, dtype=tf.int64)\r\n\r\ndataset1 = tf.contrib.data.Dataset.range(10)\r\niterator1 = dataset1.make_one_shot_iterator()\r\n\r\n# Out of dataset function scope so mutations possible\r\nelements = iterator1.get_next() + v.assign(v + 1)\r\n\r\ndataset2 = tf.contrib.data.Dataset.from_tensors(elements)\r\niterator2 = dataset2.make_initializable_iterator()\r\n\r\n# Crazy hack .. reinitialize second iterator from first for each run\r\nwith tf.control_dependencies([iterator2.initializer]):\r\n    next = iterator2.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    for i in range(10):\r\n        res = sess.run(next)\r\n        print(res)\r\n```\r\n\r\nThe reason for wanting this functionality is making oversampling part of the dataset pipeline by keeping a running average of the input distribution (i.e. calculate statistics on the original data, but output oversampled datapoints at the end of the pipeline)."}