{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/414105533", "html_url": "https://github.com/tensorflow/tensorflow/issues/21602#issuecomment-414105533", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21602", "id": 414105533, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDEwNTUzMw==", "user": {"login": "alokranjan007", "id": 19912807, "node_id": "MDQ6VXNlcjE5OTEyODA3", "avatar_url": "https://avatars1.githubusercontent.com/u/19912807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alokranjan007", "html_url": "https://github.com/alokranjan007", "followers_url": "https://api.github.com/users/alokranjan007/followers", "following_url": "https://api.github.com/users/alokranjan007/following{/other_user}", "gists_url": "https://api.github.com/users/alokranjan007/gists{/gist_id}", "starred_url": "https://api.github.com/users/alokranjan007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alokranjan007/subscriptions", "organizations_url": "https://api.github.com/users/alokranjan007/orgs", "repos_url": "https://api.github.com/users/alokranjan007/repos", "events_url": "https://api.github.com/users/alokranjan007/events{/privacy}", "received_events_url": "https://api.github.com/users/alokranjan007/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-19T05:52:49Z", "updated_at": "2018-08-19T05:52:49Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4723042\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/achowdhery\">@achowdhery</a> ,<br>\nThank you for your response. I had not tried with ssd_mobilenet_v1_coco. But I had used ssd_mobilnet_v2_coco and it worked perfect on my device. Every apps are running perfectly fine for this case. However, when I am  trying to port the custom model using tflite; installed app is getting closed repeatedly.</p>\n<p>I was trying again. While converting from .pb file to tflite format, I used inference_type=FLOAT and made a change in the DetectorActivity.java file for <code>private static final boolean TF_OD_API_IS_QUANTIZED = false;</code>. After this I rebuild the apk and installed on my device. The app is installed, but after sometime (10 sec) it is getting closed again  with a message \"TFLite Demo has stopped\" close the app. Since, I have not changed anything for the Classifier and speech files, both are working fine. Only concern is with the Detection.</p>\n<p>Please find the detailed steps below from converting frozen graph into the tflite format. (I retrained my model with my data set and have frozen_graph.pb files and labelmap.pbtxt). It may be noted here that I have not downloaded any tflite file from any source.</p>\n<p>Step 1:<br>\n<code>DETECT_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb</code><br>\n<code>STRIPPED_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb</code></p>\n<p>step 2:<br>\n<code>cd $HOME/tensorflow-master</code><br>\n<code>touch WORKSPACE</code></p>\n<p>step 3 :<br>\n<code>bazel build tensorflow/python/tools:optimize_for_inference &amp;&amp; \\ bazel-bin/tensorflow/python/tools/optimize_for_inference \\ --input=$DETECT_PB \\ --output=$STRIPPED_PB \\ --frozen_graph=True \\ --input_names=Preprocessor/sub \\ --output_names=concat,concat_1 \\ --alsologtostderr</code></p>\n<p>step 4:<br>\n<code>bazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\ --input=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb \\ --output=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\ --frozen_graph=True \\ --input_names=Preprocessor/sub \\ --output_names=concat,concat_1 \\ --alsologtostderr</code></p>\n<p>step 5: Converting in tflite format<br>\n<code>bazel run -c opt //tensorflow/contrib/lite/toco:toco -- \\ --input_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\ --output_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/detect.tflite \\ --input_shapes=1,300,300,3 \\ --input_arrays=Preprocessor/sub \\ --output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr</code></p>\n<p>Let me know if you need any further information.</p>\n<p>Thank you,</p>", "body_text": "Hi @achowdhery ,\nThank you for your response. I had not tried with ssd_mobilenet_v1_coco. But I had used ssd_mobilnet_v2_coco and it worked perfect on my device. Every apps are running perfectly fine for this case. However, when I am  trying to port the custom model using tflite; installed app is getting closed repeatedly.\nI was trying again. While converting from .pb file to tflite format, I used inference_type=FLOAT and made a change in the DetectorActivity.java file for private static final boolean TF_OD_API_IS_QUANTIZED = false;. After this I rebuild the apk and installed on my device. The app is installed, but after sometime (10 sec) it is getting closed again  with a message \"TFLite Demo has stopped\" close the app. Since, I have not changed anything for the Classifier and speech files, both are working fine. Only concern is with the Detection.\nPlease find the detailed steps below from converting frozen graph into the tflite format. (I retrained my model with my data set and have frozen_graph.pb files and labelmap.pbtxt). It may be noted here that I have not downloaded any tflite file from any source.\nStep 1:\nDETECT_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb\nSTRIPPED_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb\nstep 2:\ncd $HOME/tensorflow-master\ntouch WORKSPACE\nstep 3 :\nbazel build tensorflow/python/tools:optimize_for_inference && \\ bazel-bin/tensorflow/python/tools/optimize_for_inference \\ --input=$DETECT_PB \\ --output=$STRIPPED_PB \\ --frozen_graph=True \\ --input_names=Preprocessor/sub \\ --output_names=concat,concat_1 \\ --alsologtostderr\nstep 4:\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\ --input=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb \\ --output=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\ --frozen_graph=True \\ --input_names=Preprocessor/sub \\ --output_names=concat,concat_1 \\ --alsologtostderr\nstep 5: Converting in tflite format\nbazel run -c opt //tensorflow/contrib/lite/toco:toco -- \\ --input_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\ --output_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/detect.tflite \\ --input_shapes=1,300,300,3 \\ --input_arrays=Preprocessor/sub \\ --output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\nLet me know if you need any further information.\nThank you,", "body": "Hi @achowdhery ,\r\nThank you for your response. I had not tried with ssd_mobilenet_v1_coco. But I had used ssd_mobilnet_v2_coco and it worked perfect on my device. Every apps are running perfectly fine for this case. However, when I am  trying to port the custom model using tflite; installed app is getting closed repeatedly. \r\n\r\nI was trying again. While converting from .pb file to tflite format, I used inference_type=FLOAT and made a change in the DetectorActivity.java file for `private static final boolean TF_OD_API_IS_QUANTIZED = false;`. After this I rebuild the apk and installed on my device. The app is installed, but after sometime (10 sec) it is getting closed again  with a message \"TFLite Demo has stopped\" close the app. Since, I have not changed anything for the Classifier and speech files, both are working fine. Only concern is with the Detection.\r\n\r\nPlease find the detailed steps below from converting frozen graph into the tflite format. (I retrained my model with my data set and have frozen_graph.pb files and labelmap.pbtxt). It may be noted here that I have not downloaded any tflite file from any source. \r\n\r\nStep 1:\r\n`DETECT_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb`\r\n`STRIPPED_PB=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb`\r\n\r\nstep 2:\r\n`cd $HOME/tensorflow-master`\r\n`touch WORKSPACE`\r\n\r\nstep 3 :\r\n`bazel build tensorflow/python/tools:optimize_for_inference && \\\r\nbazel-bin/tensorflow/python/tools/optimize_for_inference \\\r\n--input=$DETECT_PB \\\r\n--output=$STRIPPED_PB \\\r\n--frozen_graph=True \\\r\n--input_names=Preprocessor/sub \\\r\n--output_names=concat,concat_1 \\\r\n--alsologtostderr`\r\n\r\nstep 4:\r\n`bazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph.pb \\\r\n--output=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\\r\n--frozen_graph=True \\\r\n--input_names=Preprocessor/sub \\\r\n--output_names=concat,concat_1 \\\r\n--alsologtostderr`\r\n\r\nstep 5: Converting in tflite format\r\n`bazel run -c opt //tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/frozen_inference_graph_stripped.pb \\\r\n--output_file=$HOME/tensorflow-master/models/research/object_detection/customized_model/inference_graph/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=Preprocessor/sub \\\r\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr`\r\n\r\nLet me know if you need any further information.\r\n\r\nThank you,\r\n\r\n\r\n"}