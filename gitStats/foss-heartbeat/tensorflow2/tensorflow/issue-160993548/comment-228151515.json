{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/228151515", "html_url": "https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-228151515", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2940", "id": 228151515, "node_id": "MDEyOklzc3VlQ29tbWVudDIyODE1MTUxNQ==", "user": {"login": "peterswang", "id": 10738534, "node_id": "MDQ6VXNlcjEwNzM4NTM0", "avatar_url": "https://avatars3.githubusercontent.com/u/10738534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterswang", "html_url": "https://github.com/peterswang", "followers_url": "https://api.github.com/users/peterswang/followers", "following_url": "https://api.github.com/users/peterswang/following{/other_user}", "gists_url": "https://api.github.com/users/peterswang/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterswang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterswang/subscriptions", "organizations_url": "https://api.github.com/users/peterswang/orgs", "repos_url": "https://api.github.com/users/peterswang/repos", "events_url": "https://api.github.com/users/peterswang/events{/privacy}", "received_events_url": "https://api.github.com/users/peterswang/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-23T19:03:04Z", "updated_at": "2016-06-23T19:03:04Z", "author_association": "NONE", "body_html": "<p>I ran into a similar issue with Python 2.7.11 on OS X 10.11.5 + MBP.</p>\n<p>$ ./configure<br>\nPlease specify the location of python. [Default is /usr/local/bin/python]:<br>\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N<br>\nNo Google Cloud Platform support will be enabled for TensorFlow<br>\nDo you wish to build TensorFlow with GPU support? [y/N] y<br>\nGPU support will be enabled for TensorFlow<br>\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]:<br>\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5<br>\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:<br>\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5<br>\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:<br>\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.<br>\nYou can find the compute capability of your device at: <a href=\"https://developer.nvidia.com/cuda-gpus\" rel=\"nofollow\">https://developer.nvidia.com/cuda-gpus</a>.<br>\nPlease note that each additional compute capability significantly increases your build time and binary size.</p>\n<p>Setting up Cuda include<br>\nSetting up Cuda lib<br>\nSetting up Cuda bin<br>\nSetting up Cuda nvvm<br>\nSetting up CUPTI include<br>\nSetting up CUPTI lib64<br>\nConfiguration finished<br>\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package<br>\n.......<br>\n.......<br>\n1 warning generated.<br>\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so [for host]:<br>\nclang: warning: argument unused during compilation: '-pthread'<br>\nld: warning: option -noall_load is obsolete and being ignored<br>\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so:<br>\nclang: warning: argument unused during compilation: '-pthread'<br>\nld: warning: option -noall_load is obsolete and being ignored<br>\nERROR: /Users/peter_wang/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 245.<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally<br>\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build<br>\nUse --verbose_failures to see the command lines of failed build steps.<br>\nINFO: Elapsed time: 1826.337s, Critical Path: 1821.08s</p>\n<p>To verify CUDA install:<br>\n$ ./bin/x86_64/darwin/release/deviceQuery<br>\n./bin/x86_64/darwin/release/deviceQuery Starting...</p>\n<p>CUDA Device Query (Runtime API) version (CUDART static linking)</p>\n<p>Detected 1 CUDA Capable device(s)</p>\n<p>Device 0: \"GeForce GT 650M\"<br>\nCUDA Driver Version / Runtime Version          7.5 / 7.5<br>\nCUDA Capability Major/Minor version number:    3.0<br>\nTotal amount of global memory:                 512 MBytes (536543232 bytes)<br>\n( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores<br>\nGPU Max Clock rate:                            405 MHz (0.41 GHz)<br>\nMemory Clock rate:                             2000 Mhz<br>\nMemory Bus Width:                              128-bit<br>\nL2 Cache Size:                                 262144 bytes<br>\nMaximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)<br>\nMaximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers<br>\nMaximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers<br>\nTotal amount of constant memory:               65536 bytes<br>\nTotal amount of shared memory per block:       49152 bytes<br>\nTotal number of registers available per block: 65536<br>\nWarp size:                                     32<br>\nMaximum number of threads per multiprocessor:  2048<br>\nMaximum number of threads per block:           1024<br>\nMax dimension size of a thread block (x,y,z): (1024, 1024, 64)<br>\nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)<br>\nMaximum memory pitch:                          2147483647 bytes<br>\nTexture alignment:                             512 bytes<br>\nConcurrent copy and kernel execution:          Yes with 1 copy engine(s)<br>\nRun time limit on kernels:                     Yes<br>\nIntegrated GPU sharing Host Memory:            No<br>\nSupport host page-locked memory mapping:       Yes<br>\nAlignment requirement for Surfaces:            Yes<br>\nDevice has ECC support:                        Disabled<br>\nDevice supports Unified Addressing (UVA):      Yes<br>\nDevice PCI Domain ID / Bus ID / location ID:   0 / 1 / 0<br>\nCompute Mode:<br>\n&lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</p>\n<p>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime Version = 7.5, NumDevs = 1, Device0 = GeForce GT 650M<br>\nResult = PASS</p>", "body_text": "I ran into a similar issue with Python 2.7.11 on OS X 10.11.5 + MBP.\n$ ./configure\nPlease specify the location of python. [Default is /usr/local/bin/python]:\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]:\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\nSetting up Cuda include\nSetting up Cuda lib\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n.......\n.......\n1 warning generated.\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so:\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nERROR: /Users/peter_wang/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 245.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1826.337s, Critical Path: 1821.08s\nTo verify CUDA install:\n$ ./bin/x86_64/darwin/release/deviceQuery\n./bin/x86_64/darwin/release/deviceQuery Starting...\nCUDA Device Query (Runtime API) version (CUDART static linking)\nDetected 1 CUDA Capable device(s)\nDevice 0: \"GeForce GT 650M\"\nCUDA Driver Version / Runtime Version          7.5 / 7.5\nCUDA Capability Major/Minor version number:    3.0\nTotal amount of global memory:                 512 MBytes (536543232 bytes)\n( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\nGPU Max Clock rate:                            405 MHz (0.41 GHz)\nMemory Clock rate:                             2000 Mhz\nMemory Bus Width:                              128-bit\nL2 Cache Size:                                 262144 bytes\nMaximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\nMaximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\nMaximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\nTotal amount of constant memory:               65536 bytes\nTotal amount of shared memory per block:       49152 bytes\nTotal number of registers available per block: 65536\nWarp size:                                     32\nMaximum number of threads per multiprocessor:  2048\nMaximum number of threads per block:           1024\nMax dimension size of a thread block (x,y,z): (1024, 1024, 64)\nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\nMaximum memory pitch:                          2147483647 bytes\nTexture alignment:                             512 bytes\nConcurrent copy and kernel execution:          Yes with 1 copy engine(s)\nRun time limit on kernels:                     Yes\nIntegrated GPU sharing Host Memory:            No\nSupport host page-locked memory mapping:       Yes\nAlignment requirement for Surfaces:            Yes\nDevice has ECC support:                        Disabled\nDevice supports Unified Addressing (UVA):      Yes\nDevice PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\nCompute Mode:\n< Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime Version = 7.5, NumDevs = 1, Device0 = GeForce GT 650M\nResult = PASS", "body": "I ran into a similar issue with Python 2.7.11 on OS X 10.11.5 + MBP.\n\n$ ./configure\nPlease specify the location of python. [Default is /usr/local/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\nSetting up Cuda include\nSetting up Cuda lib\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n.......\n.......\n1 warning generated.\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so:\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nERROR: /Users/peter_wang/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 245.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1826.337s, Critical Path: 1821.08s\n\nTo verify CUDA install:\n$ ./bin/x86_64/darwin/release/deviceQuery\n./bin/x86_64/darwin/release/deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"GeForce GT 650M\"\n  CUDA Driver Version / Runtime Version          7.5 / 7.5\n  CUDA Capability Major/Minor version number:    3.0\n  Total amount of global memory:                 512 MBytes (536543232 bytes)\n  ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n  GPU Max Clock rate:                            405 MHz (0.41 GHz)\n  Memory Clock rate:                             2000 Mhz\n  Memory Bus Width:                              128-bit\n  L2 Cache Size:                                 262144 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime Version = 7.5, NumDevs = 1, Device0 = GeForce GT 650M\nResult = PASS\n"}