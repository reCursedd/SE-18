{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/392863624", "html_url": "https://github.com/tensorflow/tensorflow/issues/18222#issuecomment-392863624", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18222", "id": 392863624, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Mjg2MzYyNA==", "user": {"login": "isaprykin", "id": 234070, "node_id": "MDQ6VXNlcjIzNDA3MA==", "avatar_url": "https://avatars1.githubusercontent.com/u/234070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isaprykin", "html_url": "https://github.com/isaprykin", "followers_url": "https://api.github.com/users/isaprykin/followers", "following_url": "https://api.github.com/users/isaprykin/following{/other_user}", "gists_url": "https://api.github.com/users/isaprykin/gists{/gist_id}", "starred_url": "https://api.github.com/users/isaprykin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isaprykin/subscriptions", "organizations_url": "https://api.github.com/users/isaprykin/orgs", "repos_url": "https://api.github.com/users/isaprykin/repos", "events_url": "https://api.github.com/users/isaprykin/events{/privacy}", "received_events_url": "https://api.github.com/users/isaprykin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-29T17:21:54Z", "updated_at": "2018-05-29T17:21:54Z", "author_association": "MEMBER", "body_html": "<p>Yuefeng, Priya,</p>\n<p>during the dev summit I mentioned that we actually could synchronize the batchnorm statistics across GPUs at every step instead of at the checkpointing time.  I said that because it's my memory that that's how we initially implemented it.  Even though it's slower, there are use cases, that if I recall correctly are related to small batch size, when synchronizing statistics at every step makes sense.</p>\n<p>The users here desire an option to enable such a behavior.</p>", "body_text": "Yuefeng, Priya,\nduring the dev summit I mentioned that we actually could synchronize the batchnorm statistics across GPUs at every step instead of at the checkpointing time.  I said that because it's my memory that that's how we initially implemented it.  Even though it's slower, there are use cases, that if I recall correctly are related to small batch size, when synchronizing statistics at every step makes sense.\nThe users here desire an option to enable such a behavior.", "body": "Yuefeng, Priya,\r\n\r\nduring the dev summit I mentioned that we actually could synchronize the batchnorm statistics across GPUs at every step instead of at the checkpointing time.  I said that because it's my memory that that's how we initially implemented it.  Even though it's slower, there are use cases, that if I recall correctly are related to small batch size, when synchronizing statistics at every step makes sense.\r\n\r\nThe users here desire an option to enable such a behavior."}