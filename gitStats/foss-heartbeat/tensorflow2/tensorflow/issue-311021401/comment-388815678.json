{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/388815678", "html_url": "https://github.com/tensorflow/tensorflow/issues/18222#issuecomment-388815678", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18222", "id": 388815678, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODgxNTY3OA==", "user": {"login": "holyseven", "id": 13829174, "node_id": "MDQ6VXNlcjEzODI5MTc0", "avatar_url": "https://avatars3.githubusercontent.com/u/13829174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/holyseven", "html_url": "https://github.com/holyseven", "followers_url": "https://api.github.com/users/holyseven/followers", "following_url": "https://api.github.com/users/holyseven/following{/other_user}", "gists_url": "https://api.github.com/users/holyseven/gists{/gist_id}", "starred_url": "https://api.github.com/users/holyseven/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/holyseven/subscriptions", "organizations_url": "https://api.github.com/users/holyseven/orgs", "repos_url": "https://api.github.com/users/holyseven/repos", "events_url": "https://api.github.com/users/holyseven/events{/privacy}", "received_events_url": "https://api.github.com/users/holyseven/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-14T13:26:40Z", "updated_at": "2018-05-14T13:26:40Z", "author_association": "NONE", "body_html": "<p>Not sure if this is helpful to this issue, but I have figured out a trick of collecting statistics across GPUs and implemented in pure Python, <a href=\"https://github.com/holyseven/PSPNet-TF-Reproduce/blob/master/model/utils_mg.py#L183\">the complete code is here</a>.</p>\n<p>In contrary to the default configuration when using multiple GPUs like this:</p>\n<pre><code>    with tf.variable_scope(tf.get_variable_scope()):\n      for i in xrange(FLAGS.num_gpus):\n        with tf.device('/gpu:%d' % i):\n          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\n            # whole model definition here, from data preparation to loss\n</code></pre>\n<p>my idea is to use a list (of tensors) as input and output of each function, and do the across-GPUs operations  <strong>in each function</strong> with specifying the GPU device for each element/tensor, like this:</p>\n<pre><code>def relu(list_input):\n    assert type(list_input) == list\n    list_output = []\n    for i in range(len(list_input)):\n        with tf.device('/gpu:%d' % i):\n            output = tf.nn.relu(list_input[i], name='relu')\n            list_output.append(output)\n    return list_output\n</code></pre>\n<p>For most layers, there is no difference between them. But for batch normalization layers, using lists makes collecting statistics possible:</p>\n<pre><code>    means = []\n    square_means = []\n    for i in range(len(list_input)):\n        with tf.device('/gpu:%d' % i):\n            batch_mean = tf.reduce_mean(list_input[i], [0, 1, 2])\n            batch_square_mean = tf.reduce_mean(tf.square(list_input[i]), [0, 1, 2])\n            means.append(batch_mean)\n            square_means.append(batch_square_mean)\n\n    with tf.device('/cpu:0'):  # or gpu:0 if nvlink\n        shape = tf.shape(list_input[0])\n        num = shape[0] * shape[1] * shape[2] * len(list_input)\n        mean = tf.reduce_mean(means, axis=0)\n        var = tf.reduce_mean(square_means, axis=0) - tf.square(mean)\n        var *= tf.cast(num, float_type) / tf.cast(num-1, float_type)  \n</code></pre>\n<p>This implementation helps me to use large batch size for image segmentation, but somehow, breaks the Tensorflow style of multi-GPU usage (or creates a new style?).</p>", "body_text": "Not sure if this is helpful to this issue, but I have figured out a trick of collecting statistics across GPUs and implemented in pure Python, the complete code is here.\nIn contrary to the default configuration when using multiple GPUs like this:\n    with tf.variable_scope(tf.get_variable_scope()):\n      for i in xrange(FLAGS.num_gpus):\n        with tf.device('/gpu:%d' % i):\n          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\n            # whole model definition here, from data preparation to loss\n\nmy idea is to use a list (of tensors) as input and output of each function, and do the across-GPUs operations  in each function with specifying the GPU device for each element/tensor, like this:\ndef relu(list_input):\n    assert type(list_input) == list\n    list_output = []\n    for i in range(len(list_input)):\n        with tf.device('/gpu:%d' % i):\n            output = tf.nn.relu(list_input[i], name='relu')\n            list_output.append(output)\n    return list_output\n\nFor most layers, there is no difference between them. But for batch normalization layers, using lists makes collecting statistics possible:\n    means = []\n    square_means = []\n    for i in range(len(list_input)):\n        with tf.device('/gpu:%d' % i):\n            batch_mean = tf.reduce_mean(list_input[i], [0, 1, 2])\n            batch_square_mean = tf.reduce_mean(tf.square(list_input[i]), [0, 1, 2])\n            means.append(batch_mean)\n            square_means.append(batch_square_mean)\n\n    with tf.device('/cpu:0'):  # or gpu:0 if nvlink\n        shape = tf.shape(list_input[0])\n        num = shape[0] * shape[1] * shape[2] * len(list_input)\n        mean = tf.reduce_mean(means, axis=0)\n        var = tf.reduce_mean(square_means, axis=0) - tf.square(mean)\n        var *= tf.cast(num, float_type) / tf.cast(num-1, float_type)  \n\nThis implementation helps me to use large batch size for image segmentation, but somehow, breaks the Tensorflow style of multi-GPU usage (or creates a new style?).", "body": "Not sure if this is helpful to this issue, but I have figured out a trick of collecting statistics across GPUs and implemented in pure Python, [the complete code is here](https://github.com/holyseven/PSPNet-TF-Reproduce/blob/master/model/utils_mg.py#L183).\r\n\r\nIn contrary to the default configuration when using multiple GPUs like this:\r\n```\r\n    with tf.variable_scope(tf.get_variable_scope()):\r\n      for i in xrange(FLAGS.num_gpus):\r\n        with tf.device('/gpu:%d' % i):\r\n          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\r\n            # whole model definition here, from data preparation to loss\r\n```\r\nmy idea is to use a list (of tensors) as input and output of each function, and do the across-GPUs operations  **in each function** with specifying the GPU device for each element/tensor, like this:\r\n```\r\ndef relu(list_input):\r\n    assert type(list_input) == list\r\n    list_output = []\r\n    for i in range(len(list_input)):\r\n        with tf.device('/gpu:%d' % i):\r\n            output = tf.nn.relu(list_input[i], name='relu')\r\n            list_output.append(output)\r\n    return list_output\r\n```\r\nFor most layers, there is no difference between them. But for batch normalization layers, using lists makes collecting statistics possible:\r\n```\r\n    means = []\r\n    square_means = []\r\n    for i in range(len(list_input)):\r\n        with tf.device('/gpu:%d' % i):\r\n            batch_mean = tf.reduce_mean(list_input[i], [0, 1, 2])\r\n            batch_square_mean = tf.reduce_mean(tf.square(list_input[i]), [0, 1, 2])\r\n            means.append(batch_mean)\r\n            square_means.append(batch_square_mean)\r\n\r\n    with tf.device('/cpu:0'):  # or gpu:0 if nvlink\r\n        shape = tf.shape(list_input[0])\r\n        num = shape[0] * shape[1] * shape[2] * len(list_input)\r\n        mean = tf.reduce_mean(means, axis=0)\r\n        var = tf.reduce_mean(square_means, axis=0) - tf.square(mean)\r\n        var *= tf.cast(num, float_type) / tf.cast(num-1, float_type)  \r\n```\r\n\r\nThis implementation helps me to use large batch size for image segmentation, but somehow, breaks the Tensorflow style of multi-GPU usage (or creates a new style?). \r\n\r\n"}