{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366055953", "html_url": "https://github.com/tensorflow/tensorflow/issues/17050#issuecomment-366055953", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17050", "id": 366055953, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjA1NTk1Mw==", "user": {"login": "illeatmyhat", "id": 5661986, "node_id": "MDQ6VXNlcjU2NjE5ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/5661986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/illeatmyhat", "html_url": "https://github.com/illeatmyhat", "followers_url": "https://api.github.com/users/illeatmyhat/followers", "following_url": "https://api.github.com/users/illeatmyhat/following{/other_user}", "gists_url": "https://api.github.com/users/illeatmyhat/gists{/gist_id}", "starred_url": "https://api.github.com/users/illeatmyhat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/illeatmyhat/subscriptions", "organizations_url": "https://api.github.com/users/illeatmyhat/orgs", "repos_url": "https://api.github.com/users/illeatmyhat/repos", "events_url": "https://api.github.com/users/illeatmyhat/events{/privacy}", "received_events_url": "https://api.github.com/users/illeatmyhat/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-15T20:45:11Z", "updated_at": "2018-02-15T20:47:01Z", "author_association": "NONE", "body_html": "<p>Whew. How silly.<br>\nThe problem is that the queues are all assigned to different devices--one on <code>/job:ps</code> and the others on their individual <code>/job:worker</code> tasks.<br>\nI've produced a minimal working example:</p>\n<pre><code>import tensorflow as tf\nimport threading\n\ndef main(job_name, task):\n    cluster = tf.train.ClusterSpec({\n        'ps': ['localhost:22222', 'localhost:22223'],\n        'worker': ['localhost: 22224','localhost: 22225','localhost: 22226']\n    })\n\n    # Create and start a server for the local task\n    server = tf.train.Server(cluster, job_name=job_name, task_index=task)\n\n    if job_name == 'ps':\n        with tf.device('/job:ps/task:%d' % task):\n            queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % task)\n        with tf.Session(server.target) as sess:\n            for i in range(cluster.num_tasks('worker')):\n                sess.run(queue.dequeue())\n                print('ps:%d received done from worker:%d' % (task, i))\n            print('ps:%d quitting' % task)\n    elif job_name == 'worker':\n        # queue needs to be visible to /job:ps\n        queues = []\n        for i in range(cluster.num_tasks('ps')):\n            with tf.device('/job:ps/task:%d' % i):\n                queues.append(tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % i))\n        with tf.Session(server.target) as sess:\n            for i in range(cluster.num_tasks('ps')):\n                _, size = sess.run([queues[i].enqueue(task), queues[i].size()])\n                print('Worker:%d sending done to ps:%d [elements=%d]' % (task, i, size))\n\nif __name__ == '__main__':\n    threads = [\n        threading.Thread(target=main, args=('ps', 0)),\n        threading.Thread(target=main, args=('ps', 1)),\n        threading.Thread(target=main, args=('worker', 0)),\n        threading.Thread(target=main, args=('worker', 1)),\n        threading.Thread(target=main, args=('worker', 2))]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n</code></pre>\n<p>It's simple to change to work with MonitoredTrainingSession using FinalOpsHook, although it certainly isn't the prettiest way to go about doing things.</p>", "body_text": "Whew. How silly.\nThe problem is that the queues are all assigned to different devices--one on /job:ps and the others on their individual /job:worker tasks.\nI've produced a minimal working example:\nimport tensorflow as tf\nimport threading\n\ndef main(job_name, task):\n    cluster = tf.train.ClusterSpec({\n        'ps': ['localhost:22222', 'localhost:22223'],\n        'worker': ['localhost: 22224','localhost: 22225','localhost: 22226']\n    })\n\n    # Create and start a server for the local task\n    server = tf.train.Server(cluster, job_name=job_name, task_index=task)\n\n    if job_name == 'ps':\n        with tf.device('/job:ps/task:%d' % task):\n            queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % task)\n        with tf.Session(server.target) as sess:\n            for i in range(cluster.num_tasks('worker')):\n                sess.run(queue.dequeue())\n                print('ps:%d received done from worker:%d' % (task, i))\n            print('ps:%d quitting' % task)\n    elif job_name == 'worker':\n        # queue needs to be visible to /job:ps\n        queues = []\n        for i in range(cluster.num_tasks('ps')):\n            with tf.device('/job:ps/task:%d' % i):\n                queues.append(tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % i))\n        with tf.Session(server.target) as sess:\n            for i in range(cluster.num_tasks('ps')):\n                _, size = sess.run([queues[i].enqueue(task), queues[i].size()])\n                print('Worker:%d sending done to ps:%d [elements=%d]' % (task, i, size))\n\nif __name__ == '__main__':\n    threads = [\n        threading.Thread(target=main, args=('ps', 0)),\n        threading.Thread(target=main, args=('ps', 1)),\n        threading.Thread(target=main, args=('worker', 0)),\n        threading.Thread(target=main, args=('worker', 1)),\n        threading.Thread(target=main, args=('worker', 2))]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n\nIt's simple to change to work with MonitoredTrainingSession using FinalOpsHook, although it certainly isn't the prettiest way to go about doing things.", "body": "Whew. How silly.\r\nThe problem is that the queues are all assigned to different devices--one on `/job:ps` and the others on their individual `/job:worker` tasks.\r\nI've produced a minimal working example:\r\n```\r\nimport tensorflow as tf\r\nimport threading\r\n\r\ndef main(job_name, task):\r\n    cluster = tf.train.ClusterSpec({\r\n        'ps': ['localhost:22222', 'localhost:22223'],\r\n        'worker': ['localhost: 22224','localhost: 22225','localhost: 22226']\r\n    })\r\n\r\n    # Create and start a server for the local task\r\n    server = tf.train.Server(cluster, job_name=job_name, task_index=task)\r\n\r\n    if job_name == 'ps':\r\n        with tf.device('/job:ps/task:%d' % task):\r\n            queue = tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % task)\r\n        with tf.Session(server.target) as sess:\r\n            for i in range(cluster.num_tasks('worker')):\r\n                sess.run(queue.dequeue())\r\n                print('ps:%d received done from worker:%d' % (task, i))\r\n            print('ps:%d quitting' % task)\r\n    elif job_name == 'worker':\r\n        # queue needs to be visible to /job:ps\r\n        queues = []\r\n        for i in range(cluster.num_tasks('ps')):\r\n            with tf.device('/job:ps/task:%d' % i):\r\n                queues.append(tf.FIFOQueue(cluster.num_tasks('worker'), tf.int32, shared_name='done_queue%d' % i))\r\n        with tf.Session(server.target) as sess:\r\n            for i in range(cluster.num_tasks('ps')):\r\n                _, size = sess.run([queues[i].enqueue(task), queues[i].size()])\r\n                print('Worker:%d sending done to ps:%d [elements=%d]' % (task, i, size))\r\n\r\nif __name__ == '__main__':\r\n    threads = [\r\n        threading.Thread(target=main, args=('ps', 0)),\r\n        threading.Thread(target=main, args=('ps', 1)),\r\n        threading.Thread(target=main, args=('worker', 0)),\r\n        threading.Thread(target=main, args=('worker', 1)),\r\n        threading.Thread(target=main, args=('worker', 2))]\r\n    for thread in threads:\r\n        thread.start()\r\n    for thread in threads:\r\n        thread.join()\r\n```\r\nIt's simple to change to work with MonitoredTrainingSession using FinalOpsHook, although it certainly isn't the prettiest way to go about doing things."}