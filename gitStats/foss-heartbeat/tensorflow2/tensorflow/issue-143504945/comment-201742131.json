{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/201742131", "html_url": "https://github.com/tensorflow/tensorflow/pull/1641#issuecomment-201742131", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1641", "id": 201742131, "node_id": "MDEyOklzc3VlQ29tbWVudDIwMTc0MjEzMQ==", "user": {"login": "amchercashin", "id": 8727497, "node_id": "MDQ6VXNlcjg3Mjc0OTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8727497?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amchercashin", "html_url": "https://github.com/amchercashin", "followers_url": "https://api.github.com/users/amchercashin/followers", "following_url": "https://api.github.com/users/amchercashin/following{/other_user}", "gists_url": "https://api.github.com/users/amchercashin/gists{/gist_id}", "starred_url": "https://api.github.com/users/amchercashin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amchercashin/subscriptions", "organizations_url": "https://api.github.com/users/amchercashin/orgs", "repos_url": "https://api.github.com/users/amchercashin/repos", "events_url": "https://api.github.com/users/amchercashin/events{/privacy}", "received_events_url": "https://api.github.com/users/amchercashin/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-26T09:10:32Z", "updated_at": "2016-03-26T09:10:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a>: I'm sorry, not a scalar, but a vector of size of number of neurons (without bias unit), and in the case of one neuron - a scalar. It was says that partial derivatives of loss function are <em>SUM( y_hat - y ) * x_i</em> . And then I look at loss function and see only one <em>x</em>: 1/2_SUM( w2_x + w1 - y)^2. So then the <em>SUM( y_hat - y ) * x_i</em> becomes <em>SUM( y_hat - y ) * x</em> which looks like one number for me. Then I look at the code and see that the result is two numbers as it should be. But there is no partial derivative with respect to w1 in description</p>\n<p>I understand that maybe it is convenient to just omit this dedicated equation for the bias unit thinking of it like it is an <em>x_whichisalwaysone</em> and thus <em>SUM( y_hat - y ) * x_i</em> will return two values here (with bias). But for a newbie like me it was a little confusing at start so I humbly propose more detailed description. If you found it bad, pls just discard it.</p>", "body_text": "@girving: I'm sorry, not a scalar, but a vector of size of number of neurons (without bias unit), and in the case of one neuron - a scalar. It was says that partial derivatives of loss function are SUM( y_hat - y ) * x_i . And then I look at loss function and see only one x: 1/2_SUM( w2_x + w1 - y)^2. So then the SUM( y_hat - y ) * x_i becomes SUM( y_hat - y ) * x which looks like one number for me. Then I look at the code and see that the result is two numbers as it should be. But there is no partial derivative with respect to w1 in description\nI understand that maybe it is convenient to just omit this dedicated equation for the bias unit thinking of it like it is an x_whichisalwaysone and thus SUM( y_hat - y ) * x_i will return two values here (with bias). But for a newbie like me it was a little confusing at start so I humbly propose more detailed description. If you found it bad, pls just discard it.", "body": "@girving: I'm sorry, not a scalar, but a vector of size of number of neurons (without bias unit), and in the case of one neuron - a scalar. It was says that partial derivatives of loss function are _SUM( y_hat - y ) \\* x_i_ . And then I look at loss function and see only one _x_: 1/2_SUM( w2_x + w1 - y)^2. So then the _SUM( y_hat - y ) \\* x_i_ becomes _SUM( y_hat - y ) \\* x_ which looks like one number for me. Then I look at the code and see that the result is two numbers as it should be. But there is no partial derivative with respect to w1 in description\n\nI understand that maybe it is convenient to just omit this dedicated equation for the bias unit thinking of it like it is an _x_whichisalwaysone_ and thus _SUM( y_hat - y ) \\* x_i_ will return two values here (with bias). But for a newbie like me it was a little confusing at start so I humbly propose more detailed description. If you found it bad, pls just discard it.\n"}