{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17713", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17713/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17713/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17713/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17713", "id": 305245750, "node_id": "MDU6SXNzdWUzMDUyNDU3NTA=", "number": 17713, "title": "Manual Placement of Graph Nodes of High-level Optimizers and Loss functions", "user": {"login": "xilenteyex", "id": 10864603, "node_id": "MDQ6VXNlcjEwODY0NjAz", "avatar_url": "https://avatars1.githubusercontent.com/u/10864603?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xilenteyex", "html_url": "https://github.com/xilenteyex", "followers_url": "https://api.github.com/users/xilenteyex/followers", "following_url": "https://api.github.com/users/xilenteyex/following{/other_user}", "gists_url": "https://api.github.com/users/xilenteyex/gists{/gist_id}", "starred_url": "https://api.github.com/users/xilenteyex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xilenteyex/subscriptions", "organizations_url": "https://api.github.com/users/xilenteyex/orgs", "repos_url": "https://api.github.com/users/xilenteyex/repos", "events_url": "https://api.github.com/users/xilenteyex/events{/privacy}", "received_events_url": "https://api.github.com/users/xilenteyex/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-03-14T16:58:00Z", "updated_at": "2018-04-30T01:02:28Z", "closed_at": "2018-04-30T01:02:28Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6.0-rc1</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.11.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.1/7.0</li>\n<li><strong>GPU model and memory</strong>: Tesla k80 (11441MiB)</li>\n<li><strong>Exact command to reproduce</strong>: python cifar10_train.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I want to use my own device placement algorithm to be used to place nodes across multiple devices my machine has. I looked into this : <a href=\"https://www.tensorflow.org/programmers_guide/using_gpu#manual_device_placement\" rel=\"nofollow\">https://www.tensorflow.org/programmers_guide/using_gpu#manual_device_placement</a> which suggests that if I figure out exact placement statically before executing the graph, I can use \"with device\" annotations to partition my graph across devices. But, if I am using some high-level optimizers and loss functions. Each of these operations are going to have a multi-node graph as well. Suppose I am running this example : <a href=\"https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a> and want to specify where each node of the complete tensor flow graph should go given a list of compute resources (cpu cores, gpu, tpu etc.) my machine has.</p>\n<p>Is there any api or framework that allows you to do fine grained placement of graph nodes across multiple compute resources ? What's the best way to achieve this ?</p>\n<p>Thanks a lot for any help!</p>\n<h3>Source code / logs</h3>\n<p>cifar10 example : <a href=\"https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.6.0-rc1\nPython version: 2.7\nBazel version (if compiling from source): 0.11.0\nGCC/Compiler version (if compiling from source): 5.4\nCUDA/cuDNN version: 9.1/7.0\nGPU model and memory: Tesla k80 (11441MiB)\nExact command to reproduce: python cifar10_train.py\n\nDescribe the problem\nI want to use my own device placement algorithm to be used to place nodes across multiple devices my machine has. I looked into this : https://www.tensorflow.org/programmers_guide/using_gpu#manual_device_placement which suggests that if I figure out exact placement statically before executing the graph, I can use \"with device\" annotations to partition my graph across devices. But, if I am using some high-level optimizers and loss functions. Each of these operations are going to have a multi-node graph as well. Suppose I am running this example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10 and want to specify where each node of the complete tensor flow graph should go given a list of compute resources (cpu cores, gpu, tpu etc.) my machine has.\nIs there any api or framework that allows you to do fine grained placement of graph nodes across multiple compute resources ? What's the best way to achieve this ?\nThanks a lot for any help!\nSource code / logs\ncifar10 example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.1/7.0\r\n- **GPU model and memory**: Tesla k80 (11441MiB)\r\n- **Exact command to reproduce**: python cifar10_train.py\r\n\r\n### Describe the problem\r\nI want to use my own device placement algorithm to be used to place nodes across multiple devices my machine has. I looked into this : https://www.tensorflow.org/programmers_guide/using_gpu#manual_device_placement which suggests that if I figure out exact placement statically before executing the graph, I can use \"with device\" annotations to partition my graph across devices. But, if I am using some high-level optimizers and loss functions. Each of these operations are going to have a multi-node graph as well. Suppose I am running this example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10 and want to specify where each node of the complete tensor flow graph should go given a list of compute resources (cpu cores, gpu, tpu etc.) my machine has.  \r\n\r\nIs there any api or framework that allows you to do fine grained placement of graph nodes across multiple compute resources ? What's the best way to achieve this ?  \r\n\r\nThanks a lot for any help!\r\n\r\n\r\n### Source code / logs\r\ncifar10 example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10"}