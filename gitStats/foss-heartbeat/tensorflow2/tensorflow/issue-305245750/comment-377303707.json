{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377303707", "html_url": "https://github.com/tensorflow/tensorflow/issues/17713#issuecomment-377303707", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17713", "id": 377303707, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzMwMzcwNw==", "user": {"login": "Benyuel", "id": 5361725, "node_id": "MDQ6VXNlcjUzNjE3MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5361725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Benyuel", "html_url": "https://github.com/Benyuel", "followers_url": "https://api.github.com/users/Benyuel/followers", "following_url": "https://api.github.com/users/Benyuel/following{/other_user}", "gists_url": "https://api.github.com/users/Benyuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Benyuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Benyuel/subscriptions", "organizations_url": "https://api.github.com/users/Benyuel/orgs", "repos_url": "https://api.github.com/users/Benyuel/repos", "events_url": "https://api.github.com/users/Benyuel/events{/privacy}", "received_events_url": "https://api.github.com/users/Benyuel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-29T17:04:25Z", "updated_at": "2018-03-29T17:04:25Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10864603\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xilenteyex\">@xilenteyex</a>: I think this is a basic example of what you're trying to do:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import meta_graph\nfrom tensorflow.python.framework import ops as tf_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope\n\ndef _buildMnist(batch_size=128,\n\t\tinput_size=256,\n\t\tnum_classes=1024,\n\t\tnum_layers=10,\n\t\thidden_size=256,\n\t\tname='mnist'):\n\tg = tf_ops.get_default_graph()\n\twith g.as_default():\n\t  ops = {}\n\t  x = random_ops.random_uniform(\n\t\t  [batch_size, input_size], -0.1, 0.1, dtype=dtypes.float32)\n\t  for layer_id in range(num_layers):\n\t\twith variable_scope.variable_scope('layer_{}'.format(layer_id)):\n\t\t  a = input_size if layer_id == 0 else hidden_size\n\t\t  b = hidden_size if layer_id &lt; num_layers - 1 else num_classes\n\t\t  w = variable_scope.get_variable('w', [a, b])\n\t\t  x = math_ops.matmul(x, w)\n\t\t  x = nn_ops.relu(x)\n\t  ops['y_preds'] = math_ops.argmax(x, axis=1)\n\n\ttrain_op = g.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP)\n\ttrain_op.append(ops['y_preds'])\n\treturn g\n\n# build your graph\ngraph = _buildMnist()\n# extract metagraph\nmg = meta_graph.create_meta_graph_def(graph=graph)\n\n# Assign your devices to nodes in the loop below\n# As a simple example, assigning all ops to CPU.\n# If you were to remove this, you would see some nodes assigned to GPUs by default\nfor node in metagraph.graph_def.node:\n\tnode.device = '/device:CPU:0'\n\t\n# define a new graph\nnew_graph = tf.Graph()\n# log_device_placement so we can see the final placement\nconfig = tf.ConfigProto(log_device_placement)\nwith tf.Session(graph=new_graph,\n                config=config) as session:\n\t# load your modified metagraph\n\tsaver = tf.train.import_meta_graph(mg)\n\t# initialize and run your train_op\n\tsession.run(tf.global_variables_initializer())\n\tsession.run(new_graph.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP))\n</code></pre>", "body_text": "@xilenteyex: I think this is a basic example of what you're trying to do:\nimport tensorflow as tf\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import meta_graph\nfrom tensorflow.python.framework import ops as tf_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope\n\ndef _buildMnist(batch_size=128,\n\t\tinput_size=256,\n\t\tnum_classes=1024,\n\t\tnum_layers=10,\n\t\thidden_size=256,\n\t\tname='mnist'):\n\tg = tf_ops.get_default_graph()\n\twith g.as_default():\n\t  ops = {}\n\t  x = random_ops.random_uniform(\n\t\t  [batch_size, input_size], -0.1, 0.1, dtype=dtypes.float32)\n\t  for layer_id in range(num_layers):\n\t\twith variable_scope.variable_scope('layer_{}'.format(layer_id)):\n\t\t  a = input_size if layer_id == 0 else hidden_size\n\t\t  b = hidden_size if layer_id < num_layers - 1 else num_classes\n\t\t  w = variable_scope.get_variable('w', [a, b])\n\t\t  x = math_ops.matmul(x, w)\n\t\t  x = nn_ops.relu(x)\n\t  ops['y_preds'] = math_ops.argmax(x, axis=1)\n\n\ttrain_op = g.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP)\n\ttrain_op.append(ops['y_preds'])\n\treturn g\n\n# build your graph\ngraph = _buildMnist()\n# extract metagraph\nmg = meta_graph.create_meta_graph_def(graph=graph)\n\n# Assign your devices to nodes in the loop below\n# As a simple example, assigning all ops to CPU.\n# If you were to remove this, you would see some nodes assigned to GPUs by default\nfor node in metagraph.graph_def.node:\n\tnode.device = '/device:CPU:0'\n\t\n# define a new graph\nnew_graph = tf.Graph()\n# log_device_placement so we can see the final placement\nconfig = tf.ConfigProto(log_device_placement)\nwith tf.Session(graph=new_graph,\n                config=config) as session:\n\t# load your modified metagraph\n\tsaver = tf.train.import_meta_graph(mg)\n\t# initialize and run your train_op\n\tsession.run(tf.global_variables_initializer())\n\tsession.run(new_graph.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP))", "body": "@xilenteyex: I think this is a basic example of what you're trying to do:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import meta_graph\r\nfrom tensorflow.python.framework import ops as tf_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import nn_ops\r\nfrom tensorflow.python.ops import random_ops\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\ndef _buildMnist(batch_size=128,\r\n\t\tinput_size=256,\r\n\t\tnum_classes=1024,\r\n\t\tnum_layers=10,\r\n\t\thidden_size=256,\r\n\t\tname='mnist'):\r\n\tg = tf_ops.get_default_graph()\r\n\twith g.as_default():\r\n\t  ops = {}\r\n\t  x = random_ops.random_uniform(\r\n\t\t  [batch_size, input_size], -0.1, 0.1, dtype=dtypes.float32)\r\n\t  for layer_id in range(num_layers):\r\n\t\twith variable_scope.variable_scope('layer_{}'.format(layer_id)):\r\n\t\t  a = input_size if layer_id == 0 else hidden_size\r\n\t\t  b = hidden_size if layer_id < num_layers - 1 else num_classes\r\n\t\t  w = variable_scope.get_variable('w', [a, b])\r\n\t\t  x = math_ops.matmul(x, w)\r\n\t\t  x = nn_ops.relu(x)\r\n\t  ops['y_preds'] = math_ops.argmax(x, axis=1)\r\n\r\n\ttrain_op = g.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP)\r\n\ttrain_op.append(ops['y_preds'])\r\n\treturn g\r\n\r\n# build your graph\r\ngraph = _buildMnist()\r\n# extract metagraph\r\nmg = meta_graph.create_meta_graph_def(graph=graph)\r\n\r\n# Assign your devices to nodes in the loop below\r\n# As a simple example, assigning all ops to CPU.\r\n# If you were to remove this, you would see some nodes assigned to GPUs by default\r\nfor node in metagraph.graph_def.node:\r\n\tnode.device = '/device:CPU:0'\r\n\t\r\n# define a new graph\r\nnew_graph = tf.Graph()\r\n# log_device_placement so we can see the final placement\r\nconfig = tf.ConfigProto(log_device_placement)\r\nwith tf.Session(graph=new_graph,\r\n                config=config) as session:\r\n\t# load your modified metagraph\r\n\tsaver = tf.train.import_meta_graph(mg)\r\n\t# initialize and run your train_op\r\n\tsession.run(tf.global_variables_initializer())\r\n\tsession.run(new_graph.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP))\r\n```"}