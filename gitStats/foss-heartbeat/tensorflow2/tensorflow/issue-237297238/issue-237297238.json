{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10862", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10862/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10862/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10862/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10862", "id": 237297238, "node_id": "MDU6SXNzdWUyMzcyOTcyMzg=", "number": 10862, "title": "Bug: MultiRNNCell.state_size is a tuple", "user": {"login": "rickyhan", "id": 1768528, "node_id": "MDQ6VXNlcjE3Njg1Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1768528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rickyhan", "html_url": "https://github.com/rickyhan", "followers_url": "https://api.github.com/users/rickyhan/followers", "following_url": "https://api.github.com/users/rickyhan/following{/other_user}", "gists_url": "https://api.github.com/users/rickyhan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rickyhan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rickyhan/subscriptions", "organizations_url": "https://api.github.com/users/rickyhan/orgs", "repos_url": "https://api.github.com/users/rickyhan/repos", "events_url": "https://api.github.com/users/rickyhan/events{/privacy}", "received_events_url": "https://api.github.com/users/rickyhan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-06-20T18:06:26Z", "updated_at": "2018-02-03T01:26:05Z", "closed_at": "2018-02-03T01:26:05Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: n/a</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: 1080 Ti</li>\n<li><strong>Exact command to reproduce</strong>: see below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Version 1:</p>\n<p>The follow code will return LSTMStateTuple with <code>h</code> and <code>c</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>        enc_cell <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMCell(hidden_dim)\n\n        enc_inp_len <span class=\"pl-k\">=</span> np.array([seq_length_in <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_size)])\n\n        ((encoder_fw_outputs,\n          encoder_bw_outputs),\n         (encoder_fw_final_state,\n          encoder_bw_final_state)) <span class=\"pl-k\">=</span> (\n            tf.nn.bidirectional_dynamic_rnn(<span class=\"pl-v\">cell_fw</span><span class=\"pl-k\">=</span>enc_cell,\n                                            <span class=\"pl-v\">cell_bw</span><span class=\"pl-k\">=</span>enc_cell,\n                                            <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>enc_inp,\n                                            <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>enc_inp_len,\n                                            <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n            )\n        encoder_outputs <span class=\"pl-k\">=</span> tf.concat((encoder_fw_outputs, encoder_bw_outputs), <span class=\"pl-c1\">2</span>)\n\n        encoder_final_state_c <span class=\"pl-k\">=</span> tf.concat(\n            (encoder_fw_final_state.c, encoder_bw_final_state.c), <span class=\"pl-c1\">1</span>)\n\n        encoder_final_state_h <span class=\"pl-k\">=</span> tf.concat(\n            (encoder_fw_final_state.h, encoder_bw_final_state.h), <span class=\"pl-c1\">1</span>)\n\n        encoder_final_state <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMStateTuple(\n            <span class=\"pl-v\">c</span><span class=\"pl-k\">=</span>encoder_final_state_c,\n            <span class=\"pl-v\">h</span><span class=\"pl-k\">=</span>encoder_final_state_h\n        )</pre></div>\n<p>However, if I run the following MultiCellRNN:</p>\n<div class=\"highlight highlight-source-python\"><pre>        enc_cells <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>, encoder_depth):\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>enc_RNN_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(i)):\n                cell <span class=\"pl-k\">=</span> tf.contrib.rnn.GRUCell(hidden_dim)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or LSTMCell(hidden_dim)</span>\n                cell <span class=\"pl-k\">=</span> tf.contrib.rnn.DropoutWrapper(cell, <span class=\"pl-v\">output_keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span><span class=\"pl-k\">-</span>dropout)\n                enc_cells.append(cell)\n        enc_cell <span class=\"pl-k\">=</span> tf.contrib.rnn.MultiRNNCell(enc_cells)\n\n        ((encoder_fw_outputs,\n          encoder_bw_outputs),\n         (encoder_fw_final_state,\n          encoder_bw_final_state)) <span class=\"pl-k\">=</span> (\n            tf.nn.bidirectional_dynamic_rnn(<span class=\"pl-v\">cell_fw</span><span class=\"pl-k\">=</span>enc_cell,\n                                            <span class=\"pl-v\">cell_bw</span><span class=\"pl-k\">=</span>enc_cell,\n                                            <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>enc_inp,\n                                            <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>enc_inp_len,\n                                            <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n            )\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> encoder_fw_final_state is a Tensor from the .c part of LSTMStateTuple</span></pre></div>\n<p>It will return state tensor(just the <code>c</code> part, not the <code>h</code> activation)</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.2\nBazel version (if compiling from source): n/a\nCUDA/cuDNN version: 8.0\nGPU model and memory: 1080 Ti\nExact command to reproduce: see below\n\nDescribe the problem\nVersion 1:\nThe follow code will return LSTMStateTuple with h and c:\n        enc_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n\n        enc_inp_len = np.array([seq_length_in for _ in range(batch_size)])\n\n        ((encoder_fw_outputs,\n          encoder_bw_outputs),\n         (encoder_fw_final_state,\n          encoder_bw_final_state)) = (\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\n                                            cell_bw=enc_cell,\n                                            inputs=enc_inp,\n                                            sequence_length=enc_inp_len,\n                                            dtype=tf.float32)\n            )\n        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n\n        encoder_final_state_c = tf.concat(\n            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n\n        encoder_final_state_h = tf.concat(\n            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n\n        encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n            c=encoder_final_state_c,\n            h=encoder_final_state_h\n        )\nHowever, if I run the following MultiCellRNN:\n        enc_cells = []\n        for i in range(0, encoder_depth):\n            with tf.variable_scope('enc_RNN_{}'.format(i)):\n                cell = tf.contrib.rnn.GRUCell(hidden_dim)  # Or LSTMCell(hidden_dim)\n                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\n                enc_cells.append(cell)\n        enc_cell = tf.contrib.rnn.MultiRNNCell(enc_cells)\n\n        ((encoder_fw_outputs,\n          encoder_bw_outputs),\n         (encoder_fw_final_state,\n          encoder_bw_final_state)) = (\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\n                                            cell_bw=enc_cell,\n                                            inputs=enc_inp,\n                                            sequence_length=enc_inp_len,\n                                            dtype=tf.float32)\n            )\n\n        # encoder_fw_final_state is a Tensor from the .c part of LSTMStateTuple\nIt will return state tensor(just the c part, not the h activation)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: 1080 Ti\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nVersion 1:\r\n\r\nThe follow code will return LSTMStateTuple with `h` and `c`:\r\n\r\n```python\r\n\r\n        enc_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\r\n\r\n        enc_inp_len = np.array([seq_length_in for _ in range(batch_size)])\r\n\r\n        ((encoder_fw_outputs,\r\n          encoder_bw_outputs),\r\n         (encoder_fw_final_state,\r\n          encoder_bw_final_state)) = (\r\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\r\n                                            cell_bw=enc_cell,\r\n                                            inputs=enc_inp,\r\n                                            sequence_length=enc_inp_len,\r\n                                            dtype=tf.float32)\r\n            )\r\n        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\r\n\r\n        encoder_final_state_c = tf.concat(\r\n            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\r\n\r\n        encoder_final_state_h = tf.concat(\r\n            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\r\n\r\n        encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\r\n            c=encoder_final_state_c,\r\n            h=encoder_final_state_h\r\n        )\r\n```\r\n\r\nHowever, if I run the following MultiCellRNN:\r\n\r\n```python\r\n        enc_cells = []\r\n        for i in range(0, encoder_depth):\r\n            with tf.variable_scope('enc_RNN_{}'.format(i)):\r\n                cell = tf.contrib.rnn.GRUCell(hidden_dim)  # Or LSTMCell(hidden_dim)\r\n                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\r\n                enc_cells.append(cell)\r\n        enc_cell = tf.contrib.rnn.MultiRNNCell(enc_cells)\r\n\r\n        ((encoder_fw_outputs,\r\n          encoder_bw_outputs),\r\n         (encoder_fw_final_state,\r\n          encoder_bw_final_state)) = (\r\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\r\n                                            cell_bw=enc_cell,\r\n                                            inputs=enc_inp,\r\n                                            sequence_length=enc_inp_len,\r\n                                            dtype=tf.float32)\r\n            )\r\n\r\n        # encoder_fw_final_state is a Tensor from the .c part of LSTMStateTuple\r\n```\r\n\r\nIt will return state tensor(just the `c` part, not the `h` activation)"}