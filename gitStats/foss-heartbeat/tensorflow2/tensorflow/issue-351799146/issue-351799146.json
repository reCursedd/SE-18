{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21698", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21698/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21698/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21698/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21698", "id": 351799146, "node_id": "MDU6SXNzdWUzNTE3OTkxNDY=", "number": 21698, "title": "INT TFLITE very much slower than FLOAT TFLITE", "user": {"login": "abhi-rf", "id": 19860524, "node_id": "MDQ6VXNlcjE5ODYwNTI0", "avatar_url": "https://avatars0.githubusercontent.com/u/19860524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhi-rf", "html_url": "https://github.com/abhi-rf", "followers_url": "https://api.github.com/users/abhi-rf/followers", "following_url": "https://api.github.com/users/abhi-rf/following{/other_user}", "gists_url": "https://api.github.com/users/abhi-rf/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhi-rf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhi-rf/subscriptions", "organizations_url": "https://api.github.com/users/abhi-rf/orgs", "repos_url": "https://api.github.com/users/abhi-rf/repos", "events_url": "https://api.github.com/users/abhi-rf/events{/privacy}", "received_events_url": "https://api.github.com/users/abhi-rf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-08-18T07:28:25Z", "updated_at": "2018-08-23T03:10:25Z", "closed_at": "2018-08-23T03:10:25Z", "author_association": "NONE", "body_html": "<h2>Hi Guys. I am using Mobilenet 0.25,128. I used the pretrained models provided in the repo for obtaining the int tflite and float tflite models for the same <a href=\"https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\">found here</a>. I am trying to infer some images. Using the imagenet images and some other test images as well, the FLOAT TFLITE is faster than the INT TFLITE (FLOAT TFLITE takes roughly 3-4 milliseconds while INT one takes 8-9 ms). Any suggestions as to why this is happening ?I am running the inferences using the tflite interpreter following the documentation <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#interpreter\">here</a>. This issue happens with the tf-nightly builds as well as the normal tensorflow. Tried on both, and also on both python 2 as well as 3.</h2>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Ubuntu</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.10 (tf-nightly)</li>\n<li><strong>Python version</strong>:2</li>\n<li><strong>Bazel version (if compiling from source)</strong></li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: No</li>\n<li><strong>GPU model and memory</strong>: No</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<h3>Source code / logs</h3>\n<h1>####INT TFLITE CODE####</h1>\n<pre><code>#Load TFLite model and allocate tensors. Select the appropriate model and give its path\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')\n\n\n\n#For allocating the model tensors\ninterpreter.allocate_tensors()\n\n#Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n#print(input_details)\n#print(output_details)\n\nk=1\nfor image_path in TEST_IMAGE_PATHS:\n\t#Loading the image and resizing into the correct shape\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.uint8)\n\n\timg = img.reshape(1,128,128,3)\n\t#print(input_details)\n\t#print (img.shape)\n\t\n\t#Setting the input image to the input tensor\n\tinterpreter.set_tensor(input_details[0]['index'], img)\n\n\t#Running inference and timing it\n\tstart = time.time()\n\tinterpreter.invoke()\n\tend = time.time()\n\n\t#Getting the output information\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\n\n\t#Converting the prediction into human readable form\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \n\tprint(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\n\t\n\ttotal_infer_time+=(end-start)\n\tk+=1\n\n#printing average inference time along with the accuracy\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)\n\n\n\n</code></pre>\n<h1>####FLOAT TFLITE CODE####</h1>\n<pre><code>#Load TFLite model and allocate tensors. Select the appropriate model and give its path\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128.tflite')\n\n\n#For allocating the model tensors\ninterpreter.allocate_tensors()\n\n#Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n#print(input_details)\n#print(output_details)\nk = 1           \nfor image_path in TEST_IMAGE_PATHS:\n\tprint image_path\n\t#Loading the image and resizing into the correct shape\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.float32) / 128 - 1\n\n\timg = img.reshape(1,128,128,3)\n\t#print (img.shape)\n\t\n\t#Setting the input image to the input tensor\n\tinterpreter.set_tensor(input_details[0]['index'], img)\n\n\t#Running inference and timing it\n\tstart = time.time()\n\tinterpreter.invoke()\n\tend = time.time()\n\n\t#Getting the output information\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\n\n\t#Converting the prediction into human readable form\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \n\t#print(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\n\t\n\t#detect_dog(output_data)\n\n\n\t'''\n\t#For accuracy\n\tnumber = []\n\tnumber = int(mobilenet_labels[k])\n\t        \n\n\tif(number==output_data.argmax()):\n\t    accuracy+=1\n\n\t'''\n\ttotal_infer_time+=(end-start)\n\tprint total_infer_time/(k+1)\n\tk+=1\n\n#printing average inference time along with the accuracy\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)\n\n</code></pre>\n<pre><code></code></pre>", "body_text": "Hi Guys. I am using Mobilenet 0.25,128. I used the pretrained models provided in the repo for obtaining the int tflite and float tflite models for the same found here. I am trying to infer some images. Using the imagenet images and some other test images as well, the FLOAT TFLITE is faster than the INT TFLITE (FLOAT TFLITE takes roughly 3-4 milliseconds while INT one takes 8-9 ms). Any suggestions as to why this is happening ?I am running the inferences using the tflite interpreter following the documentation here. This issue happens with the tf-nightly builds as well as the normal tensorflow. Tried on both, and also on both python 2 as well as 3.\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):Binary\nTensorFlow version (use command below):1.10 (tf-nightly)\nPython version:2\nBazel version (if compiling from source)\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: No\nGPU model and memory: No\n\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\n####INT TFLITE CODE####\n#Load TFLite model and allocate tensors. Select the appropriate model and give its path\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')\n\n\n\n#For allocating the model tensors\ninterpreter.allocate_tensors()\n\n#Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n#print(input_details)\n#print(output_details)\n\nk=1\nfor image_path in TEST_IMAGE_PATHS:\n\t#Loading the image and resizing into the correct shape\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.uint8)\n\n\timg = img.reshape(1,128,128,3)\n\t#print(input_details)\n\t#print (img.shape)\n\t\n\t#Setting the input image to the input tensor\n\tinterpreter.set_tensor(input_details[0]['index'], img)\n\n\t#Running inference and timing it\n\tstart = time.time()\n\tinterpreter.invoke()\n\tend = time.time()\n\n\t#Getting the output information\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\n\n\t#Converting the prediction into human readable form\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \n\tprint(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\n\t\n\ttotal_infer_time+=(end-start)\n\tk+=1\n\n#printing average inference time along with the accuracy\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)\n\n\n\n\n####FLOAT TFLITE CODE####\n#Load TFLite model and allocate tensors. Select the appropriate model and give its path\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128.tflite')\n\n\n#For allocating the model tensors\ninterpreter.allocate_tensors()\n\n#Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n#print(input_details)\n#print(output_details)\nk = 1           \nfor image_path in TEST_IMAGE_PATHS:\n\tprint image_path\n\t#Loading the image and resizing into the correct shape\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.float32) / 128 - 1\n\n\timg = img.reshape(1,128,128,3)\n\t#print (img.shape)\n\t\n\t#Setting the input image to the input tensor\n\tinterpreter.set_tensor(input_details[0]['index'], img)\n\n\t#Running inference and timing it\n\tstart = time.time()\n\tinterpreter.invoke()\n\tend = time.time()\n\n\t#Getting the output information\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\n\n\t#Converting the prediction into human readable form\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \n\t#print(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\n\t\n\t#detect_dog(output_data)\n\n\n\t'''\n\t#For accuracy\n\tnumber = []\n\tnumber = int(mobilenet_labels[k])\n\t        \n\n\tif(number==output_data.argmax()):\n\t    accuracy+=1\n\n\t'''\n\ttotal_infer_time+=(end-start)\n\tprint total_infer_time/(k+1)\n\tk+=1\n\n#printing average inference time along with the accuracy\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)", "body": "Hi Guys. I am using Mobilenet 0.25,128. I used the pretrained models provided in the repo for obtaining the int tflite and float tflite models for the same [found here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). I am trying to infer some images. Using the imagenet images and some other test images as well, the FLOAT TFLITE is faster than the INT TFLITE (FLOAT TFLITE takes roughly 3-4 milliseconds while INT one takes 8-9 ms). Any suggestions as to why this is happening ?I am running the inferences using the tflite interpreter following the documentation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#interpreter). This issue happens with the tf-nightly builds as well as the normal tensorflow. Tried on both, and also on both python 2 as well as 3.\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:Binary\r\n- **TensorFlow version (use command below)**:1.10 (tf-nightly) \r\n- **Python version**:2\r\n- **Bazel version (if compiling from source)**\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\n\r\n# ####INT TFLITE CODE####\r\n\r\n```\r\n#Load TFLite model and allocate tensors. Select the appropriate model and give its path\r\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')\r\n\r\n\r\n\r\n#For allocating the model tensors\r\ninterpreter.allocate_tensors()\r\n\r\n#Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n#print(input_details)\r\n#print(output_details)\r\n\r\nk=1\r\nfor image_path in TEST_IMAGE_PATHS:\r\n\t#Loading the image and resizing into the correct shape\r\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\r\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.uint8)\r\n\r\n\timg = img.reshape(1,128,128,3)\r\n\t#print(input_details)\r\n\t#print (img.shape)\r\n\t\r\n\t#Setting the input image to the input tensor\r\n\tinterpreter.set_tensor(input_details[0]['index'], img)\r\n\r\n\t#Running inference and timing it\r\n\tstart = time.time()\r\n\tinterpreter.invoke()\r\n\tend = time.time()\r\n\r\n\t#Getting the output information\r\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n\t#Converting the prediction into human readable form\r\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \r\n\tprint(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\r\n\t\r\n\ttotal_infer_time+=(end-start)\r\n\tk+=1\r\n\r\n#printing average inference time along with the accuracy\r\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)\r\n\r\n\r\n\r\n```\r\n# ####FLOAT TFLITE CODE####\r\n```\r\n#Load TFLite model and allocate tensors. Select the appropriate model and give its path\r\ninterpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128.tflite')\r\n\r\n\r\n#For allocating the model tensors\r\ninterpreter.allocate_tensors()\r\n\r\n#Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n#print(input_details)\r\n#print(output_details)\r\nk = 1           \r\nfor image_path in TEST_IMAGE_PATHS:\r\n\tprint image_path\r\n\t#Loading the image and resizing into the correct shape\r\n\t#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite\r\n\timg = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.float32) / 128 - 1\r\n\r\n\timg = img.reshape(1,128,128,3)\r\n\t#print (img.shape)\r\n\t\r\n\t#Setting the input image to the input tensor\r\n\tinterpreter.set_tensor(input_details[0]['index'], img)\r\n\r\n\t#Running inference and timing it\r\n\tstart = time.time()\r\n\tinterpreter.invoke()\r\n\tend = time.time()\r\n\r\n\t#Getting the output information\r\n\toutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n\t#Converting the prediction into human readable form\r\n\t#label_map = imagenet.create_readable_names_for_imagenet_labels()  \r\n\t#print(\"Top 1 Prediction: \", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])\r\n\t\r\n\t#detect_dog(output_data)\r\n\r\n\r\n\t'''\r\n\t#For accuracy\r\n\tnumber = []\r\n\tnumber = int(mobilenet_labels[k])\r\n\t        \r\n\r\n\tif(number==output_data.argmax()):\r\n\t    accuracy+=1\r\n\r\n\t'''\r\n\ttotal_infer_time+=(end-start)\r\n\tprint total_infer_time/(k+1)\r\n\tk+=1\r\n\r\n#printing average inference time along with the accuracy\r\nprint(\"Total infer time avg (in seconds) == \",total_infer_time/no_images)\r\n\r\n``````\r\n\r\n```"}