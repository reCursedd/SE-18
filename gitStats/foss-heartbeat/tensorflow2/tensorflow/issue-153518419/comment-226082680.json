{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226082680", "html_url": "https://github.com/tensorflow/tensorflow/issues/2255#issuecomment-226082680", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2255", "id": 226082680, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjA4MjY4MA==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-15T04:04:07Z", "updated_at": "2016-06-15T04:37:20Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Apart from that, it looks like tf.gradients only returns complex numbers if the diff variables are complex at the moment.<br>\nMaybe some of the variables that you differentiate by are still complex?</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a>  You could be right. I will carefully analyze all of my <code>trainable variables</code> and ensure that they are all real.</p>\n<p>Thank you very much for the comprehensive mathematical explanation for gradient understanding. Makes much more sense to me now.</p>\n<hr>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6271084\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/khaotik\">@khaotik</a></p>\n<p>Apologies for some of my misunderstandings. Makes complete sense how your variables are indeed diagonal. Really clever how you avoided wasting memory for the diagonal matrix.</p>\n<p>Thanks for the <code>fft_scale</code> explanation. Its logical that you only want this multiplication to occur twice, before it enters <code>batch_fft</code> and after its inverse <code>batch_ifft</code>.</p>\n<p>Your <code>mod_relu</code> implementation is completely correct. I just did not notice the <code>z</code> in the numerator which is what threw me off (don't know how I missed that). Will go back through my implementation and make the correction. We should either push your or mine implementation to TensorFlow so others have it.</p>\n<p>I'll also try stacking multiple layers of the uRNN and see how they do. I've heard that uLSTM is also very powerful, but hasn't been published yet.</p>\n<p>Will post back with findings a few days from now as I am away from my gpu's at the moment.</p>\n<p>In the meantime, I added a few things to your unitary implementation:</p>\n<ul>\n<li>Initialized R1 and R2 uniformly between <code>[-1,1]</code> as they indicate in their paper</li>\n<li>Initialized Diagonal matrices between <code>[-pi, pi]</code>.</li>\n<li>Initialized all Biases to zero as they do in the paper.</li>\n<li>Added Out Bias</li>\n<li>Commented out the extra <code>batch_fft</code> line</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> math <span class=\"pl-k\">import</span> sqrt\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> variable_scope <span class=\"pl-k\">as</span> vs\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> array_ops\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> math_ops\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> nn_ops\n\nrnn_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell\nseq2seq <span class=\"pl-k\">=</span> tf.nn.seq2seq\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">complex_mul_real</span>( <span class=\"pl-smi\">c</span>, <span class=\"pl-smi\">r</span> ):\n    <span class=\"pl-k\">return</span> tf.complex(tf.real(c)<span class=\"pl-k\">*</span>r, tf.imag(c)<span class=\"pl-k\">*</span>r)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">refl_c</span>(<span class=\"pl-smi\">in_</span>, <span class=\"pl-smi\">normal_</span>):\n    normal_rk2 <span class=\"pl-k\">=</span> tf.expand_dims( normal_, <span class=\"pl-c1\">1</span> )\n    scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>tf.matmul( in_, tf.conj( normal_rk2 ) )\n    <span class=\"pl-k\">return</span> in_ <span class=\"pl-k\">-</span> tf.matmul(scale, tf.transpose(normal_rk2))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">complex_abs_sq</span>(<span class=\"pl-smi\">z</span>):\n    <span class=\"pl-k\">return</span> tf.real(z)<span class=\"pl-k\">*</span>tf.real(z) <span class=\"pl-k\">+</span> tf.imag(z)<span class=\"pl-k\">*</span>tf.imag(z)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">normalize_c</span>(<span class=\"pl-smi\">in_</span>):\n    norm <span class=\"pl-k\">=</span> tf.sqrt(tf.reduce_sum(complex_abs_sq(in_)))\n    scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>(norm <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1e-5</span>)\n    <span class=\"pl-k\">return</span> complex_mul_real( in_, scale )\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_complex_variable</span>( <span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">scope</span>, <span class=\"pl-smi\">shape</span>, <span class=\"pl-smi\">initializer</span>):\n    re, im <span class=\"pl-k\">=</span> vs.get_variable(name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_re<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape), vs.get_variable(name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_im<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape, <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> initializer)\n    <span class=\"pl-k\">return</span> tf.complex(re,im, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name)\n\n\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span>multiply complex vector by parameterized unitary matrix<span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">ulinear</span>(<span class=\"pl-smi\">vec_in</span>, <span class=\"pl-smi\">out_size</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    shape <span class=\"pl-k\">=</span> vec_in.get_shape().as_list()\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(shape) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">2</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>( <span class=\"pl-s\"><span class=\"pl-pds\">'</span>argument vec_in must be a batch of vectors (2D tensor)<span class=\"pl-pds\">'</span></span> )\n    in_size <span class=\"pl-k\">=</span> shape[<span class=\"pl-c1\">1</span>]\n    fft_scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>sqrt(out_size)\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ULinear<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> _s:\n        <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Equation W = D2 R1 F-1 D1 Perm R0 FFT D0<span class=\"pl-pds\">'''</span></span>\n        diag0 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>diag0<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size], tf.random_uniform_initializer(<span class=\"pl-k\">-</span>np.pi, np.pi))\n        diag1 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>diag1<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size], tf.random_uniform_initializer(<span class=\"pl-k\">-</span>np.pi, np.pi))\n        diag2 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>diag2<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size], tf.random_uniform_initializer(<span class=\"pl-k\">-</span>np.pi, np.pi))\n        refl0 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>refl0<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size], tf.random_uniform_initializer(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.)) <span class=\"pl-c\"><span class=\"pl-c\">#</span>nick notice how all of these are O(size), and not squared</span>\n        refl1 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>refl1<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size], tf.random_uniform_initializer(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.))\n        perm0 <span class=\"pl-k\">=</span> tf.constant(np.random.permutation(out_size), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>perm0<span class=\"pl-pds\">'</span></span>,<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>int32<span class=\"pl-pds\">'</span></span>)\n        refl0 <span class=\"pl-k\">=</span> normalize_c(refl0)\n        refl1 <span class=\"pl-k\">=</span> normalize_c(refl1)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Equation Calculation</span>\n        out_ <span class=\"pl-k\">=</span> vec_in <span class=\"pl-k\">*</span> diag0\n        out_ <span class=\"pl-k\">=</span> refl_c(math_ops.batch_fft(out_)<span class=\"pl-k\">*</span>fft_scale, refl0)\n        out_ <span class=\"pl-k\">=</span> diag1 <span class=\"pl-k\">*</span> tf.transpose(tf.gather(tf.transpose(out_), perm0))\n        out_ <span class=\"pl-k\">=</span> diag2 <span class=\"pl-k\">*</span> refl_c(math_ops.batch_ifft(out_)<span class=\"pl-k\">*</span>fft_scale, refl1)\n        <span class=\"pl-k\">return</span> out_\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">modReLU</span>(<span class=\"pl-smi\">in_c</span>, <span class=\"pl-smi\">bias</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ULinear<span class=\"pl-pds\">\"</span></span>):\n        n <span class=\"pl-k\">=</span> tf.complex_abs(in_c)\n        scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>(n<span class=\"pl-k\">+</span><span class=\"pl-c1\">1e-5</span>)\n    <span class=\"pl-k\">return</span> complex_mul_real(in_c, ( nn_ops.relu(n<span class=\"pl-k\">+</span>bias)<span class=\"pl-k\">*</span>scale ))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">abssigm</span>(<span class=\"pl-smi\">in_c</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ULinear<span class=\"pl-pds\">'</span></span>):\n        re,im <span class=\"pl-k\">=</span> tf.real(in_c), tf.imag(in_c)\n        scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>tf.sqrt(re<span class=\"pl-k\">*</span>re <span class=\"pl-k\">+</span> im<span class=\"pl-k\">*</span>im <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>.)\n    <span class=\"pl-k\">return</span> complex_mul_real(in_c,scale)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">URNNCell</span>(<span class=\"pl-e\">rnn_cell</span>.<span class=\"pl-e\">RNNCell</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>._num_units <span class=\"pl-k\">=</span> num_units\n        <span class=\"pl-c1\">self</span>._input_size <span class=\"pl-k\">=</span> num_units <span class=\"pl-k\">if</span> input_size<span class=\"pl-k\">==</span><span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> input_size\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">input_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._input_size\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">output_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span> ):\n        <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__name__</span>):\n            mat_in <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mat_in<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">self</span>.input_size, <span class=\"pl-c1\">self</span>.state_size<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>])\n            mat_out <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mat_out<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">self</span>.state_size<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">self</span>.output_size])\n            in_proj <span class=\"pl-k\">=</span> math_ops.matmul(inputs, mat_in)\n            in_proj_c <span class=\"pl-k\">=</span> tf.complex( in_proj[:, :<span class=\"pl-c1\">self</span>.state_size], in_proj[:, <span class=\"pl-c1\">self</span>.state_size:] )\n            out_state <span class=\"pl-k\">=</span> modReLU( in_proj_c <span class=\"pl-k\">+</span> \n                ulinear(state, <span class=\"pl-c1\">self</span>.state_size),\n                vs.get_variable(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">self</span>.state_size], <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> tf.constant_initalizer(<span class=\"pl-c1\">0</span>.)),\n                <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope)\n            <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">            out_state = abssigm( in_proj_c +</span>\n<span class=\"pl-s\">                                ulinear(state, self.state_size),</span>\n<span class=\"pl-s\">                                scope=scope )</span>\n<span class=\"pl-s\">            <span class=\"pl-pds\">'''</span></span>\n\n            out_bias <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>out_bias<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">dtype</span> <span class=\"pl-k\">=</span> tf.float32, <span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">self</span>.output_size], <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> tf.constant_initalizer(<span class=\"pl-c1\">0</span>.))\n            out_ <span class=\"pl-k\">=</span> math_ops.matmul( array_ops.concat(<span class=\"pl-c1\">1</span>,[tf.real(out_state), tf.imag(out_state)] ), mat_out) <span class=\"pl-k\">+</span> out_bias\n        <span class=\"pl-k\">return</span> out_, out_state</pre></div>", "body_text": "Apart from that, it looks like tf.gradients only returns complex numbers if the diff variables are complex at the moment.\nMaybe some of the variables that you differentiate by are still complex?\n\n@ibab  You could be right. I will carefully analyze all of my trainable variables and ensure that they are all real.\nThank you very much for the comprehensive mathematical explanation for gradient understanding. Makes much more sense to me now.\n\n@khaotik\nApologies for some of my misunderstandings. Makes complete sense how your variables are indeed diagonal. Really clever how you avoided wasting memory for the diagonal matrix.\nThanks for the fft_scale explanation. Its logical that you only want this multiplication to occur twice, before it enters batch_fft and after its inverse batch_ifft.\nYour mod_relu implementation is completely correct. I just did not notice the z in the numerator which is what threw me off (don't know how I missed that). Will go back through my implementation and make the correction. We should either push your or mine implementation to TensorFlow so others have it.\nI'll also try stacking multiple layers of the uRNN and see how they do. I've heard that uLSTM is also very powerful, but hasn't been published yet.\nWill post back with findings a few days from now as I am away from my gpu's at the moment.\nIn the meantime, I added a few things to your unitary implementation:\n\nInitialized R1 and R2 uniformly between [-1,1] as they indicate in their paper\nInitialized Diagonal matrices between [-pi, pi].\nInitialized all Biases to zero as they do in the paper.\nAdded Out Bias\nCommented out the extra batch_fft line\n\nfrom math import sqrt\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\n\nrnn_cell = tf.nn.rnn_cell\nseq2seq = tf.nn.seq2seq\n\ndef complex_mul_real( c, r ):\n    return tf.complex(tf.real(c)*r, tf.imag(c)*r)\n\n\ndef refl_c(in_, normal_):\n    normal_rk2 = tf.expand_dims( normal_, 1 )\n    scale = 2*tf.matmul( in_, tf.conj( normal_rk2 ) )\n    return in_ - tf.matmul(scale, tf.transpose(normal_rk2))\n\n\ndef complex_abs_sq(z):\n    return tf.real(z)*tf.real(z) + tf.imag(z)*tf.imag(z)\n\n\ndef normalize_c(in_):\n    norm = tf.sqrt(tf.reduce_sum(complex_abs_sq(in_)))\n    scale = 1./(norm + 1e-5)\n    return complex_mul_real( in_, scale )\n\n\ndef get_complex_variable( name, scope, shape, initializer):\n    re, im = vs.get_variable(name+'_re', shape=shape), vs.get_variable(name+'_im', shape=shape, initializer = initializer)\n    return tf.complex(re,im, name=name)\n\n\n'''multiply complex vector by parameterized unitary matrix'''\ndef ulinear(vec_in, out_size, scope=None):\n    shape = vec_in.get_shape().as_list()\n    if len(shape) != 2:\n        raise ValueError( 'argument vec_in must be a batch of vectors (2D tensor)' )\n    in_size = shape[1]\n    fft_scale = 1./sqrt(out_size)\n    with vs.variable_scope(scope or 'ULinear') as _s:\n        '''Equation W = D2 R1 F-1 D1 Perm R0 FFT D0'''\n        diag0 = get_complex_variable('diag0', _s, shape=[out_size], tf.random_uniform_initializer(-np.pi, np.pi))\n        diag1 = get_complex_variable('diag1', _s, shape=[out_size], tf.random_uniform_initializer(-np.pi, np.pi))\n        diag2 = get_complex_variable('diag2', _s, shape=[out_size], tf.random_uniform_initializer(-np.pi, np.pi))\n        refl0 = get_complex_variable('refl0', _s, shape=[out_size], tf.random_uniform_initializer(-1., 1.)) #nick notice how all of these are O(size), and not squared\n        refl1 = get_complex_variable('refl1', _s, shape=[out_size], tf.random_uniform_initializer(-1., 1.))\n        perm0 = tf.constant(np.random.permutation(out_size), name='perm0',dtype='int32')\n        refl0 = normalize_c(refl0)\n        refl1 = normalize_c(refl1)\n        #Equation Calculation\n        out_ = vec_in * diag0\n        out_ = refl_c(math_ops.batch_fft(out_)*fft_scale, refl0)\n        out_ = diag1 * tf.transpose(tf.gather(tf.transpose(out_), perm0))\n        out_ = diag2 * refl_c(math_ops.batch_ifft(out_)*fft_scale, refl1)\n        return out_\n\n\ndef modReLU(in_c, bias, scope=None):\n    with vs.variable_scope(scope or \"ULinear\"):\n        n = tf.complex_abs(in_c)\n        scale = 1./(n+1e-5)\n    return complex_mul_real(in_c, ( nn_ops.relu(n+bias)*scale ))\n\n\ndef abssigm(in_c, scope=None):\n    with vs.variable_scope(scope or 'ULinear'):\n        re,im = tf.real(in_c), tf.imag(in_c)\n        scale = 1./tf.sqrt(re*re + im*im + 1.)\n    return complex_mul_real(in_c,scale)\n\n\nclass URNNCell(rnn_cell.RNNCell):\n    def __init__(self, num_units, input_size=None):\n        self._num_units = num_units\n        self._input_size = num_units if input_size==None else input_size\n\n    @property\n    def input_size(self):\n        return self._input_size\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None ):\n        with vs.variable_scope(scope or type(self).__name__):\n            mat_in = vs.get_variable('mat_in', [self.input_size, self.state_size*2])\n            mat_out = vs.get_variable('mat_out', [self.state_size*2, self.output_size])\n            in_proj = math_ops.matmul(inputs, mat_in)\n            in_proj_c = tf.complex( in_proj[:, :self.state_size], in_proj[:, self.state_size:] )\n            out_state = modReLU( in_proj_c + \n                ulinear(state, self.state_size),\n                vs.get_variable(name='bias', dtype=tf.float32, shape=[self.state_size], initializer = tf.constant_initalizer(0.)),\n                scope=scope)\n            '''\n            out_state = abssigm( in_proj_c +\n                                ulinear(state, self.state_size),\n                                scope=scope )\n            '''\n\n            out_bias = vs.get_variable(name = \"out_bias\", dtype = tf.float32, shape = [self.output_size], initializer = tf.constant_initalizer(0.))\n            out_ = math_ops.matmul( array_ops.concat(1,[tf.real(out_state), tf.imag(out_state)] ), mat_out) + out_bias\n        return out_, out_state", "body": "> Apart from that, it looks like tf.gradients only returns complex numbers if the diff variables are complex at the moment. \n> Maybe some of the variables that you differentiate by are still complex?\n\n@ibab  You could be right. I will carefully analyze all of my `trainable variables` and ensure that they are all real.\n\nThank you very much for the comprehensive mathematical explanation for gradient understanding. Makes much more sense to me now. \n\n---\n\n@khaotik \n\nApologies for some of my misunderstandings. Makes complete sense how your variables are indeed diagonal. Really clever how you avoided wasting memory for the diagonal matrix.\n\nThanks for the `fft_scale` explanation. Its logical that you only want this multiplication to occur twice, before it enters `batch_fft` and after its inverse `batch_ifft`. \n\nYour `mod_relu` implementation is completely correct. I just did not notice the `z` in the numerator which is what threw me off (don't know how I missed that). Will go back through my implementation and make the correction. We should either push your or mine implementation to TensorFlow so others have it. \n\nI'll also try stacking multiple layers of the uRNN and see how they do. I've heard that uLSTM is also very powerful, but hasn't been published yet. \n\nWill post back with findings a few days from now as I am away from my gpu's at the moment. \n\nIn the meantime, I added a few things to your unitary implementation:\n- Initialized R1 and R2 uniformly between `[-1,1]` as they indicate in their paper\n- Initialized Diagonal matrices between `[-pi, pi]`.\n- Initialized all Biases to zero as they do in the paper. \n- Added Out Bias\n- Commented out the extra `batch_fft` line\n\n``` python\nfrom math import sqrt\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\n\nrnn_cell = tf.nn.rnn_cell\nseq2seq = tf.nn.seq2seq\n\ndef complex_mul_real( c, r ):\n    return tf.complex(tf.real(c)*r, tf.imag(c)*r)\n\n\ndef refl_c(in_, normal_):\n    normal_rk2 = tf.expand_dims( normal_, 1 )\n    scale = 2*tf.matmul( in_, tf.conj( normal_rk2 ) )\n    return in_ - tf.matmul(scale, tf.transpose(normal_rk2))\n\n\ndef complex_abs_sq(z):\n    return tf.real(z)*tf.real(z) + tf.imag(z)*tf.imag(z)\n\n\ndef normalize_c(in_):\n    norm = tf.sqrt(tf.reduce_sum(complex_abs_sq(in_)))\n    scale = 1./(norm + 1e-5)\n    return complex_mul_real( in_, scale )\n\n\ndef get_complex_variable( name, scope, shape, initializer):\n    re, im = vs.get_variable(name+'_re', shape=shape), vs.get_variable(name+'_im', shape=shape, initializer = initializer)\n    return tf.complex(re,im, name=name)\n\n\n'''multiply complex vector by parameterized unitary matrix'''\ndef ulinear(vec_in, out_size, scope=None):\n    shape = vec_in.get_shape().as_list()\n    if len(shape) != 2:\n        raise ValueError( 'argument vec_in must be a batch of vectors (2D tensor)' )\n    in_size = shape[1]\n    fft_scale = 1./sqrt(out_size)\n    with vs.variable_scope(scope or 'ULinear') as _s:\n        '''Equation W = D2 R1 F-1 D1 Perm R0 FFT D0'''\n        diag0 = get_complex_variable('diag0', _s, shape=[out_size], tf.random_uniform_initializer(-np.pi, np.pi))\n        diag1 = get_complex_variable('diag1', _s, shape=[out_size], tf.random_uniform_initializer(-np.pi, np.pi))\n        diag2 = get_complex_variable('diag2', _s, shape=[out_size], tf.random_uniform_initializer(-np.pi, np.pi))\n        refl0 = get_complex_variable('refl0', _s, shape=[out_size], tf.random_uniform_initializer(-1., 1.)) #nick notice how all of these are O(size), and not squared\n        refl1 = get_complex_variable('refl1', _s, shape=[out_size], tf.random_uniform_initializer(-1., 1.))\n        perm0 = tf.constant(np.random.permutation(out_size), name='perm0',dtype='int32')\n        refl0 = normalize_c(refl0)\n        refl1 = normalize_c(refl1)\n        #Equation Calculation\n        out_ = vec_in * diag0\n        out_ = refl_c(math_ops.batch_fft(out_)*fft_scale, refl0)\n        out_ = diag1 * tf.transpose(tf.gather(tf.transpose(out_), perm0))\n        out_ = diag2 * refl_c(math_ops.batch_ifft(out_)*fft_scale, refl1)\n        return out_\n\n\ndef modReLU(in_c, bias, scope=None):\n    with vs.variable_scope(scope or \"ULinear\"):\n        n = tf.complex_abs(in_c)\n        scale = 1./(n+1e-5)\n    return complex_mul_real(in_c, ( nn_ops.relu(n+bias)*scale ))\n\n\ndef abssigm(in_c, scope=None):\n    with vs.variable_scope(scope or 'ULinear'):\n        re,im = tf.real(in_c), tf.imag(in_c)\n        scale = 1./tf.sqrt(re*re + im*im + 1.)\n    return complex_mul_real(in_c,scale)\n\n\nclass URNNCell(rnn_cell.RNNCell):\n    def __init__(self, num_units, input_size=None):\n        self._num_units = num_units\n        self._input_size = num_units if input_size==None else input_size\n\n    @property\n    def input_size(self):\n        return self._input_size\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None ):\n        with vs.variable_scope(scope or type(self).__name__):\n            mat_in = vs.get_variable('mat_in', [self.input_size, self.state_size*2])\n            mat_out = vs.get_variable('mat_out', [self.state_size*2, self.output_size])\n            in_proj = math_ops.matmul(inputs, mat_in)\n            in_proj_c = tf.complex( in_proj[:, :self.state_size], in_proj[:, self.state_size:] )\n            out_state = modReLU( in_proj_c + \n                ulinear(state, self.state_size),\n                vs.get_variable(name='bias', dtype=tf.float32, shape=[self.state_size], initializer = tf.constant_initalizer(0.)),\n                scope=scope)\n            '''\n            out_state = abssigm( in_proj_c +\n                                ulinear(state, self.state_size),\n                                scope=scope )\n            '''\n\n            out_bias = vs.get_variable(name = \"out_bias\", dtype = tf.float32, shape = [self.output_size], initializer = tf.constant_initalizer(0.))\n            out_ = math_ops.matmul( array_ops.concat(1,[tf.real(out_state), tf.imag(out_state)] ), mat_out) + out_bias\n        return out_, out_state\n```\n"}