{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225458017", "html_url": "https://github.com/tensorflow/tensorflow/issues/2255#issuecomment-225458017", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2255", "id": 225458017, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTQ1ODAxNw==", "user": {"login": "ibab", "id": 890531, "node_id": "MDQ6VXNlcjg5MDUzMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/890531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ibab", "html_url": "https://github.com/ibab", "followers_url": "https://api.github.com/users/ibab/followers", "following_url": "https://api.github.com/users/ibab/following{/other_user}", "gists_url": "https://api.github.com/users/ibab/gists{/gist_id}", "starred_url": "https://api.github.com/users/ibab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ibab/subscriptions", "organizations_url": "https://api.github.com/users/ibab/orgs", "repos_url": "https://api.github.com/users/ibab/repos", "events_url": "https://api.github.com/users/ibab/events{/privacy}", "received_events_url": "https://api.github.com/users/ibab/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-12T20:18:48Z", "updated_at": "2016-06-12T20:18:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Note that the gradient of a complex expression can be complex, even if the variables that you take the derivative with respect to are real.<br>\nFor example, the derivative of <code>(1 + 1j * x)</code> with regard to <code>x</code> is <code>1j</code>, even if <code>x</code> is a real parameter.<br>\nThat means you have to explicitly make sure that all expressions that you differentiate are real.<br>\nUnfortunately, the associative LSTM paper doesn't seem to explain how they've taken this into account Maybe I have to read it more carefully.</p>\n<p>That said, I just tried running the following code:</p>\n<pre><code>import tensorflow as tf\n\ns = tf.Session()\nx = tf.placeholder(tf.float32)\nz1 = tf.complex(0., x)\n\nprint(s.run(tf.gradients(z, [x]), feed_dict={x: 1}))\n</code></pre>\n<p>and it prints out <code>[0.]</code>, which seems wrong to me (I expected <code>[0 + 1j]</code>).<br>\nMy guess is that the gradient code only returns the real part of a complex gradient, if the variable that we differentiate by is real. I'll open a bug report on this.</p>", "body_text": "Note that the gradient of a complex expression can be complex, even if the variables that you take the derivative with respect to are real.\nFor example, the derivative of (1 + 1j * x) with regard to x is 1j, even if x is a real parameter.\nThat means you have to explicitly make sure that all expressions that you differentiate are real.\nUnfortunately, the associative LSTM paper doesn't seem to explain how they've taken this into account Maybe I have to read it more carefully.\nThat said, I just tried running the following code:\nimport tensorflow as tf\n\ns = tf.Session()\nx = tf.placeholder(tf.float32)\nz1 = tf.complex(0., x)\n\nprint(s.run(tf.gradients(z, [x]), feed_dict={x: 1}))\n\nand it prints out [0.], which seems wrong to me (I expected [0 + 1j]).\nMy guess is that the gradient code only returns the real part of a complex gradient, if the variable that we differentiate by is real. I'll open a bug report on this.", "body": "Note that the gradient of a complex expression can be complex, even if the variables that you take the derivative with respect to are real.\nFor example, the derivative of `(1 + 1j * x)` with regard to `x` is `1j`, even if `x` is a real parameter.\nThat means you have to explicitly make sure that all expressions that you differentiate are real.\nUnfortunately, the associative LSTM paper doesn't seem to explain how they've taken this into account Maybe I have to read it more carefully.\n\nThat said, I just tried running the following code:\n\n```\nimport tensorflow as tf\n\ns = tf.Session()\nx = tf.placeholder(tf.float32)\nz1 = tf.complex(0., x)\n\nprint(s.run(tf.gradients(z, [x]), feed_dict={x: 1}))\n```\n\nand it prints out `[0.]`, which seems wrong to me (I expected `[0 + 1j]`).\nMy guess is that the gradient code only returns the real part of a complex gradient, if the variable that we differentiate by is real. I'll open a bug report on this.\n"}