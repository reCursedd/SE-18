{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225193314", "html_url": "https://github.com/tensorflow/tensorflow/issues/2255#issuecomment-225193314", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2255", "id": 225193314, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTE5MzMxNA==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-10T14:14:42Z", "updated_at": "2016-06-10T14:14:42Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a> thanks for your pull as it did resolve the <code>tf.transpose</code> issue that was occuring. However, a new issue has arisen. When I <code>optimizer.apply_gradients()</code>, I get the following error message:</p>\n<p>gradient code:</p>\n<div class=\"highlight highlight-source-python\"><pre>         params <span class=\"pl-k\">=</span> tf.trainable_variables()\n          gradients <span class=\"pl-k\">=</span> tf.gradients(<span class=\"pl-c1\">self</span>.losses[b], params) <span class=\"pl-c\"><span class=\"pl-c\">#</span>aggregation_method = 2</span>\n          <span class=\"pl-k\">for</span> i, grad <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(gradients):\n            <span class=\"pl-k\">if</span> grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> grad.dtype.name <span class=\"pl-k\">!=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>complex64<span class=\"pl-pds\">'</span></span>:\n              gradients[i] <span class=\"pl-k\">=</span> tf.clip_by_norm(grad, max_gradient_norm)\n\n\n        <span class=\"pl-c1\">self</span>.gradient_norms.append(norm)\n        <span class=\"pl-c1\">self</span>.updates.append(opt.apply_gradients(\n            <span class=\"pl-c1\">zip</span>(gradients, params), <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.global_step))</pre></div>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train_seq2seq.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">778</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    tf.app.run()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">30</span>, <span class=\"pl-k\">in</span> run\n    sys.exit(main(sys.argv))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train_seq2seq.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">775</span>, <span class=\"pl-k\">in</span> main\n    train()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train_seq2seq.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">393</span>, <span class=\"pl-k\">in</span> train\n    model <span class=\"pl-k\">=</span> create_model(sess, <span class=\"pl-c1\">False</span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train_seq2seq.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">328</span>, <span class=\"pl-k\">in</span> create_model\n    description_text <span class=\"pl-k\">=</span> description_text)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/nick/Project_RNN_Enhancement/rnn_enhancement/seq2seq_model_enhanced.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">305</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">zip</span>(gradients, params), global_step<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.global_step))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">291</span>, <span class=\"pl-k\">in</span> apply_gradients\n    <span class=\"pl-c1\">self</span>._assert_valid_dtypes([g, v])\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">365</span>, <span class=\"pl-k\">in</span> _assert_valid_dtypes\n    dtype, t.name, [v <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> valid_dtypes]))\n<span class=\"pl-c1\">ValueError</span>: Invalid <span class=\"pl-c1\">type</span> tf.complex64 <span class=\"pl-k\">for</span> gradients<span class=\"pl-k\">/</span>model_with_buckets<span class=\"pl-k\">/</span>embedding_attention_seq2seq<span class=\"pl-k\">/</span><span class=\"pl-c1\">RNN</span><span class=\"pl-k\">/</span>MultiRNNCell<span class=\"pl-k\">/</span>Cell0<span class=\"pl-k\">/</span>AssociativeBasicLSTMCell<span class=\"pl-k\">/</span>gate_multiplication<span class=\"pl-k\">/</span>Enhanced_Linear<span class=\"pl-k\">/</span>MatMul_grad<span class=\"pl-k\">/</span>MatMul_1:<span class=\"pl-c1\">0</span>, expected: [tf.float32, tf.float64, tf.float16].</pre></div>\n<p>Basically, there are both complex64 and float32 trainable variables that I have in the <code>seq2seq.py</code> which is based upon tensorflow's <code>seq2seq.py</code>. These trainable variables enter the params argument in <code>apply_gradients</code>.</p>\n<p>Is there a problem when you use trainable variables of both <code>float32</code> and <code>complex64</code> types? I really appreciate your help. I'm running rc version 0.9.</p>", "body_text": "@ibab thanks for your pull as it did resolve the tf.transpose issue that was occuring. However, a new issue has arisen. When I optimizer.apply_gradients(), I get the following error message:\ngradient code:\n         params = tf.trainable_variables()\n          gradients = tf.gradients(self.losses[b], params) #aggregation_method = 2\n          for i, grad in enumerate(gradients):\n            if grad is not None and grad.dtype.name != 'complex64':\n              gradients[i] = tf.clip_by_norm(grad, max_gradient_norm)\n\n\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n            zip(gradients, params), global_step=self.global_step))\nTraceback (most recent call last):\n  File \"train_seq2seq.py\", line 778, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"train_seq2seq.py\", line 775, in main\n    train()\n  File \"train_seq2seq.py\", line 393, in train\n    model = create_model(sess, False)\n  File \"train_seq2seq.py\", line 328, in create_model\n    description_text = description_text)\n  File \"/home/nick/Project_RNN_Enhancement/rnn_enhancement/seq2seq_model_enhanced.py\", line 305, in __init__\n    zip(gradients, params), global_step=self.global_step))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 291, in apply_gradients\n    self._assert_valid_dtypes([g, v])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 365, in _assert_valid_dtypes\n    dtype, t.name, [v for v in valid_dtypes]))\nValueError: Invalid type tf.complex64 for gradients/model_with_buckets/embedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/AssociativeBasicLSTMCell/gate_multiplication/Enhanced_Linear/MatMul_grad/MatMul_1:0, expected: [tf.float32, tf.float64, tf.float16].\nBasically, there are both complex64 and float32 trainable variables that I have in the seq2seq.py which is based upon tensorflow's seq2seq.py. These trainable variables enter the params argument in apply_gradients.\nIs there a problem when you use trainable variables of both float32 and complex64 types? I really appreciate your help. I'm running rc version 0.9.", "body": "@ibab thanks for your pull as it did resolve the `tf.transpose` issue that was occuring. However, a new issue has arisen. When I `optimizer.apply_gradients()`, I get the following error message:\n\ngradient code:\n\n``` python\n         params = tf.trainable_variables()\n          gradients = tf.gradients(self.losses[b], params) #aggregation_method = 2\n          for i, grad in enumerate(gradients):\n            if grad is not None and grad.dtype.name != 'complex64':\n              gradients[i] = tf.clip_by_norm(grad, max_gradient_norm)\n\n\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n            zip(gradients, params), global_step=self.global_step))\n```\n\n``` python\nTraceback (most recent call last):\n  File \"train_seq2seq.py\", line 778, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"train_seq2seq.py\", line 775, in main\n    train()\n  File \"train_seq2seq.py\", line 393, in train\n    model = create_model(sess, False)\n  File \"train_seq2seq.py\", line 328, in create_model\n    description_text = description_text)\n  File \"/home/nick/Project_RNN_Enhancement/rnn_enhancement/seq2seq_model_enhanced.py\", line 305, in __init__\n    zip(gradients, params), global_step=self.global_step))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 291, in apply_gradients\n    self._assert_valid_dtypes([g, v])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 365, in _assert_valid_dtypes\n    dtype, t.name, [v for v in valid_dtypes]))\nValueError: Invalid type tf.complex64 for gradients/model_with_buckets/embedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/AssociativeBasicLSTMCell/gate_multiplication/Enhanced_Linear/MatMul_grad/MatMul_1:0, expected: [tf.float32, tf.float64, tf.float16].\n```\n\nBasically, there are both complex64 and float32 trainable variables that I have in the `seq2seq.py` which is based upon tensorflow's `seq2seq.py`. These trainable variables enter the params argument in `apply_gradients`.\n\nIs there a problem when you use trainable variables of both `float32` and `complex64` types? I really appreciate your help. I'm running rc version 0.9. \n"}