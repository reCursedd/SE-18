{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226043491", "html_url": "https://github.com/tensorflow/tensorflow/issues/2255#issuecomment-226043491", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2255", "id": 226043491, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjA0MzQ5MQ==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T23:09:52Z", "updated_at": "2016-06-14T23:09:52Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a> maybe I'm misunderstanding something here. In both the Associative LSTM and Unitary, there are intermediate complex calculations that are called for. How could all gradients be real if there are complex calculations occurring?</p>\n<p>I understand that you can initialize all the trainable variables with <code>tf.complex(var1, var2)</code>. However, when I run the <code>tf.gradient</code> function on a real cost function, some of the gradients are indeed <code>dtype</code> <code>complex64</code>. Mathematically, this makes sense as well since the gradient had to flow through complex calculations for each of the trainable variables.</p>\n<p>Perhaps I'm missing something here, but it seems clear that complex gradients need to be established before either of these two types of RNN's can work.</p>", "body_text": "@ibab maybe I'm misunderstanding something here. In both the Associative LSTM and Unitary, there are intermediate complex calculations that are called for. How could all gradients be real if there are complex calculations occurring?\nI understand that you can initialize all the trainable variables with tf.complex(var1, var2). However, when I run the tf.gradient function on a real cost function, some of the gradients are indeed dtype complex64. Mathematically, this makes sense as well since the gradient had to flow through complex calculations for each of the trainable variables.\nPerhaps I'm missing something here, but it seems clear that complex gradients need to be established before either of these two types of RNN's can work.", "body": "@ibab maybe I'm misunderstanding something here. In both the Associative LSTM and Unitary, there are intermediate complex calculations that are called for. How could all gradients be real if there are complex calculations occurring? \n\nI understand that you can initialize all the trainable variables with `tf.complex(var1, var2)`. However, when I run the `tf.gradient` function on a real cost function, some of the gradients are indeed `dtype` `complex64`. Mathematically, this makes sense as well since the gradient had to flow through complex calculations for each of the trainable variables. \n\nPerhaps I'm missing something here, but it seems clear that complex gradients need to be established before either of these two types of RNN's can work. \n"}