{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225453703", "html_url": "https://github.com/tensorflow/tensorflow/issues/2255#issuecomment-225453703", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2255", "id": 225453703, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTQ1MzcwMw==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-12T18:51:17Z", "updated_at": "2016-06-12T19:02:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a>, the loss function is the seq2seq sequence loss function from tensorflow ops:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sequence_loss_by_example</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">targets</span>, <span class=\"pl-smi\">weights</span>,\n                             <span class=\"pl-smi\">average_across_timesteps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                             <span class=\"pl-smi\">softmax_loss_function</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Weighted cross-entropy loss for a sequence of logits (per example).</span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</span>\n<span class=\"pl-s\">    targets: List of 1D batch-sized int32 Tensors of the same length as logits.</span>\n<span class=\"pl-s\">    weights: List of 1D batch-sized float-Tensors of the same length as logits.</span>\n<span class=\"pl-s\">    average_across_timesteps: If set, divide the returned cost by the total</span>\n<span class=\"pl-s\">      label weight.</span>\n<span class=\"pl-s\">    softmax_loss_function: Function (inputs-batch, labels-batch) -&gt; loss-batch</span>\n<span class=\"pl-s\">      to be used instead of the standard softmax (the default if this is None).</span>\n<span class=\"pl-s\">    name: Optional name for this operation, default: \"sequence_loss_by_example\".</span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    1D batch-sized float Tensor: The log-perplexity for each sequence.</span>\n<span class=\"pl-s\">  Raises:</span>\n<span class=\"pl-s\">    ValueError: If len(logits) is different from len(targets) or len(weights).</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(targets) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">len</span>(logits) <span class=\"pl-k\">or</span> <span class=\"pl-c1\">len</span>(weights) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">len</span>(logits):\n    <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Lengths of logits, weights, and targets must be the same <span class=\"pl-pds\">\"</span></span>\n                     <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%d</span>, <span class=\"pl-c1\">%d</span>, <span class=\"pl-c1\">%d</span>.<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (<span class=\"pl-c1\">len</span>(logits), <span class=\"pl-c1\">len</span>(weights), <span class=\"pl-c1\">len</span>(targets)))\n  <span class=\"pl-k\">with</span> ops.op_scope(logits <span class=\"pl-k\">+</span> targets <span class=\"pl-k\">+</span> weights, name,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sequence_loss_by_example<span class=\"pl-pds\">\"</span></span>):\n    log_perp_list <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> logit, target, weight <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(logits, targets, weights):\n      <span class=\"pl-k\">if</span> softmax_loss_function <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>(irving,ebrevdo): This reshape is needed because</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> sequence_loss_by_example is called with scalars sometimes, which</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> violates our general scalar strictness policy.</span>\n        target <span class=\"pl-k\">=</span> array_ops.reshape(target, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n        crossent <span class=\"pl-k\">=</span> nn_ops.sparse_softmax_cross_entropy_with_logits(\n            logit, target)\n      <span class=\"pl-k\">else</span>:\n        crossent <span class=\"pl-k\">=</span> softmax_loss_function(logit, target)\n      log_perp_list.append(crossent <span class=\"pl-k\">*</span> weight)\n    log_perps <span class=\"pl-k\">=</span> math_ops.add_n(log_perp_list)\n    <span class=\"pl-k\">if</span> average_across_timesteps:\n      total_size <span class=\"pl-k\">=</span> math_ops.add_n(weights)\n      total_size <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1e-12</span>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Just to avoid division by 0 for all-0 weights.</span>\n      log_perps <span class=\"pl-k\">/=</span> total_size\n  <span class=\"pl-k\">return</span> log_perps\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">sequence_loss</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">targets</span>, <span class=\"pl-smi\">weights</span>,\n                  <span class=\"pl-smi\">average_across_timesteps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">average_across_batch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                  <span class=\"pl-smi\">softmax_loss_function</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Weighted cross-entropy loss for a sequence of logits, batch-collapsed.</span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</span>\n<span class=\"pl-s\">    targets: List of 1D batch-sized int32 Tensors of the same length as logits.</span>\n<span class=\"pl-s\">    weights: List of 1D batch-sized float-Tensors of the same length as logits.</span>\n<span class=\"pl-s\">    average_across_timesteps: If set, divide the returned cost by the total</span>\n<span class=\"pl-s\">      label weight.</span>\n<span class=\"pl-s\">    average_across_batch: If set, divide the returned cost by the batch size.</span>\n<span class=\"pl-s\">    softmax_loss_function: Function (inputs-batch, labels-batch) -&gt; loss-batch</span>\n<span class=\"pl-s\">      to be used instead of the standard softmax (the default if this is None).</span>\n<span class=\"pl-s\">    name: Optional name for this operation, defaults to \"sequence_loss\".</span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    A scalar float Tensor: The average log-perplexity per symbol (weighted).</span>\n<span class=\"pl-s\">  Raises:</span>\n<span class=\"pl-s\">    ValueError: If len(logits) is different from len(targets) or len(weights).</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">with</span> ops.op_scope(logits <span class=\"pl-k\">+</span> targets <span class=\"pl-k\">+</span> weights, name, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sequence_loss<span class=\"pl-pds\">\"</span></span>):\n    cost <span class=\"pl-k\">=</span> math_ops.reduce_sum(sequence_loss_by_example(\n        logits, targets, weights,\n        <span class=\"pl-v\">average_across_timesteps</span><span class=\"pl-k\">=</span>average_across_timesteps,\n        <span class=\"pl-v\">softmax_loss_function</span><span class=\"pl-k\">=</span>softmax_loss_function))\n    <span class=\"pl-k\">if</span> average_across_batch:\n      batch_size <span class=\"pl-k\">=</span> array_ops.shape(targets[<span class=\"pl-c1\">0</span>])[<span class=\"pl-c1\">0</span>]\n      <span class=\"pl-k\">return</span> cost <span class=\"pl-k\">/</span> math_ops.cast(batch_size, dtypes.float32)\n    <span class=\"pl-k\">else</span>:\n      <span class=\"pl-k\">return</span> cost\n</pre></div>\n<p>I can post on this on Overflow, but I do believe it is a bug. I think inherently some of the gradients are complex because i'm doing several complex computations for the associative LSTM (and likewise would be doing the same for the unitary RNN).</p>\n<p>Associative LSTM paper<br>\n<a href=\"https://arxiv.org/abs/1602.03032\" rel=\"nofollow\">https://arxiv.org/abs/1602.03032</a></p>\n<p>Here is an example of part of the cell:</p>\n<div class=\"highlight highlight-source-python\"><pre>        <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>in_gate, out_gate, forget_gate have dim/2 each<span class=\"pl-pds\">'''</span></span>\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gate_multiplication<span class=\"pl-pds\">\"</span></span>):\n          concat <span class=\"pl-k\">=</span> lfe.enhanced_linear([inputs, prev_state], <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">1.5</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>._num_units), <span class=\"pl-c1\">True</span>, <span class=\"pl-v\">weight_initializer</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._weight_initializer, <span class=\"pl-v\">complex_weights</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n          in_gate, out_gate, forget_gate <span class=\"pl-k\">=</span> tf.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, concat)\n          in_gate, out_gate, forget_gate <span class=\"pl-k\">=</span> tf.sigmoid(in_gate), tf.sigmoid(out_gate), tf.sigmoid(forget_gate <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>._forget_bias)\n\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>key_multiplication<span class=\"pl-pds\">\"</span></span>):\n          <span class=\"pl-c\"><span class=\"pl-c\">#</span>in_key, out_key, and u are double the size -- 3 units</span>\n          concat <span class=\"pl-k\">=</span> lfe.enhanced_linear([inputs, prev_state], <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">1.5</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>._num_units), <span class=\"pl-c1\">True</span>, <span class=\"pl-v\">weight_initializer</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._weight_initializer, <span class=\"pl-v\">complex_weights</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n          in_key, out_key, u <span class=\"pl-k\">=</span> tf.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, concat)\n          in_key, out_key, u <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.bound(in_key), <span class=\"pl-c1\">self</span>.bound(out_key), <span class=\"pl-c1\">self</span>.bound(u) <span class=\"pl-c\"><span class=\"pl-c\">#</span>dimension might need to be higher here</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>B = Batch size F = dimension size</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> B x F --&gt; C x B x F</span>\n        in_keys <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.permute(in_key)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> B x F --&gt; C x B x F</span>\n        out_keys <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.permute(out_key)\n\n        <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>equation 27 -- forming next_cells<span class=\"pl-pds\">'''</span></span>\n        f_x_c <span class=\"pl-k\">=</span> tf.expand_dims(forget_gate,<span class=\"pl-c1\">0</span>) <span class=\"pl-k\">*</span> c <span class=\"pl-c\"><span class=\"pl-c\">#</span>takes on shape of c</span>\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>u<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>, u)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>in_gate<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>,in_gate)\n        i_x_u <span class=\"pl-k\">=</span> tf.expand_dims(in_gate <span class=\"pl-k\">*</span> u,<span class=\"pl-c1\">0</span>) \n        next_cells <span class=\"pl-k\">=</span> f_x_c <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.complex_mult(in_keys, i_x_u)</pre></div>\n<p>I guess my question is: are complex gradients possible? And if not, can these be implemented in soon? There are many applications that complex gradients would immensely benefit from. Currently non-complex versions take far longer to run in tensorflow and its very inefficient.</p>\n<p>When I look at the list of gradients, some of them are complex64 dtypes. This is despite making the actual trainable variables both real.</p>", "body_text": "@ibab, the loss function is the seq2seq sequence loss function from tensorflow ops:\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None, name=None):\n  \"\"\"Weighted cross-entropy loss for a sequence of logits (per example).\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, default: \"sequence_loss_by_example\".\n  Returns:\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  \"\"\"\n  if len(targets) != len(logits) or len(weights) != len(logits):\n    raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n  with ops.op_scope(logits + targets + weights, name,\n                    \"sequence_loss_by_example\"):\n    log_perp_list = []\n    for logit, target, weight in zip(logits, targets, weights):\n      if softmax_loss_function is None:\n        # TODO(irving,ebrevdo): This reshape is needed because\n        # sequence_loss_by_example is called with scalars sometimes, which\n        # violates our general scalar strictness policy.\n        target = array_ops.reshape(target, [-1])\n        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n            logit, target)\n      else:\n        crossent = softmax_loss_function(logit, target)\n      log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = math_ops.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sequence_loss(logits, targets, weights,\n                  average_across_timesteps=True, average_across_batch=True,\n                  softmax_loss_function=None, name=None):\n  \"\"\"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, defaults to \"sequence_loss\".\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  \"\"\"\n  with ops.op_scope(logits + targets + weights, name, \"sequence_loss\"):\n    cost = math_ops.reduce_sum(sequence_loss_by_example(\n        logits, targets, weights,\n        average_across_timesteps=average_across_timesteps,\n        softmax_loss_function=softmax_loss_function))\n    if average_across_batch:\n      batch_size = array_ops.shape(targets[0])[0]\n      return cost / math_ops.cast(batch_size, dtypes.float32)\n    else:\n      return cost\n\nI can post on this on Overflow, but I do believe it is a bug. I think inherently some of the gradients are complex because i'm doing several complex computations for the associative LSTM (and likewise would be doing the same for the unitary RNN).\nAssociative LSTM paper\nhttps://arxiv.org/abs/1602.03032\nHere is an example of part of the cell:\n        '''in_gate, out_gate, forget_gate have dim/2 each'''\n        with tf.variable_scope(\"gate_multiplication\"):\n          concat = lfe.enhanced_linear([inputs, prev_state], int(1.5 * self._num_units), True, weight_initializer = self._weight_initializer, complex_weights = True)\n          in_gate, out_gate, forget_gate = tf.split(1, 3, concat)\n          in_gate, out_gate, forget_gate = tf.sigmoid(in_gate), tf.sigmoid(out_gate), tf.sigmoid(forget_gate + self._forget_bias)\n\n        with tf.variable_scope(\"key_multiplication\"):\n          #in_key, out_key, and u are double the size -- 3 units\n          concat = lfe.enhanced_linear([inputs, prev_state], int(1.5 * self._num_units), True, weight_initializer = self._weight_initializer, complex_weights = True)\n          in_key, out_key, u = tf.split(1, 3, concat)\n          in_key, out_key, u = self.bound(in_key), self.bound(out_key), self.bound(u) #dimension might need to be higher here\n\n        #B = Batch size F = dimension size\n        # B x F --> C x B x F\n        in_keys = self.permute(in_key)\n        # B x F --> C x B x F\n        out_keys = self.permute(out_key)\n\n        '''equation 27 -- forming next_cells'''\n        f_x_c = tf.expand_dims(forget_gate,0) * c #takes on shape of c\n        print('u\\n', u)\n        print('in_gate\\n',in_gate)\n        i_x_u = tf.expand_dims(in_gate * u,0) \n        next_cells = f_x_c + self.complex_mult(in_keys, i_x_u)\nI guess my question is: are complex gradients possible? And if not, can these be implemented in soon? There are many applications that complex gradients would immensely benefit from. Currently non-complex versions take far longer to run in tensorflow and its very inefficient.\nWhen I look at the list of gradients, some of them are complex64 dtypes. This is despite making the actual trainable variables both real.", "body": "@ibab, the loss function is the seq2seq sequence loss function from tensorflow ops:\n\n``` python\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None, name=None):\n  \"\"\"Weighted cross-entropy loss for a sequence of logits (per example).\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, default: \"sequence_loss_by_example\".\n  Returns:\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  \"\"\"\n  if len(targets) != len(logits) or len(weights) != len(logits):\n    raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n  with ops.op_scope(logits + targets + weights, name,\n                    \"sequence_loss_by_example\"):\n    log_perp_list = []\n    for logit, target, weight in zip(logits, targets, weights):\n      if softmax_loss_function is None:\n        # TODO(irving,ebrevdo): This reshape is needed because\n        # sequence_loss_by_example is called with scalars sometimes, which\n        # violates our general scalar strictness policy.\n        target = array_ops.reshape(target, [-1])\n        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n            logit, target)\n      else:\n        crossent = softmax_loss_function(logit, target)\n      log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = math_ops.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sequence_loss(logits, targets, weights,\n                  average_across_timesteps=True, average_across_batch=True,\n                  softmax_loss_function=None, name=None):\n  \"\"\"Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n    name: Optional name for this operation, defaults to \"sequence_loss\".\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  \"\"\"\n  with ops.op_scope(logits + targets + weights, name, \"sequence_loss\"):\n    cost = math_ops.reduce_sum(sequence_loss_by_example(\n        logits, targets, weights,\n        average_across_timesteps=average_across_timesteps,\n        softmax_loss_function=softmax_loss_function))\n    if average_across_batch:\n      batch_size = array_ops.shape(targets[0])[0]\n      return cost / math_ops.cast(batch_size, dtypes.float32)\n    else:\n      return cost\n\n```\n\nI can post on this on Overflow, but I do believe it is a bug. I think inherently some of the gradients are complex because i'm doing several complex computations for the associative LSTM (and likewise would be doing the same for the unitary RNN). \n\nAssociative LSTM paper\nhttps://arxiv.org/abs/1602.03032\n\nHere is an example of part of the cell:\n\n``` python\n        '''in_gate, out_gate, forget_gate have dim/2 each'''\n        with tf.variable_scope(\"gate_multiplication\"):\n          concat = lfe.enhanced_linear([inputs, prev_state], int(1.5 * self._num_units), True, weight_initializer = self._weight_initializer, complex_weights = True)\n          in_gate, out_gate, forget_gate = tf.split(1, 3, concat)\n          in_gate, out_gate, forget_gate = tf.sigmoid(in_gate), tf.sigmoid(out_gate), tf.sigmoid(forget_gate + self._forget_bias)\n\n        with tf.variable_scope(\"key_multiplication\"):\n          #in_key, out_key, and u are double the size -- 3 units\n          concat = lfe.enhanced_linear([inputs, prev_state], int(1.5 * self._num_units), True, weight_initializer = self._weight_initializer, complex_weights = True)\n          in_key, out_key, u = tf.split(1, 3, concat)\n          in_key, out_key, u = self.bound(in_key), self.bound(out_key), self.bound(u) #dimension might need to be higher here\n\n        #B = Batch size F = dimension size\n        # B x F --> C x B x F\n        in_keys = self.permute(in_key)\n        # B x F --> C x B x F\n        out_keys = self.permute(out_key)\n\n        '''equation 27 -- forming next_cells'''\n        f_x_c = tf.expand_dims(forget_gate,0) * c #takes on shape of c\n        print('u\\n', u)\n        print('in_gate\\n',in_gate)\n        i_x_u = tf.expand_dims(in_gate * u,0) \n        next_cells = f_x_c + self.complex_mult(in_keys, i_x_u)\n```\n\nI guess my question is: are complex gradients possible? And if not, can these be implemented in soon? There are many applications that complex gradients would immensely benefit from. Currently non-complex versions take far longer to run in tensorflow and its very inefficient. \n\nWhen I look at the list of gradients, some of them are complex64 dtypes. This is despite making the actual trainable variables both real. \n"}