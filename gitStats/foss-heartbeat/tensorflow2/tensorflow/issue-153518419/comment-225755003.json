{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225755003", "html_url": "https://github.com/tensorflow/tensorflow/issues/2255#issuecomment-225755003", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2255", "id": 225755003, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTc1NTAwMw==", "user": {"login": "khaotik", "id": 6271084, "node_id": "MDQ6VXNlcjYyNzEwODQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/6271084?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khaotik", "html_url": "https://github.com/khaotik", "followers_url": "https://api.github.com/users/khaotik/followers", "following_url": "https://api.github.com/users/khaotik/following{/other_user}", "gists_url": "https://api.github.com/users/khaotik/gists{/gist_id}", "starred_url": "https://api.github.com/users/khaotik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khaotik/subscriptions", "organizations_url": "https://api.github.com/users/khaotik/orgs", "repos_url": "https://api.github.com/users/khaotik/repos", "events_url": "https://api.github.com/users/khaotik/events{/privacy}", "received_events_url": "https://api.github.com/users/khaotik/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T01:20:13Z", "updated_at": "2016-06-15T01:08:45Z", "author_association": "NONE", "body_html": "<p>@LeavesBreathe In my TF unitary RNN implementation, I had real and complex parameters, and my cost function was real. It seems working well. However I'm not sure if the above \"cast away imag part gradient\" runtime issue occurred in my implementation.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> math <span class=\"pl-k\">import</span> sqrt\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> variable_scope <span class=\"pl-k\">as</span> vs\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> array_ops\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> math_ops\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> nn_ops\n\nrnn_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell\nseq2seq <span class=\"pl-k\">=</span> tf.nn.seq2seq\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">complex_mul_real</span>( <span class=\"pl-smi\">c</span>, <span class=\"pl-smi\">r</span> ):\n    <span class=\"pl-k\">return</span> tf.complex(tf.real(c)<span class=\"pl-k\">*</span>r, tf.imag(c)<span class=\"pl-k\">*</span>r)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">refl_c</span>(<span class=\"pl-smi\">in_</span>, <span class=\"pl-smi\">normal_</span>):\n    normal_rk2 <span class=\"pl-k\">=</span> tf.expand_dims( normal_, <span class=\"pl-c1\">1</span> )\n    scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>tf.matmul( in_, tf.conj( normal_rk2 ) )\n    <span class=\"pl-k\">return</span> in_ <span class=\"pl-k\">-</span> tf.matmul(scale, tf.transpose(normal_rk2))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">complex_abs_sq</span>(<span class=\"pl-smi\">z</span>):\n    <span class=\"pl-k\">return</span> tf.real(z)<span class=\"pl-k\">*</span>tf.real(z) <span class=\"pl-k\">+</span> tf.imag(z)<span class=\"pl-k\">*</span>tf.imag(z)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">normalize_c</span>(<span class=\"pl-smi\">in_</span>):\n    norm <span class=\"pl-k\">=</span> tf.sqrt(tf.reduce_sum(complex_abs_sq(in_)))\n    scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>(norm <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1e-5</span>)\n    <span class=\"pl-k\">return</span> complex_mul_real( in_, scale )\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_complex_variable</span>( <span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">scope</span>, <span class=\"pl-smi\">shape</span> ):\n    re, im <span class=\"pl-k\">=</span> vs.get_variable(name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_re<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape), vs.get_variable(name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_im<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape)\n    <span class=\"pl-k\">return</span> tf.complex(re,im, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name)\n\n\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span>multiply complex vector by parameterized unitary matrix<span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">ulinear</span>(<span class=\"pl-smi\">vec_in</span>, <span class=\"pl-smi\">out_size</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    shape <span class=\"pl-k\">=</span> vec_in.get_shape().as_list()\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(shape) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">2</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>( <span class=\"pl-s\"><span class=\"pl-pds\">'</span>argument vec_in must be a batch of vectors (2D tensor)<span class=\"pl-pds\">'</span></span> )\n    in_size <span class=\"pl-k\">=</span> shape[<span class=\"pl-c1\">1</span>]\n    fft_scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>sqrt(out_size)\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ULinear<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> _s:\n        diag0 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>diag0<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size])\n        diag1 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>diag1<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size])\n        diag2 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>diag2<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size])\n        refl0 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>refl0<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size])\n        refl1 <span class=\"pl-k\">=</span> get_complex_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>refl1<span class=\"pl-pds\">'</span></span>, _s, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[out_size])\n        perm0 <span class=\"pl-k\">=</span> tf.constant(np.random.permutation(out_size), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>perm0<span class=\"pl-pds\">'</span></span>,<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>int32<span class=\"pl-pds\">'</span></span>)\n        out_ <span class=\"pl-k\">=</span> vec_in <span class=\"pl-k\">*</span> diag0\n        refl0 <span class=\"pl-k\">=</span> normalize_c(refl0)\n        refl1 <span class=\"pl-k\">=</span> normalize_c(refl1)\n        out_ <span class=\"pl-k\">=</span> refl_c(math_ops.batch_fft(out_)<span class=\"pl-k\">*</span>fft_scale,refl0)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>out_ = math_ops.batch_fft(out_)*fft_scale</span>\n        out_ <span class=\"pl-k\">=</span> diag1 <span class=\"pl-k\">*</span> tf.transpose(tf.gather(tf.transpose(out_), perm0))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>out_ = diag1 * out_</span>\n        out_ <span class=\"pl-k\">=</span> diag2 <span class=\"pl-k\">*</span> refl_c(math_ops.batch_ifft(out_)<span class=\"pl-k\">*</span>fft_scale, refl1)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>out_ = diag2 * math_ops.batch_ifft(out_)*fft_scale</span>\n        <span class=\"pl-k\">return</span> out_\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">modReLU</span>(<span class=\"pl-smi\">in_c</span>, <span class=\"pl-smi\">bias</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ULinear<span class=\"pl-pds\">\"</span></span>):\n        n <span class=\"pl-k\">=</span> tf.complex_abs(in_c)\n        scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>(n<span class=\"pl-k\">+</span><span class=\"pl-c1\">1e-5</span>)\n    <span class=\"pl-k\">return</span> complex_mul_real(in_c, ( nn_ops.relu(n<span class=\"pl-k\">+</span>bias)<span class=\"pl-k\">*</span>scale ))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">abssigm</span>(<span class=\"pl-smi\">in_c</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ULinear<span class=\"pl-pds\">'</span></span>):\n        re,im <span class=\"pl-k\">=</span> tf.real(in_c), tf.imag(in_c)\n        scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.<span class=\"pl-k\">/</span>tf.sqrt(re<span class=\"pl-k\">*</span>re <span class=\"pl-k\">+</span> im<span class=\"pl-k\">*</span>im <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>.)\n    <span class=\"pl-k\">return</span> complex_mul_real(in_c,scale)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">URNNCell</span>(<span class=\"pl-e\">rnn_cell</span>.<span class=\"pl-e\">RNNCell</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>._num_units <span class=\"pl-k\">=</span> num_units\n        <span class=\"pl-c1\">self</span>._input_size <span class=\"pl-k\">=</span> num_units <span class=\"pl-k\">if</span> input_size<span class=\"pl-k\">==</span><span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> input_size\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">input_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._input_size\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">output_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span> ):\n        <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__name__</span>):\n            mat_in <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mat_in<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">self</span>.input_size, <span class=\"pl-c1\">self</span>.state_size<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>])\n            mat_out <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mat_out<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">self</span>.state_size<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">self</span>.output_size])\n            in_proj <span class=\"pl-k\">=</span> math_ops.matmul(inputs, mat_in)\n            in_proj_c <span class=\"pl-k\">=</span> tf.complex( in_proj[:, :<span class=\"pl-c1\">self</span>.state_size], in_proj[:, <span class=\"pl-c1\">self</span>.state_size:] )\n            out_state <span class=\"pl-k\">=</span> modReLU( in_proj_c <span class=\"pl-k\">+</span> \n                ulinear(state, <span class=\"pl-c1\">self</span>.state_size),\n                vs.get_variable(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">self</span>.state_size]),\n                <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope)\n            <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">            out_state = abssigm( in_proj_c +</span>\n<span class=\"pl-s\">                                ulinear(state, self.state_size),</span>\n<span class=\"pl-s\">                                scope=scope )</span>\n<span class=\"pl-s\">            <span class=\"pl-pds\">'''</span></span>\n            out_ <span class=\"pl-k\">=</span> math_ops.matmul( array_ops.concat(<span class=\"pl-c1\">1</span>,[tf.real(out_state), tf.imag(out_state)] ), mat_out)\n        <span class=\"pl-k\">return</span> out_, out_state</pre></div>", "body_text": "@LeavesBreathe In my TF unitary RNN implementation, I had real and complex parameters, and my cost function was real. It seems working well. However I'm not sure if the above \"cast away imag part gradient\" runtime issue occurred in my implementation.\nfrom math import sqrt\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\n\nrnn_cell = tf.nn.rnn_cell\nseq2seq = tf.nn.seq2seq\n\ndef complex_mul_real( c, r ):\n    return tf.complex(tf.real(c)*r, tf.imag(c)*r)\n\n\ndef refl_c(in_, normal_):\n    normal_rk2 = tf.expand_dims( normal_, 1 )\n    scale = 2*tf.matmul( in_, tf.conj( normal_rk2 ) )\n    return in_ - tf.matmul(scale, tf.transpose(normal_rk2))\n\n\ndef complex_abs_sq(z):\n    return tf.real(z)*tf.real(z) + tf.imag(z)*tf.imag(z)\n\n\ndef normalize_c(in_):\n    norm = tf.sqrt(tf.reduce_sum(complex_abs_sq(in_)))\n    scale = 1./(norm + 1e-5)\n    return complex_mul_real( in_, scale )\n\n\ndef get_complex_variable( name, scope, shape ):\n    re, im = vs.get_variable(name+'_re', shape=shape), vs.get_variable(name+'_im', shape=shape)\n    return tf.complex(re,im, name=name)\n\n\n'''multiply complex vector by parameterized unitary matrix'''\ndef ulinear(vec_in, out_size, scope=None):\n    shape = vec_in.get_shape().as_list()\n    if len(shape) != 2:\n        raise ValueError( 'argument vec_in must be a batch of vectors (2D tensor)' )\n    in_size = shape[1]\n    fft_scale = 1./sqrt(out_size)\n    with vs.variable_scope(scope or 'ULinear') as _s:\n        diag0 = get_complex_variable('diag0', _s, shape=[out_size])\n        diag1 = get_complex_variable('diag1', _s, shape=[out_size])\n        diag2 = get_complex_variable('diag2', _s, shape=[out_size])\n        refl0 = get_complex_variable('refl0', _s, shape=[out_size])\n        refl1 = get_complex_variable('refl1', _s, shape=[out_size])\n        perm0 = tf.constant(np.random.permutation(out_size), name='perm0',dtype='int32')\n        out_ = vec_in * diag0\n        refl0 = normalize_c(refl0)\n        refl1 = normalize_c(refl1)\n        out_ = refl_c(math_ops.batch_fft(out_)*fft_scale,refl0)\n        #out_ = math_ops.batch_fft(out_)*fft_scale\n        out_ = diag1 * tf.transpose(tf.gather(tf.transpose(out_), perm0))\n        #out_ = diag1 * out_\n        out_ = diag2 * refl_c(math_ops.batch_ifft(out_)*fft_scale, refl1)\n        #out_ = diag2 * math_ops.batch_ifft(out_)*fft_scale\n        return out_\n\n\ndef modReLU(in_c, bias, scope=None):\n    with vs.variable_scope(scope or \"ULinear\"):\n        n = tf.complex_abs(in_c)\n        scale = 1./(n+1e-5)\n    return complex_mul_real(in_c, ( nn_ops.relu(n+bias)*scale ))\n\n\ndef abssigm(in_c, scope=None):\n    with vs.variable_scope(scope or 'ULinear'):\n        re,im = tf.real(in_c), tf.imag(in_c)\n        scale = 1./tf.sqrt(re*re + im*im + 1.)\n    return complex_mul_real(in_c,scale)\n\n\nclass URNNCell(rnn_cell.RNNCell):\n    def __init__(self, num_units, input_size=None):\n        self._num_units = num_units\n        self._input_size = num_units if input_size==None else input_size\n\n    @property\n    def input_size(self):\n        return self._input_size\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None ):\n        with vs.variable_scope(scope or type(self).__name__):\n            mat_in = vs.get_variable('mat_in', [self.input_size, self.state_size*2])\n            mat_out = vs.get_variable('mat_out', [self.state_size*2, self.output_size])\n            in_proj = math_ops.matmul(inputs, mat_in)\n            in_proj_c = tf.complex( in_proj[:, :self.state_size], in_proj[:, self.state_size:] )\n            out_state = modReLU( in_proj_c + \n                ulinear(state, self.state_size),\n                vs.get_variable(name='bias', dtype=tf.float32, shape=[self.state_size]),\n                scope=scope)\n            '''\n            out_state = abssigm( in_proj_c +\n                                ulinear(state, self.state_size),\n                                scope=scope )\n            '''\n            out_ = math_ops.matmul( array_ops.concat(1,[tf.real(out_state), tf.imag(out_state)] ), mat_out)\n        return out_, out_state", "body": "@LeavesBreathe In my TF unitary RNN implementation, I had real and complex parameters, and my cost function was real. It seems working well. However I'm not sure if the above \"cast away imag part gradient\" runtime issue occurred in my implementation.\n\n``` python\nfrom math import sqrt\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\n\nrnn_cell = tf.nn.rnn_cell\nseq2seq = tf.nn.seq2seq\n\ndef complex_mul_real( c, r ):\n    return tf.complex(tf.real(c)*r, tf.imag(c)*r)\n\n\ndef refl_c(in_, normal_):\n    normal_rk2 = tf.expand_dims( normal_, 1 )\n    scale = 2*tf.matmul( in_, tf.conj( normal_rk2 ) )\n    return in_ - tf.matmul(scale, tf.transpose(normal_rk2))\n\n\ndef complex_abs_sq(z):\n    return tf.real(z)*tf.real(z) + tf.imag(z)*tf.imag(z)\n\n\ndef normalize_c(in_):\n    norm = tf.sqrt(tf.reduce_sum(complex_abs_sq(in_)))\n    scale = 1./(norm + 1e-5)\n    return complex_mul_real( in_, scale )\n\n\ndef get_complex_variable( name, scope, shape ):\n    re, im = vs.get_variable(name+'_re', shape=shape), vs.get_variable(name+'_im', shape=shape)\n    return tf.complex(re,im, name=name)\n\n\n'''multiply complex vector by parameterized unitary matrix'''\ndef ulinear(vec_in, out_size, scope=None):\n    shape = vec_in.get_shape().as_list()\n    if len(shape) != 2:\n        raise ValueError( 'argument vec_in must be a batch of vectors (2D tensor)' )\n    in_size = shape[1]\n    fft_scale = 1./sqrt(out_size)\n    with vs.variable_scope(scope or 'ULinear') as _s:\n        diag0 = get_complex_variable('diag0', _s, shape=[out_size])\n        diag1 = get_complex_variable('diag1', _s, shape=[out_size])\n        diag2 = get_complex_variable('diag2', _s, shape=[out_size])\n        refl0 = get_complex_variable('refl0', _s, shape=[out_size])\n        refl1 = get_complex_variable('refl1', _s, shape=[out_size])\n        perm0 = tf.constant(np.random.permutation(out_size), name='perm0',dtype='int32')\n        out_ = vec_in * diag0\n        refl0 = normalize_c(refl0)\n        refl1 = normalize_c(refl1)\n        out_ = refl_c(math_ops.batch_fft(out_)*fft_scale,refl0)\n        #out_ = math_ops.batch_fft(out_)*fft_scale\n        out_ = diag1 * tf.transpose(tf.gather(tf.transpose(out_), perm0))\n        #out_ = diag1 * out_\n        out_ = diag2 * refl_c(math_ops.batch_ifft(out_)*fft_scale, refl1)\n        #out_ = diag2 * math_ops.batch_ifft(out_)*fft_scale\n        return out_\n\n\ndef modReLU(in_c, bias, scope=None):\n    with vs.variable_scope(scope or \"ULinear\"):\n        n = tf.complex_abs(in_c)\n        scale = 1./(n+1e-5)\n    return complex_mul_real(in_c, ( nn_ops.relu(n+bias)*scale ))\n\n\ndef abssigm(in_c, scope=None):\n    with vs.variable_scope(scope or 'ULinear'):\n        re,im = tf.real(in_c), tf.imag(in_c)\n        scale = 1./tf.sqrt(re*re + im*im + 1.)\n    return complex_mul_real(in_c,scale)\n\n\nclass URNNCell(rnn_cell.RNNCell):\n    def __init__(self, num_units, input_size=None):\n        self._num_units = num_units\n        self._input_size = num_units if input_size==None else input_size\n\n    @property\n    def input_size(self):\n        return self._input_size\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None ):\n        with vs.variable_scope(scope or type(self).__name__):\n            mat_in = vs.get_variable('mat_in', [self.input_size, self.state_size*2])\n            mat_out = vs.get_variable('mat_out', [self.state_size*2, self.output_size])\n            in_proj = math_ops.matmul(inputs, mat_in)\n            in_proj_c = tf.complex( in_proj[:, :self.state_size], in_proj[:, self.state_size:] )\n            out_state = modReLU( in_proj_c + \n                ulinear(state, self.state_size),\n                vs.get_variable(name='bias', dtype=tf.float32, shape=[self.state_size]),\n                scope=scope)\n            '''\n            out_state = abssigm( in_proj_c +\n                                ulinear(state, self.state_size),\n                                scope=scope )\n            '''\n            out_ = math_ops.matmul( array_ops.concat(1,[tf.real(out_state), tf.imag(out_state)] ), mat_out)\n        return out_, out_state\n```\n"}