{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225590752", "html_url": "https://github.com/tensorflow/tensorflow/issues/2255#issuecomment-225590752", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2255", "id": 225590752, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTU5MDc1Mg==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-13T14:05:32Z", "updated_at": "2016-06-13T14:05:32Z", "author_association": "NONE", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a> -- the Associative LSTM paper was done in Theano and their code is here. They do not use any complex variables, but rather simulate it with real ones. This can be done in tensorflow as well, but for unitary RNN's this can not be done since the <code>ifft</code> args require complex inputs.</p>\n<p><a href=\"https://github.com/mohammadpz/Associative_LSTM\">https://github.com/mohammadpz/Associative_LSTM</a></p>\n<p>Basically the main issue with using non-complex variables is that the implementation is much slower. If complex gradients could be handled, this would be a major advantage for tensorflow, and would open many doors.</p>", "body_text": "Thanks @ibab -- the Associative LSTM paper was done in Theano and their code is here. They do not use any complex variables, but rather simulate it with real ones. This can be done in tensorflow as well, but for unitary RNN's this can not be done since the ifft args require complex inputs.\nhttps://github.com/mohammadpz/Associative_LSTM\nBasically the main issue with using non-complex variables is that the implementation is much slower. If complex gradients could be handled, this would be a major advantage for tensorflow, and would open many doors.", "body": "Thanks @ibab -- the Associative LSTM paper was done in Theano and their code is here. They do not use any complex variables, but rather simulate it with real ones. This can be done in tensorflow as well, but for unitary RNN's this can not be done since the `ifft` args require complex inputs.\n\nhttps://github.com/mohammadpz/Associative_LSTM\n\nBasically the main issue with using non-complex variables is that the implementation is much slower. If complex gradients could be handled, this would be a major advantage for tensorflow, and would open many doors.  \n"}