{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/184862362", "pull_request_review_id": 116146049, "id": 184862362, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NDg2MjM2Mg==", "diff_hunk": "@@ -0,0 +1,236 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_util.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n+\n+using tensorflow::sparse::SparseTensor;\n+\n+namespace tensorflow {\n+\n+struct ReduceDetails {\n+  // The dimensions to call Reorder() with.\n+  std::vector<int64> reorder_dims;\n+  // The dimensions to call group() with after Reorder().\n+  std::vector<int64> group_by_dims;\n+  // The shape after reduction.\n+  TensorShape reduced_shape;\n+};\n+\n+ReduceDetails SparseTensorReduceHelper(const SparseTensor& sp,\n+                                       gtl::ArraySlice<int32> axes_slice) {\n+  ReduceDetails reduction;\n+\n+  std::vector<int32> reduction_axes(axes_slice.begin(), axes_slice.end());\n+  int ndims = sp.dims();\n+  for (int64 i = 0; i < reduction_axes.size(); ++i) {\n+    reduction_axes[i] = (reduction_axes[i] + ndims) % ndims;\n+  }\n+  std::sort(reduction_axes.begin(), reduction_axes.end());\n+\n+  // (0) Calculate the grouping dimensions:\n+  // group_by_dims == {0, .., NDIMS-1} \\ reduction_axes.\n+  std::vector<int64> perm(ndims);\n+  std::iota(perm.begin(), perm.end(), 0);\n+\n+  // Requires perm and reduction_axes_ be sorted; group_by_dims will be\n+  // sorted as well.\n+  std::set_difference(\n+      perm.begin(), perm.end(), reduction_axes.begin(), reduction_axes.end(),\n+      std::inserter(reduction.group_by_dims, reduction.group_by_dims.begin()));\n+\n+  // Now append the rest of the axes (the complement of group_by_dims_);\n+  // result is used by Reorder().\n+  reduction.reorder_dims = reduction.group_by_dims;\n+  std::set_difference(perm.begin(), perm.end(), reduction.group_by_dims.begin(),\n+                      reduction.group_by_dims.end(),\n+                      std::back_inserter(reduction.reorder_dims));\n+\n+  // (1) Calculate the shape after reduction.\n+  auto sp_shape = sp.shape();\n+  std::vector<int64> out_dim_sizes;\n+  out_dim_sizes = sp.PickDims(reduction.group_by_dims);\n+\n+  reduction.reduced_shape = TensorShape(out_dim_sizes);\n+  return reduction;\n+}\n+\n+std::vector<int64> getRowOfTensor(const Tensor t, const int64 i) {\n+  std::vector<int64> res;\n+  auto t_values = t.matrix<int64>();\n+  for (int64 j = 0; j < t.shape().dim_size(1); j++) {\n+    res.push_back(t_values(i, j));\n+  }\n+  return res;\n+}\n+\n+// This operator is used for tiling a to a new SparseTensor like b\n+template <typename T>\n+class SparseTileLikeOp : public OpKernel {\n+ public:\n+  explicit SparseTileLikeOp(OpKernelConstruction* context)\n+      : OpKernel(context) {}\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    // define tensors\n+    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b_indices_t,\n+        *b_values_t, *b_shape_t, *axes_t;\n+\n+    OP_REQUIRES_OK(ctx, ctx->input(\"a_input_indices\", &a_indices_t));\n+    OP_REQUIRES_OK(ctx, ctx->input(\"a_input_values\", &a_values_t));\n+    OP_REQUIRES_OK(ctx, ctx->input(\"a_input_shape\", &a_shape_t));\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b_input_indices\", &b_indices_t));\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b_input_values\", &b_values_t));\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b_input_shape\", &b_shape_t));\n+    OP_REQUIRES_OK(ctx, ctx->input(\"axes\", &axes_t));\n+\n+    ValidateInput(ctx, a_indices_t, a_values_t, a_shape_t, b_indices_t,\n+                  b_values_t, b_shape_t, axes_t);\n+\n+    // set values for out_values\n+    std::vector<int64> group_axes;\n+    std::vector<int64> reduction_a;\n+    auto axes = axes_t->flat<int32>();\n+    int64 a = 0;\n+    for (int32 i = 0; i < b_shape_t->dim_size(0); i++) {\n+      if (i != axes(0)) {\n+        group_axes.push_back(i);\n+        reduction_a.push_back(a);\n+        a++;\n+      }\n+    }\n+\n+    SparseTensor sp_b(tensor::DeepCopy(*b_indices_t),\n+                      tensor::DeepCopy(*b_values_t),\n+                      TensorShape(b_shape_t->vec<int64>()));\n+    ReduceDetails reduction =\n+        SparseTensorReduceHelper(sp_b, axes_t->flat<int32>());\n+    sp_b.Reorder<T>(reduction.reorder_dims);\n+\n+    SparseTensor sp_a(tensor::DeepCopy(*a_indices_t),\n+                      tensor::DeepCopy(*a_values_t),\n+                      TensorShape(a_shape_t->vec<int64>()));\n+    sp_a.Reorder<T>(reduction_a);\n+\n+    std::vector<T> out_values_vec;\n+    int64 h = 0;\n+    int64 output_shape0 = 0;\n+    int64 tmp_count = 0;\n+    std::vector<int64> output_ids;\n+    auto a_values = a_values_t->vec<T>();\n+\n+    for (const auto& g : sp_b.group(group_axes)) {\n+      std::vector<int64> group = g.group();\n+      std::vector<int64> row = getRowOfTensor(sp_a.indices(), h);\n+      int64 g_indice_len = g.values<T>().dimension(0);\n+      while (row < group) {\n+        if (h > a_values.size()) break;\n+        h++;\n+        row = getRowOfTensor(sp_a.indices(), h);\n+      }\n+      if (row == group) {\n+        auto s = a_values(h);\n+        for (int64 j = 0; j < g_indice_len; j++) {\n+          out_values_vec.push_back(s);\n+          output_ids.push_back(tmp_count + j);\n+        }\n+        output_shape0 = output_shape0 + g_indice_len;\n+      }\n+      tmp_count = tmp_count + g_indice_len;\n+    }\n+\n+    // allocate output indices first\n+    Tensor* output_indices_t;\n+    auto b_indices = sp_b.indices().matrix<int64>();\n+    auto b_indices_shape = b_indices.dimensions();\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\n+                            0, TensorShape({output_shape0, b_indices_shape[1]}),\n+                            &output_indices_t));\n+\n+    auto output_indices = output_indices_t->matrix<int64>();\n+\n+    for (int64 j = 0; j < output_shape0; j++) {\n+      output_indices.chip<0>(j) = b_indices.chip<0>(output_ids[j]);\n+    }\n+\n+    // allocate output values\n+    Tensor* out_values_t;\n+    const auto output_values_shape = TensorShape({output_shape0});\n+    OP_REQUIRES_OK(ctx,\n+                   ctx->allocate_output(1, output_values_shape, &out_values_t));\n+    auto out_flat = out_values_t->flat<T>();\n+    for (int64 j = 0; j < output_shape0; j++) {\n+      out_flat(j) = out_values_vec[j];\n+    }\n+\n+    // allocate output shape\n+    ctx->set_output(2, *b_shape_t);\n+  }\n+\n+ private:\n+  void ValidateInput(OpKernelContext* context, const Tensor* a_indices_t,\n+                     const Tensor* a_values_t, const Tensor* a_shape_t,\n+                     const Tensor* b_indices_t, const Tensor* b_values_t,\n+                     const Tensor* b_shape_t, const Tensor* axes_t) {\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a_indices_t->shape()),\n+                errors::InvalidArgument(\n+                    \"Input indices should be a matrix but received shape \",\n+                    a_indices_t->shape().DebugString(), \" at position \", 0));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(a_values_t->shape()),\n+                errors::InvalidArgument(\n+                    \"Input values should be a std::vector but received shape \",\n+                    a_values_t->shape().DebugString(), \" at position \", 1));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(a_shape_t->shape()),\n+                errors::InvalidArgument(\n+                    \"Input shapes should be a std::vector but received shape \",\n+                    a_shape_t->shape().DebugString(), \" at position \", 2));\n+\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n+                errors::InvalidArgument(\n+                    \"Input indices should be a matrix but received shape \",\n+                    b_indices_t->shape().DebugString(), \" at position \", 3));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(b_values_t->shape()),\n+                errors::InvalidArgument(\n+                    \"Input values should be a std::vector but received shape \",\n+                    b_values_t->shape().DebugString(), \" at position \", 4));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(b_shape_t->shape()),\n+                errors::InvalidArgument(\n+                    \"Input shapes should be a std::vector but received shape \",\n+                    b_shape_t->shape().DebugString(), \" at position \", 5));\n+    OP_REQUIRES(\n+        context,\n+        b_shape_t->vec<int64>().size() == a_shape_t->vec<int64>().size() + 1,\n+        errors::InvalidArgument(\n+            \"shape of tensor a should be one dim less than tensor b, but got \",\n+            a_shape_t->shape().DebugString(), \" and \",\n+            b_shape_t->shape().DebugString()));\n+  }\n+\n+};  // class", "path": "tensorflow/contrib/layers/kernels/sparse_tile_like_kernel.cc", "position": null, "original_position": 227, "commit_id": "65961683050079b9bb8d2c6a66120dd9d476cc07", "original_commit_id": "d3dbb9cc1c68e6c1c16a2a038ea91fad4cadd4f2", "user": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "body": "remove comment", "created_at": "2018-04-28T18:56:45Z", "updated_at": "2018-06-12T08:59:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/15184#discussion_r184862362", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15184", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/184862362"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/15184#discussion_r184862362"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15184"}}, "body_html": "<p>remove comment</p>", "body_text": "remove comment"}