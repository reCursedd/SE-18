{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23469", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23469/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23469/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23469/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23469", "id": 377008066, "node_id": "MDU6SXNzdWUzNzcwMDgwNjY=", "number": 23469, "title": "ModelCheckpoint Callback Doesn't Log Validation Accuracy on ML Engine", "user": {"login": "bclayman", "id": 11317232, "node_id": "MDQ6VXNlcjExMzE3MjMy", "avatar_url": "https://avatars1.githubusercontent.com/u/11317232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bclayman", "html_url": "https://github.com/bclayman", "followers_url": "https://api.github.com/users/bclayman/followers", "following_url": "https://api.github.com/users/bclayman/following{/other_user}", "gists_url": "https://api.github.com/users/bclayman/gists{/gist_id}", "starred_url": "https://api.github.com/users/bclayman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bclayman/subscriptions", "organizations_url": "https://api.github.com/users/bclayman/orgs", "repos_url": "https://api.github.com/users/bclayman/repos", "events_url": "https://api.github.com/users/bclayman/events{/privacy}", "received_events_url": "https://api.github.com/users/bclayman/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097546578, "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras", "name": "comp:keras", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-03T00:52:35Z", "updated_at": "2018-11-21T18:58:49Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ML Engine Instance (Debian)</li>\n<li>TensorFlow installed from (source or binary): source</li>\n<li>TensorFlow version (use command below): 1.10.0</li>\n<li>Python version: 2.7.14</li>\n<li>Bazel version (if compiling from source): N/A</li>\n<li>GCC/Compiler version (if compiling from source): N/A</li>\n<li>CUDA/cuDNN version: N/A</li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>I've set up a CNN and have created callbacks for model checkpointing, TensorBoard visualization, and early stopping.  I have ~408K samples in my <code>tf.data.Dataset</code>, am using a batch size of 256, and train my model with 1,000 steps per epoch with 1000 epochs.  When I do this, everything trains correctly on ML Engine but I don't get logging like I normally do.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>I should see logging on ML Engine that indicates validation accuracy for each epoch, like this:</p>\n<p>Epoch 2/2000<br>\n1/200 [..............................] - ETA: 2:00 - loss: 5.7345 - acc: 0.1055\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd</p>\n<p>2/200...</p>\n<p>When I set <code>steps_per_epoch</code> to be only 200, I <em>do</em> see validation accuracy logging as expected.  For what it's worth, the training still works and produces .h5 files.  But it's crucial that I be able to view logging as the model's training.</p>\n<p>I spoke with folks on the ML Engine team and it's not an issue on their end.  They suspect it's a bug with the <code>tf.keras.callbacks.ModelCheckpoint</code> callback or me misusing <code>steps_per_epoch</code> somehow.  Is there a certain value I shouldn't exceed with <code>steps_per_epoch</code>?  I wanted to do one full pass over the data, so that should be: # samples / (steps_per_epoch * batch_size), which would be ~1,593 steps.  Even 1,000 steps_per_epoch causes this issue though.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<p>Unfortunately, I can't share the data I'm using.  I'm using a CNN built with <code>tf.keras.layers</code> layers (called <code>model</code>) and my compile step, callbacks definitions, and fit step look like this:</p>\n<pre><code>    model.compile(\n        loss='sparse_categorical_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy'],\n    )\n    callbacks = [\n        ModelCheckpoint(\"model_{epoch:04d}_{val_acc:.4f}.h5\",\n                        monitor='val_acc',\n                        verbose=1,\n                        save_best_only=True,\n                        mode='max'),\n        TensorBoard(),\n        EarlyStopping(monitor='val_acc', patience=5, min_delta=0, mode='max')\n    ]\n\n    model.fit(dataset, steps_per_epoch=1000, epochs=1000, validation_data=val_dataset,\n              validation_steps=3, callbacks=callbacks)\n</code></pre>\n<p><strong>Other info / logs</strong></p>\n<p>Here's what I see in the ML Engine logs:</p>\n<p><code>14:24 Epoch 1/4</code></p>\n<p><code>15:00 Starting evaluation</code></p>\n<p>With no logging in between <g-emoji class=\"g-emoji\" alias=\"-1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44e.png\">\ud83d\udc4e</g-emoji></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): ML Engine Instance (Debian)\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.10.0\nPython version: 2.7.14\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\n\nDescribe the current behavior\nI've set up a CNN and have created callbacks for model checkpointing, TensorBoard visualization, and early stopping.  I have ~408K samples in my tf.data.Dataset, am using a batch size of 256, and train my model with 1,000 steps per epoch with 1000 epochs.  When I do this, everything trains correctly on ML Engine but I don't get logging like I normally do.\nDescribe the expected behavior\nI should see logging on ML Engine that indicates validation accuracy for each epoch, like this:\nEpoch 2/2000\n1/200 [..............................] - ETA: 2:00 - loss: 5.7345 - acc: 0.1055\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n2/200...\nWhen I set steps_per_epoch to be only 200, I do see validation accuracy logging as expected.  For what it's worth, the training still works and produces .h5 files.  But it's crucial that I be able to view logging as the model's training.\nI spoke with folks on the ML Engine team and it's not an issue on their end.  They suspect it's a bug with the tf.keras.callbacks.ModelCheckpoint callback or me misusing steps_per_epoch somehow.  Is there a certain value I shouldn't exceed with steps_per_epoch?  I wanted to do one full pass over the data, so that should be: # samples / (steps_per_epoch * batch_size), which would be ~1,593 steps.  Even 1,000 steps_per_epoch causes this issue though.\nCode to reproduce the issue\nUnfortunately, I can't share the data I'm using.  I'm using a CNN built with tf.keras.layers layers (called model) and my compile step, callbacks definitions, and fit step look like this:\n    model.compile(\n        loss='sparse_categorical_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy'],\n    )\n    callbacks = [\n        ModelCheckpoint(\"model_{epoch:04d}_{val_acc:.4f}.h5\",\n                        monitor='val_acc',\n                        verbose=1,\n                        save_best_only=True,\n                        mode='max'),\n        TensorBoard(),\n        EarlyStopping(monitor='val_acc', patience=5, min_delta=0, mode='max')\n    ]\n\n    model.fit(dataset, steps_per_epoch=1000, epochs=1000, validation_data=val_dataset,\n              validation_steps=3, callbacks=callbacks)\n\nOther info / logs\nHere's what I see in the ML Engine logs:\n14:24 Epoch 1/4\n15:00 Starting evaluation\nWith no logging in between \ud83d\udc4e", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ML Engine Instance (Debian)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.10.0\r\n- Python version: 2.7.14\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI've set up a CNN and have created callbacks for model checkpointing, TensorBoard visualization, and early stopping.  I have ~408K samples in my `tf.data.Dataset`, am using a batch size of 256, and train my model with 1,000 steps per epoch with 1000 epochs.  When I do this, everything trains correctly on ML Engine but I don't get logging like I normally do.\r\n\r\n**Describe the expected behavior**\r\n\r\nI should see logging on ML Engine that indicates validation accuracy for each epoch, like this:\r\n\r\nEpoch 2/2000\r\n1/200 [..............................] - ETA: 2:00 - loss: 5.7345 - acc: 0.1055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n\r\n2/200...\r\n\r\nWhen I set `steps_per_epoch` to be only 200, I *do* see validation accuracy logging as expected.  For what it's worth, the training still works and produces .h5 files.  But it's crucial that I be able to view logging as the model's training.\r\n\r\nI spoke with folks on the ML Engine team and it's not an issue on their end.  They suspect it's a bug with the `tf.keras.callbacks.ModelCheckpoint` callback or me misusing `steps_per_epoch` somehow.  Is there a certain value I shouldn't exceed with `steps_per_epoch`?  I wanted to do one full pass over the data, so that should be: # samples / (steps_per_epoch * batch_size), which would be ~1,593 steps.  Even 1,000 steps_per_epoch causes this issue though.\r\n\r\n**Code to reproduce the issue**\r\n\r\nUnfortunately, I can't share the data I'm using.  I'm using a CNN built with `tf.keras.layers` layers (called `model`) and my compile step, callbacks definitions, and fit step look like this:\r\n\r\n        model.compile(\r\n            loss='sparse_categorical_crossentropy',\r\n            optimizer='adam',\r\n            metrics=['accuracy'],\r\n        )\r\n        callbacks = [\r\n            ModelCheckpoint(\"model_{epoch:04d}_{val_acc:.4f}.h5\",\r\n                            monitor='val_acc',\r\n                            verbose=1,\r\n                            save_best_only=True,\r\n                            mode='max'),\r\n            TensorBoard(),\r\n            EarlyStopping(monitor='val_acc', patience=5, min_delta=0, mode='max')\r\n        ]\r\n\r\n        model.fit(dataset, steps_per_epoch=1000, epochs=1000, validation_data=val_dataset,\r\n                  validation_steps=3, callbacks=callbacks)\r\n\r\n**Other info / logs**\r\n\r\nHere's what I see in the ML Engine logs:\r\n\r\n`14:24 Epoch 1/4`\r\n\r\n`15:00 Starting evaluation`\r\n\r\nWith no logging in between \ud83d\udc4e \r\n"}