{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317063956", "html_url": "https://github.com/tensorflow/tensorflow/issues/11660#issuecomment-317063956", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11660", "id": 317063956, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzA2Mzk1Ng==", "user": {"login": "hongjic", "id": 14991190, "node_id": "MDQ6VXNlcjE0OTkxMTkw", "avatar_url": "https://avatars2.githubusercontent.com/u/14991190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongjic", "html_url": "https://github.com/hongjic", "followers_url": "https://api.github.com/users/hongjic/followers", "following_url": "https://api.github.com/users/hongjic/following{/other_user}", "gists_url": "https://api.github.com/users/hongjic/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongjic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongjic/subscriptions", "organizations_url": "https://api.github.com/users/hongjic/orgs", "repos_url": "https://api.github.com/users/hongjic/repos", "events_url": "https://api.github.com/users/hongjic/events{/privacy}", "received_events_url": "https://api.github.com/users/hongjic/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-21T17:33:38Z", "updated_at": "2017-07-21T17:35:26Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> you are right, but I hope it can consume all data but at the same time use backup servers to enhance training speed of each step.<br>\nIt can be as simple as let the worker know the old gradient was dropped and then it can find a way to reconsume it in next step.</p>", "body_text": "@asimshankar you are right, but I hope it can consume all data but at the same time use backup servers to enhance training speed of each step.\nIt can be as simple as let the worker know the old gradient was dropped and then it can find a way to reconsume it in next step.", "body": "@asimshankar you are right, but I hope it can consume all data but at the same time use backup servers to enhance training speed of each step.\r\nIt can be as simple as let the worker know the old gradient was dropped and then it can find a way to reconsume it in next step. "}