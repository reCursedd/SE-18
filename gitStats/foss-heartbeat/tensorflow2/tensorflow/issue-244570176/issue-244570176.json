{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11660", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11660/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11660/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11660/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11660", "id": 244570176, "node_id": "MDU6SXNzdWUyNDQ1NzAxNzY=", "number": 11660, "title": "Gradients be dropped in tf.train.SyncReplicaOptimizer", "user": {"login": "hongjic", "id": 14991190, "node_id": "MDQ6VXNlcjE0OTkxMTkw", "avatar_url": "https://avatars2.githubusercontent.com/u/14991190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongjic", "html_url": "https://github.com/hongjic", "followers_url": "https://api.github.com/users/hongjic/followers", "following_url": "https://api.github.com/users/hongjic/following{/other_user}", "gists_url": "https://api.github.com/users/hongjic/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongjic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongjic/subscriptions", "organizations_url": "https://api.github.com/users/hongjic/orgs", "repos_url": "https://api.github.com/users/hongjic/repos", "events_url": "https://api.github.com/users/hongjic/events{/privacy}", "received_events_url": "https://api.github.com/users/hongjic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-07-21T05:47:38Z", "updated_at": "2017-07-22T01:47:18Z", "closed_at": "2017-07-21T18:35:56Z", "author_association": "NONE", "body_html": "<h3>Problem Description</h3>\n<p>tf.train.SyncReplicaOptimizer is very helpful for backup workers, but the gradients computed by slow workers are just dropped. Below is the documentation I see</p>\n<blockquote>\n<p>Once the gradients have been computed, push them into gradient_queue only if local_step equals global_step, otherwise the gradients are just dropped</p>\n</blockquote>\n<p>The problem is, I want to use backup workers in online training, it means every sample can be just consumed by TensorFlow once and I don't want some samples to be just ignored, is it possible to make backup servers behave like the one in Hadoop which is guaranteed to utilize all data partitions?<br>\nOr can the workers know whether its gradient is dropped or not by the aggregator?<br>\nI think this feature will be useful.</p>", "body_text": "Problem Description\ntf.train.SyncReplicaOptimizer is very helpful for backup workers, but the gradients computed by slow workers are just dropped. Below is the documentation I see\n\nOnce the gradients have been computed, push them into gradient_queue only if local_step equals global_step, otherwise the gradients are just dropped\n\nThe problem is, I want to use backup workers in online training, it means every sample can be just consumed by TensorFlow once and I don't want some samples to be just ignored, is it possible to make backup servers behave like the one in Hadoop which is guaranteed to utilize all data partitions?\nOr can the workers know whether its gradient is dropped or not by the aggregator?\nI think this feature will be useful.", "body": "### Problem Description\r\ntf.train.SyncReplicaOptimizer is very helpful for backup workers, but the gradients computed by slow workers are just dropped. Below is the documentation I see\r\n\r\n> Once the gradients have been computed, push them into gradient_queue only if local_step equals global_step, otherwise the gradients are just dropped\r\n\r\nThe problem is, I want to use backup workers in online training, it means every sample can be just consumed by TensorFlow once and I don't want some samples to be just ignored, is it possible to make backup servers behave like the one in Hadoop which is guaranteed to utilize all data partitions?\r\nOr can the workers know whether its gradient is dropped or not by the aggregator?\r\nI think this feature will be useful."}