{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/343953342", "html_url": "https://github.com/tensorflow/tensorflow/pull/13012#issuecomment-343953342", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13012", "id": 343953342, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mzk1MzM0Mg==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-13T15:25:43Z", "updated_at": "2017-11-13T15:25:43Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">I agree these updates should be placed in the apply_gradients function, but\nI think they can be handled independently in the apply_dense / apply_sparse\nmethods.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sun, Nov 12, 2017 at 5:19 PM, JxKing ***@***.***&gt; wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In tensorflow/contrib/opt/python/training/elastic_average_optimizer.py\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"257351180\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13012\" href=\"https://github.com/tensorflow/tensorflow/pull/13012#discussion_r150432917\">#13012 (comment)</a>&gt;\n :\n\n &gt; +      An `Operation` that applies the specified gradients. If `global_step`\n +      was not None, that operation also increments `global_step`.\n +\n +    Raises:\n +      TypeError: If `grads_and_vars` is malformed.\n +      ValueError: If none of the variables have gradients.\n +    \"\"\"\n +    apply_updates = self._opt.apply_gradients(grads_and_vars)\n +    with ops.control_dependencies([apply_updates]):\n +      local_update = state_ops.assign_add(\n +        self._local_step, 1, name='local_step_update').op\n +\n +    # update global variables.\n +    def _Update_global_variables():\n +      local_vars = variables.trainable_variables()\n +      global_center_vars = ops.get_collection_ref(GLOBAL_CENTER_VARIABLE)\n\n In EASGD, the vars in grads_and_vars should be updated at each step, and\n the global variables will be updated when the step is divided by\n communication_period. I think these variables update should be placed in\n apply_gradients function. Should I add local_vars and global_center_vars to\n the apply_gradients function as arguments?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"257351180\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13012\" href=\"https://github.com/tensorflow/tensorflow/pull/13012#discussion_r150432917\">#13012 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxVXHHahWWs22bf77OkU790NBNkJbks5s15kxgaJpZM4PV-8Y\">https://github.com/notifications/unsubscribe-auth/AAATxVXHHahWWs22bf77OkU790NBNkJbks5s15kxgaJpZM4PV-8Y</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "I agree these updates should be placed in the apply_gradients function, but\nI think they can be handled independently in the apply_dense / apply_sparse\nmethods.\n\u2026\nOn Sun, Nov 12, 2017 at 5:19 PM, JxKing ***@***.***> wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In tensorflow/contrib/opt/python/training/elastic_average_optimizer.py\n <#13012 (comment)>\n :\n\n > +      An `Operation` that applies the specified gradients. If `global_step`\n +      was not None, that operation also increments `global_step`.\n +\n +    Raises:\n +      TypeError: If `grads_and_vars` is malformed.\n +      ValueError: If none of the variables have gradients.\n +    \"\"\"\n +    apply_updates = self._opt.apply_gradients(grads_and_vars)\n +    with ops.control_dependencies([apply_updates]):\n +      local_update = state_ops.assign_add(\n +        self._local_step, 1, name='local_step_update').op\n +\n +    # update global variables.\n +    def _Update_global_variables():\n +      local_vars = variables.trainable_variables()\n +      global_center_vars = ops.get_collection_ref(GLOBAL_CENTER_VARIABLE)\n\n In EASGD, the vars in grads_and_vars should be updated at each step, and\n the global variables will be updated when the step is divided by\n communication_period. I think these variables update should be placed in\n apply_gradients function. Should I add local_vars and global_center_vars to\n the apply_gradients function as arguments?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#13012 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxVXHHahWWs22bf77OkU790NBNkJbks5s15kxgaJpZM4PV-8Y>\n .\n\n\n-- \n - Alex", "body": "I agree these updates should be placed in the apply_gradients function, but\nI think they can be handled independently in the apply_dense / apply_sparse\nmethods.\n\nOn Sun, Nov 12, 2017 at 5:19 PM, JxKing <notifications@github.com> wrote:\n\n> *@jinxin0924* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/opt/python/training/elastic_average_optimizer.py\n> <https://github.com/tensorflow/tensorflow/pull/13012#discussion_r150432917>\n> :\n>\n> > +      An `Operation` that applies the specified gradients. If `global_step`\n> +      was not None, that operation also increments `global_step`.\n> +\n> +    Raises:\n> +      TypeError: If `grads_and_vars` is malformed.\n> +      ValueError: If none of the variables have gradients.\n> +    \"\"\"\n> +    apply_updates = self._opt.apply_gradients(grads_and_vars)\n> +    with ops.control_dependencies([apply_updates]):\n> +      local_update = state_ops.assign_add(\n> +        self._local_step, 1, name='local_step_update').op\n> +\n> +    # update global variables.\n> +    def _Update_global_variables():\n> +      local_vars = variables.trainable_variables()\n> +      global_center_vars = ops.get_collection_ref(GLOBAL_CENTER_VARIABLE)\n>\n> In EASGD, the vars in grads_and_vars should be updated at each step, and\n> the global variables will be updated when the step is divided by\n> communication_period. I think these variables update should be placed in\n> apply_gradients function. Should I add local_vars and global_center_vars to\n> the apply_gradients function as arguments?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13012#discussion_r150432917>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVXHHahWWs22bf77OkU790NBNkJbks5s15kxgaJpZM4PV-8Y>\n> .\n>\n\n\n\n-- \n - Alex\n"}