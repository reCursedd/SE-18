{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/211432983", "pull_request_review_id": 147852742, "id": 211432983, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMTQzMjk4Mw==", "diff_hunk": "@@ -772,33 +775,54 @@ std::pair<int, tensorflow::Allocator*> GetDeviceAndAllocator(\n     const ConversionParams& params, const EngineInfo& engine) {\n   int cuda_device_id = -1;\n   tensorflow::Allocator* dev_allocator = nullptr;\n-  if (params.cluster) {\n-    std::vector<tensorflow::Device*> devices;\n-    if (!engine.device.empty() && params.cluster->GetDeviceSet()) {\n-      DeviceNameUtils::ParsedName parsed_name;\n-      if (DeviceNameUtils::ParseFullName(engine.device, &parsed_name) &&\n-          parsed_name.has_id) {\n-        params.cluster->GetDeviceSet()->FindMatchingDevices(parsed_name,\n-                                                            &devices);\n+  if (params.cluster == nullptr || params.cluster->GetDeviceSet() == nullptr ||\n+      engine.device.empty()) {\n+    // If device is not set, use the first found GPU device for the conversion.\n+    for (int tf_gpu_id_value = 0; tf_gpu_id_value < 100; ++tf_gpu_id_value) {", "path": "tensorflow/contrib/tensorrt/convert/convert_graph.cc", "position": 25, "original_position": 25, "commit_id": "4684421d9aa3e63dc943074025ffdc89df1a1980", "original_commit_id": "0b31ebb51bcdd4b0675d3f2166f316d0f22266b3", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "That's a good point. Currently TfGpuId is always [starting from 0](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L998), so if there are any gpu device initialized before, gpu 0 should always be available. Note that TfGpuId is a virtual identifier of the gpu device owned by the process, not the physical gpu id. But if we hard coded 0 here, changes to BaseGpuDevice initialization flow can break the integration, so I added the loop here to reduce that risk. ", "created_at": "2018-08-20T22:58:28Z", "updated_at": "2018-08-20T23:04:43Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21508#discussion_r211432983", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21508", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/211432983"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21508#discussion_r211432983"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21508"}}, "body_html": "<p>That's a good point. Currently TfGpuId is always <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L998\">starting from 0</a>, so if there are any gpu device initialized before, gpu 0 should always be available. Note that TfGpuId is a virtual identifier of the gpu device owned by the process, not the physical gpu id. But if we hard coded 0 here, changes to BaseGpuDevice initialization flow can break the integration, so I added the loop here to reduce that risk.</p>", "body_text": "That's a good point. Currently TfGpuId is always starting from 0, so if there are any gpu device initialized before, gpu 0 should always be available. Note that TfGpuId is a virtual identifier of the gpu device owned by the process, not the physical gpu id. But if we hard coded 0 here, changes to BaseGpuDevice initialization flow can break the integration, so I added the loop here to reduce that risk.", "in_reply_to_id": 211129029}