{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11367", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11367/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11367/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11367/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11367", "id": 241422457, "node_id": "MDU6SXNzdWUyNDE0MjI0NTc=", "number": 11367, "title": "saver will cause crash,error message:\"InvalidArgumentError: Shape [xx] has negative dimensions\"", "user": {"login": "jowettcz", "id": 2375216, "node_id": "MDQ6VXNlcjIzNzUyMTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/2375216?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jowettcz", "html_url": "https://github.com/jowettcz", "followers_url": "https://api.github.com/users/jowettcz/followers", "following_url": "https://api.github.com/users/jowettcz/following{/other_user}", "gists_url": "https://api.github.com/users/jowettcz/gists{/gist_id}", "starred_url": "https://api.github.com/users/jowettcz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jowettcz/subscriptions", "organizations_url": "https://api.github.com/users/jowettcz/orgs", "repos_url": "https://api.github.com/users/jowettcz/repos", "events_url": "https://api.github.com/users/jowettcz/events{/privacy}", "received_events_url": "https://api.github.com/users/jowettcz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-08T03:48:21Z", "updated_at": "2017-08-14T01:50:38Z", "closed_at": "2017-07-09T08:18:50Z", "author_association": "NONE", "body_html": "<h3>It should be a bug of tensorflow</h3>\n<p>add the saver will cause the procedure crash. error message is strange, as following:<br>\n<em>### InvalidArgumentError (see above for traceback): Shape [-1,32,32,3] has negative dimensions<br>\n[[Node: x = Placeholder<a href=\"\">dtype=DT_FLOAT, shape=[?,32,32,3], _device=\"/job:localhost/replica:0/task:0/cpu:0\"</a>]]</em></p>\n<p>the added part is:<br>\nif (i % 1000 == 0) or (i == num_iterations - 1):<br>\n# Save all variables of the TensorFlow graph to a<br>\n# checkpoint. Append the global_step counter<br>\n# to the filename so we save the last several checkpoints.<br>\nsaver.save(sess,<br>\nsave_path=save_dir,<br>\nglobal_step=train_step)</p>\n<h3>System information</h3>\n<p>== cat /etc/issue ===============================================<br>\nDarwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64<br>\nMac OS X 10.12.5</p>\n<p>== are we in docker =============================================<br>\nNo</p>\n<p>== compiler =====================================================<br>\nApple LLVM version 8.1.0 (clang-802.0.42)<br>\nTarget: x86_64-apple-darwin16.6.0<br>\nThread model: posix<br>\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin</p>\n<p>== uname -a =====================================================<br>\nDarwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64</p>\n<p>== check pips ===================================================<br>\nnumpy (1.13.0)<br>\nprotobuf (3.3.0)<br>\ntensorflow (1.2.1)</p>\n<p>== check for virtualenv =========================================<br>\nFalse</p>\n<p>== tensorflow import ============================================<br>\ntf.VERSION = 1.2.1<br>\ntf.GIT_VERSION = v1.2.0-5-g435cdfc<br>\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc<br>\nSanity check: array([1], dtype=int32)</p>\n<p>== env ==========================================================<br>\nLD_LIBRARY_PATH is unset<br>\nDYLD_LIBRARY_PATH is unset</p>\n<p>== nvidia-smi ===================================================</p>\n<p>== cuda libs  ===================================================</p>\n<p>Tensorflow version:<br>\nv1.2.0-5-g435cdfc 1.2.1</p>\n<h3>Source code / logs</h3>\n<p>source code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>coding=utf-8</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> cifar10\n<span class=\"pl-k\">import</span> preprocess\n<span class=\"pl-k\">from</span> six.moves <span class=\"pl-k\">import</span> <span class=\"pl-v\">xrange</span>\n<span class=\"pl-k\">import</span> os\n\nimg_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nimg_height <span class=\"pl-k\">=</span> img_size\nimg_width <span class=\"pl-k\">=</span> img_size\nimg_channel <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\n\nfirst_conv_feamap <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>\nfilter_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\npool_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\nsecond_conv_feamap <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n\nfcn1_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1024</span>\n\nepoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">60</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">weight_variable</span>(<span class=\"pl-smi\">shape</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>weight_variable generates a weight variable of a given shape.<span class=\"pl-pds\">\"\"\"</span></span>\n    initial <span class=\"pl-k\">=</span> tf.truncated_normal(shape, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n    <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bias_variable</span>(<span class=\"pl-smi\">shape</span>):\n    initial <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.1</span>,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape)\n    <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">conv2d</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">W</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>conv2d returns a 2d convolution layer with full stride.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">return</span> tf.nn.conv2d(x, W, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">max_pool_2x2</span>(<span class=\"pl-smi\">x</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>max_pool_2x2 downsamples a feature map by 2X.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>ksize\u662f\u7a97\u53e3\u5927\u5c0f\uff0cstride\u662f\u6b65\u957f\uff0c4-&gt;1\u6b65\u957f\u6b63\u597d\u662f2</span>\n  <span class=\"pl-k\">return</span> tf.nn.max_pool(x, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>],\n                        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">sequence_batch</span>(<span class=\"pl-smi\">images</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">idx</span>,<span class=\"pl-smi\">_batch_size</span>):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Use the random index to select random images and labels.</span>\n    x_batch <span class=\"pl-k\">=</span> images[idx:idx<span class=\"pl-k\">+</span>_batch_size, :, :, :]\n    y_batch <span class=\"pl-k\">=</span> labels[idx:idx<span class=\"pl-k\">+</span>_batch_size, :]\n\n    <span class=\"pl-k\">return</span> x_batch, y_batch\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Procedure start now!</span>\n\nimages,training_class,labels <span class=\"pl-k\">=</span> cifar10.load_training_data()\n\ntest_images,test_class,test_labels <span class=\"pl-k\">=</span> cifar10.load_test_data()\n\n\nlog_dir <span class=\"pl-k\">=</span> os.getcwd() <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/log<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>log_dir is <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> log_dir)\n<span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n\nsave_dir <span class=\"pl-k\">=</span> os.getcwd() <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/checkpoints<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>save_dir is <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> save_dir)\n<span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>,img_height,img_width,img_channel], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>)\ny_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>y_<span class=\"pl-pds\">'</span></span>)\ny_cls <span class=\"pl-k\">=</span> tf.argmax(y_, <span class=\"pl-v\">dimension</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\nx <span class=\"pl-k\">=</span> tf.reshape(x, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, img_height, img_width, img_channel])\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>):\n    w_conv1 <span class=\"pl-k\">=</span> weight_variable([filter_size, filter_size, img_channel, first_conv_feamap])\n    b_conv1 <span class=\"pl-k\">=</span> bias_variable([first_conv_feamap])\n\n    h_conv1 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(x, w_conv1) <span class=\"pl-k\">+</span> b_conv1)\n    h_pool1 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv1)\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2<span class=\"pl-pds\">'</span></span>):\n    w_conv2 <span class=\"pl-k\">=</span> weight_variable([filter_size, filter_size, first_conv_feamap, second_conv_feamap])\n    b_conv2 <span class=\"pl-k\">=</span> bias_variable([second_conv_feamap])\n\n    h_conv2 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(h_pool1, w_conv2) <span class=\"pl-k\">+</span> b_conv2)\n    h_pool2 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv2)\n\nconv2_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fcn1<span class=\"pl-pds\">'</span></span>):\n    w_fcn1 <span class=\"pl-k\">=</span> weight_variable([conv2_size <span class=\"pl-k\">*</span> conv2_size <span class=\"pl-k\">*</span> second_conv_feamap, fcn1_size])\n    b_fcn1 <span class=\"pl-k\">=</span> bias_variable([fcn1_size])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> change with the image size and convlo</span>\n    h_pool2_flat <span class=\"pl-k\">=</span> tf.reshape(h_pool2, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, conv2_size <span class=\"pl-k\">*</span> conv2_size <span class=\"pl-k\">*</span> second_conv_feamap])\n    h_fcn1 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(h_pool2_flat, w_fcn1) <span class=\"pl-k\">+</span> b_fcn1)\n\n    keep_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\n    h_fcn1_drop <span class=\"pl-k\">=</span> tf.nn.dropout(h_fcn1, keep_prob)\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fcn2<span class=\"pl-pds\">'</span></span>):\n    w_fcn2 <span class=\"pl-k\">=</span> weight_variable([fcn1_size, <span class=\"pl-c1\">10</span>])\n    b_fcn2 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">10</span>])\n\n    y_cnn <span class=\"pl-k\">=</span> tf.matmul(h_fcn1_drop, w_fcn2) <span class=\"pl-k\">+</span> b_fcn2\n\n\n\ncross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>y_,<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>y_cnn))\n\ntrain_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>).minimize(cross_entropy)\n\ncorrect_prediction <span class=\"pl-k\">=</span> tf.equal(tf.arg_max(y_cnn,<span class=\"pl-c1\">1</span>),tf.arg_max(y_,<span class=\"pl-c1\">1</span>))\n\naccuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n\n\nsaver <span class=\"pl-k\">=</span> tf.train.Saver()\n\n\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    init <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n    sess.run(init)\n\n\n    num_iterations <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">len</span>(images)<span class=\"pl-k\">/</span>batch_size)\n\n    idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">60</span>):\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(num_iterations):\n\n\n            x_batch,y_batch <span class=\"pl-k\">=</span> sequence_batch(images,labels,idx,batch_size)\n\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x_batch shape: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(x_batch.shape))\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y_batch shape: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(y_batch.shape))\n\n            train_step.run(\n                <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n                    x: x_batch,\n                    y_: y_batch,\n                    keep_prob: <span class=\"pl-c1\">0.5</span>\n                }\n            )\n\n            idx <span class=\"pl-k\">=</span> idx <span class=\"pl-k\">+</span> batch_size\n\n            <span class=\"pl-k\">if</span> i<span class=\"pl-k\">%</span><span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                training_feed_dict <span class=\"pl-k\">=</span> {\n                    x: images,\n                    y_: labels,\n                    keep_prob: <span class=\"pl-c1\">1.0</span>\n                }\n\n                test_feed_dict <span class=\"pl-k\">=</span> {\n                    x: test_images,\n                    y_: test_labels,\n                    keep_prob: <span class=\"pl-c1\">1.0</span>\n                }\n\n\n\n                train_accuracy <span class=\"pl-k\">=</span> accuracy.eval(\n                    <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>training_feed_dict\n                )\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>step <span class=\"pl-c1\">%d</span>, training accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (i, train_accuracy))\n\n                test_accuracy <span class=\"pl-k\">=</span> accuracy.eval(\n                    <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>test_feed_dict\n                )\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>step <span class=\"pl-c1\">%d</span>, test accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (i, test_accuracy))\n\n            <span class=\"pl-k\">if</span> (i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">1000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>) <span class=\"pl-k\">or</span> (i <span class=\"pl-k\">==</span> num_iterations <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>):\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Save all variables of the TensorFlow graph to a</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> checkpoint. Append the global_step counter</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> to the filename so we save the last several checkpoints.</span>\n                saver.save(sess,\n                           <span class=\"pl-v\">save_path</span><span class=\"pl-k\">=</span>save_dir,\n                           <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>train_step)\n\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Saved checkpoint.<span class=\"pl-pds\">\"</span></span>)\n\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test accuracy in every epoch <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> accuracy.eval(\n        <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n            x: test_images,\n            y_: test_labels,\n            keep_prob: <span class=\"pl-c1\">1.0</span>\n        }\n        )\n    )\n</pre></div>", "body_text": "It should be a bug of tensorflow\nadd the saver will cause the procedure crash. error message is strange, as following:\n### InvalidArgumentError (see above for traceback): Shape [-1,32,32,3] has negative dimensions\n[[Node: x = Placeholderdtype=DT_FLOAT, shape=[?,32,32,3], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nthe added part is:\nif (i % 1000 == 0) or (i == num_iterations - 1):\n# Save all variables of the TensorFlow graph to a\n# checkpoint. Append the global_step counter\n# to the filename so we save the last several checkpoints.\nsaver.save(sess,\nsave_path=save_dir,\nglobal_step=train_step)\nSystem information\n== cat /etc/issue ===============================================\nDarwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64\nMac OS X 10.12.5\n== are we in docker =============================================\nNo\n== compiler =====================================================\nApple LLVM version 8.1.0 (clang-802.0.42)\nTarget: x86_64-apple-darwin16.6.0\nThread model: posix\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\n== uname -a =====================================================\nDarwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64\n== check pips ===================================================\nnumpy (1.13.0)\nprotobuf (3.3.0)\ntensorflow (1.2.1)\n== check for virtualenv =========================================\nFalse\n== tensorflow import ============================================\ntf.VERSION = 1.2.1\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\nSanity check: array([1], dtype=int32)\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n== nvidia-smi ===================================================\n== cuda libs  ===================================================\nTensorflow version:\nv1.2.0-5-g435cdfc 1.2.1\nSource code / logs\nsource code:\n#coding=utf-8\nimport tensorflow as tf\nimport numpy as np\nimport cifar10\nimport preprocess\nfrom six.moves import xrange\nimport os\n\nimg_size = 32\nimg_height = img_size\nimg_width = img_size\nimg_channel = 3\n\nfirst_conv_feamap = 16\nfilter_size = 5\npool_size = 4\n\nsecond_conv_feamap = 32\n\nfcn1_size = 1024\n\nepoch = 60\nbatch_size = 50\n\n\ndef weight_variable(shape):\n    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1,shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n  #ksize\u662f\u7a97\u53e3\u5927\u5c0f\uff0cstride\u662f\u6b65\u957f\uff0c4->1\u6b65\u957f\u6b63\u597d\u662f2\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\n\ndef sequence_batch(images, labels, idx,_batch_size):\n\n    # Use the random index to select random images and labels.\n    x_batch = images[idx:idx+_batch_size, :, :, :]\n    y_batch = labels[idx:idx+_batch_size, :]\n\n    return x_batch, y_batch\n\n#Procedure start now!\n\nimages,training_class,labels = cifar10.load_training_data()\n\ntest_images,test_class,test_labels = cifar10.load_test_data()\n\n\nlog_dir = os.getcwd() + '/log'\nprint('log_dir is ' + log_dir)\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n\nsave_dir = os.getcwd() + '/checkpoints'\nprint('save_dir is ' + save_dir)\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n\nx = tf.placeholder(tf.float32, shape=[None,img_height,img_width,img_channel], name='x')\ny_ = tf.placeholder(tf.float32, shape=[None, 10], name='y_')\ny_cls = tf.argmax(y_, dimension=1)\n\nx = tf.reshape(x, shape=[-1, img_height, img_width, img_channel])\n\nwith tf.name_scope('conv1'):\n    w_conv1 = weight_variable([filter_size, filter_size, img_channel, first_conv_feamap])\n    b_conv1 = bias_variable([first_conv_feamap])\n\n    h_conv1 = tf.nn.relu(conv2d(x, w_conv1) + b_conv1)\n    h_pool1 = max_pool_2x2(h_conv1)\n\nwith tf.name_scope('conv2'):\n    w_conv2 = weight_variable([filter_size, filter_size, first_conv_feamap, second_conv_feamap])\n    b_conv2 = bias_variable([second_conv_feamap])\n\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n    h_pool2 = max_pool_2x2(h_conv2)\n\nconv2_size = 8\n\nwith tf.name_scope('fcn1'):\n    w_fcn1 = weight_variable([conv2_size * conv2_size * second_conv_feamap, fcn1_size])\n    b_fcn1 = bias_variable([fcn1_size])\n\n    # change with the image size and convlo\n    h_pool2_flat = tf.reshape(h_pool2, [-1, conv2_size * conv2_size * second_conv_feamap])\n    h_fcn1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fcn1) + b_fcn1)\n\n    keep_prob = tf.placeholder(tf.float32)\n    h_fcn1_drop = tf.nn.dropout(h_fcn1, keep_prob)\n\nwith tf.name_scope('fcn2'):\n    w_fcn2 = weight_variable([fcn1_size, 10])\n    b_fcn2 = bias_variable([10])\n\n    y_cnn = tf.matmul(h_fcn1_drop, w_fcn2) + b_fcn2\n\n\n\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_cnn))\n\ntrain_step = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.arg_max(y_cnn,1),tf.arg_max(y_,1))\n\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n\n\nsaver = tf.train.Saver()\n\n\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n\n    num_iterations = int(len(images)/batch_size)\n\n    idx = 0\n\n    for j in xrange(60):\n        for i in xrange(num_iterations):\n\n\n            x_batch,y_batch = sequence_batch(images,labels,idx,batch_size)\n\n            print(\"x_batch shape: \" + str(x_batch.shape))\n            print(\"y_batch shape: \" + str(y_batch.shape))\n\n            train_step.run(\n                feed_dict={\n                    x: x_batch,\n                    y_: y_batch,\n                    keep_prob: 0.5\n                }\n            )\n\n            idx = idx + batch_size\n\n            if i%100 == 0:\n                training_feed_dict = {\n                    x: images,\n                    y_: labels,\n                    keep_prob: 1.0\n                }\n\n                test_feed_dict = {\n                    x: test_images,\n                    y_: test_labels,\n                    keep_prob: 1.0\n                }\n\n\n\n                train_accuracy = accuracy.eval(\n                    feed_dict=training_feed_dict\n                )\n                print('step %d, training accuracy %g' % (i, train_accuracy))\n\n                test_accuracy = accuracy.eval(\n                    feed_dict=test_feed_dict\n                )\n                print('step %d, test accuracy %g' % (i, test_accuracy))\n\n            if (i % 1000 == 0) or (i == num_iterations - 1):\n                # Save all variables of the TensorFlow graph to a\n                # checkpoint. Append the global_step counter\n                # to the filename so we save the last several checkpoints.\n                saver.save(sess,\n                           save_path=save_dir,\n                           global_step=train_step)\n\n                print(\"Saved checkpoint.\")\n\n\n    print('test accuracy in every epoch %g' % accuracy.eval(\n        feed_dict={\n            x: test_images,\n            y_: test_labels,\n            keep_prob: 1.0\n        }\n        )\n    )", "body": "### It should be a bug of tensorflow\r\nadd the saver will cause the procedure crash. error message is strange, as following:\r\n_### InvalidArgumentError (see above for traceback): Shape [-1,32,32,3] has negative dimensions\r\n\t [[Node: x = Placeholder[dtype=DT_FLOAT, shape=[?,32,32,3], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]_\r\n\r\nthe added part is:\r\n            if (i % 1000 == 0) or (i == num_iterations - 1):\r\n                # Save all variables of the TensorFlow graph to a\r\n                # checkpoint. Append the global_step counter\r\n                # to the filename so we save the last several checkpoints.\r\n                saver.save(sess,\r\n                           save_path=save_dir,\r\n                           global_step=train_step)\r\n\r\n### System information\r\n== cat /etc/issue ===============================================\r\nDarwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.5\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin zhangdeMacBook-Pro.local 16.6.0 Darwin Kernel Version 16.6.0: Fri Apr 14 16:21:16 PDT 2017; root:xnu-3789.60.24~6/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n\r\n== cuda libs  ===================================================\r\n\r\nTensorflow version:\r\nv1.2.0-5-g435cdfc 1.2.1\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nsource code:\r\n```python\r\n#coding=utf-8\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport cifar10\r\nimport preprocess\r\nfrom six.moves import xrange\r\nimport os\r\n\r\nimg_size = 32\r\nimg_height = img_size\r\nimg_width = img_size\r\nimg_channel = 3\r\n\r\nfirst_conv_feamap = 16\r\nfilter_size = 5\r\npool_size = 4\r\n\r\nsecond_conv_feamap = 32\r\n\r\nfcn1_size = 1024\r\n\r\nepoch = 60\r\nbatch_size = 50\r\n\r\n\r\ndef weight_variable(shape):\r\n    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\r\n    initial = tf.truncated_normal(shape, stddev=0.1)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1,shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef conv2d(x, W):\r\n  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\r\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\r\n  #ksize\u662f\u7a97\u53e3\u5927\u5c0f\uff0cstride\u662f\u6b65\u957f\uff0c4->1\u6b65\u957f\u6b63\u597d\u662f2\r\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\r\n                        strides=[1, 2, 2, 1], padding='SAME')\r\n\r\n\r\ndef sequence_batch(images, labels, idx,_batch_size):\r\n\r\n    # Use the random index to select random images and labels.\r\n    x_batch = images[idx:idx+_batch_size, :, :, :]\r\n    y_batch = labels[idx:idx+_batch_size, :]\r\n\r\n    return x_batch, y_batch\r\n\r\n#Procedure start now!\r\n\r\nimages,training_class,labels = cifar10.load_training_data()\r\n\r\ntest_images,test_class,test_labels = cifar10.load_test_data()\r\n\r\n\r\nlog_dir = os.getcwd() + '/log'\r\nprint('log_dir is ' + log_dir)\r\nif not os.path.exists(log_dir):\r\n    os.makedirs(log_dir)\r\n\r\n\r\nsave_dir = os.getcwd() + '/checkpoints'\r\nprint('save_dir is ' + save_dir)\r\nif not os.path.exists(save_dir):\r\n    os.makedirs(save_dir)\r\n\r\n\r\nx = tf.placeholder(tf.float32, shape=[None,img_height,img_width,img_channel], name='x')\r\ny_ = tf.placeholder(tf.float32, shape=[None, 10], name='y_')\r\ny_cls = tf.argmax(y_, dimension=1)\r\n\r\nx = tf.reshape(x, shape=[-1, img_height, img_width, img_channel])\r\n\r\nwith tf.name_scope('conv1'):\r\n    w_conv1 = weight_variable([filter_size, filter_size, img_channel, first_conv_feamap])\r\n    b_conv1 = bias_variable([first_conv_feamap])\r\n\r\n    h_conv1 = tf.nn.relu(conv2d(x, w_conv1) + b_conv1)\r\n    h_pool1 = max_pool_2x2(h_conv1)\r\n\r\nwith tf.name_scope('conv2'):\r\n    w_conv2 = weight_variable([filter_size, filter_size, first_conv_feamap, second_conv_feamap])\r\n    b_conv2 = bias_variable([second_conv_feamap])\r\n\r\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\r\n    h_pool2 = max_pool_2x2(h_conv2)\r\n\r\nconv2_size = 8\r\n\r\nwith tf.name_scope('fcn1'):\r\n    w_fcn1 = weight_variable([conv2_size * conv2_size * second_conv_feamap, fcn1_size])\r\n    b_fcn1 = bias_variable([fcn1_size])\r\n\r\n    # change with the image size and convlo\r\n    h_pool2_flat = tf.reshape(h_pool2, [-1, conv2_size * conv2_size * second_conv_feamap])\r\n    h_fcn1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fcn1) + b_fcn1)\r\n\r\n    keep_prob = tf.placeholder(tf.float32)\r\n    h_fcn1_drop = tf.nn.dropout(h_fcn1, keep_prob)\r\n\r\nwith tf.name_scope('fcn2'):\r\n    w_fcn2 = weight_variable([fcn1_size, 10])\r\n    b_fcn2 = bias_variable([10])\r\n\r\n    y_cnn = tf.matmul(h_fcn1_drop, w_fcn2) + b_fcn2\r\n\r\n\r\n\r\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_cnn))\r\n\r\ntrain_step = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)\r\n\r\ncorrect_prediction = tf.equal(tf.arg_max(y_cnn,1),tf.arg_max(y_,1))\r\n\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n\r\n\r\nsaver = tf.train.Saver()\r\n\r\n\r\n\r\nwith tf.Session() as sess:\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n\r\n    num_iterations = int(len(images)/batch_size)\r\n\r\n    idx = 0\r\n\r\n    for j in xrange(60):\r\n        for i in xrange(num_iterations):\r\n\r\n\r\n            x_batch,y_batch = sequence_batch(images,labels,idx,batch_size)\r\n\r\n            print(\"x_batch shape: \" + str(x_batch.shape))\r\n            print(\"y_batch shape: \" + str(y_batch.shape))\r\n\r\n            train_step.run(\r\n                feed_dict={\r\n                    x: x_batch,\r\n                    y_: y_batch,\r\n                    keep_prob: 0.5\r\n                }\r\n            )\r\n\r\n            idx = idx + batch_size\r\n\r\n            if i%100 == 0:\r\n                training_feed_dict = {\r\n                    x: images,\r\n                    y_: labels,\r\n                    keep_prob: 1.0\r\n                }\r\n\r\n                test_feed_dict = {\r\n                    x: test_images,\r\n                    y_: test_labels,\r\n                    keep_prob: 1.0\r\n                }\r\n\r\n\r\n\r\n                train_accuracy = accuracy.eval(\r\n                    feed_dict=training_feed_dict\r\n                )\r\n                print('step %d, training accuracy %g' % (i, train_accuracy))\r\n\r\n                test_accuracy = accuracy.eval(\r\n                    feed_dict=test_feed_dict\r\n                )\r\n                print('step %d, test accuracy %g' % (i, test_accuracy))\r\n\r\n            if (i % 1000 == 0) or (i == num_iterations - 1):\r\n                # Save all variables of the TensorFlow graph to a\r\n                # checkpoint. Append the global_step counter\r\n                # to the filename so we save the last several checkpoints.\r\n                saver.save(sess,\r\n                           save_path=save_dir,\r\n                           global_step=train_step)\r\n\r\n                print(\"Saved checkpoint.\")\r\n\r\n\r\n    print('test accuracy in every epoch %g' % accuracy.eval(\r\n        feed_dict={\r\n            x: test_images,\r\n            y_: test_labels,\r\n            keep_prob: 1.0\r\n        }\r\n        )\r\n    )\r\n\r\n```\r\n\r\n\r\n\r\n\r\n"}