{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17810", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17810/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17810/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17810/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17810", "id": 306245750, "node_id": "MDU6SXNzdWUzMDYyNDU3NTA=", "number": 17810, "title": "Dataset.list_files is impractical for large number of files", "user": {"login": "darrengarvey", "id": 260360, "node_id": "MDQ6VXNlcjI2MDM2MA==", "avatar_url": "https://avatars0.githubusercontent.com/u/260360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/darrengarvey", "html_url": "https://github.com/darrengarvey", "followers_url": "https://api.github.com/users/darrengarvey/followers", "following_url": "https://api.github.com/users/darrengarvey/following{/other_user}", "gists_url": "https://api.github.com/users/darrengarvey/gists{/gist_id}", "starred_url": "https://api.github.com/users/darrengarvey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/darrengarvey/subscriptions", "organizations_url": "https://api.github.com/users/darrengarvey/orgs", "repos_url": "https://api.github.com/users/darrengarvey/repos", "events_url": "https://api.github.com/users/darrengarvey/events{/privacy}", "received_events_url": "https://api.github.com/users/darrengarvey/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-03-18T14:38:31Z", "updated_at": "2018-11-17T14:06:52Z", "closed_at": "2018-11-17T14:06:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I would like to use <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files\" rel=\"nofollow\"><code>Dataset.list_files</code></a> on a large dataset. The dataset is similar to ImageNet in that it's a big list of images nested in subdirectories.</p>\n<p>I originally guessed that under the hood the api would use queues to start training early, but that isn't the case - the implementation walks the entire directory tree and loads the filenames into memory before the next operation starts.</p>\n<p>This means you can wait tens of minutes before training starts.</p>\n<p>Two things would help here:</p>\n<ol>\n<li>Use queues under the hood so downstream ops can start immediately</li>\n<li>Allow specifying a limit, eg. <code>Dataset.list_files(\"**/*.jpg\", limit=1000)</code></li>\n</ol>\n<p>The latter is mostly useful for quicker iteration. The former needs more work due to the different backends for <code>GetMatchingPaths</code>.</p>\n<p>Is this something being worked on already?</p>\n<p>I am using <a href=\"https://www.python.org/dev/peps/pep-0471/\" rel=\"nofollow\"><code>os.scandir()</code></a> as a workaround but the <code>Dataset.list_files()</code> is a more natural api for this task and should be faster due to not needing to pass the file names via <code>feed_dict</code>.</p>", "body_text": "I would like to use Dataset.list_files on a large dataset. The dataset is similar to ImageNet in that it's a big list of images nested in subdirectories.\nI originally guessed that under the hood the api would use queues to start training early, but that isn't the case - the implementation walks the entire directory tree and loads the filenames into memory before the next operation starts.\nThis means you can wait tens of minutes before training starts.\nTwo things would help here:\n\nUse queues under the hood so downstream ops can start immediately\nAllow specifying a limit, eg. Dataset.list_files(\"**/*.jpg\", limit=1000)\n\nThe latter is mostly useful for quicker iteration. The former needs more work due to the different backends for GetMatchingPaths.\nIs this something being worked on already?\nI am using os.scandir() as a workaround but the Dataset.list_files() is a more natural api for this task and should be faster due to not needing to pass the file names via feed_dict.", "body": "I would like to use [`Dataset.list_files`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files) on a large dataset. The dataset is similar to ImageNet in that it's a big list of images nested in subdirectories.\r\n\r\nI originally guessed that under the hood the api would use queues to start training early, but that isn't the case - the implementation walks the entire directory tree and loads the filenames into memory before the next operation starts.\r\n\r\nThis means you can wait tens of minutes before training starts.\r\n\r\nTwo things would help here:\r\n1) Use queues under the hood so downstream ops can start immediately\r\n2) Allow specifying a limit, eg. `Dataset.list_files(\"**/*.jpg\", limit=1000)`\r\n\r\nThe latter is mostly useful for quicker iteration. The former needs more work due to the different backends for `GetMatchingPaths`.\r\n\r\nIs this something being worked on already?\r\n\r\nI am using [`os.scandir()`](https://www.python.org/dev/peps/pep-0471/) as a workaround but the `Dataset.list_files()` is a more natural api for this task and should be faster due to not needing to pass the file names via `feed_dict`."}