{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5598", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5598/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5598/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5598/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5598", "id": 189133641, "node_id": "MDU6SXNzdWUxODkxMzM2NDE=", "number": 5598, "title": "tf.contrib.learn estimators that use feature columns via the input_fun not compatible with x inputs when predicting and not exportable to serving", "user": {"login": "fventer", "id": 790898, "node_id": "MDQ6VXNlcjc5MDg5OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/790898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fventer", "html_url": "https://github.com/fventer", "followers_url": "https://api.github.com/users/fventer/followers", "following_url": "https://api.github.com/users/fventer/following{/other_user}", "gists_url": "https://api.github.com/users/fventer/gists{/gist_id}", "starred_url": "https://api.github.com/users/fventer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fventer/subscriptions", "organizations_url": "https://api.github.com/users/fventer/orgs", "repos_url": "https://api.github.com/users/fventer/repos", "events_url": "https://api.github.com/users/fventer/events{/privacy}", "received_events_url": "https://api.github.com/users/fventer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2016-11-14T15:00:42Z", "updated_at": "2017-03-15T08:31:02Z", "closed_at": "2016-11-15T16:59:12Z", "author_association": "NONE", "body_html": "<p>I have run into into performance and portability issues with tf.conrtib.learn estimators that use high level feature columns and have done extensive code traversal to figure out a possible solution to make predictions faster.</p>\n<p>At the core of the problem is that high level feature column tensors created in the input_fn are transformed and added as input tensors to the actual neural network and/or logistic model. After that the hidden layers and logit (output) layer is added to the graph and the logits for the linear model (if used) are also created. This happens every time you run predict on the estimator, at the cost an overhead of 3-7 seconds.</p>\n<p>One solution I tried is based on posts that I read about the x parameter of the predict method (vs input_fn). If you create an iterator that feeds examples (that never sends the StopIteration exception) then you could keep the model \"resident\" avoiding the graph being built every time you call predict.</p>\n<p>However, the x parameter as iterable does not work,  because it seems that you can only provide it low level inputs (such as numpy arrays or pandas dataframe of type float or int only). It seems that this solution precludes the nice feature columns such as sparse tensors (for categoricals) and embeddings (to get inputs to the neural net), etc.</p>\n<p>Another problem that arises with the above issues with the estimator design is that although it has an export method to create a model that tensorflow serving loads nicely, it will not work, because the graph needs to be built at predict time to extract model inputs from the higher level feature column tensors and of course tensor flow serving does not work like that. It loads a graph and runs inputs to create outputs - similar to the predict method using the x parameter. It does not create graphs on the fly every time a predict call is made.</p>\n<p>What is needed as a new feature is that the x and y parameters of the predict method should accept raw inputs (like strings for categorical features/embeddings) that is then passed into the feed_dict parameter of the session run call.</p>\n<p>Similarly, for tensorflow serving, it should be possible to export the estimator and write the model client so that the raw feature column values (strings) can be sent to the model instantiated in tensorflow serving.</p>\n<p>This feature is quite urgent in my opinion as models based on the cool abstractions available in the contrib.learn estimator API are not very useful beyond training and evaluation without this requested feature.</p>", "body_text": "I have run into into performance and portability issues with tf.conrtib.learn estimators that use high level feature columns and have done extensive code traversal to figure out a possible solution to make predictions faster.\nAt the core of the problem is that high level feature column tensors created in the input_fn are transformed and added as input tensors to the actual neural network and/or logistic model. After that the hidden layers and logit (output) layer is added to the graph and the logits for the linear model (if used) are also created. This happens every time you run predict on the estimator, at the cost an overhead of 3-7 seconds.\nOne solution I tried is based on posts that I read about the x parameter of the predict method (vs input_fn). If you create an iterator that feeds examples (that never sends the StopIteration exception) then you could keep the model \"resident\" avoiding the graph being built every time you call predict.\nHowever, the x parameter as iterable does not work,  because it seems that you can only provide it low level inputs (such as numpy arrays or pandas dataframe of type float or int only). It seems that this solution precludes the nice feature columns such as sparse tensors (for categoricals) and embeddings (to get inputs to the neural net), etc.\nAnother problem that arises with the above issues with the estimator design is that although it has an export method to create a model that tensorflow serving loads nicely, it will not work, because the graph needs to be built at predict time to extract model inputs from the higher level feature column tensors and of course tensor flow serving does not work like that. It loads a graph and runs inputs to create outputs - similar to the predict method using the x parameter. It does not create graphs on the fly every time a predict call is made.\nWhat is needed as a new feature is that the x and y parameters of the predict method should accept raw inputs (like strings for categorical features/embeddings) that is then passed into the feed_dict parameter of the session run call.\nSimilarly, for tensorflow serving, it should be possible to export the estimator and write the model client so that the raw feature column values (strings) can be sent to the model instantiated in tensorflow serving.\nThis feature is quite urgent in my opinion as models based on the cool abstractions available in the contrib.learn estimator API are not very useful beyond training and evaluation without this requested feature.", "body": "I have run into into performance and portability issues with tf.conrtib.learn estimators that use high level feature columns and have done extensive code traversal to figure out a possible solution to make predictions faster.\r\n\r\nAt the core of the problem is that high level feature column tensors created in the input_fn are transformed and added as input tensors to the actual neural network and/or logistic model. After that the hidden layers and logit (output) layer is added to the graph and the logits for the linear model (if used) are also created. This happens every time you run predict on the estimator, at the cost an overhead of 3-7 seconds.\r\n\r\nOne solution I tried is based on posts that I read about the x parameter of the predict method (vs input_fn). If you create an iterator that feeds examples (that never sends the StopIteration exception) then you could keep the model \"resident\" avoiding the graph being built every time you call predict.\r\n\r\nHowever, the x parameter as iterable does not work,  because it seems that you can only provide it low level inputs (such as numpy arrays or pandas dataframe of type float or int only). It seems that this solution precludes the nice feature columns such as sparse tensors (for categoricals) and embeddings (to get inputs to the neural net), etc.\r\n\r\nAnother problem that arises with the above issues with the estimator design is that although it has an export method to create a model that tensorflow serving loads nicely, it will not work, because the graph needs to be built at predict time to extract model inputs from the higher level feature column tensors and of course tensor flow serving does not work like that. It loads a graph and runs inputs to create outputs - similar to the predict method using the x parameter. It does not create graphs on the fly every time a predict call is made.\r\n\r\nWhat is needed as a new feature is that the x and y parameters of the predict method should accept raw inputs (like strings for categorical features/embeddings) that is then passed into the feed_dict parameter of the session run call.\r\n\r\nSimilarly, for tensorflow serving, it should be possible to export the estimator and write the model client so that the raw feature column values (strings) can be sent to the model instantiated in tensorflow serving. \r\n\r\nThis feature is quite urgent in my opinion as models based on the cool abstractions available in the contrib.learn estimator API are not very useful beyond training and evaluation without this requested feature.\r\n"}