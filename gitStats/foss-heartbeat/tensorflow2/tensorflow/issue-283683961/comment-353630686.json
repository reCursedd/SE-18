{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/353630686", "html_url": "https://github.com/tensorflow/tensorflow/issues/15530#issuecomment-353630686", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15530", "id": 353630686, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzYzMDY4Ng==", "user": {"login": "cbockman", "id": 4667922, "node_id": "MDQ6VXNlcjQ2Njc5MjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/4667922?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbockman", "html_url": "https://github.com/cbockman", "followers_url": "https://api.github.com/users/cbockman/followers", "following_url": "https://api.github.com/users/cbockman/following{/other_user}", "gists_url": "https://api.github.com/users/cbockman/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbockman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbockman/subscriptions", "organizations_url": "https://api.github.com/users/cbockman/orgs", "repos_url": "https://api.github.com/users/cbockman/repos", "events_url": "https://api.github.com/users/cbockman/events{/privacy}", "received_events_url": "https://api.github.com/users/cbockman/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-22T16:23:59Z", "updated_at": "2017-12-22T16:26:16Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>Yes, it's by design. Distributed filesystems like GCS are designed for write once, read multiple times. You shouldn't rely on the content of the \"checkpoint\" file, because you need to read/write it multiple times.</p>\n</blockquote>\n<p>With all due respect, this doesn't make much sense.  Tensorflow is designed to use GCS as a backend for model saving, and TF is built to use GCS as a back-end for model saving.  tf.Experiment is built with a continuous_eval mode which tries to load the checkpoint repeatedly, to discover state and use it appropriately.  Even if we try to chalk this up to tf.Experiment being in contrib (albeit maintained and built by Googlers), we still have the problem that GCS is advertised as a viable and supported backend storage for Tensorflow.</p>\n<p>Meaning, if Tensorflow is going to advertise that it supports GCS as a backend, functionality should not randomly not work.  This is a pretty basic scenario for it to break.  It isn't unreasonable for this scenario to work, given that I can work with write-multiple-times, read-multiple times with the existing GCS APIs, with no problems...outside of Tensorflow.</p>", "body_text": "Yes, it's by design. Distributed filesystems like GCS are designed for write once, read multiple times. You shouldn't rely on the content of the \"checkpoint\" file, because you need to read/write it multiple times.\n\nWith all due respect, this doesn't make much sense.  Tensorflow is designed to use GCS as a backend for model saving, and TF is built to use GCS as a back-end for model saving.  tf.Experiment is built with a continuous_eval mode which tries to load the checkpoint repeatedly, to discover state and use it appropriately.  Even if we try to chalk this up to tf.Experiment being in contrib (albeit maintained and built by Googlers), we still have the problem that GCS is advertised as a viable and supported backend storage for Tensorflow.\nMeaning, if Tensorflow is going to advertise that it supports GCS as a backend, functionality should not randomly not work.  This is a pretty basic scenario for it to break.  It isn't unreasonable for this scenario to work, given that I can work with write-multiple-times, read-multiple times with the existing GCS APIs, with no problems...outside of Tensorflow.", "body": "> Yes, it's by design. Distributed filesystems like GCS are designed for write once, read multiple times. You shouldn't rely on the content of the \"checkpoint\" file, because you need to read/write it multiple times.\r\n\r\nWith all due respect, this doesn't make much sense.  Tensorflow is designed to use GCS as a backend for model saving, and TF is built to use GCS as a back-end for model saving.  tf.Experiment is built with a continuous_eval mode which tries to load the checkpoint repeatedly, to discover state and use it appropriately.  Even if we try to chalk this up to tf.Experiment being in contrib (albeit maintained and built by Googlers), we still have the problem that GCS is advertised as a viable and supported backend storage for Tensorflow.  \r\n\r\nMeaning, if Tensorflow is going to advertise that it supports GCS as a backend, functionality should not randomly not work.  This is a pretty basic scenario for it to break.  It isn't unreasonable for this scenario to work, given that I can work with write-multiple-times, read-multiple times with the existing GCS APIs, with no problems...outside of Tensorflow.\r\n\r\n\r\n\r\n\r\n"}