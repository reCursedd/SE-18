{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324332706", "html_url": "https://github.com/tensorflow/tensorflow/issues/12512#issuecomment-324332706", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12512", "id": 324332706, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDMzMjcwNg==", "user": {"login": "alanhdu", "id": 1914111, "node_id": "MDQ6VXNlcjE5MTQxMTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1914111?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanhdu", "html_url": "https://github.com/alanhdu", "followers_url": "https://api.github.com/users/alanhdu/followers", "following_url": "https://api.github.com/users/alanhdu/following{/other_user}", "gists_url": "https://api.github.com/users/alanhdu/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanhdu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanhdu/subscriptions", "organizations_url": "https://api.github.com/users/alanhdu/orgs", "repos_url": "https://api.github.com/users/alanhdu/repos", "events_url": "https://api.github.com/users/alanhdu/events{/privacy}", "received_events_url": "https://api.github.com/users/alanhdu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-23T13:33:33Z", "updated_at": "2017-08-24T16:41:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hm... after slogging through <code>tfdbg</code>, the first example looks like an overflow thing (although the gradient seems absurdly high)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.training <span class=\"pl-k\">import</span> training_ops\n\ngrad <span class=\"pl-k\">=</span> np.array(\n    [\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">7.22536593e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.11869319e+16</span>, <span class=\"pl-c1\">1.31751609e+19</span>, <span class=\"pl-c1\">6.53208027e+19</span>,\n        <span class=\"pl-c1\">8.03179962e+19</span>, <span class=\"pl-c1\">2.80957841e+16</span>, <span class=\"pl-c1\">1.08204829e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.71410878e+15</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.96436248e+15</span>, <span class=\"pl-c1\">2.08896901e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">9.15358559e+19</span>, <span class=\"pl-c1\">1.59031152e+19</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.18680801e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">8.42016120e+19</span>, <span class=\"pl-c1\">4.48369031e+16</span>, <span class=\"pl-c1\">2.01818666e+16</span>,\n        <span class=\"pl-c1\">3.37646465e+15</span>, <span class=\"pl-c1\">1.25098911e+16</span>, <span class=\"pl-c1\">2.19448051e+19</span>, <span class=\"pl-c1\">1.66769521e+16</span>,\n        <span class=\"pl-c1\">3.23241365e+16</span>, <span class=\"pl-c1\">4.21188462e+18</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.90144856e+18</span>, <span class=\"pl-c1\">2.47123573e+19</span>,\n        <span class=\"pl-c1\">2.10178466e+15</span>, <span class=\"pl-c1\">1.37156984e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.02427969e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">6.87006047e+19</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">8.75682902e+19</span>, <span class=\"pl-c1\">3.87922260e+19</span>, <span class=\"pl-c1\">7.33888846e+15</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.27278298e+15</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">6.55280261e+15</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.24261527e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.42708770e+14</span>, <span class=\"pl-c1\">1.20307027e+16</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">7.98193545e+19</span>, <span class=\"pl-c1\">5.26116710e+19</span>, <span class=\"pl-c1\">1.41174265e+16</span>, <span class=\"pl-c1\">1.92888710e+19</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">3.83293183e+14</span>, <span class=\"pl-c1\">1.04124438e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">7.68158516e+18</span>, <span class=\"pl-c1\">3.37947113e+15</span>,\n        <span class=\"pl-c1\">1.91912045e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">6.33320791e+15</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.76087407e+15</span>, <span class=\"pl-c1\">6.54729217e+15</span>,\n        <span class=\"pl-c1\">2.32993309e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.43404496e+19</span>, <span class=\"pl-c1\">3.57337199e+16</span>, <span class=\"pl-c1\">4.92618898e+15</span>,\n        <span class=\"pl-c1\">2.30337893e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">6.57711578e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">6.21250215e+15</span>, <span class=\"pl-c1\">3.39375055e+15</span>,\n        <span class=\"pl-c1\">4.12803196e+19</span>, <span class=\"pl-c1\">4.95062884e+16</span>, <span class=\"pl-c1\">4.91424489e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.25883009e+19</span>,\n        <span class=\"pl-c1\">6.75019995e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.07368472e+20</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.72674288e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.15710970e+18</span>,\n        <span class=\"pl-c1\">5.90043635e+19</span>, <span class=\"pl-c1\">2.97534074e+15</span>, <span class=\"pl-c1\">9.87953683e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.51910824e+16</span>,\n        <span class=\"pl-c1\">9.75750255e+16</span>, <span class=\"pl-c1\">2.14883079e+15</span>, <span class=\"pl-c1\">9.92727718e+18</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.58309663e+19</span>,\n        <span class=\"pl-c1\">6.07305369e+16</span>, <span class=\"pl-c1\">4.11130619e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.76947914e+19</span>, <span class=\"pl-c1\">1.46329901e+16</span>,\n        <span class=\"pl-c1\">9.57690131e+18</span>, <span class=\"pl-c1\">3.04936960e+15</span>, <span class=\"pl-c1\">3.83920426e+16</span>, <span class=\"pl-c1\">7.89846668e+19</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.52319529e+18</span>, <span class=\"pl-c1\">4.89968624e+16</span>, <span class=\"pl-c1\">2.77158535e+15</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">9.70234459e+18</span>,\n        <span class=\"pl-c1\">7.63919392e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">7.99243622e+19</span>, <span class=\"pl-c1\">7.39167363e+19</span>, <span class=\"pl-c1\">1.37408169e+16</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.07268442e+19</span>, <span class=\"pl-c1\">1.93819755e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.04015647e+16</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.94786560e+15</span>,\n        <span class=\"pl-c1\">7.09745708e+15</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.47805236e+19</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.97251429e+19</span>, <span class=\"pl-c1\">1.26310253e+16</span>\n    ],\n    <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\nvar <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>var<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">96</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.zeros_initializer)\nm <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>m<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">96</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.zeros_initializer)\nv <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>v<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">96</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.zeros_initializer)\n\nbeta1_power <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.8999999761581421</span>)\nbeta2_power <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.9990000128746033</span>)\nlr <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.0010000000474974513</span>)\nbeta1 <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.8999999761581421</span>)\nbeta2 <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.9990000128746033</span>)\nepsilon <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">9.99999993922529e-09</span>)\n\nop <span class=\"pl-k\">=</span> training_ops.apply_adam(var, m, v, beta1_power, beta2_power, lr, beta1,  beta2, epsilon, grad)\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(op)\n<span class=\"pl-c1\">print</span>(sess.run(var))   <span class=\"pl-c\"><span class=\"pl-c\">#</span> see the NaNs</span></pre></div>\n<p>There's definitely another problem lurking here though, because I've also seen <code>NaN</code>s even with gradient clipping (which should prevent the overflow) -- I'll report back with more details.</p>\n<p>EDIT: Posted second test case with gradient clipping</p>", "body_text": "Hm... after slogging through tfdbg, the first example looks like an overflow thing (although the gradient seems absurdly high)\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.training import training_ops\n\ngrad = np.array(\n    [\n        -7.22536593e+16, -2.11869319e+16, 1.31751609e+19, 6.53208027e+19,\n        8.03179962e+19, 2.80957841e+16, 1.08204829e+16, -5.71410878e+15,\n        -1.96436248e+15, 2.08896901e+16, -9.15358559e+19, 1.59031152e+19,\n        -1.18680801e+19, -8.42016120e+19, 4.48369031e+16, 2.01818666e+16,\n        3.37646465e+15, 1.25098911e+16, 2.19448051e+19, 1.66769521e+16,\n        3.23241365e+16, 4.21188462e+18, -4.90144856e+18, 2.47123573e+19,\n        2.10178466e+15, 1.37156984e+19, -1.02427969e+16, -6.87006047e+19,\n        -8.75682902e+19, 3.87922260e+19, 7.33888846e+15, -4.27278298e+15,\n        -6.55280261e+15, -1.24261527e+19, -2.42708770e+14, 1.20307027e+16,\n        -7.98193545e+19, 5.26116710e+19, 1.41174265e+16, 1.92888710e+19,\n        -3.83293183e+14, 1.04124438e+16, -7.68158516e+18, 3.37947113e+15,\n        1.91912045e+16, -6.33320791e+15, -4.76087407e+15, 6.54729217e+15,\n        2.32993309e+19, -4.43404496e+19, 3.57337199e+16, 4.92618898e+15,\n        2.30337893e+16, -6.57711578e+16, -6.21250215e+15, 3.39375055e+15,\n        4.12803196e+19, 4.95062884e+16, 4.91424489e+16, -1.25883009e+19,\n        6.75019995e+16, -1.07368472e+20, -1.72674288e+19, -4.15710970e+18,\n        5.90043635e+19, 2.97534074e+15, 9.87953683e+19, -1.51910824e+16,\n        9.75750255e+16, 2.14883079e+15, 9.92727718e+18, -5.58309663e+19,\n        6.07305369e+16, 4.11130619e+19, -2.76947914e+19, 1.46329901e+16,\n        9.57690131e+18, 3.04936960e+15, 3.83920426e+16, 7.89846668e+19,\n        -2.52319529e+18, 4.89968624e+16, 2.77158535e+15, -9.70234459e+18,\n        7.63919392e+19, -7.99243622e+19, 7.39167363e+19, 1.37408169e+16,\n        -5.07268442e+19, 1.93819755e+19, -1.04015647e+16, -5.94786560e+15,\n        7.09745708e+15, -5.47805236e+19, -2.97251429e+19, 1.26310253e+16\n    ],\n    dtype=np.float32)\n\nvar = tf.get_variable(\"var\", [96], initializer=tf.zeros_initializer)\nm = tf.get_variable(\"m\", [96], initializer=tf.zeros_initializer)\nv = tf.get_variable(\"v\", [96], initializer=tf.zeros_initializer)\n\nbeta1_power = tf.constant(0.8999999761581421)\nbeta2_power = tf.constant(0.9990000128746033)\nlr = tf.constant(0.0010000000474974513)\nbeta1 = tf.constant(0.8999999761581421)\nbeta2 = tf.constant(0.9990000128746033)\nepsilon = tf.constant(9.99999993922529e-09)\n\nop = training_ops.apply_adam(var, m, v, beta1_power, beta2_power, lr, beta1,  beta2, epsilon, grad)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(op)\nprint(sess.run(var))   # see the NaNs\nThere's definitely another problem lurking here though, because I've also seen NaNs even with gradient clipping (which should prevent the overflow) -- I'll report back with more details.\nEDIT: Posted second test case with gradient clipping", "body": "Hm... after slogging through `tfdbg`, the first example looks like an overflow thing (although the gradient seems absurdly high)\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training import training_ops\r\n\r\ngrad = np.array(\r\n    [\r\n        -7.22536593e+16, -2.11869319e+16, 1.31751609e+19, 6.53208027e+19,\r\n        8.03179962e+19, 2.80957841e+16, 1.08204829e+16, -5.71410878e+15,\r\n        -1.96436248e+15, 2.08896901e+16, -9.15358559e+19, 1.59031152e+19,\r\n        -1.18680801e+19, -8.42016120e+19, 4.48369031e+16, 2.01818666e+16,\r\n        3.37646465e+15, 1.25098911e+16, 2.19448051e+19, 1.66769521e+16,\r\n        3.23241365e+16, 4.21188462e+18, -4.90144856e+18, 2.47123573e+19,\r\n        2.10178466e+15, 1.37156984e+19, -1.02427969e+16, -6.87006047e+19,\r\n        -8.75682902e+19, 3.87922260e+19, 7.33888846e+15, -4.27278298e+15,\r\n        -6.55280261e+15, -1.24261527e+19, -2.42708770e+14, 1.20307027e+16,\r\n        -7.98193545e+19, 5.26116710e+19, 1.41174265e+16, 1.92888710e+19,\r\n        -3.83293183e+14, 1.04124438e+16, -7.68158516e+18, 3.37947113e+15,\r\n        1.91912045e+16, -6.33320791e+15, -4.76087407e+15, 6.54729217e+15,\r\n        2.32993309e+19, -4.43404496e+19, 3.57337199e+16, 4.92618898e+15,\r\n        2.30337893e+16, -6.57711578e+16, -6.21250215e+15, 3.39375055e+15,\r\n        4.12803196e+19, 4.95062884e+16, 4.91424489e+16, -1.25883009e+19,\r\n        6.75019995e+16, -1.07368472e+20, -1.72674288e+19, -4.15710970e+18,\r\n        5.90043635e+19, 2.97534074e+15, 9.87953683e+19, -1.51910824e+16,\r\n        9.75750255e+16, 2.14883079e+15, 9.92727718e+18, -5.58309663e+19,\r\n        6.07305369e+16, 4.11130619e+19, -2.76947914e+19, 1.46329901e+16,\r\n        9.57690131e+18, 3.04936960e+15, 3.83920426e+16, 7.89846668e+19,\r\n        -2.52319529e+18, 4.89968624e+16, 2.77158535e+15, -9.70234459e+18,\r\n        7.63919392e+19, -7.99243622e+19, 7.39167363e+19, 1.37408169e+16,\r\n        -5.07268442e+19, 1.93819755e+19, -1.04015647e+16, -5.94786560e+15,\r\n        7.09745708e+15, -5.47805236e+19, -2.97251429e+19, 1.26310253e+16\r\n    ],\r\n    dtype=np.float32)\r\n\r\nvar = tf.get_variable(\"var\", [96], initializer=tf.zeros_initializer)\r\nm = tf.get_variable(\"m\", [96], initializer=tf.zeros_initializer)\r\nv = tf.get_variable(\"v\", [96], initializer=tf.zeros_initializer)\r\n\r\nbeta1_power = tf.constant(0.8999999761581421)\r\nbeta2_power = tf.constant(0.9990000128746033)\r\nlr = tf.constant(0.0010000000474974513)\r\nbeta1 = tf.constant(0.8999999761581421)\r\nbeta2 = tf.constant(0.9990000128746033)\r\nepsilon = tf.constant(9.99999993922529e-09)\r\n\r\nop = training_ops.apply_adam(var, m, v, beta1_power, beta2_power, lr, beta1,  beta2, epsilon, grad)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(op)\r\nprint(sess.run(var))   # see the NaNs\r\n```\r\n\r\nThere's definitely another problem lurking here though, because I've also seen `NaN`s even with gradient clipping (which should prevent the overflow) -- I'll report back with more details.\r\n\r\nEDIT: Posted second test case with gradient clipping"}