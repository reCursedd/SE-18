{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/117067543", "pull_request_review_id": 38726834, "id": 117067543, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNzA2NzU0Mw==", "diff_hunk": "@@ -0,0 +1,301 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/mpi/mpi_rendezvous_mgr.h\"\n+\n+#include <chrono>\n+#include <functional>\n+#include <memory>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"tensorflow/core/distributed_runtime/tensor_coding.h\"\n+#include \"tensorflow/core/common_runtime/device.h\"\n+#include \"tensorflow/core/common_runtime/device_mgr.h\"\n+#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\n+#include \"tensorflow/core/distributed_runtime/session_mgr.h\"\n+\n+namespace tensorflow {\n+\n+void MPIRemoteRendezvous::RecvFromRemoteAsync(\n+    const Rendezvous::ParsedKey& parsed, const Rendezvous::Args& recv_args,\n+    DoneCallback done) {\n+\n+  Status s = Status::OK();\n+  MPIRequestTensorCall* rendezvous_call = new MPIRequestTensorCall();\n+\n+  VLOG(2) << \"MPI User requested \" << parsed.FullKey()\n+          << \" @ step: \" << step_id_ << std::endl;\n+\n+  const int dst = mpiutils_->GetSourceID(parsed.FullKey().ToString());\n+\n+  Device* dst_device;\n+  if (s.ok()) {\n+    s = env_->device_mgr->LookupDevice(parsed.dst_device, &dst_device);\n+  } else {\n+    done(s, Args(), recv_args, Tensor{}, false);\n+    return;\n+  }\n+\n+  // Set properties of the request object and create the request function\n+  rendezvous_call->Init(parsed, step_id_);\n+\n+  std::function<void()> request_call = [parsed, dst, rendezvous_call]() {\n+    // Use MPI_Alloc_mem here to force allocation inside MPI thread\n+    // this is not optimal, but prevents memory corruption and segmentation\n+    // faults during inter-server transfers...\n+    MPI_CHECK(MPI_Alloc_mem(rendezvous_call->request_buffer_size_,\n+                            MPI_INFO_NULL, &rendezvous_call->request_buffer_));\n+    rendezvous_call->req_.SerializeToArray(\n+        rendezvous_call->request_buffer_,\n+        rendezvous_call->request_buffer_size_);\n+    MPI_CHECK(MPI_Isend(rendezvous_call->request_buffer_,\n+                        rendezvous_call->request_buffer_size_, MPI_CHAR, dst,\n+                        TAG_REQTENSOR, MPI_COMM_WORLD,\n+                        &rendezvous_call->mpi_request_));\n+  };\n+\n+  // Create the function which is called when the Tensor is send by remote\n+  const int64 temp1 = step_id_;\n+  rendezvous_call->recv_call_ = [this, parsed, recv_args, done, dst, temp1,\n+                                 rendezvous_call](MPIRecvTensorResponse mRes) {\n+    Status s;\n+    Device* dst_device;\n+    if (s.ok()) {\n+      s = env_->device_mgr->LookupDevice(parsed.dst_device, &dst_device);\n+    }\n+\n+    VLOG(3) << \"MPI Received tensor \" << parsed.FullKey()\n+            << \" @ step: \" << temp1 << \" single-send: \" << mRes.singlesend()\n+            << std::endl;\n+\n+    Tensor val;\n+    if (mRes.singlesend()) {\n+      dst_device->MakeTensorFromProto(mRes.response().tensor(),\n+                                      recv_args.alloc_attrs, &val);\n+    } else {\n+      TensorResponse tr;\n+      tr.InitAlloc(dst_device, recv_args.alloc_attrs);\n+      tr.InitPartial(mRes.response());\n+      const size_t nBytes = tr.tensor().TotalBytes();\n+      void* data = const_cast<void*>(DMAHelper::base(&tr.tensor()));\n+      MPI_Status status;\n+      MPI_CHECK(MPI_Recv(data, static_cast<int>(nBytes), MPI_BYTE, dst,\n+                         TAG_SENDTENSOR2, MPI_COMM_WORLD, &status));\n+      val = std::move(tr.tensor());\n+    }\n+\n+    done(s, Args(), recv_args, val, mRes.response().is_dead());\n+  };\n+\n+  auto mgr = dynamic_cast<MPIRendezvousMgr*>(this->rendezvous_mgr_);\n+  mgr->QueueRequest(parsed.FullKey().ToString(), step_id_,\n+                    std::move(request_call), rendezvous_call);\n+}\n+\n+MPIRemoteRendezvous::~MPIRemoteRendezvous() {\n+  auto mgr = dynamic_cast<MPIRendezvousMgr*>(this->rendezvous_mgr_);\n+  mgr->RemoveStepID(step_id_);\n+}\n+\n+/*\n+ * Add the request for one of our Tensors by a remote process\n+ * to the local send/table. The here created callback will\n+ * be called once the Tensor data has arrived and is\n+ * ready to be send to the remote requester.\n+ */\n+void MPIRendezvousMgr::AddRequest(RecvTensorRequest request,\n+                                  const int mpi_dst) {\n+  const int64 step_id = request.step_id();\n+  const std::string& key = request.rendezvous_key();\n+  Rendezvous::ParsedKey parsed;\n+  Status s = Rendezvous::ParseKey(key, &parsed);\n+\n+  MPIRecvTensorCallBack send_cb = [this, mpi_dst, parsed](\n+      const Status& status, const Rendezvous::Args& send_args,\n+      const Rendezvous::Args& recv_args, const Tensor& val, bool is_dead,\n+      MPISendTensorCall* mpiSC) {\n+    // TODO(jbedorf) this should be a loop over max size\n+    CHECK(mpiSC->mRes_.ByteSize() < INT_MAX)\n+        << \"Buffer too large for single transfer\";\n+    MPI_CHECK(MPI_Alloc_mem(mpiSC->mRes_.ByteSize(), MPI_INFO_NULL,\n+                            &mpiSC->send_buffer_));\n+    mpiSC->mRes_.SerializeToArray(mpiSC->send_buffer_, mpiSC->mRes_.ByteSize());\n+\n+    MPI_CHECK(MPI_Isend(\n+        mpiSC->send_buffer_, static_cast<int>(mpiSC->mRes_.ByteSize()),\n+        MPI_CHAR, mpi_dst, TAG_SENDTENSOR, MPI_COMM_WORLD, &(mpiSC->msg1_)));\n+    MPI_CHECK(MPI_Test(&mpiSC->msg1_, &mpiSC->done1_, MPI_STATUS_IGNORE));\n+\n+    if (!mpiSC->mRes_.singlesend()) {\n+      const int tensor_size = static_cast<int>(val.TotalBytes());\n+      void* temp = const_cast<void*>(DMAHelper::base(&val));\n+\n+      // If the MPI library is not GPU aware there should be a data transfer\n+      // here to get the data on the host.\n+      // if(src_dev->tensorflow_gpu_device_info()) //memcpy to send_buffer2_\n+\n+      // TODO(jbedorf)  this should be a loop over max size\n+      MPI_CHECK(MPI_Isend(temp, tensor_size, MPI_CHAR, mpi_dst, TAG_SENDTENSOR2,\n+                          MPI_COMM_WORLD, &mpiSC->msg2_));\n+      mpiSC->done2_ = 0;\n+    }\n+    return mpiSC;\n+  };\n+\n+  // Wrapper around the read callback to place the callback on our queue\n+  Rendezvous::DoneCallback done_cb = [this, parsed, step_id, send_cb](\n+      const Status& status, const Rendezvous::Args& send_args,\n+      const Rendezvous::Args& recv_args, const Tensor& val, bool is_dead) {\n+    if (!status.ok()) {\n+      CHECK(status.ok()) << \"RecvLocalAsync was not ok, key: \"", "path": "tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc", "position": 200, "original_position": 164, "commit_id": "047546d6fee2549c5963a338cdcc2ac801097d76", "original_commit_id": "b78b5414b8ef61d41bb2bd65bd4618c3555dbf15", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "Someday you may want to propagate the bad status further along, rather than CHECK failing.  Ok for now, though.", "created_at": "2017-05-17T17:45:48Z", "updated_at": "2017-05-24T08:41:03Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9864#discussion_r117067543", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9864", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/117067543"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9864#discussion_r117067543"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9864"}}, "body_html": "<p>Someday you may want to propagate the bad status further along, rather than CHECK failing.  Ok for now, though.</p>", "body_text": "Someday you may want to propagate the bad status further along, rather than CHECK failing.  Ok for now, though."}