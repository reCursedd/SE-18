{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323146301", "html_url": "https://github.com/tensorflow/tensorflow/issues/11825#issuecomment-323146301", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11825", "id": 323146301, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzE0NjMwMQ==", "user": {"login": "junshi15", "id": 12075848, "node_id": "MDQ6VXNlcjEyMDc1ODQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/12075848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junshi15", "html_url": "https://github.com/junshi15", "followers_url": "https://api.github.com/users/junshi15/followers", "following_url": "https://api.github.com/users/junshi15/following{/other_user}", "gists_url": "https://api.github.com/users/junshi15/gists{/gist_id}", "starred_url": "https://api.github.com/users/junshi15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junshi15/subscriptions", "organizations_url": "https://api.github.com/users/junshi15/orgs", "repos_url": "https://api.github.com/users/junshi15/repos", "events_url": "https://api.github.com/users/junshi15/events{/privacy}", "received_events_url": "https://api.github.com/users/junshi15/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T17:51:17Z", "updated_at": "2017-08-17T17:51:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22274255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shamoya\">@shamoya</a> With my limited understanding of RDMA_SEND, I do not see how it can solve the problem.<br>\nWith RDMA_SEND, the receiver specifies destination MR. But the MR needs to be created first. To create it, the receiver needs to know its size. The size is on the sender side. So it seems to me you still need to duplicate recv at least twice, first, get the size; second, send the tensor.</p>\n<p>Correct me if I am wrong, my understanding of RDMA_SEND is that it requires the same ordering at both sender and receiver. If the sender post_send A, B and C, in that order, but the receiver post_recv B, C and A. Then A goes to B's buffer, B goes to C's buffer, C goes to A's buffer. I do not know if we can enforce the ordering strictly.</p>\n<p>I do not have a simple solution to work around duplicate_recv. One solution will be to buffer the tensors and its callback, if any, inside RDMARendezvous. We do local_recv once, if that tensor can not be sent for any reason, i.e. MR not initialized, size changed, buffer busy, etc, we will just make a full copy of the tensor and put it in a table inside RDMARendezvous. For future recv of the same tensor, we look it up here instead of going to that waiter-table. Basically, we need to duplicate that waiter-table inside RDMARendezvous. Not a clean solution, but maybe doable.</p>\n<p>What do you think?</p>", "body_text": "@shamoya With my limited understanding of RDMA_SEND, I do not see how it can solve the problem.\nWith RDMA_SEND, the receiver specifies destination MR. But the MR needs to be created first. To create it, the receiver needs to know its size. The size is on the sender side. So it seems to me you still need to duplicate recv at least twice, first, get the size; second, send the tensor.\nCorrect me if I am wrong, my understanding of RDMA_SEND is that it requires the same ordering at both sender and receiver. If the sender post_send A, B and C, in that order, but the receiver post_recv B, C and A. Then A goes to B's buffer, B goes to C's buffer, C goes to A's buffer. I do not know if we can enforce the ordering strictly.\nI do not have a simple solution to work around duplicate_recv. One solution will be to buffer the tensors and its callback, if any, inside RDMARendezvous. We do local_recv once, if that tensor can not be sent for any reason, i.e. MR not initialized, size changed, buffer busy, etc, we will just make a full copy of the tensor and put it in a table inside RDMARendezvous. For future recv of the same tensor, we look it up here instead of going to that waiter-table. Basically, we need to duplicate that waiter-table inside RDMARendezvous. Not a clean solution, but maybe doable.\nWhat do you think?", "body": "@shamoya With my limited understanding of RDMA_SEND, I do not see how it can solve the problem.\r\nWith RDMA_SEND, the receiver specifies destination MR. But the MR needs to be created first. To create it, the receiver needs to know its size. The size is on the sender side. So it seems to me you still need to duplicate recv at least twice, first, get the size; second, send the tensor.\r\n\r\nCorrect me if I am wrong, my understanding of RDMA_SEND is that it requires the same ordering at both sender and receiver. If the sender post_send A, B and C, in that order, but the receiver post_recv B, C and A. Then A goes to B's buffer, B goes to C's buffer, C goes to A's buffer. I do not know if we can enforce the ordering strictly.\r\n\r\nI do not have a simple solution to work around duplicate_recv. One solution will be to buffer the tensors and its callback, if any, inside RDMARendezvous. We do local_recv once, if that tensor can not be sent for any reason, i.e. MR not initialized, size changed, buffer busy, etc, we will just make a full copy of the tensor and put it in a table inside RDMARendezvous. For future recv of the same tensor, we look it up here instead of going to that waiter-table. Basically, we need to duplicate that waiter-table inside RDMARendezvous. Not a clean solution, but maybe doable.\r\n\r\nWhat do you think?\r\n\r\n\r\n"}