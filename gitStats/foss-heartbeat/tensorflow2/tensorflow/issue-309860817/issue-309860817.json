{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18099", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18099/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18099/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18099/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18099", "id": 309860817, "node_id": "MDU6SXNzdWUzMDk4NjA4MTc=", "number": 18099, "title": "Luong attention fails when used with scale=True and dtype=tf.float16", "user": {"login": "Kipok", "id": 2354422, "node_id": "MDQ6VXNlcjIzNTQ0MjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2354422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kipok", "html_url": "https://github.com/Kipok", "followers_url": "https://api.github.com/users/Kipok/followers", "following_url": "https://api.github.com/users/Kipok/following{/other_user}", "gists_url": "https://api.github.com/users/Kipok/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kipok/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kipok/subscriptions", "organizations_url": "https://api.github.com/users/Kipok/orgs", "repos_url": "https://api.github.com/users/Kipok/repos", "events_url": "https://api.github.com/users/Kipok/events{/privacy}", "received_events_url": "https://api.github.com/users/Kipok/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-29T18:38:50Z", "updated_at": "2018-03-30T17:41:49Z", "closed_at": "2018-03-30T17:41:49Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: b'v1.6.0-rc1-1857-g67e2efa' 1.6.0</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.10.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7.0</li>\n<li><strong>GPU model and memory</strong>: not relevant</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import tensorflow as tf\ndtype = tf.float16\n\nwith tf.variable_scope(\"name\", dtype=dtype):\n    cell = tf.nn.rnn_cell.LSTMCell(128)\n\n    encoder_outputs = tf.placeholder(dtype, shape=[64, None, 256])\n    input_lengths = tf.placeholder(tf.int32, shape=[64])\n    tgt_lengths = tf.placeholder(tf.int32, shape=[64])\n    input_vectors = tf.placeholder(dtype, shape=[64, None, 128])\n\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n        num_units=128,\n        memory=encoder_outputs,\n        scale=True,\n        memory_sequence_length=input_lengths,\n        probability_fn=tf.nn.softmax,\n        dtype=dtype,\n    )\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism)\n\n    helper = tf.contrib.seq2seq.TrainingHelper(\n        inputs=input_vectors,\n        sequence_length=tgt_lengths,\n    )\n\n    decoder = tf.contrib.seq2seq.BasicDecoder(\n        cell=attn_cell,\n        helper=helper,\n        initial_state=attn_cell.zero_state(64, dtype),\n    )\n\n    tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\n</code></pre>\n<h3>Describe the problem</h3>\n<p>Luong attention fails when using with scale=True and dtype=tf.float16. Changing lines 341-342 of attention_wrapper.py to:</p>\n<pre><code>g = variable_scope.get_variable(\n    \"attention_g\", dtype=dtype, shape=(),\n    initializer=init_ops.ones_initializer(),\n)\n</code></pre>\n<p>seems to solve the problem.</p>\n<h3>Source code / logs</h3>\n<p>Traceback:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-1-4dec9cd8e3b2&gt; in &lt;module&gt;()\n     31     )\n     32 \n---&gt; 33     tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\n    307         ],\n    308         parallel_iterations=parallel_iterations,\n--&gt; 309         swap_memory=swap_memory)\n    310 \n    311     final_outputs_ta = res[1]\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\n   3203     if loop_context.outer_context is None:\n   3204       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\n-&gt; 3205     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   3206     if maximum_iterations is not None:\n   3207       return result[1]\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n   2941       with ops.get_default_graph()._lock:  # pylint: disable=protected-access\n   2942         original_body_result, exit_vars = self._BuildLoop(\n-&gt; 2943             pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2944     finally:\n   2945       self.Exit()\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2878         flat_sequence=vars_for_body_with_tensor_arrays)\n   2879     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n-&gt; 2880     body_result = body(*packed_vars_for_body)\n   2881     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n   2882     if not nest.is_sequence(body_result):\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\n    252       \"\"\"\n    253       (next_outputs, decoder_state, next_inputs,\n--&gt; 254        decoder_finished) = decoder.step(time, inputs, state)\n    255       if decoder.tracks_own_finished:\n    256         next_finished = decoder_finished\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)\n    135     \"\"\"\n    136     with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\n--&gt; 137       cell_outputs, cell_state = self._cell(inputs, state)\n    138       if self._output_layer is not None:\n    139         cell_outputs = self._output_layer(cell_outputs)\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    230         setattr(self, scope_attrname, scope)\n    231       with scope:\n--&gt; 232         return super(RNNCell, self).__call__(inputs, state)\n    233 \n    234   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    712 \n    713         if not in_deferred_mode:\n--&gt; 714           outputs = self.call(inputs, *args, **kwargs)\n    715           if outputs is None:\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\n   1409       attention, alignments, next_attention_state = _compute_attention(\n   1410           attention_mechanism, cell_output, previous_attention_state[i],\n-&gt; 1411           self._attention_layers[i] if self._attention_layers else None)\n   1412       alignment_history = previous_alignment_history[i].write(\n   1413           state.time, alignments) if self._alignment_history else ()\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _compute_attention(attention_mechanism, cell_output, attention_state, attention_layer)\n   1046   \"\"\"Computes the attention and alignments for a given attention_mechanism.\"\"\"\n   1047   alignments, next_attention_state = attention_mechanism(\n-&gt; 1048       cell_output, state=attention_state)\n   1049 \n   1050   # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in __call__(self, query, state)\n    427     \"\"\"\n    428     with variable_scope.variable_scope(None, \"luong_attention\", [query]):\n--&gt; 429       score = _luong_score(query, self._keys, self._scale)\n    430     alignments = self._probability_fn(score, state)\n    431     next_state = alignments\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _luong_score(query, keys, scale)\n    340     # Scalar used in weight scaling\n    341     g = variable_scope.get_variable(\n--&gt; 342         \"attention_g\", dtype=dtype, initializer=1.)\n    343     score = g * score\n    344   return score\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1315       partitioner=partitioner, validate_shape=validate_shape,\n   1316       use_resource=use_resource, custom_getter=custom_getter,\n-&gt; 1317       constraint=constraint)\n   1318 get_variable_or_local_docstring = (\n   1319     \"\"\"%s\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1064         if init_dtype != dtype:\n   1065           raise ValueError(\"Initializer type '%s' and explicit dtype '%s' \"\n-&gt; 1066                            \"don't match.\" % (init_dtype, dtype))\n   1067       if initializer is None:\n   1068         initializer = self._initializer\n\nValueError: Initializer type '&lt;dtype: 'float32'&gt;' and explicit dtype '&lt;dtype: 'float16'&gt;' don't match.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): b'v1.6.0-rc1-1857-g67e2efa' 1.6.0\nPython version: 3.5.2\nBazel version (if compiling from source): 0.10.0\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.0/7.0\nGPU model and memory: not relevant\nExact command to reproduce:\n\nimport tensorflow as tf\ndtype = tf.float16\n\nwith tf.variable_scope(\"name\", dtype=dtype):\n    cell = tf.nn.rnn_cell.LSTMCell(128)\n\n    encoder_outputs = tf.placeholder(dtype, shape=[64, None, 256])\n    input_lengths = tf.placeholder(tf.int32, shape=[64])\n    tgt_lengths = tf.placeholder(tf.int32, shape=[64])\n    input_vectors = tf.placeholder(dtype, shape=[64, None, 128])\n\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n        num_units=128,\n        memory=encoder_outputs,\n        scale=True,\n        memory_sequence_length=input_lengths,\n        probability_fn=tf.nn.softmax,\n        dtype=dtype,\n    )\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism)\n\n    helper = tf.contrib.seq2seq.TrainingHelper(\n        inputs=input_vectors,\n        sequence_length=tgt_lengths,\n    )\n\n    decoder = tf.contrib.seq2seq.BasicDecoder(\n        cell=attn_cell,\n        helper=helper,\n        initial_state=attn_cell.zero_state(64, dtype),\n    )\n\n    tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\n\nDescribe the problem\nLuong attention fails when using with scale=True and dtype=tf.float16. Changing lines 341-342 of attention_wrapper.py to:\ng = variable_scope.get_variable(\n    \"attention_g\", dtype=dtype, shape=(),\n    initializer=init_ops.ones_initializer(),\n)\n\nseems to solve the problem.\nSource code / logs\nTraceback:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-1-4dec9cd8e3b2> in <module>()\n     31     )\n     32 \n---> 33     tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\n    307         ],\n    308         parallel_iterations=parallel_iterations,\n--> 309         swap_memory=swap_memory)\n    310 \n    311     final_outputs_ta = res[1]\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\n   3203     if loop_context.outer_context is None:\n   3204       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\n-> 3205     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   3206     if maximum_iterations is not None:\n   3207       return result[1]\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n   2941       with ops.get_default_graph()._lock:  # pylint: disable=protected-access\n   2942         original_body_result, exit_vars = self._BuildLoop(\n-> 2943             pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2944     finally:\n   2945       self.Exit()\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2878         flat_sequence=vars_for_body_with_tensor_arrays)\n   2879     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n-> 2880     body_result = body(*packed_vars_for_body)\n   2881     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n   2882     if not nest.is_sequence(body_result):\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\n    252       \"\"\"\n    253       (next_outputs, decoder_state, next_inputs,\n--> 254        decoder_finished) = decoder.step(time, inputs, state)\n    255       if decoder.tracks_own_finished:\n    256         next_finished = decoder_finished\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)\n    135     \"\"\"\n    136     with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\n--> 137       cell_outputs, cell_state = self._cell(inputs, state)\n    138       if self._output_layer is not None:\n    139         cell_outputs = self._output_layer(cell_outputs)\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    230         setattr(self, scope_attrname, scope)\n    231       with scope:\n--> 232         return super(RNNCell, self).__call__(inputs, state)\n    233 \n    234   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    712 \n    713         if not in_deferred_mode:\n--> 714           outputs = self.call(inputs, *args, **kwargs)\n    715           if outputs is None:\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\n   1409       attention, alignments, next_attention_state = _compute_attention(\n   1410           attention_mechanism, cell_output, previous_attention_state[i],\n-> 1411           self._attention_layers[i] if self._attention_layers else None)\n   1412       alignment_history = previous_alignment_history[i].write(\n   1413           state.time, alignments) if self._alignment_history else ()\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _compute_attention(attention_mechanism, cell_output, attention_state, attention_layer)\n   1046   \"\"\"Computes the attention and alignments for a given attention_mechanism.\"\"\"\n   1047   alignments, next_attention_state = attention_mechanism(\n-> 1048       cell_output, state=attention_state)\n   1049 \n   1050   # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in __call__(self, query, state)\n    427     \"\"\"\n    428     with variable_scope.variable_scope(None, \"luong_attention\", [query]):\n--> 429       score = _luong_score(query, self._keys, self._scale)\n    430     alignments = self._probability_fn(score, state)\n    431     next_state = alignments\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _luong_score(query, keys, scale)\n    340     # Scalar used in weight scaling\n    341     g = variable_scope.get_variable(\n--> 342         \"attention_g\", dtype=dtype, initializer=1.)\n    343     score = g * score\n    344   return score\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1315       partitioner=partitioner, validate_shape=validate_shape,\n   1316       use_resource=use_resource, custom_getter=custom_getter,\n-> 1317       constraint=constraint)\n   1318 get_variable_or_local_docstring = (\n   1319     \"\"\"%s\n\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1064         if init_dtype != dtype:\n   1065           raise ValueError(\"Initializer type '%s' and explicit dtype '%s' \"\n-> 1066                            \"don't match.\" % (init_dtype, dtype))\n   1067       if initializer is None:\n   1068         initializer = self._initializer\n\nValueError: Initializer type '<dtype: 'float32'>' and explicit dtype '<dtype: 'float16'>' don't match.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: b'v1.6.0-rc1-1857-g67e2efa' 1.6.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: not relevant\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\ndtype = tf.float16\r\n\r\nwith tf.variable_scope(\"name\", dtype=dtype):\r\n    cell = tf.nn.rnn_cell.LSTMCell(128)\r\n\r\n    encoder_outputs = tf.placeholder(dtype, shape=[64, None, 256])\r\n    input_lengths = tf.placeholder(tf.int32, shape=[64])\r\n    tgt_lengths = tf.placeholder(tf.int32, shape=[64])\r\n    input_vectors = tf.placeholder(dtype, shape=[64, None, 128])\r\n\r\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n        num_units=128,\r\n        memory=encoder_outputs,\r\n        scale=True,\r\n        memory_sequence_length=input_lengths,\r\n        probability_fn=tf.nn.softmax,\r\n        dtype=dtype,\r\n    )\r\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism)\r\n\r\n    helper = tf.contrib.seq2seq.TrainingHelper(\r\n        inputs=input_vectors,\r\n        sequence_length=tgt_lengths,\r\n    )\r\n\r\n    decoder = tf.contrib.seq2seq.BasicDecoder(\r\n        cell=attn_cell,\r\n        helper=helper,\r\n        initial_state=attn_cell.zero_state(64, dtype),\r\n    )\r\n\r\n    tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\r\n```\r\n\r\n### Describe the problem\r\nLuong attention fails when using with scale=True and dtype=tf.float16. Changing lines 341-342 of attention_wrapper.py to:\r\n```\r\ng = variable_scope.get_variable(\r\n    \"attention_g\", dtype=dtype, shape=(),\r\n    initializer=init_ops.ones_initializer(),\r\n)\r\n```\r\nseems to solve the problem.\r\n\r\n### Source code / logs\r\nTraceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-4dec9cd8e3b2> in <module>()\r\n     31     )\r\n     32 \r\n---> 33     tf.contrib.seq2seq.dynamic_decode(decoder=decoder)\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\r\n    307         ],\r\n    308         parallel_iterations=parallel_iterations,\r\n--> 309         swap_memory=swap_memory)\r\n    310 \r\n    311     final_outputs_ta = res[1]\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\r\n   3203     if loop_context.outer_context is None:\r\n   3204       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 3205     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   3206     if maximum_iterations is not None:\r\n   3207       return result[1]\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2941       with ops.get_default_graph()._lock:  # pylint: disable=protected-access\r\n   2942         original_body_result, exit_vars = self._BuildLoop(\r\n-> 2943             pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2944     finally:\r\n   2945       self.Exit()\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2878         flat_sequence=vars_for_body_with_tensor_arrays)\r\n   2879     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n-> 2880     body_result = body(*packed_vars_for_body)\r\n   2881     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n   2882     if not nest.is_sequence(body_result):\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\r\n    252       \"\"\"\r\n    253       (next_outputs, decoder_state, next_inputs,\r\n--> 254        decoder_finished) = decoder.step(time, inputs, state)\r\n    255       if decoder.tracks_own_finished:\r\n    256         next_finished = decoder_finished\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)\r\n    135     \"\"\"\r\n    136     with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\r\n--> 137       cell_outputs, cell_state = self._cell(inputs, state)\r\n    138       if self._output_layer is not None:\r\n    139         cell_outputs = self._output_layer(cell_outputs)\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    230         setattr(self, scope_attrname, scope)\r\n    231       with scope:\r\n--> 232         return super(RNNCell, self).__call__(inputs, state)\r\n    233 \r\n    234   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    712 \r\n    713         if not in_deferred_mode:\r\n--> 714           outputs = self.call(inputs, *args, **kwargs)\r\n    715           if outputs is None:\r\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\r\n   1409       attention, alignments, next_attention_state = _compute_attention(\r\n   1410           attention_mechanism, cell_output, previous_attention_state[i],\r\n-> 1411           self._attention_layers[i] if self._attention_layers else None)\r\n   1412       alignment_history = previous_alignment_history[i].write(\r\n   1413           state.time, alignments) if self._alignment_history else ()\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _compute_attention(attention_mechanism, cell_output, attention_state, attention_layer)\r\n   1046   \"\"\"Computes the attention and alignments for a given attention_mechanism.\"\"\"\r\n   1047   alignments, next_attention_state = attention_mechanism(\r\n-> 1048       cell_output, state=attention_state)\r\n   1049 \r\n   1050   # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in __call__(self, query, state)\r\n    427     \"\"\"\r\n    428     with variable_scope.variable_scope(None, \"luong_attention\", [query]):\r\n--> 429       score = _luong_score(query, self._keys, self._scale)\r\n    430     alignments = self._probability_fn(score, state)\r\n    431     next_state = alignments\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _luong_score(query, keys, scale)\r\n    340     # Scalar used in weight scaling\r\n    341     g = variable_scope.get_variable(\r\n--> 342         \"attention_g\", dtype=dtype, initializer=1.)\r\n    343     score = g * score\r\n    344   return score\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1315       partitioner=partitioner, validate_shape=validate_shape,\r\n   1316       use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1317       constraint=constraint)\r\n   1318 get_variable_or_local_docstring = (\r\n   1319     \"\"\"%s\r\n\r\n~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1064         if init_dtype != dtype:\r\n   1065           raise ValueError(\"Initializer type '%s' and explicit dtype '%s' \"\r\n-> 1066                            \"don't match.\" % (init_dtype, dtype))\r\n   1067       if initializer is None:\r\n   1068         initializer = self._initializer\r\n\r\nValueError: Initializer type '<dtype: 'float32'>' and explicit dtype '<dtype: 'float16'>' don't match.\r\n```"}