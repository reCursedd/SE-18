{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5092", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5092/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5092/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5092/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5092", "id": 184187143, "node_id": "MDU6SXNzdWUxODQxODcxNDM=", "number": 5092, "title": "GPU sync failed when use mesos and nvidia-docker for distributed training", "user": {"login": "chenbiaolong", "id": 9475734, "node_id": "MDQ6VXNlcjk0NzU3MzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/9475734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenbiaolong", "html_url": "https://github.com/chenbiaolong", "followers_url": "https://api.github.com/users/chenbiaolong/followers", "following_url": "https://api.github.com/users/chenbiaolong/following{/other_user}", "gists_url": "https://api.github.com/users/chenbiaolong/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenbiaolong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenbiaolong/subscriptions", "organizations_url": "https://api.github.com/users/chenbiaolong/orgs", "repos_url": "https://api.github.com/users/chenbiaolong/repos", "events_url": "https://api.github.com/users/chenbiaolong/events{/privacy}", "received_events_url": "https://api.github.com/users/chenbiaolong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-10-20T10:06:14Z", "updated_at": "2016-12-31T11:13:05Z", "closed_at": "2016-10-20T14:44:45Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>host os: centos7<br>\nhost cuda: <code>/usr/local/cuda -&gt; cuda-7.5</code><br>\ndocker images: <a href=\"https://github.com/chenbiaolong/tfmesos/blob/master/docker/Dockerfile\">Dockerfile</a><br>\ntotal 4 GPUs, output of <code>curl -s slave-ip:3476/mesos/cli</code></p>\n<pre><code>--attributes=gpus:eJzUlN9r2zAQx9_3V4x7tlL9sGTJb6mzldINwtbsZfThJEuLqWsXO00pIf_7zoEyAmFjEMaqJ0u6u-_39OG8g29xGJu-g3IHi6HZxgFKUFrOCgcZVKvFnPbFTMM-g0XcNiGOUH7fwWp1vaCbq-WK-UQrcMsMcscwFcikVYqZOmBSmCvjAtVa4mZNGRd13F5026ZukNPp576OLR3fxrHF9zeSH0L758mHlJosLFfzlJqu2bxAyemuup68Xj6NBwOcVsllyflsKnc5_yIoURv6xK5-bupJ1FIQ-a_aPtyPU3bVD1MfhbbkID70A9WW5hB02z_2bf_j5dDlsUr-qvKp6e6hVPu7DD7iQ9NSMNzEx5ZMZzAfwtSnojfLXoVk7swvpR18qCooN8NTzOCq7T3SC-SFcxl8XeMQa9rZKbcbN9htoDQ5acoKwzpCKaTlR91Jfuhvnx1RQY9ae2sYL0LOguOGuYSeKa2Tz9FH7sVJKuJsVPLfU8nPQ0W-JSo6cZnnKJg20TLpuWdJG8us1UIIZ4Wo_Ukq8q-piNNUrPoXs2Lf1Kwor40mWaYxchZ9on9ZEo5JlM4r5VQI-iQVdTYqf5iVM1FR_zeVu_27nwEAAP__G6_XAw --resources=gpus:{GPU-bffffc08-6a09-af7a-2833-6dcaf3a4369c,GPU-aba55b86-07c4-c906-9fab-355fb4abe0b1,GPU-5f0244a1-56e8-2b0b-f568-8851119811db,GPU-3b565496-5ae0-ebf8-6f19-2a29b3393cc5}\n</code></pre>\n<h3>tensorflow info</h3>\n<p><strong>In docker images</strong>:<br>\npip install from:</p>\n<pre><code>https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl; python_version == '2.7'\n&gt;&gt;&gt; import tensorflow; print(tensorflow.__version__)\n0.9.0\n</code></pre>\n<h3>details</h3>\n<p>I try to use <strong>mesos</strong> and <strong>nvidia-docker</strong> for tensorflow distributed training.The main idea of my code (code fork from <a href=\"https://github.com/douban/tfmesos\">douban/tfmesos</a>)is mapping host GPUs into nvidia-docker and run distributed training scrip in docker environment.Most of my code is in <a href=\"https://github.com/chenbiaolong/tfmesos/tree/master/docker\">chenbiaolong/tfmesos/docker</a>,you can see how I build my docker images in Dockerfile.</p>\n<p>However,when I try to start training(use <a href=\"https://github.com/chenbiaolong/tfmesos/blob/master/nvidia_docker_run.sh\">nvidia_docker_run.sh</a>), I got errors:</p>\n<pre><code>I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {node39.com:36921}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {localhost:42324, node39.com:36202, node39.com:58828}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:42324\nINFO:tensorflow:SyncReplicas enabled: replicas_to_aggregate=3; total_num_replicas=3\nI1020 09:07:12.418039 10 sync_replicas_optimizer.py:175] SyncReplicas enabled: replicas_to_aggregate=3; total_num_replicas=3\nI1020 09:07:12.592327 10 mnist_replica.py:200] Chief Worker 0: Initializing session...\nTraceback (most recent call last):\n  File \"/mnt/example/mnist/mnist_replica.py\", line 247, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/mnt/example/mnist/mnist_replica.py\", line 211, in main\n    config=sess_config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 176, in prepare_session\n    sess.run(init_op, feed_dict=init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.AbortedError: RecvTensor expects a different device incarnation: 7338322613985075898 vs. 11093212615818549611. Your worker job was probably restarted. Check your worker job for the reason why it was restarted.\n     [[Node: truncated_normal_S3 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device_incarnation=7338322613985075898, tensor_name=\"edge_42_truncated_normal\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_DEINITIALIZED :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\nTraceback (most recent call last):\n  File \"/usr/local/bin/tfrun\", line 84, in &lt;module&gt;\n    subprocess.check_call(cmd, shell=True)\n  File \"/usr/lib/python2.7/subprocess.py\", line 540, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command 'python /mnt/example/mnist/mnist_replica.py --ps_hosts node39.com:36921 --worker_hosts node39.com:42324,node39.com:36202,node39.com:58828 --job_name worker --task_index 0' returned non-zero exit status 134\n\n</code></pre>\n<p>Although <strong>nvidia_docker_run.sh</strong> throw errors, mesos tasks(both job:ps and job:worker) seems work well,util they are killed by the framework. Run <strong>nvidia-smi</strong> can see 3GPUs are used(since I ran 3 workers and each work cost 1 GPU ). My training scrip is <a href=\"https://github.com/chenbiaolong/tfmesos/blob/master/docker/example/mnist/mnist_replica.py\">mnist_replica.py</a></p>\n<h3>logs</h3>\n<p><strong>Logs from tasks</strong> is available <a href=\"https://github.com/chenbiaolong/tfmesos/tree/master/docker/logs\">here</a></p>\n<p><strong>mesos slave start</strong></p>\n<pre><code>mesos-slave  --master=$master_ip \\\n            --containerizers=docker,mesos \\\n            --hostname=slave-ip \\\n            --ip=slave-ip \\\n            --log_dir=/var/log/mesos/ \\\n            --work_dir=/var/lib/mesos/ \\\n            $(curl -s slave-ip:3476/mesos/cli)\n</code></pre>", "body_text": "Environment info\nhost os: centos7\nhost cuda: /usr/local/cuda -> cuda-7.5\ndocker images: Dockerfile\ntotal 4 GPUs, output of curl -s slave-ip:3476/mesos/cli\n--attributes=gpus:eJzUlN9r2zAQx9_3V4x7tlL9sGTJb6mzldINwtbsZfThJEuLqWsXO00pIf_7zoEyAmFjEMaqJ0u6u-_39OG8g29xGJu-g3IHi6HZxgFKUFrOCgcZVKvFnPbFTMM-g0XcNiGOUH7fwWp1vaCbq-WK-UQrcMsMcscwFcikVYqZOmBSmCvjAtVa4mZNGRd13F5026ZukNPp576OLR3fxrHF9zeSH0L758mHlJosLFfzlJqu2bxAyemuup68Xj6NBwOcVsllyflsKnc5_yIoURv6xK5-bupJ1FIQ-a_aPtyPU3bVD1MfhbbkID70A9WW5hB02z_2bf_j5dDlsUr-qvKp6e6hVPu7DD7iQ9NSMNzEx5ZMZzAfwtSnojfLXoVk7swvpR18qCooN8NTzOCq7T3SC-SFcxl8XeMQa9rZKbcbN9htoDQ5acoKwzpCKaTlR91Jfuhvnx1RQY9ae2sYL0LOguOGuYSeKa2Tz9FH7sVJKuJsVPLfU8nPQ0W-JSo6cZnnKJg20TLpuWdJG8us1UIIZ4Wo_Ukq8q-piNNUrPoXs2Lf1Kwor40mWaYxchZ9on9ZEo5JlM4r5VQI-iQVdTYqf5iVM1FR_zeVu_27nwEAAP__G6_XAw --resources=gpus:{GPU-bffffc08-6a09-af7a-2833-6dcaf3a4369c,GPU-aba55b86-07c4-c906-9fab-355fb4abe0b1,GPU-5f0244a1-56e8-2b0b-f568-8851119811db,GPU-3b565496-5ae0-ebf8-6f19-2a29b3393cc5}\n\ntensorflow info\nIn docker images:\npip install from:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl; python_version == '2.7'\n>>> import tensorflow; print(tensorflow.__version__)\n0.9.0\n\ndetails\nI try to use mesos and nvidia-docker for tensorflow distributed training.The main idea of my code (code fork from douban/tfmesos)is mapping host GPUs into nvidia-docker and run distributed training scrip in docker environment.Most of my code is in chenbiaolong/tfmesos/docker,you can see how I build my docker images in Dockerfile.\nHowever,when I try to start training(use nvidia_docker_run.sh), I got errors:\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {node39.com:36921}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:42324, node39.com:36202, node39.com:58828}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:42324\nINFO:tensorflow:SyncReplicas enabled: replicas_to_aggregate=3; total_num_replicas=3\nI1020 09:07:12.418039 10 sync_replicas_optimizer.py:175] SyncReplicas enabled: replicas_to_aggregate=3; total_num_replicas=3\nI1020 09:07:12.592327 10 mnist_replica.py:200] Chief Worker 0: Initializing session...\nTraceback (most recent call last):\n  File \"/mnt/example/mnist/mnist_replica.py\", line 247, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/mnt/example/mnist/mnist_replica.py\", line 211, in main\n    config=sess_config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 176, in prepare_session\n    sess.run(init_op, feed_dict=init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.AbortedError: RecvTensor expects a different device incarnation: 7338322613985075898 vs. 11093212615818549611. Your worker job was probably restarted. Check your worker job for the reason why it was restarted.\n     [[Node: truncated_normal_S3 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device_incarnation=7338322613985075898, tensor_name=\"edge_42_truncated_normal\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_DEINITIALIZED :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\nTraceback (most recent call last):\n  File \"/usr/local/bin/tfrun\", line 84, in <module>\n    subprocess.check_call(cmd, shell=True)\n  File \"/usr/lib/python2.7/subprocess.py\", line 540, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command 'python /mnt/example/mnist/mnist_replica.py --ps_hosts node39.com:36921 --worker_hosts node39.com:42324,node39.com:36202,node39.com:58828 --job_name worker --task_index 0' returned non-zero exit status 134\n\n\nAlthough nvidia_docker_run.sh throw errors, mesos tasks(both job:ps and job:worker) seems work well,util they are killed by the framework. Run nvidia-smi can see 3GPUs are used(since I ran 3 workers and each work cost 1 GPU ). My training scrip is mnist_replica.py\nlogs\nLogs from tasks is available here\nmesos slave start\nmesos-slave  --master=$master_ip \\\n            --containerizers=docker,mesos \\\n            --hostname=slave-ip \\\n            --ip=slave-ip \\\n            --log_dir=/var/log/mesos/ \\\n            --work_dir=/var/lib/mesos/ \\\n            $(curl -s slave-ip:3476/mesos/cli)", "body": "### Environment info\n\nhost os: centos7\nhost cuda: `/usr/local/cuda -> cuda-7.5`\ndocker images: [Dockerfile](https://github.com/chenbiaolong/tfmesos/blob/master/docker/Dockerfile)\ntotal 4 GPUs, output of `curl -s slave-ip:3476/mesos/cli`\n\n```\n--attributes=gpus:eJzUlN9r2zAQx9_3V4x7tlL9sGTJb6mzldINwtbsZfThJEuLqWsXO00pIf_7zoEyAmFjEMaqJ0u6u-_39OG8g29xGJu-g3IHi6HZxgFKUFrOCgcZVKvFnPbFTMM-g0XcNiGOUH7fwWp1vaCbq-WK-UQrcMsMcscwFcikVYqZOmBSmCvjAtVa4mZNGRd13F5026ZukNPp576OLR3fxrHF9zeSH0L758mHlJosLFfzlJqu2bxAyemuup68Xj6NBwOcVsllyflsKnc5_yIoURv6xK5-bupJ1FIQ-a_aPtyPU3bVD1MfhbbkID70A9WW5hB02z_2bf_j5dDlsUr-qvKp6e6hVPu7DD7iQ9NSMNzEx5ZMZzAfwtSnojfLXoVk7swvpR18qCooN8NTzOCq7T3SC-SFcxl8XeMQa9rZKbcbN9htoDQ5acoKwzpCKaTlR91Jfuhvnx1RQY9ae2sYL0LOguOGuYSeKa2Tz9FH7sVJKuJsVPLfU8nPQ0W-JSo6cZnnKJg20TLpuWdJG8us1UIIZ4Wo_Ukq8q-piNNUrPoXs2Lf1Kwor40mWaYxchZ9on9ZEo5JlM4r5VQI-iQVdTYqf5iVM1FR_zeVu_27nwEAAP__G6_XAw --resources=gpus:{GPU-bffffc08-6a09-af7a-2833-6dcaf3a4369c,GPU-aba55b86-07c4-c906-9fab-355fb4abe0b1,GPU-5f0244a1-56e8-2b0b-f568-8851119811db,GPU-3b565496-5ae0-ebf8-6f19-2a29b3393cc5}\n```\n### tensorflow info\n\n**In docker images**:\npip install from:\n\n```\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl; python_version == '2.7'\n>>> import tensorflow; print(tensorflow.__version__)\n0.9.0\n```\n### details\n\nI try to use **mesos** and **nvidia-docker** for tensorflow distributed training.The main idea of my code (code fork from [douban/tfmesos](https://github.com/douban/tfmesos))is mapping host GPUs into nvidia-docker and run distributed training scrip in docker environment.Most of my code is in [chenbiaolong/tfmesos/docker](https://github.com/chenbiaolong/tfmesos/tree/master/docker),you can see how I build my docker images in Dockerfile.\n\nHowever,when I try to start training(use [nvidia_docker_run.sh](https://github.com/chenbiaolong/tfmesos/blob/master/nvidia_docker_run.sh)), I got errors:\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {node39.com:36921}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:42324, node39.com:36202, node39.com:58828}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:42324\nINFO:tensorflow:SyncReplicas enabled: replicas_to_aggregate=3; total_num_replicas=3\nI1020 09:07:12.418039 10 sync_replicas_optimizer.py:175] SyncReplicas enabled: replicas_to_aggregate=3; total_num_replicas=3\nI1020 09:07:12.592327 10 mnist_replica.py:200] Chief Worker 0: Initializing session...\nTraceback (most recent call last):\n  File \"/mnt/example/mnist/mnist_replica.py\", line 247, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/mnt/example/mnist/mnist_replica.py\", line 211, in main\n    config=sess_config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 176, in prepare_session\n    sess.run(init_op, feed_dict=init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.AbortedError: RecvTensor expects a different device incarnation: 7338322613985075898 vs. 11093212615818549611. Your worker job was probably restarted. Check your worker job for the reason why it was restarted.\n     [[Node: truncated_normal_S3 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device_incarnation=7338322613985075898, tensor_name=\"edge_42_truncated_normal\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_DEINITIALIZED :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\nTraceback (most recent call last):\n  File \"/usr/local/bin/tfrun\", line 84, in <module>\n    subprocess.check_call(cmd, shell=True)\n  File \"/usr/lib/python2.7/subprocess.py\", line 540, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command 'python /mnt/example/mnist/mnist_replica.py --ps_hosts node39.com:36921 --worker_hosts node39.com:42324,node39.com:36202,node39.com:58828 --job_name worker --task_index 0' returned non-zero exit status 134\n\n```\n\nAlthough **nvidia_docker_run.sh** throw errors, mesos tasks(both job:ps and job:worker) seems work well,util they are killed by the framework. Run **nvidia-smi** can see 3GPUs are used(since I ran 3 workers and each work cost 1 GPU ). My training scrip is [mnist_replica.py](https://github.com/chenbiaolong/tfmesos/blob/master/docker/example/mnist/mnist_replica.py)\n### logs\n\n**Logs from tasks** is available [here](https://github.com/chenbiaolong/tfmesos/tree/master/docker/logs)\n\n**mesos slave start**\n\n```\nmesos-slave  --master=$master_ip \\\n            --containerizers=docker,mesos \\\n            --hostname=slave-ip \\\n            --ip=slave-ip \\\n            --log_dir=/var/log/mesos/ \\\n            --work_dir=/var/lib/mesos/ \\\n            $(curl -s slave-ip:3476/mesos/cli)\n```\n"}