{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5090", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5090/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5090/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5090/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5090", "id": 184179563, "node_id": "MDU6SXNzdWUxODQxNzk1NjM=", "number": 5090, "title": "LSTM's forget gate biases", "user": {"login": "giancds", "id": 3305726, "node_id": "MDQ6VXNlcjMzMDU3MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/3305726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/giancds", "html_url": "https://github.com/giancds", "followers_url": "https://api.github.com/users/giancds/followers", "following_url": "https://api.github.com/users/giancds/following{/other_user}", "gists_url": "https://api.github.com/users/giancds/gists{/gist_id}", "starred_url": "https://api.github.com/users/giancds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/giancds/subscriptions", "organizations_url": "https://api.github.com/users/giancds/orgs", "repos_url": "https://api.github.com/users/giancds/repos", "events_url": "https://api.github.com/users/giancds/events{/privacy}", "received_events_url": "https://api.github.com/users/giancds/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-10-20T09:32:29Z", "updated_at": "2016-10-20T23:13:31Z", "closed_at": "2016-10-20T23:13:31Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>I was looking into the LSTMCell code and I got confused:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L505\">Line 505</a> show us the computations of new input and the gates: input, forget and output.</p>\n<p>Both <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L517\">line 517</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L517\">line 520</a> show the addition of the forget gate (which is a scalar in this particular piece of code).</p>\n<p>I got confused by this addition as it seems to be already done when we split the gate's values  in line 505 as mentioned before. Am I missing something or is this a second addition to the forget gate computations? If it is a second addition, wouldn't that change the computations?</p>\n<p>By the way, that bias shouldn't be a vector instead of a single scalar?</p>\n<p>Sorry if I am saying nonsense. but I'd like to double check that before saying it is a bug.</p>", "body_text": "Hi all,\nI was looking into the LSTMCell code and I got confused:\nLine 505 show us the computations of new input and the gates: input, forget and output.\nBoth line 517 and line 520 show the addition of the forget gate (which is a scalar in this particular piece of code).\nI got confused by this addition as it seems to be already done when we split the gate's values  in line 505 as mentioned before. Am I missing something or is this a second addition to the forget gate computations? If it is a second addition, wouldn't that change the computations?\nBy the way, that bias shouldn't be a vector instead of a single scalar?\nSorry if I am saying nonsense. but I'd like to double check that before saying it is a bug.", "body": "Hi all,\n\nI was looking into the LSTMCell code and I got confused:\n\n[Line 505](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L505) show us the computations of new input and the gates: input, forget and output.\n\nBoth [line 517](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L517) and [line 520](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L517) show the addition of the forget gate (which is a scalar in this particular piece of code).\n\nI got confused by this addition as it seems to be already done when we split the gate's values  in line 505 as mentioned before. Am I missing something or is this a second addition to the forget gate computations? If it is a second addition, wouldn't that change the computations?\n\nBy the way, that bias shouldn't be a vector instead of a single scalar?\n\nSorry if I am saying nonsense. but I'd like to double check that before saying it is a bug.\n"}