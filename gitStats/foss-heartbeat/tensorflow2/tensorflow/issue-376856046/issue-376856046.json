{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23456", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23456/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23456/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23456/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23456", "id": 376856046, "node_id": "MDU6SXNzdWUzNzY4NTYwNDY=", "number": 23456, "title": "Tensorflow softmax is not updating, docu doesn't help?", "user": {"login": "jandevries123", "id": 36861945, "node_id": "MDQ6VXNlcjM2ODYxOTQ1", "avatar_url": "https://avatars2.githubusercontent.com/u/36861945?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jandevries123", "html_url": "https://github.com/jandevries123", "followers_url": "https://api.github.com/users/jandevries123/followers", "following_url": "https://api.github.com/users/jandevries123/following{/other_user}", "gists_url": "https://api.github.com/users/jandevries123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jandevries123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jandevries123/subscriptions", "organizations_url": "https://api.github.com/users/jandevries123/orgs", "repos_url": "https://api.github.com/users/jandevries123/repos", "events_url": "https://api.github.com/users/jandevries123/events{/privacy}", "received_events_url": "https://api.github.com/users/jandevries123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-02T15:25:13Z", "updated_at": "2018-11-09T18:12:28Z", "closed_at": "2018-11-09T18:12:28Z", "author_association": "NONE", "body_html": "<p>\"DDPG Algorithme not updating the softmax\"</p>\n<p>Dear reader,</p>\n<p>please have a look at my ddpg code and what my problem, thanks in advance.</p>\n<p>Code :: <a href=\"https://hastebin.com/beroramufu.py\" rel=\"nofollow\">https://hastebin.com/beroramufu.py</a></p>\n<p>What is working: The environment, S and S_ are working as they should i've checked several times if they aren't the same and there is a clear diffrence between the two. i changed the action space to discrete action spaces with 3 actions, for the actor network i changed the output to softmax, since i believe that worked out better for me to use... The Actions are working like they should and the network gives no error, i added a small change that it (should) save the model and load it in later if needed. the Batch i changed just a little bit, i believe i didn't changed it too much in this version, but i atleast changed the \"memory-size\" to 10 instead of 10000 since that would be better for testing and i will defenitly change it up after testing to a more reasonable amount, same for the \"LR-a\" and \"LR-c\" values, just to see if it acctually learns (will come to this later).</p>\n<p>What isn't working: As i said already the code works smoothly without errors, it can do actions and it gets the reward given for those actions, i use the E.G print(action[0]) to see the softmax for the 3 actions in my stepfunction (i choose actions there with the argmax method). works all smooth, but when i look at the values of the return of the softmax i can see they clearly do not change overal (not even a bit) over time, i did several runs with 2500 episodes without any changes. you can clearly see that in the picture below that indicated the reward given for the episode, you can clearly the line being straight and only moving due to a bit luck, but no real learning. the beginning of the line has a bit more volume but thats just because the first values in an average are higher, i strongly believe that you understand why it is like that.</p>\n<p>Picture :: <a href=\"https://cdn.discordapp.com/attachments/455970083617898496/505451154168217610/unknown.png\" rel=\"nofollow\">https://cdn.discordapp.com/attachments/455970083617898496/505451154168217610/unknown.png</a></p>\n<p>So i determined that there is no learning going on, i know i should then try to print out the loss and other important values but i didn't seem to mange to print that since i got errors that are almost never seen in Tensorflow, with sess.run and tf.print, nothing really worked out, but i personally believe that that is not need, since the softmax for all the actions and the chart with the rewards + average indicate already enough that its just not learning.</p>\n<p>So my conclusions i made is that the network is not updating properly, somewhere it is not passing the correct updates or something is a malfunction, i am not sure and i know for sure that it's probaly on my end with the code, i don't doubt your code, but more mine, thats why i would like to see your opinion about this and if you maybe know what it's causing by... seeing it.... i am sorry i can't send it running. i must tell you already that the implemented python multithreading and some people warned of this that that could affect the algorithme and that that the reason is that it's not working, but i am fairly condifent that that is NOT the issue, since the algorithme is the main thread and the environment is the thread that is even located in an other file, the multithreading is only created so that my environment can run whiles the algorithme extracts data and feeds actions to it whiles it's running. Not to forget the reset function returns a flat array of just values, some hot encoded some not.</p>\n<p>i wanna thank you in advance for reading, your time, your reply and your knownledge, if i missed anything, please notify me.</p>\n<p>Jan</p>", "body_text": "\"DDPG Algorithme not updating the softmax\"\nDear reader,\nplease have a look at my ddpg code and what my problem, thanks in advance.\nCode :: https://hastebin.com/beroramufu.py\nWhat is working: The environment, S and S_ are working as they should i've checked several times if they aren't the same and there is a clear diffrence between the two. i changed the action space to discrete action spaces with 3 actions, for the actor network i changed the output to softmax, since i believe that worked out better for me to use... The Actions are working like they should and the network gives no error, i added a small change that it (should) save the model and load it in later if needed. the Batch i changed just a little bit, i believe i didn't changed it too much in this version, but i atleast changed the \"memory-size\" to 10 instead of 10000 since that would be better for testing and i will defenitly change it up after testing to a more reasonable amount, same for the \"LR-a\" and \"LR-c\" values, just to see if it acctually learns (will come to this later).\nWhat isn't working: As i said already the code works smoothly without errors, it can do actions and it gets the reward given for those actions, i use the E.G print(action[0]) to see the softmax for the 3 actions in my stepfunction (i choose actions there with the argmax method). works all smooth, but when i look at the values of the return of the softmax i can see they clearly do not change overal (not even a bit) over time, i did several runs with 2500 episodes without any changes. you can clearly see that in the picture below that indicated the reward given for the episode, you can clearly the line being straight and only moving due to a bit luck, but no real learning. the beginning of the line has a bit more volume but thats just because the first values in an average are higher, i strongly believe that you understand why it is like that.\nPicture :: https://cdn.discordapp.com/attachments/455970083617898496/505451154168217610/unknown.png\nSo i determined that there is no learning going on, i know i should then try to print out the loss and other important values but i didn't seem to mange to print that since i got errors that are almost never seen in Tensorflow, with sess.run and tf.print, nothing really worked out, but i personally believe that that is not need, since the softmax for all the actions and the chart with the rewards + average indicate already enough that its just not learning.\nSo my conclusions i made is that the network is not updating properly, somewhere it is not passing the correct updates or something is a malfunction, i am not sure and i know for sure that it's probaly on my end with the code, i don't doubt your code, but more mine, thats why i would like to see your opinion about this and if you maybe know what it's causing by... seeing it.... i am sorry i can't send it running. i must tell you already that the implemented python multithreading and some people warned of this that that could affect the algorithme and that that the reason is that it's not working, but i am fairly condifent that that is NOT the issue, since the algorithme is the main thread and the environment is the thread that is even located in an other file, the multithreading is only created so that my environment can run whiles the algorithme extracts data and feeds actions to it whiles it's running. Not to forget the reset function returns a flat array of just values, some hot encoded some not.\ni wanna thank you in advance for reading, your time, your reply and your knownledge, if i missed anything, please notify me.\nJan", "body": "\"DDPG Algorithme not updating the softmax\"\r\n\r\n\r\nDear reader,\r\n\r\n\r\n\r\nplease have a look at my ddpg code and what my problem, thanks in advance.\r\n\r\n\r\n\r\nCode :: https://hastebin.com/beroramufu.py\r\n\r\n\r\n\r\nWhat is working: The environment, S and S_ are working as they should i've checked several times if they aren't the same and there is a clear diffrence between the two. i changed the action space to discrete action spaces with 3 actions, for the actor network i changed the output to softmax, since i believe that worked out better for me to use... The Actions are working like they should and the network gives no error, i added a small change that it (should) save the model and load it in later if needed. the Batch i changed just a little bit, i believe i didn't changed it too much in this version, but i atleast changed the \"memory-size\" to 10 instead of 10000 since that would be better for testing and i will defenitly change it up after testing to a more reasonable amount, same for the \"LR-a\" and \"LR-c\" values, just to see if it acctually learns (will come to this later). \r\n\r\n\r\n\r\nWhat isn't working: As i said already the code works smoothly without errors, it can do actions and it gets the reward given for those actions, i use the E.G print(action[0]) to see the softmax for the 3 actions in my stepfunction (i choose actions there with the argmax method). works all smooth, but when i look at the values of the return of the softmax i can see they clearly do not change overal (not even a bit) over time, i did several runs with 2500 episodes without any changes. you can clearly see that in the picture below that indicated the reward given for the episode, you can clearly the line being straight and only moving due to a bit luck, but no real learning. the beginning of the line has a bit more volume but thats just because the first values in an average are higher, i strongly believe that you understand why it is like that.\r\n\r\n\r\n\r\nPicture :: https://cdn.discordapp.com/attachments/455970083617898496/505451154168217610/unknown.png\r\n\r\n\r\n\r\nSo i determined that there is no learning going on, i know i should then try to print out the loss and other important values but i didn't seem to mange to print that since i got errors that are almost never seen in Tensorflow, with sess.run and tf.print, nothing really worked out, but i personally believe that that is not need, since the softmax for all the actions and the chart with the rewards + average indicate already enough that its just not learning.\r\n\r\n\r\n\r\nSo my conclusions i made is that the network is not updating properly, somewhere it is not passing the correct updates or something is a malfunction, i am not sure and i know for sure that it's probaly on my end with the code, i don't doubt your code, but more mine, thats why i would like to see your opinion about this and if you maybe know what it's causing by... seeing it.... i am sorry i can't send it running. i must tell you already that the implemented python multithreading and some people warned of this that that could affect the algorithme and that that the reason is that it's not working, but i am fairly condifent that that is NOT the issue, since the algorithme is the main thread and the environment is the thread that is even located in an other file, the multithreading is only created so that my environment can run whiles the algorithme extracts data and feeds actions to it whiles it's running. Not to forget the reset function returns a flat array of just values, some hot encoded some not. \r\n\r\n\r\n\r\ni wanna thank you in advance for reading, your time, your reply and your knownledge, if i missed anything, please notify me. \r\n\r\n\r\n\r\nJan"}