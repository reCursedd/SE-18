{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328599834", "html_url": "https://github.com/tensorflow/tensorflow/issues/12898#issuecomment-328599834", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12898", "id": 328599834, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODU5OTgzNA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T17:27:05Z", "updated_at": "2017-09-11T17:27:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As a general comment -- it's preferable to to optimize existing allocators to be as fast as \"manual preallocation\" rather than give users a way to preallocate output tensor. This should be feasible in situation like yours where you run the same op thousands of times.</p>\n<p>As an example of what \"preallocating\" does to code, here's an example of l-BFGS in <a href=\"https://github.com/torch/optim/blob/master/lbfgs.lua\">Torch</a>, search for lines with \"table\" -- all that logic just complicates things</p>", "body_text": "As a general comment -- it's preferable to to optimize existing allocators to be as fast as \"manual preallocation\" rather than give users a way to preallocate output tensor. This should be feasible in situation like yours where you run the same op thousands of times.\nAs an example of what \"preallocating\" does to code, here's an example of l-BFGS in Torch, search for lines with \"table\" -- all that logic just complicates things", "body": "As a general comment -- it's preferable to to optimize existing allocators to be as fast as \"manual preallocation\" rather than give users a way to preallocate output tensor. This should be feasible in situation like yours where you run the same op thousands of times.\r\n \r\nAs an example of what \"preallocating\" does to code, here's an example of l-BFGS in [Torch](https://github.com/torch/optim/blob/master/lbfgs.lua), search for lines with \"table\" -- all that logic just complicates things"}