{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/329246578", "html_url": "https://github.com/tensorflow/tensorflow/issues/12898#issuecomment-329246578", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12898", "id": 329246578, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTI0NjU3OA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-13T17:53:38Z", "updated_at": "2017-09-13T17:53:38Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8175586\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jiazhe0909\">@jiazhe0909</a> I am going to close this issue because as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> stated, the delay is currently not a bottleneck. As your diagram indicates, as GPUs get faster and the \"Compute for Op1\" box gets smaller, this could become a bottleneck in the future, but the solution will probably be to improve the allocator. Static allocation uses significantly more memory.</p>\n<p>As you stated, for inference (or training) with small batches sizes, it is more likely the GPU compute time will be small relative to the CPU time, and the  increased memory usage from static allocation would not be a significant problem due to the small batch size. But as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>, this would complicate the code and take a lot of work. If this becomes a serious problem in the future, we can revisit this issue.</p>", "body_text": "@jiazhe0909 I am going to close this issue because as @yaroslavvb stated, the delay is currently not a bottleneck. As your diagram indicates, as GPUs get faster and the \"Compute for Op1\" box gets smaller, this could become a bottleneck in the future, but the solution will probably be to improve the allocator. Static allocation uses significantly more memory.\nAs you stated, for inference (or training) with small batches sizes, it is more likely the GPU compute time will be small relative to the CPU time, and the  increased memory usage from static allocation would not be a significant problem due to the small batch size. But as @yaroslavvb, this would complicate the code and take a lot of work. If this becomes a serious problem in the future, we can revisit this issue.", "body": "@jiazhe0909 I am going to close this issue because as @yaroslavvb stated, the delay is currently not a bottleneck. As your diagram indicates, as GPUs get faster and the \"Compute for Op1\" box gets smaller, this could become a bottleneck in the future, but the solution will probably be to improve the allocator. Static allocation uses significantly more memory.\r\n\r\nAs you stated, for inference (or training) with small batches sizes, it is more likely the GPU compute time will be small relative to the CPU time, and the  increased memory usage from static allocation would not be a significant problem due to the small batch size. But as @yaroslavvb, this would complicate the code and take a lot of work. If this becomes a serious problem in the future, we can revisit this issue.\r\n\r\n"}