{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328481599", "html_url": "https://github.com/tensorflow/tensorflow/issues/12898#issuecomment-328481599", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12898", "id": 328481599, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODQ4MTU5OQ==", "user": {"login": "jiazhe0909", "id": 8175586, "node_id": "MDQ6VXNlcjgxNzU1ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8175586?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiazhe0909", "html_url": "https://github.com/jiazhe0909", "followers_url": "https://api.github.com/users/jiazhe0909/followers", "following_url": "https://api.github.com/users/jiazhe0909/following{/other_user}", "gists_url": "https://api.github.com/users/jiazhe0909/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiazhe0909/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiazhe0909/subscriptions", "organizations_url": "https://api.github.com/users/jiazhe0909/orgs", "repos_url": "https://api.github.com/users/jiazhe0909/repos", "events_url": "https://api.github.com/users/jiazhe0909/events{/privacy}", "received_events_url": "https://api.github.com/users/jiazhe0909/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T09:57:37Z", "updated_at": "2017-09-11T10:19:02Z", "author_association": "NONE", "body_html": "<p>Thanks for your reply, yaroslavvb.</p>\n<p>If I understand correctly, (GPU)BFCAllocator does the caching stuff as you mentioned. And I do notice the same internal output tensors from different inference runs share the same \"true ptr\" addresses.</p>\n<p>However, when I changed code in \"framework/op_kernel.cc\" file, and timed the processes of \"allocate output\", I found these processes still cost 18-20 usec, which seems too long for me because the calculating time of my ops is only 30~50 usec on average.</p>\n<pre><code>2017-09-11 04:19:11.107759: I tensorflow/core/framework/op_kernel.cc:592] My log :: Start allocate output tensor\n2017-09-11 04:19:11.107772: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 10 kernel_name: \"MatMul\" tensor { dtype: DT_FLOAT shape {} allocation_description {allocator_name: \"GPU_0_bfc\" } }\n2017-09-11 04:19:11.107778: I tensorflow/core/framework/op_kernel.cc:608] My log :: complete allocate output tensor\n</code></pre>\n<p>I was wondering if it's possible to walk around the invocation of \"allocate_output\" completely.</p>", "body_text": "Thanks for your reply, yaroslavvb.\nIf I understand correctly, (GPU)BFCAllocator does the caching stuff as you mentioned. And I do notice the same internal output tensors from different inference runs share the same \"true ptr\" addresses.\nHowever, when I changed code in \"framework/op_kernel.cc\" file, and timed the processes of \"allocate output\", I found these processes still cost 18-20 usec, which seems too long for me because the calculating time of my ops is only 30~50 usec on average.\n2017-09-11 04:19:11.107759: I tensorflow/core/framework/op_kernel.cc:592] My log :: Start allocate output tensor\n2017-09-11 04:19:11.107772: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 10 kernel_name: \"MatMul\" tensor { dtype: DT_FLOAT shape {} allocation_description {allocator_name: \"GPU_0_bfc\" } }\n2017-09-11 04:19:11.107778: I tensorflow/core/framework/op_kernel.cc:608] My log :: complete allocate output tensor\n\nI was wondering if it's possible to walk around the invocation of \"allocate_output\" completely.", "body": "Thanks for your reply, yaroslavvb.\r\n\r\nIf I understand correctly, (GPU)BFCAllocator does the caching stuff as you mentioned. And I do notice the same internal output tensors from different inference runs share the same \"true ptr\" addresses. \r\n\r\nHowever, when I changed code in \"framework/op_kernel.cc\" file, and timed the processes of \"allocate output\", I found these processes still cost 18-20 usec, which seems too long for me because the calculating time of my ops is only 30~50 usec on average.\r\n\r\n```\r\n2017-09-11 04:19:11.107759: I tensorflow/core/framework/op_kernel.cc:592] My log :: Start allocate output tensor\r\n2017-09-11 04:19:11.107772: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 10 kernel_name: \"MatMul\" tensor { dtype: DT_FLOAT shape {} allocation_description {allocator_name: \"GPU_0_bfc\" } }\r\n2017-09-11 04:19:11.107778: I tensorflow/core/framework/op_kernel.cc:608] My log :: complete allocate output tensor\r\n```\r\n\r\nI was wondering if it's possible to walk around the invocation of \"allocate_output\" completely. "}