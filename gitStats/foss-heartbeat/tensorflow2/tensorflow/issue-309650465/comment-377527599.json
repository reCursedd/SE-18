{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377527599", "html_url": "https://github.com/tensorflow/tensorflow/issues/18083#issuecomment-377527599", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18083", "id": 377527599, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzUyNzU5OQ==", "user": {"login": "tchaton", "id": 12861981, "node_id": "MDQ6VXNlcjEyODYxOTgx", "avatar_url": "https://avatars0.githubusercontent.com/u/12861981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tchaton", "html_url": "https://github.com/tchaton", "followers_url": "https://api.github.com/users/tchaton/followers", "following_url": "https://api.github.com/users/tchaton/following{/other_user}", "gists_url": "https://api.github.com/users/tchaton/gists{/gist_id}", "starred_url": "https://api.github.com/users/tchaton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tchaton/subscriptions", "organizations_url": "https://api.github.com/users/tchaton/orgs", "repos_url": "https://api.github.com/users/tchaton/repos", "events_url": "https://api.github.com/users/tchaton/events{/privacy}", "received_events_url": "https://api.github.com/users/tchaton/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T13:47:28Z", "updated_at": "2018-03-30T13:49:30Z", "author_association": "NONE", "body_html": "<p>`<a href=\"http://www.netlib.org/lapack/lug/node54.html\" rel=\"nofollow\">http://www.netlib.org/lapack/lug/node54.html</a> GEP with cholesky<br>\nimport tensorflow as tf</p>\n<p>def lda_loss(n_components, margin):<br>\n\"\"\"<br>\nThe main loss function (inner_lda_objective) is wrapped in this function due to<br>\nthe constraints imposed by Keras on objective functions<br>\n\"\"\"<br>\ndef inner_lda_objective(y_true, y_pred):<br>\n\"\"\"<br>\nIt is the loss function of LDA as introduced in the original paper.<br>\nIt is adopted from the the original implementation in the following link:<br>\n<a href=\"https://github.com/CPJKU/deep_lda\">https://github.com/CPJKU/deep_lda</a><br>\nNote: it is implemented by Theano tensor operations, and does not work on Tensorflow backend<br>\n\"\"\"<br>\nr = 1e-4</p>\n<pre><code>    # init groups\n    #yt = tf.cast(tf.contrib.layers.flatten(y_true), tf.float32)\n    #indexes = tf.argmax (y_true, axis=-1)\n    locations = tf.where (tf.equal (y_true, 1))\n    indices = locations[:, 1]\n    y, idx = tf.unique(indices)\n    \n    def fn(unique, indexes, preds):\n        u_indexes = tf.where(tf.equal(unique, indexes))\n        u_indexes = tf.reshape(u_indexes, (1, -1))\n        X = tf.gather(preds, u_indexes)\n        X_mean = X - tf.reduce_mean(X, axis=0)\n        m = tf.cast(tf.shape(X_mean)[1], tf.float32)\n        #X_mean = tf.squeeze(X_mean)\n        return (1/(m-1)) * tf.matmul(tf.transpose(X_mean[0]), X_mean[0])\n\n\n    covs_t = tf.map_fn(lambda x: fn(x, indices, y_pred), y, dtype=tf.float32)\n\n    # compute average covariance matrix (within scatter)\n    Sw_t = tf.reduce_mean(covs_t, axis=0)\n</code></pre>\n<p>``<br>\nXt_bar = y_pred - tf.reduce_mean(y_pred, axis=0)<br>\nm = tf.cast(tf.shape(Xt_bar)[1], tf.float32)<br>\nSt_t = (1/(m-1)) * tf.matmul(tf.transpose(Xt_bar), Xt_bar)</p>\n<pre><code>    # compute between scatter\n    dim = tf.shape(y)[0]\n    Sb_t = St_t - Sw_t\n\n    # cope for numerical instability (regularize)\n    \n    Sw_t += tf.eye(dim) * r    \n    \n    cho = tf.cholesky(St_t + tf.eye(dim) * r ) \n    inv_cho = tf.matrix_inverse(cho)\n    evals_t = tf.linalg.eigvalsh(inv_cho * Sb_t * tf.transpose(inv_cho)) # Sb_t, St_t    \n    #evals_t = tf.abs(tf.linalg.eigvalsh(tf.matrix_inverse(Sw_t) * Sb_t ))\n    top_k_evals = evals_t[-n_components:]\n    \n    #index_max = tf.argmax(top_k_evals, 0)\n    #thresh_max = top_k_evals[index_max] - margin\n    \n    index_min = tf.argmin(top_k_evals, 0)\n    thresh_min = top_k_evals[index_min] + margin\n    #thresh = tf.contrib.distributions.percentile(top_k_evals, 33)\n    #mask_max = top_k_evals &gt; thresh_max\n    mask_min = top_k_evals &lt; thresh_min\n    \n    #cost_max = tf.boolean_mask(top_k_evals, mask_max) \n    cost_min = tf.boolean_mask(top_k_evals, mask_min)\n    \n    return  - tf.reduce_mean(cost_min)\n\nreturn inner_lda_objective\n</code></pre>\n<p>from keras.layers import Dense, Merge<br>\nfrom keras.models import Sequential<br>\nfrom keras.regularizers import l2</p>\n<p>def create_model(input_dim, reg_par, outdim_size):<br>\n\"\"\"<br>\nBuilds the model<br>\nThe structure of the model can get easily substituted with a more efficient and powerful network like CNN<br>\n\"\"\"<br>\nmodel = Sequential()</p>\n<pre><code>model.add(Dense(1024, input_shape=(input_dim,), activation='sigmoid', kernel_regularizer=l2(reg_par)))\nmodel.add(Dense(1024, activation='sigmoid', kernel_regularizer=l2(reg_par)))\nmodel.add(Dense(1024, activation='sigmoid', kernel_regularizer=l2(reg_par)))\nmodel.add(Dense(outdim_size, activation='linear', kernel_regularizer=l2(reg_par)))\n\nreturn model\n</code></pre>\n<p>import pickle<br>\nimport gzip<br>\nimport numpy as np<br>\nfrom keras.datasets import mnist<br>\nfrom keras.optimizers import Adam</p>\n<p>if <strong>name</strong> == '<strong>main</strong>':</p>\n<pre><code>from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\nmnist = read_data_sets(\"path_mnist/\", one_hot=True)\n\n\n\n\noutdim_size = 10\n\n\nepoch_num = 100\nbatch_size = 800\n\nreg_par = 1e-5\n\n\nmargin = 1.0\nn_components = 9\n\n\n\n\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = np.reshape(x_train, (len(x_train), -1))\nx_test = np.reshape(x_test, (len(x_test), -1))\n\nprint(x_train.shape, y_train.shape)\n\nmodel = create_model(x_train.shape[-1], reg_par, outdim_size)\n\nmodel_optimizer = Adam()\nmodel.compile(loss=lda_loss1(n_components, margin), optimizer=model_optimizer)\n\nmodel.summary()\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epoch_num, shuffle=True, validation_data=(x_test, y_test), verbose=2)`\n</code></pre>", "body_text": "`http://www.netlib.org/lapack/lug/node54.html GEP with cholesky\nimport tensorflow as tf\ndef lda_loss(n_components, margin):\n\"\"\"\nThe main loss function (inner_lda_objective) is wrapped in this function due to\nthe constraints imposed by Keras on objective functions\n\"\"\"\ndef inner_lda_objective(y_true, y_pred):\n\"\"\"\nIt is the loss function of LDA as introduced in the original paper.\nIt is adopted from the the original implementation in the following link:\nhttps://github.com/CPJKU/deep_lda\nNote: it is implemented by Theano tensor operations, and does not work on Tensorflow backend\n\"\"\"\nr = 1e-4\n    # init groups\n    #yt = tf.cast(tf.contrib.layers.flatten(y_true), tf.float32)\n    #indexes = tf.argmax (y_true, axis=-1)\n    locations = tf.where (tf.equal (y_true, 1))\n    indices = locations[:, 1]\n    y, idx = tf.unique(indices)\n    \n    def fn(unique, indexes, preds):\n        u_indexes = tf.where(tf.equal(unique, indexes))\n        u_indexes = tf.reshape(u_indexes, (1, -1))\n        X = tf.gather(preds, u_indexes)\n        X_mean = X - tf.reduce_mean(X, axis=0)\n        m = tf.cast(tf.shape(X_mean)[1], tf.float32)\n        #X_mean = tf.squeeze(X_mean)\n        return (1/(m-1)) * tf.matmul(tf.transpose(X_mean[0]), X_mean[0])\n\n\n    covs_t = tf.map_fn(lambda x: fn(x, indices, y_pred), y, dtype=tf.float32)\n\n    # compute average covariance matrix (within scatter)\n    Sw_t = tf.reduce_mean(covs_t, axis=0)\n\n``\nXt_bar = y_pred - tf.reduce_mean(y_pred, axis=0)\nm = tf.cast(tf.shape(Xt_bar)[1], tf.float32)\nSt_t = (1/(m-1)) * tf.matmul(tf.transpose(Xt_bar), Xt_bar)\n    # compute between scatter\n    dim = tf.shape(y)[0]\n    Sb_t = St_t - Sw_t\n\n    # cope for numerical instability (regularize)\n    \n    Sw_t += tf.eye(dim) * r    \n    \n    cho = tf.cholesky(St_t + tf.eye(dim) * r ) \n    inv_cho = tf.matrix_inverse(cho)\n    evals_t = tf.linalg.eigvalsh(inv_cho * Sb_t * tf.transpose(inv_cho)) # Sb_t, St_t    \n    #evals_t = tf.abs(tf.linalg.eigvalsh(tf.matrix_inverse(Sw_t) * Sb_t ))\n    top_k_evals = evals_t[-n_components:]\n    \n    #index_max = tf.argmax(top_k_evals, 0)\n    #thresh_max = top_k_evals[index_max] - margin\n    \n    index_min = tf.argmin(top_k_evals, 0)\n    thresh_min = top_k_evals[index_min] + margin\n    #thresh = tf.contrib.distributions.percentile(top_k_evals, 33)\n    #mask_max = top_k_evals > thresh_max\n    mask_min = top_k_evals < thresh_min\n    \n    #cost_max = tf.boolean_mask(top_k_evals, mask_max) \n    cost_min = tf.boolean_mask(top_k_evals, mask_min)\n    \n    return  - tf.reduce_mean(cost_min)\n\nreturn inner_lda_objective\n\nfrom keras.layers import Dense, Merge\nfrom keras.models import Sequential\nfrom keras.regularizers import l2\ndef create_model(input_dim, reg_par, outdim_size):\n\"\"\"\nBuilds the model\nThe structure of the model can get easily substituted with a more efficient and powerful network like CNN\n\"\"\"\nmodel = Sequential()\nmodel.add(Dense(1024, input_shape=(input_dim,), activation='sigmoid', kernel_regularizer=l2(reg_par)))\nmodel.add(Dense(1024, activation='sigmoid', kernel_regularizer=l2(reg_par)))\nmodel.add(Dense(1024, activation='sigmoid', kernel_regularizer=l2(reg_par)))\nmodel.add(Dense(outdim_size, activation='linear', kernel_regularizer=l2(reg_par)))\n\nreturn model\n\nimport pickle\nimport gzip\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.optimizers import Adam\nif name == 'main':\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\nmnist = read_data_sets(\"path_mnist/\", one_hot=True)\n\n\n\n\noutdim_size = 10\n\n\nepoch_num = 100\nbatch_size = 800\n\nreg_par = 1e-5\n\n\nmargin = 1.0\nn_components = 9\n\n\n\n\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = np.reshape(x_train, (len(x_train), -1))\nx_test = np.reshape(x_test, (len(x_test), -1))\n\nprint(x_train.shape, y_train.shape)\n\nmodel = create_model(x_train.shape[-1], reg_par, outdim_size)\n\nmodel_optimizer = Adam()\nmodel.compile(loss=lda_loss1(n_components, margin), optimizer=model_optimizer)\n\nmodel.summary()\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epoch_num, shuffle=True, validation_data=(x_test, y_test), verbose=2)`", "body": "`http://www.netlib.org/lapack/lug/node54.html GEP with cholesky\r\nimport tensorflow as tf\r\n\r\n\r\ndef lda_loss(n_components, margin):\r\n    \"\"\"\r\n    The main loss function (inner_lda_objective) is wrapped in this function due to\r\n    the constraints imposed by Keras on objective functions\r\n    \"\"\"\r\n    def inner_lda_objective(y_true, y_pred):\r\n        \"\"\"\r\n        It is the loss function of LDA as introduced in the original paper. \r\n        It is adopted from the the original implementation in the following link:\r\n        https://github.com/CPJKU/deep_lda\r\n        Note: it is implemented by Theano tensor operations, and does not work on Tensorflow backend\r\n        \"\"\"\r\n        r = 1e-4\r\n\r\n        # init groups\r\n        #yt = tf.cast(tf.contrib.layers.flatten(y_true), tf.float32)\r\n        #indexes = tf.argmax (y_true, axis=-1)\r\n        locations = tf.where (tf.equal (y_true, 1))\r\n        indices = locations[:, 1]\r\n        y, idx = tf.unique(indices)\r\n        \r\n        def fn(unique, indexes, preds):\r\n            u_indexes = tf.where(tf.equal(unique, indexes))\r\n            u_indexes = tf.reshape(u_indexes, (1, -1))\r\n            X = tf.gather(preds, u_indexes)\r\n            X_mean = X - tf.reduce_mean(X, axis=0)\r\n            m = tf.cast(tf.shape(X_mean)[1], tf.float32)\r\n            #X_mean = tf.squeeze(X_mean)\r\n            return (1/(m-1)) * tf.matmul(tf.transpose(X_mean[0]), X_mean[0])\r\n\r\n\r\n        covs_t = tf.map_fn(lambda x: fn(x, indices, y_pred), y, dtype=tf.float32)\r\n\r\n        # compute average covariance matrix (within scatter)\r\n        Sw_t = tf.reduce_mean(covs_t, axis=0)\r\n        \r\n``\r\n        Xt_bar = y_pred - tf.reduce_mean(y_pred, axis=0)\r\n        m = tf.cast(tf.shape(Xt_bar)[1], tf.float32)\r\n        St_t = (1/(m-1)) * tf.matmul(tf.transpose(Xt_bar), Xt_bar)\r\n\r\n        # compute between scatter\r\n        dim = tf.shape(y)[0]\r\n        Sb_t = St_t - Sw_t\r\n\r\n        # cope for numerical instability (regularize)\r\n        \r\n        Sw_t += tf.eye(dim) * r    \r\n        \r\n        cho = tf.cholesky(St_t + tf.eye(dim) * r ) \r\n        inv_cho = tf.matrix_inverse(cho)\r\n        evals_t = tf.linalg.eigvalsh(inv_cho * Sb_t * tf.transpose(inv_cho)) # Sb_t, St_t    \r\n        #evals_t = tf.abs(tf.linalg.eigvalsh(tf.matrix_inverse(Sw_t) * Sb_t ))\r\n        top_k_evals = evals_t[-n_components:]\r\n        \r\n        #index_max = tf.argmax(top_k_evals, 0)\r\n        #thresh_max = top_k_evals[index_max] - margin\r\n        \r\n        index_min = tf.argmin(top_k_evals, 0)\r\n        thresh_min = top_k_evals[index_min] + margin\r\n        #thresh = tf.contrib.distributions.percentile(top_k_evals, 33)\r\n        #mask_max = top_k_evals > thresh_max\r\n        mask_min = top_k_evals < thresh_min\r\n        \r\n        #cost_max = tf.boolean_mask(top_k_evals, mask_max) \r\n        cost_min = tf.boolean_mask(top_k_evals, mask_min)\r\n        \r\n        return  - tf.reduce_mean(cost_min)\r\n    \r\n    return inner_lda_objective\r\nfrom keras.layers import Dense, Merge\r\nfrom keras.models import Sequential\r\nfrom keras.regularizers import l2\r\n\r\n\r\ndef create_model(input_dim, reg_par, outdim_size):\r\n    \"\"\"\r\n    Builds the model\r\n    The structure of the model can get easily substituted with a more efficient and powerful network like CNN\r\n    \"\"\"\r\n    model = Sequential()\r\n\r\n    model.add(Dense(1024, input_shape=(input_dim,), activation='sigmoid', kernel_regularizer=l2(reg_par)))\r\n    model.add(Dense(1024, activation='sigmoid', kernel_regularizer=l2(reg_par)))\r\n    model.add(Dense(1024, activation='sigmoid', kernel_regularizer=l2(reg_par)))\r\n    model.add(Dense(outdim_size, activation='linear', kernel_regularizer=l2(reg_par)))\r\n\r\n    return model\r\nimport pickle\r\nimport gzip\r\nimport numpy as np\r\nfrom keras.datasets import mnist\r\nfrom keras.optimizers import Adam\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n    mnist = read_data_sets(\"path_mnist/\", one_hot=True)\r\n\r\n\r\n\r\n\r\n    outdim_size = 10\r\n\r\n\r\n    epoch_num = 100\r\n    batch_size = 800\r\n\r\n    reg_par = 1e-5\r\n\r\n\r\n    margin = 1.0\r\n    n_components = 9\r\n\r\n\r\n\r\n\r\n\r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\n    x_train = np.reshape(x_train, (len(x_train), -1))\r\n    x_test = np.reshape(x_test, (len(x_test), -1))\r\n\r\n    print(x_train.shape, y_train.shape)\r\n\r\n    model = create_model(x_train.shape[-1], reg_par, outdim_size)\r\n\r\n    model_optimizer = Adam()\r\n    model.compile(loss=lda_loss1(n_components, margin), optimizer=model_optimizer)\r\n\r\n    model.summary()\r\n\r\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epoch_num, shuffle=True, validation_data=(x_test, y_test), verbose=2)`"}