{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5263", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5263/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5263/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5263/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5263", "id": 185943745, "node_id": "MDU6SXNzdWUxODU5NDM3NDU=", "number": 5263, "title": "OOM Error Message Should Show Which GPU is Out of Memory", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-10-28T14:53:08Z", "updated_at": "2017-06-16T17:46:42Z", "closed_at": "2017-06-16T17:46:42Z", "author_association": "NONE", "body_html": "<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I've created a few overflow threads in regards to balancing seq2seq memory loads over multiple gpus here:</p>\n<p><a href=\"http://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow\" rel=\"nofollow\">http://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow</a></p>\n<h3>Environment info</h3>\n<p>Operating System:</p>\n<p>Ubuntu 14.04 CUDA 7.5 -- tensorflow 0.11</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>Simply load two matrices. One incredibly larger one on a gpu0 and a smaller one on gpu1. It would be great if the OOM message could tell you which gpu ran out of memory. This way you can redistribute resources from one gpu to another.</p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>Currently I look at all tensors assigned per device and how large each tensor it is. I then sum the size of these tensors using the following code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">calculate_variable_sizes_per_device</span>(<span class=\"pl-smi\">batch_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>):\n  dev_list <span class=\"pl-k\">=</span> get_available_gpus()\n  dev_size_list <span class=\"pl-k\">=</span> [[dev_list[i], <span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">len</span>(dev_list))]\n\n  tf.logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>calculating variable sizes on respective devices: <span class=\"pl-c1\">%d</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">len</span>(dev_list))\n  <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dev_list<span class=\"pl-pds\">'</span></span>, dev_list)\n  <span class=\"pl-k\">for</span> eachvar <span class=\"pl-k\">in</span> tf.all_variables():\n    device <span class=\"pl-k\">=</span> eachvar.device\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>device<span class=\"pl-pds\">'</span></span>, device)\n    var_shape <span class=\"pl-k\">=</span> find_replace_list(eachvar.get_shape().as_list(), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>?<span class=\"pl-pds\">'</span></span>, batch_size)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>varshape<span class=\"pl-pds\">'</span></span>,var_shape)\n    var_size <span class=\"pl-k\">=</span> np.prod(np.array(var_shape))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>varsize<span class=\"pl-pds\">'</span></span>, var_size)  \n\n    <span class=\"pl-k\">for</span> i,dev <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(dev_list):\n      <span class=\"pl-k\">if</span> dev.replace(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">in</span> device.lower():\n        dev_size_list[i][<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">+=</span> var_size\n\n  <span class=\"pl-k\">return</span> dev_size_list\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">find_replace_list</span>(<span class=\"pl-smi\">list_</span>, <span class=\"pl-smi\">find</span>, <span class=\"pl-smi\">replace</span>):\n  <span class=\"pl-k\">for</span> n,i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(list_):\n    <span class=\"pl-k\">if</span> i<span class=\"pl-k\">==</span>find:\n      list_[n]<span class=\"pl-k\">=</span>replace\n  <span class=\"pl-k\">return</span> list_\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_available_gpus</span>():\n  local_device_protos <span class=\"pl-k\">=</span> device_lib.list_local_devices()\n  <span class=\"pl-k\">return</span> [x.name <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> local_device_protos] <span class=\"pl-c\"><span class=\"pl-c\">#</span>if x.device_type == 'GPU']</span></pre></div>", "body_text": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI've created a few overflow threads in regards to balancing seq2seq memory loads over multiple gpus here:\nhttp://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow\nEnvironment info\nOperating System:\nUbuntu 14.04 CUDA 7.5 -- tensorflow 0.11\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nSimply load two matrices. One incredibly larger one on a gpu0 and a smaller one on gpu1. It would be great if the OOM message could tell you which gpu ran out of memory. This way you can redistribute resources from one gpu to another.\nWhat other attempted solutions have you tried?\nCurrently I look at all tensors assigned per device and how large each tensor it is. I then sum the size of these tensors using the following code:\ndef calculate_variable_sizes_per_device(batch_size = 64):\n  dev_list = get_available_gpus()\n  dev_size_list = [[dev_list[i], 0] for i in xrange(len(dev_list))]\n\n  tf.logging.info('calculating variable sizes on respective devices: %d' % len(dev_list))\n  print('dev_list', dev_list)\n  for eachvar in tf.all_variables():\n    device = eachvar.device\n    print('device', device)\n    var_shape = find_replace_list(eachvar.get_shape().as_list(), '?', batch_size)\n    print('varshape',var_shape)\n    var_size = np.prod(np.array(var_shape))\n    print('varsize', var_size)  \n\n    for i,dev in enumerate(dev_list):\n      if dev.replace('/','') in device.lower():\n        dev_size_list[i][1] += var_size\n\n  return dev_size_list\n\ndef find_replace_list(list_, find, replace):\n  for n,i in enumerate(list_):\n    if i==find:\n      list_[n]=replace\n  return list_\n\n\ndef get_available_gpus():\n  local_device_protos = device_lib.list_local_devices()\n  return [x.name for x in local_device_protos] #if x.device_type == 'GPU']", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI've created a few overflow threads in regards to balancing seq2seq memory loads over multiple gpus here:\n\nhttp://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow\n### Environment info\n\nOperating System:\n\nUbuntu 14.04 CUDA 7.5 -- tensorflow 0.11\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nSimply load two matrices. One incredibly larger one on a gpu0 and a smaller one on gpu1. It would be great if the OOM message could tell you which gpu ran out of memory. This way you can redistribute resources from one gpu to another.\n### What other attempted solutions have you tried?\n\nCurrently I look at all tensors assigned per device and how large each tensor it is. I then sum the size of these tensors using the following code:\n\n``` python\ndef calculate_variable_sizes_per_device(batch_size = 64):\n  dev_list = get_available_gpus()\n  dev_size_list = [[dev_list[i], 0] for i in xrange(len(dev_list))]\n\n  tf.logging.info('calculating variable sizes on respective devices: %d' % len(dev_list))\n  print('dev_list', dev_list)\n  for eachvar in tf.all_variables():\n    device = eachvar.device\n    print('device', device)\n    var_shape = find_replace_list(eachvar.get_shape().as_list(), '?', batch_size)\n    print('varshape',var_shape)\n    var_size = np.prod(np.array(var_shape))\n    print('varsize', var_size)  \n\n    for i,dev in enumerate(dev_list):\n      if dev.replace('/','') in device.lower():\n        dev_size_list[i][1] += var_size\n\n  return dev_size_list\n\ndef find_replace_list(list_, find, replace):\n  for n,i in enumerate(list_):\n    if i==find:\n      list_[n]=replace\n  return list_\n\n\ndef get_available_gpus():\n  local_device_protos = device_lib.list_local_devices()\n  return [x.name for x in local_device_protos] #if x.device_type == 'GPU']\n```\n"}