{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/230733172", "html_url": "https://github.com/tensorflow/tensorflow/issues/3199#issuecomment-230733172", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3199", "id": 230733172, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMDczMzE3Mg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-06T10:14:44Z", "updated_at": "2016-07-06T10:14:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>NaN's are a common situation with neural network training. If the dataset<br>\nis easy enough, then SGD will drive your log loss to negative infinity,<br>\nwhich will overflow at some point and cause NaNs.</p>\n<p>An indication of a problem is when you run the same network on two<br>\ndifferent versions of tensorflow, and it only diverges on one, but not the<br>\nother.</p>\n<p>On Wed, Jul 6, 2016 at 2:09 PM, Yu Li <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>I do not know that's a hardware problem or software problem. However, at<br>\nthe end, I indeed got some warning of running out of memory.<br>\n[image: screen shot 2016-07-06 at 12 53 51]<br>\n<a href=\"https://cloud.githubusercontent.com/assets/19503832/16614604/d82f2e38-437a-11e6-9f8e-afea48f5e5b9.png\" rel=\"nofollow\">https://cloud.githubusercontent.com/assets/19503832/16614604/d82f2e38-437a-11e6-9f8e-afea48f5e5b9.png</a></p>\n<p>\u2014<br>\nYou are receiving this because you are subscribed to this thread.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"163969549\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3199\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3199/hovercard?comment_id=230732164&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/3199#issuecomment-230732164\">#3199 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe/AABaHMjH4Vi-1iekm1TLQzOx6eg7XbTMks5qS37qgaJpZM4JFoR6\">https://github.com/notifications/unsubscribe/AABaHMjH4Vi-1iekm1TLQzOx6eg7XbTMks5qS37qgaJpZM4JFoR6</a><br>\n.</p>\n</blockquote>", "body_text": "NaN's are a common situation with neural network training. If the dataset\nis easy enough, then SGD will drive your log loss to negative infinity,\nwhich will overflow at some point and cause NaNs.\nAn indication of a problem is when you run the same network on two\ndifferent versions of tensorflow, and it only diverges on one, but not the\nother.\nOn Wed, Jul 6, 2016 at 2:09 PM, Yu Li notifications@github.com wrote:\n\nI do not know that's a hardware problem or software problem. However, at\nthe end, I indeed got some warning of running out of memory.\n[image: screen shot 2016-07-06 at 12 53 51]\nhttps://cloud.githubusercontent.com/assets/19503832/16614604/d82f2e38-437a-11e6-9f8e-afea48f5e5b9.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n#3199 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AABaHMjH4Vi-1iekm1TLQzOx6eg7XbTMks5qS37qgaJpZM4JFoR6\n.", "body": "NaN's are a common situation with neural network training. If the dataset\nis easy enough, then SGD will drive your log loss to negative infinity,\nwhich will overflow at some point and cause NaNs.\n\nAn indication of a problem is when you run the same network on two\ndifferent versions of tensorflow, and it only diverges on one, but not the\nother.\n\nOn Wed, Jul 6, 2016 at 2:09 PM, Yu Li notifications@github.com wrote:\n\n> I do not know that's a hardware problem or software problem. However, at\n> the end, I indeed got some warning of running out of memory.\n> [image: screen shot 2016-07-06 at 12 53 51]\n> https://cloud.githubusercontent.com/assets/19503832/16614604/d82f2e38-437a-11e6-9f8e-afea48f5e5b9.png\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3199#issuecomment-230732164,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHMjH4Vi-1iekm1TLQzOx6eg7XbTMks5qS37qgaJpZM4JFoR6\n> .\n"}