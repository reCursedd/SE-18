{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3199", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3199/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3199/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3199/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3199", "id": 163969549, "node_id": "MDU6SXNzdWUxNjM5Njk1NDk=", "number": 3199, "title": "All weights become nan after training more than about 20000+10000 times", "user": {"login": "lykaust15", "id": 19503832, "node_id": "MDQ6VXNlcjE5NTAzODMy", "avatar_url": "https://avatars0.githubusercontent.com/u/19503832?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lykaust15", "html_url": "https://github.com/lykaust15", "followers_url": "https://api.github.com/users/lykaust15/followers", "following_url": "https://api.github.com/users/lykaust15/following{/other_user}", "gists_url": "https://api.github.com/users/lykaust15/gists{/gist_id}", "starred_url": "https://api.github.com/users/lykaust15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lykaust15/subscriptions", "organizations_url": "https://api.github.com/users/lykaust15/orgs", "repos_url": "https://api.github.com/users/lykaust15/repos", "events_url": "https://api.github.com/users/lykaust15/events{/privacy}", "received_events_url": "https://api.github.com/users/lykaust15/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-07-05T23:37:24Z", "updated_at": "2016-07-06T11:07:16Z", "closed_at": "2016-07-06T11:07:16Z", "author_association": "NONE", "body_html": "<p>Operating System: CentOS 7<br>\nInstalled version of CUDA and cuDNN: cuda 7.5.18, cudnn4</p>\n<p>When I run my own project, I found that all weights would become nan after training more than 20000+10000 times (That means I first trained the model for 20000 times, and then I continued training the model for 10000 times). The accuracy would suddenly change from 0.8 to 0.5. I thought that might be my own problem of setting weights and they become divergent after a certain point.</p>\n<p>But we I rerun the Tensorflow tutorial: \"Deep MNIST for Experts\", I got the same problem.<br>\nFollow the instruction, I run the following script:</p>\n<p>for i in range(20000):<br>\nbatch = mnist.train.next_batch(50)<br>\nif i%100 == 0:<br>\ntrain_accuracy = accuracy.eval(feed_dict={<br>\nx:batch[0], y_: batch[1], keep_prob: 1.0})<br>\nprint(\"step %d, training accuracy %g\"%(i, train_accuracy))<br>\ntrain_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})</p>\n<p>Everything is perfect until now, and the training accuracy is as high as 0.99.</p>\n<p>However, when I rerun the above script, something strange happened. The training accuracy suddenly become around 0.1 and all weights become nan. Like following:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/19503832/16603033/79ca3a3a-4321-11e6-92b7-19750de858c9.png\"><img width=\"372\" alt=\"screen shot 2016-07-06 at 02 19 17\" src=\"https://cloud.githubusercontent.com/assets/19503832/16603033/79ca3a3a-4321-11e6-92b7-19750de858c9.png\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/19503832/16603036/8074b87e-4321-11e6-9736-6ff2a98f5a4c.png\"><img width=\"775\" alt=\"screen shot 2016-07-06 at 02 20 15\" src=\"https://cloud.githubusercontent.com/assets/19503832/16603036/8074b87e-4321-11e6-9736-6ff2a98f5a4c.png\" style=\"max-width:100%;\"></a></p>\n<p>To reproduce the problem, first train the model for 20000 times, and then continue training the module for 20000 times, using another for loop.</p>", "body_text": "Operating System: CentOS 7\nInstalled version of CUDA and cuDNN: cuda 7.5.18, cudnn4\nWhen I run my own project, I found that all weights would become nan after training more than 20000+10000 times (That means I first trained the model for 20000 times, and then I continued training the model for 10000 times). The accuracy would suddenly change from 0.8 to 0.5. I thought that might be my own problem of setting weights and they become divergent after a certain point.\nBut we I rerun the Tensorflow tutorial: \"Deep MNIST for Experts\", I got the same problem.\nFollow the instruction, I run the following script:\nfor i in range(20000):\nbatch = mnist.train.next_batch(50)\nif i%100 == 0:\ntrain_accuracy = accuracy.eval(feed_dict={\nx:batch[0], y_: batch[1], keep_prob: 1.0})\nprint(\"step %d, training accuracy %g\"%(i, train_accuracy))\ntrain_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\nEverything is perfect until now, and the training accuracy is as high as 0.99.\nHowever, when I rerun the above script, something strange happened. The training accuracy suddenly become around 0.1 and all weights become nan. Like following:\n\n\nTo reproduce the problem, first train the model for 20000 times, and then continue training the module for 20000 times, using another for loop.", "body": "Operating System: CentOS 7\nInstalled version of CUDA and cuDNN: cuda 7.5.18, cudnn4\n\nWhen I run my own project, I found that all weights would become nan after training more than 20000+10000 times (That means I first trained the model for 20000 times, and then I continued training the model for 10000 times). The accuracy would suddenly change from 0.8 to 0.5. I thought that might be my own problem of setting weights and they become divergent after a certain point.\n\nBut we I rerun the Tensorflow tutorial: \"Deep MNIST for Experts\", I got the same problem.\nFollow the instruction, I run the following script:\n\nfor i in range(20000):\n  batch = mnist.train.next_batch(50)\n  if i%100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\nEverything is perfect until now, and the training accuracy is as high as 0.99.\n\nHowever, when I rerun the above script, something strange happened. The training accuracy suddenly become around 0.1 and all weights become nan. Like following: \n<img width=\"372\" alt=\"screen shot 2016-07-06 at 02 19 17\" src=\"https://cloud.githubusercontent.com/assets/19503832/16603033/79ca3a3a-4321-11e6-92b7-19750de858c9.png\">\n<img width=\"775\" alt=\"screen shot 2016-07-06 at 02 20 15\" src=\"https://cloud.githubusercontent.com/assets/19503832/16603036/8074b87e-4321-11e6-9736-6ff2a98f5a4c.png\">\n\nTo reproduce the problem, first train the model for 20000 times, and then continue training the module for 20000 times, using another for loop.\n"}