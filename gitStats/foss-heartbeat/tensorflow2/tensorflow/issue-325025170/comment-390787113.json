{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/390787113", "html_url": "https://github.com/tensorflow/tensorflow/issues/19442#issuecomment-390787113", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19442", "id": 390787113, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDc4NzExMw==", "user": {"login": "sleighsoft", "id": 9438971, "node_id": "MDQ6VXNlcjk0Mzg5NzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/9438971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sleighsoft", "html_url": "https://github.com/sleighsoft", "followers_url": "https://api.github.com/users/sleighsoft/followers", "following_url": "https://api.github.com/users/sleighsoft/following{/other_user}", "gists_url": "https://api.github.com/users/sleighsoft/gists{/gist_id}", "starred_url": "https://api.github.com/users/sleighsoft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sleighsoft/subscriptions", "organizations_url": "https://api.github.com/users/sleighsoft/orgs", "repos_url": "https://api.github.com/users/sleighsoft/repos", "events_url": "https://api.github.com/users/sleighsoft/events{/privacy}", "received_events_url": "https://api.github.com/users/sleighsoft/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-21T21:23:06Z", "updated_at": "2018-05-21T21:23:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think I did something stupid. I also believe the error should not have happened the way it did.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\n<span class=\"pl-k\">from</span> tensorflow.python.training <span class=\"pl-k\">import</span> checkpointable\n<span class=\"pl-k\">from</span> utils <span class=\"pl-k\">import</span> cell_util\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">lstm_initial_state</span>(<span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">cell_size</span>, <span class=\"pl-smi\">layers</span>, <span class=\"pl-smi\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Create initial state for LSTMCell</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    batch_size: Size of batch</span>\n<span class=\"pl-s\">    cell_size: Size of LSTMCell</span>\n<span class=\"pl-s\">    layers: Number of LSTMCell layers, when using MultiRNNCell</span>\n<span class=\"pl-s\">    initializer: State initializer function that takes batch_size and cell_size</span>\n<span class=\"pl-s\">      as input and returns state.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Returns: Initial state for the tf.contrib.seq2seq.BasicDecoder inital_state</span>\n<span class=\"pl-s\">    input parameter.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">if</span> initializer <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">initializer</span>(<span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">cell_size</span>):\n      <span class=\"pl-k\">return</span> tf.zeros([batch_size, cell_size])\n\n  state <span class=\"pl-k\">=</span> ()\n  <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(layers):\n    c_state <span class=\"pl-k\">=</span> initializer(batch_size, cell_size)\n    h_state <span class=\"pl-k\">=</span> initializer(batch_size, cell_size)\n    initial_state <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMStateTuple(c_state, h_state)\n    state <span class=\"pl-k\">+=</span> (initial_state,)\n  <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(state) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n    state <span class=\"pl-k\">=</span> state[<span class=\"pl-c1\">0</span>]\n  <span class=\"pl-k\">return</span> state\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyModel</span>(<span class=\"pl-e\">checkpointable</span>.<span class=\"pl-e\">Checkpointable</span>):\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">self</span>.optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.1</span>)\n    <span class=\"pl-c1\">self</span>.cell <span class=\"pl-k\">=</span> cell_util.rnn_cell(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>lstm<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">0.2</span>, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-c1\">self</span>.var <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>var<span class=\"pl-pds\">'</span></span>, [])\n    <span class=\"pl-c1\">self</span>.optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer()\n\n    <span class=\"pl-c1\">self</span>.input <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">10</span>])\n    state <span class=\"pl-k\">=</span> lstm_initial_state(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">4</span>)\n    <span class=\"pl-c1\">self</span>.calc <span class=\"pl-k\">=</span> tf.reduce_sum(<span class=\"pl-c1\">self</span>.cell(<span class=\"pl-c1\">self</span>.input, state)[<span class=\"pl-c1\">0</span>])\n\n    <span class=\"pl-c1\">self</span>.train_op <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.optimizer.minimize(<span class=\"pl-c1\">self</span>.calc)\n\n    <span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.optimizer.variables():\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Adding <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(var.name))\n      <span class=\"pl-c1\">setattr</span>(<span class=\"pl-c1\">self</span>, var.name, var)\n\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n  model <span class=\"pl-k\">=</span> MyModel()\n  session <span class=\"pl-k\">=</span> tf.Session()\n  checkpoint <span class=\"pl-k\">=</span> tfe.Checkpoint(<span class=\"pl-v\">model</span><span class=\"pl-k\">=</span>model)\n  checkpoint.restore(<span class=\"pl-c1\">None</span>).initialize_or_restore(session)\n  <span class=\"pl-c1\">print</span>()</pre></div>\n<p><code>cell_util.rnn_cell</code> creates a 4 layer <code>MultiRnnCell</code> with dropout.</p>", "body_text": "I think I did something stupid. I also believe the error should not have happened the way it did.\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nfrom tensorflow.python.training import checkpointable\nfrom utils import cell_util\n\n\ndef lstm_initial_state(batch_size, cell_size, layers, initializer=None):\n  \"\"\"Create initial state for LSTMCell\n\n  Args:\n    batch_size: Size of batch\n    cell_size: Size of LSTMCell\n    layers: Number of LSTMCell layers, when using MultiRNNCell\n    initializer: State initializer function that takes batch_size and cell_size\n      as input and returns state.\n\n  Returns: Initial state for the tf.contrib.seq2seq.BasicDecoder inital_state\n    input parameter.\"\"\"\n  if initializer is None:\n    def initializer(batch_size, cell_size):\n      return tf.zeros([batch_size, cell_size])\n\n  state = ()\n  for _ in range(layers):\n    c_state = initializer(batch_size, cell_size)\n    h_state = initializer(batch_size, cell_size)\n    initial_state = tf.contrib.rnn.LSTMStateTuple(c_state, h_state)\n    state += (initial_state,)\n  if len(state) == 1:\n    state = state[0]\n  return state\n\n\nclass MyModel(checkpointable.Checkpointable):\n\n  def __init__(self):\n    self.optimizer = tf.train.GradientDescentOptimizer(0.1)\n    self.cell = cell_util.rnn_cell('lstm', 128, 4, 0.2, training=True)\n    self.var = tf.get_variable('var', [])\n    self.optimizer = tf.train.AdamOptimizer()\n\n    self.input = tf.placeholder(tf.float32, [5, 10])\n    state = lstm_initial_state(5, 128, 4)\n    self.calc = tf.reduce_sum(self.cell(self.input, state)[0])\n\n    self.train_op = self.optimizer.minimize(self.calc)\n\n    for var in self.optimizer.variables():\n      print('Adding {}'.format(var.name))\n      setattr(self, var.name, var)\n\n\nwith tf.Graph().as_default():\n  model = MyModel()\n  session = tf.Session()\n  checkpoint = tfe.Checkpoint(model=model)\n  checkpoint.restore(None).initialize_or_restore(session)\n  print()\ncell_util.rnn_cell creates a 4 layer MultiRnnCell with dropout.", "body": "I think I did something stupid. I also believe the error should not have happened the way it did.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nfrom tensorflow.python.training import checkpointable\r\nfrom utils import cell_util\r\n\r\n\r\ndef lstm_initial_state(batch_size, cell_size, layers, initializer=None):\r\n  \"\"\"Create initial state for LSTMCell\r\n\r\n  Args:\r\n    batch_size: Size of batch\r\n    cell_size: Size of LSTMCell\r\n    layers: Number of LSTMCell layers, when using MultiRNNCell\r\n    initializer: State initializer function that takes batch_size and cell_size\r\n      as input and returns state.\r\n\r\n  Returns: Initial state for the tf.contrib.seq2seq.BasicDecoder inital_state\r\n    input parameter.\"\"\"\r\n  if initializer is None:\r\n    def initializer(batch_size, cell_size):\r\n      return tf.zeros([batch_size, cell_size])\r\n\r\n  state = ()\r\n  for _ in range(layers):\r\n    c_state = initializer(batch_size, cell_size)\r\n    h_state = initializer(batch_size, cell_size)\r\n    initial_state = tf.contrib.rnn.LSTMStateTuple(c_state, h_state)\r\n    state += (initial_state,)\r\n  if len(state) == 1:\r\n    state = state[0]\r\n  return state\r\n\r\n\r\nclass MyModel(checkpointable.Checkpointable):\r\n\r\n  def __init__(self):\r\n    self.optimizer = tf.train.GradientDescentOptimizer(0.1)\r\n    self.cell = cell_util.rnn_cell('lstm', 128, 4, 0.2, training=True)\r\n    self.var = tf.get_variable('var', [])\r\n    self.optimizer = tf.train.AdamOptimizer()\r\n\r\n    self.input = tf.placeholder(tf.float32, [5, 10])\r\n    state = lstm_initial_state(5, 128, 4)\r\n    self.calc = tf.reduce_sum(self.cell(self.input, state)[0])\r\n\r\n    self.train_op = self.optimizer.minimize(self.calc)\r\n\r\n    for var in self.optimizer.variables():\r\n      print('Adding {}'.format(var.name))\r\n      setattr(self, var.name, var)\r\n\r\n\r\nwith tf.Graph().as_default():\r\n  model = MyModel()\r\n  session = tf.Session()\r\n  checkpoint = tfe.Checkpoint(model=model)\r\n  checkpoint.restore(None).initialize_or_restore(session)\r\n  print()\r\n```\r\n\r\n`cell_util.rnn_cell` creates a 4 layer `MultiRnnCell` with dropout."}