{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/194117171", "html_url": "https://github.com/tensorflow/tensorflow/issues/410#issuecomment-194117171", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/410", "id": 194117171, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NDExNzE3MQ==", "user": {"login": "panmari", "id": 719020, "node_id": "MDQ6VXNlcjcxOTAyMA==", "avatar_url": "https://avatars1.githubusercontent.com/u/719020?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panmari", "html_url": "https://github.com/panmari", "followers_url": "https://api.github.com/users/panmari/followers", "following_url": "https://api.github.com/users/panmari/following{/other_user}", "gists_url": "https://api.github.com/users/panmari/gists{/gist_id}", "starred_url": "https://api.github.com/users/panmari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panmari/subscriptions", "organizations_url": "https://api.github.com/users/panmari/orgs", "repos_url": "https://api.github.com/users/panmari/repos", "events_url": "https://api.github.com/users/panmari/events{/privacy}", "received_events_url": "https://api.github.com/users/panmari/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-09T05:21:36Z", "updated_at": "2016-03-09T05:21:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I ended up going a completely different route, since the non-sequential nature of reading (rather small) png files this way ended up being the bottle neck of my learning pipeline. Starting from <a href=\"https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/how_tos/reading_data/convert_to_records.py\">this example</a>, I wrote a small script that dumps multiple images into one <code>example</code> under different keys. That way, the two images are always read by the same reader, so I don't have to care about concurrency issues anymore. And since I dump 2000 examples into one file (which can be read sequentially), this way of reading data proved to be much faster (at least on traditional hard drives).</p>\n<p>I'll close this issue, since I feel this is the proper way to tackle this problem, not the one outlined in the original issue.</p>", "body_text": "I ended up going a completely different route, since the non-sequential nature of reading (rather small) png files this way ended up being the bottle neck of my learning pipeline. Starting from this example, I wrote a small script that dumps multiple images into one example under different keys. That way, the two images are always read by the same reader, so I don't have to care about concurrency issues anymore. And since I dump 2000 examples into one file (which can be read sequentially), this way of reading data proved to be much faster (at least on traditional hard drives).\nI'll close this issue, since I feel this is the proper way to tackle this problem, not the one outlined in the original issue.", "body": "I ended up going a completely different route, since the non-sequential nature of reading (rather small) png files this way ended up being the bottle neck of my learning pipeline. Starting from [this example](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/how_tos/reading_data/convert_to_records.py), I wrote a small script that dumps multiple images into one `example` under different keys. That way, the two images are always read by the same reader, so I don't have to care about concurrency issues anymore. And since I dump 2000 examples into one file (which can be read sequentially), this way of reading data proved to be much faster (at least on traditional hard drives). \n\nI'll close this issue, since I feel this is the proper way to tackle this problem, not the one outlined in the original issue.\n"}