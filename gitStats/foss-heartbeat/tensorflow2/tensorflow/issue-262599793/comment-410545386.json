{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/410545386", "html_url": "https://github.com/tensorflow/tensorflow/issues/13477#issuecomment-410545386", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13477", "id": 410545386, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDU0NTM4Ng==", "user": {"login": "StoyanVenDimitrov", "id": 19803516, "node_id": "MDQ6VXNlcjE5ODAzNTE2", "avatar_url": "https://avatars3.githubusercontent.com/u/19803516?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StoyanVenDimitrov", "html_url": "https://github.com/StoyanVenDimitrov", "followers_url": "https://api.github.com/users/StoyanVenDimitrov/followers", "following_url": "https://api.github.com/users/StoyanVenDimitrov/following{/other_user}", "gists_url": "https://api.github.com/users/StoyanVenDimitrov/gists{/gist_id}", "starred_url": "https://api.github.com/users/StoyanVenDimitrov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StoyanVenDimitrov/subscriptions", "organizations_url": "https://api.github.com/users/StoyanVenDimitrov/orgs", "repos_url": "https://api.github.com/users/StoyanVenDimitrov/repos", "events_url": "https://api.github.com/users/StoyanVenDimitrov/events{/privacy}", "received_events_url": "https://api.github.com/users/StoyanVenDimitrov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-05T20:23:10Z", "updated_at": "2018-08-20T09:23:09Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a>, in your first answer, what do you mean with \"single string tensor containing serialized tf.Examples.\"? I saw some examples of<a href=\"https://stackoverflow.com/questions/45900653/tensorflow-how-to-predict-from-a-savedmodel/47645229#47645229\" rel=\"nofollow\"> tf.Example</a>, but what does the rest mean?<br>\nI try to predict from a exported model I trained with a tensorflow hub module for German:<br>\n`      embedded_text_feature_column = hub.text_embedding_column(<br>\nkey='sentence',<br>\nmodule_spec='<a href=\"https://tfhub.dev/google/nnlm-de-dim128/1\" rel=\"nofollow\">https://tfhub.dev/google/nnlm-de-dim128/1</a>')</p>\n<pre><code>#Estimator\nestimator = tf.estimator.DNNClassifier(\n    hidden_units=[500, 100],\n    feature_columns=[embedded_text_feature_column],\n    n_classes=num_of_class,\n    optimizer=tf.train.AdagradOptimizer(learning_rate=0.003) )\n</code></pre>\n<p>...</p>\n<pre><code>feature_spec = tf.feature_column.make_parse_example_spec([embedded_text_feature_column])\nserving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n\nexport_dir_base = self.cfg['model_path']\nservable_model_path = estimator.export_savedmodel(export_dir_base, serving_input_receiver_fn)\n\n# Example message for inference\nmessage = \"Was ist denn los\"\nsaved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\ncontent_tf_list = tf.train.BytesList(value=[str.encode(message)])\nexample = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                'sentence': tf.train.Feature(\n                    bytes_list=content_tf_list\n                )\n            }\n        )\n    )\n\nwith tf.python_io.TFRecordWriter('the_message.tfrecords') as writer:\n    writer.write(example.SerializeToString())\n\nreader = tf.TFRecordReader()\ndata_path = 'the_message.tfrecords'\nfilename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\n_, serialized_example = reader.read(filename_queue)\noutput_dict = saved_model_predictor({'inputs': [serialized_example]})\n</code></pre>\n<p>`<br>\nBut what I get is:</p>\n<p><code>Traceback (most recent call last): .... output_dict = saved_model_predictor({'inputs': [serialized_example]}) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/contrib/predictor/predictor.py\", line 77, in __call__ return self._session.run(fetches=self.fetch_tensors, feed_dict=feed_dict) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run run_metadata_ptr) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run feed_dict_tensor, options, run_metadata) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run run_metadata) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.InternalError: Unable to get element as bytes.</code></p>\n<p>So the way I introduce the request is not correct?</p>\n<h1>=========<br>\nEDIT</h1>\n<p>In case somebody does the same like me - reading with tf.TFRecordReader() requires starting a session. That's where this <code>InternalError: Unable to get element as bytes.</code> comes from.<br>\nFor the above example, simply serialising the the example works:</p>\n<pre><code>        message = \"Was ist denn los\"\n        saved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\n        content_tf_list = tf.train.BytesList(value=[message.encode('utf-8')])\n        sentence = tf.train.Feature(bytes_list=content_tf_list)\n        sentence_dict = {'sentence': sentence}\n        features = tf.train.Features(feature=sentence_dict)\n        example = tf.train.Example(features=features)\n        serialized_example = example.SerializeToString()\n        output_dict = saved_model_predictor({'inputs': [serialized_example]})\n</code></pre>", "body_text": "Hi,\n@martinwicke, in your first answer, what do you mean with \"single string tensor containing serialized tf.Examples.\"? I saw some examples of tf.Example, but what does the rest mean?\nI try to predict from a exported model I trained with a tensorflow hub module for German:\n`      embedded_text_feature_column = hub.text_embedding_column(\nkey='sentence',\nmodule_spec='https://tfhub.dev/google/nnlm-de-dim128/1')\n#Estimator\nestimator = tf.estimator.DNNClassifier(\n    hidden_units=[500, 100],\n    feature_columns=[embedded_text_feature_column],\n    n_classes=num_of_class,\n    optimizer=tf.train.AdagradOptimizer(learning_rate=0.003) )\n\n...\nfeature_spec = tf.feature_column.make_parse_example_spec([embedded_text_feature_column])\nserving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n\nexport_dir_base = self.cfg['model_path']\nservable_model_path = estimator.export_savedmodel(export_dir_base, serving_input_receiver_fn)\n\n# Example message for inference\nmessage = \"Was ist denn los\"\nsaved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\ncontent_tf_list = tf.train.BytesList(value=[str.encode(message)])\nexample = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                'sentence': tf.train.Feature(\n                    bytes_list=content_tf_list\n                )\n            }\n        )\n    )\n\nwith tf.python_io.TFRecordWriter('the_message.tfrecords') as writer:\n    writer.write(example.SerializeToString())\n\nreader = tf.TFRecordReader()\ndata_path = 'the_message.tfrecords'\nfilename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\n_, serialized_example = reader.read(filename_queue)\noutput_dict = saved_model_predictor({'inputs': [serialized_example]})\n\n`\nBut what I get is:\nTraceback (most recent call last): .... output_dict = saved_model_predictor({'inputs': [serialized_example]}) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/contrib/predictor/predictor.py\", line 77, in __call__ return self._session.run(fetches=self.fetch_tensors, feed_dict=feed_dict) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run run_metadata_ptr) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run feed_dict_tensor, options, run_metadata) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run run_metadata) File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.InternalError: Unable to get element as bytes.\nSo the way I introduce the request is not correct?\n=========\nEDIT\nIn case somebody does the same like me - reading with tf.TFRecordReader() requires starting a session. That's where this InternalError: Unable to get element as bytes. comes from.\nFor the above example, simply serialising the the example works:\n        message = \"Was ist denn los\"\n        saved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\n        content_tf_list = tf.train.BytesList(value=[message.encode('utf-8')])\n        sentence = tf.train.Feature(bytes_list=content_tf_list)\n        sentence_dict = {'sentence': sentence}\n        features = tf.train.Features(feature=sentence_dict)\n        example = tf.train.Example(features=features)\n        serialized_example = example.SerializeToString()\n        output_dict = saved_model_predictor({'inputs': [serialized_example]})", "body": "Hi,\r\n@martinwicke, in your first answer, what do you mean with \"single string tensor containing serialized tf.Examples.\"? I saw some examples of[ tf.Example](https://stackoverflow.com/questions/45900653/tensorflow-how-to-predict-from-a-savedmodel/47645229#47645229), but what does the rest mean? \r\nI try to predict from a exported model I trained with a tensorflow hub module for German: \r\n`      embedded_text_feature_column = hub.text_embedding_column(\r\n        key='sentence',\r\n        module_spec='https://tfhub.dev/google/nnlm-de-dim128/1')\r\n\r\n    #Estimator\r\n    estimator = tf.estimator.DNNClassifier(\r\n        hidden_units=[500, 100],\r\n        feature_columns=[embedded_text_feature_column],\r\n        n_classes=num_of_class,\r\n        optimizer=tf.train.AdagradOptimizer(learning_rate=0.003) )\r\n\r\n...\r\n\r\n    feature_spec = tf.feature_column.make_parse_example_spec([embedded_text_feature_column])\r\n    serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\n\r\n    export_dir_base = self.cfg['model_path']\r\n    servable_model_path = estimator.export_savedmodel(export_dir_base, serving_input_receiver_fn)\r\n\r\n    # Example message for inference\r\n    message = \"Was ist denn los\"\r\n    saved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\r\n    content_tf_list = tf.train.BytesList(value=[str.encode(message)])\r\n    example = tf.train.Example(\r\n            features=tf.train.Features(\r\n                feature={\r\n                    'sentence': tf.train.Feature(\r\n                        bytes_list=content_tf_list\r\n                    )\r\n                }\r\n            )\r\n        )\r\n\r\n    with tf.python_io.TFRecordWriter('the_message.tfrecords') as writer:\r\n        writer.write(example.SerializeToString())\r\n\r\n    reader = tf.TFRecordReader()\r\n    data_path = 'the_message.tfrecords'\r\n    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\r\n    _, serialized_example = reader.read(filename_queue)\r\n    output_dict = saved_model_predictor({'inputs': [serialized_example]})\r\n\r\n`\r\nBut what I get is:\r\n\r\n`Traceback (most recent call last):\r\n....\r\n    output_dict = saved_model_predictor({'inputs': [serialized_example]})\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/contrib/predictor/predictor.py\", line 77, in __call__\r\n    return self._session.run(fetches=self.fetch_tensors, feed_dict=feed_dict)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Unable to get element as bytes.`\r\n\r\nSo the way I introduce the request is not correct?\r\n\r\n=========\r\nEDIT\r\n=========\r\n\r\nIn case somebody does the same like me - reading with tf.TFRecordReader() requires starting a session. That's where this `InternalError: Unable to get element as bytes.` comes from.\r\nFor the above example, simply serialising the the example works: \r\n\r\n```\r\n        message = \"Was ist denn los\"\r\n        saved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\r\n        content_tf_list = tf.train.BytesList(value=[message.encode('utf-8')])\r\n        sentence = tf.train.Feature(bytes_list=content_tf_list)\r\n        sentence_dict = {'sentence': sentence}\r\n        features = tf.train.Features(feature=sentence_dict)\r\n        example = tf.train.Example(features=features)\r\n        serialized_example = example.SerializeToString()\r\n        output_dict = saved_model_predictor({'inputs': [serialized_example]})\r\n```"}