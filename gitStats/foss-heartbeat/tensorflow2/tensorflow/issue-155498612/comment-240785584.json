{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/240785584", "html_url": "https://github.com/tensorflow/tensorflow/issues/2416#issuecomment-240785584", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2416", "id": 240785584, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MDc4NTU4NA==", "user": {"login": "erickrf", "id": 294483, "node_id": "MDQ6VXNlcjI5NDQ4Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/294483?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erickrf", "html_url": "https://github.com/erickrf", "followers_url": "https://api.github.com/users/erickrf/followers", "following_url": "https://api.github.com/users/erickrf/following{/other_user}", "gists_url": "https://api.github.com/users/erickrf/gists{/gist_id}", "starred_url": "https://api.github.com/users/erickrf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erickrf/subscriptions", "organizations_url": "https://api.github.com/users/erickrf/orgs", "repos_url": "https://api.github.com/users/erickrf/repos", "events_url": "https://api.github.com/users/erickrf/events{/privacy}", "received_events_url": "https://api.github.com/users/erickrf/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-18T16:51:35Z", "updated_at": "2016-08-18T16:51:35Z", "author_association": "NONE", "body_html": "<p>So I finally could look again into this problem.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> , I could not allocate the server just for myself, but here is the output of <code>nvidia-smi</code> at the time of a new similar <code>MemoryError</code>:</p>\n<pre><code>+------------------------------------------------------+                       \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:05:00.0     Off |                    0 |\n| N/A   32C    P8    46W / 149W |   3501MiB / 11519MiB |     89%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   69C    P0   172W / 149W |   3800MiB / 11519MiB |     91%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:09:00.0     Off |                    0 |\n| N/A   46C    P0    91W / 149W |   3500MiB / 11519MiB |     84%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:0A:00.0     Off |                    0 |\n| N/A   47C    P8    30W / 149W |     22MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     56514    C   python                                        3477MiB |\n|    1     43200    C   python                                        3776MiB |\n|    2     57034    C   python                                        3476MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>So one of the GPU's was completely free.<br>\nI could not recompile tensorflow with the printf statements yet, because I don't have root access in this server and installing all the necessary requirements is quite difficult.</p>\n<p>But I also tried running the following dummy script:</p>\n<pre><code>import tensorflow as tf\n\nsess = tf.InteractiveSession()\nx = tf.constant(1, shape=[100, 100, 100])\ny = tf.constant(2, shape=[100, 100, 100])\nz = x + y\n\nprint(z.eval())\n</code></pre>\n<p>Curiously, running the above script gives me different outputs depending on the amount of RAM allocated, and none of them is a Python <code>MemoryError</code>.</p>\n<ul>\n<li>With 8GB or less:</li>\n</ul>\n<pre><code>I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 7.92GiB\nFree memory: 7.79GiB\nSegmentation fault (core dumped)\n</code></pre>\n<p>gdb says it was terminated with SIGSEGV. Stack:</p>\n<pre><code>#0  0x00002abe9c8880bc in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00002abe9c623258 in tensorflow::GPUMachineManager() () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00002abe9c6214d0 in tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds(std::vector&lt;int, std::allocator&lt;int&gt; &gt;*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00002abe9c621d50 in tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&amp;, std::string const&amp;, std::vector&lt;tensorflow::Device*, std::allocator&lt;tensorflow::Device*&gt; &gt;*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00002abe9c7fa0d6 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&amp;, std::string const&amp;, std::vector&lt;tensorflow::Device*, std::allocator&lt;tensorflow::Device*&gt; &gt;*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00002abe9c5e5b71 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&amp;) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00002abe9c81c6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&amp;, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00002abe9c7eb0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00002abe9ba4ec3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002abe9219fe15 in call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7ffdfdfbd588) at Python/ceval.c:4350\n#10 PyEval_EvalFrameEx (f=&lt;optimized out&gt;, throwflag=&lt;optimized out&gt;) at Python/ceval.c:2987\n#11 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee194cb0, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, args=&lt;optimized out&gt;, argcount=4, kws=0x2abf45186d48, kwcount=0, \n    defs=0x2abeee0932e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#12 0x00002abe9219fa55 in fast_function (nk=&lt;optimized out&gt;, na=4, n=&lt;optimized out&gt;, pp_stack=0x7ffdfdfbd7a8, func=0x2abeee08ccf8) at Python/ceval.c:4446\n#13 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7ffdfdfbd7a8) at Python/ceval.c:4371\n#14 PyEval_EvalFrameEx (f=&lt;optimized out&gt;, throwflag=&lt;optimized out&gt;) at Python/ceval.c:2987\n#15 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee0953b0, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, args=&lt;optimized out&gt;, argcount=1, kws=0x0, kwcount=0, defs=0x2abeee0934c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n#16 0x00002abe9211c3a1 in function_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/funcobject.c:526\n#17 0x00002abe920ecd23 in PyObject_Call (func=0x2abeee098aa0, arg=&lt;optimized out&gt;, kw=&lt;optimized out&gt;) at Objects/abstract.c:2546\n#18 0x00002abe920ff4bf in instancemethod_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/classobject.c:2602\n#19 0x00002abe920ecd23 in PyObject_Call (func=0x2abef7f5ecd0, arg=&lt;optimized out&gt;, kw=&lt;optimized out&gt;) at Objects/abstract.c:2546\n#20 0x00002abe92159320 in slot_tp_init (self=0x2abf4517e2d0, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:5715\n#21 0x00002abe9214fdd8 in type_call (type=&lt;optimized out&gt;, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:745\n#22 0x00002abe920ecd23 in PyObject_Call (func=0x30dac40, arg=&lt;optimized out&gt;, kw=&lt;optimized out&gt;) at Objects/abstract.c:2546\n#23 0x00002abe9219df54 in do_call (nk=&lt;optimized out&gt;, na=&lt;optimized out&gt;, pp_stack=0x7ffdfdfbddc8, func=0x30dac40) at Python/ceval.c:4568\n#24 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7ffdfdfbddc8) at Python/ceval.c:4373\n#25 PyEval_EvalFrameEx (f=&lt;optimized out&gt;, throwflag=&lt;optimized out&gt;) at Python/ceval.c:2987\n(continues to vanilla python calls)\n</code></pre>\n<ul>\n<li>With 10GB:</li>\n</ul>\n<pre><code>I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 8.92GiB\nFree memory: 8.10GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:06:00.0\nTotal memory: 8.92GiB\nFree memory: 7.50GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:09:00.0\nTotal memory: 8.92GiB\nFree memory: 8.12GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:0a:00.0\nTotal memory: 8.92GiB\nFree memory: 8.40GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla K80, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla K80, pci bus id: 0000:0a:00.0)\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)\n</code></pre>\n<p>gdb says the program was terminated with SIGABRT and the following stack:</p>\n<pre><code>#0  0x00002b80422765f7 in raise () from /lib64/libc.so.6\n#1  0x00002b8042277ce8 in abort () from /lib64/libc.so.6\n#2  0x00002b8054de59d5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6\n#3  0x00002b8054de3946 in ?? () from /lib64/libstdc++.so.6\n#4  0x00002b8054de3973 in std::terminate() () from /lib64/libstdc++.so.6\n#5  0x00002b8054de3b93 in __cxa_throw () from /lib64/libstdc++.so.6\n#6  0x00002b8054e38e30 in std::__throw_system_error(int) () from /lib64/libstdc++.so.6\n#7  0x00002b8054e3a559 in std::thread::_M_start_thread(std::shared_ptr&lt;std::thread::_Impl_base&gt;) () from /lib64/libstdc++.so.6\n#8  0x00002b804be074b0 in tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&amp;, std::string const&amp;, std::function&lt;void ()&gt;) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002b804bdf1338 in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&amp;, std::string const&amp;, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00002b804bdf1f2f in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&amp;, std::string const&amp;, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00002b804bdf205a in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::string const&amp;, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00002b804ba71f98 in tensorflow::(anonymous namespace)::NewThreadPool(tensorflow::SessionOptions const&amp;) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00002b804ba77abc in tensorflow::DirectSession::DirectSession(tensorflow::SessionOptions const&amp;, tensorflow::DeviceMgr const*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00002b804ba77bb5 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&amp;) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00002b804bcae6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&amp;, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00002b804bc7d0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00002b804aee0c3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00002b8041631e15 in call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffb2412cd8) at Python/ceval.c:4350\n#19 PyEval_EvalFrameEx (f=&lt;optimized out&gt;, throwflag=&lt;optimized out&gt;) at Python/ceval.c:2987\n#20 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d626cb0, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, args=&lt;optimized out&gt;, argcount=4, kws=0x2b810c4c0d48, kwcount=0, \n    defs=0x2b809d5252e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#21 0x00002b8041631a55 in fast_function (nk=&lt;optimized out&gt;, na=4, n=&lt;optimized out&gt;, pp_stack=0x7fffb2412ef8, func=0x2b809d51ecf8) at Python/ceval.c:4446\n#22 call_function (oparg=&lt;optimized out&gt;, pp_stack=0x7fffb2412ef8) at Python/ceval.c:4371\n#23 PyEval_EvalFrameEx (f=&lt;optimized out&gt;, throwflag=&lt;optimized out&gt;) at Python/ceval.c:2987\n#24 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d5273b0, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;, args=&lt;optimized out&gt;, argcount=1, kws=0x0, kwcount=0, defs=0x2b809d5254c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n(continues to vanilla python calls)\n</code></pre>\n<ul>\n<li>With 12GB it works.</li>\n</ul>\n<p>Now, I have no idea how much RAM is required to have tensorflow running in GPU's, but 12GB seems definitely a huge amount for such a basic script.</p>", "body_text": "So I finally could look again into this problem.\n@zheng-xq , I could not allocate the server just for myself, but here is the output of nvidia-smi at the time of a new similar MemoryError:\n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:05:00.0     Off |                    0 |\n| N/A   32C    P8    46W / 149W |   3501MiB / 11519MiB |     89%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   69C    P0   172W / 149W |   3800MiB / 11519MiB |     91%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:09:00.0     Off |                    0 |\n| N/A   46C    P0    91W / 149W |   3500MiB / 11519MiB |     84%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:0A:00.0     Off |                    0 |\n| N/A   47C    P8    30W / 149W |     22MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     56514    C   python                                        3477MiB |\n|    1     43200    C   python                                        3776MiB |\n|    2     57034    C   python                                        3476MiB |\n+-----------------------------------------------------------------------------+\n\nSo one of the GPU's was completely free.\nI could not recompile tensorflow with the printf statements yet, because I don't have root access in this server and installing all the necessary requirements is quite difficult.\nBut I also tried running the following dummy script:\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\nx = tf.constant(1, shape=[100, 100, 100])\ny = tf.constant(2, shape=[100, 100, 100])\nz = x + y\n\nprint(z.eval())\n\nCuriously, running the above script gives me different outputs depending on the amount of RAM allocated, and none of them is a Python MemoryError.\n\nWith 8GB or less:\n\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 7.92GiB\nFree memory: 7.79GiB\nSegmentation fault (core dumped)\n\ngdb says it was terminated with SIGSEGV. Stack:\n#0  0x00002abe9c8880bc in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00002abe9c623258 in tensorflow::GPUMachineManager() () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00002abe9c6214d0 in tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds(std::vector<int, std::allocator<int> >*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00002abe9c621d50 in tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00002abe9c7fa0d6 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00002abe9c5e5b71 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00002abe9c81c6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00002abe9c7eb0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00002abe9ba4ec3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002abe9219fe15 in call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbd588) at Python/ceval.c:4350\n#10 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#11 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee194cb0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x2abf45186d48, kwcount=0, \n    defs=0x2abeee0932e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#12 0x00002abe9219fa55 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7ffdfdfbd7a8, func=0x2abeee08ccf8) at Python/ceval.c:4446\n#13 call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbd7a8) at Python/ceval.c:4371\n#14 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#15 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee0953b0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x0, kwcount=0, defs=0x2abeee0934c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n#16 0x00002abe9211c3a1 in function_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/funcobject.c:526\n#17 0x00002abe920ecd23 in PyObject_Call (func=0x2abeee098aa0, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#18 0x00002abe920ff4bf in instancemethod_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/classobject.c:2602\n#19 0x00002abe920ecd23 in PyObject_Call (func=0x2abef7f5ecd0, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#20 0x00002abe92159320 in slot_tp_init (self=0x2abf4517e2d0, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:5715\n#21 0x00002abe9214fdd8 in type_call (type=<optimized out>, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:745\n#22 0x00002abe920ecd23 in PyObject_Call (func=0x30dac40, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#23 0x00002abe9219df54 in do_call (nk=<optimized out>, na=<optimized out>, pp_stack=0x7ffdfdfbddc8, func=0x30dac40) at Python/ceval.c:4568\n#24 call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbddc8) at Python/ceval.c:4373\n#25 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n(continues to vanilla python calls)\n\n\nWith 10GB:\n\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 8.92GiB\nFree memory: 8.10GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:06:00.0\nTotal memory: 8.92GiB\nFree memory: 7.50GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:09:00.0\nTotal memory: 8.92GiB\nFree memory: 8.12GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:0a:00.0\nTotal memory: 8.92GiB\nFree memory: 8.40GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:0a:00.0)\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)\n\ngdb says the program was terminated with SIGABRT and the following stack:\n#0  0x00002b80422765f7 in raise () from /lib64/libc.so.6\n#1  0x00002b8042277ce8 in abort () from /lib64/libc.so.6\n#2  0x00002b8054de59d5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6\n#3  0x00002b8054de3946 in ?? () from /lib64/libstdc++.so.6\n#4  0x00002b8054de3973 in std::terminate() () from /lib64/libstdc++.so.6\n#5  0x00002b8054de3b93 in __cxa_throw () from /lib64/libstdc++.so.6\n#6  0x00002b8054e38e30 in std::__throw_system_error(int) () from /lib64/libstdc++.so.6\n#7  0x00002b8054e3a559 in std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>) () from /lib64/libstdc++.so.6\n#8  0x00002b804be074b0 in tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::string const&, std::function<void ()>) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002b804bdf1338 in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00002b804bdf1f2f in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00002b804bdf205a in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00002b804ba71f98 in tensorflow::(anonymous namespace)::NewThreadPool(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00002b804ba77abc in tensorflow::DirectSession::DirectSession(tensorflow::SessionOptions const&, tensorflow::DeviceMgr const*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00002b804ba77bb5 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00002b804bcae6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00002b804bc7d0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00002b804aee0c3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00002b8041631e15 in call_function (oparg=<optimized out>, pp_stack=0x7fffb2412cd8) at Python/ceval.c:4350\n#19 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#20 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d626cb0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x2b810c4c0d48, kwcount=0, \n    defs=0x2b809d5252e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#21 0x00002b8041631a55 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7fffb2412ef8, func=0x2b809d51ecf8) at Python/ceval.c:4446\n#22 call_function (oparg=<optimized out>, pp_stack=0x7fffb2412ef8) at Python/ceval.c:4371\n#23 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#24 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d5273b0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x0, kwcount=0, defs=0x2b809d5254c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n(continues to vanilla python calls)\n\n\nWith 12GB it works.\n\nNow, I have no idea how much RAM is required to have tensorflow running in GPU's, but 12GB seems definitely a huge amount for such a basic script.", "body": "So I finally could look again into this problem.\n\n@zheng-xq , I could not allocate the server just for myself, but here is the output of `nvidia-smi` at the time of a new similar `MemoryError`:\n\n```\n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:05:00.0     Off |                    0 |\n| N/A   32C    P8    46W / 149W |   3501MiB / 11519MiB |     89%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   69C    P0   172W / 149W |   3800MiB / 11519MiB |     91%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:09:00.0     Off |                    0 |\n| N/A   46C    P0    91W / 149W |   3500MiB / 11519MiB |     84%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:0A:00.0     Off |                    0 |\n| N/A   47C    P8    30W / 149W |     22MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     56514    C   python                                        3477MiB |\n|    1     43200    C   python                                        3776MiB |\n|    2     57034    C   python                                        3476MiB |\n+-----------------------------------------------------------------------------+\n```\n\nSo one of the GPU's was completely free.\nI could not recompile tensorflow with the printf statements yet, because I don't have root access in this server and installing all the necessary requirements is quite difficult.\n\nBut I also tried running the following dummy script:\n\n```\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\nx = tf.constant(1, shape=[100, 100, 100])\ny = tf.constant(2, shape=[100, 100, 100])\nz = x + y\n\nprint(z.eval())\n```\n\nCuriously, running the above script gives me different outputs depending on the amount of RAM allocated, and none of them is a Python `MemoryError`.\n- With 8GB or less:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 7.92GiB\nFree memory: 7.79GiB\nSegmentation fault (core dumped)\n```\n\ngdb says it was terminated with SIGSEGV. Stack:\n\n```\n#0  0x00002abe9c8880bc in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00002abe9c623258 in tensorflow::GPUMachineManager() () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00002abe9c6214d0 in tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds(std::vector<int, std::allocator<int> >*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00002abe9c621d50 in tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00002abe9c7fa0d6 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00002abe9c5e5b71 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00002abe9c81c6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00002abe9c7eb0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00002abe9ba4ec3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002abe9219fe15 in call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbd588) at Python/ceval.c:4350\n#10 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#11 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee194cb0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x2abf45186d48, kwcount=0, \n    defs=0x2abeee0932e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#12 0x00002abe9219fa55 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7ffdfdfbd7a8, func=0x2abeee08ccf8) at Python/ceval.c:4446\n#13 call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbd7a8) at Python/ceval.c:4371\n#14 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#15 0x00002abe921a0a2e in PyEval_EvalCodeEx (co=0x2abeee0953b0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x0, kwcount=0, defs=0x2abeee0934c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n#16 0x00002abe9211c3a1 in function_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/funcobject.c:526\n#17 0x00002abe920ecd23 in PyObject_Call (func=0x2abeee098aa0, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#18 0x00002abe920ff4bf in instancemethod_call (func=0x2abeee098aa0, arg=0x2abe91f6ca50, kw=0x0) at Objects/classobject.c:2602\n#19 0x00002abe920ecd23 in PyObject_Call (func=0x2abef7f5ecd0, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#20 0x00002abe92159320 in slot_tp_init (self=0x2abf4517e2d0, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:5715\n#21 0x00002abe9214fdd8 in type_call (type=<optimized out>, args=0x2abe91eb2050, kwds=0x0) at Objects/typeobject.c:745\n#22 0x00002abe920ecd23 in PyObject_Call (func=0x30dac40, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546\n#23 0x00002abe9219df54 in do_call (nk=<optimized out>, na=<optimized out>, pp_stack=0x7ffdfdfbddc8, func=0x30dac40) at Python/ceval.c:4568\n#24 call_function (oparg=<optimized out>, pp_stack=0x7ffdfdfbddc8) at Python/ceval.c:4373\n#25 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n(continues to vanilla python calls)\n```\n- With 10GB:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:05:00.0\nTotal memory: 8.92GiB\nFree memory: 8.10GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:06:00.0\nTotal memory: 8.92GiB\nFree memory: 7.50GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:09:00.0\nTotal memory: 8.92GiB\nFree memory: 8.12GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:0a:00.0\nTotal memory: 8.92GiB\nFree memory: 8.40GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:0a:00.0)\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)\n```\n\ngdb says the program was terminated with SIGABRT and the following stack: \n\n```\n#0  0x00002b80422765f7 in raise () from /lib64/libc.so.6\n#1  0x00002b8042277ce8 in abort () from /lib64/libc.so.6\n#2  0x00002b8054de59d5 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6\n#3  0x00002b8054de3946 in ?? () from /lib64/libstdc++.so.6\n#4  0x00002b8054de3973 in std::terminate() () from /lib64/libstdc++.so.6\n#5  0x00002b8054de3b93 in __cxa_throw () from /lib64/libstdc++.so.6\n#6  0x00002b8054e38e30 in std::__throw_system_error(int) () from /lib64/libstdc++.so.6\n#7  0x00002b8054e3a559 in std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>) () from /lib64/libstdc++.so.6\n#8  0x00002b804be074b0 in tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::string const&, std::function<void ()>) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x00002b804bdf1338 in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#10 0x00002b804bdf1f2f in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00002b804bdf205a in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::string const&, int) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00002b804ba71f98 in tensorflow::(anonymous namespace)::NewThreadPool(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00002b804ba77abc in tensorflow::DirectSession::DirectSession(tensorflow::SessionOptions const&, tensorflow::DeviceMgr const*) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00002b804ba77bb5 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x00002b804bcae6d7 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\n   from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#16 0x00002b804bc7d0e1 in TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#17 0x00002b804aee0c3b in _wrap_TF_NewSession () from /hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#18 0x00002b8041631e15 in call_function (oparg=<optimized out>, pp_stack=0x7fffb2412cd8) at Python/ceval.c:4350\n#19 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#20 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d626cb0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=4, kws=0x2b810c4c0d48, kwcount=0, \n    defs=0x2b809d5252e8, defcount=3, closure=0x0) at Python/ceval.c:3582\n#21 0x00002b8041631a55 in fast_function (nk=<optimized out>, na=4, n=<optimized out>, pp_stack=0x7fffb2412ef8, func=0x2b809d51ecf8) at Python/ceval.c:4446\n#22 call_function (oparg=<optimized out>, pp_stack=0x7fffb2412ef8) at Python/ceval.c:4371\n#23 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2987\n#24 0x00002b8041632a2e in PyEval_EvalCodeEx (co=0x2b809d5273b0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=1, kws=0x0, kwcount=0, defs=0x2b809d5254c8, \n    defcount=3, closure=0x0) at Python/ceval.c:3582\n(continues to vanilla python calls)\n```\n- With 12GB it works. \n\nNow, I have no idea how much RAM is required to have tensorflow running in GPU's, but 12GB seems definitely a huge amount for such a basic script. \n"}