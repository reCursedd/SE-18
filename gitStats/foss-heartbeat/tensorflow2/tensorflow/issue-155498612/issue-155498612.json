{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2416", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2416/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2416/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2416/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2416", "id": 155498612, "node_id": "MDU6SXNzdWUxNTU0OTg2MTI=", "number": 2416, "title": "Memory error running tensorflow code on GPU", "user": {"login": "erickrf", "id": 294483, "node_id": "MDQ6VXNlcjI5NDQ4Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/294483?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erickrf", "html_url": "https://github.com/erickrf", "followers_url": "https://api.github.com/users/erickrf/followers", "following_url": "https://api.github.com/users/erickrf/following{/other_user}", "gists_url": "https://api.github.com/users/erickrf/gists{/gist_id}", "starred_url": "https://api.github.com/users/erickrf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erickrf/subscriptions", "organizations_url": "https://api.github.com/users/erickrf/orgs", "repos_url": "https://api.github.com/users/erickrf/repos", "events_url": "https://api.github.com/users/erickrf/events{/privacy}", "received_events_url": "https://api.github.com/users/erickrf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 20, "created_at": "2016-05-18T13:16:28Z", "updated_at": "2017-12-22T18:27:57Z", "closed_at": "2017-12-22T18:27:57Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Scientific Linux release 7.2 (Nitrogen)</p>\n<p>Installed version of CUDA and cuDNN:<br>\n$ ll /usr/local/cuda/lib/libcud*</p>\n<pre><code>-rw-r--r-- 1 root root 185K Mar 18 15:29 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root   16 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so -&gt; libcudart.so.7.5\nlrwxrwxrwx 1 root root   19 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 305K Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 545K Mar 18 15:29 /usr/local/cuda/lib/libcudart_static.a\n</code></pre>\n<p>Installed version 0.8.0 with anaconda.</p>\n<p>I'm getting a MemoryError when I try to run a tensorflow script on a server with GPU support. The same code, with the same inputs, runs without problems on my local machine, which is CPU only and has 8 GB of RAM.</p>\n<p>I tried to allocate up to 64 GB to run the script, and the same problem occurred. Here's the stacktrace:</p>\n<pre><code>Traceback (most recent call last):\n  File \"src/train.py\", line 93, in &lt;module&gt;\n    learning_rate=args.rate, l2_constant=args.l2)\n  File \"/hltsrv0/rocha/rte-lstm/src/rte_lstm.py\", line 189, in __init__\n    gradients, v = zip(*optimizer.compute_gradients(self.loss))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 485, in gradients\n    in_grads = control_flow_ops.tuple(in_grads)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1784, in tuple\n    tpl.append(with_dependencies([gate], t))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1664, in with_dependencies\n    return _Identity(output_tensor, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 114, in _Identity\n    return array_ops.identity(data, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 609, in identity\n    return _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1165, in __init__\n    self._recompute_node_def()\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1318, in _recompute_node_def\n    self._control_inputs])\nMemoryError\n</code></pre>\n<p>From the stack trace, I see that the error happens sometime during the gradient computation, but I have no idea why it only happens when I run the code in the server.</p>\n<p>By the way, I'm not experienced in GPU computation. I just tried running the same code in an environment with the cuda libraries and tensorflow with GPU support installed.</p>", "body_text": "Environment info\nOperating System: Scientific Linux release 7.2 (Nitrogen)\nInstalled version of CUDA and cuDNN:\n$ ll /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 185K Mar 18 15:29 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root   16 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root   19 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 305K Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 545K Mar 18 15:29 /usr/local/cuda/lib/libcudart_static.a\n\nInstalled version 0.8.0 with anaconda.\nI'm getting a MemoryError when I try to run a tensorflow script on a server with GPU support. The same code, with the same inputs, runs without problems on my local machine, which is CPU only and has 8 GB of RAM.\nI tried to allocate up to 64 GB to run the script, and the same problem occurred. Here's the stacktrace:\nTraceback (most recent call last):\n  File \"src/train.py\", line 93, in <module>\n    learning_rate=args.rate, l2_constant=args.l2)\n  File \"/hltsrv0/rocha/rte-lstm/src/rte_lstm.py\", line 189, in __init__\n    gradients, v = zip(*optimizer.compute_gradients(self.loss))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 485, in gradients\n    in_grads = control_flow_ops.tuple(in_grads)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1784, in tuple\n    tpl.append(with_dependencies([gate], t))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1664, in with_dependencies\n    return _Identity(output_tensor, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 114, in _Identity\n    return array_ops.identity(data, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 609, in identity\n    return _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1165, in __init__\n    self._recompute_node_def()\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1318, in _recompute_node_def\n    self._control_inputs])\nMemoryError\n\nFrom the stack trace, I see that the error happens sometime during the gradient computation, but I have no idea why it only happens when I run the code in the server.\nBy the way, I'm not experienced in GPU computation. I just tried running the same code in an environment with the cuda libraries and tensorflow with GPU support installed.", "body": "### Environment info\n\nOperating System: Scientific Linux release 7.2 (Nitrogen)\n\nInstalled version of CUDA and cuDNN: \n$ ll /usr/local/cuda/lib/libcud*\n\n```\n-rw-r--r-- 1 root root 185K Mar 18 15:29 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root   16 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root   19 Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 305K Mar 18 15:29 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 545K Mar 18 15:29 /usr/local/cuda/lib/libcudart_static.a\n```\n\nInstalled version 0.8.0 with anaconda.\n\nI'm getting a MemoryError when I try to run a tensorflow script on a server with GPU support. The same code, with the same inputs, runs without problems on my local machine, which is CPU only and has 8 GB of RAM. \n\nI tried to allocate up to 64 GB to run the script, and the same problem occurred. Here's the stacktrace:\n\n```\nTraceback (most recent call last):\n  File \"src/train.py\", line 93, in <module>\n    learning_rate=args.rate, l2_constant=args.l2)\n  File \"/hltsrv0/rocha/rte-lstm/src/rte_lstm.py\", line 189, in __init__\n    gradients, v = zip(*optimizer.compute_gradients(self.loss))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 485, in gradients\n    in_grads = control_flow_ops.tuple(in_grads)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1784, in tuple\n    tpl.append(with_dependencies([gate], t))\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1664, in with_dependencies\n    return _Identity(output_tensor, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 114, in _Identity\n    return array_ops.identity(data, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 609, in identity\n    return _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1165, in __init__\n    self._recompute_node_def()\n  File \"/hltsrv0/rocha/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1318, in _recompute_node_def\n    self._control_inputs])\nMemoryError\n```\n\nFrom the stack trace, I see that the error happens sometime during the gradient computation, but I have no idea why it only happens when I run the code in the server. \n\nBy the way, I'm not experienced in GPU computation. I just tried running the same code in an environment with the cuda libraries and tensorflow with GPU support installed.\n"}