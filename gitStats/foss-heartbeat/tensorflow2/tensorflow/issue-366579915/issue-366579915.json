{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22710", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22710/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22710/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22710/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22710", "id": 366579915, "node_id": "MDU6SXNzdWUzNjY1Nzk5MTU=", "number": 22710, "title": "[Cloud TPU] Intermittent freezes requiring reset of the TPU", "user": {"login": "Keno", "id": 1291671, "node_id": "MDQ6VXNlcjEyOTE2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Keno", "html_url": "https://github.com/Keno", "followers_url": "https://api.github.com/users/Keno/followers", "following_url": "https://api.github.com/users/Keno/following{/other_user}", "gists_url": "https://api.github.com/users/Keno/gists{/gist_id}", "starred_url": "https://api.github.com/users/Keno/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Keno/subscriptions", "organizations_url": "https://api.github.com/users/Keno/orgs", "repos_url": "https://api.github.com/users/Keno/repos", "events_url": "https://api.github.com/users/Keno/events{/privacy}", "received_events_url": "https://api.github.com/users/Keno/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, {"login": "fdxmw", "id": 30300826, "node_id": "MDQ6VXNlcjMwMzAwODI2", "avatar_url": "https://avatars3.githubusercontent.com/u/30300826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fdxmw", "html_url": "https://github.com/fdxmw", "followers_url": "https://api.github.com/users/fdxmw/followers", "following_url": "https://api.github.com/users/fdxmw/following{/other_user}", "gists_url": "https://api.github.com/users/fdxmw/gists{/gist_id}", "starred_url": "https://api.github.com/users/fdxmw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fdxmw/subscriptions", "organizations_url": "https://api.github.com/users/fdxmw/orgs", "repos_url": "https://api.github.com/users/fdxmw/repos", "events_url": "https://api.github.com/users/fdxmw/events{/privacy}", "received_events_url": "https://api.github.com/users/fdxmw/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-04T00:35:51Z", "updated_at": "2018-11-14T19:27:08Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 18.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: r1.11</li>\n<li><strong>Python version</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.16.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 7.2.0</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>We are seeing intermittent freezes of the Cloud TPU when running VGG19 inference from the Julia frontend via xrt (do note that we're also seeing incorrect answers, which is filed as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"366577256\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/22709\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/22709/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/22709\">#22709</a> and may or may not be related). If the Cloud TPU freezes, the <code>TF_SessionRun</code> call never returns. An aggravating, but non-essential factor is the concurrent execution of the TPU profiler via <code>capture_tpu_profile</code>. In that situation both the main program and the <code>capture_tpu_profile</code> invocation never exit. If the profiler is running, we usually see only 2-3 successful runs until things start hanging. If I manually disconnect and forcefully kill the session (by severing the socket) and then reconnect, I see either continued freezes or errors of the form</p>\n<pre><code>ERROR: Tensorflow error: Status: Unable to enqueue when not opened, queue: [0000:00:04.0 PE0 C0 MC0 TN0 Queue HBM_WRITE]. State is: CLOSED\n\t [[{{node XRTAllocate}} = XRTAllocate[_device=\"/job:tpu_worker/replica:0/task:0/device:TPU:0\"](XRTAllocate/Const_G1)]]\n</code></pre>\n<p>my standard protocol to recover from this has been the following:</p>\n<ul>\n<li>Reconnect the session</li>\n<li>Run the <code>ShutdownDistributedTPU</code> op (Remote session closes)</li>\n<li>Reconnect the session</li>\n<li>Run the <code>ShutdownDistributedTPU</code> op (Causes a non-fatal error, but the next step hangs if not done)</li>\n<li>Run the <code>ConfigureDistributedTPU</code> op (works)</li>\n</ul>\n<p>afterwards I can usually use the TPU again.</p>\n<h3>Source code / logs</h3>\n<p>XLA dump (batch size N=1, but the same happens for at least N=10 and N=100): <a href=\"https://gist.github.com/Keno/b54e4be146096daf4d464c1319639404\">https://gist.github.com/Keno/b54e4be146096daf4d464c1319639404</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): r1.11\nPython version: N/A\nBazel version (if compiling from source): 0.16.1\nGCC/Compiler version (if compiling from source): 7.2.0\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nWe are seeing intermittent freezes of the Cloud TPU when running VGG19 inference from the Julia frontend via xrt (do note that we're also seeing incorrect answers, which is filed as #22709 and may or may not be related). If the Cloud TPU freezes, the TF_SessionRun call never returns. An aggravating, but non-essential factor is the concurrent execution of the TPU profiler via capture_tpu_profile. In that situation both the main program and the capture_tpu_profile invocation never exit. If the profiler is running, we usually see only 2-3 successful runs until things start hanging. If I manually disconnect and forcefully kill the session (by severing the socket) and then reconnect, I see either continued freezes or errors of the form\nERROR: Tensorflow error: Status: Unable to enqueue when not opened, queue: [0000:00:04.0 PE0 C0 MC0 TN0 Queue HBM_WRITE]. State is: CLOSED\n\t [[{{node XRTAllocate}} = XRTAllocate[_device=\"/job:tpu_worker/replica:0/task:0/device:TPU:0\"](XRTAllocate/Const_G1)]]\n\nmy standard protocol to recover from this has been the following:\n\nReconnect the session\nRun the ShutdownDistributedTPU op (Remote session closes)\nReconnect the session\nRun the ShutdownDistributedTPU op (Causes a non-fatal error, but the next step hangs if not done)\nRun the ConfigureDistributedTPU op (works)\n\nafterwards I can usually use the TPU again.\nSource code / logs\nXLA dump (batch size N=1, but the same happens for at least N=10 and N=100): https://gist.github.com/Keno/b54e4be146096daf4d464c1319639404", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.11\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nWe are seeing intermittent freezes of the Cloud TPU when running VGG19 inference from the Julia frontend via xrt (do note that we're also seeing incorrect answers, which is filed as #22709 and may or may not be related). If the Cloud TPU freezes, the `TF_SessionRun` call never returns. An aggravating, but non-essential factor is the concurrent execution of the TPU profiler via `capture_tpu_profile`. In that situation both the main program and the `capture_tpu_profile` invocation never exit. If the profiler is running, we usually see only 2-3 successful runs until things start hanging. If I manually disconnect and forcefully kill the session (by severing the socket) and then reconnect, I see either continued freezes or errors of the form\r\n\r\n```\r\nERROR: Tensorflow error: Status: Unable to enqueue when not opened, queue: [0000:00:04.0 PE0 C0 MC0 TN0 Queue HBM_WRITE]. State is: CLOSED\r\n\t [[{{node XRTAllocate}} = XRTAllocate[_device=\"/job:tpu_worker/replica:0/task:0/device:TPU:0\"](XRTAllocate/Const_G1)]]\r\n```\r\n\r\nmy standard protocol to recover from this has been the following:\r\n- Reconnect the session\r\n- Run the `ShutdownDistributedTPU` op (Remote session closes)\r\n- Reconnect the session\r\n- Run the `ShutdownDistributedTPU` op (Causes a non-fatal error, but the next step hangs if not done)\r\n- Run the `ConfigureDistributedTPU` op (works)\r\n\r\nafterwards I can usually use the TPU again.\r\n\r\n### Source code / logs\r\nXLA dump (batch size N=1, but the same happens for at least N=10 and N=100): https://gist.github.com/Keno/b54e4be146096daf4d464c1319639404\r\n"}