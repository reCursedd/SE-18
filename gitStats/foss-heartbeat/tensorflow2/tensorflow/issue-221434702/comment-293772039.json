{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293772039", "html_url": "https://github.com/tensorflow/tensorflow/issues/9176#issuecomment-293772039", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9176", "id": 293772039, "node_id": "MDEyOklzc3VlQ29tbWVudDI5Mzc3MjAzOQ==", "user": {"login": "vamsivarada", "id": 11680996, "node_id": "MDQ6VXNlcjExNjgwOTk2", "avatar_url": "https://avatars3.githubusercontent.com/u/11680996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vamsivarada", "html_url": "https://github.com/vamsivarada", "followers_url": "https://api.github.com/users/vamsivarada/followers", "following_url": "https://api.github.com/users/vamsivarada/following{/other_user}", "gists_url": "https://api.github.com/users/vamsivarada/gists{/gist_id}", "starred_url": "https://api.github.com/users/vamsivarada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vamsivarada/subscriptions", "organizations_url": "https://api.github.com/users/vamsivarada/orgs", "repos_url": "https://api.github.com/users/vamsivarada/repos", "events_url": "https://api.github.com/users/vamsivarada/events{/privacy}", "received_events_url": "https://api.github.com/users/vamsivarada/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-13T03:48:30Z", "updated_at": "2017-04-13T03:48:30Z", "author_association": "NONE", "body_html": "<p>hello ciasq iam using 1.0.1 tensorflow  flow while running the below code iam getting error</p>\n<h1>Copyright 2015 Conchylicultor. All Rights Reserved.</h1>\n<h1></h1>\n<h1>Licensed under the Apache License, Version 2.0 (the \"License\");</h1>\n<h1>you may not use this file except in compliance with the License.</h1>\n<h1>You may obtain a copy of the License at</h1>\n<h1></h1>\n<h1><a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\">http://www.apache.org/licenses/LICENSE-2.0</a></h1>\n<h1></h1>\n<h1>Unless required by applicable law or agreed to in writing, software</h1>\n<h1>distributed under the License is distributed on an \"AS IS\" BASIS,</h1>\n<h1>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</h1>\n<h1>See the License for the specific language governing permissions and</h1>\n<h1>limitations under the License.</h1>\n<h1>==============================================================================</h1>\n<p>\"\"\"<br>\nMain script. See README.md for more information</p>\n<p>Use python 3<br>\n\"\"\"</p>\n<p>import argparse  # Command line parsing<br>\nimport configparser  # Saving the models parameters<br>\nimport datetime  # Chronometer<br>\nimport os  # Files management<br>\nimport tensorflow as tf<br>\nimport numpy as np<br>\nimport math</p>\n<p>from tqdm import tqdm  # Progress bar<br>\nfrom tensorflow.python import debug as tf_debug</p>\n<p>from chatbot.textdata import TextData<br>\nfrom chatbot.model import Model</p>\n<p>class Chatbot:<br>\n\"\"\"<br>\nMain class which launch the training or testing mode<br>\n\"\"\"</p>\n<pre><code>class TestMode:\n    \"\"\" Simple structure representing the different testing modes\n    \"\"\"\n    ALL = 'all'\n    INTERACTIVE = 'interactive'  # The user can write his own questions\n    DAEMON = 'daemon'  # The chatbot runs on background and can regularly be called to predict something\n\ndef __init__(self):\n    \"\"\"\n    \"\"\"\n    # Model/dataset parameters\n    self.args = None\n\n    # Task specific object\n    self.textData = None  # Dataset\n    self.model = None  # Sequence to sequence model\n\n    # Tensorflow utilities for convenience saving/logging\n    self.writer = None\n    self.saver = None\n    self.modelDir = ''  # Where the model is saved\n    self.globStep = 0  # Represent the number of iteration for the current model\n\n    # TensorFlow main session (we keep track for the daemon)\n    self.sess = None\n\n    # Filename and directories constants\n    self.MODEL_DIR_BASE = 'save/model'\n    self.MODEL_NAME_BASE = 'model'\n    self.MODEL_EXT = '.ckpt'\n    self.CONFIG_FILENAME = 'params.ini'\n    self.CONFIG_VERSION = '0.5'\n    self.TEST_IN_NAME = 'data/test/samples.txt'\n    self.TEST_OUT_SUFFIX = '_predictions.txt'\n    self.SENTENCES_PREFIX = ['Q: ', 'A: ']\n\n@staticmethod\ndef parseArgs(args):\n    \"\"\"\n    Parse the arguments from the given command line\n    Args:\n        args (list&lt;str&gt;): List of arguments to parse. If None, the default sys.argv will be parsed\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n\n    # Global options\n    globalArgs = parser.add_argument_group('Global options')\n    globalArgs.add_argument('--test',\n                            nargs='?',\n                            choices=[Chatbot.TestMode.ALL, Chatbot.TestMode.INTERACTIVE, Chatbot.TestMode.DAEMON],\n                            const=Chatbot.TestMode.ALL, default=None,\n                            help='if present, launch the program try to answer all sentences from data/test/ with'\n                                 ' the defined model(s), in interactive mode, the user can wrote his own sentences,'\n                                 ' use daemon mode to integrate the chatbot in another program')\n    globalArgs.add_argument('--createDataset', action='store_true', help='if present, the program will only generate the dataset from the corpus (no training/testing)')\n    globalArgs.add_argument('--playDataset', type=int, nargs='?', const=10, default=None,  help='if set, the program  will randomly play some samples(can be use conjointly with createDataset if this is the only action you want to perform)')\n    globalArgs.add_argument('--reset', action='store_true', help='use this if you want to ignore the previous model present on the model directory (Warning: the model will be destroyed with all the folder content)')\n    globalArgs.add_argument('--verbose', action='store_true', help='When testing, will plot the outputs at the same time they are computed')\n    globalArgs.add_argument('--debug', action='store_true', help='run DeepQA with Tensorflow debug mode. Read TF documentation for more details on this.')\n    globalArgs.add_argument('--keepAll', action='store_true', help='If this option is set, all saved model will be kept (Warning: make sure you have enough free disk space or increase saveEvery)')  # TODO: Add an option to delimit the max size\n    globalArgs.add_argument('--modelTag', type=str, default=None, help='tag to differentiate which model to store/load')\n    globalArgs.add_argument('--rootDir', type=str, default=None, help='folder where to look for the models and data')\n    globalArgs.add_argument('--watsonMode', action='store_true', help='Inverse the questions and answer when training (the network try to guess the question)')\n    globalArgs.add_argument('--autoEncode', action='store_true', help='Randomly pick the question or the answer and use it both as input and output')\n    globalArgs.add_argument('--device', type=str, default=None, help='\\'gpu\\' or \\'cpu\\' (Warning: make sure you have enough free RAM), allow to choose on which hardware run the model')\n    globalArgs.add_argument('--seed', type=int, default=None, help='random seed for replication')\n\n    # Dataset options\n    datasetArgs = parser.add_argument_group('Dataset options')\n    datasetArgs.add_argument('--corpus', choices=TextData.corpusChoices(), default=TextData.corpusChoices()[0], help='corpus on which extract the dataset.')\n    datasetArgs.add_argument('--datasetTag', type=str, default='', help='add a tag to the dataset (file where to load the vocabulary and the precomputed samples, not the original corpus). Useful to manage multiple versions. Also used to define the file used for the lightweight format.')  # The samples are computed from the corpus if it does not exist already. There are saved in \\'data/samples/\\'\n    datasetArgs.add_argument('--ratioDataset', type=float, default=1.0, help='ratio of dataset used to avoid using the whole dataset')  # Not implemented, useless ?\n    datasetArgs.add_argument('--maxLength', type=int, default=10, help='maximum length of the sentence (for input and output), define number of maximum step of the RNN')\n    datasetArgs.add_argument('--filterVocab', type=int, default=1, help='remove rarelly used words (by default words used only once). 0 to keep all words.')\n    datasetArgs.add_argument('--skipLines', action='store_true', help='Generate training samples by only using even conversation lines as questions (and odd lines as answer). Useful to train the network on a particular person.')\n    datasetArgs.add_argument('--vocabularySize', type=int, default=40000, help='Limit the number of words in the vocabulary (0 for unlimited)')\n\n    # Network options (Warning: if modifying something here, also make the change on save/loadParams() )\n    nnArgs = parser.add_argument_group('Network options', 'architecture related option')\n    nnArgs.add_argument('--hiddenSize', type=int, default=512, help='number of hidden units in each RNN cell')\n    nnArgs.add_argument('--numLayers', type=int, default=2, help='number of rnn layers')\n    nnArgs.add_argument('--softmaxSamples', type=int, default=0, help='Number of samples in the sampled softmax loss function. A value of 0 deactivates sampled softmax')\n    nnArgs.add_argument('--initEmbeddings', action='store_true', help='if present, the program will initialize the embeddings with pre-trained word2vec vectors')\n    nnArgs.add_argument('--embeddingSize', type=int, default=64, help='embedding size of the word representation')\n    nnArgs.add_argument('--embeddingSource', type=str, default=\"GoogleNews-vectors-negative300.bin\", help='embedding file to use for the word representation')\n\n    # Training options\n    trainingArgs = parser.add_argument_group('Training options')\n    trainingArgs.add_argument('--numEpochs', type=int, default=30, help='maximum number of epochs to run')\n    trainingArgs.add_argument('--saveEvery', type=int, default=2000, help='nb of mini-batch step before creating a model checkpoint')\n    trainingArgs.add_argument('--batchSize', type=int, default=256, help='mini-batch size')\n    trainingArgs.add_argument('--learningRate', type=float, default=0.002, help='Learning rate')\n    trainingArgs.add_argument('--dropout', type=float, default=0.9, help='Dropout rate (keep probabilities)')\n\n    return parser.parse_args(args)\n\ndef main(self, args=None):\n    \"\"\"\n    Launch the training and/or the interactive mode\n    \"\"\"\n    print('Welcome to DeepQA v0.1 !')\n    print()\n    print('TensorFlow detected: v{}'.format(tf.__version__))\n\n    # General initialisation\n\n    self.args = self.parseArgs(args)\n\n    if not self.args.rootDir:\n        self.args.rootDir = os.getcwd()  # Use the current working directory\n\n    #tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\n\n    self.loadModelParams()  # Update the self.modelDir and self.globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\n\n    self.textData = TextData(self.args)\n    # TODO: Add a mode where we can force the input of the decoder // Try to visualize the predictions for\n    # each word of the vocabulary / decoder input\n    # TODO: For now, the model are trained for a specific dataset (because of the maxLength which define the\n    # vocabulary). Add a compatibility mode which allow to launch a model trained on a different vocabulary (\n    # remap the word2id/id2word variables).\n    if self.args.createDataset:\n        print('Dataset created! Thanks for using this program')\n        return  # No need to go further\n\n    # Prepare the model\n    with tf.device(self.getDevice()):\n        self.model = Model(self.args, self.textData)\n\n    # Saver/summaries\n    self.writer = tf.summary.FileWriter(self._getSummaryName())\n    self.saver = tf.train.Saver(max_to_keep=200)\n\n    # TODO: Fixed seed (WARNING: If dataset shuffling, make sure to do that after saving the\n    # dataset, otherwise, all which cames after the shuffling won't be replicable when\n    # reloading the dataset). How to restore the seed after loading ??\n    # Also fix seed for random.shuffle (does it works globally for all files ?)\n\n    # Running session\n    self.sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,  # Allows backup device for non GPU-available operations (when forcing GPU)\n        log_device_placement=False)  # Too verbose ?\n    )  # TODO: Replace all sess by self.sess (not necessary a good idea) ?\n\n    if self.args.debug:\n        self.sess = tf_debug.LocalCLIDebugWrapperSession(self.sess)\n        self.sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n\n    print('Initialize variables...')\n    self.sess.run(tf.global_variables_initializer())\n\n    # Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\n    if self.args.test != Chatbot.TestMode.ALL:\n        self.managePreviousModel(self.sess)\n\n    # Initialize embeddings with pre-trained word2vec vectors\n    if self.args.initEmbeddings:\n        self.loadEmbedding(self.sess)\n\n    if self.args.test:\n        if self.args.test == Chatbot.TestMode.INTERACTIVE:\n            self.mainTestInteractive(self.sess)\n        elif self.args.test == Chatbot.TestMode.ALL:\n            print('Start predicting...')\n            self.predictTestset(self.sess)\n            print('All predictions done')\n        elif self.args.test == Chatbot.TestMode.DAEMON:\n            print('Daemon mode, running in background...')\n        else:\n            raise RuntimeError('Unknown test mode: {}'.format(self.args.test))  # Should never happen\n    else:\n        self.mainTrain(self.sess)\n\n    if self.args.test != Chatbot.TestMode.DAEMON:\n        self.sess.close()\n        print(\"The End! Thanks for using this program\")\n\ndef mainTrain(self, sess):\n    \"\"\" Training loop\n    Args:\n        sess: The current running session\n    \"\"\"\n\n    # Specific training dependent loading\n\n    self.textData.makeLighter(self.args.ratioDataset)  # Limit the number of training samples\n\n    mergedSummaries = tf.summary.merge_all()  # Define the summary operator (Warning: Won't appear on the tensorboard graph)\n    if self.globStep == 0:  # Not restoring from previous run\n        self.writer.add_graph(sess.graph)  # First time only\n\n    # If restoring a model, restore the progression bar ? and current batch ?\n\n    print('Start training (press Ctrl+C to save and exit)...')\n\n    try:  # If the user exit while training, we still try to save the model\n        for e in range(self.args.numEpochs):\n\n            print()\n            print(\"----- Epoch {}/{} ; (lr={}) -----\".format(e+1, self.args.numEpochs, self.args.learningRate))\n\n            batches = self.textData.getBatches()\n\n            # TODO: Also update learning parameters eventually\n\n            tic = datetime.datetime.now()\n            for nextBatch in tqdm(batches, desc=\"Training\"):\n                # Training pass\n                ops, feedDict = self.model.step(nextBatch)\n                assert len(ops) == 2  # training, loss\n                _, loss, summary = sess.run(ops + (mergedSummaries,), feedDict)\n                self.writer.add_summary(summary, self.globStep)\n                self.globStep += 1\n\n                # Output training status\n                if self.globStep % 100 == 0:\n                    perplexity = math.exp(float(loss)) if loss &lt; 300 else float(\"inf\")\n                    tqdm.write(\"----- Step %d -- Loss %.2f -- Perplexity %.2f\" % (self.globStep, loss, perplexity))\n\n                # Checkpoint\n                if self.globStep % self.args.saveEvery == 0:\n                    self._saveSession(sess)\n\n            toc = datetime.datetime.now()\n\n            print(\"Epoch finished in {}\".format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn't really nicer\n    except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\n        print('Interruption detected, exiting the program...')\n\n    self._saveSession(sess)  # Ultimate saving before complete exit\n\ndef predictTestset(self, sess):\n    \"\"\" Try predicting the sentences from the samples.txt file.\n    The sentences are saved on the modelDir under the same name\n    Args:\n        sess: The current running session\n    \"\"\"\n\n    # Loading the file to predict\n    with open(os.path.join(self.args.rootDir, self.TEST_IN_NAME), 'r') as f:\n        lines = f.readlines()\n\n    modelList = self._getModelList()\n    if not modelList:\n        print('Warning: No model found in \\'{}\\'. Please train a model before trying to predict'.format(self.modelDir))\n        return\n\n    # Predicting for each model present in modelDir\n    for modelName in sorted(modelList):  # TODO: Natural sorting\n        print('Restoring previous model from {}'.format(modelName))\n        self.saver.restore(sess, modelName)\n        print('Testing...')\n\n        saveName = modelName[:-len(self.MODEL_EXT)] + self.TEST_OUT_SUFFIX  # We remove the model extension and add the prediction suffix\n        with open(saveName, 'w') as f:\n            nbIgnored = 0\n            for line in tqdm(lines, desc='Sentences'):\n                question = line[:-1]  # Remove the endl character\n\n                answer = self.singlePredict(question)\n                if not answer:\n                    nbIgnored += 1\n                    continue  # Back to the beginning, try again\n\n                predString = '{x[0]}{0}\\n{x[1]}{1}\\n\\n'.format(question, self.textData.sequence2str(answer, clean=True), x=self.SENTENCES_PREFIX)\n                if self.args.verbose:\n                    tqdm.write(predString)\n                f.write(predString)\n            print('Prediction finished, {}/{} sentences ignored (too long)'.format(nbIgnored, len(lines)))\n\ndef mainTestInteractive(self, sess):\n    \"\"\" Try predicting the sentences that the user will enter in the console\n    Args:\n        sess: The current running session\n    \"\"\"\n    # TODO: If verbose mode, also show similar sentences from the training set with the same words (include in mainTest also)\n    # TODO: Also show the top 10 most likely predictions for each predicted output (when verbose mode)\n    # TODO: Log the questions asked for latter re-use (merge with test/samples.txt)\n\n    print('Testing: Launch interactive mode:')\n    print('')\n    print('Welcome to the interactive mode, here you can ask to Deep Q&amp;A the sentence you want. Don\\'t have high '\n          'expectation. Type \\'exit\\' or just press ENTER to quit the program. Have fun.')\n\n    while True:\n        question = input(self.SENTENCES_PREFIX[0])\n        if question == '' or question == 'exit':\n            break\n\n        questionSeq = []  # Will be contain the question as seen by the encoder\n        answer = self.singlePredict(question, questionSeq)\n        if not answer:\n            print('Warning: sentence too long, sorry. Maybe try a simpler sentence.')\n            continue  # Back to the beginning, try again\n\n        print('{}{}'.format(self.SENTENCES_PREFIX[1], self.textData.sequence2str(answer, clean=True)))\n\n        if self.args.verbose:\n            print(self.textData.batchSeq2str(questionSeq, clean=True, reverse=True))\n            print(self.textData.sequence2str(answer))\n\n        print()\n\ndef singlePredict(self, question, questionSeq=None):\n    \"\"\" Predict the sentence\n    Args:\n        question (str): the raw input sentence\n        questionSeq (List&lt;int&gt;): output argument. If given will contain the input batch sequence\n    Return:\n        list &lt;int&gt;: the word ids corresponding to the answer\n    \"\"\"\n    # Create the input batch\n    batch = self.textData.sentence2enco(question)\n    if not batch:\n        return None\n    if questionSeq is not None:  # If the caller want to have the real input\n        questionSeq.extend(batch.encoderSeqs)\n\n    # Run the model\n    ops, feedDict = self.model.step(batch)\n    output = self.sess.run(ops[0], feedDict)  # TODO: Summarize the output too (histogram, ...)\n    answer = self.textData.deco2sentence(output)\n\n    return answer\n\ndef daemonPredict(self, sentence):\n    \"\"\" Return the answer to a given sentence (same as singlePredict() but with additional cleaning)\n    Args:\n        sentence (str): the raw input sentence\n    Return:\n        str: the human readable sentence\n    \"\"\"\n    return self.textData.sequence2str(\n        self.singlePredict(sentence),\n        clean=True\n    )\n\ndef daemonClose(self):\n    \"\"\" A utility function to close the daemon when finish\n    \"\"\"\n    print('Exiting the daemon mode...')\n    self.sess.close()\n    print('Daemon closed.')\n\ndef loadEmbedding(self, sess):\n    \"\"\" Initialize embeddings with pre-trained word2vec vectors\n    Will modify the embedding weights of the current loaded model\n    Uses the GoogleNews pre-trained values (path hardcoded)\n    \"\"\"\n\n    # Fetch embedding variables from model\n    with tf.variable_scope(\"embedding_rnn_seq2seq/rnn/embedding_wrapper\", reuse=True):\n        em_in = tf.get_variable(\"embedding\")\n    with tf.variable_scope(\"embedding_rnn_seq2seq/embedding_rnn_decoder\", reuse=True):\n        em_out = tf.get_variable(\"embedding\")\n\n    # Disable training for embeddings\n    variables = tf.get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES)\n    variables.remove(em_in)\n    variables.remove(em_out)\n\n    # If restoring a model, we can leave here\n    if self.globStep != 0:\n        return\n\n    # New model, we load the pre-trained word2vec data and initialize embeddings\n    embeddings_path = os.path.join(self.args.rootDir, 'data', 'embeddings', self.args.embeddingSource)\n    embeddings_format = os.path.splitext(embeddings_path)[1][1:]\n    print(\"Loading pre-trained word embeddings from %s \" % embeddings_path)\n    with open(embeddings_path, \"rb\") as f:\n        header = f.readline()\n        vocab_size, vector_size = map(int, header.split())\n        binary_len = np.dtype('float32').itemsize * vector_size\n        initW = np.random.uniform(-0.25,0.25,(len(self.textData.word2id), vector_size))\n        for line in tqdm(range(vocab_size)):\n            word = []\n            while True:\n                ch = f.read(1)\n                if ch == b' ':\n                    word = b''.join(word).decode('utf-8')\n                    break\n                if ch != b'\\n':\n                    word.append(ch)\n            if word in self.textData.word2id:\n                if embeddings_format == 'bin':\n                    vector = np.fromstring(f.read(binary_len), dtype='float32')\n                elif embeddings_format == 'vec':\n                    vector = np.fromstring(f.readline(), sep=' ', dtype='float32')\n                else:\n                    raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\n                initW[self.textData.word2id[word]] = vector\n            else:\n                if embeddings_format == 'bin':\n                    f.read(binary_len)\n                elif embeddings_format == 'vec':\n                    f.readline()\n                else:\n                    raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\n\n    # PCA Decomposition to reduce word2vec dimensionality\n    if self.args.embeddingSize &lt; vector_size:\n        U, s, Vt = np.linalg.svd(initW, full_matrices=False)\n        S = np.zeros((vector_size, vector_size), dtype=complex)\n        S[:vector_size, :vector_size] = np.diag(s)\n        initW = np.dot(U[:, :self.args.embeddingSize], S[:self.args.embeddingSize, :self.args.embeddingSize])\n\n    # Initialize input and output embeddings\n    sess.run(em_in.assign(initW))\n    sess.run(em_out.assign(initW))\n\n\ndef managePreviousModel(self, sess):\n    \"\"\" Restore or reset the model, depending of the parameters\n    If the destination directory already contains some file, it will handle the conflict as following:\n     * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\n     restart from scratch (globStep &amp; cie reinitialized)\n     * Otherwise, it will depend of the directory content. If the directory contains:\n       * No model files (only summary logs): works as a reset (restart from scratch)\n       * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\n       decide by himself what to do\n       * The right model file (eventually some other): no problem, simply resume the training\n    In any case, the directory will exist as it has been created by the summary writer\n    Args:\n        sess: The current running session\n    \"\"\"\n\n    print('WARNING: ', end='')\n\n    modelName = self._getModelName()\n\n    if os.listdir(self.modelDir):\n        if self.args.reset:\n            print('Reset: Destroying previous model at {}'.format(self.modelDir))\n        # Analysing directory content\n        elif os.path.exists(modelName):  # Restore the model\n            print('Restoring previous model from {}'.format(modelName))\n            self.saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\n        elif self._getModelList():\n            print('Conflict with previous models.')\n            raise RuntimeError('Some models are already present in \\'{}\\'. You should check them first (or re-try with the keepAll flag)'.format(self.modelDir))\n        else:  # No other model to conflict with (probably summary files)\n            print('No previous model found, but some files found at {}. Cleaning...'.format(self.modelDir))  # Warning: No confirmation asked\n            self.args.reset = True\n\n        if self.args.reset:\n            fileList = [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir)]\n            for f in fileList:\n                print('Removing {}'.format(f))\n                os.remove(f)\n\n    else:\n        print('No previous model found, starting from clean directory: {}'.format(self.modelDir))\n\ndef _saveSession(self, sess):\n    \"\"\" Save the model parameters and the variables\n    Args:\n        sess: the current session\n    \"\"\"\n    tqdm.write('Checkpoint reached: saving model (don\\'t stop the run)...')\n    self.saveModelParams()\n    model_name = self._getModelName()\n    with open(model_name, 'w') as f:  # HACK: Simulate the old model existance to avoid rewriting the file parser\n        f.write('This file is used internally by DeepQA to check the model existance. Please do not remove.\\n')\n    self.saver.save(sess, model_name)  # TODO: Put a limit size (ex: 3GB for the modelDir)\n    tqdm.write('Model saved.')\n\ndef _getModelList(self):\n    \"\"\" Return the list of the model files inside the model directory\n    \"\"\"\n    return [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir) if f.endswith(self.MODEL_EXT)]\n\ndef loadModelParams(self):\n    \"\"\" Load the some values associated with the current model, like the current globStep value\n    For now, this function does not need to be called before loading the model (no parameters restored). However,\n    the modelDir name will be initialized here so it is required to call this function before managePreviousModel(),\n    _getModelName() or _getSummaryName()\n    Warning: if you modify this function, make sure the changes mirror saveModelParams, also check if the parameters\n    should be reset in managePreviousModel\n    \"\"\"\n    # Compute the current model path\n    self.modelDir = os.path.join(self.args.rootDir, self.MODEL_DIR_BASE)\n    if self.args.modelTag:\n        self.modelDir += '-' + self.args.modelTag\n\n    # If there is a previous model, restore some parameters\n    configName = os.path.join(self.modelDir, self.CONFIG_FILENAME)\n    if not self.args.reset and not self.args.createDataset and os.path.exists(configName):\n        # Loading\n        config = configparser.ConfigParser()\n        config.read(configName)\n\n        # Check the version\n        currentVersion = config['General'].get('version')\n        if currentVersion != self.CONFIG_VERSION:\n            raise UserWarning('Present configuration version {0} does not match {1}. You can try manual changes on \\'{2}\\''.format(currentVersion, self.CONFIG_VERSION, configName))\n\n        # Restoring the the parameters\n        self.globStep = config['General'].getint('globStep')\n        self.args.watsonMode = config['General'].getboolean('watsonMode')\n        self.args.autoEncode = config['General'].getboolean('autoEncode')\n        self.args.corpus = config['General'].get('corpus')\n\n        self.args.datasetTag = config['Dataset'].get('datasetTag')\n        self.args.maxLength = config['Dataset'].getint('maxLength')  # We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength)\n        self.args.filterVocab = config['Dataset'].getint('filterVocab')\n        self.args.skipLines = config['Dataset'].getboolean('skipLines')\n        self.args.vocabularySize = config['Dataset'].getint('vocabularySize')\n\n        self.args.hiddenSize = config['Network'].getint('hiddenSize')\n        self.args.numLayers = config['Network'].getint('numLayers')\n        self.args.softmaxSamples = config['Network'].getint('softmaxSamples')\n        self.args.initEmbeddings = config['Network'].getboolean('initEmbeddings')\n        self.args.embeddingSize = config['Network'].getint('embeddingSize')\n        self.args.embeddingSource = config['Network'].get('embeddingSource')\n\n        # No restoring for training params, batch size or other non model dependent parameters\n\n        # Show the restored params\n        print()\n        print('Warning: Restoring parameters:')\n        print('globStep: {}'.format(self.globStep))\n        print('watsonMode: {}'.format(self.args.watsonMode))\n        print('autoEncode: {}'.format(self.args.autoEncode))\n        print('corpus: {}'.format(self.args.corpus))\n        print('datasetTag: {}'.format(self.args.datasetTag))\n        print('maxLength: {}'.format(self.args.maxLength))\n        print('filterVocab: {}'.format(self.args.filterVocab))\n        print('skipLines: {}'.format(self.args.skipLines))\n        print('vocabularySize: {}'.format(self.args.vocabularySize))\n        print('hiddenSize: {}'.format(self.args.hiddenSize))\n        print('numLayers: {}'.format(self.args.numLayers))\n        print('softmaxSamples: {}'.format(self.args.softmaxSamples))\n        print('initEmbeddings: {}'.format(self.args.initEmbeddings))\n        print('embeddingSize: {}'.format(self.args.embeddingSize))\n        print('embeddingSource: {}'.format(self.args.embeddingSource))\n        print()\n\n    # For now, not arbitrary  independent maxLength between encoder and decoder\n    self.args.maxLengthEnco = self.args.maxLength\n    self.args.maxLengthDeco = self.args.maxLength + 2\n\n    if self.args.watsonMode:\n        self.SENTENCES_PREFIX.reverse()\n\n\ndef saveModelParams(self):\n    \"\"\" Save the params of the model, like the current globStep value\n    Warning: if you modify this function, make sure the changes mirror loadModelParams\n    \"\"\"\n    config = configparser.ConfigParser()\n    config['General'] = {}\n    config['General']['version']  = self.CONFIG_VERSION\n    config['General']['globStep']  = str(self.globStep)\n    config['General']['watsonMode'] = str(self.args.watsonMode)\n    config['General']['autoEncode'] = str(self.args.autoEncode)\n    config['General']['corpus'] = str(self.args.corpus)\n\n    config['Dataset'] = {}\n    config['Dataset']['datasetTag'] = str(self.args.datasetTag)\n    config['Dataset']['maxLength'] = str(self.args.maxLength)\n    config['Dataset']['filterVocab'] = str(self.args.filterVocab)\n    config['Dataset']['skipLines'] = str(self.args.skipLines)\n    config['Dataset']['vocabularySize'] = str(self.args.vocabularySize)\n\n    config['Network'] = {}\n    config['Network']['hiddenSize'] = str(self.args.hiddenSize)\n    config['Network']['numLayers'] = str(self.args.numLayers)\n    config['Network']['softmaxSamples'] = str(self.args.softmaxSamples)\n    config['Network']['initEmbeddings'] = str(self.args.initEmbeddings)\n    config['Network']['embeddingSize'] = str(self.args.embeddingSize)\n    config['Network']['embeddingSource'] = str(self.args.embeddingSource)\n\n    # Keep track of the learning params (but without restoring them)\n    config['Training (won\\'t be restored)'] = {}\n    config['Training (won\\'t be restored)']['learningRate'] = str(self.args.learningRate)\n    config['Training (won\\'t be restored)']['batchSize'] = str(self.args.batchSize)\n    config['Training (won\\'t be restored)']['dropout'] = str(self.args.dropout)\n\n    with open(os.path.join(self.modelDir, self.CONFIG_FILENAME), 'w') as configFile:\n        config.write(configFile)\n\ndef _getSummaryName(self):\n    \"\"\" Parse the argument to decide were to save the summary, at the same place that the model\n    The folder could already contain logs if we restore the training, those will be merged\n    Return:\n        str: The path and name of the summary\n    \"\"\"\n    return self.modelDir\n\ndef _getModelName(self):\n    \"\"\" Parse the argument to decide were to save/load the model\n    This function is called at each checkpoint and the first time the model is load. If keepAll option is set, the\n    globStep value will be included in the name.\n    Return:\n        str: The path and name were the model need to be saved\n    \"\"\"\n    modelName = os.path.join(self.modelDir, self.MODEL_NAME_BASE)\n    if self.args.keepAll:  # We do not erase the previously saved model by including the current step on the name\n        modelName += '-' + str(self.globStep)\n    return modelName + self.MODEL_EXT\n\ndef getDevice(self):\n    \"\"\" Parse the argument to decide on which device run the model\n    Return:\n        str: The name of the device on which run the program\n    \"\"\"\n    if self.args.device == 'cpu':\n        return '/cpu:0'\n    elif self.args.device == 'gpu':\n        return '/gpu:0'\n    elif self.args.device is None:  # No specified device (default)\n        return None\n    else:\n        print('Warning: Error in the device name: {}, use the default device'.format(self.args.device))\n        return None\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nchat = Chatbot()<br>\nchat.main()</p>", "body_text": "hello ciasq iam using 1.0.1 tensorflow  flow while running the below code iam getting error\nCopyright 2015 Conchylicultor. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================\n\"\"\"\nMain script. See README.md for more information\nUse python 3\n\"\"\"\nimport argparse  # Command line parsing\nimport configparser  # Saving the models parameters\nimport datetime  # Chronometer\nimport os  # Files management\nimport tensorflow as tf\nimport numpy as np\nimport math\nfrom tqdm import tqdm  # Progress bar\nfrom tensorflow.python import debug as tf_debug\nfrom chatbot.textdata import TextData\nfrom chatbot.model import Model\nclass Chatbot:\n\"\"\"\nMain class which launch the training or testing mode\n\"\"\"\nclass TestMode:\n    \"\"\" Simple structure representing the different testing modes\n    \"\"\"\n    ALL = 'all'\n    INTERACTIVE = 'interactive'  # The user can write his own questions\n    DAEMON = 'daemon'  # The chatbot runs on background and can regularly be called to predict something\n\ndef __init__(self):\n    \"\"\"\n    \"\"\"\n    # Model/dataset parameters\n    self.args = None\n\n    # Task specific object\n    self.textData = None  # Dataset\n    self.model = None  # Sequence to sequence model\n\n    # Tensorflow utilities for convenience saving/logging\n    self.writer = None\n    self.saver = None\n    self.modelDir = ''  # Where the model is saved\n    self.globStep = 0  # Represent the number of iteration for the current model\n\n    # TensorFlow main session (we keep track for the daemon)\n    self.sess = None\n\n    # Filename and directories constants\n    self.MODEL_DIR_BASE = 'save/model'\n    self.MODEL_NAME_BASE = 'model'\n    self.MODEL_EXT = '.ckpt'\n    self.CONFIG_FILENAME = 'params.ini'\n    self.CONFIG_VERSION = '0.5'\n    self.TEST_IN_NAME = 'data/test/samples.txt'\n    self.TEST_OUT_SUFFIX = '_predictions.txt'\n    self.SENTENCES_PREFIX = ['Q: ', 'A: ']\n\n@staticmethod\ndef parseArgs(args):\n    \"\"\"\n    Parse the arguments from the given command line\n    Args:\n        args (list<str>): List of arguments to parse. If None, the default sys.argv will be parsed\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n\n    # Global options\n    globalArgs = parser.add_argument_group('Global options')\n    globalArgs.add_argument('--test',\n                            nargs='?',\n                            choices=[Chatbot.TestMode.ALL, Chatbot.TestMode.INTERACTIVE, Chatbot.TestMode.DAEMON],\n                            const=Chatbot.TestMode.ALL, default=None,\n                            help='if present, launch the program try to answer all sentences from data/test/ with'\n                                 ' the defined model(s), in interactive mode, the user can wrote his own sentences,'\n                                 ' use daemon mode to integrate the chatbot in another program')\n    globalArgs.add_argument('--createDataset', action='store_true', help='if present, the program will only generate the dataset from the corpus (no training/testing)')\n    globalArgs.add_argument('--playDataset', type=int, nargs='?', const=10, default=None,  help='if set, the program  will randomly play some samples(can be use conjointly with createDataset if this is the only action you want to perform)')\n    globalArgs.add_argument('--reset', action='store_true', help='use this if you want to ignore the previous model present on the model directory (Warning: the model will be destroyed with all the folder content)')\n    globalArgs.add_argument('--verbose', action='store_true', help='When testing, will plot the outputs at the same time they are computed')\n    globalArgs.add_argument('--debug', action='store_true', help='run DeepQA with Tensorflow debug mode. Read TF documentation for more details on this.')\n    globalArgs.add_argument('--keepAll', action='store_true', help='If this option is set, all saved model will be kept (Warning: make sure you have enough free disk space or increase saveEvery)')  # TODO: Add an option to delimit the max size\n    globalArgs.add_argument('--modelTag', type=str, default=None, help='tag to differentiate which model to store/load')\n    globalArgs.add_argument('--rootDir', type=str, default=None, help='folder where to look for the models and data')\n    globalArgs.add_argument('--watsonMode', action='store_true', help='Inverse the questions and answer when training (the network try to guess the question)')\n    globalArgs.add_argument('--autoEncode', action='store_true', help='Randomly pick the question or the answer and use it both as input and output')\n    globalArgs.add_argument('--device', type=str, default=None, help='\\'gpu\\' or \\'cpu\\' (Warning: make sure you have enough free RAM), allow to choose on which hardware run the model')\n    globalArgs.add_argument('--seed', type=int, default=None, help='random seed for replication')\n\n    # Dataset options\n    datasetArgs = parser.add_argument_group('Dataset options')\n    datasetArgs.add_argument('--corpus', choices=TextData.corpusChoices(), default=TextData.corpusChoices()[0], help='corpus on which extract the dataset.')\n    datasetArgs.add_argument('--datasetTag', type=str, default='', help='add a tag to the dataset (file where to load the vocabulary and the precomputed samples, not the original corpus). Useful to manage multiple versions. Also used to define the file used for the lightweight format.')  # The samples are computed from the corpus if it does not exist already. There are saved in \\'data/samples/\\'\n    datasetArgs.add_argument('--ratioDataset', type=float, default=1.0, help='ratio of dataset used to avoid using the whole dataset')  # Not implemented, useless ?\n    datasetArgs.add_argument('--maxLength', type=int, default=10, help='maximum length of the sentence (for input and output), define number of maximum step of the RNN')\n    datasetArgs.add_argument('--filterVocab', type=int, default=1, help='remove rarelly used words (by default words used only once). 0 to keep all words.')\n    datasetArgs.add_argument('--skipLines', action='store_true', help='Generate training samples by only using even conversation lines as questions (and odd lines as answer). Useful to train the network on a particular person.')\n    datasetArgs.add_argument('--vocabularySize', type=int, default=40000, help='Limit the number of words in the vocabulary (0 for unlimited)')\n\n    # Network options (Warning: if modifying something here, also make the change on save/loadParams() )\n    nnArgs = parser.add_argument_group('Network options', 'architecture related option')\n    nnArgs.add_argument('--hiddenSize', type=int, default=512, help='number of hidden units in each RNN cell')\n    nnArgs.add_argument('--numLayers', type=int, default=2, help='number of rnn layers')\n    nnArgs.add_argument('--softmaxSamples', type=int, default=0, help='Number of samples in the sampled softmax loss function. A value of 0 deactivates sampled softmax')\n    nnArgs.add_argument('--initEmbeddings', action='store_true', help='if present, the program will initialize the embeddings with pre-trained word2vec vectors')\n    nnArgs.add_argument('--embeddingSize', type=int, default=64, help='embedding size of the word representation')\n    nnArgs.add_argument('--embeddingSource', type=str, default=\"GoogleNews-vectors-negative300.bin\", help='embedding file to use for the word representation')\n\n    # Training options\n    trainingArgs = parser.add_argument_group('Training options')\n    trainingArgs.add_argument('--numEpochs', type=int, default=30, help='maximum number of epochs to run')\n    trainingArgs.add_argument('--saveEvery', type=int, default=2000, help='nb of mini-batch step before creating a model checkpoint')\n    trainingArgs.add_argument('--batchSize', type=int, default=256, help='mini-batch size')\n    trainingArgs.add_argument('--learningRate', type=float, default=0.002, help='Learning rate')\n    trainingArgs.add_argument('--dropout', type=float, default=0.9, help='Dropout rate (keep probabilities)')\n\n    return parser.parse_args(args)\n\ndef main(self, args=None):\n    \"\"\"\n    Launch the training and/or the interactive mode\n    \"\"\"\n    print('Welcome to DeepQA v0.1 !')\n    print()\n    print('TensorFlow detected: v{}'.format(tf.__version__))\n\n    # General initialisation\n\n    self.args = self.parseArgs(args)\n\n    if not self.args.rootDir:\n        self.args.rootDir = os.getcwd()  # Use the current working directory\n\n    #tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\n\n    self.loadModelParams()  # Update the self.modelDir and self.globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\n\n    self.textData = TextData(self.args)\n    # TODO: Add a mode where we can force the input of the decoder // Try to visualize the predictions for\n    # each word of the vocabulary / decoder input\n    # TODO: For now, the model are trained for a specific dataset (because of the maxLength which define the\n    # vocabulary). Add a compatibility mode which allow to launch a model trained on a different vocabulary (\n    # remap the word2id/id2word variables).\n    if self.args.createDataset:\n        print('Dataset created! Thanks for using this program')\n        return  # No need to go further\n\n    # Prepare the model\n    with tf.device(self.getDevice()):\n        self.model = Model(self.args, self.textData)\n\n    # Saver/summaries\n    self.writer = tf.summary.FileWriter(self._getSummaryName())\n    self.saver = tf.train.Saver(max_to_keep=200)\n\n    # TODO: Fixed seed (WARNING: If dataset shuffling, make sure to do that after saving the\n    # dataset, otherwise, all which cames after the shuffling won't be replicable when\n    # reloading the dataset). How to restore the seed after loading ??\n    # Also fix seed for random.shuffle (does it works globally for all files ?)\n\n    # Running session\n    self.sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,  # Allows backup device for non GPU-available operations (when forcing GPU)\n        log_device_placement=False)  # Too verbose ?\n    )  # TODO: Replace all sess by self.sess (not necessary a good idea) ?\n\n    if self.args.debug:\n        self.sess = tf_debug.LocalCLIDebugWrapperSession(self.sess)\n        self.sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n\n    print('Initialize variables...')\n    self.sess.run(tf.global_variables_initializer())\n\n    # Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\n    if self.args.test != Chatbot.TestMode.ALL:\n        self.managePreviousModel(self.sess)\n\n    # Initialize embeddings with pre-trained word2vec vectors\n    if self.args.initEmbeddings:\n        self.loadEmbedding(self.sess)\n\n    if self.args.test:\n        if self.args.test == Chatbot.TestMode.INTERACTIVE:\n            self.mainTestInteractive(self.sess)\n        elif self.args.test == Chatbot.TestMode.ALL:\n            print('Start predicting...')\n            self.predictTestset(self.sess)\n            print('All predictions done')\n        elif self.args.test == Chatbot.TestMode.DAEMON:\n            print('Daemon mode, running in background...')\n        else:\n            raise RuntimeError('Unknown test mode: {}'.format(self.args.test))  # Should never happen\n    else:\n        self.mainTrain(self.sess)\n\n    if self.args.test != Chatbot.TestMode.DAEMON:\n        self.sess.close()\n        print(\"The End! Thanks for using this program\")\n\ndef mainTrain(self, sess):\n    \"\"\" Training loop\n    Args:\n        sess: The current running session\n    \"\"\"\n\n    # Specific training dependent loading\n\n    self.textData.makeLighter(self.args.ratioDataset)  # Limit the number of training samples\n\n    mergedSummaries = tf.summary.merge_all()  # Define the summary operator (Warning: Won't appear on the tensorboard graph)\n    if self.globStep == 0:  # Not restoring from previous run\n        self.writer.add_graph(sess.graph)  # First time only\n\n    # If restoring a model, restore the progression bar ? and current batch ?\n\n    print('Start training (press Ctrl+C to save and exit)...')\n\n    try:  # If the user exit while training, we still try to save the model\n        for e in range(self.args.numEpochs):\n\n            print()\n            print(\"----- Epoch {}/{} ; (lr={}) -----\".format(e+1, self.args.numEpochs, self.args.learningRate))\n\n            batches = self.textData.getBatches()\n\n            # TODO: Also update learning parameters eventually\n\n            tic = datetime.datetime.now()\n            for nextBatch in tqdm(batches, desc=\"Training\"):\n                # Training pass\n                ops, feedDict = self.model.step(nextBatch)\n                assert len(ops) == 2  # training, loss\n                _, loss, summary = sess.run(ops + (mergedSummaries,), feedDict)\n                self.writer.add_summary(summary, self.globStep)\n                self.globStep += 1\n\n                # Output training status\n                if self.globStep % 100 == 0:\n                    perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n                    tqdm.write(\"----- Step %d -- Loss %.2f -- Perplexity %.2f\" % (self.globStep, loss, perplexity))\n\n                # Checkpoint\n                if self.globStep % self.args.saveEvery == 0:\n                    self._saveSession(sess)\n\n            toc = datetime.datetime.now()\n\n            print(\"Epoch finished in {}\".format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn't really nicer\n    except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\n        print('Interruption detected, exiting the program...')\n\n    self._saveSession(sess)  # Ultimate saving before complete exit\n\ndef predictTestset(self, sess):\n    \"\"\" Try predicting the sentences from the samples.txt file.\n    The sentences are saved on the modelDir under the same name\n    Args:\n        sess: The current running session\n    \"\"\"\n\n    # Loading the file to predict\n    with open(os.path.join(self.args.rootDir, self.TEST_IN_NAME), 'r') as f:\n        lines = f.readlines()\n\n    modelList = self._getModelList()\n    if not modelList:\n        print('Warning: No model found in \\'{}\\'. Please train a model before trying to predict'.format(self.modelDir))\n        return\n\n    # Predicting for each model present in modelDir\n    for modelName in sorted(modelList):  # TODO: Natural sorting\n        print('Restoring previous model from {}'.format(modelName))\n        self.saver.restore(sess, modelName)\n        print('Testing...')\n\n        saveName = modelName[:-len(self.MODEL_EXT)] + self.TEST_OUT_SUFFIX  # We remove the model extension and add the prediction suffix\n        with open(saveName, 'w') as f:\n            nbIgnored = 0\n            for line in tqdm(lines, desc='Sentences'):\n                question = line[:-1]  # Remove the endl character\n\n                answer = self.singlePredict(question)\n                if not answer:\n                    nbIgnored += 1\n                    continue  # Back to the beginning, try again\n\n                predString = '{x[0]}{0}\\n{x[1]}{1}\\n\\n'.format(question, self.textData.sequence2str(answer, clean=True), x=self.SENTENCES_PREFIX)\n                if self.args.verbose:\n                    tqdm.write(predString)\n                f.write(predString)\n            print('Prediction finished, {}/{} sentences ignored (too long)'.format(nbIgnored, len(lines)))\n\ndef mainTestInteractive(self, sess):\n    \"\"\" Try predicting the sentences that the user will enter in the console\n    Args:\n        sess: The current running session\n    \"\"\"\n    # TODO: If verbose mode, also show similar sentences from the training set with the same words (include in mainTest also)\n    # TODO: Also show the top 10 most likely predictions for each predicted output (when verbose mode)\n    # TODO: Log the questions asked for latter re-use (merge with test/samples.txt)\n\n    print('Testing: Launch interactive mode:')\n    print('')\n    print('Welcome to the interactive mode, here you can ask to Deep Q&A the sentence you want. Don\\'t have high '\n          'expectation. Type \\'exit\\' or just press ENTER to quit the program. Have fun.')\n\n    while True:\n        question = input(self.SENTENCES_PREFIX[0])\n        if question == '' or question == 'exit':\n            break\n\n        questionSeq = []  # Will be contain the question as seen by the encoder\n        answer = self.singlePredict(question, questionSeq)\n        if not answer:\n            print('Warning: sentence too long, sorry. Maybe try a simpler sentence.')\n            continue  # Back to the beginning, try again\n\n        print('{}{}'.format(self.SENTENCES_PREFIX[1], self.textData.sequence2str(answer, clean=True)))\n\n        if self.args.verbose:\n            print(self.textData.batchSeq2str(questionSeq, clean=True, reverse=True))\n            print(self.textData.sequence2str(answer))\n\n        print()\n\ndef singlePredict(self, question, questionSeq=None):\n    \"\"\" Predict the sentence\n    Args:\n        question (str): the raw input sentence\n        questionSeq (List<int>): output argument. If given will contain the input batch sequence\n    Return:\n        list <int>: the word ids corresponding to the answer\n    \"\"\"\n    # Create the input batch\n    batch = self.textData.sentence2enco(question)\n    if not batch:\n        return None\n    if questionSeq is not None:  # If the caller want to have the real input\n        questionSeq.extend(batch.encoderSeqs)\n\n    # Run the model\n    ops, feedDict = self.model.step(batch)\n    output = self.sess.run(ops[0], feedDict)  # TODO: Summarize the output too (histogram, ...)\n    answer = self.textData.deco2sentence(output)\n\n    return answer\n\ndef daemonPredict(self, sentence):\n    \"\"\" Return the answer to a given sentence (same as singlePredict() but with additional cleaning)\n    Args:\n        sentence (str): the raw input sentence\n    Return:\n        str: the human readable sentence\n    \"\"\"\n    return self.textData.sequence2str(\n        self.singlePredict(sentence),\n        clean=True\n    )\n\ndef daemonClose(self):\n    \"\"\" A utility function to close the daemon when finish\n    \"\"\"\n    print('Exiting the daemon mode...')\n    self.sess.close()\n    print('Daemon closed.')\n\ndef loadEmbedding(self, sess):\n    \"\"\" Initialize embeddings with pre-trained word2vec vectors\n    Will modify the embedding weights of the current loaded model\n    Uses the GoogleNews pre-trained values (path hardcoded)\n    \"\"\"\n\n    # Fetch embedding variables from model\n    with tf.variable_scope(\"embedding_rnn_seq2seq/rnn/embedding_wrapper\", reuse=True):\n        em_in = tf.get_variable(\"embedding\")\n    with tf.variable_scope(\"embedding_rnn_seq2seq/embedding_rnn_decoder\", reuse=True):\n        em_out = tf.get_variable(\"embedding\")\n\n    # Disable training for embeddings\n    variables = tf.get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES)\n    variables.remove(em_in)\n    variables.remove(em_out)\n\n    # If restoring a model, we can leave here\n    if self.globStep != 0:\n        return\n\n    # New model, we load the pre-trained word2vec data and initialize embeddings\n    embeddings_path = os.path.join(self.args.rootDir, 'data', 'embeddings', self.args.embeddingSource)\n    embeddings_format = os.path.splitext(embeddings_path)[1][1:]\n    print(\"Loading pre-trained word embeddings from %s \" % embeddings_path)\n    with open(embeddings_path, \"rb\") as f:\n        header = f.readline()\n        vocab_size, vector_size = map(int, header.split())\n        binary_len = np.dtype('float32').itemsize * vector_size\n        initW = np.random.uniform(-0.25,0.25,(len(self.textData.word2id), vector_size))\n        for line in tqdm(range(vocab_size)):\n            word = []\n            while True:\n                ch = f.read(1)\n                if ch == b' ':\n                    word = b''.join(word).decode('utf-8')\n                    break\n                if ch != b'\\n':\n                    word.append(ch)\n            if word in self.textData.word2id:\n                if embeddings_format == 'bin':\n                    vector = np.fromstring(f.read(binary_len), dtype='float32')\n                elif embeddings_format == 'vec':\n                    vector = np.fromstring(f.readline(), sep=' ', dtype='float32')\n                else:\n                    raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\n                initW[self.textData.word2id[word]] = vector\n            else:\n                if embeddings_format == 'bin':\n                    f.read(binary_len)\n                elif embeddings_format == 'vec':\n                    f.readline()\n                else:\n                    raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\n\n    # PCA Decomposition to reduce word2vec dimensionality\n    if self.args.embeddingSize < vector_size:\n        U, s, Vt = np.linalg.svd(initW, full_matrices=False)\n        S = np.zeros((vector_size, vector_size), dtype=complex)\n        S[:vector_size, :vector_size] = np.diag(s)\n        initW = np.dot(U[:, :self.args.embeddingSize], S[:self.args.embeddingSize, :self.args.embeddingSize])\n\n    # Initialize input and output embeddings\n    sess.run(em_in.assign(initW))\n    sess.run(em_out.assign(initW))\n\n\ndef managePreviousModel(self, sess):\n    \"\"\" Restore or reset the model, depending of the parameters\n    If the destination directory already contains some file, it will handle the conflict as following:\n     * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\n     restart from scratch (globStep & cie reinitialized)\n     * Otherwise, it will depend of the directory content. If the directory contains:\n       * No model files (only summary logs): works as a reset (restart from scratch)\n       * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\n       decide by himself what to do\n       * The right model file (eventually some other): no problem, simply resume the training\n    In any case, the directory will exist as it has been created by the summary writer\n    Args:\n        sess: The current running session\n    \"\"\"\n\n    print('WARNING: ', end='')\n\n    modelName = self._getModelName()\n\n    if os.listdir(self.modelDir):\n        if self.args.reset:\n            print('Reset: Destroying previous model at {}'.format(self.modelDir))\n        # Analysing directory content\n        elif os.path.exists(modelName):  # Restore the model\n            print('Restoring previous model from {}'.format(modelName))\n            self.saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\n        elif self._getModelList():\n            print('Conflict with previous models.')\n            raise RuntimeError('Some models are already present in \\'{}\\'. You should check them first (or re-try with the keepAll flag)'.format(self.modelDir))\n        else:  # No other model to conflict with (probably summary files)\n            print('No previous model found, but some files found at {}. Cleaning...'.format(self.modelDir))  # Warning: No confirmation asked\n            self.args.reset = True\n\n        if self.args.reset:\n            fileList = [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir)]\n            for f in fileList:\n                print('Removing {}'.format(f))\n                os.remove(f)\n\n    else:\n        print('No previous model found, starting from clean directory: {}'.format(self.modelDir))\n\ndef _saveSession(self, sess):\n    \"\"\" Save the model parameters and the variables\n    Args:\n        sess: the current session\n    \"\"\"\n    tqdm.write('Checkpoint reached: saving model (don\\'t stop the run)...')\n    self.saveModelParams()\n    model_name = self._getModelName()\n    with open(model_name, 'w') as f:  # HACK: Simulate the old model existance to avoid rewriting the file parser\n        f.write('This file is used internally by DeepQA to check the model existance. Please do not remove.\\n')\n    self.saver.save(sess, model_name)  # TODO: Put a limit size (ex: 3GB for the modelDir)\n    tqdm.write('Model saved.')\n\ndef _getModelList(self):\n    \"\"\" Return the list of the model files inside the model directory\n    \"\"\"\n    return [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir) if f.endswith(self.MODEL_EXT)]\n\ndef loadModelParams(self):\n    \"\"\" Load the some values associated with the current model, like the current globStep value\n    For now, this function does not need to be called before loading the model (no parameters restored). However,\n    the modelDir name will be initialized here so it is required to call this function before managePreviousModel(),\n    _getModelName() or _getSummaryName()\n    Warning: if you modify this function, make sure the changes mirror saveModelParams, also check if the parameters\n    should be reset in managePreviousModel\n    \"\"\"\n    # Compute the current model path\n    self.modelDir = os.path.join(self.args.rootDir, self.MODEL_DIR_BASE)\n    if self.args.modelTag:\n        self.modelDir += '-' + self.args.modelTag\n\n    # If there is a previous model, restore some parameters\n    configName = os.path.join(self.modelDir, self.CONFIG_FILENAME)\n    if not self.args.reset and not self.args.createDataset and os.path.exists(configName):\n        # Loading\n        config = configparser.ConfigParser()\n        config.read(configName)\n\n        # Check the version\n        currentVersion = config['General'].get('version')\n        if currentVersion != self.CONFIG_VERSION:\n            raise UserWarning('Present configuration version {0} does not match {1}. You can try manual changes on \\'{2}\\''.format(currentVersion, self.CONFIG_VERSION, configName))\n\n        # Restoring the the parameters\n        self.globStep = config['General'].getint('globStep')\n        self.args.watsonMode = config['General'].getboolean('watsonMode')\n        self.args.autoEncode = config['General'].getboolean('autoEncode')\n        self.args.corpus = config['General'].get('corpus')\n\n        self.args.datasetTag = config['Dataset'].get('datasetTag')\n        self.args.maxLength = config['Dataset'].getint('maxLength')  # We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength)\n        self.args.filterVocab = config['Dataset'].getint('filterVocab')\n        self.args.skipLines = config['Dataset'].getboolean('skipLines')\n        self.args.vocabularySize = config['Dataset'].getint('vocabularySize')\n\n        self.args.hiddenSize = config['Network'].getint('hiddenSize')\n        self.args.numLayers = config['Network'].getint('numLayers')\n        self.args.softmaxSamples = config['Network'].getint('softmaxSamples')\n        self.args.initEmbeddings = config['Network'].getboolean('initEmbeddings')\n        self.args.embeddingSize = config['Network'].getint('embeddingSize')\n        self.args.embeddingSource = config['Network'].get('embeddingSource')\n\n        # No restoring for training params, batch size or other non model dependent parameters\n\n        # Show the restored params\n        print()\n        print('Warning: Restoring parameters:')\n        print('globStep: {}'.format(self.globStep))\n        print('watsonMode: {}'.format(self.args.watsonMode))\n        print('autoEncode: {}'.format(self.args.autoEncode))\n        print('corpus: {}'.format(self.args.corpus))\n        print('datasetTag: {}'.format(self.args.datasetTag))\n        print('maxLength: {}'.format(self.args.maxLength))\n        print('filterVocab: {}'.format(self.args.filterVocab))\n        print('skipLines: {}'.format(self.args.skipLines))\n        print('vocabularySize: {}'.format(self.args.vocabularySize))\n        print('hiddenSize: {}'.format(self.args.hiddenSize))\n        print('numLayers: {}'.format(self.args.numLayers))\n        print('softmaxSamples: {}'.format(self.args.softmaxSamples))\n        print('initEmbeddings: {}'.format(self.args.initEmbeddings))\n        print('embeddingSize: {}'.format(self.args.embeddingSize))\n        print('embeddingSource: {}'.format(self.args.embeddingSource))\n        print()\n\n    # For now, not arbitrary  independent maxLength between encoder and decoder\n    self.args.maxLengthEnco = self.args.maxLength\n    self.args.maxLengthDeco = self.args.maxLength + 2\n\n    if self.args.watsonMode:\n        self.SENTENCES_PREFIX.reverse()\n\n\ndef saveModelParams(self):\n    \"\"\" Save the params of the model, like the current globStep value\n    Warning: if you modify this function, make sure the changes mirror loadModelParams\n    \"\"\"\n    config = configparser.ConfigParser()\n    config['General'] = {}\n    config['General']['version']  = self.CONFIG_VERSION\n    config['General']['globStep']  = str(self.globStep)\n    config['General']['watsonMode'] = str(self.args.watsonMode)\n    config['General']['autoEncode'] = str(self.args.autoEncode)\n    config['General']['corpus'] = str(self.args.corpus)\n\n    config['Dataset'] = {}\n    config['Dataset']['datasetTag'] = str(self.args.datasetTag)\n    config['Dataset']['maxLength'] = str(self.args.maxLength)\n    config['Dataset']['filterVocab'] = str(self.args.filterVocab)\n    config['Dataset']['skipLines'] = str(self.args.skipLines)\n    config['Dataset']['vocabularySize'] = str(self.args.vocabularySize)\n\n    config['Network'] = {}\n    config['Network']['hiddenSize'] = str(self.args.hiddenSize)\n    config['Network']['numLayers'] = str(self.args.numLayers)\n    config['Network']['softmaxSamples'] = str(self.args.softmaxSamples)\n    config['Network']['initEmbeddings'] = str(self.args.initEmbeddings)\n    config['Network']['embeddingSize'] = str(self.args.embeddingSize)\n    config['Network']['embeddingSource'] = str(self.args.embeddingSource)\n\n    # Keep track of the learning params (but without restoring them)\n    config['Training (won\\'t be restored)'] = {}\n    config['Training (won\\'t be restored)']['learningRate'] = str(self.args.learningRate)\n    config['Training (won\\'t be restored)']['batchSize'] = str(self.args.batchSize)\n    config['Training (won\\'t be restored)']['dropout'] = str(self.args.dropout)\n\n    with open(os.path.join(self.modelDir, self.CONFIG_FILENAME), 'w') as configFile:\n        config.write(configFile)\n\ndef _getSummaryName(self):\n    \"\"\" Parse the argument to decide were to save the summary, at the same place that the model\n    The folder could already contain logs if we restore the training, those will be merged\n    Return:\n        str: The path and name of the summary\n    \"\"\"\n    return self.modelDir\n\ndef _getModelName(self):\n    \"\"\" Parse the argument to decide were to save/load the model\n    This function is called at each checkpoint and the first time the model is load. If keepAll option is set, the\n    globStep value will be included in the name.\n    Return:\n        str: The path and name were the model need to be saved\n    \"\"\"\n    modelName = os.path.join(self.modelDir, self.MODEL_NAME_BASE)\n    if self.args.keepAll:  # We do not erase the previously saved model by including the current step on the name\n        modelName += '-' + str(self.globStep)\n    return modelName + self.MODEL_EXT\n\ndef getDevice(self):\n    \"\"\" Parse the argument to decide on which device run the model\n    Return:\n        str: The name of the device on which run the program\n    \"\"\"\n    if self.args.device == 'cpu':\n        return '/cpu:0'\n    elif self.args.device == 'gpu':\n        return '/gpu:0'\n    elif self.args.device is None:  # No specified device (default)\n        return None\n    else:\n        print('Warning: Error in the device name: {}, use the default device'.format(self.args.device))\n        return None\n\nif name == 'main':\nchat = Chatbot()\nchat.main()", "body": "hello ciasq iam using 1.0.1 tensorflow  flow while running the below code iam getting error\r\n# Copyright 2015 Conchylicultor. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\n\"\"\"\r\nMain script. See README.md for more information\r\n\r\nUse python 3\r\n\"\"\"\r\n\r\nimport argparse  # Command line parsing\r\nimport configparser  # Saving the models parameters\r\nimport datetime  # Chronometer\r\nimport os  # Files management\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport math\r\n\r\nfrom tqdm import tqdm  # Progress bar\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nfrom chatbot.textdata import TextData\r\nfrom chatbot.model import Model\r\n\r\n\r\nclass Chatbot:\r\n    \"\"\"\r\n    Main class which launch the training or testing mode\r\n    \"\"\"\r\n\r\n    class TestMode:\r\n        \"\"\" Simple structure representing the different testing modes\r\n        \"\"\"\r\n        ALL = 'all'\r\n        INTERACTIVE = 'interactive'  # The user can write his own questions\r\n        DAEMON = 'daemon'  # The chatbot runs on background and can regularly be called to predict something\r\n\r\n    def __init__(self):\r\n        \"\"\"\r\n        \"\"\"\r\n        # Model/dataset parameters\r\n        self.args = None\r\n\r\n        # Task specific object\r\n        self.textData = None  # Dataset\r\n        self.model = None  # Sequence to sequence model\r\n\r\n        # Tensorflow utilities for convenience saving/logging\r\n        self.writer = None\r\n        self.saver = None\r\n        self.modelDir = ''  # Where the model is saved\r\n        self.globStep = 0  # Represent the number of iteration for the current model\r\n\r\n        # TensorFlow main session (we keep track for the daemon)\r\n        self.sess = None\r\n\r\n        # Filename and directories constants\r\n        self.MODEL_DIR_BASE = 'save/model'\r\n        self.MODEL_NAME_BASE = 'model'\r\n        self.MODEL_EXT = '.ckpt'\r\n        self.CONFIG_FILENAME = 'params.ini'\r\n        self.CONFIG_VERSION = '0.5'\r\n        self.TEST_IN_NAME = 'data/test/samples.txt'\r\n        self.TEST_OUT_SUFFIX = '_predictions.txt'\r\n        self.SENTENCES_PREFIX = ['Q: ', 'A: ']\r\n\r\n    @staticmethod\r\n    def parseArgs(args):\r\n        \"\"\"\r\n        Parse the arguments from the given command line\r\n        Args:\r\n            args (list<str>): List of arguments to parse. If None, the default sys.argv will be parsed\r\n        \"\"\"\r\n\r\n        parser = argparse.ArgumentParser()\r\n\r\n        # Global options\r\n        globalArgs = parser.add_argument_group('Global options')\r\n        globalArgs.add_argument('--test',\r\n                                nargs='?',\r\n                                choices=[Chatbot.TestMode.ALL, Chatbot.TestMode.INTERACTIVE, Chatbot.TestMode.DAEMON],\r\n                                const=Chatbot.TestMode.ALL, default=None,\r\n                                help='if present, launch the program try to answer all sentences from data/test/ with'\r\n                                     ' the defined model(s), in interactive mode, the user can wrote his own sentences,'\r\n                                     ' use daemon mode to integrate the chatbot in another program')\r\n        globalArgs.add_argument('--createDataset', action='store_true', help='if present, the program will only generate the dataset from the corpus (no training/testing)')\r\n        globalArgs.add_argument('--playDataset', type=int, nargs='?', const=10, default=None,  help='if set, the program  will randomly play some samples(can be use conjointly with createDataset if this is the only action you want to perform)')\r\n        globalArgs.add_argument('--reset', action='store_true', help='use this if you want to ignore the previous model present on the model directory (Warning: the model will be destroyed with all the folder content)')\r\n        globalArgs.add_argument('--verbose', action='store_true', help='When testing, will plot the outputs at the same time they are computed')\r\n        globalArgs.add_argument('--debug', action='store_true', help='run DeepQA with Tensorflow debug mode. Read TF documentation for more details on this.')\r\n        globalArgs.add_argument('--keepAll', action='store_true', help='If this option is set, all saved model will be kept (Warning: make sure you have enough free disk space or increase saveEvery)')  # TODO: Add an option to delimit the max size\r\n        globalArgs.add_argument('--modelTag', type=str, default=None, help='tag to differentiate which model to store/load')\r\n        globalArgs.add_argument('--rootDir', type=str, default=None, help='folder where to look for the models and data')\r\n        globalArgs.add_argument('--watsonMode', action='store_true', help='Inverse the questions and answer when training (the network try to guess the question)')\r\n        globalArgs.add_argument('--autoEncode', action='store_true', help='Randomly pick the question or the answer and use it both as input and output')\r\n        globalArgs.add_argument('--device', type=str, default=None, help='\\'gpu\\' or \\'cpu\\' (Warning: make sure you have enough free RAM), allow to choose on which hardware run the model')\r\n        globalArgs.add_argument('--seed', type=int, default=None, help='random seed for replication')\r\n\r\n        # Dataset options\r\n        datasetArgs = parser.add_argument_group('Dataset options')\r\n        datasetArgs.add_argument('--corpus', choices=TextData.corpusChoices(), default=TextData.corpusChoices()[0], help='corpus on which extract the dataset.')\r\n        datasetArgs.add_argument('--datasetTag', type=str, default='', help='add a tag to the dataset (file where to load the vocabulary and the precomputed samples, not the original corpus). Useful to manage multiple versions. Also used to define the file used for the lightweight format.')  # The samples are computed from the corpus if it does not exist already. There are saved in \\'data/samples/\\'\r\n        datasetArgs.add_argument('--ratioDataset', type=float, default=1.0, help='ratio of dataset used to avoid using the whole dataset')  # Not implemented, useless ?\r\n        datasetArgs.add_argument('--maxLength', type=int, default=10, help='maximum length of the sentence (for input and output), define number of maximum step of the RNN')\r\n        datasetArgs.add_argument('--filterVocab', type=int, default=1, help='remove rarelly used words (by default words used only once). 0 to keep all words.')\r\n        datasetArgs.add_argument('--skipLines', action='store_true', help='Generate training samples by only using even conversation lines as questions (and odd lines as answer). Useful to train the network on a particular person.')\r\n        datasetArgs.add_argument('--vocabularySize', type=int, default=40000, help='Limit the number of words in the vocabulary (0 for unlimited)')\r\n\r\n        # Network options (Warning: if modifying something here, also make the change on save/loadParams() )\r\n        nnArgs = parser.add_argument_group('Network options', 'architecture related option')\r\n        nnArgs.add_argument('--hiddenSize', type=int, default=512, help='number of hidden units in each RNN cell')\r\n        nnArgs.add_argument('--numLayers', type=int, default=2, help='number of rnn layers')\r\n        nnArgs.add_argument('--softmaxSamples', type=int, default=0, help='Number of samples in the sampled softmax loss function. A value of 0 deactivates sampled softmax')\r\n        nnArgs.add_argument('--initEmbeddings', action='store_true', help='if present, the program will initialize the embeddings with pre-trained word2vec vectors')\r\n        nnArgs.add_argument('--embeddingSize', type=int, default=64, help='embedding size of the word representation')\r\n        nnArgs.add_argument('--embeddingSource', type=str, default=\"GoogleNews-vectors-negative300.bin\", help='embedding file to use for the word representation')\r\n\r\n        # Training options\r\n        trainingArgs = parser.add_argument_group('Training options')\r\n        trainingArgs.add_argument('--numEpochs', type=int, default=30, help='maximum number of epochs to run')\r\n        trainingArgs.add_argument('--saveEvery', type=int, default=2000, help='nb of mini-batch step before creating a model checkpoint')\r\n        trainingArgs.add_argument('--batchSize', type=int, default=256, help='mini-batch size')\r\n        trainingArgs.add_argument('--learningRate', type=float, default=0.002, help='Learning rate')\r\n        trainingArgs.add_argument('--dropout', type=float, default=0.9, help='Dropout rate (keep probabilities)')\r\n\r\n        return parser.parse_args(args)\r\n\r\n    def main(self, args=None):\r\n        \"\"\"\r\n        Launch the training and/or the interactive mode\r\n        \"\"\"\r\n        print('Welcome to DeepQA v0.1 !')\r\n        print()\r\n        print('TensorFlow detected: v{}'.format(tf.__version__))\r\n\r\n        # General initialisation\r\n\r\n        self.args = self.parseArgs(args)\r\n\r\n        if not self.args.rootDir:\r\n            self.args.rootDir = os.getcwd()  # Use the current working directory\r\n\r\n        #tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\r\n\r\n        self.loadModelParams()  # Update the self.modelDir and self.globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\r\n\r\n        self.textData = TextData(self.args)\r\n        # TODO: Add a mode where we can force the input of the decoder // Try to visualize the predictions for\r\n        # each word of the vocabulary / decoder input\r\n        # TODO: For now, the model are trained for a specific dataset (because of the maxLength which define the\r\n        # vocabulary). Add a compatibility mode which allow to launch a model trained on a different vocabulary (\r\n        # remap the word2id/id2word variables).\r\n        if self.args.createDataset:\r\n            print('Dataset created! Thanks for using this program')\r\n            return  # No need to go further\r\n\r\n        # Prepare the model\r\n        with tf.device(self.getDevice()):\r\n            self.model = Model(self.args, self.textData)\r\n\r\n        # Saver/summaries\r\n        self.writer = tf.summary.FileWriter(self._getSummaryName())\r\n        self.saver = tf.train.Saver(max_to_keep=200)\r\n\r\n        # TODO: Fixed seed (WARNING: If dataset shuffling, make sure to do that after saving the\r\n        # dataset, otherwise, all which cames after the shuffling won't be replicable when\r\n        # reloading the dataset). How to restore the seed after loading ??\r\n        # Also fix seed for random.shuffle (does it works globally for all files ?)\r\n\r\n        # Running session\r\n        self.sess = tf.Session(config=tf.ConfigProto(\r\n            allow_soft_placement=True,  # Allows backup device for non GPU-available operations (when forcing GPU)\r\n            log_device_placement=False)  # Too verbose ?\r\n        )  # TODO: Replace all sess by self.sess (not necessary a good idea) ?\r\n\r\n        if self.args.debug:\r\n            self.sess = tf_debug.LocalCLIDebugWrapperSession(self.sess)\r\n            self.sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\n\r\n        print('Initialize variables...')\r\n        self.sess.run(tf.global_variables_initializer())\r\n\r\n        # Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\r\n        if self.args.test != Chatbot.TestMode.ALL:\r\n            self.managePreviousModel(self.sess)\r\n\r\n        # Initialize embeddings with pre-trained word2vec vectors\r\n        if self.args.initEmbeddings:\r\n            self.loadEmbedding(self.sess)\r\n\r\n        if self.args.test:\r\n            if self.args.test == Chatbot.TestMode.INTERACTIVE:\r\n                self.mainTestInteractive(self.sess)\r\n            elif self.args.test == Chatbot.TestMode.ALL:\r\n                print('Start predicting...')\r\n                self.predictTestset(self.sess)\r\n                print('All predictions done')\r\n            elif self.args.test == Chatbot.TestMode.DAEMON:\r\n                print('Daemon mode, running in background...')\r\n            else:\r\n                raise RuntimeError('Unknown test mode: {}'.format(self.args.test))  # Should never happen\r\n        else:\r\n            self.mainTrain(self.sess)\r\n\r\n        if self.args.test != Chatbot.TestMode.DAEMON:\r\n            self.sess.close()\r\n            print(\"The End! Thanks for using this program\")\r\n\r\n    def mainTrain(self, sess):\r\n        \"\"\" Training loop\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n\r\n        # Specific training dependent loading\r\n\r\n        self.textData.makeLighter(self.args.ratioDataset)  # Limit the number of training samples\r\n\r\n        mergedSummaries = tf.summary.merge_all()  # Define the summary operator (Warning: Won't appear on the tensorboard graph)\r\n        if self.globStep == 0:  # Not restoring from previous run\r\n            self.writer.add_graph(sess.graph)  # First time only\r\n\r\n        # If restoring a model, restore the progression bar ? and current batch ?\r\n\r\n        print('Start training (press Ctrl+C to save and exit)...')\r\n\r\n        try:  # If the user exit while training, we still try to save the model\r\n            for e in range(self.args.numEpochs):\r\n\r\n                print()\r\n                print(\"----- Epoch {}/{} ; (lr={}) -----\".format(e+1, self.args.numEpochs, self.args.learningRate))\r\n\r\n                batches = self.textData.getBatches()\r\n\r\n                # TODO: Also update learning parameters eventually\r\n\r\n                tic = datetime.datetime.now()\r\n                for nextBatch in tqdm(batches, desc=\"Training\"):\r\n                    # Training pass\r\n                    ops, feedDict = self.model.step(nextBatch)\r\n                    assert len(ops) == 2  # training, loss\r\n                    _, loss, summary = sess.run(ops + (mergedSummaries,), feedDict)\r\n                    self.writer.add_summary(summary, self.globStep)\r\n                    self.globStep += 1\r\n\r\n                    # Output training status\r\n                    if self.globStep % 100 == 0:\r\n                        perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\r\n                        tqdm.write(\"----- Step %d -- Loss %.2f -- Perplexity %.2f\" % (self.globStep, loss, perplexity))\r\n\r\n                    # Checkpoint\r\n                    if self.globStep % self.args.saveEvery == 0:\r\n                        self._saveSession(sess)\r\n\r\n                toc = datetime.datetime.now()\r\n\r\n                print(\"Epoch finished in {}\".format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn't really nicer\r\n        except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\r\n            print('Interruption detected, exiting the program...')\r\n\r\n        self._saveSession(sess)  # Ultimate saving before complete exit\r\n\r\n    def predictTestset(self, sess):\r\n        \"\"\" Try predicting the sentences from the samples.txt file.\r\n        The sentences are saved on the modelDir under the same name\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n\r\n        # Loading the file to predict\r\n        with open(os.path.join(self.args.rootDir, self.TEST_IN_NAME), 'r') as f:\r\n            lines = f.readlines()\r\n\r\n        modelList = self._getModelList()\r\n        if not modelList:\r\n            print('Warning: No model found in \\'{}\\'. Please train a model before trying to predict'.format(self.modelDir))\r\n            return\r\n\r\n        # Predicting for each model present in modelDir\r\n        for modelName in sorted(modelList):  # TODO: Natural sorting\r\n            print('Restoring previous model from {}'.format(modelName))\r\n            self.saver.restore(sess, modelName)\r\n            print('Testing...')\r\n\r\n            saveName = modelName[:-len(self.MODEL_EXT)] + self.TEST_OUT_SUFFIX  # We remove the model extension and add the prediction suffix\r\n            with open(saveName, 'w') as f:\r\n                nbIgnored = 0\r\n                for line in tqdm(lines, desc='Sentences'):\r\n                    question = line[:-1]  # Remove the endl character\r\n\r\n                    answer = self.singlePredict(question)\r\n                    if not answer:\r\n                        nbIgnored += 1\r\n                        continue  # Back to the beginning, try again\r\n\r\n                    predString = '{x[0]}{0}\\n{x[1]}{1}\\n\\n'.format(question, self.textData.sequence2str(answer, clean=True), x=self.SENTENCES_PREFIX)\r\n                    if self.args.verbose:\r\n                        tqdm.write(predString)\r\n                    f.write(predString)\r\n                print('Prediction finished, {}/{} sentences ignored (too long)'.format(nbIgnored, len(lines)))\r\n\r\n    def mainTestInteractive(self, sess):\r\n        \"\"\" Try predicting the sentences that the user will enter in the console\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n        # TODO: If verbose mode, also show similar sentences from the training set with the same words (include in mainTest also)\r\n        # TODO: Also show the top 10 most likely predictions for each predicted output (when verbose mode)\r\n        # TODO: Log the questions asked for latter re-use (merge with test/samples.txt)\r\n\r\n        print('Testing: Launch interactive mode:')\r\n        print('')\r\n        print('Welcome to the interactive mode, here you can ask to Deep Q&A the sentence you want. Don\\'t have high '\r\n              'expectation. Type \\'exit\\' or just press ENTER to quit the program. Have fun.')\r\n\r\n        while True:\r\n            question = input(self.SENTENCES_PREFIX[0])\r\n            if question == '' or question == 'exit':\r\n                break\r\n\r\n            questionSeq = []  # Will be contain the question as seen by the encoder\r\n            answer = self.singlePredict(question, questionSeq)\r\n            if not answer:\r\n                print('Warning: sentence too long, sorry. Maybe try a simpler sentence.')\r\n                continue  # Back to the beginning, try again\r\n\r\n            print('{}{}'.format(self.SENTENCES_PREFIX[1], self.textData.sequence2str(answer, clean=True)))\r\n\r\n            if self.args.verbose:\r\n                print(self.textData.batchSeq2str(questionSeq, clean=True, reverse=True))\r\n                print(self.textData.sequence2str(answer))\r\n\r\n            print()\r\n\r\n    def singlePredict(self, question, questionSeq=None):\r\n        \"\"\" Predict the sentence\r\n        Args:\r\n            question (str): the raw input sentence\r\n            questionSeq (List<int>): output argument. If given will contain the input batch sequence\r\n        Return:\r\n            list <int>: the word ids corresponding to the answer\r\n        \"\"\"\r\n        # Create the input batch\r\n        batch = self.textData.sentence2enco(question)\r\n        if not batch:\r\n            return None\r\n        if questionSeq is not None:  # If the caller want to have the real input\r\n            questionSeq.extend(batch.encoderSeqs)\r\n\r\n        # Run the model\r\n        ops, feedDict = self.model.step(batch)\r\n        output = self.sess.run(ops[0], feedDict)  # TODO: Summarize the output too (histogram, ...)\r\n        answer = self.textData.deco2sentence(output)\r\n\r\n        return answer\r\n\r\n    def daemonPredict(self, sentence):\r\n        \"\"\" Return the answer to a given sentence (same as singlePredict() but with additional cleaning)\r\n        Args:\r\n            sentence (str): the raw input sentence\r\n        Return:\r\n            str: the human readable sentence\r\n        \"\"\"\r\n        return self.textData.sequence2str(\r\n            self.singlePredict(sentence),\r\n            clean=True\r\n        )\r\n\r\n    def daemonClose(self):\r\n        \"\"\" A utility function to close the daemon when finish\r\n        \"\"\"\r\n        print('Exiting the daemon mode...')\r\n        self.sess.close()\r\n        print('Daemon closed.')\r\n\r\n    def loadEmbedding(self, sess):\r\n        \"\"\" Initialize embeddings with pre-trained word2vec vectors\r\n        Will modify the embedding weights of the current loaded model\r\n        Uses the GoogleNews pre-trained values (path hardcoded)\r\n        \"\"\"\r\n\r\n        # Fetch embedding variables from model\r\n        with tf.variable_scope(\"embedding_rnn_seq2seq/rnn/embedding_wrapper\", reuse=True):\r\n            em_in = tf.get_variable(\"embedding\")\r\n        with tf.variable_scope(\"embedding_rnn_seq2seq/embedding_rnn_decoder\", reuse=True):\r\n            em_out = tf.get_variable(\"embedding\")\r\n\r\n        # Disable training for embeddings\r\n        variables = tf.get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n        variables.remove(em_in)\r\n        variables.remove(em_out)\r\n\r\n        # If restoring a model, we can leave here\r\n        if self.globStep != 0:\r\n            return\r\n\r\n        # New model, we load the pre-trained word2vec data and initialize embeddings\r\n        embeddings_path = os.path.join(self.args.rootDir, 'data', 'embeddings', self.args.embeddingSource)\r\n        embeddings_format = os.path.splitext(embeddings_path)[1][1:]\r\n        print(\"Loading pre-trained word embeddings from %s \" % embeddings_path)\r\n        with open(embeddings_path, \"rb\") as f:\r\n            header = f.readline()\r\n            vocab_size, vector_size = map(int, header.split())\r\n            binary_len = np.dtype('float32').itemsize * vector_size\r\n            initW = np.random.uniform(-0.25,0.25,(len(self.textData.word2id), vector_size))\r\n            for line in tqdm(range(vocab_size)):\r\n                word = []\r\n                while True:\r\n                    ch = f.read(1)\r\n                    if ch == b' ':\r\n                        word = b''.join(word).decode('utf-8')\r\n                        break\r\n                    if ch != b'\\n':\r\n                        word.append(ch)\r\n                if word in self.textData.word2id:\r\n                    if embeddings_format == 'bin':\r\n                        vector = np.fromstring(f.read(binary_len), dtype='float32')\r\n                    elif embeddings_format == 'vec':\r\n                        vector = np.fromstring(f.readline(), sep=' ', dtype='float32')\r\n                    else:\r\n                        raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\r\n                    initW[self.textData.word2id[word]] = vector\r\n                else:\r\n                    if embeddings_format == 'bin':\r\n                        f.read(binary_len)\r\n                    elif embeddings_format == 'vec':\r\n                        f.readline()\r\n                    else:\r\n                        raise Exception(\"Unkown format for embeddings: %s \" % embeddings_format)\r\n\r\n        # PCA Decomposition to reduce word2vec dimensionality\r\n        if self.args.embeddingSize < vector_size:\r\n            U, s, Vt = np.linalg.svd(initW, full_matrices=False)\r\n            S = np.zeros((vector_size, vector_size), dtype=complex)\r\n            S[:vector_size, :vector_size] = np.diag(s)\r\n            initW = np.dot(U[:, :self.args.embeddingSize], S[:self.args.embeddingSize, :self.args.embeddingSize])\r\n\r\n        # Initialize input and output embeddings\r\n        sess.run(em_in.assign(initW))\r\n        sess.run(em_out.assign(initW))\r\n\r\n\r\n    def managePreviousModel(self, sess):\r\n        \"\"\" Restore or reset the model, depending of the parameters\r\n        If the destination directory already contains some file, it will handle the conflict as following:\r\n         * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\r\n         restart from scratch (globStep & cie reinitialized)\r\n         * Otherwise, it will depend of the directory content. If the directory contains:\r\n           * No model files (only summary logs): works as a reset (restart from scratch)\r\n           * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\r\n           decide by himself what to do\r\n           * The right model file (eventually some other): no problem, simply resume the training\r\n        In any case, the directory will exist as it has been created by the summary writer\r\n        Args:\r\n            sess: The current running session\r\n        \"\"\"\r\n\r\n        print('WARNING: ', end='')\r\n\r\n        modelName = self._getModelName()\r\n\r\n        if os.listdir(self.modelDir):\r\n            if self.args.reset:\r\n                print('Reset: Destroying previous model at {}'.format(self.modelDir))\r\n            # Analysing directory content\r\n            elif os.path.exists(modelName):  # Restore the model\r\n                print('Restoring previous model from {}'.format(modelName))\r\n                self.saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\r\n            elif self._getModelList():\r\n                print('Conflict with previous models.')\r\n                raise RuntimeError('Some models are already present in \\'{}\\'. You should check them first (or re-try with the keepAll flag)'.format(self.modelDir))\r\n            else:  # No other model to conflict with (probably summary files)\r\n                print('No previous model found, but some files found at {}. Cleaning...'.format(self.modelDir))  # Warning: No confirmation asked\r\n                self.args.reset = True\r\n\r\n            if self.args.reset:\r\n                fileList = [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir)]\r\n                for f in fileList:\r\n                    print('Removing {}'.format(f))\r\n                    os.remove(f)\r\n\r\n        else:\r\n            print('No previous model found, starting from clean directory: {}'.format(self.modelDir))\r\n\r\n    def _saveSession(self, sess):\r\n        \"\"\" Save the model parameters and the variables\r\n        Args:\r\n            sess: the current session\r\n        \"\"\"\r\n        tqdm.write('Checkpoint reached: saving model (don\\'t stop the run)...')\r\n        self.saveModelParams()\r\n        model_name = self._getModelName()\r\n        with open(model_name, 'w') as f:  # HACK: Simulate the old model existance to avoid rewriting the file parser\r\n            f.write('This file is used internally by DeepQA to check the model existance. Please do not remove.\\n')\r\n        self.saver.save(sess, model_name)  # TODO: Put a limit size (ex: 3GB for the modelDir)\r\n        tqdm.write('Model saved.')\r\n\r\n    def _getModelList(self):\r\n        \"\"\" Return the list of the model files inside the model directory\r\n        \"\"\"\r\n        return [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir) if f.endswith(self.MODEL_EXT)]\r\n\r\n    def loadModelParams(self):\r\n        \"\"\" Load the some values associated with the current model, like the current globStep value\r\n        For now, this function does not need to be called before loading the model (no parameters restored). However,\r\n        the modelDir name will be initialized here so it is required to call this function before managePreviousModel(),\r\n        _getModelName() or _getSummaryName()\r\n        Warning: if you modify this function, make sure the changes mirror saveModelParams, also check if the parameters\r\n        should be reset in managePreviousModel\r\n        \"\"\"\r\n        # Compute the current model path\r\n        self.modelDir = os.path.join(self.args.rootDir, self.MODEL_DIR_BASE)\r\n        if self.args.modelTag:\r\n            self.modelDir += '-' + self.args.modelTag\r\n\r\n        # If there is a previous model, restore some parameters\r\n        configName = os.path.join(self.modelDir, self.CONFIG_FILENAME)\r\n        if not self.args.reset and not self.args.createDataset and os.path.exists(configName):\r\n            # Loading\r\n            config = configparser.ConfigParser()\r\n            config.read(configName)\r\n\r\n            # Check the version\r\n            currentVersion = config['General'].get('version')\r\n            if currentVersion != self.CONFIG_VERSION:\r\n                raise UserWarning('Present configuration version {0} does not match {1}. You can try manual changes on \\'{2}\\''.format(currentVersion, self.CONFIG_VERSION, configName))\r\n\r\n            # Restoring the the parameters\r\n            self.globStep = config['General'].getint('globStep')\r\n            self.args.watsonMode = config['General'].getboolean('watsonMode')\r\n            self.args.autoEncode = config['General'].getboolean('autoEncode')\r\n            self.args.corpus = config['General'].get('corpus')\r\n\r\n            self.args.datasetTag = config['Dataset'].get('datasetTag')\r\n            self.args.maxLength = config['Dataset'].getint('maxLength')  # We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength)\r\n            self.args.filterVocab = config['Dataset'].getint('filterVocab')\r\n            self.args.skipLines = config['Dataset'].getboolean('skipLines')\r\n            self.args.vocabularySize = config['Dataset'].getint('vocabularySize')\r\n\r\n            self.args.hiddenSize = config['Network'].getint('hiddenSize')\r\n            self.args.numLayers = config['Network'].getint('numLayers')\r\n            self.args.softmaxSamples = config['Network'].getint('softmaxSamples')\r\n            self.args.initEmbeddings = config['Network'].getboolean('initEmbeddings')\r\n            self.args.embeddingSize = config['Network'].getint('embeddingSize')\r\n            self.args.embeddingSource = config['Network'].get('embeddingSource')\r\n\r\n            # No restoring for training params, batch size or other non model dependent parameters\r\n\r\n            # Show the restored params\r\n            print()\r\n            print('Warning: Restoring parameters:')\r\n            print('globStep: {}'.format(self.globStep))\r\n            print('watsonMode: {}'.format(self.args.watsonMode))\r\n            print('autoEncode: {}'.format(self.args.autoEncode))\r\n            print('corpus: {}'.format(self.args.corpus))\r\n            print('datasetTag: {}'.format(self.args.datasetTag))\r\n            print('maxLength: {}'.format(self.args.maxLength))\r\n            print('filterVocab: {}'.format(self.args.filterVocab))\r\n            print('skipLines: {}'.format(self.args.skipLines))\r\n            print('vocabularySize: {}'.format(self.args.vocabularySize))\r\n            print('hiddenSize: {}'.format(self.args.hiddenSize))\r\n            print('numLayers: {}'.format(self.args.numLayers))\r\n            print('softmaxSamples: {}'.format(self.args.softmaxSamples))\r\n            print('initEmbeddings: {}'.format(self.args.initEmbeddings))\r\n            print('embeddingSize: {}'.format(self.args.embeddingSize))\r\n            print('embeddingSource: {}'.format(self.args.embeddingSource))\r\n            print()\r\n\r\n        # For now, not arbitrary  independent maxLength between encoder and decoder\r\n        self.args.maxLengthEnco = self.args.maxLength\r\n        self.args.maxLengthDeco = self.args.maxLength + 2\r\n\r\n        if self.args.watsonMode:\r\n            self.SENTENCES_PREFIX.reverse()\r\n\r\n\r\n    def saveModelParams(self):\r\n        \"\"\" Save the params of the model, like the current globStep value\r\n        Warning: if you modify this function, make sure the changes mirror loadModelParams\r\n        \"\"\"\r\n        config = configparser.ConfigParser()\r\n        config['General'] = {}\r\n        config['General']['version']  = self.CONFIG_VERSION\r\n        config['General']['globStep']  = str(self.globStep)\r\n        config['General']['watsonMode'] = str(self.args.watsonMode)\r\n        config['General']['autoEncode'] = str(self.args.autoEncode)\r\n        config['General']['corpus'] = str(self.args.corpus)\r\n\r\n        config['Dataset'] = {}\r\n        config['Dataset']['datasetTag'] = str(self.args.datasetTag)\r\n        config['Dataset']['maxLength'] = str(self.args.maxLength)\r\n        config['Dataset']['filterVocab'] = str(self.args.filterVocab)\r\n        config['Dataset']['skipLines'] = str(self.args.skipLines)\r\n        config['Dataset']['vocabularySize'] = str(self.args.vocabularySize)\r\n\r\n        config['Network'] = {}\r\n        config['Network']['hiddenSize'] = str(self.args.hiddenSize)\r\n        config['Network']['numLayers'] = str(self.args.numLayers)\r\n        config['Network']['softmaxSamples'] = str(self.args.softmaxSamples)\r\n        config['Network']['initEmbeddings'] = str(self.args.initEmbeddings)\r\n        config['Network']['embeddingSize'] = str(self.args.embeddingSize)\r\n        config['Network']['embeddingSource'] = str(self.args.embeddingSource)\r\n\r\n        # Keep track of the learning params (but without restoring them)\r\n        config['Training (won\\'t be restored)'] = {}\r\n        config['Training (won\\'t be restored)']['learningRate'] = str(self.args.learningRate)\r\n        config['Training (won\\'t be restored)']['batchSize'] = str(self.args.batchSize)\r\n        config['Training (won\\'t be restored)']['dropout'] = str(self.args.dropout)\r\n\r\n        with open(os.path.join(self.modelDir, self.CONFIG_FILENAME), 'w') as configFile:\r\n            config.write(configFile)\r\n\r\n    def _getSummaryName(self):\r\n        \"\"\" Parse the argument to decide were to save the summary, at the same place that the model\r\n        The folder could already contain logs if we restore the training, those will be merged\r\n        Return:\r\n            str: The path and name of the summary\r\n        \"\"\"\r\n        return self.modelDir\r\n\r\n    def _getModelName(self):\r\n        \"\"\" Parse the argument to decide were to save/load the model\r\n        This function is called at each checkpoint and the first time the model is load. If keepAll option is set, the\r\n        globStep value will be included in the name.\r\n        Return:\r\n            str: The path and name were the model need to be saved\r\n        \"\"\"\r\n        modelName = os.path.join(self.modelDir, self.MODEL_NAME_BASE)\r\n        if self.args.keepAll:  # We do not erase the previously saved model by including the current step on the name\r\n            modelName += '-' + str(self.globStep)\r\n        return modelName + self.MODEL_EXT\r\n\r\n    def getDevice(self):\r\n        \"\"\" Parse the argument to decide on which device run the model\r\n        Return:\r\n            str: The name of the device on which run the program\r\n        \"\"\"\r\n        if self.args.device == 'cpu':\r\n            return '/cpu:0'\r\n        elif self.args.device == 'gpu':\r\n            return '/gpu:0'\r\n        elif self.args.device is None:  # No specified device (default)\r\n            return None\r\n        else:\r\n            print('Warning: Error in the device name: {}, use the default device'.format(self.args.device))\r\n            return None\r\nif __name__ == '__main__':\r\n    chat = Chatbot()\r\n    chat.main()\r\n\r\n"}