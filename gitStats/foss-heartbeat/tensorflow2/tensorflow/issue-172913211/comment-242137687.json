{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242137687", "html_url": "https://github.com/tensorflow/tensorflow/issues/4013#issuecomment-242137687", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4013", "id": 242137687, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjEzNzY4Nw==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-24T17:02:57Z", "updated_at": "2016-08-24T17:02:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think this is working as intended.  The DMA matrix shows which GPUs can talk as 'peers' to each other (e.g., via direct GPU to GPU copies using DMA, rather than going through CPU).</p>\n<p>Because you are using two generations of cards, they can't talk to each other, it seems.  The only implication is that the bandwidth between all the GPUs is not uniform, but you can still use all 4 GPUs in the same model and TensorFlow will properly use them.</p>", "body_text": "I think this is working as intended.  The DMA matrix shows which GPUs can talk as 'peers' to each other (e.g., via direct GPU to GPU copies using DMA, rather than going through CPU).\nBecause you are using two generations of cards, they can't talk to each other, it seems.  The only implication is that the bandwidth between all the GPUs is not uniform, but you can still use all 4 GPUs in the same model and TensorFlow will properly use them.", "body": "I think this is working as intended.  The DMA matrix shows which GPUs can talk as 'peers' to each other (e.g., via direct GPU to GPU copies using DMA, rather than going through CPU).\n\nBecause you are using two generations of cards, they can't talk to each other, it seems.  The only implication is that the bandwidth between all the GPUs is not uniform, but you can still use all 4 GPUs in the same model and TensorFlow will properly use them.\n"}