{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10851", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10851/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10851/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10851/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10851", "id": 237242741, "node_id": "MDU6SXNzdWUyMzcyNDI3NDE=", "number": 10851, "title": "ValueError: Attempt to reuse RNNCell - reuse flag does not work", "user": {"login": "fferroni", "id": 16327442, "node_id": "MDQ6VXNlcjE2MzI3NDQy", "avatar_url": "https://avatars1.githubusercontent.com/u/16327442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fferroni", "html_url": "https://github.com/fferroni", "followers_url": "https://api.github.com/users/fferroni/followers", "following_url": "https://api.github.com/users/fferroni/following{/other_user}", "gists_url": "https://api.github.com/users/fferroni/gists{/gist_id}", "starred_url": "https://api.github.com/users/fferroni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fferroni/subscriptions", "organizations_url": "https://api.github.com/users/fferroni/orgs", "repos_url": "https://api.github.com/users/fferroni/repos", "events_url": "https://api.github.com/users/fferroni/events{/privacy}", "received_events_url": "https://api.github.com/users/fferroni/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-20T15:01:52Z", "updated_at": "2017-06-20T20:16:29Z", "closed_at": "2017-06-20T20:16:29Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Linux Ubuntu 16.04</li>\n<li>tensorflow-gpu==1.1.0</li>\n</ul>\n<p>I am getting this error in quite a complex graph, but I can reproduce it with a minimal (but hopefully representative) example below:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n\nclass Controller(object):\n    def __init__(self, batch_size, input_size):\n\n        self.batch_size = batch_size\n        self.input_size = input_size\n\n        with tf.name_scope(\"controller\"):\n            self.network_vars()\n\n            self.nn_output_size = None\n            with tf.variable_scope(\"shape_inference\"):\n                self.nn_output_size = self.get_nn_output_size()\n\n    def network_vars(self):\n        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(256)\n        self.state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n\n    def network_op(self, x, state):\n        x = tf.convert_to_tensor(x)\n        return self.lstm_cell(x, state)\n\n    def get_state(self):\n        return self.state\n\n    def update_state(self, new_state):\n        return tf.no_op()\n\n    def process_input(self, x, state=None):\n        nn_output, nn_state = self.network_op(x, state)\n        return nn_output, nn_state\n\n    def get_nn_output_size(self):\n        input_tensor = np.zeros([self.batch_size, self.input_size], dtype=np.float32)\n        output_vector, _ = self.network_op(input_tensor, self.get_state())\n        shape = output_vector.get_shape().as_list()\n\n        if len(shape) &gt; 2:\n            raise ValueError(\"Expected the neural network to output a 1D vector\")\n        else:\n            return shape[1]\n\n\nclass DNC(object):\n    def __init__(self, batch_size, input_size):\n        self.batch_size = batch_size\n        self.input_size = input_size\n        self.controller = Controller(batch_size, input_size)\n        self.build_graph()\n\n    def _step_op(self, x, controller_state=None):\n        _, nn_state = self.controller.process_input(x, controller_state)\n        return [nn_state[0], nn_state[1]]\n\n    def _loop_body(self, t, controller_state):\n        x = np.random.random_sample((self.batch_size, self.input_size)).astype(np.float32)\n        output_list = self._step_op(x, controller_state)\n        new_controller_state = tf.contrib.rnn.LSTMStateTuple(output_list[0], output_list[1])\n        return t+1, new_controller_state\n\n    def build_graph(self):\n        controller_state = self.controller.get_state()\n        if not isinstance(controller_state, tf.contrib.rnn.LSTMStateTuple):\n            controller_state = tf.contrib.rnn.LSTMStateTuple(controller_state[0], controller_state[1])\n\n        with tf.variable_scope(\"sequence_loop\") as scope:\n            time = tf.constant(0, dtype=tf.int32)\n\n            final_results = tf.while_loop(\n                cond=lambda time, *_: time &lt; 50,\n                body=self._loop_body,\n                loop_vars=(time, controller_state),\n                parallel_iterations=32,\n                swap_memory=True\n            )\n\nif __name__ == \"__main__\":\n    batch_size = 32\n    input_size = 10\n    dnc = DNC(batch_size, input_size)\n</code></pre>\n<p>The traceback of the issue is:</p>\n<pre><code>francescoferroni@francescoferroni:~$ python controller.py \nTraceback (most recent call last):\n  File \"controller.py\", line 83, in &lt;module&gt;\n    dnc = DNC(batch_size, input_size)\n  File \"controller.py\", line 52, in __init__\n    self.build_graph()\n  File \"controller.py\", line 77, in build_graph\n    swap_memory=True\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"controller.py\", line 60, in _loop_body\n    output_list = self._step_op(x, controller_state)\n  File \"controller.py\", line 55, in _step_op\n    _, nn_state = self.controller.process_input(x, controller_state)\n  File \"controller.py\", line 33, in process_input\n    nn_output, nn_state = self.network_op(x, state)\n  File \"controller.py\", line 24, in network_op\n    return self.lstm_cell(x, state)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/contextlib.py\", line 82, in __enter__\n    return next(self.gen)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 77, in _checked_scope\n    type(cell).__name__))\nValueError: Attempt to reuse RNNCell &lt;tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7ff4075754e0&gt; with a different variable scope than its first use.  First use of cell was with scope 'shape_inference/basic_lstm_cell', this attempt is with scope 'sequence_loop/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\n</code></pre>\n<p>If I use tensorflow 1.0 rather than 1.1 it causes no issues.</p>\n<pre><code>francescoferroni@francescoferroni:~$ source Repositories/tfr10/bin/activate\n(tfr10) francescoferroni@francescoferroni:~$ python controller.py \nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n(tfr10) francescoferroni@francescoferroni:~$ \n</code></pre>\n<p>For the new tensorflow version, I have tried to add a reuse=True flag, as per <a href=\"https://github.com/tensorflow/tensorflow/issues/9401\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9401/hovercard\">#9401</a>, when defining the LSTM cell, but then I  get another error:<br>\n<code>ValueError: Variable shape_inference/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope? </code></p>\n<p>Any help would be greatly appreciated.</p>\n<p>Best,<br>\nFrancesco</p>", "body_text": "System information\n\nLinux Ubuntu 16.04\ntensorflow-gpu==1.1.0\n\nI am getting this error in quite a complex graph, but I can reproduce it with a minimal (but hopefully representative) example below:\nimport tensorflow as tf\nimport numpy as np\n\n\nclass Controller(object):\n    def __init__(self, batch_size, input_size):\n\n        self.batch_size = batch_size\n        self.input_size = input_size\n\n        with tf.name_scope(\"controller\"):\n            self.network_vars()\n\n            self.nn_output_size = None\n            with tf.variable_scope(\"shape_inference\"):\n                self.nn_output_size = self.get_nn_output_size()\n\n    def network_vars(self):\n        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(256)\n        self.state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n\n    def network_op(self, x, state):\n        x = tf.convert_to_tensor(x)\n        return self.lstm_cell(x, state)\n\n    def get_state(self):\n        return self.state\n\n    def update_state(self, new_state):\n        return tf.no_op()\n\n    def process_input(self, x, state=None):\n        nn_output, nn_state = self.network_op(x, state)\n        return nn_output, nn_state\n\n    def get_nn_output_size(self):\n        input_tensor = np.zeros([self.batch_size, self.input_size], dtype=np.float32)\n        output_vector, _ = self.network_op(input_tensor, self.get_state())\n        shape = output_vector.get_shape().as_list()\n\n        if len(shape) > 2:\n            raise ValueError(\"Expected the neural network to output a 1D vector\")\n        else:\n            return shape[1]\n\n\nclass DNC(object):\n    def __init__(self, batch_size, input_size):\n        self.batch_size = batch_size\n        self.input_size = input_size\n        self.controller = Controller(batch_size, input_size)\n        self.build_graph()\n\n    def _step_op(self, x, controller_state=None):\n        _, nn_state = self.controller.process_input(x, controller_state)\n        return [nn_state[0], nn_state[1]]\n\n    def _loop_body(self, t, controller_state):\n        x = np.random.random_sample((self.batch_size, self.input_size)).astype(np.float32)\n        output_list = self._step_op(x, controller_state)\n        new_controller_state = tf.contrib.rnn.LSTMStateTuple(output_list[0], output_list[1])\n        return t+1, new_controller_state\n\n    def build_graph(self):\n        controller_state = self.controller.get_state()\n        if not isinstance(controller_state, tf.contrib.rnn.LSTMStateTuple):\n            controller_state = tf.contrib.rnn.LSTMStateTuple(controller_state[0], controller_state[1])\n\n        with tf.variable_scope(\"sequence_loop\") as scope:\n            time = tf.constant(0, dtype=tf.int32)\n\n            final_results = tf.while_loop(\n                cond=lambda time, *_: time < 50,\n                body=self._loop_body,\n                loop_vars=(time, controller_state),\n                parallel_iterations=32,\n                swap_memory=True\n            )\n\nif __name__ == \"__main__\":\n    batch_size = 32\n    input_size = 10\n    dnc = DNC(batch_size, input_size)\n\nThe traceback of the issue is:\nfrancescoferroni@francescoferroni:~$ python controller.py \nTraceback (most recent call last):\n  File \"controller.py\", line 83, in <module>\n    dnc = DNC(batch_size, input_size)\n  File \"controller.py\", line 52, in __init__\n    self.build_graph()\n  File \"controller.py\", line 77, in build_graph\n    swap_memory=True\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"controller.py\", line 60, in _loop_body\n    output_list = self._step_op(x, controller_state)\n  File \"controller.py\", line 55, in _step_op\n    _, nn_state = self.controller.process_input(x, controller_state)\n  File \"controller.py\", line 33, in process_input\n    nn_output, nn_state = self.network_op(x, state)\n  File \"controller.py\", line 24, in network_op\n    return self.lstm_cell(x, state)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/contextlib.py\", line 82, in __enter__\n    return next(self.gen)\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 77, in _checked_scope\n    type(cell).__name__))\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7ff4075754e0> with a different variable scope than its first use.  First use of cell was with scope 'shape_inference/basic_lstm_cell', this attempt is with scope 'sequence_loop/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\n\nIf I use tensorflow 1.0 rather than 1.1 it causes no issues.\nfrancescoferroni@francescoferroni:~$ source Repositories/tfr10/bin/activate\n(tfr10) francescoferroni@francescoferroni:~$ python controller.py \nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n(tfr10) francescoferroni@francescoferroni:~$ \n\nFor the new tensorflow version, I have tried to add a reuse=True flag, as per #9401, when defining the LSTM cell, but then I  get another error:\nValueError: Variable shape_inference/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope? \nAny help would be greatly appreciated.\nBest,\nFrancesco", "body": "### System information\r\n\r\n-  Linux Ubuntu 16.04\r\n- tensorflow-gpu==1.1.0\r\n\r\nI am getting this error in quite a complex graph, but I can reproduce it with a minimal (but hopefully representative) example below:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass Controller(object):\r\n    def __init__(self, batch_size, input_size):\r\n\r\n        self.batch_size = batch_size\r\n        self.input_size = input_size\r\n\r\n        with tf.name_scope(\"controller\"):\r\n            self.network_vars()\r\n\r\n            self.nn_output_size = None\r\n            with tf.variable_scope(\"shape_inference\"):\r\n                self.nn_output_size = self.get_nn_output_size()\r\n\r\n    def network_vars(self):\r\n        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(256)\r\n        self.state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\r\n\r\n    def network_op(self, x, state):\r\n        x = tf.convert_to_tensor(x)\r\n        return self.lstm_cell(x, state)\r\n\r\n    def get_state(self):\r\n        return self.state\r\n\r\n    def update_state(self, new_state):\r\n        return tf.no_op()\r\n\r\n    def process_input(self, x, state=None):\r\n        nn_output, nn_state = self.network_op(x, state)\r\n        return nn_output, nn_state\r\n\r\n    def get_nn_output_size(self):\r\n        input_tensor = np.zeros([self.batch_size, self.input_size], dtype=np.float32)\r\n        output_vector, _ = self.network_op(input_tensor, self.get_state())\r\n        shape = output_vector.get_shape().as_list()\r\n\r\n        if len(shape) > 2:\r\n            raise ValueError(\"Expected the neural network to output a 1D vector\")\r\n        else:\r\n            return shape[1]\r\n\r\n\r\nclass DNC(object):\r\n    def __init__(self, batch_size, input_size):\r\n        self.batch_size = batch_size\r\n        self.input_size = input_size\r\n        self.controller = Controller(batch_size, input_size)\r\n        self.build_graph()\r\n\r\n    def _step_op(self, x, controller_state=None):\r\n        _, nn_state = self.controller.process_input(x, controller_state)\r\n        return [nn_state[0], nn_state[1]]\r\n\r\n    def _loop_body(self, t, controller_state):\r\n        x = np.random.random_sample((self.batch_size, self.input_size)).astype(np.float32)\r\n        output_list = self._step_op(x, controller_state)\r\n        new_controller_state = tf.contrib.rnn.LSTMStateTuple(output_list[0], output_list[1])\r\n        return t+1, new_controller_state\r\n\r\n    def build_graph(self):\r\n        controller_state = self.controller.get_state()\r\n        if not isinstance(controller_state, tf.contrib.rnn.LSTMStateTuple):\r\n            controller_state = tf.contrib.rnn.LSTMStateTuple(controller_state[0], controller_state[1])\r\n\r\n        with tf.variable_scope(\"sequence_loop\") as scope:\r\n            time = tf.constant(0, dtype=tf.int32)\r\n\r\n            final_results = tf.while_loop(\r\n                cond=lambda time, *_: time < 50,\r\n                body=self._loop_body,\r\n                loop_vars=(time, controller_state),\r\n                parallel_iterations=32,\r\n                swap_memory=True\r\n            )\r\n\r\nif __name__ == \"__main__\":\r\n    batch_size = 32\r\n    input_size = 10\r\n    dnc = DNC(batch_size, input_size)\r\n```\r\n\r\nThe traceback of the issue is:\r\n```\r\nfrancescoferroni@francescoferroni:~$ python controller.py \r\nTraceback (most recent call last):\r\n  File \"controller.py\", line 83, in <module>\r\n    dnc = DNC(batch_size, input_size)\r\n  File \"controller.py\", line 52, in __init__\r\n    self.build_graph()\r\n  File \"controller.py\", line 77, in build_graph\r\n    swap_memory=True\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"controller.py\", line 60, in _loop_body\r\n    output_list = self._step_op(x, controller_state)\r\n  File \"controller.py\", line 55, in _step_op\r\n    _, nn_state = self.controller.process_input(x, controller_state)\r\n  File \"controller.py\", line 33, in process_input\r\n    nn_output, nn_state = self.network_op(x, state)\r\n  File \"controller.py\", line 24, in network_op\r\n    return self.lstm_cell(x, state)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\r\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/contextlib.py\", line 82, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 77, in _checked_scope\r\n    type(cell).__name__))\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7ff4075754e0> with a different variable scope than its first use.  First use of cell was with scope 'shape_inference/basic_lstm_cell', this attempt is with scope 'sequence_loop/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n```\r\n\r\nIf I use tensorflow 1.0 rather than 1.1 it causes no issues.\r\n```\r\nfrancescoferroni@francescoferroni:~$ source Repositories/tfr10/bin/activate\r\n(tfr10) francescoferroni@francescoferroni:~$ python controller.py \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n(tfr10) francescoferroni@francescoferroni:~$ \r\n```\r\n\r\nFor the new tensorflow version, I have tried to add a reuse=True flag, as per [#9401](https://github.com/tensorflow/tensorflow/issues/9401), when defining the LSTM cell, but then I  get another error:\r\n`ValueError: Variable shape_inference/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n`\r\n\r\nAny help would be greatly appreciated.\r\n\r\nBest,\r\nFrancesco"}