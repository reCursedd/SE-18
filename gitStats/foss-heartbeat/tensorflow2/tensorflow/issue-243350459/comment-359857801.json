{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/359857801", "html_url": "https://github.com/tensorflow/tensorflow/issues/11549#issuecomment-359857801", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11549", "id": 359857801, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTg1NzgwMQ==", "user": {"login": "mas-dse-greina", "id": 22306322, "node_id": "MDQ6VXNlcjIyMzA2MzIy", "avatar_url": "https://avatars0.githubusercontent.com/u/22306322?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mas-dse-greina", "html_url": "https://github.com/mas-dse-greina", "followers_url": "https://api.github.com/users/mas-dse-greina/followers", "following_url": "https://api.github.com/users/mas-dse-greina/following{/other_user}", "gists_url": "https://api.github.com/users/mas-dse-greina/gists{/gist_id}", "starred_url": "https://api.github.com/users/mas-dse-greina/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mas-dse-greina/subscriptions", "organizations_url": "https://api.github.com/users/mas-dse-greina/orgs", "repos_url": "https://api.github.com/users/mas-dse-greina/repos", "events_url": "https://api.github.com/users/mas-dse-greina/events{/privacy}", "received_events_url": "https://api.github.com/users/mas-dse-greina/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-23T16:59:06Z", "updated_at": "2018-01-23T16:59:06Z", "author_association": "NONE", "body_html": "<p>Yes. I think this is due to the issue of the parameter server and chief worker being on different nodes. Apparently they need to save their checkpoints and model files to the same common directory (e.g. NFS or cloud bucket). Alternatively, you can run the distributed setup without a parameter server (in which case the chief node acts like the ps). However, I've found that this is much slower and basically negates the speed up you get by going distributed.</p>\n<p>It'd be great if TensorFlow added a function to allow us to consolidate the graph at the end onto the chief node in order to save the trained model.</p>", "body_text": "Yes. I think this is due to the issue of the parameter server and chief worker being on different nodes. Apparently they need to save their checkpoints and model files to the same common directory (e.g. NFS or cloud bucket). Alternatively, you can run the distributed setup without a parameter server (in which case the chief node acts like the ps). However, I've found that this is much slower and basically negates the speed up you get by going distributed.\nIt'd be great if TensorFlow added a function to allow us to consolidate the graph at the end onto the chief node in order to save the trained model.", "body": "Yes. I think this is due to the issue of the parameter server and chief worker being on different nodes. Apparently they need to save their checkpoints and model files to the same common directory (e.g. NFS or cloud bucket). Alternatively, you can run the distributed setup without a parameter server (in which case the chief node acts like the ps). However, I've found that this is much slower and basically negates the speed up you get by going distributed. \r\n\r\nIt'd be great if TensorFlow added a function to allow us to consolidate the graph at the end onto the chief node in order to save the trained model. "}