{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439459312", "html_url": "https://github.com/tensorflow/tensorflow/issues/18383#issuecomment-439459312", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18383", "id": 439459312, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTQ1OTMxMg==", "user": {"login": "javidcf", "id": 1098280, "node_id": "MDQ6VXNlcjEwOTgyODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1098280?v=4", "gravatar_id": "", "url": "https://api.github.com/users/javidcf", "html_url": "https://github.com/javidcf", "followers_url": "https://api.github.com/users/javidcf/followers", "following_url": "https://api.github.com/users/javidcf/following{/other_user}", "gists_url": "https://api.github.com/users/javidcf/gists{/gist_id}", "starred_url": "https://api.github.com/users/javidcf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/javidcf/subscriptions", "organizations_url": "https://api.github.com/users/javidcf/orgs", "repos_url": "https://api.github.com/users/javidcf/repos", "events_url": "https://api.github.com/users/javidcf/events{/privacy}", "received_events_url": "https://api.github.com/users/javidcf/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-16T17:03:23Z", "updated_at": "2018-11-21T11:31:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I made a Python-level implementation of this, at least for the simple case without strides (analogous to <a href=\"https://www.tensorflow.org/api_docs/python/tf/slice\" rel=\"nofollow\"><code>tf.slice</code></a>). Surely it is less efficient (in time and space) than a proper op and kernel, but I think it's quite convenient. It allows me to write something like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    x <span class=\"pl-k\">=</span> tf.reshape(tf.range(<span class=\"pl-c1\">60</span>), (<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>))\n    x2 <span class=\"pl-k\">=</span> replace_slice_in(x)[:<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span>:].with_value([<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">300</span>])\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        <span class=\"pl-c1\">print</span>(sess.run(x2))</pre></div>\n<p>Which prints:</p>\n<pre><code>[[[  0   1 100 200 300]\n  [  5   6 100 200 300]\n  [ 10  11 100 200 300]]\n\n [[ 15  16 100 200 300]\n  [ 20  21 100 200 300]\n  [ 25  26 100 200 300]]\n\n [[ 30  31  32  33  34]\n  [ 35  36  37  38  39]\n  [ 40  41  42  43  44]]\n\n [[ 45  46  47  48  49]\n  [ 50  51  52  53  54]\n  [ 55  56  57  58  59]]]\n</code></pre>\n<p>The main function is this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">replace_slice</span>(<span class=\"pl-smi\">input_</span>, <span class=\"pl-smi\">replacement</span>, <span class=\"pl-smi\">begin</span>, <span class=\"pl-smi\">size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    inp_shape <span class=\"pl-k\">=</span> tf.shape(input_)\n    <span class=\"pl-k\">if</span> size <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n        size <span class=\"pl-k\">=</span> tf.shape(replacement)\n    <span class=\"pl-k\">else</span>:\n        replacement <span class=\"pl-k\">=</span> tf.broadcast_to(replacement, size)\n    padding <span class=\"pl-k\">=</span> tf.stack([begin, inp_shape <span class=\"pl-k\">-</span> (begin <span class=\"pl-k\">+</span> size)], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    replacement_pad <span class=\"pl-k\">=</span> tf.pad(replacement, padding)\n    mask <span class=\"pl-k\">=</span> tf.pad(tf.ones_like(replacement, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.bool), padding)\n    <span class=\"pl-k\">return</span> tf.where(mask, replacement_pad, input_)</pre></div>\n<p><code>size</code> is optional because in principle it is inferred from <code>replacement</code>, but can be specified to broadcast it.</p>\n<p>The nicety of Python-like indexing is supported with this blob of ugly code, which is very likely a subset of similar logic already existing in TensorFlow. I tested it in a few cases and seems to work in general, although it assumes indices are <code>tf.int32</code> and there may be more special cases I'm not seeing.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">replace_slice_in</span>(<span class=\"pl-smi\">tensor</span>):\n    <span class=\"pl-k\">return</span> _SliceReplacer(tensor)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">_SliceReplacer</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">tensor</span>):\n        <span class=\"pl-c1\">self</span>._tensor <span class=\"pl-k\">=</span> tensor\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">slices</span>):\n        <span class=\"pl-k\">return</span> _SliceReplacer._Inner(<span class=\"pl-c1\">self</span>._tensor, slices)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">with_value</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">replacement</span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Just for convenience in case you skip the indexing</span>\n        <span class=\"pl-k\">return</span> _SliceReplacer._Inner(<span class=\"pl-c1\">self</span>._tensor, (<span class=\"pl-c1\">...</span>,)).with_value(replacement)\n    <span class=\"pl-k\">class</span> <span class=\"pl-en\">_Inner</span>:\n        <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">tensor</span>, <span class=\"pl-smi\">slices</span>):\n            <span class=\"pl-c1\">self</span>._tensor <span class=\"pl-k\">=</span> tensor\n            <span class=\"pl-c1\">self</span>._slices <span class=\"pl-k\">=</span> slices\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">with_value</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">replacement</span>):\n            begin, size <span class=\"pl-k\">=</span> _make_slices_begin_size(<span class=\"pl-c1\">self</span>._tensor, <span class=\"pl-c1\">self</span>._slices)\n            <span class=\"pl-k\">return</span> replace_slice(<span class=\"pl-c1\">self</span>._tensor, replacement, begin, size)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This computes begin and size values for a set of slices</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_make_slices_begin_size</span>(<span class=\"pl-smi\">input_</span>, <span class=\"pl-smi\">slices</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(slices, (<span class=\"pl-c1\">tuple</span>, <span class=\"pl-c1\">list</span>)):\n        slices <span class=\"pl-k\">=</span> (slices,)\n    inp_rank <span class=\"pl-k\">=</span> tf.rank(input_)\n    inp_shape <span class=\"pl-k\">=</span> tf.shape(input_)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Did we see a ellipsis already?</span>\n    before_ellipsis <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Sliced dimensions</span>\n    dim_idx <span class=\"pl-k\">=</span> []\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Slice start points</span>\n    begins <span class=\"pl-k\">=</span> []\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Slice sizes</span>\n    sizes <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> i, s <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(slices):\n        <span class=\"pl-k\">if</span> s <span class=\"pl-k\">is</span> <span class=\"pl-c1\">Ellipsis</span>:\n            <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> before_ellipsis:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Cannot use more than one ellipsis in slice spec.<span class=\"pl-pds\">'</span></span>)\n            before_ellipsis <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n            <span class=\"pl-k\">continue</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(s, <span class=\"pl-c1\">slice</span>):\n            start <span class=\"pl-k\">=</span> s.start\n            stop <span class=\"pl-k\">=</span> s.stop\n            <span class=\"pl-k\">if</span> s.step <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Step value not supported.<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">else</span>:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assumed to be a single integer value</span>\n            start <span class=\"pl-k\">=</span> s\n            stop <span class=\"pl-k\">=</span> s <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Dimension this slice refers to</span>\n        i_dim <span class=\"pl-k\">=</span> i <span class=\"pl-k\">if</span> before_ellipsis <span class=\"pl-k\">else</span> inp_rank <span class=\"pl-k\">-</span> (<span class=\"pl-c1\">len</span>(slices) <span class=\"pl-k\">-</span> i)\n        dim_size <span class=\"pl-k\">=</span> inp_shape[i_dim]\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Default slice values</span>\n        start <span class=\"pl-k\">=</span> start <span class=\"pl-k\">if</span> start <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">0</span>\n        stop <span class=\"pl-k\">=</span> stop <span class=\"pl-k\">if</span> stop <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> dim_size\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Fix negative indices</span>\n        start <span class=\"pl-k\">=</span> tf.cond(tf.convert_to_tensor(start <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">0</span>), <span class=\"pl-k\">lambda</span>: start, <span class=\"pl-k\">lambda</span>: start <span class=\"pl-k\">+</span> dim_size)\n        stop <span class=\"pl-k\">=</span> tf.cond(tf.convert_to_tensor(stop <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">0</span>), <span class=\"pl-k\">lambda</span>: stop, <span class=\"pl-k\">lambda</span>: stop <span class=\"pl-k\">+</span> dim_size)\n        dim_idx.append([i_dim])\n        begins.append(start)\n        sizes.append(stop <span class=\"pl-k\">-</span> start)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> For empty slice specs like [...]</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> dim_idx:\n        <span class=\"pl-k\">return</span> tf.zeros_like(inp_shape), inp_shape\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make full begin and size array (including omitted dimensions)</span>\n    begin_full <span class=\"pl-k\">=</span> tf.scatter_nd(dim_idx, begins, [inp_rank])\n    size_mask <span class=\"pl-k\">=</span> tf.scatter_nd(dim_idx, tf.ones_like(sizes, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.bool), [inp_rank])\n    size_full <span class=\"pl-k\">=</span> tf.where(size_mask,\n                         tf.scatter_nd(dim_idx, sizes, [inp_rank]),\n                         inp_shape)\n    <span class=\"pl-k\">return</span> begin_full, size_full</pre></div>", "body_text": "I made a Python-level implementation of this, at least for the simple case without strides (analogous to tf.slice). Surely it is less efficient (in time and space) than a proper op and kernel, but I think it's quite convenient. It allows me to write something like this:\nimport tensorflow as tf\n\nwith tf.Graph().as_default():\n    x = tf.reshape(tf.range(60), (4, 3, 5))\n    x2 = replace_slice_in(x)[:2, ..., -3:].with_value([100, 200, 300])\n    with tf.Session() as sess:\n        print(sess.run(x2))\nWhich prints:\n[[[  0   1 100 200 300]\n  [  5   6 100 200 300]\n  [ 10  11 100 200 300]]\n\n [[ 15  16 100 200 300]\n  [ 20  21 100 200 300]\n  [ 25  26 100 200 300]]\n\n [[ 30  31  32  33  34]\n  [ 35  36  37  38  39]\n  [ 40  41  42  43  44]]\n\n [[ 45  46  47  48  49]\n  [ 50  51  52  53  54]\n  [ 55  56  57  58  59]]]\n\nThe main function is this:\nimport tensorflow as tf\n\ndef replace_slice(input_, replacement, begin, size=None):\n    inp_shape = tf.shape(input_)\n    if size is None:\n        size = tf.shape(replacement)\n    else:\n        replacement = tf.broadcast_to(replacement, size)\n    padding = tf.stack([begin, inp_shape - (begin + size)], axis=1)\n    replacement_pad = tf.pad(replacement, padding)\n    mask = tf.pad(tf.ones_like(replacement, dtype=tf.bool), padding)\n    return tf.where(mask, replacement_pad, input_)\nsize is optional because in principle it is inferred from replacement, but can be specified to broadcast it.\nThe nicety of Python-like indexing is supported with this blob of ugly code, which is very likely a subset of similar logic already existing in TensorFlow. I tested it in a few cases and seems to work in general, although it assumes indices are tf.int32 and there may be more special cases I'm not seeing.\ndef replace_slice_in(tensor):\n    return _SliceReplacer(tensor)\n\nclass _SliceReplacer:\n    def __init__(self, tensor):\n        self._tensor = tensor\n    def __getitem__(self, slices):\n        return _SliceReplacer._Inner(self._tensor, slices)\n    def with_value(self, replacement):  # Just for convenience in case you skip the indexing\n        return _SliceReplacer._Inner(self._tensor, (...,)).with_value(replacement)\n    class _Inner:\n        def __init__(self, tensor, slices):\n            self._tensor = tensor\n            self._slices = slices\n        def with_value(self, replacement):\n            begin, size = _make_slices_begin_size(self._tensor, self._slices)\n            return replace_slice(self._tensor, replacement, begin, size)\n\n# This computes begin and size values for a set of slices\ndef _make_slices_begin_size(input_, slices):\n    if not isinstance(slices, (tuple, list)):\n        slices = (slices,)\n    inp_rank = tf.rank(input_)\n    inp_shape = tf.shape(input_)\n    # Did we see a ellipsis already?\n    before_ellipsis = True\n    # Sliced dimensions\n    dim_idx = []\n    # Slice start points\n    begins = []\n    # Slice sizes\n    sizes = []\n    for i, s in enumerate(slices):\n        if s is Ellipsis:\n            if not before_ellipsis:\n                raise ValueError('Cannot use more than one ellipsis in slice spec.')\n            before_ellipsis = False\n            continue\n        if isinstance(s, slice):\n            start = s.start\n            stop = s.stop\n            if s.step is not None:\n                raise ValueError('Step value not supported.')\n        else:  # Assumed to be a single integer value\n            start = s\n            stop = s + 1\n        # Dimension this slice refers to\n        i_dim = i if before_ellipsis else inp_rank - (len(slices) - i)\n        dim_size = inp_shape[i_dim]\n        # Default slice values\n        start = start if start is not None else 0\n        stop = stop if stop is not None else dim_size\n        # Fix negative indices\n        start = tf.cond(tf.convert_to_tensor(start >= 0), lambda: start, lambda: start + dim_size)\n        stop = tf.cond(tf.convert_to_tensor(stop >= 0), lambda: stop, lambda: stop + dim_size)\n        dim_idx.append([i_dim])\n        begins.append(start)\n        sizes.append(stop - start)\n    # For empty slice specs like [...]\n    if not dim_idx:\n        return tf.zeros_like(inp_shape), inp_shape\n    # Make full begin and size array (including omitted dimensions)\n    begin_full = tf.scatter_nd(dim_idx, begins, [inp_rank])\n    size_mask = tf.scatter_nd(dim_idx, tf.ones_like(sizes, dtype=tf.bool), [inp_rank])\n    size_full = tf.where(size_mask,\n                         tf.scatter_nd(dim_idx, sizes, [inp_rank]),\n                         inp_shape)\n    return begin_full, size_full", "body": "I made a Python-level implementation of this, at least for the simple case without strides (analogous to [`tf.slice`](https://www.tensorflow.org/api_docs/python/tf/slice)). Surely it is less efficient (in time and space) than a proper op and kernel, but I think it's quite convenient. It allows me to write something like this:\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n    x = tf.reshape(tf.range(60), (4, 3, 5))\r\n    x2 = replace_slice_in(x)[:2, ..., -3:].with_value([100, 200, 300])\r\n    with tf.Session() as sess:\r\n        print(sess.run(x2))\r\n```\r\n\r\nWhich prints:\r\n\r\n```\r\n[[[  0   1 100 200 300]\r\n  [  5   6 100 200 300]\r\n  [ 10  11 100 200 300]]\r\n\r\n [[ 15  16 100 200 300]\r\n  [ 20  21 100 200 300]\r\n  [ 25  26 100 200 300]]\r\n\r\n [[ 30  31  32  33  34]\r\n  [ 35  36  37  38  39]\r\n  [ 40  41  42  43  44]]\r\n\r\n [[ 45  46  47  48  49]\r\n  [ 50  51  52  53  54]\r\n  [ 55  56  57  58  59]]]\r\n```\r\n\r\nThe main function is this:\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\ndef replace_slice(input_, replacement, begin, size=None):\r\n    inp_shape = tf.shape(input_)\r\n    if size is None:\r\n        size = tf.shape(replacement)\r\n    else:\r\n        replacement = tf.broadcast_to(replacement, size)\r\n    padding = tf.stack([begin, inp_shape - (begin + size)], axis=1)\r\n    replacement_pad = tf.pad(replacement, padding)\r\n    mask = tf.pad(tf.ones_like(replacement, dtype=tf.bool), padding)\r\n    return tf.where(mask, replacement_pad, input_)\r\n```\r\n\r\n`size` is optional because in principle it is inferred from `replacement`, but can be specified to broadcast it.\r\n\r\nThe nicety of Python-like indexing is supported with this blob of ugly code, which is very likely a subset of similar logic already existing in TensorFlow. I tested it in a few cases and seems to work in general, although it assumes indices are `tf.int32` and there may be more special cases I'm not seeing.\r\n\r\n```py\r\ndef replace_slice_in(tensor):\r\n    return _SliceReplacer(tensor)\r\n\r\nclass _SliceReplacer:\r\n    def __init__(self, tensor):\r\n        self._tensor = tensor\r\n    def __getitem__(self, slices):\r\n        return _SliceReplacer._Inner(self._tensor, slices)\r\n    def with_value(self, replacement):  # Just for convenience in case you skip the indexing\r\n        return _SliceReplacer._Inner(self._tensor, (...,)).with_value(replacement)\r\n    class _Inner:\r\n        def __init__(self, tensor, slices):\r\n            self._tensor = tensor\r\n            self._slices = slices\r\n        def with_value(self, replacement):\r\n            begin, size = _make_slices_begin_size(self._tensor, self._slices)\r\n            return replace_slice(self._tensor, replacement, begin, size)\r\n\r\n# This computes begin and size values for a set of slices\r\ndef _make_slices_begin_size(input_, slices):\r\n    if not isinstance(slices, (tuple, list)):\r\n        slices = (slices,)\r\n    inp_rank = tf.rank(input_)\r\n    inp_shape = tf.shape(input_)\r\n    # Did we see a ellipsis already?\r\n    before_ellipsis = True\r\n    # Sliced dimensions\r\n    dim_idx = []\r\n    # Slice start points\r\n    begins = []\r\n    # Slice sizes\r\n    sizes = []\r\n    for i, s in enumerate(slices):\r\n        if s is Ellipsis:\r\n            if not before_ellipsis:\r\n                raise ValueError('Cannot use more than one ellipsis in slice spec.')\r\n            before_ellipsis = False\r\n            continue\r\n        if isinstance(s, slice):\r\n            start = s.start\r\n            stop = s.stop\r\n            if s.step is not None:\r\n                raise ValueError('Step value not supported.')\r\n        else:  # Assumed to be a single integer value\r\n            start = s\r\n            stop = s + 1\r\n        # Dimension this slice refers to\r\n        i_dim = i if before_ellipsis else inp_rank - (len(slices) - i)\r\n        dim_size = inp_shape[i_dim]\r\n        # Default slice values\r\n        start = start if start is not None else 0\r\n        stop = stop if stop is not None else dim_size\r\n        # Fix negative indices\r\n        start = tf.cond(tf.convert_to_tensor(start >= 0), lambda: start, lambda: start + dim_size)\r\n        stop = tf.cond(tf.convert_to_tensor(stop >= 0), lambda: stop, lambda: stop + dim_size)\r\n        dim_idx.append([i_dim])\r\n        begins.append(start)\r\n        sizes.append(stop - start)\r\n    # For empty slice specs like [...]\r\n    if not dim_idx:\r\n        return tf.zeros_like(inp_shape), inp_shape\r\n    # Make full begin and size array (including omitted dimensions)\r\n    begin_full = tf.scatter_nd(dim_idx, begins, [inp_rank])\r\n    size_mask = tf.scatter_nd(dim_idx, tf.ones_like(sizes, dtype=tf.bool), [inp_rank])\r\n    size_full = tf.where(size_mask,\r\n                         tf.scatter_nd(dim_idx, sizes, [inp_rank]),\r\n                         inp_shape)\r\n    return begin_full, size_full\r\n```"}