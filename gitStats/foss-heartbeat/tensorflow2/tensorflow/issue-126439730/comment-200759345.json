{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/200759345", "html_url": "https://github.com/tensorflow/tensorflow/pull/763#issuecomment-200759345", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/763", "id": 200759345, "node_id": "MDEyOklzc3VlQ29tbWVudDIwMDc1OTM0NQ==", "user": {"login": "dvyukov", "id": 1095328, "node_id": "MDQ6VXNlcjEwOTUzMjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1095328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dvyukov", "html_url": "https://github.com/dvyukov", "followers_url": "https://api.github.com/users/dvyukov/followers", "following_url": "https://api.github.com/users/dvyukov/following{/other_user}", "gists_url": "https://api.github.com/users/dvyukov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dvyukov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dvyukov/subscriptions", "organizations_url": "https://api.github.com/users/dvyukov/orgs", "repos_url": "https://api.github.com/users/dvyukov/repos", "events_url": "https://api.github.com/users/dvyukov/events{/privacy}", "received_events_url": "https://api.github.com/users/dvyukov/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-24T09:46:48Z", "updated_at": "2016-03-24T09:46:48Z", "author_association": "NONE", "body_html": "<p>There definitely was lack of communication. Sorry.</p>\n<p>And Reviewable was not helping as well. I basically can't make any sense out of the interface. I have no idea what's pending, what's answered, when, etc.</p>\n<p>I am working on eigen contraction issue. It blocks any non-trivial thread pool implementation. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=112556\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jeremybarnes\">@jeremybarnes</a> you were lucky to not hit deadlocks. But if we would continue to improve your implementation, we would hit them as well (e.g. if we steal half of elements from remote queue which would effectively randomize execution order).<br>\nUnfortunately it turned out to be not completely trivial. Contraction performance characteristics are also tightly tied to execution order (e.g. packed rhs reuse while it is hot in cache). And any asynchronous continuation-based implementation shuffles execution order. So most time is taken by ensuring that there are no significant performance degradations on any of hundreds of different matrix configurations, thread counts and instruction sets (sse, avx, fma).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=112556\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jeremybarnes\">@jeremybarnes</a> It would be great if you provide one or several benchmarks representative of workloads you are interested in. I know that you asked for acknowledgement from somebody from core tensorflow team and got no response. Anybody from tensorflow?</p>\n<p>Re pluggable thread pools, I would very much warn you against this idea.<br>\nWhat it will do is it will fragment users, provoke components that work with one thread pool but does not work with another and disperse optimization efforts.<br>\nThread pool implementation must not (in theory) differ by provided semantics, they can only differ by buginess level and performance/scalability. That's not what users much choose between. We need a single thread pool that is tested as much as possible, works with all components and provides best performance/scalability. There is no need for anything else.<br>\nI understand that there can be different implementations that provide different performance for different algorithms. But what we should concentrate on it tuning the single implementation to support broader class of algorithms well. We are nowhere close to being able to reasonedly conclude that that is not possible, and the only choice we have is to fragment user base.</p>", "body_text": "There definitely was lack of communication. Sorry.\nAnd Reviewable was not helping as well. I basically can't make any sense out of the interface. I have no idea what's pending, what's answered, when, etc.\nI am working on eigen contraction issue. It blocks any non-trivial thread pool implementation. @jeremybarnes you were lucky to not hit deadlocks. But if we would continue to improve your implementation, we would hit them as well (e.g. if we steal half of elements from remote queue which would effectively randomize execution order).\nUnfortunately it turned out to be not completely trivial. Contraction performance characteristics are also tightly tied to execution order (e.g. packed rhs reuse while it is hot in cache). And any asynchronous continuation-based implementation shuffles execution order. So most time is taken by ensuring that there are no significant performance degradations on any of hundreds of different matrix configurations, thread counts and instruction sets (sse, avx, fma).\n@jeremybarnes It would be great if you provide one or several benchmarks representative of workloads you are interested in. I know that you asked for acknowledgement from somebody from core tensorflow team and got no response. Anybody from tensorflow?\nRe pluggable thread pools, I would very much warn you against this idea.\nWhat it will do is it will fragment users, provoke components that work with one thread pool but does not work with another and disperse optimization efforts.\nThread pool implementation must not (in theory) differ by provided semantics, they can only differ by buginess level and performance/scalability. That's not what users much choose between. We need a single thread pool that is tested as much as possible, works with all components and provides best performance/scalability. There is no need for anything else.\nI understand that there can be different implementations that provide different performance for different algorithms. But what we should concentrate on it tuning the single implementation to support broader class of algorithms well. We are nowhere close to being able to reasonedly conclude that that is not possible, and the only choice we have is to fragment user base.", "body": "There definitely was lack of communication. Sorry.\n\nAnd Reviewable was not helping as well. I basically can't make any sense out of the interface. I have no idea what's pending, what's answered, when, etc.\n\nI am working on eigen contraction issue. It blocks any non-trivial thread pool implementation. @jeremybarnes you were lucky to not hit deadlocks. But if we would continue to improve your implementation, we would hit them as well (e.g. if we steal half of elements from remote queue which would effectively randomize execution order).\nUnfortunately it turned out to be not completely trivial. Contraction performance characteristics are also tightly tied to execution order (e.g. packed rhs reuse while it is hot in cache). And any asynchronous continuation-based implementation shuffles execution order. So most time is taken by ensuring that there are no significant performance degradations on any of hundreds of different matrix configurations, thread counts and instruction sets (sse, avx, fma).\n\n@jeremybarnes It would be great if you provide one or several benchmarks representative of workloads you are interested in. I know that you asked for acknowledgement from somebody from core tensorflow team and got no response. Anybody from tensorflow?\n\nRe pluggable thread pools, I would very much warn you against this idea.\nWhat it will do is it will fragment users, provoke components that work with one thread pool but does not work with another and disperse optimization efforts.\nThread pool implementation must not (in theory) differ by provided semantics, they can only differ by buginess level and performance/scalability. That's not what users much choose between. We need a single thread pool that is tested as much as possible, works with all components and provides best performance/scalability. There is no need for anything else.\nI understand that there can be different implementations that provide different performance for different algorithms. But what we should concentrate on it tuning the single implementation to support broader class of algorithms well. We are nowhere close to being able to reasonedly conclude that that is not possible, and the only choice we have is to fragment user base.\n"}