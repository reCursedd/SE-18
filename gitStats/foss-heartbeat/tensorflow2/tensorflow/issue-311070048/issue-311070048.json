{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18227", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18227/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18227/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18227/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18227", "id": 311070048, "node_id": "MDU6SXNzdWUzMTEwNzAwNDg=", "number": 18227, "title": "(feature request) batch results from tf.estimator.Estimator.predict", "user": {"login": "ed-alertedh", "id": 24605895, "node_id": "MDQ6VXNlcjI0NjA1ODk1", "avatar_url": "https://avatars1.githubusercontent.com/u/24605895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ed-alertedh", "html_url": "https://github.com/ed-alertedh", "followers_url": "https://api.github.com/users/ed-alertedh/followers", "following_url": "https://api.github.com/users/ed-alertedh/following{/other_user}", "gists_url": "https://api.github.com/users/ed-alertedh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ed-alertedh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ed-alertedh/subscriptions", "organizations_url": "https://api.github.com/users/ed-alertedh/orgs", "repos_url": "https://api.github.com/users/ed-alertedh/repos", "events_url": "https://api.github.com/users/ed-alertedh/events{/privacy}", "received_events_url": "https://api.github.com/users/ed-alertedh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-04-04T03:09:35Z", "updated_at": "2018-04-06T01:10:26Z", "closed_at": "2018-04-05T16:01:54Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>: Binary</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>: tensorflow-gpu 1.6.0</p>\n</li>\n<li>\n<p><strong>Python version</strong>: 3.6.3 64-bit</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>: N/A</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>: CUDA 9.0 / cuDNN 6</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>: Nvidia Geforce GTX 1060 6GB</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:</p>\n<pre><code>  import tensorflow as tf\n  import numpy as np\n  import itertools\n  \n  net_inputs = np.array(...)\n  estimator = tf.estimator.Estimator(...) # custom model_fn, not all that relevant\n  \n  # feed a large numpy array in as a single batch\n  my_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={\"inputs\": net_inputs},\n      y=None, shuffle=False, batch_size=net_inputs.shape[0])\n\n  # this generator only gives one result at a time\n  predictions_generator = estimator.predict(input_fn=my_input_fn, predict_keys=['raw_predictions'])\n  \n  # aggregate the whole batch back together again (slow!)\n  predictions = np.reshape(\n      np.fromiter(itertools.chain.from_iterable(x['raw_predictions'] for x in predictions_generator), input_vects.dtype),\n      [-1, net_inputs.shape[1]])\n</code></pre>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The way the generator in <code>tf.estimator.Estimator.predict()</code> is currently implemented to yield individual predictions from batched predictions adds quite a bit of overhead. Specifically the loop here<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L516\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L516</a></p>\n<p>I hacked together my own copy of <code>predict()</code> that directly yields <code>preds_evaluated</code> and the total time taken on my test-set of a few million examples with a batch size of 128k was 64% of the time taken when I used the method above to re-aggregate back to a numpy array. Is there any chance of adding an optional parameter to <code>predict()</code> to change the behaviour to this? I realise that estimators are more intended for ease-of-use but this would only be a few lines to change and by keeping it as an optional parameter backwards-compatibility would be maintained. Happy to prepare a PR if there's a chance it would be accepted.</p>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\n\nTensorFlow installed from (source or binary): Binary\n\n\nTensorFlow version (use command below): tensorflow-gpu 1.6.0\n\n\nPython version: 3.6.3 64-bit\n\n\nBazel version (if compiling from source): N/A\n\n\nGCC/Compiler version (if compiling from source): N/A\n\n\nCUDA/cuDNN version: CUDA 9.0 / cuDNN 6\n\n\nGPU model and memory: Nvidia Geforce GTX 1060 6GB\n\n\nExact command to reproduce:\n  import tensorflow as tf\n  import numpy as np\n  import itertools\n  \n  net_inputs = np.array(...)\n  estimator = tf.estimator.Estimator(...) # custom model_fn, not all that relevant\n  \n  # feed a large numpy array in as a single batch\n  my_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={\"inputs\": net_inputs},\n      y=None, shuffle=False, batch_size=net_inputs.shape[0])\n\n  # this generator only gives one result at a time\n  predictions_generator = estimator.predict(input_fn=my_input_fn, predict_keys=['raw_predictions'])\n  \n  # aggregate the whole batch back together again (slow!)\n  predictions = np.reshape(\n      np.fromiter(itertools.chain.from_iterable(x['raw_predictions'] for x in predictions_generator), input_vects.dtype),\n      [-1, net_inputs.shape[1]])\n\n\n\nDescribe the problem\nThe way the generator in tf.estimator.Estimator.predict() is currently implemented to yield individual predictions from batched predictions adds quite a bit of overhead. Specifically the loop here\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L516\nI hacked together my own copy of predict() that directly yields preds_evaluated and the total time taken on my test-set of a few million examples with a batch size of 128k was 64% of the time taken when I used the method above to re-aggregate back to a numpy array. Is there any chance of adding an optional parameter to predict() to change the behaviour to this? I realise that estimators are more intended for ease-of-use but this would only be a few lines to change and by keeping it as an optional parameter backwards-compatibility would be maintained. Happy to prepare a PR if there's a chance it would be accepted.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu 1.6.0\r\n- **Python version**: 3.6.3 64-bit\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 6\r\n- **GPU model and memory**: Nvidia Geforce GTX 1060 6GB\r\n- **Exact command to reproduce**:\r\n\r\n        import tensorflow as tf\r\n        import numpy as np\r\n        import itertools\r\n        \r\n        net_inputs = np.array(...)\r\n        estimator = tf.estimator.Estimator(...) # custom model_fn, not all that relevant\r\n        \r\n        # feed a large numpy array in as a single batch\r\n        my_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n            x={\"inputs\": net_inputs},\r\n            y=None, shuffle=False, batch_size=net_inputs.shape[0])\r\n\r\n        # this generator only gives one result at a time\r\n        predictions_generator = estimator.predict(input_fn=my_input_fn, predict_keys=['raw_predictions'])\r\n        \r\n        # aggregate the whole batch back together again (slow!)\r\n        predictions = np.reshape(\r\n            np.fromiter(itertools.chain.from_iterable(x['raw_predictions'] for x in predictions_generator), input_vects.dtype),\r\n            [-1, net_inputs.shape[1]])\r\n\r\n### Describe the problem\r\n\r\nThe way the generator in `tf.estimator.Estimator.predict()` is currently implemented to yield individual predictions from batched predictions adds quite a bit of overhead. Specifically the loop here \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L516\r\n\r\nI hacked together my own copy of `predict()` that directly yields `preds_evaluated` and the total time taken on my test-set of a few million examples with a batch size of 128k was 64% of the time taken when I used the method above to re-aggregate back to a numpy array. Is there any chance of adding an optional parameter to `predict()` to change the behaviour to this? I realise that estimators are more intended for ease-of-use but this would only be a few lines to change and by keeping it as an optional parameter backwards-compatibility would be maintained. Happy to prepare a PR if there's a chance it would be accepted."}