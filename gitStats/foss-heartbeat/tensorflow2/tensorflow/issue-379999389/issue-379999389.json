{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23697", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23697/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23697/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23697/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23697", "id": 379999389, "node_id": "MDU6SXNzdWUzNzk5OTkzODk=", "number": 23697, "title": "Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. \t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]", "user": {"login": "skavula", "id": 34525437, "node_id": "MDQ6VXNlcjM0NTI1NDM3", "avatar_url": "https://avatars1.githubusercontent.com/u/34525437?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skavula", "html_url": "https://github.com/skavula", "followers_url": "https://api.github.com/users/skavula/followers", "following_url": "https://api.github.com/users/skavula/following{/other_user}", "gists_url": "https://api.github.com/users/skavula/gists{/gist_id}", "starred_url": "https://api.github.com/users/skavula/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skavula/subscriptions", "organizations_url": "https://api.github.com/users/skavula/orgs", "repos_url": "https://api.github.com/users/skavula/repos", "events_url": "https://api.github.com/users/skavula/events{/privacy}", "received_events_url": "https://api.github.com/users/skavula/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-11-12T23:53:20Z", "updated_at": "2018-11-21T23:22:20Z", "closed_at": "2018-11-21T23:14:11Z", "author_association": "NONE", "body_html": "<p>The error is when I am trying to run distributed training with edward. Below is the code :</p>\n<pre><code>parameter_servers = [\"localhost:2222\"]\nworkers = [\"localhost:2223\"]\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# start a server for a specific task\nserver = tf.train.Server(\n     cluster,\n     job_name=FLAGS.job_name,\n     task_index=FLAGS.task_index)\n     \nclass VAE(object):\n   def __init__():\n       # MODEL\n       self.n_features = n_features\n       self.params = {\n                'M': 2048,\n                'd': 5,\n                'n_epoch': 2,\n                'hidden_d': [25, 5],\n                'learning_rate': 0.01\n            }\n       self.saved_dir_path = dir_path\n       self.ckpt_path = os.path.join(self.saved_dir_path,\n                                     'checkpointFiles/') + 'model.ckpt'\n       # distributed training\n       if FLAGS.job_name == \"ps\":\n           server.join()\n       elif FLAGS.job_name == \"worker\":\n           # Between-graph replication\n           with tf.device(tf.train.replica_device_setter(\n                   worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                   cluster=cluster)):\n               self.global_step = tf.get_variable(\n                                                    'global_step',\n                                                    [],\n                                                    dtype=tf.int64,\n                                                    initializer=tf.constant_initializer(0),\n                                                    trainable=False)\n               self.z = Normal(\n                   loc=tf.zeros([self.params['M'], self.params['d']]),\n                   scale=tf.ones([self.params['M'], self.params['d']]))\n               # self.hidden0 = tf.layers.dense(\n               self.hidden = tf.layers.dense(\n                   self.z, self.params['hidden_d'][0], activation=tf.nn.relu)\n               if len(self.params['hidden_d']) &gt; 1:\n                   # for i in xrange(1, len(params[hidden_d])):\n                   #     self.__dict__['hidden' + str(i)] = \\\n                   #         tf.layers.dense(\n                   #             self.__dict__['hidden' + str(i-1)],\n                   #             self.params['hidden_d'][i],\n                   #             activation=tf.nn.relu)\n                   for i in xrange(1, len(params['hidden_d'])):\n                       self.hidden = \\\n                           tf.layers.dense(\n                               self.hidden,\n                               self.params['hidden_d'][i],\n                               activation=tf.nn.relu)\n               self.x = Bernoulli(\n                   logits=tf.layers.dense(\n                       # self.__dict__['hidden' + str(len(params['hidden_d'] - 1))],\n                       self.hidden,\n                       self.n_features), dtype=tf.float64)\n\n               # INFERENCE\n               self.x_ph = tf.placeholder(dtype=tf.float64,shape=[None, self.n_features])\n               self.hidden = tf.layers.dense(\n                   tf.cast(self.x_ph, tf.float32),\n                   self.params['hidden_d'][-1],\n                   activation=tf.nn.relu)\n               if len(self.params['hidden_d']) &gt; 1:\n                   for i in xrange(1, len(params['hidden_d'])):\n                       j = -(1+i)\n                       self.hidden = \\\n                           tf.layers.dense(\n                               self.hidden,\n                               self.params['hidden_d'][j],\n                               activation=tf.nn.relu)\n               self.qz = Normal(\n                   loc=tf.layers.dense(self.hidden, self.params['d']),\n                   scale=tf.layers.dense(\n                       self.hidden, self.params['d'], activation=tf.nn.softplus))\n               self.x_avg = Bernoulli(\n                   logits=tf.reduce_mean(self.x.parameters['logits'], 0),\n                   name='x_avg')\n               self.log_likli = tf.reduce_mean(self.x_avg.log_prob(self.x_ph), 1)\n               self.optimizer = tf.train.RMSPropOptimizer(\n                   self.params['learning_rate'], epsilon=1.0)\n               # self.\n               self.inference = ed.KLqp({self.z: self.qz}, data={self.x: self.x_ph})\n               self.inference_init = self.inference.initialize(\n                   optimizer=self.optimizer, global_step = self.global_step, logdir='log')\n               self.init = tf.global_variables_initializer()\n               self.saver = tf.train.Saver()\n\n   def train(self, train_data):\n      #Generate x_batch\n      start = 0  # pointer to where we are in iteration\n      while True:\n         stop = start + self.params['M']\n         diff = stop - train_data.shape[0]\n         if diff &lt;= 0:\n             batch = train_data[start:stop]\n             start += self.params['M']\n         else:\n             batch = np.concatenate((train_data[start:], train_data[:diff]))\n             start = diff\n         yield batch\n       train_data_generator = batch\n\n       saver_hook = tf.train.CheckpointSaverHook(\n                                                 checkpoint_dir=FLAGS.model_path,\n                                                 save_steps=100,\n                                                 saver=tf.train.Saver(),\n                                                 checkpoint_basename='model.ckpt',\n                                                 scaffold=None\n                                                 )\n\n       hooks = [saver_hook]\n\n       with tf.train.MonitoredTrainingSession(\n                                              master=server.target,\n                                              is_chief=(FLAGS.task_index == 0),\n                                              checkpoint_dir=FLAGS.model_path,\n                                              hooks=hooks,\n                                              config= tf.ConfigProto(allow_soft_placement=True,\n                                                                     log_device_placement=True),\n                                              save_summaries_steps=None,\n                                              save_summaries_secs=None\n                                              ) as sess:\n           sess.run(self.init)\n           # sess.run(self.inference_init)\n           # self.inference.initialize(optimizer=self.optimizer)\n           n_iter_per_epoch = np.ceil(\n               train_data.shape[0] / self.params['M']).astype(int)\n\n           for epoch in xrange(1, self.params['n_epoch'] + 1):\n               print \"Epoch: {0}\".format(epoch)\n               avg_loss = 0.0\n               pbar = Progbar(n_iter_per_epoch)\n               for t in xrange(1, n_iter_per_epoch + 1):\n                   pbar.update(t)\n                   x_batch = next(train_data_generator)\n                   info_dict = self.inference.update(\n                       feed_dict={self.x_ph: x_batch})\n                   avg_loss += info_dict['loss']\n               avg_loss /= n_iter_per_epoch\n               avg_loss /= self.params['M']\n               print \"-log p(x) &lt;= {:0.3f}\".format(avg_loss)\n           print \"Done training the model.\"\n           \n if __name__ == \"__main__\":\n     vae = VAE()\n     vae.train(data)\n\n</code></pre>\n<p>The error stack is as follows :</p>\n<pre><code>2018-11-12 15:46:11.525824: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-11-12 15:46:11.527300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2222}\n2018-11-12 15:46:11.527310: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223}\n2018-11-12 15:46:11.527775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\n51 features are used.\n2018-11-12 15:46:16.294466: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session e39a9b7d7a1216dc with config:\nEpoch: 1\n 1/10 [ 10%] \u2588\u2588\u2588                            ETA: 0sTraceback (most recent call last):\n  File \"dist_vae.py\", line 357, in &lt;module&gt;\n    vae.train(data)\n  File \"dist_vae.py\", line 220, in train\n    feed_dict={self.x_ph: x_batch})\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 154, in update\n    _, t, loss = sess.run([self.train, self.increment_t, self.loss], feed_dict)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\n\nCaused by op u'optimizer/dense_1/bias/RMSProp_1', defined at:\n  File \"dist_vae.py\", line 356, in &lt;module&gt;\n    vae = VAE(filepath, params, n_features)\n  File \"dist_vae.py\", line 173, in __init__\n    optimizer=self.optimizer, global_step = self.global_step, logdir='log')\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/klqp.py\", line 110, in initialize\n    return super(KLqp, self).initialize(*args, **kwargs)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 121, in initialize\n    global_step=global_step)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 520, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/rmsprop.py\", line 115, in _create_slots\n    self._zeros_slot(v, \"momentum\", self._name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 910, in _zeros_slot\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\n    dtype)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 800, in _get_single_variable\n    use_resource=use_resource)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\n    use_resource=use_resource)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in &lt;lambda&gt;\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n    name=name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 134, in variable_op_v2\n    shared_name=shared_name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 1043, in _variable_v2\n    shared_name=shared_name, name=name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\n\n</code></pre>\n<p>When config= tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) :</p>\n<pre><code>optimizer/RMSProp/value: (Const): /job:ps/replica:0/task:0/device:CPU:0\n2018-11-12 15:50:32.823393: I tensorflow/core/common_runtime/placer.cc:875] optimizer/RMSProp/value: (Const)/job:ps/replica:0/task:0/device:CPU:0\nglobal_step/Initializer/Const: (Const): /job:ps/replica:0/task:0/device:CPU:0\n\n</code></pre>\n<p>Thanks for the help! :)</p>", "body_text": "The error is when I am trying to run distributed training with edward. Below is the code :\nparameter_servers = [\"localhost:2222\"]\nworkers = [\"localhost:2223\"]\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# start a server for a specific task\nserver = tf.train.Server(\n     cluster,\n     job_name=FLAGS.job_name,\n     task_index=FLAGS.task_index)\n     \nclass VAE(object):\n   def __init__():\n       # MODEL\n       self.n_features = n_features\n       self.params = {\n                'M': 2048,\n                'd': 5,\n                'n_epoch': 2,\n                'hidden_d': [25, 5],\n                'learning_rate': 0.01\n            }\n       self.saved_dir_path = dir_path\n       self.ckpt_path = os.path.join(self.saved_dir_path,\n                                     'checkpointFiles/') + 'model.ckpt'\n       # distributed training\n       if FLAGS.job_name == \"ps\":\n           server.join()\n       elif FLAGS.job_name == \"worker\":\n           # Between-graph replication\n           with tf.device(tf.train.replica_device_setter(\n                   worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                   cluster=cluster)):\n               self.global_step = tf.get_variable(\n                                                    'global_step',\n                                                    [],\n                                                    dtype=tf.int64,\n                                                    initializer=tf.constant_initializer(0),\n                                                    trainable=False)\n               self.z = Normal(\n                   loc=tf.zeros([self.params['M'], self.params['d']]),\n                   scale=tf.ones([self.params['M'], self.params['d']]))\n               # self.hidden0 = tf.layers.dense(\n               self.hidden = tf.layers.dense(\n                   self.z, self.params['hidden_d'][0], activation=tf.nn.relu)\n               if len(self.params['hidden_d']) > 1:\n                   # for i in xrange(1, len(params[hidden_d])):\n                   #     self.__dict__['hidden' + str(i)] = \\\n                   #         tf.layers.dense(\n                   #             self.__dict__['hidden' + str(i-1)],\n                   #             self.params['hidden_d'][i],\n                   #             activation=tf.nn.relu)\n                   for i in xrange(1, len(params['hidden_d'])):\n                       self.hidden = \\\n                           tf.layers.dense(\n                               self.hidden,\n                               self.params['hidden_d'][i],\n                               activation=tf.nn.relu)\n               self.x = Bernoulli(\n                   logits=tf.layers.dense(\n                       # self.__dict__['hidden' + str(len(params['hidden_d'] - 1))],\n                       self.hidden,\n                       self.n_features), dtype=tf.float64)\n\n               # INFERENCE\n               self.x_ph = tf.placeholder(dtype=tf.float64,shape=[None, self.n_features])\n               self.hidden = tf.layers.dense(\n                   tf.cast(self.x_ph, tf.float32),\n                   self.params['hidden_d'][-1],\n                   activation=tf.nn.relu)\n               if len(self.params['hidden_d']) > 1:\n                   for i in xrange(1, len(params['hidden_d'])):\n                       j = -(1+i)\n                       self.hidden = \\\n                           tf.layers.dense(\n                               self.hidden,\n                               self.params['hidden_d'][j],\n                               activation=tf.nn.relu)\n               self.qz = Normal(\n                   loc=tf.layers.dense(self.hidden, self.params['d']),\n                   scale=tf.layers.dense(\n                       self.hidden, self.params['d'], activation=tf.nn.softplus))\n               self.x_avg = Bernoulli(\n                   logits=tf.reduce_mean(self.x.parameters['logits'], 0),\n                   name='x_avg')\n               self.log_likli = tf.reduce_mean(self.x_avg.log_prob(self.x_ph), 1)\n               self.optimizer = tf.train.RMSPropOptimizer(\n                   self.params['learning_rate'], epsilon=1.0)\n               # self.\n               self.inference = ed.KLqp({self.z: self.qz}, data={self.x: self.x_ph})\n               self.inference_init = self.inference.initialize(\n                   optimizer=self.optimizer, global_step = self.global_step, logdir='log')\n               self.init = tf.global_variables_initializer()\n               self.saver = tf.train.Saver()\n\n   def train(self, train_data):\n      #Generate x_batch\n      start = 0  # pointer to where we are in iteration\n      while True:\n         stop = start + self.params['M']\n         diff = stop - train_data.shape[0]\n         if diff <= 0:\n             batch = train_data[start:stop]\n             start += self.params['M']\n         else:\n             batch = np.concatenate((train_data[start:], train_data[:diff]))\n             start = diff\n         yield batch\n       train_data_generator = batch\n\n       saver_hook = tf.train.CheckpointSaverHook(\n                                                 checkpoint_dir=FLAGS.model_path,\n                                                 save_steps=100,\n                                                 saver=tf.train.Saver(),\n                                                 checkpoint_basename='model.ckpt',\n                                                 scaffold=None\n                                                 )\n\n       hooks = [saver_hook]\n\n       with tf.train.MonitoredTrainingSession(\n                                              master=server.target,\n                                              is_chief=(FLAGS.task_index == 0),\n                                              checkpoint_dir=FLAGS.model_path,\n                                              hooks=hooks,\n                                              config= tf.ConfigProto(allow_soft_placement=True,\n                                                                     log_device_placement=True),\n                                              save_summaries_steps=None,\n                                              save_summaries_secs=None\n                                              ) as sess:\n           sess.run(self.init)\n           # sess.run(self.inference_init)\n           # self.inference.initialize(optimizer=self.optimizer)\n           n_iter_per_epoch = np.ceil(\n               train_data.shape[0] / self.params['M']).astype(int)\n\n           for epoch in xrange(1, self.params['n_epoch'] + 1):\n               print \"Epoch: {0}\".format(epoch)\n               avg_loss = 0.0\n               pbar = Progbar(n_iter_per_epoch)\n               for t in xrange(1, n_iter_per_epoch + 1):\n                   pbar.update(t)\n                   x_batch = next(train_data_generator)\n                   info_dict = self.inference.update(\n                       feed_dict={self.x_ph: x_batch})\n                   avg_loss += info_dict['loss']\n               avg_loss /= n_iter_per_epoch\n               avg_loss /= self.params['M']\n               print \"-log p(x) <= {:0.3f}\".format(avg_loss)\n           print \"Done training the model.\"\n           \n if __name__ == \"__main__\":\n     vae = VAE()\n     vae.train(data)\n\n\nThe error stack is as follows :\n2018-11-12 15:46:11.525824: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-11-12 15:46:11.527300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\n2018-11-12 15:46:11.527310: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223}\n2018-11-12 15:46:11.527775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\n51 features are used.\n2018-11-12 15:46:16.294466: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session e39a9b7d7a1216dc with config:\nEpoch: 1\n 1/10 [ 10%] \u2588\u2588\u2588                            ETA: 0sTraceback (most recent call last):\n  File \"dist_vae.py\", line 357, in <module>\n    vae.train(data)\n  File \"dist_vae.py\", line 220, in train\n    feed_dict={self.x_ph: x_batch})\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 154, in update\n    _, t, loss = sess.run([self.train, self.increment_t, self.loss], feed_dict)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\n\nCaused by op u'optimizer/dense_1/bias/RMSProp_1', defined at:\n  File \"dist_vae.py\", line 356, in <module>\n    vae = VAE(filepath, params, n_features)\n  File \"dist_vae.py\", line 173, in __init__\n    optimizer=self.optimizer, global_step = self.global_step, logdir='log')\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/klqp.py\", line 110, in initialize\n    return super(KLqp, self).initialize(*args, **kwargs)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 121, in initialize\n    global_step=global_step)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 520, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/rmsprop.py\", line 115, in _create_slots\n    self._zeros_slot(v, \"momentum\", self._name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 910, in _zeros_slot\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\n    dtype)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 800, in _get_single_variable\n    use_resource=use_resource)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\n    use_resource=use_resource)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\n    constraint=constraint)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n    name=name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 134, in variable_op_v2\n    shared_name=shared_name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 1043, in _variable_v2\n    shared_name=shared_name, name=name)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\n\n\nWhen config= tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) :\noptimizer/RMSProp/value: (Const): /job:ps/replica:0/task:0/device:CPU:0\n2018-11-12 15:50:32.823393: I tensorflow/core/common_runtime/placer.cc:875] optimizer/RMSProp/value: (Const)/job:ps/replica:0/task:0/device:CPU:0\nglobal_step/Initializer/Const: (Const): /job:ps/replica:0/task:0/device:CPU:0\n\n\nThanks for the help! :)", "body": "The error is when I am trying to run distributed training with edward. Below is the code : \r\n\r\n```\r\nparameter_servers = [\"localhost:2222\"]\r\nworkers = [\"localhost:2223\"]\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# start a server for a specific task\r\nserver = tf.train.Server(\r\n     cluster,\r\n     job_name=FLAGS.job_name,\r\n     task_index=FLAGS.task_index)\r\n     \r\nclass VAE(object):\r\n   def __init__():\r\n       # MODEL\r\n       self.n_features = n_features\r\n       self.params = {\r\n                'M': 2048,\r\n                'd': 5,\r\n                'n_epoch': 2,\r\n                'hidden_d': [25, 5],\r\n                'learning_rate': 0.01\r\n            }\r\n       self.saved_dir_path = dir_path\r\n       self.ckpt_path = os.path.join(self.saved_dir_path,\r\n                                     'checkpointFiles/') + 'model.ckpt'\r\n       # distributed training\r\n       if FLAGS.job_name == \"ps\":\r\n           server.join()\r\n       elif FLAGS.job_name == \"worker\":\r\n           # Between-graph replication\r\n           with tf.device(tf.train.replica_device_setter(\r\n                   worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                   cluster=cluster)):\r\n               self.global_step = tf.get_variable(\r\n                                                    'global_step',\r\n                                                    [],\r\n                                                    dtype=tf.int64,\r\n                                                    initializer=tf.constant_initializer(0),\r\n                                                    trainable=False)\r\n               self.z = Normal(\r\n                   loc=tf.zeros([self.params['M'], self.params['d']]),\r\n                   scale=tf.ones([self.params['M'], self.params['d']]))\r\n               # self.hidden0 = tf.layers.dense(\r\n               self.hidden = tf.layers.dense(\r\n                   self.z, self.params['hidden_d'][0], activation=tf.nn.relu)\r\n               if len(self.params['hidden_d']) > 1:\r\n                   # for i in xrange(1, len(params[hidden_d])):\r\n                   #     self.__dict__['hidden' + str(i)] = \\\r\n                   #         tf.layers.dense(\r\n                   #             self.__dict__['hidden' + str(i-1)],\r\n                   #             self.params['hidden_d'][i],\r\n                   #             activation=tf.nn.relu)\r\n                   for i in xrange(1, len(params['hidden_d'])):\r\n                       self.hidden = \\\r\n                           tf.layers.dense(\r\n                               self.hidden,\r\n                               self.params['hidden_d'][i],\r\n                               activation=tf.nn.relu)\r\n               self.x = Bernoulli(\r\n                   logits=tf.layers.dense(\r\n                       # self.__dict__['hidden' + str(len(params['hidden_d'] - 1))],\r\n                       self.hidden,\r\n                       self.n_features), dtype=tf.float64)\r\n\r\n               # INFERENCE\r\n               self.x_ph = tf.placeholder(dtype=tf.float64,shape=[None, self.n_features])\r\n               self.hidden = tf.layers.dense(\r\n                   tf.cast(self.x_ph, tf.float32),\r\n                   self.params['hidden_d'][-1],\r\n                   activation=tf.nn.relu)\r\n               if len(self.params['hidden_d']) > 1:\r\n                   for i in xrange(1, len(params['hidden_d'])):\r\n                       j = -(1+i)\r\n                       self.hidden = \\\r\n                           tf.layers.dense(\r\n                               self.hidden,\r\n                               self.params['hidden_d'][j],\r\n                               activation=tf.nn.relu)\r\n               self.qz = Normal(\r\n                   loc=tf.layers.dense(self.hidden, self.params['d']),\r\n                   scale=tf.layers.dense(\r\n                       self.hidden, self.params['d'], activation=tf.nn.softplus))\r\n               self.x_avg = Bernoulli(\r\n                   logits=tf.reduce_mean(self.x.parameters['logits'], 0),\r\n                   name='x_avg')\r\n               self.log_likli = tf.reduce_mean(self.x_avg.log_prob(self.x_ph), 1)\r\n               self.optimizer = tf.train.RMSPropOptimizer(\r\n                   self.params['learning_rate'], epsilon=1.0)\r\n               # self.\r\n               self.inference = ed.KLqp({self.z: self.qz}, data={self.x: self.x_ph})\r\n               self.inference_init = self.inference.initialize(\r\n                   optimizer=self.optimizer, global_step = self.global_step, logdir='log')\r\n               self.init = tf.global_variables_initializer()\r\n               self.saver = tf.train.Saver()\r\n\r\n   def train(self, train_data):\r\n      #Generate x_batch\r\n      start = 0  # pointer to where we are in iteration\r\n      while True:\r\n         stop = start + self.params['M']\r\n         diff = stop - train_data.shape[0]\r\n         if diff <= 0:\r\n             batch = train_data[start:stop]\r\n             start += self.params['M']\r\n         else:\r\n             batch = np.concatenate((train_data[start:], train_data[:diff]))\r\n             start = diff\r\n         yield batch\r\n       train_data_generator = batch\r\n\r\n       saver_hook = tf.train.CheckpointSaverHook(\r\n                                                 checkpoint_dir=FLAGS.model_path,\r\n                                                 save_steps=100,\r\n                                                 saver=tf.train.Saver(),\r\n                                                 checkpoint_basename='model.ckpt',\r\n                                                 scaffold=None\r\n                                                 )\r\n\r\n       hooks = [saver_hook]\r\n\r\n       with tf.train.MonitoredTrainingSession(\r\n                                              master=server.target,\r\n                                              is_chief=(FLAGS.task_index == 0),\r\n                                              checkpoint_dir=FLAGS.model_path,\r\n                                              hooks=hooks,\r\n                                              config= tf.ConfigProto(allow_soft_placement=True,\r\n                                                                     log_device_placement=True),\r\n                                              save_summaries_steps=None,\r\n                                              save_summaries_secs=None\r\n                                              ) as sess:\r\n           sess.run(self.init)\r\n           # sess.run(self.inference_init)\r\n           # self.inference.initialize(optimizer=self.optimizer)\r\n           n_iter_per_epoch = np.ceil(\r\n               train_data.shape[0] / self.params['M']).astype(int)\r\n\r\n           for epoch in xrange(1, self.params['n_epoch'] + 1):\r\n               print \"Epoch: {0}\".format(epoch)\r\n               avg_loss = 0.0\r\n               pbar = Progbar(n_iter_per_epoch)\r\n               for t in xrange(1, n_iter_per_epoch + 1):\r\n                   pbar.update(t)\r\n                   x_batch = next(train_data_generator)\r\n                   info_dict = self.inference.update(\r\n                       feed_dict={self.x_ph: x_batch})\r\n                   avg_loss += info_dict['loss']\r\n               avg_loss /= n_iter_per_epoch\r\n               avg_loss /= self.params['M']\r\n               print \"-log p(x) <= {:0.3f}\".format(avg_loss)\r\n           print \"Done training the model.\"\r\n           \r\n if __name__ == \"__main__\":\r\n     vae = VAE()\r\n     vae.train(data)\r\n\r\n```\r\n\r\nThe error stack is as follows : \r\n\r\n```\r\n2018-11-12 15:46:11.525824: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-11-12 15:46:11.527300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-11-12 15:46:11.527310: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223}\r\n2018-11-12 15:46:11.527775: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\r\n51 features are used.\r\n2018-11-12 15:46:16.294466: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session e39a9b7d7a1216dc with config:\r\nEpoch: 1\r\n 1/10 [ 10%] \u2588\u2588\u2588                            ETA: 0sTraceback (most recent call last):\r\n  File \"dist_vae.py\", line 357, in <module>\r\n    vae.train(data)\r\n  File \"dist_vae.py\", line 220, in train\r\n    feed_dict={self.x_ph: x_batch})\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 154, in update\r\n    _, t, loss = sess.run([self.train, self.increment_t, self.loss], feed_dict)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\r\n\r\nCaused by op u'optimizer/dense_1/bias/RMSProp_1', defined at:\r\n  File \"dist_vae.py\", line 356, in <module>\r\n    vae = VAE(filepath, params, n_features)\r\n  File \"dist_vae.py\", line 173, in __init__\r\n    optimizer=self.optimizer, global_step = self.global_step, logdir='log')\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/klqp.py\", line 110, in initialize\r\n    return super(KLqp, self).initialize(*args, **kwargs)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/edward/inferences/variational_inference.py\", line 121, in initialize\r\n    global_step=global_step)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 520, in apply_gradients\r\n    self._create_slots([_get_variable_for(v) for v in var_list])\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/rmsprop.py\", line 115, in _create_slots\r\n    self._zeros_slot(v, \"momentum\", self._name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 910, in _zeros_slot\r\n    named_slots[_var_key(var)] = slot_creator.create_zeros_slot(var, op_name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\r\n    colocate_with_primary=colocate_with_primary)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 148, in create_slot_with_initializer\r\n    dtype)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\r\n    validate_shape=validate_shape)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 800, in _get_single_variable\r\n    use_resource=use_resource)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\r\n    use_resource=use_resource)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\r\n    constraint=constraint)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\r\n    name=name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 134, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 1043, in _variable_v2\r\n    shared_name=shared_name, name=name)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shaarvanikavula/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'optimizer/dense_1/bias/RMSProp_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\r\n\t [[Node: optimizer/dense_1/bias/RMSProp_1 = VariableV2[_class=[\"loc:@dense_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[5], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\r\n\r\n```\r\n\r\nWhen config= tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) : \r\n\r\n```\r\noptimizer/RMSProp/value: (Const): /job:ps/replica:0/task:0/device:CPU:0\r\n2018-11-12 15:50:32.823393: I tensorflow/core/common_runtime/placer.cc:875] optimizer/RMSProp/value: (Const)/job:ps/replica:0/task:0/device:CPU:0\r\nglobal_step/Initializer/Const: (Const): /job:ps/replica:0/task:0/device:CPU:0\r\n\r\n```\r\n\r\nThanks for the help! :) "}