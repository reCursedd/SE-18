{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/327531678", "html_url": "https://github.com/tensorflow/tensorflow/issues/12851#issuecomment-327531678", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12851", "id": 327531678, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzUzMTY3OA==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-06T16:02:33Z", "updated_at": "2017-09-06T16:02:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> Thanks for the fast and elaborate response! :)</p>\n<p>First of all, let me provide a simplified example of the use case I have in mind. I want to create a functional API for constructing and training networks. I'm thinking of having a notion of <code>Layer[T, R]</code>, which maps from input type <code>T</code> to output type <code>R</code>. A layer is a function that given a <code>T</code> will construct relevant ops and return an <code>R</code>. These types could be tuples of op outputs, or other arbitrary structures for that matter. Layers can be combined in various ways (e.g., composed or mapped) to create new layers. A model is then defined using:</p>\n<ul>\n<li>Input layer that returns type <code>I</code></li>\n<li>Inference layer that goes from <code>I</code> to <code>P</code> (think of this as the main model -- the prediction function -- e.g., a multi-layer perceptron)</li>\n<li>Train input layer that returns type <code>IT</code> (optional -- e.g., labels used when training a classifier)</li>\n<li>Train processing layer that goes from <code>IT</code> to <code>PT</code> (optional -- e.g., one-hot encoding of labels)</li>\n<li>Loss layer that goes from <code>(P, PT)</code> to a scalar tensor (optional -- e.g., cross-entropy)</li>\n<li>...<br>\nThe optional components are only used when training so if you create a model that requires no training, you can skip them. The model defines <code>train</code> and <code>predict</code> methods that accept data (train accepts a tuple of <code>(I, IT)</code>, but predict can accept either such a tuple, or simply data of type <code>I</code>).</li>\n</ul>\n<p>Layers do not need to construct any ops before being \"applied\" to some input. Before creating a model, no interaction with TensorFlow is necessary and no ops are created. The TF graph is created, populated, and managed by the model. So, while initializing, the model will create a graph and use it for all its operations. I want the model to use the same graph for both training and prediction and I want to avoid adding redundant ops to that graph as much as possible. I think it's more elegant to limit graph management to the model so the user never has to deal with that (I've noticed it often confuses many people using TF).</p>\n<p><strong>This point is important to our discussion:</strong> Since I want to have a fixed graph, I need the inputs of that graph to be fixed at construction time and I want to use the dataset API iterators for that to allow flexibility with respect to how users provide data for training and/or prediction. Given this constraint, at graph construction time I need to either use two iterators (one for type <code>I</code> and one for <code>IT</code>), or an iterator over tuples <code>(I, IT)</code>. <em>If I use an iterator over tuples (which makes sense given that we want to advance the iterators at the same time), then how do I initialize that iterator with a dataset of type <code>I</code> for the model predict method?</em></p>\n<p>I hope that this simplified description of what I'm hoping to achieve makes sense.</p>\n<p>With respect to your points, I have a couple of comments/questions:</p>\n<ul>\n<li>What you mention about the unzip operation and the semantics of traversing the unzipped iterators makes sense and I hadn't really thought about it. I always had the synchronized advancement of both in my mind. In this case, not unzipping them also makes sense as it incorporates that constraint in the structure.</li>\n<li>In general, I'm a bit unclear as to when the iterators are advanced. The simple use cases makes sense, but what about the following example: we have a model that has multiple entry points for a single input (i.e., multiple ops consuming the same input). So, during a single <code>Session.run</code> call, multiple ops will consume the same input provided by a single iterator. Will the iterator only advance once per <code>Session.run</code> call, or will it advance once for every single op that consumes its output? If the latter happens, then how can we enforce the once per <code>Session.run</code> call behavior?</li>\n</ul>\n<p>Thank you for your response and I'm sorry for the super long post. I just want to try and make the setting clear so the use case may make sense. I do realize now though that unzipping may not be the right approach and I might be missing some simple solution to my example. :)</p>", "body_text": "@mrry Thanks for the fast and elaborate response! :)\nFirst of all, let me provide a simplified example of the use case I have in mind. I want to create a functional API for constructing and training networks. I'm thinking of having a notion of Layer[T, R], which maps from input type T to output type R. A layer is a function that given a T will construct relevant ops and return an R. These types could be tuples of op outputs, or other arbitrary structures for that matter. Layers can be combined in various ways (e.g., composed or mapped) to create new layers. A model is then defined using:\n\nInput layer that returns type I\nInference layer that goes from I to P (think of this as the main model -- the prediction function -- e.g., a multi-layer perceptron)\nTrain input layer that returns type IT (optional -- e.g., labels used when training a classifier)\nTrain processing layer that goes from IT to PT (optional -- e.g., one-hot encoding of labels)\nLoss layer that goes from (P, PT) to a scalar tensor (optional -- e.g., cross-entropy)\n...\nThe optional components are only used when training so if you create a model that requires no training, you can skip them. The model defines train and predict methods that accept data (train accepts a tuple of (I, IT), but predict can accept either such a tuple, or simply data of type I).\n\nLayers do not need to construct any ops before being \"applied\" to some input. Before creating a model, no interaction with TensorFlow is necessary and no ops are created. The TF graph is created, populated, and managed by the model. So, while initializing, the model will create a graph and use it for all its operations. I want the model to use the same graph for both training and prediction and I want to avoid adding redundant ops to that graph as much as possible. I think it's more elegant to limit graph management to the model so the user never has to deal with that (I've noticed it often confuses many people using TF).\nThis point is important to our discussion: Since I want to have a fixed graph, I need the inputs of that graph to be fixed at construction time and I want to use the dataset API iterators for that to allow flexibility with respect to how users provide data for training and/or prediction. Given this constraint, at graph construction time I need to either use two iterators (one for type I and one for IT), or an iterator over tuples (I, IT). If I use an iterator over tuples (which makes sense given that we want to advance the iterators at the same time), then how do I initialize that iterator with a dataset of type I for the model predict method?\nI hope that this simplified description of what I'm hoping to achieve makes sense.\nWith respect to your points, I have a couple of comments/questions:\n\nWhat you mention about the unzip operation and the semantics of traversing the unzipped iterators makes sense and I hadn't really thought about it. I always had the synchronized advancement of both in my mind. In this case, not unzipping them also makes sense as it incorporates that constraint in the structure.\nIn general, I'm a bit unclear as to when the iterators are advanced. The simple use cases makes sense, but what about the following example: we have a model that has multiple entry points for a single input (i.e., multiple ops consuming the same input). So, during a single Session.run call, multiple ops will consume the same input provided by a single iterator. Will the iterator only advance once per Session.run call, or will it advance once for every single op that consumes its output? If the latter happens, then how can we enforce the once per Session.run call behavior?\n\nThank you for your response and I'm sorry for the super long post. I just want to try and make the setting clear so the use case may make sense. I do realize now though that unzipping may not be the right approach and I might be missing some simple solution to my example. :)", "body": "@mrry Thanks for the fast and elaborate response! :)\r\n\r\nFirst of all, let me provide a simplified example of the use case I have in mind. I want to create a functional API for constructing and training networks. I'm thinking of having a notion of `Layer[T, R]`, which maps from input type `T` to output type `R`. A layer is a function that given a `T` will construct relevant ops and return an `R`. These types could be tuples of op outputs, or other arbitrary structures for that matter. Layers can be combined in various ways (e.g., composed or mapped) to create new layers. A model is then defined using:\r\n- Input layer that returns type `I`\r\n- Inference layer that goes from `I` to `P` (think of this as the main model -- the prediction function -- e.g., a multi-layer perceptron)\r\n- Train input layer that returns type `IT` (optional -- e.g., labels used when training a classifier)\r\n- Train processing layer that goes from `IT` to `PT` (optional -- e.g., one-hot encoding of labels)\r\n- Loss layer that goes from `(P, PT)` to a scalar tensor (optional -- e.g., cross-entropy)\r\n- ...\r\nThe optional components are only used when training so if you create a model that requires no training, you can skip them. The model defines `train` and `predict` methods that accept data (train accepts a tuple of `(I, IT)`, but predict can accept either such a tuple, or simply data of type `I`).\r\n\r\nLayers do not need to construct any ops before being \"applied\" to some input. Before creating a model, no interaction with TensorFlow is necessary and no ops are created. The TF graph is created, populated, and managed by the model. So, while initializing, the model will create a graph and use it for all its operations. I want the model to use the same graph for both training and prediction and I want to avoid adding redundant ops to that graph as much as possible. I think it's more elegant to limit graph management to the model so the user never has to deal with that (I've noticed it often confuses many people using TF).\r\n\r\n**This point is important to our discussion:** Since I want to have a fixed graph, I need the inputs of that graph to be fixed at construction time and I want to use the dataset API iterators for that to allow flexibility with respect to how users provide data for training and/or prediction. Given this constraint, at graph construction time I need to either use two iterators (one for type `I` and one for `IT`), or an iterator over tuples `(I, IT)`. _If I use an iterator over tuples (which makes sense given that we want to advance the iterators at the same time), then how do I initialize that iterator with a dataset of type `I` for the model predict method?_\r\n\r\nI hope that this simplified description of what I'm hoping to achieve makes sense.\r\n\r\nWith respect to your points, I have a couple of comments/questions:\r\n- What you mention about the unzip operation and the semantics of traversing the unzipped iterators makes sense and I hadn't really thought about it. I always had the synchronized advancement of both in my mind. In this case, not unzipping them also makes sense as it incorporates that constraint in the structure.\r\n- In general, I'm a bit unclear as to when the iterators are advanced. The simple use cases makes sense, but what about the following example: we have a model that has multiple entry points for a single input (i.e., multiple ops consuming the same input). So, during a single `Session.run` call, multiple ops will consume the same input provided by a single iterator. Will the iterator only advance once per `Session.run` call, or will it advance once for every single op that consumes its output? If the latter happens, then how can we enforce the once per `Session.run` call behavior?\r\n\r\nThank you for your response and I'm sorry for the super long post. I just want to try and make the setting clear so the use case may make sense. I do realize now though that unzipping may not be the right approach and I might be missing some simple solution to my example. :)"}