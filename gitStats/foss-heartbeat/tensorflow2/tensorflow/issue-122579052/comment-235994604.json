{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235994604", "html_url": "https://github.com/tensorflow/tensorflow/issues/526#issuecomment-235994604", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/526", "id": 235994604, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTk5NDYwNA==", "user": {"login": "danpovey", "id": 3298747, "node_id": "MDQ6VXNlcjMyOTg3NDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3298747?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danpovey", "html_url": "https://github.com/danpovey", "followers_url": "https://api.github.com/users/danpovey/followers", "following_url": "https://api.github.com/users/danpovey/following{/other_user}", "gists_url": "https://api.github.com/users/danpovey/gists{/gist_id}", "starred_url": "https://api.github.com/users/danpovey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danpovey/subscriptions", "organizations_url": "https://api.github.com/users/danpovey/orgs", "repos_url": "https://api.github.com/users/danpovey/repos", "events_url": "https://api.github.com/users/danpovey/events{/privacy}", "received_events_url": "https://api.github.com/users/danpovey/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-28T19:08:59Z", "updated_at": "2016-07-28T19:08:59Z", "author_association": "NONE", "body_html": "<p>Tensorflow guys, if I were you I would change the assert statement that<br>\nfails here:</p>\n<p>cuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS ==<br>\ndynload::cuCtxSetCurrent(cuda_context-&gt;context()) (0 vs. 216)</p>\n<p>to some code that prints out the text form of the CUDA exit code, and maybe<br>\nfor good measure tries to invoke nvidia-smi to get extra information.  We<br>\nfound this necessary in Kaldi in order to ensure that when there are<br>\nproblems, all the information needed is in the log.<br>\nDan</p>\n<p>On Thu, Jul 28, 2016 at 11:52 AM, Mark Whitney <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>I am seeing a similar issue on 0.9 compiled from source, HEAD:</p>\n<p>commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/554ddd9ad2d4abad5a9a31f2d245f0b1012f0d10/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/554ddd9ad2d4abad5a9a31f2d245f0b1012f0d10\"><tt>554ddd9</tt></a><br>\nMerge: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/89e1cc59681b78e8193f899dca16474c19a7fc5b/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/89e1cc59681b78e8193f899dca16474c19a7fc5b\"><tt>89e1cc5</tt></a> <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/a0745a7ce2b1b6c6a7f43ca2359d6bceefd2f909/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/a0745a7ce2b1b6c6a7f43ca2359d6bceefd2f909\"><tt>a0745a7</tt></a><br>\nAuthor: yifeif <a href=\"mailto:fengyifei2026@gmail.com\">fengyifei2026@gmail.com</a><br>\nDate:   Tue Jul 26 16:17:21 2016 -0700</p>\n<p>I am on a shared cluster with a scheduler, so I should have exclusive<br>\naccess to the node during my time slice. It looks like exclusive mode is<br>\nset, but there are no running processes at the time time I try to use it:</p>\n<p><a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: | NVIDIA-SMI 352.39     Driver Version: 352.39         |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: |-------------------------------+----------------------+----------------------+<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: |===============================+======================+======================|<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: |   0  Tesla K40m          Off  | 0000:08:00.0     Off |                    0 |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: | N/A   25C    P8    19W / 235W |     23MiB / 11519MiB |      0%    E. Thread |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: +-------------------------------+----------------------+----------------------+<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>:<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: +-----------------------------------------------------------------------------+<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: | Processes:                                                       GPU Memory |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: |  GPU       PID  Type  Process name                               Usage      |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: |=============================================================================|<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: |  No running processes found                                                 |<br>\n<a href=\"+------------------------------------------------------+\">2016-07-28T17:59:19Z</a>: +-----------------------------------------------------------------------------+<br>\n[2016-07-28T17:59:21Z]: Using TensorFlow backend.<br>\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally<br>\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally<br>\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally<br>\n[2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally<br>\n[2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally<br>\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:<br>\n[2016-07-28T17:59:34Z]: name: Tesla K40m<br>\n[2016-07-28T17:59:34Z]: major: 3 minor: 5 memoryClockRate (GHz) 0.745<br>\n[2016-07-28T17:59:34Z]: pciBusID 0000:08:00.0<br>\n[2016-07-28T17:59:34Z]: Total memory: 11.25GiB<br>\n[2016-07-28T17:59:34Z]: Free memory: 11.15GiB<br>\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0<br>\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y<br>\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)<br>\n[2016-07-28T17:59:34Z]: F tensorflow/stream_executor/cuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(cuda_context-&gt;context()) (0 vs. 216)<br>\n[2016-07-28T17:59:34Z]: X_train shape: (60000, 1, 28, 28)<br>\n[2016-07-28T17:59:34Z]: 60000 train samples<br>\n[2016-07-28T17:59:34Z]: 10000 test samples<br>\n[2016-07-28T17:59:41Z]: /tmp/wrapper5271742824235601482.sh: line 12: 61655 Aborted                 (core dumped) python mnist_cnn.py<br>\n[2016-07-28T17:59:41Z]: Exited with code 0</p>\n<p>\u2014<br>\nYou are receiving this because you commented.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"122579052\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/526\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/526/hovercard?comment_id=235989554&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/526#issuecomment-235989554\">#526 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ADJVu5zDQGnFRHmS-sFWKkA2dnlN3v1Cks5qaPqBgaJpZM4G2yCN\">https://github.com/notifications/unsubscribe-auth/ADJVu5zDQGnFRHmS-sFWKkA2dnlN3v1Cks5qaPqBgaJpZM4G2yCN</a><br>\n.</p>\n</blockquote>", "body_text": "Tensorflow guys, if I were you I would change the assert statement that\nfails here:\ncuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS ==\ndynload::cuCtxSetCurrent(cuda_context->context()) (0 vs. 216)\nto some code that prints out the text form of the CUDA exit code, and maybe\nfor good measure tries to invoke nvidia-smi to get extra information.  We\nfound this necessary in Kaldi in order to ensure that when there are\nproblems, all the information needed is in the log.\nDan\nOn Thu, Jul 28, 2016 at 11:52 AM, Mark Whitney notifications@github.com\nwrote:\n\nI am seeing a similar issue on 0.9 compiled from source, HEAD:\ncommit 554ddd9\nMerge: 89e1cc5 a0745a7\nAuthor: yifeif fengyifei2026@gmail.com\nDate:   Tue Jul 26 16:17:21 2016 -0700\nI am on a shared cluster with a scheduler, so I should have exclusive\naccess to the node during my time slice. It looks like exclusive mode is\nset, but there are no running processes at the time time I try to use it:\n2016-07-28T17:59:19Z: | NVIDIA-SMI 352.39     Driver Version: 352.39         |\n2016-07-28T17:59:19Z: |-------------------------------+----------------------+----------------------+\n2016-07-28T17:59:19Z: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n2016-07-28T17:59:19Z: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n2016-07-28T17:59:19Z: |===============================+======================+======================|\n2016-07-28T17:59:19Z: |   0  Tesla K40m          Off  | 0000:08:00.0     Off |                    0 |\n2016-07-28T17:59:19Z: | N/A   25C    P8    19W / 235W |     23MiB / 11519MiB |      0%    E. Thread |\n2016-07-28T17:59:19Z: +-------------------------------+----------------------+----------------------+\n2016-07-28T17:59:19Z:\n2016-07-28T17:59:19Z: +-----------------------------------------------------------------------------+\n2016-07-28T17:59:19Z: | Processes:                                                       GPU Memory |\n2016-07-28T17:59:19Z: |  GPU       PID  Type  Process name                               Usage      |\n2016-07-28T17:59:19Z: |=============================================================================|\n2016-07-28T17:59:19Z: |  No running processes found                                                 |\n2016-07-28T17:59:19Z: +-----------------------------------------------------------------------------+\n[2016-07-28T17:59:21Z]: Using TensorFlow backend.\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n[2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\n[2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\n[2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\n[2016-07-28T17:59:34Z]: name: Tesla K40m\n[2016-07-28T17:59:34Z]: major: 3 minor: 5 memoryClockRate (GHz) 0.745\n[2016-07-28T17:59:34Z]: pciBusID 0000:08:00.0\n[2016-07-28T17:59:34Z]: Total memory: 11.25GiB\n[2016-07-28T17:59:34Z]: Free memory: 11.15GiB\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\n[2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n[2016-07-28T17:59:34Z]: F tensorflow/stream_executor/cuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(cuda_context->context()) (0 vs. 216)\n[2016-07-28T17:59:34Z]: X_train shape: (60000, 1, 28, 28)\n[2016-07-28T17:59:34Z]: 60000 train samples\n[2016-07-28T17:59:34Z]: 10000 test samples\n[2016-07-28T17:59:41Z]: /tmp/wrapper5271742824235601482.sh: line 12: 61655 Aborted                 (core dumped) python mnist_cnn.py\n[2016-07-28T17:59:41Z]: Exited with code 0\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n#526 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADJVu5zDQGnFRHmS-sFWKkA2dnlN3v1Cks5qaPqBgaJpZM4G2yCN\n.", "body": "Tensorflow guys, if I were you I would change the assert statement that\nfails here:\n\ncuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS ==\ndynload::cuCtxSetCurrent(cuda_context->context()) (0 vs. 216)\n\nto some code that prints out the text form of the CUDA exit code, and maybe\nfor good measure tries to invoke nvidia-smi to get extra information.  We\nfound this necessary in Kaldi in order to ensure that when there are\nproblems, all the information needed is in the log.\nDan\n\nOn Thu, Jul 28, 2016 at 11:52 AM, Mark Whitney notifications@github.com\nwrote:\n\n> I am seeing a similar issue on 0.9 compiled from source, HEAD:\n> \n> commit 554ddd9ad2d4abad5a9a31f2d245f0b1012f0d10\n> Merge: 89e1cc5 a0745a7\n> Author: yifeif fengyifei2026@gmail.com\n> Date:   Tue Jul 26 16:17:21 2016 -0700\n> \n> I am on a shared cluster with a scheduler, so I should have exclusive\n> access to the node during my time slice. It looks like exclusive mode is\n> set, but there are no running processes at the time time I try to use it:\n> \n> [2016-07-28T17:59:19Z]: +------------------------------------------------------+\n> [2016-07-28T17:59:19Z]: | NVIDIA-SMI 352.39     Driver Version: 352.39         |\n> [2016-07-28T17:59:19Z]: |-------------------------------+----------------------+----------------------+\n> [2016-07-28T17:59:19Z]: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n> [2016-07-28T17:59:19Z]: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n> [2016-07-28T17:59:19Z]: |===============================+======================+======================|\n> [2016-07-28T17:59:19Z]: |   0  Tesla K40m          Off  | 0000:08:00.0     Off |                    0 |\n> [2016-07-28T17:59:19Z]: | N/A   25C    P8    19W / 235W |     23MiB / 11519MiB |      0%    E. Thread |\n> [2016-07-28T17:59:19Z]: +-------------------------------+----------------------+----------------------+\n> [2016-07-28T17:59:19Z]:\n> [2016-07-28T17:59:19Z]: +-----------------------------------------------------------------------------+\n> [2016-07-28T17:59:19Z]: | Processes:                                                       GPU Memory |\n> [2016-07-28T17:59:19Z]: |  GPU       PID  Type  Process name                               Usage      |\n> [2016-07-28T17:59:19Z]: |=============================================================================|\n> [2016-07-28T17:59:19Z]: |  No running processes found                                                 |\n> [2016-07-28T17:59:19Z]: +-----------------------------------------------------------------------------+\n> [2016-07-28T17:59:21Z]: Using TensorFlow backend.\n> [2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n> [2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n> [2016-07-28T17:59:22Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\n> [2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\n> [2016-07-28T17:59:23Z]: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\n> [2016-07-28T17:59:34Z]: name: Tesla K40m\n> [2016-07-28T17:59:34Z]: major: 3 minor: 5 memoryClockRate (GHz) 0.745\n> [2016-07-28T17:59:34Z]: pciBusID 0000:08:00.0\n> [2016-07-28T17:59:34Z]: Total memory: 11.25GiB\n> [2016-07-28T17:59:34Z]: Free memory: 11.15GiB\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\n> [2016-07-28T17:59:34Z]: I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\n> [2016-07-28T17:59:34Z]: F tensorflow/stream_executor/cuda/cuda_driver.cc:395] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(cuda_context->context()) (0 vs. 216)\n> [2016-07-28T17:59:34Z]: X_train shape: (60000, 1, 28, 28)\n> [2016-07-28T17:59:34Z]: 60000 train samples\n> [2016-07-28T17:59:34Z]: 10000 test samples\n> [2016-07-28T17:59:41Z]: /tmp/wrapper5271742824235601482.sh: line 12: 61655 Aborted                 (core dumped) python mnist_cnn.py\n> [2016-07-28T17:59:41Z]: Exited with code 0\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/526#issuecomment-235989554,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADJVu5zDQGnFRHmS-sFWKkA2dnlN3v1Cks5qaPqBgaJpZM4G2yCN\n> .\n"}