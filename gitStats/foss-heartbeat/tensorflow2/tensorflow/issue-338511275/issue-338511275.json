{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20564", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20564/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20564/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20564/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20564", "id": 338511275, "node_id": "MDU6SXNzdWUzMzg1MTEyNzU=", "number": 20564, "title": "How tf.contrib.seq2seq.AttentionWrapper . working ?", "user": {"login": "monk1337", "id": 17107749, "node_id": "MDQ6VXNlcjE3MTA3NzQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/17107749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/monk1337", "html_url": "https://github.com/monk1337", "followers_url": "https://api.github.com/users/monk1337/followers", "following_url": "https://api.github.com/users/monk1337/following{/other_user}", "gists_url": "https://api.github.com/users/monk1337/gists{/gist_id}", "starred_url": "https://api.github.com/users/monk1337/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/monk1337/subscriptions", "organizations_url": "https://api.github.com/users/monk1337/orgs", "repos_url": "https://api.github.com/users/monk1337/repos", "events_url": "https://api.github.com/users/monk1337/events{/privacy}", "received_events_url": "https://api.github.com/users/monk1337/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-07-05T10:12:52Z", "updated_at": "2018-09-27T16:50:56Z", "closed_at": "2018-09-27T16:50:56Z", "author_association": "NONE", "body_html": "<p>I am trying to understand tf.contrib.seq2seq.AttentionWrapper  working , I have created simple program to understand the working of AttentionWrapper :</p>\n<pre><code>num_units=32\n\nbatch_size = 12\nmax_time = 10\nnum_dim = 11\n\n\n\ninput_data = np.random.randn(batch_size,max_time,num_dim).astype(np.float32)\n\n\n# atch_size = np.random.randn(batch_size,num_dim).astype(np.float32)\n\nsequence_length =[len(i) for i in input_data]\n\nlstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\n\noutput,last_state = tf.nn.dynamic_rnn(lstm_cell,input_data,sequence_length=sequence_length,dtype=tf.float32)\n\nprint(output,last_state)\n\n\n\n#now let's use attention\n\n#rnn_no_units , encoder output , sequence_length , dtype , name\n\nattention_Bahdanau = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units,memory=output,memory_sequence_length=sequence_length,dtype=tf.float32,name='BahdanauAttention')\n\nbatch_size = attention_Bahdanau.batch_size\nmemory_layer = attention_Bahdanau.memory_layer\nkeys=attention_Bahdanau.keys\nvalues=attention_Bahdanau.values\nalignment_size= attention_Bahdanau.alignments_size\nstate_size = attention_Bahdanau.state_size\n\n\n\nquery_ = tf.get_variable(name='query_dta_1',\n                         shape=[batch_size,num_units],\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\n\nstate_ = tf.get_variable(name='state__dta_l',\n                         shape=[batch_size,alignment_size],\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\n\n\n\n\ninitial_alignments = attention_Bahdanau.initial_alignments(batch_size,dtype=tf.float32)\n\ninitial_state   =    attention_Bahdanau.initial_state(batch_size,dtype=tf.float32)\n\nfg=attention_Bahdanau.__call__(query_,state_)\n\n#(12, 10)\n#(12, 10)\n\n#lstm_cell , attention_mech , rnn_num_units\n\nattention_wrapper = tf.contrib.seq2seq.AttentionWrapper(lstm_cell,attention_Bahdanau,num_units)\n\nzero_s= attention_wrapper.zero_state(batch_size=batch_size,dtype=tf.float32)\nprint(zero_s)\n\nprint(attention_wrapper.__call__(query_,zero_s))\n</code></pre>\n<p>Then i am getting this error:</p>\n<pre><code>AttentionWrapperState(cell_state=LSTMStateTuple(c=&lt;tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(12, 32) dtype=float32&gt;, h=&lt;tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(12, 32) dtype=float32&gt;), attention=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(12, 32) dtype=float32&gt;, time=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=() dtype=int32&gt;, alignments=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=(12, 10) dtype=float32&gt;, alignment_history=(), attention_state=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros_3:0' shape=(12, 10) dtype=float32&gt;)\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\n   1588   try:\n-&gt; 1589     c_op = c_api.TF_FinishOperation(op_desc)\n   1590   except errors.InvalidArgumentError as e:\n\nInvalidArgumentError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-80-0926d476036c&gt; in &lt;module&gt;()\n      5 print(zero_s)\n      6 \n----&gt; 7 print(attention_wrapper.__call__(query_,zero_s))\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    230         setattr(self, scope_attrname, scope)\n    231       with scope:\n--&gt; 232         return super(RNNCell, self).__call__(inputs, state)\n    233 \n    234   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    327 \n    328       # Actually call layer\n--&gt; 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    330 \n    331     if not context.executing_eagerly():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    701 \n    702       if not in_deferred_mode:\n--&gt; 703         outputs = self.call(inputs, *args, **kwargs)\n    704         if outputs is None:\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\n   1410     cell_inputs = self._cell_input_fn(inputs, state.attention)\n   1411     cell_state = state.cell_state\n-&gt; 1412     cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n   1413 \n   1414     cell_batch_size = (\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)\n    337     # method.  See the class docstring for more details.\n    338     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n--&gt; 339                                      *args, **kwargs)\n    340 \n    341 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    327 \n    328       # Actually call layer\n--&gt; 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    330 \n    331     if not context.executing_eagerly():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    701 \n    702       if not in_deferred_mode:\n--&gt; 703         outputs = self.call(inputs, *args, **kwargs)\n    704         if outputs is None:\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\n    855     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    856     lstm_matrix = math_ops.matmul(\n--&gt; 857         array_ops.concat([inputs, m_prev], 1), self._kernel)\n    858     lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias)\n    859 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\n   2012     else:\n   2013       return gen_math_ops.mat_mul(\n-&gt; 2014           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n   2015 \n   2016 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)\n   4277     _, _, _op = _op_def_lib._apply_op_helper(\n   4278         \"MatMul\", a=a, b=b, transpose_a=transpose_a, transpose_b=transpose_b,\n-&gt; 4279         name=name)\n   4280     _result = _op.outputs[:]\n   4281     _inputs_flat = _op.inputs\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    786                          input_types=input_types, attrs=attr_protos,\n--&gt; 787                          op_def=op_def)\n    788       return output_structure, op_def.is_stateful, op\n    789 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   3412           input_types=input_types,\n   3413           original_op=self._default_original_op,\n-&gt; 3414           op_def=op_def)\n   3415 \n   3416       # Note: shapes are lazily computed with the C API enabled.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\n   1754           op_def, inputs, node_def.attr)\n   1755       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n-&gt; 1756                                 control_input_ops)\n   1757     else:\n   1758       self._c_op = None\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\n   1590   except errors.InvalidArgumentError as e:\n   1591     # Convert to ValueError for backwards compatibility.\n-&gt; 1592     raise ValueError(str(e))\n   1593 \n   1594   return c_op\n\nValueError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\n</code></pre>\n<p>I am not getting from where this [43,128] coming from and how to compute_attention successfully using this wrapper ?</p>\n<p>For live experiment i have created google colab notebook , You can run and check code there :</p>\n<p><a href=\"https://colab.research.google.com/drive/1oR039KJgtpNsJFGh8VtQt-NRcAtXi4NR\" rel=\"nofollow\">https://colab.research.google.com/drive/1oR039KJgtpNsJFGh8VtQt-NRcAtXi4NR</a></p>", "body_text": "I am trying to understand tf.contrib.seq2seq.AttentionWrapper  working , I have created simple program to understand the working of AttentionWrapper :\nnum_units=32\n\nbatch_size = 12\nmax_time = 10\nnum_dim = 11\n\n\n\ninput_data = np.random.randn(batch_size,max_time,num_dim).astype(np.float32)\n\n\n# atch_size = np.random.randn(batch_size,num_dim).astype(np.float32)\n\nsequence_length =[len(i) for i in input_data]\n\nlstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\n\noutput,last_state = tf.nn.dynamic_rnn(lstm_cell,input_data,sequence_length=sequence_length,dtype=tf.float32)\n\nprint(output,last_state)\n\n\n\n#now let's use attention\n\n#rnn_no_units , encoder output , sequence_length , dtype , name\n\nattention_Bahdanau = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units,memory=output,memory_sequence_length=sequence_length,dtype=tf.float32,name='BahdanauAttention')\n\nbatch_size = attention_Bahdanau.batch_size\nmemory_layer = attention_Bahdanau.memory_layer\nkeys=attention_Bahdanau.keys\nvalues=attention_Bahdanau.values\nalignment_size= attention_Bahdanau.alignments_size\nstate_size = attention_Bahdanau.state_size\n\n\n\nquery_ = tf.get_variable(name='query_dta_1',\n                         shape=[batch_size,num_units],\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\n\nstate_ = tf.get_variable(name='state__dta_l',\n                         shape=[batch_size,alignment_size],\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\n\n\n\n\ninitial_alignments = attention_Bahdanau.initial_alignments(batch_size,dtype=tf.float32)\n\ninitial_state   =    attention_Bahdanau.initial_state(batch_size,dtype=tf.float32)\n\nfg=attention_Bahdanau.__call__(query_,state_)\n\n#(12, 10)\n#(12, 10)\n\n#lstm_cell , attention_mech , rnn_num_units\n\nattention_wrapper = tf.contrib.seq2seq.AttentionWrapper(lstm_cell,attention_Bahdanau,num_units)\n\nzero_s= attention_wrapper.zero_state(batch_size=batch_size,dtype=tf.float32)\nprint(zero_s)\n\nprint(attention_wrapper.__call__(query_,zero_s))\n\nThen i am getting this error:\nAttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(12, 32) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(12, 32) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(12, 32) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=(12, 10) dtype=float32>, alignment_history=(), attention_state=<tf.Tensor 'AttentionWrapperZeroState/zeros_3:0' shape=(12, 10) dtype=float32>)\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\n   1588   try:\n-> 1589     c_op = c_api.TF_FinishOperation(op_desc)\n   1590   except errors.InvalidArgumentError as e:\n\nInvalidArgumentError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-80-0926d476036c> in <module>()\n      5 print(zero_s)\n      6 \n----> 7 print(attention_wrapper.__call__(query_,zero_s))\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    230         setattr(self, scope_attrname, scope)\n    231       with scope:\n--> 232         return super(RNNCell, self).__call__(inputs, state)\n    233 \n    234   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    327 \n    328       # Actually call layer\n--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    330 \n    331     if not context.executing_eagerly():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    701 \n    702       if not in_deferred_mode:\n--> 703         outputs = self.call(inputs, *args, **kwargs)\n    704         if outputs is None:\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\n   1410     cell_inputs = self._cell_input_fn(inputs, state.attention)\n   1411     cell_state = state.cell_state\n-> 1412     cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n   1413 \n   1414     cell_batch_size = (\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)\n    337     # method.  See the class docstring for more details.\n    338     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n--> 339                                      *args, **kwargs)\n    340 \n    341 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\n    327 \n    328       # Actually call layer\n--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n    330 \n    331     if not context.executing_eagerly():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    701 \n    702       if not in_deferred_mode:\n--> 703         outputs = self.call(inputs, *args, **kwargs)\n    704         if outputs is None:\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\n    855     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    856     lstm_matrix = math_ops.matmul(\n--> 857         array_ops.concat([inputs, m_prev], 1), self._kernel)\n    858     lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias)\n    859 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\n   2012     else:\n   2013       return gen_math_ops.mat_mul(\n-> 2014           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n   2015 \n   2016 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)\n   4277     _, _, _op = _op_def_lib._apply_op_helper(\n   4278         \"MatMul\", a=a, b=b, transpose_a=transpose_a, transpose_b=transpose_b,\n-> 4279         name=name)\n   4280     _result = _op.outputs[:]\n   4281     _inputs_flat = _op.inputs\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    786                          input_types=input_types, attrs=attr_protos,\n--> 787                          op_def=op_def)\n    788       return output_structure, op_def.is_stateful, op\n    789 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   3412           input_types=input_types,\n   3413           original_op=self._default_original_op,\n-> 3414           op_def=op_def)\n   3415 \n   3416       # Note: shapes are lazily computed with the C API enabled.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\n   1754           op_def, inputs, node_def.attr)\n   1755       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n-> 1756                                 control_input_ops)\n   1757     else:\n   1758       self._c_op = None\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\n   1590   except errors.InvalidArgumentError as e:\n   1591     # Convert to ValueError for backwards compatibility.\n-> 1592     raise ValueError(str(e))\n   1593 \n   1594   return c_op\n\nValueError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\n\nI am not getting from where this [43,128] coming from and how to compute_attention successfully using this wrapper ?\nFor live experiment i have created google colab notebook , You can run and check code there :\nhttps://colab.research.google.com/drive/1oR039KJgtpNsJFGh8VtQt-NRcAtXi4NR", "body": "I am trying to understand tf.contrib.seq2seq.AttentionWrapper  working , I have created simple program to understand the working of AttentionWrapper : \r\n\r\n\r\n```\r\nnum_units=32\r\n\r\nbatch_size = 12\r\nmax_time = 10\r\nnum_dim = 11\r\n\r\n\r\n\r\ninput_data = np.random.randn(batch_size,max_time,num_dim).astype(np.float32)\r\n\r\n\r\n# atch_size = np.random.randn(batch_size,num_dim).astype(np.float32)\r\n\r\nsequence_length =[len(i) for i in input_data]\r\n\r\nlstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\r\n\r\noutput,last_state = tf.nn.dynamic_rnn(lstm_cell,input_data,sequence_length=sequence_length,dtype=tf.float32)\r\n\r\nprint(output,last_state)\r\n\r\n\r\n\r\n#now let's use attention\r\n\r\n#rnn_no_units , encoder output , sequence_length , dtype , name\r\n\r\nattention_Bahdanau = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units,memory=output,memory_sequence_length=sequence_length,dtype=tf.float32,name='BahdanauAttention')\r\n\r\nbatch_size = attention_Bahdanau.batch_size\r\nmemory_layer = attention_Bahdanau.memory_layer\r\nkeys=attention_Bahdanau.keys\r\nvalues=attention_Bahdanau.values\r\nalignment_size= attention_Bahdanau.alignments_size\r\nstate_size = attention_Bahdanau.state_size\r\n\r\n\r\n\r\nquery_ = tf.get_variable(name='query_dta_1',\r\n                         shape=[batch_size,num_units],\r\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\r\n\r\nstate_ = tf.get_variable(name='state__dta_l',\r\n                         shape=[batch_size,alignment_size],\r\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\r\n\r\n\r\n\r\n\r\ninitial_alignments = attention_Bahdanau.initial_alignments(batch_size,dtype=tf.float32)\r\n\r\ninitial_state   =    attention_Bahdanau.initial_state(batch_size,dtype=tf.float32)\r\n\r\nfg=attention_Bahdanau.__call__(query_,state_)\r\n\r\n#(12, 10)\r\n#(12, 10)\r\n\r\n#lstm_cell , attention_mech , rnn_num_units\r\n\r\nattention_wrapper = tf.contrib.seq2seq.AttentionWrapper(lstm_cell,attention_Bahdanau,num_units)\r\n\r\nzero_s= attention_wrapper.zero_state(batch_size=batch_size,dtype=tf.float32)\r\nprint(zero_s)\r\n\r\nprint(attention_wrapper.__call__(query_,zero_s))\r\n```\r\n\r\nThen i am getting this error:\r\n\r\n```\r\nAttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(12, 32) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(12, 32) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(12, 32) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=(12, 10) dtype=float32>, alignment_history=(), attention_state=<tf.Tensor 'AttentionWrapperZeroState/zeros_3:0' shape=(12, 10) dtype=float32>)\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1588   try:\r\n-> 1589     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1590   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-80-0926d476036c> in <module>()\r\n      5 print(zero_s)\r\n      6 \r\n----> 7 print(attention_wrapper.__call__(query_,zero_s))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    230         setattr(self, scope_attrname, scope)\r\n    231       with scope:\r\n--> 232         return super(RNNCell, self).__call__(inputs, state)\r\n    233 \r\n    234   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    327 \r\n    328       # Actually call layer\r\n--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n    330 \r\n    331     if not context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    701 \r\n    702       if not in_deferred_mode:\r\n--> 703         outputs = self.call(inputs, *args, **kwargs)\r\n    704         if outputs is None:\r\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\r\n   1410     cell_inputs = self._cell_input_fn(inputs, state.attention)\r\n   1411     cell_state = state.cell_state\r\n-> 1412     cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n   1413 \r\n   1414     cell_batch_size = (\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)\r\n    337     # method.  See the class docstring for more details.\r\n    338     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\r\n--> 339                                      *args, **kwargs)\r\n    340 \r\n    341 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    327 \r\n    328       # Actually call layer\r\n--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n    330 \r\n    331     if not context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    701 \r\n    702       if not in_deferred_mode:\r\n--> 703         outputs = self.call(inputs, *args, **kwargs)\r\n    704         if outputs is None:\r\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\r\n    855     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n    856     lstm_matrix = math_ops.matmul(\r\n--> 857         array_ops.concat([inputs, m_prev], 1), self._kernel)\r\n    858     lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias)\r\n    859 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\r\n   2012     else:\r\n   2013       return gen_math_ops.mat_mul(\r\n-> 2014           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n   2015 \r\n   2016 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)\r\n   4277     _, _, _op = _op_def_lib._apply_op_helper(\r\n   4278         \"MatMul\", a=a, b=b, transpose_a=transpose_a, transpose_b=transpose_b,\r\n-> 4279         name=name)\r\n   4280     _result = _op.outputs[:]\r\n   4281     _inputs_flat = _op.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   3412           input_types=input_types,\r\n   3413           original_op=self._default_original_op,\r\n-> 3414           op_def=op_def)\r\n   3415 \r\n   3416       # Note: shapes are lazily computed with the C API enabled.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1754           op_def, inputs, node_def.attr)\r\n   1755       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1756                                 control_input_ops)\r\n   1757     else:\r\n   1758       self._c_op = None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1590   except errors.InvalidArgumentError as e:\r\n   1591     # Convert to ValueError for backwards compatibility.\r\n-> 1592     raise ValueError(str(e))\r\n   1593 \r\n   1594   return c_op\r\n\r\nValueError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\r\n```\r\n\r\nI am not getting from where this [43,128] coming from and how to compute_attention successfully using this wrapper ?\r\n\r\nFor live experiment i have created google colab notebook , You can run and check code there : \r\n\r\nhttps://colab.research.google.com/drive/1oR039KJgtpNsJFGh8VtQt-NRcAtXi4NR"}