{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312032067", "html_url": "https://github.com/tensorflow/tensorflow/pull/10629#issuecomment-312032067", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10629", "id": 312032067, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjAzMjA2Nw==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-29T17:12:04Z", "updated_at": "2017-06-29T17:12:04Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2613663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/byronyi\">@byronyi</a></p>\n<p>If you look at <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.h#L43\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.h#L43</a><br>\nyou'll see some indication of how these issues are handled in the Google internal version, which is specific to our hardware platform.  (Handling of networking related issues is nearly the only difference between internal and open-source versions.)   Memory is either GPU RAM or CPU RAM.  CPU RAM may optionally be gpu_registered which means that it was allocated via CUDAHostAllocator and both page locked and known to the CUDA driver as such.  gpu_registered memory is used for CPU resident tensors which are going to be transferred to/from GPU RAM via a copy operation.  nic_registered memory has been preregistered for RDMA.  GPU RAM is pre-allocated in large regions by the allocator (BFCAllocator), and when we're planning to do RDMA and GPUDirect is available, we go ahead and preregister all of those GPU memory regions with the bus-adjacent NIC.  If we plan on doing RDMA with CPU RAM as well, we preregister all memory in the pool obtained from CUDAHostAllocator with all NICs bus-adjacent to CPU RAM.</p>\n<p>The numa-related bits that you're touching in this change were written into the code base a long time ago on the assumption that someday we'd want do numa-specific memory allocation and registration.  In fact, we don't.  The page locking and NIC registration issues are important, with significant performance gain.  But we have not been able to get any gain out of maintaining multiple numa-specific memory pools, and having multiple pools will probably incur greater memory overhead.</p>\n<p>This lack of benefit probably has something to do with characteristics of our platforms and execution environment, but I would encourage you to address the other issues first, and only try to change the numa-0 default here when doing so realizes demonstrable benefit.</p>", "body_text": "@byronyi\nIf you look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.h#L43\nyou'll see some indication of how these issues are handled in the Google internal version, which is specific to our hardware platform.  (Handling of networking related issues is nearly the only difference between internal and open-source versions.)   Memory is either GPU RAM or CPU RAM.  CPU RAM may optionally be gpu_registered which means that it was allocated via CUDAHostAllocator and both page locked and known to the CUDA driver as such.  gpu_registered memory is used for CPU resident tensors which are going to be transferred to/from GPU RAM via a copy operation.  nic_registered memory has been preregistered for RDMA.  GPU RAM is pre-allocated in large regions by the allocator (BFCAllocator), and when we're planning to do RDMA and GPUDirect is available, we go ahead and preregister all of those GPU memory regions with the bus-adjacent NIC.  If we plan on doing RDMA with CPU RAM as well, we preregister all memory in the pool obtained from CUDAHostAllocator with all NICs bus-adjacent to CPU RAM.\nThe numa-related bits that you're touching in this change were written into the code base a long time ago on the assumption that someday we'd want do numa-specific memory allocation and registration.  In fact, we don't.  The page locking and NIC registration issues are important, with significant performance gain.  But we have not been able to get any gain out of maintaining multiple numa-specific memory pools, and having multiple pools will probably incur greater memory overhead.\nThis lack of benefit probably has something to do with characteristics of our platforms and execution environment, but I would encourage you to address the other issues first, and only try to change the numa-0 default here when doing so realizes demonstrable benefit.", "body": "@byronyi \r\n\r\nIf you look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.h#L43\r\nyou'll see some indication of how these issues are handled in the Google internal version, which is specific to our hardware platform.  (Handling of networking related issues is nearly the only difference between internal and open-source versions.)   Memory is either GPU RAM or CPU RAM.  CPU RAM may optionally be gpu_registered which means that it was allocated via CUDAHostAllocator and both page locked and known to the CUDA driver as such.  gpu_registered memory is used for CPU resident tensors which are going to be transferred to/from GPU RAM via a copy operation.  nic_registered memory has been preregistered for RDMA.  GPU RAM is pre-allocated in large regions by the allocator (BFCAllocator), and when we're planning to do RDMA and GPUDirect is available, we go ahead and preregister all of those GPU memory regions with the bus-adjacent NIC.  If we plan on doing RDMA with CPU RAM as well, we preregister all memory in the pool obtained from CUDAHostAllocator with all NICs bus-adjacent to CPU RAM.\r\n\r\nThe numa-related bits that you're touching in this change were written into the code base a long time ago on the assumption that someday we'd want do numa-specific memory allocation and registration.  In fact, we don't.  The page locking and NIC registration issues are important, with significant performance gain.  But we have not been able to get any gain out of maintaining multiple numa-specific memory pools, and having multiple pools will probably incur greater memory overhead. \r\n\r\nThis lack of benefit probably has something to do with characteristics of our platforms and execution environment, but I would encourage you to address the other issues first, and only try to change the numa-0 default here when doing so realizes demonstrable benefit.  \r\n"}