{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312058560", "html_url": "https://github.com/tensorflow/tensorflow/pull/10629#issuecomment-312058560", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10629", "id": 312058560, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjA1ODU2MA==", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-29T18:35:59Z", "updated_at": "2017-06-29T18:35:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a> Thanks a lot to that bit of information. It clearly points out the direction for my next step, so I won't waste too much time on NUMA-specific issues. In fact, I think your feedback confirmed many observations I had, which is worth my effort digging into the internals of memory management. At least I know I am on the right track.</p>\n<p>Personally I found it quite interesting that GPU DMA does share a lot of similarities with RDMA, and there is plenty of GPU DMA code in the open sourced TF that could be reused. So I will just go ahead and implement it.</p>\n<p>If you don't mind, I have two more questions:</p>\n<ol>\n<li>As we do have verbs support now, would you mind to accept another PR doing RDMA? It is designed to handle many issues in a better way, i.e. an interface that utilises out-of-band <code>transport_options</code> in current gRPC, connection management using <code>librdmacm</code> (I already implemented these), zero-copy of tensor buffer and no extra memory allocation (as we discussed above), and GPU direct when available. However, the performance gain may not be substantial than what we have now, which, after several patches since it's merged, is close to linear scaling on a small testbed. Additionally, it will probably touch a lot different places outside <code>core/distributed_runtime</code> and adds more code complexity.</li>\n<li>You mentioned there are multiple NICs bus-adjacent to different PCI-e devices in Google's internal platform, and we are also working on hardware spec that fits TF running on RoCE. I understand it might be confidential, but would it be a performance boost good enough to justify the extra NICs? I will probably consider that in my patch as well, just to deal with different pairings of GPU and NIC, or to decide if we should do GPU direct at all.</li>\n</ol>\n<p>Thanks again for your comments!</p>", "body_text": "@poxvoculi Thanks a lot to that bit of information. It clearly points out the direction for my next step, so I won't waste too much time on NUMA-specific issues. In fact, I think your feedback confirmed many observations I had, which is worth my effort digging into the internals of memory management. At least I know I am on the right track.\nPersonally I found it quite interesting that GPU DMA does share a lot of similarities with RDMA, and there is plenty of GPU DMA code in the open sourced TF that could be reused. So I will just go ahead and implement it.\nIf you don't mind, I have two more questions:\n\nAs we do have verbs support now, would you mind to accept another PR doing RDMA? It is designed to handle many issues in a better way, i.e. an interface that utilises out-of-band transport_options in current gRPC, connection management using librdmacm (I already implemented these), zero-copy of tensor buffer and no extra memory allocation (as we discussed above), and GPU direct when available. However, the performance gain may not be substantial than what we have now, which, after several patches since it's merged, is close to linear scaling on a small testbed. Additionally, it will probably touch a lot different places outside core/distributed_runtime and adds more code complexity.\nYou mentioned there are multiple NICs bus-adjacent to different PCI-e devices in Google's internal platform, and we are also working on hardware spec that fits TF running on RoCE. I understand it might be confidential, but would it be a performance boost good enough to justify the extra NICs? I will probably consider that in my patch as well, just to deal with different pairings of GPU and NIC, or to decide if we should do GPU direct at all.\n\nThanks again for your comments!", "body": "@poxvoculi Thanks a lot to that bit of information. It clearly points out the direction for my next step, so I won't waste too much time on NUMA-specific issues. In fact, I think your feedback confirmed many observations I had, which is worth my effort digging into the internals of memory management. At least I know I am on the right track.\r\n\r\nPersonally I found it quite interesting that GPU DMA does share a lot of similarities with RDMA, and there is plenty of GPU DMA code in the open sourced TF that could be reused. So I will just go ahead and implement it.\r\n\r\nIf you don't mind, I have two more questions: \r\n\r\n1. As we do have verbs support now, would you mind to accept another PR doing RDMA? It is designed to handle many issues in a better way, i.e. an interface that utilises out-of-band ``transport_options`` in current gRPC, connection management using `librdmacm` (I already implemented these), zero-copy of tensor buffer and no extra memory allocation (as we discussed above), and GPU direct when available. However, the performance gain may not be substantial than what we have now, which, after several patches since it's merged, is close to linear scaling on a small testbed. Additionally, it will probably touch a lot different places outside `core/distributed_runtime` and adds more code complexity.\r\n2. You mentioned there are multiple NICs bus-adjacent to different PCI-e devices in Google's internal platform, and we are also working on hardware spec that fits TF running on RoCE. I understand it might be confidential, but would it be a performance boost good enough to justify the extra NICs? I will probably consider that in my patch as well, just to deal with different pairings of GPU and NIC, or to decide if we should do GPU direct at all.\r\n\r\nThanks again for your comments!"}