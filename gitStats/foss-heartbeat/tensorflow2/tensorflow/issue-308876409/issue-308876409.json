{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18012", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18012/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18012/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18012/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18012", "id": 308876409, "node_id": "MDU6SXNzdWUzMDg4NzY0MDk=", "number": 18012, "title": "Error: absl.flags._exception:IllegalFlagValueError: flag --input=: cannot convert string to float in tensorflow serving", "user": {"login": "dhinar1991", "id": 26268279, "node_id": "MDQ6VXNlcjI2MjY4Mjc5", "avatar_url": "https://avatars2.githubusercontent.com/u/26268279?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dhinar1991", "html_url": "https://github.com/dhinar1991", "followers_url": "https://api.github.com/users/dhinar1991/followers", "following_url": "https://api.github.com/users/dhinar1991/following{/other_user}", "gists_url": "https://api.github.com/users/dhinar1991/gists{/gist_id}", "starred_url": "https://api.github.com/users/dhinar1991/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dhinar1991/subscriptions", "organizations_url": "https://api.github.com/users/dhinar1991/orgs", "repos_url": "https://api.github.com/users/dhinar1991/repos", "events_url": "https://api.github.com/users/dhinar1991/events{/privacy}", "received_events_url": "https://api.github.com/users/dhinar1991/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-27T08:48:41Z", "updated_at": "2018-03-29T10:00:06Z", "closed_at": "2018-03-29T10:00:06Z", "author_association": "NONE", "body_html": "<p>I have written a simple program for tensorflow serving to deploy and check how it is working. I followed many tutorials on how to deploy these models using tensorflow serving inside docker environment.</p>\n<pre><code>sess = tf.InteractiveSession()\n# define the tensorflow network and do some trains\nx = tf.placeholder(\"float\", name=\"x\")\nw = tf.Variable(2.0, name=\"w\")\nb = tf.Variable(0.0, name=\"bias\")\nh = tf.multiply(x, w)\n\nsess.run(tf.global_variables_initializer())\ny = tf.add(h, b, name=\"y\")\n\n\nexport_path_base = FLAGS.work_dir\nexport_path = os.path.join(tf.compat.as_bytes(export_path_base),\n  tf.compat.as_bytes(str(FLAGS.model_version)))\nprint('Exporting trained model to', export_path)\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)\n\ntensor_info_x = tf.saved_model.utils.build_tensor_info(x)\ntensor_info_y = tf.saved_model.utils.build_tensor_info(y)\n\nprediction_signature = (\n  tf.saved_model.signature_def_utils.build_signature_def(\n  inputs={'input': tensor_info_x},\n  outputs={'output': tensor_info_y},\n  method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n\nlegacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n\nbuilder.add_meta_graph_and_variables(\n  sess, [tf.saved_model.tag_constants.SERVING],\n  signature_def_map={\n  'prediction':\n  prediction_signature,\n  },\n  legacy_init_op=legacy_init_op)\n\nbuilder.save()\n</code></pre>\n<p>This saved_model of above programs is currently running inside the docker.I now want to create a client.py file to take input and produce output. I want to give a single number as input to my client file and not to declare inside . I mean</p>\n<p>i want to give input like this</p>\n<blockquote>\n<p>python client.py --server=localhost:9000 --input=3</p>\n</blockquote>\n<p>so i created a client file with input as tf.app.flags.float('input','','input for the model')</p>\n<pre><code>\nfrom grpc.beta import implementations\nimport numpy\nimport tensorflow as tf\nimport sys \nfrom datetime import datetime\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\ntf.app.flags.DEFINE_float('input','', 'input for the model')\nFLAGS = tf.app.flags.FLAGS\n\ndef do_inference(hostport,args):\n  \"\"\"Tests PredictionService with concurrent requests.\n  Args:\n  hostport: Host:port address of the Prediction Service.\n  Returns:\n  pred values, ground truth label\n  \"\"\"\n  # create connection\n  host, port = hostport.split(':')\n  channel = implementations.insecure_channel(host, int(port))\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n  # initialize a request\n  data = args\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = 'example_model'\n  request.model_spec.signature_name = 'prediction'\n\n  request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\n  # predict\n  result = stub.Predict(request, 5.0) # 5 seconds\n  return result\n\ndef main(_):\n    if not FLAGS.server:\n        print('please specify server host:port')\n    return\n\n    result = do_inference(FLAGS.server,FLAGS.input)\n    print('Result is: ', result)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n</code></pre>\n<p>This gives error ,<br>\nCan someone tell me what i did wrong here please? How i send a float number as input to the client file?</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/26268279/37956646-87ae7df8-31c9-11e8-99e2-11de878f1608.JPG\"><img src=\"https://user-images.githubusercontent.com/26268279/37956646-87ae7df8-31c9-11e8-99e2-11de878f1608.JPG\" alt=\"capture\" style=\"max-width:100%;\"></a></p>", "body_text": "I have written a simple program for tensorflow serving to deploy and check how it is working. I followed many tutorials on how to deploy these models using tensorflow serving inside docker environment.\nsess = tf.InteractiveSession()\n# define the tensorflow network and do some trains\nx = tf.placeholder(\"float\", name=\"x\")\nw = tf.Variable(2.0, name=\"w\")\nb = tf.Variable(0.0, name=\"bias\")\nh = tf.multiply(x, w)\n\nsess.run(tf.global_variables_initializer())\ny = tf.add(h, b, name=\"y\")\n\n\nexport_path_base = FLAGS.work_dir\nexport_path = os.path.join(tf.compat.as_bytes(export_path_base),\n  tf.compat.as_bytes(str(FLAGS.model_version)))\nprint('Exporting trained model to', export_path)\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)\n\ntensor_info_x = tf.saved_model.utils.build_tensor_info(x)\ntensor_info_y = tf.saved_model.utils.build_tensor_info(y)\n\nprediction_signature = (\n  tf.saved_model.signature_def_utils.build_signature_def(\n  inputs={'input': tensor_info_x},\n  outputs={'output': tensor_info_y},\n  method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n\nlegacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n\nbuilder.add_meta_graph_and_variables(\n  sess, [tf.saved_model.tag_constants.SERVING],\n  signature_def_map={\n  'prediction':\n  prediction_signature,\n  },\n  legacy_init_op=legacy_init_op)\n\nbuilder.save()\n\nThis saved_model of above programs is currently running inside the docker.I now want to create a client.py file to take input and produce output. I want to give a single number as input to my client file and not to declare inside . I mean\ni want to give input like this\n\npython client.py --server=localhost:9000 --input=3\n\nso i created a client file with input as tf.app.flags.float('input','','input for the model')\n\nfrom grpc.beta import implementations\nimport numpy\nimport tensorflow as tf\nimport sys \nfrom datetime import datetime\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\ntf.app.flags.DEFINE_float('input','', 'input for the model')\nFLAGS = tf.app.flags.FLAGS\n\ndef do_inference(hostport,args):\n  \"\"\"Tests PredictionService with concurrent requests.\n  Args:\n  hostport: Host:port address of the Prediction Service.\n  Returns:\n  pred values, ground truth label\n  \"\"\"\n  # create connection\n  host, port = hostport.split(':')\n  channel = implementations.insecure_channel(host, int(port))\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n  # initialize a request\n  data = args\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = 'example_model'\n  request.model_spec.signature_name = 'prediction'\n\n  request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\n  # predict\n  result = stub.Predict(request, 5.0) # 5 seconds\n  return result\n\ndef main(_):\n    if not FLAGS.server:\n        print('please specify server host:port')\n    return\n\n    result = do_inference(FLAGS.server,FLAGS.input)\n    print('Result is: ', result)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n\nThis gives error ,\nCan someone tell me what i did wrong here please? How i send a float number as input to the client file?", "body": "I have written a simple program for tensorflow serving to deploy and check how it is working. I followed many tutorials on how to deploy these models using tensorflow serving inside docker environment.\r\n\r\n```\r\nsess = tf.InteractiveSession()\r\n# define the tensorflow network and do some trains\r\nx = tf.placeholder(\"float\", name=\"x\")\r\nw = tf.Variable(2.0, name=\"w\")\r\nb = tf.Variable(0.0, name=\"bias\")\r\nh = tf.multiply(x, w)\r\n\r\nsess.run(tf.global_variables_initializer())\r\ny = tf.add(h, b, name=\"y\")\r\n\r\n\r\nexport_path_base = FLAGS.work_dir\r\nexport_path = os.path.join(tf.compat.as_bytes(export_path_base),\r\n  tf.compat.as_bytes(str(FLAGS.model_version)))\r\nprint('Exporting trained model to', export_path)\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)\r\n\r\ntensor_info_x = tf.saved_model.utils.build_tensor_info(x)\r\ntensor_info_y = tf.saved_model.utils.build_tensor_info(y)\r\n\r\nprediction_signature = (\r\n  tf.saved_model.signature_def_utils.build_signature_def(\r\n  inputs={'input': tensor_info_x},\r\n  outputs={'output': tensor_info_y},\r\n  method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\r\n\r\nlegacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\r\n\r\nbuilder.add_meta_graph_and_variables(\r\n  sess, [tf.saved_model.tag_constants.SERVING],\r\n  signature_def_map={\r\n  'prediction':\r\n  prediction_signature,\r\n  },\r\n  legacy_init_op=legacy_init_op)\r\n\r\nbuilder.save()\r\n```\r\nThis saved_model of above programs is currently running inside the docker.I now want to create a client.py file to take input and produce output. I want to give a single number as input to my client file and not to declare inside . I mean\r\n\r\ni want to give input like this\r\n\r\n> python client.py --server=localhost:9000 --input=3\r\n\r\nso i created a client file with input as tf.app.flags.float('input','','input for the model')\r\n\r\n```\r\n\r\nfrom grpc.beta import implementations\r\nimport numpy\r\nimport tensorflow as tf\r\nimport sys \r\nfrom datetime import datetime\r\nfrom tensorflow_serving.apis import predict_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2\r\n\r\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\r\ntf.app.flags.DEFINE_float('input','', 'input for the model')\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ndef do_inference(hostport,args):\r\n  \"\"\"Tests PredictionService with concurrent requests.\r\n  Args:\r\n  hostport: Host:port address of the Prediction Service.\r\n  Returns:\r\n  pred values, ground truth label\r\n  \"\"\"\r\n  # create connection\r\n  host, port = hostport.split(':')\r\n  channel = implementations.insecure_channel(host, int(port))\r\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n  # initialize a request\r\n  data = args\r\n  request = predict_pb2.PredictRequest()\r\n  request.model_spec.name = 'example_model'\r\n  request.model_spec.signature_name = 'prediction'\r\n\r\n  request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\r\n  # predict\r\n  result = stub.Predict(request, 5.0) # 5 seconds\r\n  return result\r\n\r\ndef main(_):\r\n    if not FLAGS.server:\r\n        print('please specify server host:port')\r\n    return\r\n\r\n    result = do_inference(FLAGS.server,FLAGS.input)\r\n    print('Result is: ', result)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\nThis gives error , \r\nCan someone tell me what i did wrong here please? How i send a float number as input to the client file?\r\n\r\n![capture](https://user-images.githubusercontent.com/26268279/37956646-87ae7df8-31c9-11e8-99e2-11de878f1608.JPG)\r\n"}