{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4545", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4545/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4545/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4545/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4545", "id": 178783935, "node_id": "MDU6SXNzdWUxNzg3ODM5MzU=", "number": 4545, "title": "TypeError: Expected int for argument 'num' not <tf.Tensor 'Squeeze:0' shape=() dtype=int32>.", "user": {"login": "qinghua2016", "id": 21454111, "node_id": "MDQ6VXNlcjIxNDU0MTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/21454111?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qinghua2016", "html_url": "https://github.com/qinghua2016", "followers_url": "https://api.github.com/users/qinghua2016/followers", "following_url": "https://api.github.com/users/qinghua2016/following{/other_user}", "gists_url": "https://api.github.com/users/qinghua2016/gists{/gist_id}", "starred_url": "https://api.github.com/users/qinghua2016/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qinghua2016/subscriptions", "organizations_url": "https://api.github.com/users/qinghua2016/orgs", "repos_url": "https://api.github.com/users/qinghua2016/repos", "events_url": "https://api.github.com/users/qinghua2016/events{/privacy}", "received_events_url": "https://api.github.com/users/qinghua2016/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-09-23T04:46:46Z", "updated_at": "2016-11-21T11:15:07Z", "closed_at": "2016-10-13T00:34:29Z", "author_association": "NONE", "body_html": "<p>I run the following code, but it has one error at    \"tf.unpack(inputs,self.bathsize)\",the error is as follows:<br>\nTypeError: Expected int for argument 'num' not &lt;tf.Tensor 'Squeeze:0' shape=() dtype=int32&gt;.</p>\n<pre><code> def build(n_dict, word_dim):\n       self.word_ids = tf.placeholder(tf.int32, shape=(None, None, 7), name='word_ids')\n        self.bathsize = tf.shape(self.word_ids)[0]\n        self.word_len=tf.shape(self.word_ids)[1]\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([n_dict, word_dim], -1.0, 1.0), name='embedding', dtype='float32')\n            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.word_ids)\n        inputs = tf.reshape(self.embedded_input, [self.bathsize, self.word_len, -1])\n        # inputs=tf.transpose(inputs,[1,0,2])\n        inputs = tf.nn.dropout(inputs, self.dropout)\n        self.inputs = inputs\n\n        self.input_length = tf.fill([self.word_len], self.bathsize)\n        self.input_length = tf.cast(self.input_length, dtype=tf.int64)\n        with tf.variable_scope('forward'):\n            self.lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        with tf.variable_scope('backward'):\n            self.lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.word_len, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.word_len, tf.float32)\n        # sequence_length=self.input_length\n\n        outputs, output_state_fw, output_state_bw = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(inputs,self.bathsize),\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n            sequence_length=self.input_length\n        )\n</code></pre>\n<p>As I know, tf.unpack have one parameter num, when the code run to \"tf.unpack(inputs,self.bathsize)\", num=None.  When the bachsize if fixed, it succeed to run. But  I want the model inputs have variable bacthsize, do you have any idea to solve this?<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710923\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/geoffrey\">@geoffrey</a> Irving</p>", "body_text": "I run the following code, but it has one error at    \"tf.unpack(inputs,self.bathsize)\",the error is as follows:\nTypeError: Expected int for argument 'num' not <tf.Tensor 'Squeeze:0' shape=() dtype=int32>.\n def build(n_dict, word_dim):\n       self.word_ids = tf.placeholder(tf.int32, shape=(None, None, 7), name='word_ids')\n        self.bathsize = tf.shape(self.word_ids)[0]\n        self.word_len=tf.shape(self.word_ids)[1]\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([n_dict, word_dim], -1.0, 1.0), name='embedding', dtype='float32')\n            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.word_ids)\n        inputs = tf.reshape(self.embedded_input, [self.bathsize, self.word_len, -1])\n        # inputs=tf.transpose(inputs,[1,0,2])\n        inputs = tf.nn.dropout(inputs, self.dropout)\n        self.inputs = inputs\n\n        self.input_length = tf.fill([self.word_len], self.bathsize)\n        self.input_length = tf.cast(self.input_length, dtype=tf.int64)\n        with tf.variable_scope('forward'):\n            self.lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        with tf.variable_scope('backward'):\n            self.lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.word_len, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.word_len, tf.float32)\n        # sequence_length=self.input_length\n\n        outputs, output_state_fw, output_state_bw = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(inputs,self.bathsize),\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n            sequence_length=self.input_length\n        )\n\nAs I know, tf.unpack have one parameter num, when the code run to \"tf.unpack(inputs,self.bathsize)\", num=None.  When the bachsize if fixed, it succeed to run. But  I want the model inputs have variable bacthsize, do you have any idea to solve this?@geoffrey Irving", "body": "I run the following code, but it has one error at    \"tf.unpack(inputs,self.bathsize)\",the error is as follows:\nTypeError: Expected int for argument 'num' not <tf.Tensor 'Squeeze:0' shape=() dtype=int32>.\n\n```\n def build(n_dict, word_dim):\n       self.word_ids = tf.placeholder(tf.int32, shape=(None, None, 7), name='word_ids')\n        self.bathsize = tf.shape(self.word_ids)[0]\n        self.word_len=tf.shape(self.word_ids)[1]\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([n_dict, word_dim], -1.0, 1.0), name='embedding', dtype='float32')\n            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.word_ids)\n        inputs = tf.reshape(self.embedded_input, [self.bathsize, self.word_len, -1])\n        # inputs=tf.transpose(inputs,[1,0,2])\n        inputs = tf.nn.dropout(inputs, self.dropout)\n        self.inputs = inputs\n\n        self.input_length = tf.fill([self.word_len], self.bathsize)\n        self.input_length = tf.cast(self.input_length, dtype=tf.int64)\n        with tf.variable_scope('forward'):\n            self.lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        with tf.variable_scope('backward'):\n            self.lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.word_len, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.word_len, tf.float32)\n        # sequence_length=self.input_length\n\n        outputs, output_state_fw, output_state_bw = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(inputs,self.bathsize),\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n            sequence_length=self.input_length\n        )\n```\n\nAs I know, tf.unpack have one parameter num, when the code run to \"tf.unpack(inputs,self.bathsize)\", num=None.  When the bachsize if fixed, it succeed to run. But  I want the model inputs have variable bacthsize, do you have any idea to solve this?@Geoffrey Irving\n"}