{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11626", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11626/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11626/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11626/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11626", "id": 244209461, "node_id": "MDU6SXNzdWUyNDQyMDk0NjE=", "number": 11626, "title": "SparseSoftmaxCrossEntropyWithLogits error while calculating gradient in c++ mode", "user": {"login": "saman-aghazadeh", "id": 3586023, "node_id": "MDQ6VXNlcjM1ODYwMjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3586023?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saman-aghazadeh", "html_url": "https://github.com/saman-aghazadeh", "followers_url": "https://api.github.com/users/saman-aghazadeh/followers", "following_url": "https://api.github.com/users/saman-aghazadeh/following{/other_user}", "gists_url": "https://api.github.com/users/saman-aghazadeh/gists{/gist_id}", "starred_url": "https://api.github.com/users/saman-aghazadeh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saman-aghazadeh/subscriptions", "organizations_url": "https://api.github.com/users/saman-aghazadeh/orgs", "repos_url": "https://api.github.com/users/saman-aghazadeh/repos", "events_url": "https://api.github.com/users/saman-aghazadeh/events{/privacy}", "received_events_url": "https://api.github.com/users/saman-aghazadeh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-07-20T00:07:02Z", "updated_at": "2018-11-15T19:01:08Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm trying to use the C++ API to train a CNN model. My last layer is using SparseSoftmaxCrossEntropyWithLogits. Here is the python code which generates the prototxt of the model:</p>\n<pre><code>import tensorflow as tf \nfrom tensorflow.python.framework import ops \nfrom tensorflow.python.framework import dtypes\n\nimport random  import numpy as np\n\nNUM_CLASSES = 102  IMAGE_HEIGHT = 224  IMAGE_WIDTH = 224  BATCH_SIZE = 25  NUM_CHANNELS = 3  LEARNING_RATE = 0.0001\n\nwith tf.Session() as sess:\n\n\n\nimages_placeholder = tf.placeholder (tf.float32,\n                                                shape=(BATCH_SIZE, IMAGE_HEIGHT,\n                                                IMAGE_WIDTH, NUM_CHANNELS), name=\"input\")   \nlabels_placeholder = tf.placeholder (tf.float32,\n                                                shape=(BATCH_SIZE), name=\"label\")\n\n    with tf.name_scope(\"conv1_1\") as scope:         \n                 kernel = tf.Variable (tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32, stddev=1e-2),\n                                                  name=\"weights\")         \n                 conv = tf.nn.conv2d (images_placeholder, kernel, [1, 1, 1, 1], padding='SAME')      \n                 biases = tf.Variable (tf.constant(0.0, shape=[64], dtype=tf.float32),\n                                                  trainable=True, name='biases')      \n                 out = tf.nn.bias_add (conv, biases)         \n                 conv1_1 = tf.nn.relu (out, name=scope)\n\n    pool1 = tf.nn.max_pool (conv1_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool1')\n\n    with tf.name_scope('conv2_1') as scope:         \n                 kernel = tf.Variable (tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-2),\n                                                  name='weights')       \n                 conv = tf.nn.conv2d (pool1, kernel, [1, 1, 1, 1], padding='SAME')       \n                 biases = tf.Variable (tf.constant(0.0, shape=[128], dtype=tf.float32),\n                                                  trainable=True, name='biases')      \n                 out = tf.nn.bias_add (conv, biases)         \n                 conv2_1 = tf.nn.relu (out, name=scope)\n\n    pool2 = tf.nn.max_pool (conv2_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool2')\n\n    with tf.name_scope('conv3_1') as scope:         \n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32, stddev=1e-2),\n                                                name='weights')       \n                 conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')        \n                 biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                                 trainable=True, name='biases')      \n                 out = tf.nn.bias_add(conv, biases)      \n                 conv3_1 = tf.nn.relu(out, name=scope)\n\n    pool3 = tf.nn.max_pool (conv3_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool3')\n\n    with tf.name_scope('conv4_1') as scope:         \n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32, stddev=1e-2),\n                                                name='weights')       \n                conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')        \n                biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                                trainable=True, name='biases')      \n                out = tf.nn.bias_add(conv, biases)      \n                conv4_1 = tf.nn.relu(out, name=scope)\n\n    pool4 = tf.nn.max_pool (conv4_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool4')   \n\n    with tf.name_scope('mentee_conv5_1') as scope:      \n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=1e-2),\n                                                name='weights')       \n                 conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')        \n                 biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True,\n                                                name='biases')         \n                 out = tf.nn.bias_add(conv, biases)      \n                 conv5_1 = tf.nn.relu(out, name=scope)\n\n    pool5 = tf.nn.max_pool (conv5_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool5')\n\n    with tf.name_scope('fc1') as scope:         \n                 shape = int(np.prod(pool5.get_shape()[1:]))         \n                 fc1w = tf.Variable(tf.truncated_normal([shape, 4096], dtype=tf.float32, \n                                             stddev=1e-2), name='weights')       \n                 fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                                             trainable=True, name='biases')      \n                 pool5_flat = tf.reshape(pool5, [-1, shape])         \n                 fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)        \n                 fc1 = tf.nn.relu(fc1l)\n                 fc1 = tf.nn.dropout(fc1, 0.5)\n\n\n    labels = tf.to_int64(labels_placeholder)    \n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits (labels=labels,\n                        logits=fc1, name=\"xentropy\")    \n    loss = tf.reduce_mean (cross_entropy, name='loss')\n\n    optimizer = tf.train.AdamOptimizer (LEARNING_RATE)  \n    global_step = tf.Variable (0, name='global_step', trainable=False)  \n    train_op = optimizer.minimize (loss, global_step=global_step, name=\"train\")\n\n    init = tf.initialize_variables (tf.all_variables(), name='init_all_vars_op')    \n    tf.train.write_graph (sess.graph_def, \"models/\", \"graph.pb\", as_text=False)\n</code></pre>\n<p>Unfortunately when I call the c++ code to run the train_op node, it'll throw below error:</p>\n<p><code>E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'message' not in Op&lt;name=PreventGradient; signature=input:T -&gt; output:T; attr=T:type&gt;; NodeDef: gradients/xentropy/xentropy_grad/PreventGradient = PreventGradient[T=DT_FLOAT, message=\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\'s interaction with tf.gradients()\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](xentropy/xentropy:1) </code><br>\nI'm still confused whether the error comes from a bug inside the TF code or not.</p>", "body_text": "I'm trying to use the C++ API to train a CNN model. My last layer is using SparseSoftmaxCrossEntropyWithLogits. Here is the python code which generates the prototxt of the model:\nimport tensorflow as tf \nfrom tensorflow.python.framework import ops \nfrom tensorflow.python.framework import dtypes\n\nimport random  import numpy as np\n\nNUM_CLASSES = 102  IMAGE_HEIGHT = 224  IMAGE_WIDTH = 224  BATCH_SIZE = 25  NUM_CHANNELS = 3  LEARNING_RATE = 0.0001\n\nwith tf.Session() as sess:\n\n\n\nimages_placeholder = tf.placeholder (tf.float32,\n                                                shape=(BATCH_SIZE, IMAGE_HEIGHT,\n                                                IMAGE_WIDTH, NUM_CHANNELS), name=\"input\")   \nlabels_placeholder = tf.placeholder (tf.float32,\n                                                shape=(BATCH_SIZE), name=\"label\")\n\n    with tf.name_scope(\"conv1_1\") as scope:         \n                 kernel = tf.Variable (tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32, stddev=1e-2),\n                                                  name=\"weights\")         \n                 conv = tf.nn.conv2d (images_placeholder, kernel, [1, 1, 1, 1], padding='SAME')      \n                 biases = tf.Variable (tf.constant(0.0, shape=[64], dtype=tf.float32),\n                                                  trainable=True, name='biases')      \n                 out = tf.nn.bias_add (conv, biases)         \n                 conv1_1 = tf.nn.relu (out, name=scope)\n\n    pool1 = tf.nn.max_pool (conv1_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool1')\n\n    with tf.name_scope('conv2_1') as scope:         \n                 kernel = tf.Variable (tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-2),\n                                                  name='weights')       \n                 conv = tf.nn.conv2d (pool1, kernel, [1, 1, 1, 1], padding='SAME')       \n                 biases = tf.Variable (tf.constant(0.0, shape=[128], dtype=tf.float32),\n                                                  trainable=True, name='biases')      \n                 out = tf.nn.bias_add (conv, biases)         \n                 conv2_1 = tf.nn.relu (out, name=scope)\n\n    pool2 = tf.nn.max_pool (conv2_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool2')\n\n    with tf.name_scope('conv3_1') as scope:         \n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32, stddev=1e-2),\n                                                name='weights')       \n                 conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')        \n                 biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                                 trainable=True, name='biases')      \n                 out = tf.nn.bias_add(conv, biases)      \n                 conv3_1 = tf.nn.relu(out, name=scope)\n\n    pool3 = tf.nn.max_pool (conv3_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool3')\n\n    with tf.name_scope('conv4_1') as scope:         \n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32, stddev=1e-2),\n                                                name='weights')       \n                conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')        \n                biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                                trainable=True, name='biases')      \n                out = tf.nn.bias_add(conv, biases)      \n                conv4_1 = tf.nn.relu(out, name=scope)\n\n    pool4 = tf.nn.max_pool (conv4_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool4')   \n\n    with tf.name_scope('mentee_conv5_1') as scope:      \n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=1e-2),\n                                                name='weights')       \n                 conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')        \n                 biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True,\n                                                name='biases')         \n                 out = tf.nn.bias_add(conv, biases)      \n                 conv5_1 = tf.nn.relu(out, name=scope)\n\n    pool5 = tf.nn.max_pool (conv5_1,\n                            ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME',\n                            name='pool5')\n\n    with tf.name_scope('fc1') as scope:         \n                 shape = int(np.prod(pool5.get_shape()[1:]))         \n                 fc1w = tf.Variable(tf.truncated_normal([shape, 4096], dtype=tf.float32, \n                                             stddev=1e-2), name='weights')       \n                 fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                                             trainable=True, name='biases')      \n                 pool5_flat = tf.reshape(pool5, [-1, shape])         \n                 fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)        \n                 fc1 = tf.nn.relu(fc1l)\n                 fc1 = tf.nn.dropout(fc1, 0.5)\n\n\n    labels = tf.to_int64(labels_placeholder)    \n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits (labels=labels,\n                        logits=fc1, name=\"xentropy\")    \n    loss = tf.reduce_mean (cross_entropy, name='loss')\n\n    optimizer = tf.train.AdamOptimizer (LEARNING_RATE)  \n    global_step = tf.Variable (0, name='global_step', trainable=False)  \n    train_op = optimizer.minimize (loss, global_step=global_step, name=\"train\")\n\n    init = tf.initialize_variables (tf.all_variables(), name='init_all_vars_op')    \n    tf.train.write_graph (sess.graph_def, \"models/\", \"graph.pb\", as_text=False)\n\nUnfortunately when I call the c++ code to run the train_op node, it'll throw below error:\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'message' not in Op<name=PreventGradient; signature=input:T -> output:T; attr=T:type>; NodeDef: gradients/xentropy/xentropy_grad/PreventGradient = PreventGradient[T=DT_FLOAT, message=\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\'s interaction with tf.gradients()\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](xentropy/xentropy:1) \nI'm still confused whether the error comes from a bug inside the TF code or not.", "body": "I'm trying to use the C++ API to train a CNN model. My last layer is using SparseSoftmaxCrossEntropyWithLogits. Here is the python code which generates the prototxt of the model:\r\n\r\n```\r\nimport tensorflow as tf \r\nfrom tensorflow.python.framework import ops \r\nfrom tensorflow.python.framework import dtypes\r\n\r\nimport random  import numpy as np\r\n\r\nNUM_CLASSES = 102  IMAGE_HEIGHT = 224  IMAGE_WIDTH = 224  BATCH_SIZE = 25  NUM_CHANNELS = 3  LEARNING_RATE = 0.0001\r\n\r\nwith tf.Session() as sess:\r\n\r\n\r\n\r\nimages_placeholder = tf.placeholder (tf.float32,\r\n                                                shape=(BATCH_SIZE, IMAGE_HEIGHT,\r\n                                                IMAGE_WIDTH, NUM_CHANNELS), name=\"input\")   \r\nlabels_placeholder = tf.placeholder (tf.float32,\r\n                                                shape=(BATCH_SIZE), name=\"label\")\r\n\r\n    with tf.name_scope(\"conv1_1\") as scope:         \r\n                 kernel = tf.Variable (tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32, stddev=1e-2),\r\n                                                  name=\"weights\")         \r\n                 conv = tf.nn.conv2d (images_placeholder, kernel, [1, 1, 1, 1], padding='SAME')      \r\n                 biases = tf.Variable (tf.constant(0.0, shape=[64], dtype=tf.float32),\r\n                                                  trainable=True, name='biases')      \r\n                 out = tf.nn.bias_add (conv, biases)         \r\n                 conv1_1 = tf.nn.relu (out, name=scope)\r\n\r\n    pool1 = tf.nn.max_pool (conv1_1,\r\n                            ksize=[1, 2, 2, 1],\r\n                            strides=[1, 2, 2, 1],\r\n                            padding='SAME',\r\n                            name='pool1')\r\n\r\n    with tf.name_scope('conv2_1') as scope:         \r\n                 kernel = tf.Variable (tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-2),\r\n                                                  name='weights')       \r\n                 conv = tf.nn.conv2d (pool1, kernel, [1, 1, 1, 1], padding='SAME')       \r\n                 biases = tf.Variable (tf.constant(0.0, shape=[128], dtype=tf.float32),\r\n                                                  trainable=True, name='biases')      \r\n                 out = tf.nn.bias_add (conv, biases)         \r\n                 conv2_1 = tf.nn.relu (out, name=scope)\r\n\r\n    pool2 = tf.nn.max_pool (conv2_1,\r\n                            ksize=[1, 2, 2, 1],\r\n                            strides=[1, 2, 2, 1],\r\n                            padding='SAME',\r\n                            name='pool2')\r\n\r\n    with tf.name_scope('conv3_1') as scope:         \r\n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32, stddev=1e-2),\r\n                                                name='weights')       \r\n                 conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')        \r\n                 biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\r\n                                                 trainable=True, name='biases')      \r\n                 out = tf.nn.bias_add(conv, biases)      \r\n                 conv3_1 = tf.nn.relu(out, name=scope)\r\n\r\n    pool3 = tf.nn.max_pool (conv3_1,\r\n                            ksize=[1, 2, 2, 1],\r\n                            strides=[1, 2, 2, 1],\r\n                            padding='SAME',\r\n                            name='pool3')\r\n\r\n    with tf.name_scope('conv4_1') as scope:         \r\n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32, stddev=1e-2),\r\n                                                name='weights')       \r\n                conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')        \r\n                biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                                                trainable=True, name='biases')      \r\n                out = tf.nn.bias_add(conv, biases)      \r\n                conv4_1 = tf.nn.relu(out, name=scope)\r\n\r\n    pool4 = tf.nn.max_pool (conv4_1,\r\n                            ksize=[1, 2, 2, 1],\r\n                            strides=[1, 2, 2, 1],\r\n                            padding='SAME',\r\n                            name='pool4')   \r\n\r\n    with tf.name_scope('mentee_conv5_1') as scope:      \r\n                 kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=1e-2),\r\n                                                name='weights')       \r\n                 conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')        \r\n                 biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True,\r\n                                                name='biases')         \r\n                 out = tf.nn.bias_add(conv, biases)      \r\n                 conv5_1 = tf.nn.relu(out, name=scope)\r\n\r\n    pool5 = tf.nn.max_pool (conv5_1,\r\n                            ksize=[1, 2, 2, 1],\r\n                            strides=[1, 2, 2, 1],\r\n                            padding='SAME',\r\n                            name='pool5')\r\n\r\n    with tf.name_scope('fc1') as scope:         \r\n                 shape = int(np.prod(pool5.get_shape()[1:]))         \r\n                 fc1w = tf.Variable(tf.truncated_normal([shape, 4096], dtype=tf.float32, \r\n                                             stddev=1e-2), name='weights')       \r\n                 fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\r\n                                             trainable=True, name='biases')      \r\n                 pool5_flat = tf.reshape(pool5, [-1, shape])         \r\n                 fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)        \r\n                 fc1 = tf.nn.relu(fc1l)\r\n                 fc1 = tf.nn.dropout(fc1, 0.5)\r\n\r\n\r\n    labels = tf.to_int64(labels_placeholder)    \r\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits (labels=labels,\r\n                        logits=fc1, name=\"xentropy\")    \r\n    loss = tf.reduce_mean (cross_entropy, name='loss')\r\n\r\n    optimizer = tf.train.AdamOptimizer (LEARNING_RATE)  \r\n    global_step = tf.Variable (0, name='global_step', trainable=False)  \r\n    train_op = optimizer.minimize (loss, global_step=global_step, name=\"train\")\r\n\r\n    init = tf.initialize_variables (tf.all_variables(), name='init_all_vars_op')    \r\n    tf.train.write_graph (sess.graph_def, \"models/\", \"graph.pb\", as_text=False)\r\n```\r\n\r\nUnfortunately when I call the c++ code to run the train_op node, it'll throw below error:\r\n\r\n`E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'message' not in Op<name=PreventGradient; signature=input:T -> output:T; attr=T:type>; NodeDef: gradients/xentropy/xentropy_grad/PreventGradient = PreventGradient[T=DT_FLOAT, message=\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\'s interaction with tf.gradients()\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](xentropy/xentropy:1)\r\n`\r\nI'm still confused whether the error comes from a bug inside the TF code or not."}