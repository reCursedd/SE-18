{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/427718267", "html_url": "https://github.com/tensorflow/tensorflow/issues/11626#issuecomment-427718267", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11626", "id": 427718267, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzcxODI2Nw==", "user": {"login": "vipinpillai", "id": 1018780, "node_id": "MDQ6VXNlcjEwMTg3ODA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1018780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vipinpillai", "html_url": "https://github.com/vipinpillai", "followers_url": "https://api.github.com/users/vipinpillai/followers", "following_url": "https://api.github.com/users/vipinpillai/following{/other_user}", "gists_url": "https://api.github.com/users/vipinpillai/gists{/gist_id}", "starred_url": "https://api.github.com/users/vipinpillai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vipinpillai/subscriptions", "organizations_url": "https://api.github.com/users/vipinpillai/orgs", "repos_url": "https://api.github.com/users/vipinpillai/repos", "events_url": "https://api.github.com/users/vipinpillai/events{/privacy}", "received_events_url": "https://api.github.com/users/vipinpillai/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-08T04:16:01Z", "updated_at": "2018-10-08T04:16:01Z", "author_association": "NONE", "body_html": "<p>This issue is reproducible with the python API as well.</p>\n<p>In my case, I have added an additional term to the loss function (gradient of the cross entropy with respect to the input). In this case, second derivative of sparse_softmax_cross_entropy_with_logits needs to be computed during backprop and the following error is thrown:<br>\nLookupError: Gradient explicitly disabled. Reason: b\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()\"</p>\n<p>It is surprising that support for second order derivative is still missing for some of the key operations in TF.</p>", "body_text": "This issue is reproducible with the python API as well.\nIn my case, I have added an additional term to the loss function (gradient of the cross entropy with respect to the input). In this case, second derivative of sparse_softmax_cross_entropy_with_logits needs to be computed during backprop and the following error is thrown:\nLookupError: Gradient explicitly disabled. Reason: b\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()\"\nIt is surprising that support for second order derivative is still missing for some of the key operations in TF.", "body": "This issue is reproducible with the python API as well.\r\n\r\nIn my case, I have added an additional term to the loss function (gradient of the cross entropy with respect to the input). In this case, second derivative of sparse_softmax_cross_entropy_with_logits needs to be computed during backprop and the following error is thrown:\r\nLookupError: Gradient explicitly disabled. Reason: b\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()\"\r\n\r\nIt is surprising that support for second order derivative is still missing for some of the key operations in TF."}