{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15219", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15219/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15219/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15219/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15219", "id": 280567237, "node_id": "MDU6SXNzdWUyODA1NjcyMzc=", "number": 15219, "title": "tf.while_loop and tf.foldl do not support second order gradients", "user": {"login": "Bonnevie", "id": 5861991, "node_id": "MDQ6VXNlcjU4NjE5OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/5861991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bonnevie", "html_url": "https://github.com/Bonnevie", "followers_url": "https://api.github.com/users/Bonnevie/followers", "following_url": "https://api.github.com/users/Bonnevie/following{/other_user}", "gists_url": "https://api.github.com/users/Bonnevie/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bonnevie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bonnevie/subscriptions", "organizations_url": "https://api.github.com/users/Bonnevie/orgs", "repos_url": "https://api.github.com/users/Bonnevie/repos", "events_url": "https://api.github.com/users/Bonnevie/events{/privacy}", "received_events_url": "https://api.github.com/users/Bonnevie/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-12-08T17:48:02Z", "updated_at": "2017-12-11T18:40:27Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>The example</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nx <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">1</span>.)\nA <span class=\"pl-k\">=</span> tf.Variable(tf.ones((<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>))) \ncost <span class=\"pl-k\">=</span> tf.trace(tf.foldl(tf.matmul,tf.stack([x<span class=\"pl-k\">*</span>A <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>)])))\ntf.gradients(tf.gradients(cost, A), x)  \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> TypeError: Second-order gradient for while loops not supported.</span></pre></div>\n<p>illustrates that despite applying <code>tf.foldl</code> to a static list, the internal implementation via while loops leads to a type error. The problem disappears if the fold operation is carried out manually using a for loop. While implementing <code>foldl</code> using the while loop clearly makes the operation  more widely applicable, it seems problematic if syntactic sugar can lead to code that has qualitative differences from a naive implementation using a static loop. I cannot help but wonder whether <code>foldl</code> could be more efficient in the static case as well, although that is more of a conjecture.</p>\n<p>I think it would be nice if <code>foldl</code> (and other while loop derivatives) had a keyword that enabled or disabled the \"dynamic mode\" using while, or if, at the very least, the TypeError would occur at the <code>foldl</code>operation so that the error is easier to trace.</p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nCustom code.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 16.04 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\npip install.</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nv1.4.0-rc1-11-g130a514 1.4.0</li>\n<li><strong>Python version</strong>:<br>\n3.5.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nNot applicable.</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nNot applicable.</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nDid not use CUDA.</li>\n<li><strong>GPU model and memory</strong>:<br>\nDid not use GPU.</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nx <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">1</span>.)\nA <span class=\"pl-k\">=</span> tf.Variable(tf.ones((<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>))) \ncost <span class=\"pl-k\">=</span> tf.trace(tf.foldl(tf.matmul,tf.stack([x<span class=\"pl-k\">*</span>A <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>)])))\ntf.gradients(tf.gradients(cost, A), x)  \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> TypeError: Second-order gradient for while loops not supported.</span></pre></div>", "body_text": "The example\nimport tensorflow as tf\nx = tf.Variable(1.)\nA = tf.Variable(tf.ones((3,3))) \ncost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))\ntf.gradients(tf.gradients(cost, A), x)  \n# TypeError: Second-order gradient for while loops not supported.\nillustrates that despite applying tf.foldl to a static list, the internal implementation via while loops leads to a type error. The problem disappears if the fold operation is carried out manually using a for loop. While implementing foldl using the while loop clearly makes the operation  more widely applicable, it seems problematic if syntactic sugar can lead to code that has qualitative differences from a naive implementation using a static loop. I cannot help but wonder whether foldl could be more efficient in the static case as well, although that is more of a conjecture.\nI think it would be nice if foldl (and other while loop derivatives) had a keyword that enabled or disabled the \"dynamic mode\" using while, or if, at the very least, the TypeError would occur at the foldloperation so that the error is easier to trace.\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nCustom code.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 16.04 LTS\nTensorFlow installed from (source or binary):\npip install.\nTensorFlow version (use command below):\nv1.4.0-rc1-11-g130a514 1.4.0\nPython version:\n3.5.4\nBazel version (if compiling from source):\nNot applicable.\nGCC/Compiler version (if compiling from source):\nNot applicable.\nCUDA/cuDNN version:\nDid not use CUDA.\nGPU model and memory:\nDid not use GPU.\nExact command to reproduce:\n\nimport tensorflow as tf\nx = tf.Variable(1.)\nA = tf.Variable(tf.ones((3,3))) \ncost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))\ntf.gradients(tf.gradients(cost, A), x)  \n# TypeError: Second-order gradient for while loops not supported.", "body": "The example\r\n```python\r\nimport tensorflow as tf\r\nx = tf.Variable(1.)\r\nA = tf.Variable(tf.ones((3,3))) \r\ncost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))\r\ntf.gradients(tf.gradients(cost, A), x)  \r\n# TypeError: Second-order gradient for while loops not supported.\r\n```\r\nillustrates that despite applying `tf.foldl` to a static list, the internal implementation via while loops leads to a type error. The problem disappears if the fold operation is carried out manually using a for loop. While implementing `foldl` using the while loop clearly makes the operation  more widely applicable, it seems problematic if syntactic sugar can lead to code that has qualitative differences from a naive implementation using a static loop. I cannot help but wonder whether `foldl` could be more efficient in the static case as well, although that is more of a conjecture.\r\n\r\nI think it would be nice if `foldl` (and other while loop derivatives) had a keyword that enabled or disabled the \"dynamic mode\" using while, or if, at the very least, the TypeError would occur at the `foldl`operation so that the error is easier to trace.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCustom code.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\npip install.\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**:\r\n3.5.4 \r\n- **Bazel version (if compiling from source)**:\r\nNot applicable.\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNot applicable.\r\n- **CUDA/cuDNN version**:\r\nDid not use CUDA.\r\n- **GPU model and memory**:\r\nDid not use GPU.\r\n- **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.Variable(1.)\r\nA = tf.Variable(tf.ones((3,3))) \r\ncost = tf.trace(tf.foldl(tf.matmul,tf.stack([x*A for _ in range(3)])))\r\ntf.gradients(tf.gradients(cost, A), x)  \r\n# TypeError: Second-order gradient for while loops not supported.\r\n```"}