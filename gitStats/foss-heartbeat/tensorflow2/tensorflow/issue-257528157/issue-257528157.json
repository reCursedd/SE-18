{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13022", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13022/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13022/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13022/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/13022", "id": 257528157, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQwOTM1Mzk2", "number": 13022, "title": "Give accumulate_n op a gradient", "user": {"login": "frreiss", "id": 12436991, "node_id": "MDQ6VXNlcjEyNDM2OTkx", "avatar_url": "https://avatars1.githubusercontent.com/u/12436991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frreiss", "html_url": "https://github.com/frreiss", "followers_url": "https://api.github.com/users/frreiss/followers", "following_url": "https://api.github.com/users/frreiss/following{/other_user}", "gists_url": "https://api.github.com/users/frreiss/gists{/gist_id}", "starred_url": "https://api.github.com/users/frreiss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frreiss/subscriptions", "organizations_url": "https://api.github.com/users/frreiss/orgs", "repos_url": "https://api.github.com/users/frreiss/repos", "events_url": "https://api.github.com/users/frreiss/events{/privacy}", "received_events_url": "https://api.github.com/users/frreiss/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-09-13T21:32:46Z", "updated_at": "2018-10-05T18:42:06Z", "closed_at": "2017-09-26T23:51:11Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13022", "html_url": "https://github.com/tensorflow/tensorflow/pull/13022", "diff_url": "https://github.com/tensorflow/tensorflow/pull/13022.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/13022.patch"}, "body_html": "<p>This pull request addresses issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"234982067\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/10607\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/10607/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/10607\">#10607</a> by adding a gradient to the existing <code>accumulate_n</code> operator. I followed the approach suggested by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a>: rewrite <code>accumulate_n</code> as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation. Previously, this op had been implemented in Python as a constellation of lower-level ops, some of which are not differentiable.</p>\n<p><strong>Implementation Details</strong><br>\nI have added a new C++ op, <code>AccumulateN</code>, which serves as a placeholder for type inference and gradient computation. A new rewrite, implemented in <code>accumulate_n_optimizer.cc</code>, replaces this placeholder with a group of <code>AssignAdd</code> ops and some additional ops that create, initialize, and destroy temporary variables.</p>\n<p>The original Python code for <code>accumulate_n</code> has been replaced by a function that validates its arguments and creates an instance of the <code>AccumulateN</code> placeholder op.</p>\n<p><strong>Testing</strong><br>\nI added a more complete set of tests for <code>accumulate_n</code> in a previous pull request (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"249513778\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12196\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/12196/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/12196\">#12196</a>) to ensure that the op would still be correct after the changes in the current pull request. I also added one additional test to verify that <code>accumulate_n</code> now has a gradient. All the tests under <code>//tensorflow/python/...</code> currently pass on my MacOS and Linux test machines.</p>\n<p><strong>Things to Note</strong><br>\nThe semantics of the new implementation are broadly the same as the original, with the exception of one corner case. The original implementation allowed all the inputs to <code>accumulate_n</code> to have an undefined shape. My new code requires that at least one input have a defined shape; or that the user provides a shape using the <code>shape</code> argument to the <code>accumulate_n</code> function.</p>\n<p>While implementing the <code>AccumulateN</code> op, I noticed that the code to do the kind of shape initialization I needed was repeated verbatim at several places in the TensorFlow code base. I refactored this code into a new function <code>shape_inference::ExplicitShape()</code> and replaced all the existing copies.</p>", "body_text": "This pull request addresses issue #10607 by adding a gradient to the existing accumulate_n operator. I followed the approach suggested by @alextp: rewrite accumulate_n as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation. Previously, this op had been implemented in Python as a constellation of lower-level ops, some of which are not differentiable.\nImplementation Details\nI have added a new C++ op, AccumulateN, which serves as a placeholder for type inference and gradient computation. A new rewrite, implemented in accumulate_n_optimizer.cc, replaces this placeholder with a group of AssignAdd ops and some additional ops that create, initialize, and destroy temporary variables.\nThe original Python code for accumulate_n has been replaced by a function that validates its arguments and creates an instance of the AccumulateN placeholder op.\nTesting\nI added a more complete set of tests for accumulate_n in a previous pull request (#12196) to ensure that the op would still be correct after the changes in the current pull request. I also added one additional test to verify that accumulate_n now has a gradient. All the tests under //tensorflow/python/... currently pass on my MacOS and Linux test machines.\nThings to Note\nThe semantics of the new implementation are broadly the same as the original, with the exception of one corner case. The original implementation allowed all the inputs to accumulate_n to have an undefined shape. My new code requires that at least one input have a defined shape; or that the user provides a shape using the shape argument to the accumulate_n function.\nWhile implementing the AccumulateN op, I noticed that the code to do the kind of shape initialization I needed was repeated verbatim at several places in the TensorFlow code base. I refactored this code into a new function shape_inference::ExplicitShape() and replaced all the existing copies.", "body": "This pull request addresses issue #10607 by adding a gradient to the existing `accumulate_n` operator. I followed the approach suggested by @alextp: rewrite `accumulate_n` as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation. Previously, this op had been implemented in Python as a constellation of lower-level ops, some of which are not differentiable.\r\n\r\n**Implementation Details**\r\nI have added a new C++ op, `AccumulateN`, which serves as a placeholder for type inference and gradient computation. A new rewrite, implemented in `accumulate_n_optimizer.cc`, replaces this placeholder with a group of `AssignAdd` ops and some additional ops that create, initialize, and destroy temporary variables.\r\n\r\nThe original Python code for `accumulate_n` has been replaced by a function that validates its arguments and creates an instance of the `AccumulateN` placeholder op.\r\n\r\n**Testing**\r\nI added a more complete set of tests for `accumulate_n` in a previous pull request (https://github.com/tensorflow/tensorflow/pull/12196) to ensure that the op would still be correct after the changes in the current pull request. I also added one additional test to verify that `accumulate_n` now has a gradient. All the tests under `//tensorflow/python/...` currently pass on my MacOS and Linux test machines.\r\n\r\n**Things to Note**\r\nThe semantics of the new implementation are broadly the same as the original, with the exception of one corner case. The original implementation allowed all the inputs to `accumulate_n` to have an undefined shape. My new code requires that at least one input have a defined shape; or that the user provides a shape using the `shape` argument to the `accumulate_n` function.\r\n\r\nWhile implementing the `AccumulateN` op, I noticed that the code to do the kind of shape initialization I needed was repeated verbatim at several places in the TensorFlow code base. I refactored this code into a new function `shape_inference::ExplicitShape()` and replaced all the existing copies.\r\n\r\n\r\n"}