{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9596", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9596/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9596/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9596/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9596", "id": 225714873, "node_id": "MDU6SXNzdWUyMjU3MTQ4NzM=", "number": 9596, "title": "Synchronous distributed tensorflow training doesn't synchronize among workers", "user": {"login": "GD06", "id": 13307515, "node_id": "MDQ6VXNlcjEzMzA3NTE1", "avatar_url": "https://avatars2.githubusercontent.com/u/13307515?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GD06", "html_url": "https://github.com/GD06", "followers_url": "https://api.github.com/users/GD06/followers", "following_url": "https://api.github.com/users/GD06/following{/other_user}", "gists_url": "https://api.github.com/users/GD06/gists{/gist_id}", "starred_url": "https://api.github.com/users/GD06/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GD06/subscriptions", "organizations_url": "https://api.github.com/users/GD06/orgs", "repos_url": "https://api.github.com/users/GD06/repos", "events_url": "https://api.github.com/users/GD06/events{/privacy}", "received_events_url": "https://api.github.com/users/GD06/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-05-02T14:35:01Z", "updated_at": "2017-06-13T09:24:26Z", "closed_at": "2017-05-02T16:42:51Z", "author_association": "NONE", "body_html": "<h3>System Information:</h3>\n<ul>\n<li><strong>Debian 4.5.5</strong></li>\n<li><strong>TF installed from binary (pip3 install tensorflow-gpu==1.0.1 --user)</strong></li>\n<li><strong>TF version: v1.0.0-65-g4763edf-dirty 1.0.1</strong></li>\n<li><strong>Bazel version: N.A.</strong></li>\n<li><strong>CUDA 8.0 cuDNN v5.1</strong></li>\n</ul>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Make a directory and download the following files into it:<br>\n<a href=\"https://gist.github.com/GD06/b88d4b20a4587add984323525bbf4be2\">training.py</a> <a href=\"https://gist.github.com/GD06/0bdb36ba6b76070b7cd7fadfd30fce6b\">run.sh</a></li>\n<li>Run the command ./run.sh to simply reproduce this issue.</li>\n</ol>\n<h3>Detailed descriptions for the bug</h3>\n<p>Recently, I tried to deploy the synchronous distributed tensorflow training on the cluster. I followed the tutorial and the inception example to write my own program. The <a href=\"https://gist.github.com/GD06/b88d4b20a4587add984323525bbf4be2\">training.py</a> is from other user's <a href=\"http://ischlag.github.io/2016/06/12/async-distributed-tensorflow/\" rel=\"nofollow\">implementation</a>, which follows the same API usage as the official example. I modified it to enable it running on a single machine with multiple GPUs by making them communicate through localhost and mapping each worker to see only one GPU.</p>\n<p>The <a href=\"https://gist.github.com/GD06/0bdb36ba6b76070b7cd7fadfd30fce6b\">run.sh</a> launched three processes. One of them is the parameter server and the others are two workers implemented by between-graph replication. I created the training supervisor by tf.train.Supervisor() to manage multiple sessions in the distributed training for the initialization and synchronization.</p>\n<p>I expect these two workers would synchronize each batch and work in the same epoch. However, the worker 0, which is launched prior to the worker 1, completed the whole training set without waiting for the worker 1. After that, the process of the worker 0 finished training process and exited normally while worker 1 behaved like falling into the deadlock and keep near 0% utilization of CPU and GPU for several hours.</p>\n<p>Based on my observation, I suspect these two workers didn't communicate and synchronize at all for the data they passed. I report this problem as a bug because I create the optimizer tf.train.SyncReplicasOptimizer as suggested by the official website and the inception example. However, it seems that the synchronization behaviors, if any, are very strange and the program can not exit normally.</p>\n<h3>Source code / logs</h3>\n<p>Two files:<br>\n<a href=\"https://gist.github.com/GD06/b88d4b20a4587add984323525bbf4be2\">training.py</a>: This file contains the source code for the parameter server and workers created to use synchronous distributed optimizers (tf.train.SyncReplicasOptimizer).<br>\n<a href=\"https://gist.github.com/GD06/0bdb36ba6b76070b7cd7fadfd30fce6b\">run.sh</a>: This file launched the parameter server and the workers.<br>\nLog:<br>\nPlease produce according to the steps and look at worker_0_log and worker_1_log</p>\n<h3>Update</h3>\n<p>To ensure that two workers don't synchronize with each other, I write another <a href=\"https://gist.github.com/GD06/b56d62b718bbd0b033703320e4b214dc\">training.py</a> by making the worker 1 exist after the first time output information. Then the worker 0 continue to execute without waiting for the worker 1 and obtain the final trained model. This kind of behavior surely disobeys the definition of the synchronous distributed training.</p>", "body_text": "System Information:\n\nDebian 4.5.5\nTF installed from binary (pip3 install tensorflow-gpu==1.0.1 --user)\nTF version: v1.0.0-65-g4763edf-dirty 1.0.1\nBazel version: N.A.\nCUDA 8.0 cuDNN v5.1\n\nSteps to reproduce\n\nMake a directory and download the following files into it:\ntraining.py run.sh\nRun the command ./run.sh to simply reproduce this issue.\n\nDetailed descriptions for the bug\nRecently, I tried to deploy the synchronous distributed tensorflow training on the cluster. I followed the tutorial and the inception example to write my own program. The training.py is from other user's implementation, which follows the same API usage as the official example. I modified it to enable it running on a single machine with multiple GPUs by making them communicate through localhost and mapping each worker to see only one GPU.\nThe run.sh launched three processes. One of them is the parameter server and the others are two workers implemented by between-graph replication. I created the training supervisor by tf.train.Supervisor() to manage multiple sessions in the distributed training for the initialization and synchronization.\nI expect these two workers would synchronize each batch and work in the same epoch. However, the worker 0, which is launched prior to the worker 1, completed the whole training set without waiting for the worker 1. After that, the process of the worker 0 finished training process and exited normally while worker 1 behaved like falling into the deadlock and keep near 0% utilization of CPU and GPU for several hours.\nBased on my observation, I suspect these two workers didn't communicate and synchronize at all for the data they passed. I report this problem as a bug because I create the optimizer tf.train.SyncReplicasOptimizer as suggested by the official website and the inception example. However, it seems that the synchronization behaviors, if any, are very strange and the program can not exit normally.\nSource code / logs\nTwo files:\ntraining.py: This file contains the source code for the parameter server and workers created to use synchronous distributed optimizers (tf.train.SyncReplicasOptimizer).\nrun.sh: This file launched the parameter server and the workers.\nLog:\nPlease produce according to the steps and look at worker_0_log and worker_1_log\nUpdate\nTo ensure that two workers don't synchronize with each other, I write another training.py by making the worker 1 exist after the first time output information. Then the worker 0 continue to execute without waiting for the worker 1 and obtain the final trained model. This kind of behavior surely disobeys the definition of the synchronous distributed training.", "body": "### System Information:\r\n- **Debian 4.5.5**\r\n- **TF installed from binary (pip3 install tensorflow-gpu==1.0.1 --user)**\r\n- **TF version: v1.0.0-65-g4763edf-dirty 1.0.1**\r\n- **Bazel version: N.A.**\r\n- **CUDA 8.0 cuDNN v5.1**\r\n\r\n### Steps to reproduce\r\n1. Make a directory and download the following files into it:\r\n[training.py](https://gist.github.com/GD06/b88d4b20a4587add984323525bbf4be2) [run.sh](https://gist.github.com/GD06/0bdb36ba6b76070b7cd7fadfd30fce6b)\r\n2. Run the command ./run.sh to simply reproduce this issue.\r\n\r\n### Detailed descriptions for the bug\r\n\r\nRecently, I tried to deploy the synchronous distributed tensorflow training on the cluster. I followed the tutorial and the inception example to write my own program. The [training.py](https://gist.github.com/GD06/b88d4b20a4587add984323525bbf4be2) is from other user's [implementation](http://ischlag.github.io/2016/06/12/async-distributed-tensorflow/), which follows the same API usage as the official example. I modified it to enable it running on a single machine with multiple GPUs by making them communicate through localhost and mapping each worker to see only one GPU.\r\n\r\nThe [run.sh](https://gist.github.com/GD06/0bdb36ba6b76070b7cd7fadfd30fce6b) launched three processes. One of them is the parameter server and the others are two workers implemented by between-graph replication. I created the training supervisor by tf.train.Supervisor() to manage multiple sessions in the distributed training for the initialization and synchronization. \r\n\r\nI expect these two workers would synchronize each batch and work in the same epoch. However, the worker 0, which is launched prior to the worker 1, completed the whole training set without waiting for the worker 1. After that, the process of the worker 0 finished training process and exited normally while worker 1 behaved like falling into the deadlock and keep near 0% utilization of CPU and GPU for several hours.\r\n\r\nBased on my observation, I suspect these two workers didn't communicate and synchronize at all for the data they passed. I report this problem as a bug because I create the optimizer tf.train.SyncReplicasOptimizer as suggested by the official website and the inception example. However, it seems that the synchronization behaviors, if any, are very strange and the program can not exit normally.\r\n\r\n### Source code / logs\r\nTwo files:\r\n[training.py](https://gist.github.com/GD06/b88d4b20a4587add984323525bbf4be2): This file contains the source code for the parameter server and workers created to use synchronous distributed optimizers (tf.train.SyncReplicasOptimizer). \r\n[run.sh](https://gist.github.com/GD06/0bdb36ba6b76070b7cd7fadfd30fce6b): This file launched the parameter server and the workers.\r\nLog:\r\nPlease produce according to the steps and look at worker_0_log and worker_1_log\r\n\r\n### Update\r\nTo ensure that two workers don't synchronize with each other, I write another [training.py](https://gist.github.com/GD06/b56d62b718bbd0b033703320e4b214dc) by making the worker 1 exist after the first time output information. Then the worker 0 continue to execute without waiting for the worker 1 and obtain the final trained model. This kind of behavior surely disobeys the definition of the synchronous distributed training."}