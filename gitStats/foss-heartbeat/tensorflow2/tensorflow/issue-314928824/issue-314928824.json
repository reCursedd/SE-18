{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18588", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18588/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18588/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18588/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18588", "id": 314928824, "node_id": "MDU6SXNzdWUzMTQ5Mjg4MjQ=", "number": 18588, "title": "XLA implementation of FFT on CPU pulls in tf/core:framework", "user": {"login": "nuchi", "id": 16771734, "node_id": "MDQ6VXNlcjE2NzcxNzM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16771734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nuchi", "html_url": "https://github.com/nuchi", "followers_url": "https://api.github.com/users/nuchi/followers", "following_url": "https://api.github.com/users/nuchi/following{/other_user}", "gists_url": "https://api.github.com/users/nuchi/gists{/gist_id}", "starred_url": "https://api.github.com/users/nuchi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nuchi/subscriptions", "organizations_url": "https://api.github.com/users/nuchi/orgs", "repos_url": "https://api.github.com/users/nuchi/repos", "events_url": "https://api.github.com/users/nuchi/events{/privacy}", "received_events_url": "https://api.github.com/users/nuchi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "sanjoy", "id": 136291, "node_id": "MDQ6VXNlcjEzNjI5MQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/136291?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanjoy", "html_url": "https://github.com/sanjoy", "followers_url": "https://api.github.com/users/sanjoy/followers", "following_url": "https://api.github.com/users/sanjoy/following{/other_user}", "gists_url": "https://api.github.com/users/sanjoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanjoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanjoy/subscriptions", "organizations_url": "https://api.github.com/users/sanjoy/orgs", "repos_url": "https://api.github.com/users/sanjoy/repos", "events_url": "https://api.github.com/users/sanjoy/events{/privacy}", "received_events_url": "https://api.github.com/users/sanjoy/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sanjoy", "id": 136291, "node_id": "MDQ6VXNlcjEzNjI5MQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/136291?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sanjoy", "html_url": "https://github.com/sanjoy", "followers_url": "https://api.github.com/users/sanjoy/followers", "following_url": "https://api.github.com/users/sanjoy/following{/other_user}", "gists_url": "https://api.github.com/users/sanjoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/sanjoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sanjoy/subscriptions", "organizations_url": "https://api.github.com/users/sanjoy/orgs", "repos_url": "https://api.github.com/users/sanjoy/repos", "events_url": "https://api.github.com/users/sanjoy/events{/privacy}", "received_events_url": "https://api.github.com/users/sanjoy/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-04-17T06:57:14Z", "updated_at": "2018-05-20T21:28:44Z", "closed_at": "2018-05-20T21:28:44Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macos 10.11</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/63c6562df68ade3a03481874a71b536a4e02b6f5/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/63c6562df68ade3a03481874a71b536a4e02b6f5\"><tt>63c6562</tt></a> (master as of April 15 2018)</li>\n<li><strong>Python version</strong>:  n/a</li>\n<li><strong>Bazel version (if compiling from source)</strong>: n/a</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: n/a</li>\n<li><strong>CUDA/cuDNN version</strong>: n/a</li>\n<li><strong>GPU model and memory</strong>: n/a</li>\n<li><strong>Exact command to reproduce</strong>: <code>bazel build --config=opt //tensorflow/compiler/xla/service/cpu:runtime_fft</code>. It <em>works</em>, but see below.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Short version: The CPU implementation of the XLA FFT operation appears to pull in <code>tensorflow::Tensor</code> and <code>...::TensorShape</code> as a dependency, via <code>//tensorflow/core:framework</code>.</p>\n<p>Long version: The FFT implementation comes in three flavors; real-to-complex, complex-to-real, and complex-to-complex. The first two flavors involve allocating a temporary buffer for an intermediate step in the computation. This is currently achieved by creating a <code>tensorflow::Tensor</code> object. This requires linking against <code>//tensorflow/core:framework</code>.</p>\n<p>This feels like a bug, or at least unintentional and undesirable. For instance, every other op listed in <code>tensorflow/compiler/xla/service/cpu/BUILD</code>, besides runtime_fft, depends only on <code>:framework_lite</code>. My understanding re: allocating temporary space was that (at least for the AOT compiler, not sure about JIT) there were specific temporary buffers set aside, and that allocation should work through that system; not by just letting malloc run wild. Is that understanding correct? (<em>Aside: Eigen's FFT op internally calls malloc, regardless of FFT flavor, which likewise bypasses the AOT temporary buffers. Is that ok?</em>)</p>\n<p>Is anyone currently working on this? (Has anyone noticed anything awry?) If I were to try fixing this myself, would anyone have any suggestions on how to allocate a temporary buffer in an XLA-friendly way? I thought one possibility would be writing an Algebraic Simplifier pass to rewrite real-to-complex and complex-to-real flavors in terms of the complex-to-complex flavor (which doesn't need to create a <code>tf::Tensor</code>), but that's my only idea.</p>\n<h3>Source code / logs</h3>\n<p>References:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD\">tensorflow/compiler/xla/service/cpu/BUILD</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h\">tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos 10.11\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): commit 63c6562 (master as of April 15 2018)\nPython version:  n/a\nBazel version (if compiling from source): n/a\nGCC/Compiler version (if compiling from source): n/a\nCUDA/cuDNN version: n/a\nGPU model and memory: n/a\nExact command to reproduce: bazel build --config=opt //tensorflow/compiler/xla/service/cpu:runtime_fft. It works, but see below.\n\nDescribe the problem\nShort version: The CPU implementation of the XLA FFT operation appears to pull in tensorflow::Tensor and ...::TensorShape as a dependency, via //tensorflow/core:framework.\nLong version: The FFT implementation comes in three flavors; real-to-complex, complex-to-real, and complex-to-complex. The first two flavors involve allocating a temporary buffer for an intermediate step in the computation. This is currently achieved by creating a tensorflow::Tensor object. This requires linking against //tensorflow/core:framework.\nThis feels like a bug, or at least unintentional and undesirable. For instance, every other op listed in tensorflow/compiler/xla/service/cpu/BUILD, besides runtime_fft, depends only on :framework_lite. My understanding re: allocating temporary space was that (at least for the AOT compiler, not sure about JIT) there were specific temporary buffers set aside, and that allocation should work through that system; not by just letting malloc run wild. Is that understanding correct? (Aside: Eigen's FFT op internally calls malloc, regardless of FFT flavor, which likewise bypasses the AOT temporary buffers. Is that ok?)\nIs anyone currently working on this? (Has anyone noticed anything awry?) If I were to try fixing this myself, would anyone have any suggestions on how to allocate a temporary buffer in an XLA-friendly way? I thought one possibility would be writing an Algebraic Simplifier pass to rewrite real-to-complex and complex-to-real flavors in terms of the complex-to-complex flavor (which doesn't need to create a tf::Tensor), but that's my only idea.\nSource code / logs\nReferences:\ntensorflow/compiler/xla/service/cpu/BUILD\ntensorflow/compiler/xla/service/cpu/runtime_fft_impl.h", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos 10.11\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: commit 63c6562df68ade3a03481874a71b536a4e02b6f5 (master as of April 15 2018)\r\n- **Python version**:  n/a\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: `bazel build --config=opt //tensorflow/compiler/xla/service/cpu:runtime_fft`. It *works*, but see below.\r\n\r\n### Describe the problem\r\nShort version: The CPU implementation of the XLA FFT operation appears to pull in `tensorflow::Tensor` and `...::TensorShape` as a dependency, via `//tensorflow/core:framework`.\r\n\r\nLong version: The FFT implementation comes in three flavors; real-to-complex, complex-to-real, and complex-to-complex. The first two flavors involve allocating a temporary buffer for an intermediate step in the computation. This is currently achieved by creating a `tensorflow::Tensor` object. This requires linking against `//tensorflow/core:framework`.\r\n\r\nThis feels like a bug, or at least unintentional and undesirable. For instance, every other op listed in `tensorflow/compiler/xla/service/cpu/BUILD`, besides runtime_fft, depends only on `:framework_lite`. My understanding re: allocating temporary space was that (at least for the AOT compiler, not sure about JIT) there were specific temporary buffers set aside, and that allocation should work through that system; not by just letting malloc run wild. Is that understanding correct? (*Aside: Eigen's FFT op internally calls malloc, regardless of FFT flavor, which likewise bypasses the AOT temporary buffers. Is that ok?*)\r\n\r\nIs anyone currently working on this? (Has anyone noticed anything awry?) If I were to try fixing this myself, would anyone have any suggestions on how to allocate a temporary buffer in an XLA-friendly way? I thought one possibility would be writing an Algebraic Simplifier pass to rewrite real-to-complex and complex-to-real flavors in terms of the complex-to-complex flavor (which doesn't need to create a `tf::Tensor`), but that's my only idea.\r\n\r\n### Source code / logs\r\nReferences:\r\n[tensorflow/compiler/xla/service/cpu/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD)\r\n[tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h)"}