{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20914", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20914/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20914/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20914/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20914", "id": 342247728, "node_id": "MDU6SXNzdWUzNDIyNDc3Mjg=", "number": 20914, "title": "ResourceVariable save will lead to OOM in distributed mode", "user": {"login": "jackonan", "id": 9108860, "node_id": "MDQ6VXNlcjkxMDg4NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/9108860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jackonan", "html_url": "https://github.com/jackonan", "followers_url": "https://api.github.com/users/jackonan/followers", "following_url": "https://api.github.com/users/jackonan/following{/other_user}", "gists_url": "https://api.github.com/users/jackonan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jackonan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jackonan/subscriptions", "organizations_url": "https://api.github.com/users/jackonan/orgs", "repos_url": "https://api.github.com/users/jackonan/repos", "events_url": "https://api.github.com/users/jackonan/events{/privacy}", "received_events_url": "https://api.github.com/users/jackonan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "agarwal-ashish", "id": 19335798, "node_id": "MDQ6VXNlcjE5MzM1Nzk4", "avatar_url": "https://avatars3.githubusercontent.com/u/19335798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agarwal-ashish", "html_url": "https://github.com/agarwal-ashish", "followers_url": "https://api.github.com/users/agarwal-ashish/followers", "following_url": "https://api.github.com/users/agarwal-ashish/following{/other_user}", "gists_url": "https://api.github.com/users/agarwal-ashish/gists{/gist_id}", "starred_url": "https://api.github.com/users/agarwal-ashish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agarwal-ashish/subscriptions", "organizations_url": "https://api.github.com/users/agarwal-ashish/orgs", "repos_url": "https://api.github.com/users/agarwal-ashish/repos", "events_url": "https://api.github.com/users/agarwal-ashish/events{/privacy}", "received_events_url": "https://api.github.com/users/agarwal-ashish/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "agarwal-ashish", "id": 19335798, "node_id": "MDQ6VXNlcjE5MzM1Nzk4", "avatar_url": "https://avatars3.githubusercontent.com/u/19335798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agarwal-ashish", "html_url": "https://github.com/agarwal-ashish", "followers_url": "https://api.github.com/users/agarwal-ashish/followers", "following_url": "https://api.github.com/users/agarwal-ashish/following{/other_user}", "gists_url": "https://api.github.com/users/agarwal-ashish/gists{/gist_id}", "starred_url": "https://api.github.com/users/agarwal-ashish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agarwal-ashish/subscriptions", "organizations_url": "https://api.github.com/users/agarwal-ashish/orgs", "repos_url": "https://api.github.com/users/agarwal-ashish/repos", "events_url": "https://api.github.com/users/agarwal-ashish/events{/privacy}", "received_events_url": "https://api.github.com/users/agarwal-ashish/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-18T09:28:59Z", "updated_at": "2018-07-23T21:31:53Z", "closed_at": "2018-07-23T21:31:53Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 2.7.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.9</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.8.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 7.5</li>\n<li><strong>GPU model and memory</strong>: None</li>\n<li><strong>Exact command to reproduce</strong>: save ResourceVariable in a distributed mode</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>My model is more than 200GB so I run it in distributed mode on CPU, including 1000 workers and 100 ps. All the variables of my model are ResourceVariable partitioned by size,  and these variables are placed by the default tf.replica_device_setter. The model is triggered by a MonitoredTrainingSession.</p>\n<p>Problem happens when it begins to save model. The memory of ps0 rises to 200GB rapidly and then OOM.  I open the log placement and find that all variables are identity to ps0 when running save_op.<br>\nIn ResourceVariableSaveable, line 182 reset the device, which leads to all save_ops are placed on ps0. I remove this line and re-run, it works correctly.</p>\n<pre><code> 168   class ResourceVariableSaveable(SaveableObject):\n 169     \"\"\"SaveableObject implementation that handles ResourceVariables.\"\"\"\n 170\n 171     def __init__(self, var, slice_spec, name):\n 172       self._var_device = var.device\n 173       if isinstance(var, ops.Tensor):\n 174         self.handle_op = var.op.inputs[0]\n 175         tensor = var\n 176       elif isinstance(var, resource_variable_ops.ResourceVariable):\n 177\n 178         def _read_variable_closure(v):\n 179           def f():\n 180             with ops.device(v.device):\n 181               x = v.read_value()\n 182             with ops.device(\"/device:CPU:0\"):\n 183               return array_ops.identity(x)\n 184           return f\n 185\n 186         self.handle_op = var.handle\n 187         tensor = _read_variable_closure(var)\n 188       else:\n 189         raise ValueError(\n 190             \"Saveable is neither a resource variable nor a read operation.\"\n 191             \" Got: %s\" % repr(var))\n 192       spec = BaseSaverBuilder.SaveSpec(tensor, slice_spec, name)\n 193       super(BaseSaverBuilder.ResourceVariableSaveable, self).__init__(\n 194           var, [spec], name)\n</code></pre>\n<h3>Source code / logs</h3>\n<p>Open the log placement to see all save_ops are placed on ps0. So many such logs</p>\n<pre><code> [2018-07-18 16:02:41.883522] [INFO] [31791] [tensorflow/core/common_runtime/placer.cc:698] Ignoring device specification /device:CPU:0 for node 'save_2/AssignVariableOp_176' because the input edge from 'Optimize/OptimizeLoss/CTR-PositionNetwork/position_hiddenlayer_1/weights/part_7/AdagradDecay_1'       is a reference connection and already has a device field set to /job:ps/task:16\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2205249/log.txt\">log.txt</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.4\nPython version: 2.7.5\nBazel version (if compiling from source): 0.9\nGCC/Compiler version (if compiling from source): 4.8.5\nCUDA/cuDNN version: 7.5\nGPU model and memory: None\nExact command to reproduce: save ResourceVariable in a distributed mode\n\nDescribe the problem\nMy model is more than 200GB so I run it in distributed mode on CPU, including 1000 workers and 100 ps. All the variables of my model are ResourceVariable partitioned by size,  and these variables are placed by the default tf.replica_device_setter. The model is triggered by a MonitoredTrainingSession.\nProblem happens when it begins to save model. The memory of ps0 rises to 200GB rapidly and then OOM.  I open the log placement and find that all variables are identity to ps0 when running save_op.\nIn ResourceVariableSaveable, line 182 reset the device, which leads to all save_ops are placed on ps0. I remove this line and re-run, it works correctly.\n 168   class ResourceVariableSaveable(SaveableObject):\n 169     \"\"\"SaveableObject implementation that handles ResourceVariables.\"\"\"\n 170\n 171     def __init__(self, var, slice_spec, name):\n 172       self._var_device = var.device\n 173       if isinstance(var, ops.Tensor):\n 174         self.handle_op = var.op.inputs[0]\n 175         tensor = var\n 176       elif isinstance(var, resource_variable_ops.ResourceVariable):\n 177\n 178         def _read_variable_closure(v):\n 179           def f():\n 180             with ops.device(v.device):\n 181               x = v.read_value()\n 182             with ops.device(\"/device:CPU:0\"):\n 183               return array_ops.identity(x)\n 184           return f\n 185\n 186         self.handle_op = var.handle\n 187         tensor = _read_variable_closure(var)\n 188       else:\n 189         raise ValueError(\n 190             \"Saveable is neither a resource variable nor a read operation.\"\n 191             \" Got: %s\" % repr(var))\n 192       spec = BaseSaverBuilder.SaveSpec(tensor, slice_spec, name)\n 193       super(BaseSaverBuilder.ResourceVariableSaveable, self).__init__(\n 194           var, [spec], name)\n\nSource code / logs\nOpen the log placement to see all save_ops are placed on ps0. So many such logs\n [2018-07-18 16:02:41.883522] [INFO] [31791] [tensorflow/core/common_runtime/placer.cc:698] Ignoring device specification /device:CPU:0 for node 'save_2/AssignVariableOp_176' because the input edge from 'Optimize/OptimizeLoss/CTR-PositionNetwork/position_hiddenlayer_1/weights/part_7/AdagradDecay_1'       is a reference connection and already has a device field set to /job:ps/task:16\n\nlog.txt", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: 0.9\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 7.5\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: save ResourceVariable in a distributed mode\r\n\r\n\r\n### Describe the problem\r\nMy model is more than 200GB so I run it in distributed mode on CPU, including 1000 workers and 100 ps. All the variables of my model are ResourceVariable partitioned by size,  and these variables are placed by the default tf.replica_device_setter. The model is triggered by a MonitoredTrainingSession. \r\n\r\nProblem happens when it begins to save model. The memory of ps0 rises to 200GB rapidly and then OOM.  I open the log placement and find that all variables are identity to ps0 when running save_op.\r\nIn ResourceVariableSaveable, line 182 reset the device, which leads to all save_ops are placed on ps0. I remove this line and re-run, it works correctly.\r\n\r\n```\r\n 168   class ResourceVariableSaveable(SaveableObject):\r\n 169     \"\"\"SaveableObject implementation that handles ResourceVariables.\"\"\"\r\n 170\r\n 171     def __init__(self, var, slice_spec, name):\r\n 172       self._var_device = var.device\r\n 173       if isinstance(var, ops.Tensor):\r\n 174         self.handle_op = var.op.inputs[0]\r\n 175         tensor = var\r\n 176       elif isinstance(var, resource_variable_ops.ResourceVariable):\r\n 177\r\n 178         def _read_variable_closure(v):\r\n 179           def f():\r\n 180             with ops.device(v.device):\r\n 181               x = v.read_value()\r\n 182             with ops.device(\"/device:CPU:0\"):\r\n 183               return array_ops.identity(x)\r\n 184           return f\r\n 185\r\n 186         self.handle_op = var.handle\r\n 187         tensor = _read_variable_closure(var)\r\n 188       else:\r\n 189         raise ValueError(\r\n 190             \"Saveable is neither a resource variable nor a read operation.\"\r\n 191             \" Got: %s\" % repr(var))\r\n 192       spec = BaseSaverBuilder.SaveSpec(tensor, slice_spec, name)\r\n 193       super(BaseSaverBuilder.ResourceVariableSaveable, self).__init__(\r\n 194           var, [spec], name)\r\n```\r\n\r\n### Source code / logs\r\nOpen the log placement to see all save_ops are placed on ps0. So many such logs\r\n```\r\n [2018-07-18 16:02:41.883522] [INFO] [31791] [tensorflow/core/common_runtime/placer.cc:698] Ignoring device specification /device:CPU:0 for node 'save_2/AssignVariableOp_176' because the input edge from 'Optimize/OptimizeLoss/CTR-PositionNetwork/position_hiddenlayer_1/weights/part_7/AdagradDecay_1'       is a reference connection and already has a device field set to /job:ps/task:16\r\n```\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/2205249/log.txt)\r\n\r\n"}