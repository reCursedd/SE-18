{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436836068", "html_url": "https://github.com/tensorflow/tensorflow/pull/21222#issuecomment-436836068", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21222", "id": 436836068, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjgzNjA2OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-08T01:07:14Z", "updated_at": "2018-11-08T01:07:14Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">+francois, +alex passos\n\nHere's a case where using vs.get_variable() vs using self.add_variable()\nleads to different model convergence quality.  the call is happening inside\nthe NTM RNN layer's zero_state() function.  Any thoughts on what could be\nthe issue?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Nov 7, 2018 at 4:36 PM, Mark Collier ***@***.***&gt; wrote:\n I am specifying the initializer (see below).\n\n def build(self, inputs_shape):\n     ...\n     self._M = self.add_variable(\n         'memory',\n         shape=[self.memory_size, self.memory_vector_dim],\n         initializer=init_ops.constant_initializer(1e-6, dtype=self.dtype),\n         trainable=True)\n     print('Memory Variable (add_variable)', self._M)\n     print('Memory initializer', self._M.initializer)\n     print('Memory initial value', self._M.initial_value)\n\n Output:\n\n Memory Variable (add_variable) &lt;tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref&gt;\n Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n op: \"Assign\"\n input: \"root/rnn/ntm_cell/memory\"\n input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n attr {\n   key: \"T\"\n   value {\n     type: DT_FLOAT\n   }\n }\n attr {\n   key: \"_class\"\n   value {\n     list {\n       s: ***@***.***/rnn/ntm_cell/memory\"\n     }\n   }\n }\n attr {\n   key: \"use_locking\"\n   value {\n     b: true\n   }\n }\n attr {\n   key: \"validate_shape\"\n   value {\n     b: true\n   }\n }\n\n Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n\n def build(self, inputs_shape):\n     ...\n     self._M = vs.get_variable('memory',\n       [self.memory_size, self.memory_vector_dim],\n       initializer=init_ops.constant_initializer(1e-6))\n     print('Memory Variable (get_variable)', self._M)\n     print('Memory initializer', self._M.initializer)\n     print('Memory initial value', self._M.initial_value)\n\n Output:\n\n Memory Variable (get_variable) &lt;tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref&gt;\n Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n op: \"Assign\"\n input: \"root/rnn/ntm_cell/memory\"\n input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n attr {\n   key: \"T\"\n   value {\n     type: DT_FLOAT\n   }\n }\n attr {\n   key: \"_class\"\n   value {\n     list {\n       s: ***@***.***/rnn/ntm_cell/memory\"\n     }\n   }\n }\n attr {\n   key: \"use_locking\"\n   value {\n     b: true\n   }\n }\n attr {\n   key: \"validate_shape\"\n   value {\n     b: true\n   }\n }\n\n Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n\n As far as I can tell these should be exactly the same - but I consistently\n get different performance with the two approaches.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345536176\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/21222\" href=\"https://github.com/tensorflow/tensorflow/pull/21222#issuecomment-436829978\">#21222 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb\">https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "+francois, +alex passos\n\nHere's a case where using vs.get_variable() vs using self.add_variable()\nleads to different model convergence quality.  the call is happening inside\nthe NTM RNN layer's zero_state() function.  Any thoughts on what could be\nthe issue?\n\u2026\nOn Wed, Nov 7, 2018 at 4:36 PM, Mark Collier ***@***.***> wrote:\n I am specifying the initializer (see below).\n\n def build(self, inputs_shape):\n     ...\n     self._M = self.add_variable(\n         'memory',\n         shape=[self.memory_size, self.memory_vector_dim],\n         initializer=init_ops.constant_initializer(1e-6, dtype=self.dtype),\n         trainable=True)\n     print('Memory Variable (add_variable)', self._M)\n     print('Memory initializer', self._M.initializer)\n     print('Memory initial value', self._M.initial_value)\n\n Output:\n\n Memory Variable (add_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n op: \"Assign\"\n input: \"root/rnn/ntm_cell/memory\"\n input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n attr {\n   key: \"T\"\n   value {\n     type: DT_FLOAT\n   }\n }\n attr {\n   key: \"_class\"\n   value {\n     list {\n       s: ***@***.***/rnn/ntm_cell/memory\"\n     }\n   }\n }\n attr {\n   key: \"use_locking\"\n   value {\n     b: true\n   }\n }\n attr {\n   key: \"validate_shape\"\n   value {\n     b: true\n   }\n }\n\n Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n\n def build(self, inputs_shape):\n     ...\n     self._M = vs.get_variable('memory',\n       [self.memory_size, self.memory_vector_dim],\n       initializer=init_ops.constant_initializer(1e-6))\n     print('Memory Variable (get_variable)', self._M)\n     print('Memory initializer', self._M.initializer)\n     print('Memory initial value', self._M.initial_value)\n\n Output:\n\n Memory Variable (get_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n op: \"Assign\"\n input: \"root/rnn/ntm_cell/memory\"\n input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n attr {\n   key: \"T\"\n   value {\n     type: DT_FLOAT\n   }\n }\n attr {\n   key: \"_class\"\n   value {\n     list {\n       s: ***@***.***/rnn/ntm_cell/memory\"\n     }\n   }\n }\n attr {\n   key: \"use_locking\"\n   value {\n     b: true\n   }\n }\n attr {\n   key: \"validate_shape\"\n   value {\n     b: true\n   }\n }\n\n Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n\n As far as I can tell these should be exactly the same - but I consistently\n get different performance with the two approaches.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#21222 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb>\n .", "body": "+francois, +alex passos\n\nHere's a case where using vs.get_variable() vs using self.add_variable()\nleads to different model convergence quality.  the call is happening inside\nthe NTM RNN layer's zero_state() function.  Any thoughts on what could be\nthe issue?\n\nOn Wed, Nov 7, 2018 at 4:36 PM, Mark Collier <notifications@github.com>\nwrote:\n\n> I am specifying the initializer (see below).\n>\n> def build(self, inputs_shape):\n>     ...\n>     self._M = self.add_variable(\n>         'memory',\n>         shape=[self.memory_size, self.memory_vector_dim],\n>         initializer=init_ops.constant_initializer(1e-6, dtype=self.dtype),\n>         trainable=True)\n>     print('Memory Variable (add_variable)', self._M)\n>     print('Memory initializer', self._M.initializer)\n>     print('Memory initial value', self._M.initial_value)\n>\n> Output:\n>\n> Memory Variable (add_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n> Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n> op: \"Assign\"\n> input: \"root/rnn/ntm_cell/memory\"\n> input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n> attr {\n>   key: \"T\"\n>   value {\n>     type: DT_FLOAT\n>   }\n> }\n> attr {\n>   key: \"_class\"\n>   value {\n>     list {\n>       s: \"loc:@root/rnn/ntm_cell/memory\"\n>     }\n>   }\n> }\n> attr {\n>   key: \"use_locking\"\n>   value {\n>     b: true\n>   }\n> }\n> attr {\n>   key: \"validate_shape\"\n>   value {\n>     b: true\n>   }\n> }\n>\n> Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n>\n> def build(self, inputs_shape):\n>     ...\n>     self._M = vs.get_variable('memory',\n>       [self.memory_size, self.memory_vector_dim],\n>       initializer=init_ops.constant_initializer(1e-6))\n>     print('Memory Variable (get_variable)', self._M)\n>     print('Memory initializer', self._M.initializer)\n>     print('Memory initial value', self._M.initial_value)\n>\n> Output:\n>\n> Memory Variable (get_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n> Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n> op: \"Assign\"\n> input: \"root/rnn/ntm_cell/memory\"\n> input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n> attr {\n>   key: \"T\"\n>   value {\n>     type: DT_FLOAT\n>   }\n> }\n> attr {\n>   key: \"_class\"\n>   value {\n>     list {\n>       s: \"loc:@root/rnn/ntm_cell/memory\"\n>     }\n>   }\n> }\n> attr {\n>   key: \"use_locking\"\n>   value {\n>     b: true\n>   }\n> }\n> attr {\n>   key: \"validate_shape\"\n>   value {\n>     b: true\n>   }\n> }\n>\n> Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n>\n> As far as I can tell these should be exactly the same - but I consistently\n> get different performance with the two approaches.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21222#issuecomment-436829978>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb>\n> .\n>\n"}