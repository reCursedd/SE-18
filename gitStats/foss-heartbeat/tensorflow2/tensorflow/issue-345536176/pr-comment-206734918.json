{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/206734918", "pull_request_review_id": 142203668, "id": 206734918, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjczNDkxOA==", "diff_hunk": "@@ -3392,3 +3392,232 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)\n     return new_h, new_state\n+\n+class NTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Neural Turing Machine Cell with LSTM controller.\n+\n+    Implementation based on:\n+    https://arxiv.org/abs/1807.08518\n+    Mark Collier, Joeran Beel\n+\n+    which is in turn based on the source code of:\n+    https://github.com/snowkylin/ntm\n+\n+    and of course the original NTM paper:\n+    Neural Turing Machines\n+    https://arxiv.org/abs/1410.5401\n+    A Graves, G Wayne, I Danihelka\n+  \"\"\"\n+  def __init__(self, controller_layers, controller_units, memory_size,\n+    memory_vector_dim, read_head_num, write_head_num, shift_range=1,\n+    output_dim=None, clip_value=20, dtype=dtypes.float32, reuse=False):\n+    \"\"\"Initialize the NTM Cell.\n+\n+      Args:\n+        controller_layers: int, The number of layers in the LSTM controller.\n+        controller_units: int, The number of units per layer in the\n+          LSTM controller\n+        memory_size: int, The number of memory locations in the\n+          NTM memory matrix\n+        memory_vector_dim: int, The dimensionality of each location in the\n+          NTM memory matrix\n+        read_head_num: int, The number of read heads from the controller\n+          into memory\n+        write_head_num: int, The number of write heads from the controller\n+          into memory\n+        shift_range: int, The number of places to the left/right it is possible\n+          to iterate the previous address to in a single step\n+        output_dim: int, The number of dimensions to make a linear projection\n+          of the NTM controller outputs to.\n+          If None, no linear projection is applied\n+        clip_value: float, The maximum absolute value the controller parameters\n+          are clipped to\n+        dtype: Default dtype of the layer (default of `None` means use the type\n+          of the first input). Required when `build` is called before `call`.\n+        reuse: (optional) Python boolean describing whether to reuse variables\n+          in an existing scope.  If not `True`, and the existing scope already\n+          has the given variables, an error is raised.\n+    \"\"\"\n+    super(NTMCell, self).__init__(_reuse=reuse, dtype=dtype)\n+\n+    self.controller_layers = controller_layers\n+    self.controller_units = controller_units\n+    self.memory_size = memory_size\n+    self.memory_vector_dim = memory_vector_dim\n+    self.read_head_num = read_head_num\n+    self.write_head_num = write_head_num\n+    self.clip_value = clip_value\n+    self.reuse = reuse\n+\n+    def single_cell(num_units):\n+      return rnn_cell_impl.BasicLSTMCell(num_units, forget_bias=1.0)\n+\n+    self.controller = rnn_cell_impl.MultiRNNCell(\n+      [single_cell(self.controller_units)\n+        for _ in range(self.controller_layers)])\n+    self.NTMControllerState = collections.namedtuple('NTMControllerState',\n+      ('controller_state', 'read_vector_list', 'w_list', 'M'))\n+\n+    self.step = 0\n+    self.output_dim = output_dim\n+    self.shift_range = shift_range\n+\n+    self.o2p_initializer = init_ops.truncated_normal_initializer(\n+      stddev=1.0 / math.sqrt(self.controller_units), dtype=self.dtype)\n+    self.o2o_initializer = init_ops.truncated_normal_initializer(\n+      stddev=1.0 / math.sqrt(\n+        self.controller_units + self.memory_vector_dim * self.read_head_num),\n+      dtype=self.dtype)\n+\n+  def __call__(self, x, prev_state):\n+    prev_read_vector_list = prev_state.read_vector_list\n+\n+    controller_input = array_ops.concat([x] + prev_read_vector_list, axis=1)\n+    with vs.variable_scope('controller', reuse=self.reuse):\n+      controller_output, controller_state = self.controller(controller_input,\n+        prev_state.controller_state)\n+\n+    num_parameters_per_head = self.memory_vector_dim + 2 * self.shift_range + 4\n+    num_heads = self.read_head_num + self.write_head_num\n+    total_parameter_num = num_parameters_per_head * num_heads + \\\n+      self.memory_vector_dim * 2 * self.write_head_num\n+    with vs.variable_scope('o2p', reuse=(self.step > 0) or self.reuse):\n+      parameters = layers.fully_connected(\n+        controller_output, total_parameter_num, activation_fn=None,\n+        weights_initializer=self.o2p_initializer)\n+    parameters = clip_ops.clip_by_value(parameters,\n+      -self.clip_value, self.clip_value)\n+    head_parameter_list = array_ops.split(\n+      parameters[:, :num_parameters_per_head * num_heads], num_heads, axis=1)\n+    erase_add_list = array_ops.split(\n+      parameters[:, num_parameters_per_head * num_heads:],\n+      2 * self.write_head_num, axis=1)\n+\n+    prev_w_list = prev_state.w_list\n+    prev_M = prev_state.M\n+    w_list = []\n+    for i, head_parameter in enumerate(head_parameter_list):\n+      k = math_ops.tanh(head_parameter[:, 0:self.memory_vector_dim])\n+      beta = nn_ops.softplus(head_parameter[:, self.memory_vector_dim])\n+      g = math_ops.sigmoid(head_parameter[:, self.memory_vector_dim + 1])\n+      s = nn_ops.softmax(head_parameter[:,self.memory_vector_dim + 2:\n+        self.memory_vector_dim + 2 + (self.shift_range * 2 + 1)])\n+      gamma = nn_ops.softplus(head_parameter[:, -1]) + 1\n+      with vs.variable_scope('addressing_head_%d' % i):\n+        w = self.addressing(k, beta, g, s, gamma, prev_M, prev_w_list[i])\n+      w_list.append(w)\n+\n+    read_w_list = w_list[:self.read_head_num]\n+    read_vector_list = []\n+    for i in range(self.read_head_num):\n+      read_vector = math_ops.reduce_sum(\n+        array_ops.expand_dims(read_w_list[i], dim=2) * prev_M, axis=1)\n+      read_vector_list.append(read_vector)\n+\n+    write_w_list = w_list[self.read_head_num:]\n+    M = prev_M\n+    for i in range(self.write_head_num):\n+      w = array_ops.expand_dims(write_w_list[i], axis=2)\n+      erase_vector = array_ops.expand_dims(\n+        math_ops.sigmoid(erase_add_list[i * 2]), axis=1)\n+      add_vector = array_ops.expand_dims(\n+        math_ops.tanh(erase_add_list[i * 2 + 1]), axis=1)\n+      erase_M = array_ops.ones(M.get_shape()) - math_ops.matmul(w, erase_vector)\n+      M = M * erase_M + math_ops.matmul(w, add_vector)\n+\n+    if not self.output_dim:\n+      output_dim = x.get_shape()[1]", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 138, "commit_id": "88ede0cf971ecadea4931eeec134795a5bea43fd", "original_commit_id": "c7223e77c0896562dfcd779c5f880a4d08ad8ea8", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "Handle the case that x.shape[1].value is None.  Perhaps raise a ValueError", "created_at": "2018-08-01T02:01:31Z", "updated_at": "2018-11-09T16:01:50Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r206734918", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/206734918"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r206734918"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222"}}, "body_html": "<p>Handle the case that x.shape[1].value is None.  Perhaps raise a ValueError</p>", "body_text": "Handle the case that x.shape[1].value is None.  Perhaps raise a ValueError"}