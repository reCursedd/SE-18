{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/206734379", "pull_request_review_id": 142203668, "id": 206734379, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjczNDM3OQ==", "diff_hunk": "@@ -3392,3 +3392,232 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)\n     return new_h, new_state\n+\n+class NTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Neural Turing Machine Cell with LSTM controller.\n+\n+    Implementation based on:\n+    https://arxiv.org/abs/1807.08518\n+    Mark Collier, Joeran Beel\n+\n+    which is in turn based on the source code of:\n+    https://github.com/snowkylin/ntm\n+\n+    and of course the original NTM paper:\n+    Neural Turing Machines\n+    https://arxiv.org/abs/1410.5401\n+    A Graves, G Wayne, I Danihelka\n+  \"\"\"\n+  def __init__(self, controller_layers, controller_units, memory_size,\n+    memory_vector_dim, read_head_num, write_head_num, shift_range=1,\n+    output_dim=None, clip_value=20, dtype=dtypes.float32, reuse=False):\n+    \"\"\"Initialize the NTM Cell.\n+\n+      Args:\n+        controller_layers: int, The number of layers in the LSTM controller.\n+        controller_units: int, The number of units per layer in the\n+          LSTM controller\n+        memory_size: int, The number of memory locations in the\n+          NTM memory matrix\n+        memory_vector_dim: int, The dimensionality of each location in the\n+          NTM memory matrix\n+        read_head_num: int, The number of read heads from the controller\n+          into memory\n+        write_head_num: int, The number of write heads from the controller\n+          into memory\n+        shift_range: int, The number of places to the left/right it is possible\n+          to iterate the previous address to in a single step\n+        output_dim: int, The number of dimensions to make a linear projection\n+          of the NTM controller outputs to.\n+          If None, no linear projection is applied\n+        clip_value: float, The maximum absolute value the controller parameters\n+          are clipped to\n+        dtype: Default dtype of the layer (default of `None` means use the type\n+          of the first input). Required when `build` is called before `call`.\n+        reuse: (optional) Python boolean describing whether to reuse variables\n+          in an existing scope.  If not `True`, and the existing scope already\n+          has the given variables, an error is raised.\n+    \"\"\"\n+    super(NTMCell, self).__init__(_reuse=reuse, dtype=dtype)\n+\n+    self.controller_layers = controller_layers\n+    self.controller_units = controller_units\n+    self.memory_size = memory_size\n+    self.memory_vector_dim = memory_vector_dim\n+    self.read_head_num = read_head_num\n+    self.write_head_num = write_head_num\n+    self.clip_value = clip_value\n+    self.reuse = reuse\n+\n+    def single_cell(num_units):\n+      return rnn_cell_impl.BasicLSTMCell(num_units, forget_bias=1.0)\n+\n+    self.controller = rnn_cell_impl.MultiRNNCell(\n+      [single_cell(self.controller_units)\n+        for _ in range(self.controller_layers)])\n+    self.NTMControllerState = collections.namedtuple('NTMControllerState',\n+      ('controller_state', 'read_vector_list', 'w_list', 'M'))\n+\n+    self.step = 0\n+    self.output_dim = output_dim\n+    self.shift_range = shift_range\n+\n+    self.o2p_initializer = init_ops.truncated_normal_initializer(\n+      stddev=1.0 / math.sqrt(self.controller_units), dtype=self.dtype)\n+    self.o2o_initializer = init_ops.truncated_normal_initializer(\n+      stddev=1.0 / math.sqrt(\n+        self.controller_units + self.memory_vector_dim * self.read_head_num),\n+      dtype=self.dtype)\n+\n+  def __call__(self, x, prev_state):\n+    prev_read_vector_list = prev_state.read_vector_list\n+\n+    controller_input = array_ops.concat([x] + prev_read_vector_list, axis=1)\n+    with vs.variable_scope('controller', reuse=self.reuse):\n+      controller_output, controller_state = self.controller(controller_input,\n+        prev_state.controller_state)\n+\n+    num_parameters_per_head = self.memory_vector_dim + 2 * self.shift_range + 4\n+    num_heads = self.read_head_num + self.write_head_num\n+    total_parameter_num = num_parameters_per_head * num_heads + \\\n+      self.memory_vector_dim * 2 * self.write_head_num\n+    with vs.variable_scope('o2p', reuse=(self.step > 0) or self.reuse):", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 93, "commit_id": "88ede0cf971ecadea4931eeec134795a5bea43fd", "original_commit_id": "c7223e77c0896562dfcd779c5f880a4d08ad8ea8", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "Moving variable creation to build will remove the need you to this. However you'll need to create the fully connected layer variables yourself using self.add_variable", "created_at": "2018-08-01T01:56:45Z", "updated_at": "2018-11-09T16:01:50Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r206734379", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/206734379"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r206734379"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222"}}, "body_html": "<p>Moving variable creation to build will remove the need you to this. However you'll need to create the fully connected layer variables yourself using self.add_variable</p>", "body_text": "Moving variable creation to build will remove the need you to this. However you'll need to create the fully connected layer variables yourself using self.add_variable"}