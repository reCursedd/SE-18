{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/223063622", "pull_request_review_id": 162107345, "id": 223063622, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzA2MzYyMg==", "diff_hunk": "@@ -3392,3 +3393,305 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)\n     return new_h, new_state\n+\n+NTMControllerState = collections.namedtuple('NTMControllerState',\n+      ('controller_state', 'read_vector_list', 'w_list', 'M', 'time'))\n+\n+class NTMCell(rnn_cell_impl.LayerRNNCell):\n+  \"\"\"Neural Turing Machine Cell with RNN controller.\n+\n+    Implementation based on:\n+    https://arxiv.org/abs/1807.08518\n+    Mark Collier, Joeran Beel\n+\n+    which is in turn based on the source code of:\n+    https://github.com/snowkylin/ntm\n+\n+    and of course the original NTM paper:\n+    Neural Turing Machines\n+    https://arxiv.org/abs/1410.5401\n+    A Graves, G Wayne, I Danihelka\n+  \"\"\"\n+  def __init__(self, controller, memory_size,\n+    memory_vector_dim, read_head_num, write_head_num, shift_range=1,\n+    output_dim=None, clip_value=20, dtype=dtypes.float32,\n+    reuse=None, name=None):\n+    \"\"\"Initialize the NTM Cell.\n+\n+      Args:\n+        controller: an RNNCell, the RNN controller.\n+        memory_size: int, The number of memory locations in the\n+          NTM memory matrix\n+        memory_vector_dim: int, The dimensionality of each location in the\n+          NTM memory matrix\n+        read_head_num: int, The number of read heads from the controller\n+          into memory\n+        write_head_num: int, The number of write heads from the controller\n+          into memory\n+        shift_range: int, The number of places to the left/right it is possible\n+          to iterate the previous address to in a single step\n+        output_dim: int, The number of dimensions to make a linear projection\n+          of the NTM controller outputs to.\n+          If None, no linear projection is applied\n+        clip_value: float, The maximum absolute value the controller parameters\n+          are clipped to\n+        dtype: Default dtype of the layer (default of `None` means use the type\n+          of the first input). Required when `build` is called before `call`.\n+        reuse: (optional) Python boolean describing whether to reuse variables\n+          in an existing scope.  If not `True`, and the existing scope already\n+          has the given variables, an error is raised.\n+        name: String, the name of the layer. Layers with the same name will\n+          share weights, but to avoid mistakes we require reuse=True in such\n+          cases.\n+    \"\"\"\n+    super(NTMCell, self).__init__(_reuse=reuse, dtype=dtype, name=name)\n+\n+    rnn_cell_impl.assert_like_rnncell(\"NTM RNN controller cell\", controller)\n+\n+    self.controller = controller\n+    self.memory_size = memory_size\n+    self.memory_vector_dim = memory_vector_dim\n+    self.read_head_num = read_head_num\n+    self.write_head_num = write_head_num\n+    self.clip_value = clip_value\n+    self.reuse = reuse\n+\n+    self.output_dim = output_dim\n+    self.shift_range = shift_range\n+\n+    self.num_parameters_per_head = (\n+      self.memory_vector_dim + 2 * self.shift_range + 4)\n+    self.num_heads = self.read_head_num + self.write_head_num\n+    self.total_parameter_num = (\n+      self.num_parameters_per_head * self.num_heads +\n+      self.memory_vector_dim * 2 * self.write_head_num)\n+\n+  @property\n+  def state_size(self):\n+    return NTMControllerState(\n+      controller_state=self.controller.state_size,\n+      read_vector_list=[self.memory_vector_dim\n+        for _ in range(self.read_head_num)],\n+      w_list=[self.memory_size\n+        for _ in range(self.read_head_num + self.write_head_num)],\n+      M=tensor_shape.TensorShape([self.memory_size * self.memory_vector_dim]),\n+      time=tensor_shape.TensorShape([]))\n+\n+  @property\n+  def output_size(self):\n+    return self.output_dim\n+\n+  def build(self, inputs_shape):\n+    if self.output_dim is None:\n+      if inputs_shape[1].value is None:\n+        raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\n+                         % inputs_shape)\n+      else:\n+        self.output_dim = inputs_shape[1].value\n+\n+    def create_linear_initializer(input_size, dtype=dtypes.float32):\n+      stddev = 1.0 / math.sqrt(input_size)\n+      return init_ops.truncated_normal_initializer(stddev=stddev, dtype=dtype)\n+\n+    self._params_kernel = self.add_variable(\n+        'parameters_kernel',\n+        shape=[self.controller.output_size, self.total_parameter_num],\n+        initializer=create_linear_initializer(self.controller.output_size))\n+\n+    self._params_bias = self.add_variable(\n+        'parameters_bias',\n+        shape=[self.total_parameter_num],\n+        initializer=init_ops.constant_initializer(0.0, dtype=self.dtype))\n+\n+    self._output_kernel = self.add_variable(\n+        'output_kernel',\n+        shape=[self.controller.output_size +\n+          self.memory_vector_dim * self.read_head_num, self.output_dim],\n+        initializer=create_linear_initializer(\n+          self.controller.output_size +\n+          self.memory_vector_dim * self.read_head_num))\n+\n+    self._output_bias = self.add_variable(\n+        'output_bias',\n+        shape=[self.output_dim],\n+        initializer=init_ops.constant_initializer(0.0, dtype=self.dtype))\n+\n+    self._init_read_vectors = [self.add_variable(\n+        'initial_read_vector_%d' % i,\n+        shape=[1, self.memory_vector_dim],\n+        initializer=initializers.xavier_initializer())\n+        for i in range(self.read_head_num)]\n+\n+    self._init_address_weights = [self.add_variable(\n+        'initial_address_weights_%d' % i,\n+        shape=[1, self.memory_size],\n+        initializer=initializers.xavier_initializer())\n+        for i in range(self.read_head_num + self.write_head_num)]\n+\n+    self.built = True\n+\n+  def call(self, x, prev_state):\n+    # Addressing Mechanisms (Sec 3.3)\n+\n+    prev_read_vector_list = control_flow_ops.cond(\n+      math_ops.equal(prev_state.time, 0),\n+      lambda: [self._expand(", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 154, "commit_id": "88ede0cf971ecadea4931eeec134795a5bea43fd", "original_commit_id": "c372067163874a09c91c7aea008d2c5550d2b4ae", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "long lambdas are against our style guide.  can you move this to a def above, lke:\r\n\r\ndef _prev_read_vector_list_initial_value():\r\n  ...\r\n\r\nthen here, use cond(time == 0, _prev_read_vector_list_initial_value, ...)", "created_at": "2018-10-05T16:17:25Z", "updated_at": "2018-11-09T16:01:50Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r223063622", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/223063622"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r223063622"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222"}}, "body_html": "<p>long lambdas are against our style guide.  can you move this to a def above, lke:</p>\n<p>def _prev_read_vector_list_initial_value():<br>\n...</p>\n<p>then here, use cond(time == 0, _prev_read_vector_list_initial_value, ...)</p>", "body_text": "long lambdas are against our style guide.  can you move this to a def above, lke:\ndef _prev_read_vector_list_initial_value():\n...\nthen here, use cond(time == 0, _prev_read_vector_list_initial_value, ...)"}