{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/231724339", "pull_request_review_id": 172756884, "id": 231724339, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMTcyNDMzOQ==", "diff_hunk": "@@ -3392,3 +3393,305 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)\n     return new_h, new_state\n+\n+NTMControllerState = collections.namedtuple('NTMControllerState',\n+      ('controller_state', 'read_vector_list', 'w_list', 'M', 'time'))\n+\n+class NTMCell(rnn_cell_impl.LayerRNNCell):\n+  \"\"\"Neural Turing Machine Cell with RNN controller.\n+\n+    Implementation based on:\n+    https://arxiv.org/abs/1807.08518\n+    Mark Collier, Joeran Beel\n+\n+    which is in turn based on the source code of:\n+    https://github.com/snowkylin/ntm\n+\n+    and of course the original NTM paper:\n+    Neural Turing Machines\n+    https://arxiv.org/abs/1410.5401\n+    A Graves, G Wayne, I Danihelka\n+  \"\"\"\n+  def __init__(self, controller, memory_size,\n+    memory_vector_dim, read_head_num, write_head_num, shift_range=1,\n+    output_dim=None, clip_value=20, dtype=dtypes.float32,\n+    reuse=None, name=None):\n+    \"\"\"Initialize the NTM Cell.\n+\n+      Args:\n+        controller: an RNNCell, the RNN controller.\n+        memory_size: int, The number of memory locations in the\n+          NTM memory matrix\n+        memory_vector_dim: int, The dimensionality of each location in the\n+          NTM memory matrix\n+        read_head_num: int, The number of read heads from the controller\n+          into memory\n+        write_head_num: int, The number of write heads from the controller\n+          into memory\n+        shift_range: int, The number of places to the left/right it is possible\n+          to iterate the previous address to in a single step\n+        output_dim: int, The number of dimensions to make a linear projection\n+          of the NTM controller outputs to.\n+          If None, no linear projection is applied\n+        clip_value: float, The maximum absolute value the controller parameters\n+          are clipped to\n+        dtype: Default dtype of the layer (default of `None` means use the type\n+          of the first input). Required when `build` is called before `call`.\n+        reuse: (optional) Python boolean describing whether to reuse variables\n+          in an existing scope.  If not `True`, and the existing scope already\n+          has the given variables, an error is raised.\n+        name: String, the name of the layer. Layers with the same name will\n+          share weights, but to avoid mistakes we require reuse=True in such\n+          cases.\n+    \"\"\"\n+    super(NTMCell, self).__init__(_reuse=reuse, dtype=dtype, name=name)\n+\n+    rnn_cell_impl.assert_like_rnncell(\"NTM RNN controller cell\", controller)\n+\n+    self.controller = controller\n+    self.memory_size = memory_size\n+    self.memory_vector_dim = memory_vector_dim\n+    self.read_head_num = read_head_num\n+    self.write_head_num = write_head_num\n+    self.clip_value = clip_value\n+    self.reuse = reuse\n+\n+    self.output_dim = output_dim\n+    self.shift_range = shift_range\n+\n+    self.num_parameters_per_head = (\n+      self.memory_vector_dim + 2 * self.shift_range + 4)\n+    self.num_heads = self.read_head_num + self.write_head_num\n+    self.total_parameter_num = (\n+      self.num_parameters_per_head * self.num_heads +\n+      self.memory_vector_dim * 2 * self.write_head_num)\n+\n+  @property\n+  def state_size(self):\n+    return NTMControllerState(\n+      controller_state=self.controller.state_size,\n+      read_vector_list=[self.memory_vector_dim\n+        for _ in range(self.read_head_num)],\n+      w_list=[self.memory_size\n+        for _ in range(self.read_head_num + self.write_head_num)],\n+      M=tensor_shape.TensorShape([self.memory_size * self.memory_vector_dim]),\n+      time=tensor_shape.TensorShape([]))\n+\n+  @property\n+  def output_size(self):\n+    return self.output_dim\n+\n+  def build(self, inputs_shape):\n+    if self.output_dim is None:\n+      if inputs_shape[1].value is None:\n+        raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\n+                         % inputs_shape)\n+      else:\n+        self.output_dim = inputs_shape[1].value\n+\n+    def create_linear_initializer(input_size, dtype=dtypes.float32):\n+      stddev = 1.0 / math.sqrt(input_size)\n+      return init_ops.truncated_normal_initializer(stddev=stddev, dtype=dtype)\n+\n+    self._params_kernel = self.add_variable(\n+        'parameters_kernel',\n+        shape=[self.controller.output_size, self.total_parameter_num],\n+        initializer=create_linear_initializer(self.controller.output_size))\n+\n+    self._params_bias = self.add_variable(\n+        'parameters_bias',\n+        shape=[self.total_parameter_num],\n+        initializer=init_ops.constant_initializer(0.0, dtype=self.dtype))\n+\n+    self._output_kernel = self.add_variable(\n+        'output_kernel',\n+        shape=[self.controller.output_size +\n+          self.memory_vector_dim * self.read_head_num, self.output_dim],\n+        initializer=create_linear_initializer(\n+          self.controller.output_size +\n+          self.memory_vector_dim * self.read_head_num))\n+\n+    self._output_bias = self.add_variable(\n+        'output_bias',\n+        shape=[self.output_dim],\n+        initializer=init_ops.constant_initializer(0.0, dtype=self.dtype))\n+\n+    self._init_read_vectors = [self.add_variable(\n+        'initial_read_vector_%d' % i,\n+        shape=[1, self.memory_vector_dim],\n+        initializer=initializers.xavier_initializer())\n+        for i in range(self.read_head_num)]\n+\n+    self._init_address_weights = [self.add_variable(\n+        'initial_address_weights_%d' % i,\n+        shape=[1, self.memory_size],\n+        initializer=initializers.xavier_initializer())\n+        for i in range(self.read_head_num + self.write_head_num)]\n+\n+    self.built = True\n+\n+  def call(self, x, prev_state):\n+    # Addressing Mechanisms (Sec 3.3)\n+\n+    prev_read_vector_list = control_flow_ops.cond(\n+      math_ops.equal(prev_state.time, 0),\n+      lambda: [self._expand(\n+        math_ops.tanh(\n+          array_ops.squeeze(\n+            math_ops.matmul(\n+              array_ops.ones([1, 1]),\n+              self._init_read_vectors[i]))),\n+        dim=0, N=x.get_shape()[0])\n+        for i in range(self.read_head_num)],\n+      lambda: prev_state.read_vector_list)\n+    if self.read_head_num == 1:\n+      prev_read_vector_list = [prev_read_vector_list]\n+\n+    controller_input = array_ops.concat([x] + prev_read_vector_list, axis=1)\n+    with vs.variable_scope('controller', reuse=self.reuse):\n+      controller_output, controller_state = self.controller(controller_input,\n+        prev_state.controller_state)\n+\n+    parameters = math_ops.matmul(controller_output, self._params_kernel)\n+    parameters = nn_ops.bias_add(parameters, self._params_bias)\n+    parameters = clip_ops.clip_by_value(parameters,\n+      -self.clip_value, self.clip_value)\n+    head_parameter_list = array_ops.split(\n+      parameters[:, :self.num_parameters_per_head * self.num_heads],\n+      self.num_heads,\n+      axis=1)\n+    erase_add_list = array_ops.split(\n+      parameters[:, self.num_parameters_per_head * self.num_heads:],\n+      2 * self.write_head_num,\n+      axis=1)\n+\n+    prev_w_list = control_flow_ops.cond(\n+      math_ops.equal(prev_state.time, 0),\n+      lambda: [self._expand(\n+        nn_ops.softmax(array_ops.squeeze(\n+            math_ops.matmul(\n+              array_ops.ones([1, 1]),\n+              self._init_address_weights[i]))),\n+        dim=0, N=x.get_shape()[0])\n+        for i in range(self.read_head_num + self.write_head_num)],\n+      lambda: prev_state.w_list)\n+    if (self.read_head_num + self.write_head_num) == 1:\n+      prev_w_list = [prev_w_list]\n+\n+    prev_M = prev_state.M\n+    w_list = []\n+    for i, head_parameter in enumerate(head_parameter_list):\n+      k = math_ops.tanh(head_parameter[:, 0:self.memory_vector_dim])\n+      beta = nn_ops.softplus(head_parameter[:, self.memory_vector_dim])\n+      g = math_ops.sigmoid(head_parameter[:, self.memory_vector_dim + 1])\n+      s = nn_ops.softmax(head_parameter[:,self.memory_vector_dim + 2:\n+        self.memory_vector_dim + 2 + (self.shift_range * 2 + 1)])\n+      gamma = nn_ops.softplus(head_parameter[:, -1]) + 1\n+      with vs.variable_scope('addressing_head_%d' % i):\n+        w = self._addressing(k, beta, g, s, gamma, prev_M, prev_w_list[i])\n+      w_list.append(w)\n+\n+    # Reading (Sec 3.1)\n+\n+    read_w_list = w_list[:self.read_head_num]\n+    read_vector_list = []\n+    for i in range(self.read_head_num):\n+      read_vector = math_ops.reduce_sum(\n+        array_ops.expand_dims(read_w_list[i], dim=2) * prev_M, axis=1)\n+      read_vector_list.append(read_vector)\n+\n+    # Writing (Sec 3.2)\n+\n+    write_w_list = w_list[self.read_head_num:]\n+    M = prev_M\n+    for i in range(self.write_head_num):\n+      w = array_ops.expand_dims(write_w_list[i], axis=2)\n+      erase_vector = array_ops.expand_dims(\n+        math_ops.sigmoid(erase_add_list[i * 2]), axis=1)\n+      add_vector = array_ops.expand_dims(\n+        math_ops.tanh(erase_add_list[i * 2 + 1]), axis=1)\n+      erase_M = array_ops.ones_like(M) - math_ops.matmul(w, erase_vector)\n+      M = M * erase_M + math_ops.matmul(w, add_vector)\n+\n+    output = math_ops.matmul(\n+      array_ops.concat([controller_output] + read_vector_list, axis=1),\n+      self._output_kernel)\n+    output = nn_ops.bias_add(output, self._output_bias)\n+    output = clip_ops.clip_by_value(output, -self.clip_value, self.clip_value)\n+\n+    return output, NTMControllerState(\n+      controller_state=controller_state, read_vector_list=read_vector_list,\n+      w_list=w_list, M=M, time=prev_state.time+1)\n+\n+  def _expand(self, x, dim, N):\n+    return array_ops.concat(\n+      [array_ops.expand_dims(x, dim) for _ in range(N)], axis=dim)\n+\n+  def _addressing(self, k, beta, g, s, gamma, prev_M, prev_w):\n+    # Sec 3.3.1 Focusing by Content\n+\n+    k = array_ops.expand_dims(k, axis=2)\n+    inner_product = math_ops.matmul(prev_M, k)\n+    k_norm = math_ops.sqrt(math_ops.reduce_sum(\n+      math_ops.square(k), axis=1, keepdims=True))\n+    M_norm = math_ops.sqrt(math_ops.reduce_sum(\n+      math_ops.square(prev_M), axis=2, keepdims=True))\n+    norm_product = M_norm * k_norm\n+    \n+    # eq (6)\n+    K = array_ops.squeeze(inner_product / (norm_product + 1e-8))\n+\n+    K_amplified = math_ops.exp(array_ops.expand_dims(beta, axis=1) * K)\n+    \n+    # eq (5)\n+    w_c = K_amplified / math_ops.reduce_sum(K_amplified, axis=1, keepdims=True)\n+\n+    # Sec 3.3.2 Focusing by Location\n+\n+    g = array_ops.expand_dims(g, axis=1)\n+    \n+    # eq (7)\n+    w_g = g * w_c + (1 - g) * prev_w\n+\n+    s = array_ops.concat([s[:, :self.shift_range + 1],\n+      array_ops.zeros([s.get_shape()[0],\n+        self.memory_size - (self.shift_range * 2 + 1)]),\n+      s[:, -self.shift_range:]], axis=1)\n+    t = array_ops.concat([array_ops.reverse(s, axis=[1]),\n+      array_ops.reverse(s, axis=[1])], axis=1)\n+    s_matrix = array_ops.stack(\n+      [t[:, self.memory_size - i - 1:self.memory_size * 2 - i - 1]\n+        for i in range(self.memory_size)],\n+      axis=1)\n+    \n+    # eq (8)\n+    w_ = math_ops.reduce_sum(\n+      array_ops.expand_dims(w_g, axis=1) * s_matrix, axis=2)\n+    w_sharpen = math_ops.pow(w_, array_ops.expand_dims(gamma, axis=1))\n+\n+    # eq (9)\n+    w = w_sharpen / math_ops.reduce_sum(w_sharpen, axis=1, keepdims=True)\n+\n+    return w\n+\n+  def zero_state(self, batch_size, dtype):\n+    with vs.variable_scope('init', reuse=self.reuse):\n+      read_vector_list = [array_ops.zeros([batch_size, self.memory_vector_dim])\n+        for _ in range(self.read_head_num)]\n+\n+      w_list = [array_ops.zeros([batch_size, self.memory_size])\n+        for _ in range(self.read_head_num + self.write_head_num)]\n+\n+      controller_init_state = self.controller.zero_state(batch_size, dtype)\n+\n+      M = self._expand(vs.get_variable('init_M',", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 303, "commit_id": "88ede0cf971ecadea4931eeec134795a5bea43fd", "original_commit_id": "c372067163874a09c91c7aea008d2c5550d2b4ae", "user": {"login": "MarkPKCollier", "id": 1159632, "node_id": "MDQ6VXNlcjExNTk2MzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1159632?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkPKCollier", "html_url": "https://github.com/MarkPKCollier", "followers_url": "https://api.github.com/users/MarkPKCollier/followers", "following_url": "https://api.github.com/users/MarkPKCollier/following{/other_user}", "gists_url": "https://api.github.com/users/MarkPKCollier/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkPKCollier/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkPKCollier/subscriptions", "organizations_url": "https://api.github.com/users/MarkPKCollier/orgs", "repos_url": "https://api.github.com/users/MarkPKCollier/repos", "events_url": "https://api.github.com/users/MarkPKCollier/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkPKCollier/received_events", "type": "User", "site_admin": false}, "body": "Ok, this is where the issue arises. When I replace vs.get_variable with self.add_variable - I get much slower convergence, looking at the docs I don't see why this would be the case but I have tested that the initializer, trainable, etc. parameters are the same in both case.\r\n\r\nSo given that for whatever reason the convergence rates are different with what should be a change that doesn't make a difference, is it acceptable to use vs.get_variable in the `build` method and avoid creating a variable in `zero_state`?", "created_at": "2018-11-08T00:02:30Z", "updated_at": "2018-11-09T16:01:50Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r231724339", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/231724339"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21222#discussion_r231724339"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21222"}}, "body_html": "<p>Ok, this is where the issue arises. When I replace vs.get_variable with self.add_variable - I get much slower convergence, looking at the docs I don't see why this would be the case but I have tested that the initializer, trainable, etc. parameters are the same in both case.</p>\n<p>So given that for whatever reason the convergence rates are different with what should be a change that doesn't make a difference, is it acceptable to use vs.get_variable in the <code>build</code> method and avoid creating a variable in <code>zero_state</code>?</p>", "body_text": "Ok, this is where the issue arises. When I replace vs.get_variable with self.add_variable - I get much slower convergence, looking at the docs I don't see why this would be the case but I have tested that the initializer, trainable, etc. parameters are the same in both case.\nSo given that for whatever reason the convergence rates are different with what should be a change that doesn't make a difference, is it acceptable to use vs.get_variable in the build method and avoid creating a variable in zero_state?", "in_reply_to_id": 223065015}