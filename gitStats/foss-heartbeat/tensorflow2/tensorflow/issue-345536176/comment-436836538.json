{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436836538", "html_url": "https://github.com/tensorflow/tensorflow/pull/21222#issuecomment-436836538", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21222", "id": 436836538, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjgzNjUzOA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-08T01:09:53Z", "updated_at": "2018-11-08T01:09:53Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Weird, both are creating ref variables and seem to have the same\ninitializers. Could they be ending up in different collections making one\naccidentally trainable while the other isn't?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Nov 7, 2018 at 5:07 PM Eugene Brevdo ***@***.***&gt; wrote:\n +francois, +alex passos\n\n Here's a case where using vs.get_variable() vs using self.add_variable()\n leads to different model convergence quality.  the call is happening inside\n the NTM RNN layer's zero_state() function.  Any thoughts on what could be\n the issue?\n\n On Wed, Nov 7, 2018 at 4:36 PM, Mark Collier ***@***.***&gt;\n wrote:\n\n&gt; I am specifying the initializer (see below).\n&gt;\n&gt; def build(self, inputs_shape):\n&gt;     ...\n&gt;     self._M = self.add_variable(\n&gt;         'memory',\n&gt;         shape=[self.memory_size, self.memory_vector_dim],\n&gt;         initializer=init_ops.constant_initializer(1e-6, dtype=self.dtype),\n&gt;         trainable=True)\n&gt;     print('Memory Variable (add_variable)', self._M)\n&gt;     print('Memory initializer', self._M.initializer)\n&gt;     print('Memory initial value', self._M.initial_value)\n&gt;\n&gt; Output:\n&gt;\n&gt; Memory Variable (add_variable) &lt;tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref&gt;\n&gt; Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n&gt; op: \"Assign\"\n&gt; input: \"root/rnn/ntm_cell/memory\"\n&gt; input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n&gt; attr {\n&gt;   key: \"T\"\n&gt;   value {\n&gt;     type: DT_FLOAT\n&gt;   }\n&gt; }\n&gt; attr {\n&gt;   key: \"_class\"\n&gt;   value {\n&gt;     list {\n&gt;       s: ***@***.***/rnn/ntm_cell/memory\"\n&gt;     }\n&gt;   }\n&gt; }\n&gt; attr {\n&gt;   key: \"use_locking\"\n&gt;   value {\n&gt;     b: true\n&gt;   }\n&gt; }\n&gt; attr {\n&gt;   key: \"validate_shape\"\n&gt;   value {\n&gt;     b: true\n&gt;   }\n&gt; }\n&gt;\n&gt; Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n&gt;\n&gt; def build(self, inputs_shape):\n&gt;     ...\n&gt;     self._M = vs.get_variable('memory',\n&gt;       [self.memory_size, self.memory_vector_dim],\n&gt;       initializer=init_ops.constant_initializer(1e-6))\n&gt;     print('Memory Variable (get_variable)', self._M)\n&gt;     print('Memory initializer', self._M.initializer)\n&gt;     print('Memory initial value', self._M.initial_value)\n&gt;\n&gt; Output:\n&gt;\n&gt; Memory Variable (get_variable) &lt;tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref&gt;\n&gt; Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n&gt; op: \"Assign\"\n&gt; input: \"root/rnn/ntm_cell/memory\"\n&gt; input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n&gt; attr {\n&gt;   key: \"T\"\n&gt;   value {\n&gt;     type: DT_FLOAT\n&gt;   }\n&gt; }\n&gt; attr {\n&gt;   key: \"_class\"\n&gt;   value {\n&gt;     list {\n&gt;       s: ***@***.***/rnn/ntm_cell/memory\"\n&gt;     }\n&gt;   }\n&gt; }\n&gt; attr {\n&gt;   key: \"use_locking\"\n&gt;   value {\n&gt;     b: true\n&gt;   }\n&gt; }\n&gt; attr {\n&gt;   key: \"validate_shape\"\n&gt;   value {\n&gt;     b: true\n&gt;   }\n&gt; }\n&gt;\n&gt; Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n&gt;\n&gt; As far as I can tell these should be exactly the same - but I\n&gt; consistently get different performance with the two approaches.\n&gt;\n&gt; \u2014\n&gt; You are receiving this because you were mentioned.\n&gt; Reply to this email directly, view it on GitHub\n&gt; &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345536176\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/21222\" href=\"https://github.com/tensorflow/tensorflow/pull/21222#issuecomment-436829978\">#21222 (comment)</a>&gt;,\n&gt; or mute the thread\n&gt; &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb\">https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb</a>&gt;\n&gt; .\n&gt;\n\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "Weird, both are creating ref variables and seem to have the same\ninitializers. Could they be ending up in different collections making one\naccidentally trainable while the other isn't?\n\u2026\nOn Wed, Nov 7, 2018 at 5:07 PM Eugene Brevdo ***@***.***> wrote:\n +francois, +alex passos\n\n Here's a case where using vs.get_variable() vs using self.add_variable()\n leads to different model convergence quality.  the call is happening inside\n the NTM RNN layer's zero_state() function.  Any thoughts on what could be\n the issue?\n\n On Wed, Nov 7, 2018 at 4:36 PM, Mark Collier ***@***.***>\n wrote:\n\n> I am specifying the initializer (see below).\n>\n> def build(self, inputs_shape):\n>     ...\n>     self._M = self.add_variable(\n>         'memory',\n>         shape=[self.memory_size, self.memory_vector_dim],\n>         initializer=init_ops.constant_initializer(1e-6, dtype=self.dtype),\n>         trainable=True)\n>     print('Memory Variable (add_variable)', self._M)\n>     print('Memory initializer', self._M.initializer)\n>     print('Memory initial value', self._M.initial_value)\n>\n> Output:\n>\n> Memory Variable (add_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n> Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n> op: \"Assign\"\n> input: \"root/rnn/ntm_cell/memory\"\n> input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n> attr {\n>   key: \"T\"\n>   value {\n>     type: DT_FLOAT\n>   }\n> }\n> attr {\n>   key: \"_class\"\n>   value {\n>     list {\n>       s: ***@***.***/rnn/ntm_cell/memory\"\n>     }\n>   }\n> }\n> attr {\n>   key: \"use_locking\"\n>   value {\n>     b: true\n>   }\n> }\n> attr {\n>   key: \"validate_shape\"\n>   value {\n>     b: true\n>   }\n> }\n>\n> Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n>\n> def build(self, inputs_shape):\n>     ...\n>     self._M = vs.get_variable('memory',\n>       [self.memory_size, self.memory_vector_dim],\n>       initializer=init_ops.constant_initializer(1e-6))\n>     print('Memory Variable (get_variable)', self._M)\n>     print('Memory initializer', self._M.initializer)\n>     print('Memory initial value', self._M.initial_value)\n>\n> Output:\n>\n> Memory Variable (get_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n> Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n> op: \"Assign\"\n> input: \"root/rnn/ntm_cell/memory\"\n> input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n> attr {\n>   key: \"T\"\n>   value {\n>     type: DT_FLOAT\n>   }\n> }\n> attr {\n>   key: \"_class\"\n>   value {\n>     list {\n>       s: ***@***.***/rnn/ntm_cell/memory\"\n>     }\n>   }\n> }\n> attr {\n>   key: \"use_locking\"\n>   value {\n>     b: true\n>   }\n> }\n> attr {\n>   key: \"validate_shape\"\n>   value {\n>     b: true\n>   }\n> }\n>\n> Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n>\n> As far as I can tell these should be exactly the same - but I\n> consistently get different performance with the two approaches.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <#21222 (comment)>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb>\n> .\n>\n\n\n\n-- \n - Alex", "body": "Weird, both are creating ref variables and seem to have the same\ninitializers. Could they be ending up in different collections making one\naccidentally trainable while the other isn't?\n\nOn Wed, Nov 7, 2018 at 5:07 PM Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> +francois, +alex passos\n>\n> Here's a case where using vs.get_variable() vs using self.add_variable()\n> leads to different model convergence quality.  the call is happening inside\n> the NTM RNN layer's zero_state() function.  Any thoughts on what could be\n> the issue?\n>\n> On Wed, Nov 7, 2018 at 4:36 PM, Mark Collier <notifications@github.com>\n> wrote:\n>\n>> I am specifying the initializer (see below).\n>>\n>> def build(self, inputs_shape):\n>>     ...\n>>     self._M = self.add_variable(\n>>         'memory',\n>>         shape=[self.memory_size, self.memory_vector_dim],\n>>         initializer=init_ops.constant_initializer(1e-6, dtype=self.dtype),\n>>         trainable=True)\n>>     print('Memory Variable (add_variable)', self._M)\n>>     print('Memory initializer', self._M.initializer)\n>>     print('Memory initial value', self._M.initial_value)\n>>\n>> Output:\n>>\n>> Memory Variable (add_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n>> Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n>> op: \"Assign\"\n>> input: \"root/rnn/ntm_cell/memory\"\n>> input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n>> attr {\n>>   key: \"T\"\n>>   value {\n>>     type: DT_FLOAT\n>>   }\n>> }\n>> attr {\n>>   key: \"_class\"\n>>   value {\n>>     list {\n>>       s: \"loc:@root/rnn/ntm_cell/memory\"\n>>     }\n>>   }\n>> }\n>> attr {\n>>   key: \"use_locking\"\n>>   value {\n>>     b: true\n>>   }\n>> }\n>> attr {\n>>   key: \"validate_shape\"\n>>   value {\n>>     b: true\n>>   }\n>> }\n>>\n>> Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n>>\n>> def build(self, inputs_shape):\n>>     ...\n>>     self._M = vs.get_variable('memory',\n>>       [self.memory_size, self.memory_vector_dim],\n>>       initializer=init_ops.constant_initializer(1e-6))\n>>     print('Memory Variable (get_variable)', self._M)\n>>     print('Memory initializer', self._M.initializer)\n>>     print('Memory initial value', self._M.initial_value)\n>>\n>> Output:\n>>\n>> Memory Variable (get_variable) <tf.Variable 'root/rnn/ntm_cell/memory:0' shape=(128, 20) dtype=float32_ref>\n>> Memory initializer name: \"root/rnn/ntm_cell/memory/Assign\"\n>> op: \"Assign\"\n>> input: \"root/rnn/ntm_cell/memory\"\n>> input: \"root/rnn/ntm_cell/memory/Initializer/Const\"\n>> attr {\n>>   key: \"T\"\n>>   value {\n>>     type: DT_FLOAT\n>>   }\n>> }\n>> attr {\n>>   key: \"_class\"\n>>   value {\n>>     list {\n>>       s: \"loc:@root/rnn/ntm_cell/memory\"\n>>     }\n>>   }\n>> }\n>> attr {\n>>   key: \"use_locking\"\n>>   value {\n>>     b: true\n>>   }\n>> }\n>> attr {\n>>   key: \"validate_shape\"\n>>   value {\n>>     b: true\n>>   }\n>> }\n>>\n>> Memory initial value Tensor(\"root/rnn/ntm_cell/memory/Initializer/Const:0\", shape=(128, 20), dtype=float32)\n>>\n>> As far as I can tell these should be exactly the same - but I\n>> consistently get different performance with the two approaches.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/21222#issuecomment-436829978>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim_k3X1wyPIS6_jxUHPnfSMsiretoks5us3yggaJpZM4Vlceb>\n>> .\n>>\n>\n>\n\n-- \n - Alex\n"}