{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361818116", "html_url": "https://github.com/tensorflow/tensorflow/issues/16279#issuecomment-361818116", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16279", "id": 361818116, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTgxODExNg==", "user": {"login": "Aashit-Sharma", "id": 29089622, "node_id": "MDQ6VXNlcjI5MDg5NjIy", "avatar_url": "https://avatars0.githubusercontent.com/u/29089622?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aashit-Sharma", "html_url": "https://github.com/Aashit-Sharma", "followers_url": "https://api.github.com/users/Aashit-Sharma/followers", "following_url": "https://api.github.com/users/Aashit-Sharma/following{/other_user}", "gists_url": "https://api.github.com/users/Aashit-Sharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aashit-Sharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aashit-Sharma/subscriptions", "organizations_url": "https://api.github.com/users/Aashit-Sharma/orgs", "repos_url": "https://api.github.com/users/Aashit-Sharma/repos", "events_url": "https://api.github.com/users/Aashit-Sharma/events{/privacy}", "received_events_url": "https://api.github.com/users/Aashit-Sharma/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T04:11:11Z", "updated_at": "2018-01-31T04:12:04Z", "author_association": "NONE", "body_html": "<p>If you refer to my 1st and 2nd reply , where i mention of the main decoder function and in the following comment , of its sub-functions and placeholders (and graph variables) that is all the required stuff for the main function BUT the graph function does refer to other functions in the code to which it is related.</p>\n<pre><code>`def nl_decoder(self):\n\t\t\n\t\twith tf.variable_scope(\"NL_Decoder\"):\n\t\t\tself.decoder_nl_cell = tf.contrib.rnn.GRUCell(self.nl_dec_size)\n\t\t\t\n\t\t\tself.iter_decoder_nl = tf.constant(0)\n\t\t\tself.iter_decoder_nl_test = tf.constant(0)\n\t\t\t\n\t\t\ttrain_decoder = tf.while_loop(self.nl_decoder_condition,self.nl_decoder_function, [ self.iter_decoder_nl, tf.zeros([1,self.nl_dec_size]) , tf.ones([self.nl_dec_size])],\\\n\t\t\tshape_invariants = [self.iter_decoder_nl.get_shape(), tf.TensorShape([None,self.nl_dec_size]),\\\n\t\t\ttf.TensorShape([self.nl_dec_size])])\n\n\t\treturn train_decoder[1][1:]\n        \n def nl_decoder_condition(self,it,outputs,hidden):\n\t\t\treturn it[0] &lt; self.oplen_nl[0]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t# This is the step where we concatenate hidden state with context hidden state \t\t\t\n\ndef nl_decoder_function(self,it,outputs,hidden):\n\t\t\tout,hidden = self.decoder_nl_cell(tf.stack([tf.concat(0,[outputs[-1,:],self.nl_context_concat])]) , tf.stack([hidden]))\n\t\t\t\n\t\t\toutputs = tf.concat(0,[outputs,out])\n\t\t\treturn it+1,outputs,hidden[0,:]\n\n            \nself.oplen_nl = tf.placeholder(tf.int32,[1],name=\"nl_oplen\")\n\n#this comes in the graph function\nself.nl_context_concat = tf.concat(0,[self.current_hidden_nl_context,self.encoded_prediction])`\n\n</code></pre>\n<p>This can't be avoided as this is an implementation of HRED , which essentially involves an encoder , a context and a decoder RNN</p>\n<p>The feed into the decoder RNN comes from context , to which comes from encoder .</p>\n<p>I think if you take a look at the command prompt log , you can do with just glancing through the code to specific areas.</p>\n<p>essentially these lines , where the slicehelper is referenced , which references the stridedslice instead of general slice method (which requires 3 params)</p>\n<pre><code>File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2576, in _BuildLoop\n  c = ops.convert_to_tensor(pred(*packed_vars))\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\n  name=name)\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\n  name=name)\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\n  shrink_axis_mask=shrink_axis_mask)\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\n  name=name) \n</code></pre>\n<p>essentially it automatically references to strided slice instead of just tf.slice.  Just have a look at shape_invariants and the code around it in the decoder function</p>\n<p>Feel free to ask me any questions or correct me if I am wrong</p>", "body_text": "If you refer to my 1st and 2nd reply , where i mention of the main decoder function and in the following comment , of its sub-functions and placeholders (and graph variables) that is all the required stuff for the main function BUT the graph function does refer to other functions in the code to which it is related.\n`def nl_decoder(self):\n\t\t\n\t\twith tf.variable_scope(\"NL_Decoder\"):\n\t\t\tself.decoder_nl_cell = tf.contrib.rnn.GRUCell(self.nl_dec_size)\n\t\t\t\n\t\t\tself.iter_decoder_nl = tf.constant(0)\n\t\t\tself.iter_decoder_nl_test = tf.constant(0)\n\t\t\t\n\t\t\ttrain_decoder = tf.while_loop(self.nl_decoder_condition,self.nl_decoder_function, [ self.iter_decoder_nl, tf.zeros([1,self.nl_dec_size]) , tf.ones([self.nl_dec_size])],\\\n\t\t\tshape_invariants = [self.iter_decoder_nl.get_shape(), tf.TensorShape([None,self.nl_dec_size]),\\\n\t\t\ttf.TensorShape([self.nl_dec_size])])\n\n\t\treturn train_decoder[1][1:]\n        \n def nl_decoder_condition(self,it,outputs,hidden):\n\t\t\treturn it[0] < self.oplen_nl[0]\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t# This is the step where we concatenate hidden state with context hidden state \t\t\t\n\ndef nl_decoder_function(self,it,outputs,hidden):\n\t\t\tout,hidden = self.decoder_nl_cell(tf.stack([tf.concat(0,[outputs[-1,:],self.nl_context_concat])]) , tf.stack([hidden]))\n\t\t\t\n\t\t\toutputs = tf.concat(0,[outputs,out])\n\t\t\treturn it+1,outputs,hidden[0,:]\n\n            \nself.oplen_nl = tf.placeholder(tf.int32,[1],name=\"nl_oplen\")\n\n#this comes in the graph function\nself.nl_context_concat = tf.concat(0,[self.current_hidden_nl_context,self.encoded_prediction])`\n\n\nThis can't be avoided as this is an implementation of HRED , which essentially involves an encoder , a context and a decoder RNN\nThe feed into the decoder RNN comes from context , to which comes from encoder .\nI think if you take a look at the command prompt log , you can do with just glancing through the code to specific areas.\nessentially these lines , where the slicehelper is referenced , which references the stridedslice instead of general slice method (which requires 3 params)\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2576, in _BuildLoop\n  c = ops.convert_to_tensor(pred(*packed_vars))\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\n  name=name)\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\n  name=name)\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\n  shrink_axis_mask=shrink_axis_mask)\n\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\n  name=name) \n\nessentially it automatically references to strided slice instead of just tf.slice.  Just have a look at shape_invariants and the code around it in the decoder function\nFeel free to ask me any questions or correct me if I am wrong", "body": "If you refer to my 1st and 2nd reply , where i mention of the main decoder function and in the following comment , of its sub-functions and placeholders (and graph variables) that is all the required stuff for the main function BUT the graph function does refer to other functions in the code to which it is related.\r\n```\r\n`def nl_decoder(self):\r\n\t\t\r\n\t\twith tf.variable_scope(\"NL_Decoder\"):\r\n\t\t\tself.decoder_nl_cell = tf.contrib.rnn.GRUCell(self.nl_dec_size)\r\n\t\t\t\r\n\t\t\tself.iter_decoder_nl = tf.constant(0)\r\n\t\t\tself.iter_decoder_nl_test = tf.constant(0)\r\n\t\t\t\r\n\t\t\ttrain_decoder = tf.while_loop(self.nl_decoder_condition,self.nl_decoder_function, [ self.iter_decoder_nl, tf.zeros([1,self.nl_dec_size]) , tf.ones([self.nl_dec_size])],\\\r\n\t\t\tshape_invariants = [self.iter_decoder_nl.get_shape(), tf.TensorShape([None,self.nl_dec_size]),\\\r\n\t\t\ttf.TensorShape([self.nl_dec_size])])\r\n\r\n\t\treturn train_decoder[1][1:]\r\n        \r\n def nl_decoder_condition(self,it,outputs,hidden):\r\n\t\t\treturn it[0] < self.oplen_nl[0]\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n\t\t# This is the step where we concatenate hidden state with context hidden state \t\t\t\r\n\r\ndef nl_decoder_function(self,it,outputs,hidden):\r\n\t\t\tout,hidden = self.decoder_nl_cell(tf.stack([tf.concat(0,[outputs[-1,:],self.nl_context_concat])]) , tf.stack([hidden]))\r\n\t\t\t\r\n\t\t\toutputs = tf.concat(0,[outputs,out])\r\n\t\t\treturn it+1,outputs,hidden[0,:]\r\n\r\n            \r\nself.oplen_nl = tf.placeholder(tf.int32,[1],name=\"nl_oplen\")\r\n\r\n#this comes in the graph function\r\nself.nl_context_concat = tf.concat(0,[self.current_hidden_nl_context,self.encoded_prediction])`\r\n\r\n```\r\n\r\nThis can't be avoided as this is an implementation of HRED , which essentially involves an encoder , a context and a decoder RNN\r\n\r\nThe feed into the decoder RNN comes from context , to which comes from encoder .\r\n\r\n\r\nI think if you take a look at the command prompt log , you can do with just glancing through the code to specific areas. \r\n\r\nessentially these lines , where the slicehelper is referenced , which references the stridedslice instead of general slice method (which requires 3 params)\r\n\r\n  ```\r\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2576, in _BuildLoop\r\n    c = ops.convert_to_tensor(pred(*packed_vars))\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\r\n    name=name) \r\n```\r\n\r\nessentially it automatically references to strided slice instead of just tf.slice.  Just have a look at shape_invariants and the code around it in the decoder function\r\n\r\nFeel free to ask me any questions or correct me if I am wrong"}