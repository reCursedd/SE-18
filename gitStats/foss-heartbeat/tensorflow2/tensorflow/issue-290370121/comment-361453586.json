{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361453586", "html_url": "https://github.com/tensorflow/tensorflow/issues/16279#issuecomment-361453586", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16279", "id": 361453586, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQ1MzU4Ng==", "user": {"login": "Aashit-Sharma", "id": 29089622, "node_id": "MDQ6VXNlcjI5MDg5NjIy", "avatar_url": "https://avatars0.githubusercontent.com/u/29089622?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aashit-Sharma", "html_url": "https://github.com/Aashit-Sharma", "followers_url": "https://api.github.com/users/Aashit-Sharma/followers", "following_url": "https://api.github.com/users/Aashit-Sharma/following{/other_user}", "gists_url": "https://api.github.com/users/Aashit-Sharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aashit-Sharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aashit-Sharma/subscriptions", "organizations_url": "https://api.github.com/users/Aashit-Sharma/orgs", "repos_url": "https://api.github.com/users/Aashit-Sharma/repos", "events_url": "https://api.github.com/users/Aashit-Sharma/events{/privacy}", "received_events_url": "https://api.github.com/users/Aashit-Sharma/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T02:11:12Z", "updated_at": "2018-01-30T02:15:24Z", "author_association": "NONE", "body_html": "<p>Train.py -</p>\n<pre><code>`import random\nimport pickle\nfrom mrrnn import MRRNN\nfrom mrrnn import Configuration\n\n\n\n# tokenid's for end of utterance\nend_nl = 12575\nend_c = 15\nif __name__ == \"__main__\":\n\n#Importing training , testing , validation and dictionary data\n\n\tnltrain_path = \"./data/Training.dialogues.pkl\"\n\twith open(nltrain_path,\"r\") as file:\n\t\tnltrain = pickle.load(file)\n\t\t\n\tctrain_path = \"./data/abstract.train.dialogues.pkl\"\n\twith open(ctrain_path,\"r\") as file:\n\t\tctrain = pickle.load(file)\n\n\t\t\n\tnltest_path = \"./data/Test.dialogues.pkl\"\n\twith open(nltest_path,\"r\") as file:\n\t\tnltest = pickle.load(file)\n\t\t\n\tctest_path = \"./data/abstract.test.dialogues.pkl\"\n\twith open(ctest_path,\"r\") as file:\n\t\tctest = pickle.load(file)\n\n\t\t\n\tnlvalid_path = \"./data/Validation.dialogues.pkl\"\n\twith open(nlvalid_path,\"r\") as file:\n\t\tnlvalid = pickle.load(file)\n\t\t\n\tcvalid_path = \"./data/abstract.valid.dialogues.pkl\"\n\twith open(cvalid_path,\"r\") as file:\n\t\tcvalid = pickle.load(file)\n\n\t\t\n\tnldict_path = \"./data/Dataset.dict.pkl\"\n\twith open(nldict_path,\"r\") as file:\n\t\tnlvocab = pickle.load(file)\n\t\tnlvocab = sorted( nlvocab,key=lambda tup: tup[1] )\n\t\n\tcdict_path = \"./data/abstract.dict.pkl\"\n\twith open(cdict_path,\"r\") as file:\n\t\tcvocab = pickle.load(file)\n\t\tcvocab = sorted(cvocab,key=lambda tup: tup[1] )\n\t\t\n\t\t\n\tconfig = Configuration()\n\tconfig.learning_rate = 0.0003\n\tconfig.nlvocab = len(nlvocab) #to be changed in mrrnn\n\tconfig.cvocab = len(cvocab)\n\tconfig.end_of_word_utt = end_nl \n\tconfig.end_of_coarse_utt = end_c\n\t\n\n#the model\n\tmodel = MRRNN(config)\n\n\tbatchSize = 80\t\n\tprint_every = 1\n\tn_train = len(nltrain)\n\tn_epochs = 5\n\tmaxtrain_id = n_train - batchSize\n\n\tsaveDir = \"./ckpts/training_5/\"\n\tfileRestore = \"./ckpts/training_5/trained.ckpt\"\n\tfileName = \"./ckpts/training_5/trained.ckpt\"\n\n\tif fileRestore:\n\t\tmodel.restore(fileRestore)\n\n\tloss = model.cost(nlvalid[:100],cvalid[:100])\n\tprint (loss)\n\n\tfor ep in xrange(n_epochs):\n\t\tstart_id = 0\n\t\titer = 0\n\t\twhile start_id &lt; maxtrain_id:\n\t\t\tend_id = start_id + batchSize\n\t\t\tmodel.partial_fit(nltrain[start_id:end_id],ctrain[start_id:end_id])\n\t\t\t\n\t\t\tif not (iter % print_every):\n\t\t\t\tloss = model.cost(nlvalid[:10],cvalid[:10])\n\t\t\t\tmodel.save(fileName)\n\t\t\t\tprint (\"{0} of {1}: {2}\".format(start_id,n_train,loss))\n\t\t\t\n\t\t\tstart_id += batchSize\n\t\t\titer += 1\n`\n</code></pre>", "body_text": "Train.py -\n`import random\nimport pickle\nfrom mrrnn import MRRNN\nfrom mrrnn import Configuration\n\n\n\n# tokenid's for end of utterance\nend_nl = 12575\nend_c = 15\nif __name__ == \"__main__\":\n\n#Importing training , testing , validation and dictionary data\n\n\tnltrain_path = \"./data/Training.dialogues.pkl\"\n\twith open(nltrain_path,\"r\") as file:\n\t\tnltrain = pickle.load(file)\n\t\t\n\tctrain_path = \"./data/abstract.train.dialogues.pkl\"\n\twith open(ctrain_path,\"r\") as file:\n\t\tctrain = pickle.load(file)\n\n\t\t\n\tnltest_path = \"./data/Test.dialogues.pkl\"\n\twith open(nltest_path,\"r\") as file:\n\t\tnltest = pickle.load(file)\n\t\t\n\tctest_path = \"./data/abstract.test.dialogues.pkl\"\n\twith open(ctest_path,\"r\") as file:\n\t\tctest = pickle.load(file)\n\n\t\t\n\tnlvalid_path = \"./data/Validation.dialogues.pkl\"\n\twith open(nlvalid_path,\"r\") as file:\n\t\tnlvalid = pickle.load(file)\n\t\t\n\tcvalid_path = \"./data/abstract.valid.dialogues.pkl\"\n\twith open(cvalid_path,\"r\") as file:\n\t\tcvalid = pickle.load(file)\n\n\t\t\n\tnldict_path = \"./data/Dataset.dict.pkl\"\n\twith open(nldict_path,\"r\") as file:\n\t\tnlvocab = pickle.load(file)\n\t\tnlvocab = sorted( nlvocab,key=lambda tup: tup[1] )\n\t\n\tcdict_path = \"./data/abstract.dict.pkl\"\n\twith open(cdict_path,\"r\") as file:\n\t\tcvocab = pickle.load(file)\n\t\tcvocab = sorted(cvocab,key=lambda tup: tup[1] )\n\t\t\n\t\t\n\tconfig = Configuration()\n\tconfig.learning_rate = 0.0003\n\tconfig.nlvocab = len(nlvocab) #to be changed in mrrnn\n\tconfig.cvocab = len(cvocab)\n\tconfig.end_of_word_utt = end_nl \n\tconfig.end_of_coarse_utt = end_c\n\t\n\n#the model\n\tmodel = MRRNN(config)\n\n\tbatchSize = 80\t\n\tprint_every = 1\n\tn_train = len(nltrain)\n\tn_epochs = 5\n\tmaxtrain_id = n_train - batchSize\n\n\tsaveDir = \"./ckpts/training_5/\"\n\tfileRestore = \"./ckpts/training_5/trained.ckpt\"\n\tfileName = \"./ckpts/training_5/trained.ckpt\"\n\n\tif fileRestore:\n\t\tmodel.restore(fileRestore)\n\n\tloss = model.cost(nlvalid[:100],cvalid[:100])\n\tprint (loss)\n\n\tfor ep in xrange(n_epochs):\n\t\tstart_id = 0\n\t\titer = 0\n\t\twhile start_id < maxtrain_id:\n\t\t\tend_id = start_id + batchSize\n\t\t\tmodel.partial_fit(nltrain[start_id:end_id],ctrain[start_id:end_id])\n\t\t\t\n\t\t\tif not (iter % print_every):\n\t\t\t\tloss = model.cost(nlvalid[:10],cvalid[:10])\n\t\t\t\tmodel.save(fileName)\n\t\t\t\tprint (\"{0} of {1}: {2}\".format(start_id,n_train,loss))\n\t\t\t\n\t\t\tstart_id += batchSize\n\t\t\titer += 1\n`", "body": "Train.py -\r\n\r\n```\r\n`import random\r\nimport pickle\r\nfrom mrrnn import MRRNN\r\nfrom mrrnn import Configuration\r\n\r\n\r\n\r\n# tokenid's for end of utterance\r\nend_nl = 12575\r\nend_c = 15\r\nif __name__ == \"__main__\":\r\n\r\n#Importing training , testing , validation and dictionary data\r\n\r\n\tnltrain_path = \"./data/Training.dialogues.pkl\"\r\n\twith open(nltrain_path,\"r\") as file:\r\n\t\tnltrain = pickle.load(file)\r\n\t\t\r\n\tctrain_path = \"./data/abstract.train.dialogues.pkl\"\r\n\twith open(ctrain_path,\"r\") as file:\r\n\t\tctrain = pickle.load(file)\r\n\r\n\t\t\r\n\tnltest_path = \"./data/Test.dialogues.pkl\"\r\n\twith open(nltest_path,\"r\") as file:\r\n\t\tnltest = pickle.load(file)\r\n\t\t\r\n\tctest_path = \"./data/abstract.test.dialogues.pkl\"\r\n\twith open(ctest_path,\"r\") as file:\r\n\t\tctest = pickle.load(file)\r\n\r\n\t\t\r\n\tnlvalid_path = \"./data/Validation.dialogues.pkl\"\r\n\twith open(nlvalid_path,\"r\") as file:\r\n\t\tnlvalid = pickle.load(file)\r\n\t\t\r\n\tcvalid_path = \"./data/abstract.valid.dialogues.pkl\"\r\n\twith open(cvalid_path,\"r\") as file:\r\n\t\tcvalid = pickle.load(file)\r\n\r\n\t\t\r\n\tnldict_path = \"./data/Dataset.dict.pkl\"\r\n\twith open(nldict_path,\"r\") as file:\r\n\t\tnlvocab = pickle.load(file)\r\n\t\tnlvocab = sorted( nlvocab,key=lambda tup: tup[1] )\r\n\t\r\n\tcdict_path = \"./data/abstract.dict.pkl\"\r\n\twith open(cdict_path,\"r\") as file:\r\n\t\tcvocab = pickle.load(file)\r\n\t\tcvocab = sorted(cvocab,key=lambda tup: tup[1] )\r\n\t\t\r\n\t\t\r\n\tconfig = Configuration()\r\n\tconfig.learning_rate = 0.0003\r\n\tconfig.nlvocab = len(nlvocab) #to be changed in mrrnn\r\n\tconfig.cvocab = len(cvocab)\r\n\tconfig.end_of_word_utt = end_nl \r\n\tconfig.end_of_coarse_utt = end_c\r\n\t\r\n\r\n#the model\r\n\tmodel = MRRNN(config)\r\n\r\n\tbatchSize = 80\t\r\n\tprint_every = 1\r\n\tn_train = len(nltrain)\r\n\tn_epochs = 5\r\n\tmaxtrain_id = n_train - batchSize\r\n\r\n\tsaveDir = \"./ckpts/training_5/\"\r\n\tfileRestore = \"./ckpts/training_5/trained.ckpt\"\r\n\tfileName = \"./ckpts/training_5/trained.ckpt\"\r\n\r\n\tif fileRestore:\r\n\t\tmodel.restore(fileRestore)\r\n\r\n\tloss = model.cost(nlvalid[:100],cvalid[:100])\r\n\tprint (loss)\r\n\r\n\tfor ep in xrange(n_epochs):\r\n\t\tstart_id = 0\r\n\t\titer = 0\r\n\t\twhile start_id < maxtrain_id:\r\n\t\t\tend_id = start_id + batchSize\r\n\t\t\tmodel.partial_fit(nltrain[start_id:end_id],ctrain[start_id:end_id])\r\n\t\t\t\r\n\t\t\tif not (iter % print_every):\r\n\t\t\t\tloss = model.cost(nlvalid[:10],cvalid[:10])\r\n\t\t\t\tmodel.save(fileName)\r\n\t\t\t\tprint (\"{0} of {1}: {2}\".format(start_id,n_train,loss))\r\n\t\t\t\r\n\t\t\tstart_id += batchSize\r\n\t\t\titer += 1\r\n`\r\n```"}