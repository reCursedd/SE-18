{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8624", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8624/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8624/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8624/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8624", "id": 216059948, "node_id": "MDU6SXNzdWUyMTYwNTk5NDg=", "number": 8624, "title": "Constant folding does not work across devices?", "user": {"login": "gaohuazuo", "id": 10446514, "node_id": "MDQ6VXNlcjEwNDQ2NTE0", "avatar_url": "https://avatars0.githubusercontent.com/u/10446514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaohuazuo", "html_url": "https://github.com/gaohuazuo", "followers_url": "https://api.github.com/users/gaohuazuo/followers", "following_url": "https://api.github.com/users/gaohuazuo/following{/other_user}", "gists_url": "https://api.github.com/users/gaohuazuo/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaohuazuo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaohuazuo/subscriptions", "organizations_url": "https://api.github.com/users/gaohuazuo/orgs", "repos_url": "https://api.github.com/users/gaohuazuo/repos", "events_url": "https://api.github.com/users/gaohuazuo/events{/privacy}", "received_events_url": "https://api.github.com/users/gaohuazuo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-22T13:22:29Z", "updated_at": "2017-06-16T21:14:14Z", "closed_at": "2017-06-16T21:14:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've been trying to understand tensorflow internals recently. I found in <code>tensorflow/core/common_runtime/direct_session.cc</code>, if I understand it correctly, that constant folding only take place at <a href=\"https://github.com/tensorflow/tensorflow/blob/ef56133461079f28b61b5a83a62685051408aadb/tensorflow/core/common_runtime/direct_session.cc#L1051\">#L1051</a> after graph partitioning, so constants won't propagate through device boundary.</p>\n<p>This is also evidenced by a simple experiment:</p>\n<pre><code>with tf.device('gpu'):\n    a = tf.constant(0)\nwith tf.device('cpu'):\n    b = a + 1\n    c = b + 1\n</code></pre>\n<p>Resulting computation time graph is<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/10446514/24199102/62c291ae-0f43-11e7-8689-a595ded2c7ed.png\"><img src=\"https://cloud.githubusercontent.com/assets/10446514/24199102/62c291ae-0f43-11e7-8689-a595ded2c7ed.png\" alt=\"graph-run\" style=\"max-width:100%;\"></a><br>\nWhereas placing all ops on GPU gives a fully shaded graph.</p>\n<p>Did I miss something? Or is there any consideration not to run constant folding before partitioning the graph?</p>", "body_text": "I've been trying to understand tensorflow internals recently. I found in tensorflow/core/common_runtime/direct_session.cc, if I understand it correctly, that constant folding only take place at #L1051 after graph partitioning, so constants won't propagate through device boundary.\nThis is also evidenced by a simple experiment:\nwith tf.device('gpu'):\n    a = tf.constant(0)\nwith tf.device('cpu'):\n    b = a + 1\n    c = b + 1\n\nResulting computation time graph is\n\nWhereas placing all ops on GPU gives a fully shaded graph.\nDid I miss something? Or is there any consideration not to run constant folding before partitioning the graph?", "body": "I've been trying to understand tensorflow internals recently. I found in `tensorflow/core/common_runtime/direct_session.cc`, if I understand it correctly, that constant folding only take place at [#L1051](https://github.com/tensorflow/tensorflow/blob/ef56133461079f28b61b5a83a62685051408aadb/tensorflow/core/common_runtime/direct_session.cc#L1051) after graph partitioning, so constants won't propagate through device boundary.\r\n\r\nThis is also evidenced by a simple experiment:\r\n```\r\nwith tf.device('gpu'):\r\n    a = tf.constant(0)\r\nwith tf.device('cpu'):\r\n    b = a + 1\r\n    c = b + 1\r\n```\r\nResulting computation time graph is\r\n![graph-run](https://cloud.githubusercontent.com/assets/10446514/24199102/62c291ae-0f43-11e7-8689-a595ded2c7ed.png)\r\nWhereas placing all ops on GPU gives a fully shaded graph.\r\n\r\nDid I miss something? Or is there any consideration not to run constant folding before partitioning the graph?"}