{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299940463", "html_url": "https://github.com/tensorflow/tensorflow/issues/9679#issuecomment-299940463", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9679", "id": 299940463, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTk0MDQ2Mw==", "user": {"login": "IMG-PRCSNG", "id": 3027352, "node_id": "MDQ6VXNlcjMwMjczNTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3027352?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IMG-PRCSNG", "html_url": "https://github.com/IMG-PRCSNG", "followers_url": "https://api.github.com/users/IMG-PRCSNG/followers", "following_url": "https://api.github.com/users/IMG-PRCSNG/following{/other_user}", "gists_url": "https://api.github.com/users/IMG-PRCSNG/gists{/gist_id}", "starred_url": "https://api.github.com/users/IMG-PRCSNG/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IMG-PRCSNG/subscriptions", "organizations_url": "https://api.github.com/users/IMG-PRCSNG/orgs", "repos_url": "https://api.github.com/users/IMG-PRCSNG/repos", "events_url": "https://api.github.com/users/IMG-PRCSNG/events{/privacy}", "received_events_url": "https://api.github.com/users/IMG-PRCSNG/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T17:53:54Z", "updated_at": "2017-05-08T17:53:54Z", "author_association": "NONE", "body_html": "<p>I could share it but I feel it would add more confusion as I hard-coded certain paths in <code>third_party/gpus/cuda_configure.bzl</code> for pointing to paths of the executables  <code>nvcc</code> and <code>nvvm</code>. On giving the toolkit path, the build process chooses the <code>nvcc</code> and <code>nvvm</code> from inside it. Since I am cross compiling, I would need the host executables <code>nvcc</code> and <code>nvvm</code>  but the target libraries. I will put things together tomorrow and share them.</p>\n<p>I have also tried to dig in and see why host cuda runtime is being required. I bazel queried the graph and saw that it is trying to generate <code>xyz_ops_gen_cc</code>, a binary which is being generated and is linking against cuda runtime of the host because of the dependency<br>\n<code>xyz_ops_genrule</code> -&gt; <code>xyz_ops_gen_cc</code> -&gt; <code>xyz_ops_op_lib</code> -&gt; <code>core:framework</code>. I found this in  <code>tensorflow/tensorflow.bzl</code> and used in <code>tensorflow/cc/BUILD</code></p>\n<p>I also came across this <a href=\"https://github.com/bazelbuild/bazel/issues/464#issuecomment-141708071\" data-hovercard-type=\"issue\" data-hovercard-url=\"/bazelbuild/bazel/issues/464/hovercard\">link</a> on bazelbuild as to how it switches to host toolchain with <code>genrule</code>. Thus, I globally exported the library path for host cuda libraries in the toolchain section for \"k8\" cpu  in the CROSSTOOL.tpl file (in third_party/gpus/crosstool) using <code>linker_flag</code> options and passed this toolchain as <code>host_crosstool_top</code> instead of the default bazel cpp toolchain and it compiled. From the compilation of code point of view, I think it is safe to close this issue.</p>\n<p>But if supporting cross-compilation is in your plan, then we need two CUDA toolkit paths - one for host and one for target since the host_crosstool looks for host cuda runtime while generating the wrappers and we need target libraries for linking with target_crosstool.</p>\n<p>PS; I also dont understand why the <code>xxx_op_gen_cc</code> needs to be linked against cuda runtime though. If anybody can explain, it would be nice.</p>", "body_text": "I could share it but I feel it would add more confusion as I hard-coded certain paths in third_party/gpus/cuda_configure.bzl for pointing to paths of the executables  nvcc and nvvm. On giving the toolkit path, the build process chooses the nvcc and nvvm from inside it. Since I am cross compiling, I would need the host executables nvcc and nvvm  but the target libraries. I will put things together tomorrow and share them.\nI have also tried to dig in and see why host cuda runtime is being required. I bazel queried the graph and saw that it is trying to generate xyz_ops_gen_cc, a binary which is being generated and is linking against cuda runtime of the host because of the dependency\nxyz_ops_genrule -> xyz_ops_gen_cc -> xyz_ops_op_lib -> core:framework. I found this in  tensorflow/tensorflow.bzl and used in tensorflow/cc/BUILD\nI also came across this link on bazelbuild as to how it switches to host toolchain with genrule. Thus, I globally exported the library path for host cuda libraries in the toolchain section for \"k8\" cpu  in the CROSSTOOL.tpl file (in third_party/gpus/crosstool) using linker_flag options and passed this toolchain as host_crosstool_top instead of the default bazel cpp toolchain and it compiled. From the compilation of code point of view, I think it is safe to close this issue.\nBut if supporting cross-compilation is in your plan, then we need two CUDA toolkit paths - one for host and one for target since the host_crosstool looks for host cuda runtime while generating the wrappers and we need target libraries for linking with target_crosstool.\nPS; I also dont understand why the xxx_op_gen_cc needs to be linked against cuda runtime though. If anybody can explain, it would be nice.", "body": "I could share it but I feel it would add more confusion as I hard-coded certain paths in `third_party/gpus/cuda_configure.bzl` for pointing to paths of the executables  `nvcc` and `nvvm`. On giving the toolkit path, the build process chooses the `nvcc` and `nvvm` from inside it. Since I am cross compiling, I would need the host executables `nvcc` and `nvvm`  but the target libraries. I will put things together tomorrow and share them.\r\n\r\nI have also tried to dig in and see why host cuda runtime is being required. I bazel queried the graph and saw that it is trying to generate `xyz_ops_gen_cc`, a binary which is being generated and is linking against cuda runtime of the host because of the dependency\r\n`xyz_ops_genrule` -> `xyz_ops_gen_cc` -> `xyz_ops_op_lib` -> `core:framework`. I found this in  `tensorflow/tensorflow.bzl` and used in `tensorflow/cc/BUILD` \r\n\r\nI also came across this [link](https://github.com/bazelbuild/bazel/issues/464#issuecomment-141708071) on bazelbuild as to how it switches to host toolchain with `genrule`. Thus, I globally exported the library path for host cuda libraries in the toolchain section for \"k8\" cpu  in the CROSSTOOL.tpl file (in third_party/gpus/crosstool) using `linker_flag` options and passed this toolchain as `host_crosstool_top` instead of the default bazel cpp toolchain and it compiled. From the compilation of code point of view, I think it is safe to close this issue.\r\n\r\nBut if supporting cross-compilation is in your plan, then we need two CUDA toolkit paths - one for host and one for target since the host_crosstool looks for host cuda runtime while generating the wrappers and we need target libraries for linking with target_crosstool. \r\n\r\nPS; I also dont understand why the `xxx_op_gen_cc` needs to be linked against cuda runtime though. If anybody can explain, it would be nice."}