{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22412", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22412/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22412/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22412/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22412", "id": 362176250, "node_id": "MDU6SXNzdWUzNjIxNzYyNTA=", "number": 22412, "title": "Tensor2Tensor Intro tensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested", "user": {"login": "davypepe", "id": 24248699, "node_id": "MDQ6VXNlcjI0MjQ4Njk5", "avatar_url": "https://avatars2.githubusercontent.com/u/24248699?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davypepe", "html_url": "https://github.com/davypepe", "followers_url": "https://api.github.com/users/davypepe/followers", "following_url": "https://api.github.com/users/davypepe/following{/other_user}", "gists_url": "https://api.github.com/users/davypepe/gists{/gist_id}", "starred_url": "https://api.github.com/users/davypepe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davypepe/subscriptions", "organizations_url": "https://api.github.com/users/davypepe/orgs", "repos_url": "https://api.github.com/users/davypepe/repos", "events_url": "https://api.github.com/users/davypepe/events{/privacy}", "received_events_url": "https://api.github.com/users/davypepe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-20T12:59:36Z", "updated_at": "2018-09-26T12:35:39Z", "closed_at": "2018-09-26T12:35:39Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nno</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nwindows 10, x64</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\npip (pip install --upgrade tensorflow)</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.10.0</li>\n<li><strong>Python version</strong>:<br>\n3.6</li>\n<li><strong>GPU model and memory</strong>:<br>\nnot using GPU, 12GB memory<br>\nMobile device: N/A<br>\nBazel version : N/A<br>\nCUDA/cuDNN version : N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm trying to make the Tensor2Tensor intro: English to German translation with a pre-trained model work locally. I have downloaded the checkpoints to the right directory, however it's returning this error:<br>\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested</p>\n<p>As you can see the checkpoints are in the proper directory:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/24248699/45819470-ac0e8c00-bce4-11e8-85bf-fccc4e2e7c22.png\"><img src=\"https://user-images.githubusercontent.com/24248699/45819470-ac0e8c00-bce4-11e8-85bf-fccc4e2e7c22.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>I have tried redownloading the checkpoints many times, I can't seem to get the English to German translation working with the provided checkpoints.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a></p>\n<ul>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code># Imports we need.\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport collections\n\nfrom tensor2tensor import models\nfrom tensor2tensor import problems\nfrom tensor2tensor.layers import common_layers\nfrom tensor2tensor.utils import trainer_lib\nfrom tensor2tensor.utils import t2t_model\nfrom tensor2tensor.utils import registry\nfrom tensor2tensor.utils import metrics\n\n# Enable TF Eager execution\ntfe = tf.contrib.eager\ntfe.enable_eager_execution()\n\n# Other setup\nModes = tf.estimator.ModeKeys\n\n# Setup some directories\ndata_dir = os.path.expanduser(\"~/t2t-ende/data\")\ntmp_dir = os.path.expanduser(\"~/t2t-ende/tmp\")\ntrain_dir = os.path.expanduser(\"~/t2t-ende/train\")\ncheckpoint_dir = os.path.expanduser(\"~/t2t-ende/checkpoints\")\ntf.gfile.MakeDirs(data_dir)\ntf.gfile.MakeDirs(tmp_dir)\ntf.gfile.MakeDirs(train_dir)\ntf.gfile.MakeDirs(checkpoint_dir)\ngs_data_dir = \"gs://tensor2tensor-data\"\ngs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\"\n\n# Fetch the problem\nende_problem = problems.problem(\"translate_ende_wmt32k\")\n\n# Copy the vocab file locally so we can encode inputs and decode model outputs\n# All vocabs are stored on GCS\nvocab_name = \"vocab.translate_ende_wmt32k.32768.subwords\"\nvocab_file = os.path.join(gs_data_dir, vocab_name)\n\n# Get the encoders from the problem\nencoders = ende_problem.feature_encoders(data_dir)\n\n# Setup helper functions for encoding and decoding\n\n\ndef encode(input_str, output_str=None):\n    \"\"\"Input str to features dict, ready for inference\"\"\"\n    inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n    batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n    return {\"inputs\": batch_inputs}\n\n\ndef decode(integers):\n    \"\"\"List of ints to str\"\"\"\n    integers = list(np.squeeze(integers))\n    if 1 in integers:\n        integers = integers[:integers.index(1)]\n    return encoders[\"inputs\"].decode(np.squeeze(integers))\n\n# Generate and view the data\n# This cell is commented out because WMT data generation can take hours\n\n\nende_problem.generate_data(data_dir, tmp_dir)\nexample = tfe.Iterator(ende_problem.dataset(Modes.TRAIN, data_dir)).next()\ninputs = [int(x) for x in example[\"inputs\"].numpy()]  # Cast to ints.\ntargets = [int(x) for x in example[\"targets\"].numpy()]  # Cast to ints.\n\n# Example inputs as int-tensor.\nprint(\"Inputs, encoded:\")\nprint(inputs)\nprint(\"Inputs, decoded:\")\n# Example inputs as a sentence.\nprint(decode(inputs))\n# Example targets as int-tensor.\nprint(\"Targets, encoded:\")\nprint(targets)\n# Example targets as a sentence.\nprint(\"Targets, decoded:\")\nprint(decode(targets))\n\n# Create hparams and the model\nmodel_name = \"transformer\"\nhparams_set = \"transformer_base\"\n\nhparams = trainer_lib.create_hparams(\n    hparams_set, data_dir=data_dir, problem_name=\"translate_ende_wmt32k\")\n\n# NOTE: Only create the model once when restoring from a checkpoint; it's a\n# Layer and so subsequent instantiations will have different variable scopes\n# that will not match the checkpoint.\ntranslate_model = registry.model(model_name)(hparams, Modes.EVAL)\n\n# Copy the pretrained checkpoint locally\nckpt_name = \"transformer_ende_test\"\ngs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\nckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))\n\n# Restore and translate!\ndef translate(inputs):\n    encoded_inputs = encode(inputs)\n    print(encoded_inputs)\n    with tfe.restore_variables_on_create(ckpt_path):\n        model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n        print(model_output)\n    return decode(model_output)\n\n\ninputs = \"The animal didn't cross the street because it was too tired\"\noutputs = translate(inputs)\n\nprint(\"Inputs: %s\" % inputs)\nprint(\"Outputs: %s\" % outputs)`\n</code></pre>\n<h3>Source code / logs</h3>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>def translate(inputs):<br>\n...     encoded_inputs = encode(inputs)<br>\n...     print(encoded_inputs)<br>\n...     with tfe.restore_variables_on_create(ckpt_path):<br>\n...         model_output = translate_model.infer(encoded_inputs)[\"outputs\"]<br>\n...         print(model_output)<br>\n...     return decode(model_output)<br>\n...<br>\ninputs = \"The animal didn't cross the street because it was too tired\"<br>\noutputs = translate(inputs)<br>\n{'inputs': &lt;tf.Tensor: id=202, shape=(1, 15, 1), dtype=int32, numpy=<br>\narray([[[   28],<br>\n[ 4705],<br>\n[ 6253],<br>\n[   83],<br>\n[   62],<br>\n[ 3444],<br>\n[    4],<br>\n[ 3825],<br>\n[  244],<br>\n[   40],<br>\n[   53],<br>\n[  362],<br>\n[19285],<br>\n[   85],<br>\n[    1]]])&gt;}<br>\nTraceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nFile \"\", line 4, in translate<br>\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\contextlib.py\", line 81, in <strong>enter</strong><br>\nreturn next(self.gen)<br>\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\eager\\python\\saver.py\", line 91, in restore_variables_on_create<br>\nckpt_var_cache[k] = reader.get_tensor(k)<br>\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 348, in get_tensor<br>\nstatus)<br>\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in <strong>exit</strong><br>\nc_api.TF_GetCode(self.status.status))<br>\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested<br>\nprint(\"Inputs: %s\" % inputs)<br>\nInputs: The animal didn't cross the street because it was too tired<br>\nprint(\"Outputs: %s\" % outputs)<br>\nTraceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nNameError: name 'outputs' is not defined</p>\n</blockquote>\n</blockquote>\n</blockquote>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nno\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nwindows 10, x64\nTensorFlow installed from (source or binary):\npip (pip install --upgrade tensorflow)\nTensorFlow version (use command below):\n1.10.0\nPython version:\n3.6\nGPU model and memory:\nnot using GPU, 12GB memory\nMobile device: N/A\nBazel version : N/A\nCUDA/cuDNN version : N/A\n\nDescribe the problem\nI'm trying to make the Tensor2Tensor intro: English to German translation with a pre-trained model work locally. I have downloaded the checkpoints to the right directory, however it's returning this error:\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\nAs you can see the checkpoints are in the proper directory:\n\nI have tried redownloading the checkpoints many times, I can't seem to get the English to German translation working with the provided checkpoints.\n@lukaszkaiser\n\nExact command to reproduce:\n\n# Imports we need.\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport collections\n\nfrom tensor2tensor import models\nfrom tensor2tensor import problems\nfrom tensor2tensor.layers import common_layers\nfrom tensor2tensor.utils import trainer_lib\nfrom tensor2tensor.utils import t2t_model\nfrom tensor2tensor.utils import registry\nfrom tensor2tensor.utils import metrics\n\n# Enable TF Eager execution\ntfe = tf.contrib.eager\ntfe.enable_eager_execution()\n\n# Other setup\nModes = tf.estimator.ModeKeys\n\n# Setup some directories\ndata_dir = os.path.expanduser(\"~/t2t-ende/data\")\ntmp_dir = os.path.expanduser(\"~/t2t-ende/tmp\")\ntrain_dir = os.path.expanduser(\"~/t2t-ende/train\")\ncheckpoint_dir = os.path.expanduser(\"~/t2t-ende/checkpoints\")\ntf.gfile.MakeDirs(data_dir)\ntf.gfile.MakeDirs(tmp_dir)\ntf.gfile.MakeDirs(train_dir)\ntf.gfile.MakeDirs(checkpoint_dir)\ngs_data_dir = \"gs://tensor2tensor-data\"\ngs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\"\n\n# Fetch the problem\nende_problem = problems.problem(\"translate_ende_wmt32k\")\n\n# Copy the vocab file locally so we can encode inputs and decode model outputs\n# All vocabs are stored on GCS\nvocab_name = \"vocab.translate_ende_wmt32k.32768.subwords\"\nvocab_file = os.path.join(gs_data_dir, vocab_name)\n\n# Get the encoders from the problem\nencoders = ende_problem.feature_encoders(data_dir)\n\n# Setup helper functions for encoding and decoding\n\n\ndef encode(input_str, output_str=None):\n    \"\"\"Input str to features dict, ready for inference\"\"\"\n    inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n    batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n    return {\"inputs\": batch_inputs}\n\n\ndef decode(integers):\n    \"\"\"List of ints to str\"\"\"\n    integers = list(np.squeeze(integers))\n    if 1 in integers:\n        integers = integers[:integers.index(1)]\n    return encoders[\"inputs\"].decode(np.squeeze(integers))\n\n# Generate and view the data\n# This cell is commented out because WMT data generation can take hours\n\n\nende_problem.generate_data(data_dir, tmp_dir)\nexample = tfe.Iterator(ende_problem.dataset(Modes.TRAIN, data_dir)).next()\ninputs = [int(x) for x in example[\"inputs\"].numpy()]  # Cast to ints.\ntargets = [int(x) for x in example[\"targets\"].numpy()]  # Cast to ints.\n\n# Example inputs as int-tensor.\nprint(\"Inputs, encoded:\")\nprint(inputs)\nprint(\"Inputs, decoded:\")\n# Example inputs as a sentence.\nprint(decode(inputs))\n# Example targets as int-tensor.\nprint(\"Targets, encoded:\")\nprint(targets)\n# Example targets as a sentence.\nprint(\"Targets, decoded:\")\nprint(decode(targets))\n\n# Create hparams and the model\nmodel_name = \"transformer\"\nhparams_set = \"transformer_base\"\n\nhparams = trainer_lib.create_hparams(\n    hparams_set, data_dir=data_dir, problem_name=\"translate_ende_wmt32k\")\n\n# NOTE: Only create the model once when restoring from a checkpoint; it's a\n# Layer and so subsequent instantiations will have different variable scopes\n# that will not match the checkpoint.\ntranslate_model = registry.model(model_name)(hparams, Modes.EVAL)\n\n# Copy the pretrained checkpoint locally\nckpt_name = \"transformer_ende_test\"\ngs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\nckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))\n\n# Restore and translate!\ndef translate(inputs):\n    encoded_inputs = encode(inputs)\n    print(encoded_inputs)\n    with tfe.restore_variables_on_create(ckpt_path):\n        model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n        print(model_output)\n    return decode(model_output)\n\n\ninputs = \"The animal didn't cross the street because it was too tired\"\noutputs = translate(inputs)\n\nprint(\"Inputs: %s\" % inputs)\nprint(\"Outputs: %s\" % outputs)`\n\nSource code / logs\n\n\n\ndef translate(inputs):\n...     encoded_inputs = encode(inputs)\n...     print(encoded_inputs)\n...     with tfe.restore_variables_on_create(ckpt_path):\n...         model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n...         print(model_output)\n...     return decode(model_output)\n...\ninputs = \"The animal didn't cross the street because it was too tired\"\noutputs = translate(inputs)\n{'inputs': <tf.Tensor: id=202, shape=(1, 15, 1), dtype=int32, numpy=\narray([[[   28],\n[ 4705],\n[ 6253],\n[   83],\n[   62],\n[ 3444],\n[    4],\n[ 3825],\n[  244],\n[   40],\n[   53],\n[  362],\n[19285],\n[   85],\n[    1]]])>}\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"\", line 4, in translate\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\contextlib.py\", line 81, in enter\nreturn next(self.gen)\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\eager\\python\\saver.py\", line 91, in restore_variables_on_create\nckpt_var_cache[k] = reader.get_tensor(k)\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 348, in get_tensor\nstatus)\nFile \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\nprint(\"Inputs: %s\" % inputs)\nInputs: The animal didn't cross the street because it was too tired\nprint(\"Outputs: %s\" % outputs)\nTraceback (most recent call last):\nFile \"\", line 1, in \nNameError: name 'outputs' is not defined", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nwindows 10, x64\r\n- **TensorFlow installed from (source or binary)**:\r\n pip (pip install --upgrade tensorflow)\r\n- **TensorFlow version (use command below)**:\r\n1.10.0\r\n- **Python version**:\r\n3.6\r\n- **GPU model and memory**:\r\nnot using GPU, 12GB memory\r\nMobile device: N/A\r\nBazel version : N/A\r\nCUDA/cuDNN version : N/A\r\n\r\n### Describe the problem \r\nI'm trying to make the Tensor2Tensor intro: English to German translation with a pre-trained model work locally. I have downloaded the checkpoints to the right directory, however it's returning this error:\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n\r\nAs you can see the checkpoints are in the proper directory:\r\n![image](https://user-images.githubusercontent.com/24248699/45819470-ac0e8c00-bce4-11e8-85bf-fccc4e2e7c22.png)\r\n\r\nI have tried redownloading the checkpoints many times, I can't seem to get the English to German translation working with the provided checkpoints.\r\n@lukaszkaiser \r\n\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n# Imports we need.\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport collections\r\n\r\nfrom tensor2tensor import models\r\nfrom tensor2tensor import problems\r\nfrom tensor2tensor.layers import common_layers\r\nfrom tensor2tensor.utils import trainer_lib\r\nfrom tensor2tensor.utils import t2t_model\r\nfrom tensor2tensor.utils import registry\r\nfrom tensor2tensor.utils import metrics\r\n\r\n# Enable TF Eager execution\r\ntfe = tf.contrib.eager\r\ntfe.enable_eager_execution()\r\n\r\n# Other setup\r\nModes = tf.estimator.ModeKeys\r\n\r\n# Setup some directories\r\ndata_dir = os.path.expanduser(\"~/t2t-ende/data\")\r\ntmp_dir = os.path.expanduser(\"~/t2t-ende/tmp\")\r\ntrain_dir = os.path.expanduser(\"~/t2t-ende/train\")\r\ncheckpoint_dir = os.path.expanduser(\"~/t2t-ende/checkpoints\")\r\ntf.gfile.MakeDirs(data_dir)\r\ntf.gfile.MakeDirs(tmp_dir)\r\ntf.gfile.MakeDirs(train_dir)\r\ntf.gfile.MakeDirs(checkpoint_dir)\r\ngs_data_dir = \"gs://tensor2tensor-data\"\r\ngs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\"\r\n\r\n# Fetch the problem\r\nende_problem = problems.problem(\"translate_ende_wmt32k\")\r\n\r\n# Copy the vocab file locally so we can encode inputs and decode model outputs\r\n# All vocabs are stored on GCS\r\nvocab_name = \"vocab.translate_ende_wmt32k.32768.subwords\"\r\nvocab_file = os.path.join(gs_data_dir, vocab_name)\r\n\r\n# Get the encoders from the problem\r\nencoders = ende_problem.feature_encoders(data_dir)\r\n\r\n# Setup helper functions for encoding and decoding\r\n\r\n\r\ndef encode(input_str, output_str=None):\r\n    \"\"\"Input str to features dict, ready for inference\"\"\"\r\n    inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\r\n    batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\r\n    return {\"inputs\": batch_inputs}\r\n\r\n\r\ndef decode(integers):\r\n    \"\"\"List of ints to str\"\"\"\r\n    integers = list(np.squeeze(integers))\r\n    if 1 in integers:\r\n        integers = integers[:integers.index(1)]\r\n    return encoders[\"inputs\"].decode(np.squeeze(integers))\r\n\r\n# Generate and view the data\r\n# This cell is commented out because WMT data generation can take hours\r\n\r\n\r\nende_problem.generate_data(data_dir, tmp_dir)\r\nexample = tfe.Iterator(ende_problem.dataset(Modes.TRAIN, data_dir)).next()\r\ninputs = [int(x) for x in example[\"inputs\"].numpy()]  # Cast to ints.\r\ntargets = [int(x) for x in example[\"targets\"].numpy()]  # Cast to ints.\r\n\r\n# Example inputs as int-tensor.\r\nprint(\"Inputs, encoded:\")\r\nprint(inputs)\r\nprint(\"Inputs, decoded:\")\r\n# Example inputs as a sentence.\r\nprint(decode(inputs))\r\n# Example targets as int-tensor.\r\nprint(\"Targets, encoded:\")\r\nprint(targets)\r\n# Example targets as a sentence.\r\nprint(\"Targets, decoded:\")\r\nprint(decode(targets))\r\n\r\n# Create hparams and the model\r\nmodel_name = \"transformer\"\r\nhparams_set = \"transformer_base\"\r\n\r\nhparams = trainer_lib.create_hparams(\r\n    hparams_set, data_dir=data_dir, problem_name=\"translate_ende_wmt32k\")\r\n\r\n# NOTE: Only create the model once when restoring from a checkpoint; it's a\r\n# Layer and so subsequent instantiations will have different variable scopes\r\n# that will not match the checkpoint.\r\ntranslate_model = registry.model(model_name)(hparams, Modes.EVAL)\r\n\r\n# Copy the pretrained checkpoint locally\r\nckpt_name = \"transformer_ende_test\"\r\ngs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\r\nckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))\r\n\r\n# Restore and translate!\r\ndef translate(inputs):\r\n    encoded_inputs = encode(inputs)\r\n    print(encoded_inputs)\r\n    with tfe.restore_variables_on_create(ckpt_path):\r\n        model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\r\n        print(model_output)\r\n    return decode(model_output)\r\n\r\n\r\ninputs = \"The animal didn't cross the street because it was too tired\"\r\noutputs = translate(inputs)\r\n\r\nprint(\"Inputs: %s\" % inputs)\r\nprint(\"Outputs: %s\" % outputs)`\r\n```\r\n\r\n### Source code / logs\r\n>>> def translate(inputs):\r\n...     encoded_inputs = encode(inputs)\r\n...     print(encoded_inputs)\r\n...     with tfe.restore_variables_on_create(ckpt_path):\r\n...         model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\r\n...         print(model_output)\r\n...     return decode(model_output)\r\n...\r\n>>> inputs = \"The animal didn't cross the street because it was too tired\"\r\n>>> outputs = translate(inputs)\r\n{'inputs': <tf.Tensor: id=202, shape=(1, 15, 1), dtype=int32, numpy=\r\narray([[[   28],\r\n        [ 4705],\r\n        [ 6253],\r\n        [   83],\r\n        [   62],\r\n        [ 3444],\r\n        [    4],\r\n        [ 3825],\r\n        [  244],\r\n        [   40],\r\n        [   53],\r\n        [  362],\r\n        [19285],\r\n        [   85],\r\n        [    1]]])>}\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 4, in translate\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\eager\\python\\saver.py\", line 91, in restore_variables_on_create\r\n    ckpt_var_cache[k] = reader.get_tensor(k)\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 348, in get_tensor\r\n    status)\r\n  File \"C:\\Users\\david.salsbach\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read fewer bytes than requested\r\n>>> print(\"Inputs: %s\" % inputs)\r\nInputs: The animal didn't cross the street because it was too tired\r\n>>> print(\"Outputs: %s\" % outputs)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nNameError: name 'outputs' is not defined\r\n"}