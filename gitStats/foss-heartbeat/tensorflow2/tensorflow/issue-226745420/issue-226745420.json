{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9716", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9716/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9716/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9716/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9716", "id": 226745420, "node_id": "MDU6SXNzdWUyMjY3NDU0MjA=", "number": 9716, "title": "applying different batch_sizes for each buckets in bucketd_rnn?", "user": {"login": "yhg0112", "id": 5001738, "node_id": "MDQ6VXNlcjUwMDE3Mzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5001738?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yhg0112", "html_url": "https://github.com/yhg0112", "followers_url": "https://api.github.com/users/yhg0112/followers", "following_url": "https://api.github.com/users/yhg0112/following{/other_user}", "gists_url": "https://api.github.com/users/yhg0112/gists{/gist_id}", "starred_url": "https://api.github.com/users/yhg0112/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yhg0112/subscriptions", "organizations_url": "https://api.github.com/users/yhg0112/orgs", "repos_url": "https://api.github.com/users/yhg0112/repos", "events_url": "https://api.github.com/users/yhg0112/events{/privacy}", "received_events_url": "https://api.github.com/users/yhg0112/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-05-06T06:56:28Z", "updated_at": "2017-05-12T03:54:57Z", "closed_at": "2017-05-12T03:49:04Z", "author_association": "NONE", "body_html": "<p>I've found an obvious fact that, in seq2seq model with buckets(especially in NMT applications), the shorter buckets takes extremely smaller memory size in GPU than the longer buckets in same batch_size in both training and inferencing; i.e 64 sentences for (5, 10) takes extremely smaller memory size than 64 sentences for (30, 40).</p>\n<p>Then why don't we have different bucket sizes for each bucket?</p>\n<p>Actually i've run my experiments in these way and it really helps cutting down my training time.</p>\n<p>There may be some issues in having different bucket sizes like sampling frequency for each buckets, the issue from recent paper in ICLR that claiming the smaller batch size make the model better in generalization, etc...</p>\n<p>Yet, my preemptive concern was the training time so i managed different batch sizes like 1024 sentences for (5, 10) bucket and 64 sentences for (30, 40) bucket that maximize my GPU's utility.</p>\n<p>(absolutely this cannot be done with dynamic_rnn structure.)</p>\n<p>what do you guys think about this idea?</p>", "body_text": "I've found an obvious fact that, in seq2seq model with buckets(especially in NMT applications), the shorter buckets takes extremely smaller memory size in GPU than the longer buckets in same batch_size in both training and inferencing; i.e 64 sentences for (5, 10) takes extremely smaller memory size than 64 sentences for (30, 40).\nThen why don't we have different bucket sizes for each bucket?\nActually i've run my experiments in these way and it really helps cutting down my training time.\nThere may be some issues in having different bucket sizes like sampling frequency for each buckets, the issue from recent paper in ICLR that claiming the smaller batch size make the model better in generalization, etc...\nYet, my preemptive concern was the training time so i managed different batch sizes like 1024 sentences for (5, 10) bucket and 64 sentences for (30, 40) bucket that maximize my GPU's utility.\n(absolutely this cannot be done with dynamic_rnn structure.)\nwhat do you guys think about this idea?", "body": "I've found an obvious fact that, in seq2seq model with buckets(especially in NMT applications), the shorter buckets takes extremely smaller memory size in GPU than the longer buckets in same batch_size in both training and inferencing; i.e 64 sentences for (5, 10) takes extremely smaller memory size than 64 sentences for (30, 40).\r\n\r\nThen why don't we have different bucket sizes for each bucket? \r\n\r\nActually i've run my experiments in these way and it really helps cutting down my training time.\r\n\r\nThere may be some issues in having different bucket sizes like sampling frequency for each buckets, the issue from recent paper in ICLR that claiming the smaller batch size make the model better in generalization, etc...\r\n\r\nYet, my preemptive concern was the training time so i managed different batch sizes like 1024 sentences for (5, 10) bucket and 64 sentences for (30, 40) bucket that maximize my GPU's utility.\r\n\r\n(absolutely this cannot be done with dynamic_rnn structure.)\r\n\r\nwhat do you guys think about this idea?"}