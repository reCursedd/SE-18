{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/405854556", "html_url": "https://github.com/tensorflow/tensorflow/issues/20769#issuecomment-405854556", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20769", "id": 405854556, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTg1NDU1Ng==", "user": {"login": "DutAlex", "id": 30656390, "node_id": "MDQ6VXNlcjMwNjU2Mzkw", "avatar_url": "https://avatars2.githubusercontent.com/u/30656390?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DutAlex", "html_url": "https://github.com/DutAlex", "followers_url": "https://api.github.com/users/DutAlex/followers", "following_url": "https://api.github.com/users/DutAlex/following{/other_user}", "gists_url": "https://api.github.com/users/DutAlex/gists{/gist_id}", "starred_url": "https://api.github.com/users/DutAlex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DutAlex/subscriptions", "organizations_url": "https://api.github.com/users/DutAlex/orgs", "repos_url": "https://api.github.com/users/DutAlex/repos", "events_url": "https://api.github.com/users/DutAlex/events{/privacy}", "received_events_url": "https://api.github.com/users/DutAlex/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-18T08:35:40Z", "updated_at": "2018-07-18T08:47:17Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> Yes, all the GPU memory is allocated to the JVM process (using <code>nvidia-smi</code> ). To run the program, I am building the .jar file using <code>mvn package</code> and running it with using <code>java -cp jar-file-with-dependencies.jar</code>.</p>\n<p>In the main file, I am loading a model and open a file where I make prediction on 10000 inputs (1-by-1). I am using <code>System.nanoTime()</code> to profile how long did the prediction of those 10000 examples take.<br>\nI see no difference in execution time whether I am setting the GPUOptions (<code>GPUOptions.newBuilder().setPerProcessGpuMemoryFraction(0.01).build()</code>) or using the historical version of the SavedModelBundle (<code>SavedModelBundle.load(modelDir, \"serve\")</code>).<br>\nIn addition to that, I am also profiling my GPU activity every second while the model is making predictions (using <code>gpustat</code>) and see that its activity during the predictions is around ~65% whether I am setting the GPUOptions or not.</p>", "body_text": "@asimshankar Yes, all the GPU memory is allocated to the JVM process (using nvidia-smi ). To run the program, I am building the .jar file using mvn package and running it with using java -cp jar-file-with-dependencies.jar.\nIn the main file, I am loading a model and open a file where I make prediction on 10000 inputs (1-by-1). I am using System.nanoTime() to profile how long did the prediction of those 10000 examples take.\nI see no difference in execution time whether I am setting the GPUOptions (GPUOptions.newBuilder().setPerProcessGpuMemoryFraction(0.01).build()) or using the historical version of the SavedModelBundle (SavedModelBundle.load(modelDir, \"serve\")).\nIn addition to that, I am also profiling my GPU activity every second while the model is making predictions (using gpustat) and see that its activity during the predictions is around ~65% whether I am setting the GPUOptions or not.", "body": "@asimshankar Yes, all the GPU memory is allocated to the JVM process (using `nvidia-smi` ). To run the program, I am building the .jar file using `mvn package` and running it with using `java -cp jar-file-with-dependencies.jar`.\r\n\r\nIn the main file, I am loading a model and open a file where I make prediction on 10000 inputs (1-by-1). I am using `System.nanoTime()` to profile how long did the prediction of those 10000 examples take.\r\nI see no difference in execution time whether I am setting the GPUOptions (`GPUOptions.newBuilder().setPerProcessGpuMemoryFraction(0.01).build()`) or using the historical version of the SavedModelBundle (`SavedModelBundle.load(modelDir, \"serve\")`).\r\nIn addition to that, I am also profiling my GPU activity every second while the model is making predictions (using `gpustat`) and see that its activity during the predictions is around ~65% whether I am setting the GPUOptions or not."}