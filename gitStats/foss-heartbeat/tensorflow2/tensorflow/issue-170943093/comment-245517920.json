{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/245517920", "html_url": "https://github.com/tensorflow/tensorflow/issues/3775#issuecomment-245517920", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3775", "id": 245517920, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NTUxNzkyMA==", "user": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-08T07:38:13Z", "updated_at": "2016-09-08T07:38:13Z", "author_association": "MEMBER", "body_html": "<p>I believe this is a duplicate of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"162952191\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3103\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3103/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3103\">#3103</a>, which was closed as \"working as intended\".</p>\n<p>The basic problem is that there's a speed/accuracy tradeoff for reduce_mean.  Even though your example seems trivial, with the entire matrix initialized to -1.0, there are 20M+ entries (4500*4500).  Note that float32 can represent integral numbers in the range [-16M,+16M] without loss of precison (24 bits):<br>\n<a href=\"https://en.wikipedia.org/wiki/Single-precision_floating-point_format\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Single-precision_floating-point_format</a></p>\n<p>So we expect to see some error if we accumulate all of the -1.0 entries and divide at the end to compute the mean.  This also explains why smaller matrices didn't exhibit the problem.</p>\n<p>Hope this answers your question!</p>", "body_text": "I believe this is a duplicate of #3103, which was closed as \"working as intended\".\nThe basic problem is that there's a speed/accuracy tradeoff for reduce_mean.  Even though your example seems trivial, with the entire matrix initialized to -1.0, there are 20M+ entries (4500*4500).  Note that float32 can represent integral numbers in the range [-16M,+16M] without loss of precison (24 bits):\nhttps://en.wikipedia.org/wiki/Single-precision_floating-point_format\nSo we expect to see some error if we accumulate all of the -1.0 entries and divide at the end to compute the mean.  This also explains why smaller matrices didn't exhibit the problem.\nHope this answers your question!", "body": "I believe this is a duplicate of #3103, which was closed as \"working as intended\".\n\nThe basic problem is that there's a speed/accuracy tradeoff for reduce_mean.  Even though your example seems trivial, with the entire matrix initialized to -1.0, there are 20M+ entries (4500*4500).  Note that float32 can represent integral numbers in the range [-16M,+16M] without loss of precison (24 bits):\nhttps://en.wikipedia.org/wiki/Single-precision_floating-point_format\n\nSo we expect to see some error if we accumulate all of the -1.0 entries and divide at the end to compute the mean.  This also explains why smaller matrices didn't exhibit the problem.\n\nHope this answers your question!\n"}