{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13906", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13906/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13906/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13906/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13906", "id": 267480414, "node_id": "MDU6SXNzdWUyNjc0ODA0MTQ=", "number": 13906, "title": "Conditional input to a sequence to sequence model (word+character hybrid network)", "user": {"login": "Raghava14", "id": 22686557, "node_id": "MDQ6VXNlcjIyNjg2NTU3", "avatar_url": "https://avatars2.githubusercontent.com/u/22686557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Raghava14", "html_url": "https://github.com/Raghava14", "followers_url": "https://api.github.com/users/Raghava14/followers", "following_url": "https://api.github.com/users/Raghava14/following{/other_user}", "gists_url": "https://api.github.com/users/Raghava14/gists{/gist_id}", "starred_url": "https://api.github.com/users/Raghava14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Raghava14/subscriptions", "organizations_url": "https://api.github.com/users/Raghava14/orgs", "repos_url": "https://api.github.com/users/Raghava14/repos", "events_url": "https://api.github.com/users/Raghava14/events{/privacy}", "received_events_url": "https://api.github.com/users/Raghava14/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-10-22T16:54:43Z", "updated_at": "2017-10-23T22:32:09Z", "closed_at": "2017-10-23T22:27:37Z", "author_association": "NONE", "body_html": "<p>I need to create a sequence to sequence model where in the encoder and decoder are both LSTM networks but the encoder takes the inputs from either of the following cases</p>\n<p>1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary</p>\n<p>2.Output of another LSTM network - when the word is out of vocabulary and a separate character based LSTM is used to generate an embedding on the fly</p>\n<p>Consider the following example sentence:<br>\n\"The brown fox jumped over the lazy dog\"</p>\n<p>Assume these are the words present in the vocabulary: The, brown, jumped, over, dog - These words are fed to the seq2seq encoder as such</p>\n<p>out of vocabulary(OOV) words are: fox, lazy - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words</p>\n<p>These both word level and character level encoder needs to be trained end to end simultaneously. How do we model the input layer of the seq2seq model to achieve this scenario?</p>\n<p>Reference paper:<br>\n<a href=\"http://aclweb.org/anthology/P/P16/P16-1100.pdf\" rel=\"nofollow\">http://aclweb.org/anthology/P/P16/P16-1100.pdf</a></p>", "body_text": "I need to create a sequence to sequence model where in the encoder and decoder are both LSTM networks but the encoder takes the inputs from either of the following cases\n1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary\n2.Output of another LSTM network - when the word is out of vocabulary and a separate character based LSTM is used to generate an embedding on the fly\nConsider the following example sentence:\n\"The brown fox jumped over the lazy dog\"\nAssume these are the words present in the vocabulary: The, brown, jumped, over, dog - These words are fed to the seq2seq encoder as such\nout of vocabulary(OOV) words are: fox, lazy - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words\nThese both word level and character level encoder needs to be trained end to end simultaneously. How do we model the input layer of the seq2seq model to achieve this scenario?\nReference paper:\nhttp://aclweb.org/anthology/P/P16/P16-1100.pdf", "body": "I need to create a sequence to sequence model where in the encoder and decoder are both LSTM networks but the encoder takes the inputs from either of the following cases\r\n\r\n1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary\r\n\r\n2.Output of another LSTM network - when the word is out of vocabulary and a separate character based LSTM is used to generate an embedding on the fly\r\n\r\nConsider the following example sentence:\r\n\"The brown fox jumped over the lazy dog\"\r\n\r\nAssume these are the words present in the vocabulary: The, brown, jumped, over, dog - These words are fed to the seq2seq encoder as such\r\n\r\nout of vocabulary(OOV) words are: fox, lazy - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words\r\n\r\nThese both word level and character level encoder needs to be trained end to end simultaneously. How do we model the input layer of the seq2seq model to achieve this scenario?\r\n\r\nReference paper:\r\nhttp://aclweb.org/anthology/P/P16/P16-1100.pdf\r\n"}