{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23246", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23246/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23246/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23246/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23246", "id": 373828288, "node_id": "MDU6SXNzdWUzNzM4MjgyODg=", "number": 23246, "title": "Grpc+RDMA problem", "user": {"login": "lianyunwen", "id": 11433174, "node_id": "MDQ6VXNlcjExNDMzMTc0", "avatar_url": "https://avatars3.githubusercontent.com/u/11433174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lianyunwen", "html_url": "https://github.com/lianyunwen", "followers_url": "https://api.github.com/users/lianyunwen/followers", "following_url": "https://api.github.com/users/lianyunwen/following{/other_user}", "gists_url": "https://api.github.com/users/lianyunwen/gists{/gist_id}", "starred_url": "https://api.github.com/users/lianyunwen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lianyunwen/subscriptions", "organizations_url": "https://api.github.com/users/lianyunwen/orgs", "repos_url": "https://api.github.com/users/lianyunwen/repos", "events_url": "https://api.github.com/users/lianyunwen/events{/privacy}", "received_events_url": "https://api.github.com/users/lianyunwen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 1093464312, "node_id": "MDU6TGFiZWwxMDkzNDY0MzEy", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:others", "name": "type:others", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-10-25T08:23:46Z", "updated_at": "2018-11-20T18:26:55Z", "closed_at": "2018-11-20T18:26:54Z", "author_association": "NONE", "body_html": "<p>Same as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"315347747\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/18630\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/18630/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/18630\">#18630</a></p>\n<p>Only start ps, it will got error, when worker is on other node. And will waiting when worker is on the same node.</p>\n<p><strong>System information</strong><br>\nHave I written custom code\uff1a No<br>\nOS Platform\uff1a Ubuntu 16.04.3 LTS<br>\nTensorFlow installed from\uff1a by source code<br>\nTensorFlow version\uff1a1.8.0<br>\nBazel version: bazel release 0.16.0<br>\nCUDA/cuDNN version: 9.0<br>\nGPU model and memory: P100-PCIE 16280MiB</p>\n<p>config 2Ps and 2 workers, but only start one ps and one worker.<br>\nHere is the ps log. It doesn't wait for the ps/worker, and failure after  retry 5 times.<br>\n2018-10-18 09:09:22.257003: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 192.168.96.135:20002, 1 -&gt; 192.168.96.74:20003}<br>\n2018-10-18 09:09:22.264672: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:20001, 1 -&gt; 192.168.96.74:20004}<br>\n2018-10-18 09:09:22.264716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 192.168.96.135:20002, 1 -&gt; 192.168.96.74:20003}<br>\n2018-10-18 09:09:22.277272: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0<br>\n2018-10-18 09:09:22.285727: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20001<br>\n2018-10-18 09:10:57.293232: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0<br>\n2018-10-18 09:10:57.295504: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (1/5)...<br>\n2018-10-18 09:10:59.296411: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (2/5)...<br>\n2018-10-18 09:11:01.297541: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (3/5)...<br>\n2018-10-18 09:11:04.298136: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (4/5)...<br>\n2018-10-18 09:11:06.299257: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (5/5)...<br>\n2018-10-18 09:11:06.299300: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:1<br>\n2018-10-18 09:11:06.302309: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:1<br>\n2018-10-18 09:11:06.302436: F tensorflow/contrib/verbs/rdma_mgr.cc:142] Check failed: rc-&gt;PingPostSend() == 0 Couldn't post send  to /job:worker/replica:0/task:1 with error: Invalid argument<br>\nAborted (core dumped)</p>", "body_text": "Same as #18630\nOnly start ps, it will got error, when worker is on other node. And will waiting when worker is on the same node.\nSystem information\nHave I written custom code\uff1a No\nOS Platform\uff1a Ubuntu 16.04.3 LTS\nTensorFlow installed from\uff1a by source code\nTensorFlow version\uff1a1.8.0\nBazel version: bazel release 0.16.0\nCUDA/cuDNN version: 9.0\nGPU model and memory: P100-PCIE 16280MiB\nconfig 2Ps and 2 workers, but only start one ps and one worker.\nHere is the ps log. It doesn't wait for the ps/worker, and failure after  retry 5 times.\n2018-10-18 09:09:22.257003: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}\n2018-10-18 09:09:22.264672: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20001, 1 -> 192.168.96.74:20004}\n2018-10-18 09:09:22.264716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}\n2018-10-18 09:09:22.277272: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\n2018-10-18 09:09:22.285727: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20001\n2018-10-18 09:10:57.293232: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\n2018-10-18 09:10:57.295504: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (1/5)...\n2018-10-18 09:10:59.296411: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (2/5)...\n2018-10-18 09:11:01.297541: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (3/5)...\n2018-10-18 09:11:04.298136: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (4/5)...\n2018-10-18 09:11:06.299257: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (5/5)...\n2018-10-18 09:11:06.299300: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:1\n2018-10-18 09:11:06.302309: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:1\n2018-10-18 09:11:06.302436: F tensorflow/contrib/verbs/rdma_mgr.cc:142] Check failed: rc->PingPostSend() == 0 Couldn't post send  to /job:worker/replica:0/task:1 with error: Invalid argument\nAborted (core dumped)", "body": "Same as https://github.com/tensorflow/tensorflow/issues/18630\r\n\r\nOnly start ps, it will got error, when worker is on other node. And will waiting when worker is on the same node.\r\n\r\n**System information**\r\nHave I written custom code\uff1a No\r\nOS Platform\uff1a Ubuntu 16.04.3 LTS\r\nTensorFlow installed from\uff1a by source code\r\nTensorFlow version\uff1a1.8.0\r\nBazel version: bazel release 0.16.0\r\nCUDA/cuDNN version: 9.0\r\nGPU model and memory: P100-PCIE 16280MiB\r\n\r\nconfig 2Ps and 2 workers, but only start one ps and one worker.\r\nHere is the ps log. It doesn't wait for the ps/worker, and failure after  retry 5 times.\r\n2018-10-18 09:09:22.257003: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}\r\n2018-10-18 09:09:22.264672: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20001, 1 -> 192.168.96.74:20004}\r\n2018-10-18 09:09:22.264716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.96.135:20002, 1 -> 192.168.96.74:20003}\r\n2018-10-18 09:09:22.277272: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\r\n2018-10-18 09:09:22.285727: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20001\r\n2018-10-18 09:10:57.293232: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\r\n2018-10-18 09:10:57.295504: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (1/5)...\r\n2018-10-18 09:10:59.296411: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (2/5)...\r\n2018-10-18 09:11:01.297541: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (3/5)...\r\n2018-10-18 09:11:04.298136: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (4/5)...\r\n2018-10-18 09:11:06.299257: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:1: Got OS Error. Retrying (5/5)...\r\n2018-10-18 09:11:06.299300: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:1\r\n2018-10-18 09:11:06.302309: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:1\r\n2018-10-18 09:11:06.302436: F tensorflow/contrib/verbs/rdma_mgr.cc:142] Check failed: rc->PingPostSend() == 0 Couldn't post send  to /job:worker/replica:0/task:1 with error: Invalid argument\r\nAborted (core dumped)\r\n"}