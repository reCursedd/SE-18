{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22603", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22603/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22603/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22603/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22603", "id": 364978431, "node_id": "MDU6SXNzdWUzNjQ5Nzg0MzE=", "number": 22603, "title": "[BUG] possible memory corruption when multiplying large matices", "user": {"login": "vojirt", "id": 8818326, "node_id": "MDQ6VXNlcjg4MTgzMjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8818326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vojirt", "html_url": "https://github.com/vojirt", "followers_url": "https://api.github.com/users/vojirt/followers", "following_url": "https://api.github.com/users/vojirt/following{/other_user}", "gists_url": "https://api.github.com/users/vojirt/gists{/gist_id}", "starred_url": "https://api.github.com/users/vojirt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vojirt/subscriptions", "organizations_url": "https://api.github.com/users/vojirt/orgs", "repos_url": "https://api.github.com/users/vojirt/repos", "events_url": "https://api.github.com/users/vojirt/events{/privacy}", "received_events_url": "https://api.github.com/users/vojirt/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "chsigg", "id": 7523982, "node_id": "MDQ6VXNlcjc1MjM5ODI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7523982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chsigg", "html_url": "https://github.com/chsigg", "followers_url": "https://api.github.com/users/chsigg/followers", "following_url": "https://api.github.com/users/chsigg/following{/other_user}", "gists_url": "https://api.github.com/users/chsigg/gists{/gist_id}", "starred_url": "https://api.github.com/users/chsigg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chsigg/subscriptions", "organizations_url": "https://api.github.com/users/chsigg/orgs", "repos_url": "https://api.github.com/users/chsigg/repos", "events_url": "https://api.github.com/users/chsigg/events{/privacy}", "received_events_url": "https://api.github.com/users/chsigg/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "chsigg", "id": 7523982, "node_id": "MDQ6VXNlcjc1MjM5ODI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7523982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chsigg", "html_url": "https://github.com/chsigg", "followers_url": "https://api.github.com/users/chsigg/followers", "following_url": "https://api.github.com/users/chsigg/following{/other_user}", "gists_url": "https://api.github.com/users/chsigg/gists{/gist_id}", "starred_url": "https://api.github.com/users/chsigg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chsigg/subscriptions", "organizations_url": "https://api.github.com/users/chsigg/orgs", "repos_url": "https://api.github.com/users/chsigg/repos", "events_url": "https://api.github.com/users/chsigg/events{/privacy}", "received_events_url": "https://api.github.com/users/chsigg/received_events", "type": "User", "site_admin": false}, {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-09-28T17:33:40Z", "updated_at": "2018-11-05T13:30:12Z", "closed_at": "2018-11-05T13:30:12Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary (through pip)</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.11.0</li>\n<li><strong>Python version</strong>:<br>\n2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.1.85</li>\n<li><strong>GPU model and memory</strong>:<br>\nTITAN Xp 12GB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>more system information in <a href=\"https://github.com/tensorflow/tensorflow/files/2429099/tf_env.txt\">tf_env.txt</a></p>\n<h3>Describe the problem</h3>\n<p>For large matrix multiplications, part of the result is not correctly computed or its memory is partially overwritten. Not sure what exactly the problem is. I created as small as possible reproducible code that can be run and visualise the problem.</p>\n<p>Here are also visual outputs that I run. The result should be array of constant value 3.<br>\nwith tf.tile:<br>\nsampling = 6<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8818326/46223432-45741880-c34b-11e8-8d0e-88c4a4682073.png\"><img src=\"https://user-images.githubusercontent.com/8818326/46223432-45741880-c34b-11e8-8d0e-88c4a4682073.png\" alt=\"sampling = 6\" style=\"max-width:100%;\"></a><br>\nsampling = 8<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8818326/46223453-591f7f00-c34b-11e8-9fcc-6edf42434fc3.png\"><img src=\"https://user-images.githubusercontent.com/8818326/46223453-591f7f00-c34b-11e8-9fcc-6edf42434fc3.png\" alt=\"sampling = 8\" style=\"max-width:100%;\"></a><br>\nwith broadcasting:<br>\nsampling = 6<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8818326/46223487-73595d00-c34b-11e8-9314-179848bb2f96.png\"><img src=\"https://user-images.githubusercontent.com/8818326/46223487-73595d00-c34b-11e8-9314-179848bb2f96.png\" alt=\"sampling = 6\" style=\"max-width:100%;\"></a><br>\nsampling = 8<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8818326/46223500-7b190180-c34b-11e8-9122-31fb2e0991c7.png\"><img src=\"https://user-images.githubusercontent.com/8818326/46223500-7b190180-c34b-11e8-9122-31fb2e0991c7.png\" alt=\"sampling = 8\" style=\"max-width:100%;\"></a></p>\n<h3>Source code / logs</h3>\n<p>by varying the sampling variable from 6 to 12 (OOM) you can observe the behaviour by plotting the result (as done in the code)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> matplotlib.pyplot <span class=\"pl-k\">as</span> plt\n<span class=\"pl-k\">from</span> mpl_toolkits.axes_grid1 <span class=\"pl-k\">import</span> make_axes_locatable\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> On Nvidia Titan XP 12GB:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 1) MEMORY <span class=\"pl-k\">BUG</span> - for shape [960, 480] and sampling &gt; 6 the output </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>                 variable is partially overwritten (see figure) </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2) OOM error when sampling &gt;= 12</span>\nsampling <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>  \nshape <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">480</span>, <span class=\"pl-c1\">960</span>]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> \"random\" input vectors</span>\ni1 <span class=\"pl-k\">=</span> np.ones(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(shape[<span class=\"pl-c1\">0</span>]<span class=\"pl-k\">*</span>shape[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float)\ni2 <span class=\"pl-k\">=</span> np.ones(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, sampling<span class=\"pl-k\">**</span><span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float)\n\ninput1 <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(i1.shape[<span class=\"pl-c1\">0</span>], i1.shape[<span class=\"pl-c1\">1</span>], i1.shape[<span class=\"pl-c1\">2</span>]), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\ninput2 <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(i2.shape[<span class=\"pl-c1\">0</span>], i2.shape[<span class=\"pl-c1\">1</span>], i2.shape[<span class=\"pl-c1\">2</span>]), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Operation v1:  </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tile before multiplying to have same dimensions</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> vectors (batch size, shape[0]*shape[1], sampling^2 , 3) and</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)</span>\ni1_tile <span class=\"pl-k\">=</span> tf.tile(input1, (<span class=\"pl-c1\">1</span>, sampling<span class=\"pl-k\">**</span><span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>))\ni2_tile <span class=\"pl-k\">=</span> tf.tile(input2, (shape[<span class=\"pl-c1\">0</span>]<span class=\"pl-k\">*</span>shape[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\nres1 <span class=\"pl-k\">=</span> tf.reduce_sum(i1_tile <span class=\"pl-k\">*</span> i2_tile, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Operation v2:  </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> use broadcasting to multiply </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> vectors i1 = (batch size, shape[0]*shape[1],     1     , 3) and</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>         i2 = (batch size,         1        , sampling^3, 3) </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)</span>\nres2 <span class=\"pl-k\">=</span> tf.reduce_sum(input1<span class=\"pl-k\">*</span>input2, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\nres <span class=\"pl-k\">=</span> res1 <span class=\"pl-c\"><span class=\"pl-c\">#</span>res2</span>\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    pred_out <span class=\"pl-k\">=</span> sess.run(res, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{input1: i1, input2: i2})\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> result should be image of size shape with all values equal to 3</span>\nfig <span class=\"pl-k\">=</span> plt.figure()\npred_out <span class=\"pl-k\">=</span> np.reshape(np.mean(pred_out, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>), (shape[<span class=\"pl-c1\">0</span>], shape[<span class=\"pl-c1\">1</span>]))\nimgplot <span class=\"pl-k\">=</span> plt.imshow(pred_out)\nimgplot.set_cmap(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>jet<span class=\"pl-pds\">'</span></span>)\ndivider <span class=\"pl-k\">=</span> make_axes_locatable(plt.gca())\ncax1 <span class=\"pl-k\">=</span> divider.append_axes(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>right<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>5%<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">pad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.05</span>)\nfig.colorbar(imgplot, <span class=\"pl-v\">cax</span><span class=\"pl-k\">=</span>cax1)\nplt.clim(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3.1</span>)\n\nplt.savefig(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>result.png<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">dpi</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>)\n</pre></div>\n<p><strong>Console output of the run of the script:</strong><br>\n/home/mifs/tv278/.virtualenvs/tf-keras/local/lib/python2.7/site-packages/h5py/<strong>init</strong>.py:36: FutureWarning: Conversion of the second argument of issubdtype from <code>float</code> to <code>np.floating</code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type</code>.<br>\nfrom ._conv import register_converters as _register_converters<br>\n2018-09-28 18:22:45.596062: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA<br>\n2018-09-28 18:22:45.865097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:<br>\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582<br>\npciBusID: 0000:17:00.0<br>\ntotalMemory: 11.91GiB freeMemory: 11.74GiB<br>\n2018-09-28 18:22:45.865123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0<br>\n2018-09-28 18:22:46.099514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:<br>\n2018-09-28 18:22:46.099541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0<br>\n2018-09-28 18:22:46.099546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N<br>\n2018-09-28 18:22:46.099791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11359 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:17:00.0, compute capability: 6.1)</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nbinary (through pip)\nTensorFlow version (use command below):\n1.11.0\nPython version:\n2.7.12\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\n9.1.85\nGPU model and memory:\nTITAN Xp 12GB\nExact command to reproduce:\n\nmore system information in tf_env.txt\nDescribe the problem\nFor large matrix multiplications, part of the result is not correctly computed or its memory is partially overwritten. Not sure what exactly the problem is. I created as small as possible reproducible code that can be run and visualise the problem.\nHere are also visual outputs that I run. The result should be array of constant value 3.\nwith tf.tile:\nsampling = 6\n\nsampling = 8\n\nwith broadcasting:\nsampling = 6\n\nsampling = 8\n\nSource code / logs\nby varying the sampling variable from 6 to 12 (OOM) you can observe the behaviour by plotting the result (as done in the code)\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n\n# On Nvidia Titan XP 12GB:\n# 1) MEMORY BUG - for shape [960, 480] and sampling > 6 the output \n#                 variable is partially overwritten (see figure) \n# 2) OOM error when sampling >= 12\nsampling = 8  \nshape = [480, 960]\n\n# \"random\" input vectors\ni1 = np.ones(shape=(shape[0]*shape[1], 1, 3), dtype=np.float)\ni2 = np.ones(shape=(1, sampling**3, 3), dtype=np.float)\n\ninput1 = tf.placeholder(shape=(i1.shape[0], i1.shape[1], i1.shape[2]), dtype=tf.float32)\ninput2 = tf.placeholder(shape=(i2.shape[0], i2.shape[1], i2.shape[2]), dtype=tf.float32)\n\n# Operation v1:  \n# tile before multiplying to have same dimensions\n# vectors (batch size, shape[0]*shape[1], sampling^2 , 3) and\n# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)\ni1_tile = tf.tile(input1, (1, sampling**3, 1))\ni2_tile = tf.tile(input2, (shape[0]*shape[1], 1, 1))\nres1 = tf.reduce_sum(i1_tile * i2_tile, axis=-1)\n\n# Operation v2:  \n# use broadcasting to multiply \n# vectors i1 = (batch size, shape[0]*shape[1],     1     , 3) and\n#         i2 = (batch size,         1        , sampling^3, 3) \n# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)\nres2 = tf.reduce_sum(input1*input2, axis=-1)\n\nres = res1 #res2\n\nwith tf.Session() as sess:\n    pred_out = sess.run(res, feed_dict={input1: i1, input2: i2})\n\n# result should be image of size shape with all values equal to 3\nfig = plt.figure()\npred_out = np.reshape(np.mean(pred_out, axis=-1), (shape[0], shape[1]))\nimgplot = plt.imshow(pred_out)\nimgplot.set_cmap('jet')\ndivider = make_axes_locatable(plt.gca())\ncax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nfig.colorbar(imgplot, cax=cax1)\nplt.clim(0, 3.1)\n\nplt.savefig(\"result.png\", dpi=100)\n\nConsole output of the run of the script:\n/home/mifs/tv278/.virtualenvs/tf-keras/local/lib/python2.7/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\nfrom ._conv import register_converters as _register_converters\n2018-09-28 18:22:45.596062: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n2018-09-28 18:22:45.865097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:17:00.0\ntotalMemory: 11.91GiB freeMemory: 11.74GiB\n2018-09-28 18:22:45.865123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-09-28 18:22:46.099514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-28 18:22:46.099541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0\n2018-09-28 18:22:46.099546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N\n2018-09-28 18:22:46.099791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11359 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:17:00.0, compute capability: 6.1)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (through pip)\r\n- **TensorFlow version (use command below)**:\r\n1.11.0\r\n- **Python version**:\r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.1.85\r\n- **GPU model and memory**:\r\nTITAN Xp 12GB\r\n- **Exact command to reproduce**:\r\n\r\nmore system information in [tf_env.txt](https://github.com/tensorflow/tensorflow/files/2429099/tf_env.txt)\r\n\r\n\r\n### Describe the problem\r\nFor large matrix multiplications, part of the result is not correctly computed or its memory is partially overwritten. Not sure what exactly the problem is. I created as small as possible reproducible code that can be run and visualise the problem. \r\n\r\nHere are also visual outputs that I run. The result should be array of constant value 3.\r\nwith tf.tile:\r\nsampling = 6\r\n![sampling = 6](https://user-images.githubusercontent.com/8818326/46223432-45741880-c34b-11e8-8d0e-88c4a4682073.png)\r\nsampling = 8\r\n![sampling = 8](https://user-images.githubusercontent.com/8818326/46223453-591f7f00-c34b-11e8-9fcc-6edf42434fc3.png)\r\nwith broadcasting:\r\nsampling = 6\r\n![sampling = 6](https://user-images.githubusercontent.com/8818326/46223487-73595d00-c34b-11e8-9314-179848bb2f96.png)\r\nsampling = 8\r\n![sampling = 8](https://user-images.githubusercontent.com/8818326/46223500-7b190180-c34b-11e8-9122-31fb2e0991c7.png)\r\n\r\n### Source code / logs\r\nby varying the sampling variable from 6 to 12 (OOM) you can observe the behaviour by plotting the result (as done in the code)\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\r\n\r\n\r\n# On Nvidia Titan XP 12GB:\r\n# 1) MEMORY BUG - for shape [960, 480] and sampling > 6 the output \r\n#                 variable is partially overwritten (see figure) \r\n# 2) OOM error when sampling >= 12\r\nsampling = 8  \r\nshape = [480, 960]\r\n\r\n# \"random\" input vectors\r\ni1 = np.ones(shape=(shape[0]*shape[1], 1, 3), dtype=np.float)\r\ni2 = np.ones(shape=(1, sampling**3, 3), dtype=np.float)\r\n\r\ninput1 = tf.placeholder(shape=(i1.shape[0], i1.shape[1], i1.shape[2]), dtype=tf.float32)\r\ninput2 = tf.placeholder(shape=(i2.shape[0], i2.shape[1], i2.shape[2]), dtype=tf.float32)\r\n\r\n# Operation v1:  \r\n# tile before multiplying to have same dimensions\r\n# vectors (batch size, shape[0]*shape[1], sampling^2 , 3) and\r\n# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)\r\ni1_tile = tf.tile(input1, (1, sampling**3, 1))\r\ni2_tile = tf.tile(input2, (shape[0]*shape[1], 1, 1))\r\nres1 = tf.reduce_sum(i1_tile * i2_tile, axis=-1)\r\n\r\n# Operation v2:  \r\n# use broadcasting to multiply \r\n# vectors i1 = (batch size, shape[0]*shape[1],     1     , 3) and\r\n#         i2 = (batch size,         1        , sampling^3, 3) \r\n# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)\r\nres2 = tf.reduce_sum(input1*input2, axis=-1)\r\n\r\nres = res1 #res2\r\n\r\nwith tf.Session() as sess:\r\n    pred_out = sess.run(res, feed_dict={input1: i1, input2: i2})\r\n\r\n# result should be image of size shape with all values equal to 3\r\nfig = plt.figure()\r\npred_out = np.reshape(np.mean(pred_out, axis=-1), (shape[0], shape[1]))\r\nimgplot = plt.imshow(pred_out)\r\nimgplot.set_cmap('jet')\r\ndivider = make_axes_locatable(plt.gca())\r\ncax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\r\nfig.colorbar(imgplot, cax=cax1)\r\nplt.clim(0, 3.1)\r\n\r\nplt.savefig(\"result.png\", dpi=100)\r\n\r\n```\r\n\r\n**Console output of the run of the script:**\r\n/home/mifs/tv278/.virtualenvs/tf-keras/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-09-28 18:22:45.596062: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-09-28 18:22:45.865097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.74GiB\r\n2018-09-28 18:22:45.865123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-28 18:22:46.099514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-28 18:22:46.099541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-09-28 18:22:46.099546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-09-28 18:22:46.099791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11359 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n\r\n\r\n\r\n"}