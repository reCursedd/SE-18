{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264031361", "html_url": "https://github.com/tensorflow/tensorflow/issues/5907#issuecomment-264031361", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5907", "id": 264031361, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDAzMTM2MQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-30T23:35:21Z", "updated_at": "2016-11-30T23:35:21Z", "author_association": "MEMBER", "body_html": "<p>Good questions :)</p>\n<p>Second one first: Why is there not a comparable slowdown due to protobuf serialization when talking to a parameter server shard?  Because we optimized the way in which Tensors are protobuf encoded-decoded at the RPC interface for the RecvTensor method.  A numeric Tensor is backed by a dense memory buffer of same-size binary fields.  See core/framework/tensor.proto for how as a protobuf a Tensor can be coded as a large collection of small items, or as a relatively untyped tensor_content byte array.  See the TensorResponse class and supporting code in core/distributed_runtime/tensor_coding.cc and distributed_runtime/rpc/grpc_tensor_coding.cc.  Whenever possible, when building the protobuf for sending a Tensor via RPC, we use a grpc::ByteBuffer to hold the content, rather than as a protobuf of lots of little items.</p>\n<p>Back to your first question, using the tensor_content field of tensor.proto, or the TensorResponse interface defined by tensor_coding.h would be the way to go.  I think the issue you're running into with FIFOQUEUE with the distributed interface is that maybe the RPCs involved are not the optimized RecvTensor method, but some other RPC that doesn't have efficient coding.</p>", "body_text": "Good questions :)\nSecond one first: Why is there not a comparable slowdown due to protobuf serialization when talking to a parameter server shard?  Because we optimized the way in which Tensors are protobuf encoded-decoded at the RPC interface for the RecvTensor method.  A numeric Tensor is backed by a dense memory buffer of same-size binary fields.  See core/framework/tensor.proto for how as a protobuf a Tensor can be coded as a large collection of small items, or as a relatively untyped tensor_content byte array.  See the TensorResponse class and supporting code in core/distributed_runtime/tensor_coding.cc and distributed_runtime/rpc/grpc_tensor_coding.cc.  Whenever possible, when building the protobuf for sending a Tensor via RPC, we use a grpc::ByteBuffer to hold the content, rather than as a protobuf of lots of little items.\nBack to your first question, using the tensor_content field of tensor.proto, or the TensorResponse interface defined by tensor_coding.h would be the way to go.  I think the issue you're running into with FIFOQUEUE with the distributed interface is that maybe the RPCs involved are not the optimized RecvTensor method, but some other RPC that doesn't have efficient coding.", "body": "Good questions :)\r\n\r\nSecond one first: Why is there not a comparable slowdown due to protobuf serialization when talking to a parameter server shard?  Because we optimized the way in which Tensors are protobuf encoded-decoded at the RPC interface for the RecvTensor method.  A numeric Tensor is backed by a dense memory buffer of same-size binary fields.  See core/framework/tensor.proto for how as a protobuf a Tensor can be coded as a large collection of small items, or as a relatively untyped tensor_content byte array.  See the TensorResponse class and supporting code in core/distributed_runtime/tensor_coding.cc and distributed_runtime/rpc/grpc_tensor_coding.cc.  Whenever possible, when building the protobuf for sending a Tensor via RPC, we use a grpc::ByteBuffer to hold the content, rather than as a protobuf of lots of little items.\r\n\r\nBack to your first question, using the tensor_content field of tensor.proto, or the TensorResponse interface defined by tensor_coding.h would be the way to go.  I think the issue you're running into with FIFOQUEUE with the distributed interface is that maybe the RPCs involved are not the optimized RecvTensor method, but some other RPC that doesn't have efficient coding.\r\n"}