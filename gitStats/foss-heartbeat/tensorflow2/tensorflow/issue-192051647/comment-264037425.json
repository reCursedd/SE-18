{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264037425", "html_url": "https://github.com/tensorflow/tensorflow/issues/5907#issuecomment-264037425", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5907", "id": 264037425, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDAzNzQyNQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-01T00:06:38Z", "updated_at": "2016-12-01T00:07:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4019775\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gowithqi\">@gowithqi</a> I've noticed same kind of problem, not limited to FIFOQueue -- specifically, when Python client requests data from another TensorFlow worker, the data is transferred at a rate of 50-200MB/s even if all the workers are local. The bottleneck seems to be protobuf decoding which happens on a single core. (more cores = smaller cores = worse performance)</p>\n<p>Self-contained benchmark that reproduces it is <a href=\"https://github.com/tensorflow/tensorflow/issues/4498#issuecomment-248483967\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4498/hovercard\">here</a>.</p>\n<p>So, in your case, your buffer is 576x3x220x220x4=334MB, so I would expect a single dequeue to take 3 seconds.</p>\n<p>regarding on why \"dequeue\" transfer was faster than \"ps\" transfer -- there are two kinds of transfers:</p>\n<ol>\n<li>Between TensorFlow processes (two C runtimes)</li>\n<li>Between C runtime and Python runtime of the same process</li>\n</ol>\n<p>So you could think of 4 different kind of scenarios on 2 dimensions:</p>\n<ol>\n<li>is data transferred between devices in single process, or between multiple processes</li>\n<li>result fetched in Python client vs remains in C runtime</li>\n</ol>\n<p>So for single process mode on one machine, the speed I saw was 24GB/second (no Python fetch) vs 4GB/second (Python fetch), and for distributed version I saw 1 GB/second (no Python fetch) vs. 60 MB/second (Python fetch)</p>\n<p>The way to run without Python transfer in your example is to do <code>sess.run(dequeue_op.op)</code> instead of <code>sess.run(dequeue_op)</code></p>", "body_text": "@gowithqi I've noticed same kind of problem, not limited to FIFOQueue -- specifically, when Python client requests data from another TensorFlow worker, the data is transferred at a rate of 50-200MB/s even if all the workers are local. The bottleneck seems to be protobuf decoding which happens on a single core. (more cores = smaller cores = worse performance)\nSelf-contained benchmark that reproduces it is here.\nSo, in your case, your buffer is 576x3x220x220x4=334MB, so I would expect a single dequeue to take 3 seconds.\nregarding on why \"dequeue\" transfer was faster than \"ps\" transfer -- there are two kinds of transfers:\n\nBetween TensorFlow processes (two C runtimes)\nBetween C runtime and Python runtime of the same process\n\nSo you could think of 4 different kind of scenarios on 2 dimensions:\n\nis data transferred between devices in single process, or between multiple processes\nresult fetched in Python client vs remains in C runtime\n\nSo for single process mode on one machine, the speed I saw was 24GB/second (no Python fetch) vs 4GB/second (Python fetch), and for distributed version I saw 1 GB/second (no Python fetch) vs. 60 MB/second (Python fetch)\nThe way to run without Python transfer in your example is to do sess.run(dequeue_op.op) instead of sess.run(dequeue_op)", "body": "@gowithqi I've noticed same kind of problem, not limited to FIFOQueue -- specifically, when Python client requests data from another TensorFlow worker, the data is transferred at a rate of 50-200MB/s even if all the workers are local. The bottleneck seems to be protobuf decoding which happens on a single core. (more cores = smaller cores = worse performance)\r\n\r\nSelf-contained benchmark that reproduces it is [here](https://github.com/tensorflow/tensorflow/issues/4498#issuecomment-248483967\r\n).\r\n\r\nSo, in your case, your buffer is 576x3x220x220x4=334MB, so I would expect a single dequeue to take 3 seconds.\r\n\r\nregarding on why \"dequeue\" transfer was faster than \"ps\" transfer -- there are two kinds of transfers:\r\n1. Between TensorFlow processes (two C runtimes)\r\n2. Between C runtime and Python runtime of the same process\r\n\r\nSo you could think of 4 different kind of scenarios on 2 dimensions:\r\n1. is data transferred between devices in single process, or between multiple processes\r\n2. result fetched in Python client vs remains in C runtime\r\n\r\nSo for single process mode on one machine, the speed I saw was 24GB/second (no Python fetch) vs 4GB/second (Python fetch), and for distributed version I saw 1 GB/second (no Python fetch) vs. 60 MB/second (Python fetch)\r\n\r\nThe way to run without Python transfer in your example is to do `sess.run(dequeue_op.op)` instead of `sess.run(dequeue_op)`"}