{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5907", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5907/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5907/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5907/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5907", "id": 192051647, "node_id": "MDU6SXNzdWUxOTIwNTE2NDc=", "number": 5907, "title": "FIFOQueue speed problem ", "user": {"login": "gowithqi", "id": 4019775, "node_id": "MDQ6VXNlcjQwMTk3NzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4019775?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gowithqi", "html_url": "https://github.com/gowithqi", "followers_url": "https://api.github.com/users/gowithqi/followers", "following_url": "https://api.github.com/users/gowithqi/following{/other_user}", "gists_url": "https://api.github.com/users/gowithqi/gists{/gist_id}", "starred_url": "https://api.github.com/users/gowithqi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gowithqi/subscriptions", "organizations_url": "https://api.github.com/users/gowithqi/orgs", "repos_url": "https://api.github.com/users/gowithqi/repos", "events_url": "https://api.github.com/users/gowithqi/events{/privacy}", "received_events_url": "https://api.github.com/users/gowithqi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-11-28T16:30:02Z", "updated_at": "2017-01-16T03:21:12Z", "closed_at": "2017-01-16T03:21:12Z", "author_association": "NONE", "body_html": "<p>Hi I am testing the speed of FIFOQueue. I create a queue of Tensor with type of tf.float32 and shape of (576, 3, 220, 220). Then I push a tensor to the queue, followed by pop the tensor from the queue.<br>\nThe speed of single node version( sess = tf.Session())  and distributed version of tensorflow (even in single machine single process scenario, sess = tf.Session(server.target) ) differs much.<br>\nPushing and poping take about 0.2s in single node version while 4s in  distributed version.<br>\nI know that using distributed version of tensorflow will encounter some proto serialize overhead, but the overhead seems too much.</p>\n<p>The main code is like this (I also give a link to full source code below, which can be used to reproduce the problem)</p>\n<pre><code>    raw_shape = [576, 3, 220, 220]\n    shape = tf.TensorShape(raw_shape)\n    if FLAGS.cluster:    \n        server = tf.train.Server.create_local_server()\n        sess = tf.Session(server.target)\n    else:\n        sess = tf.Session()\n    with tf.device('/cpu:0'): \n        q = tf.FIFOQueue(10, tf.float32, shape)\n        rand_data = tf.zeros(shape)\n        init_op = tf.initialize_local_variables()\n        sess.run(init_op)\n        result = q.dequeue()\n        x = tf.placeholder(tf.float32, shape, 'data')\n        enqueue_op = q.enqueue(x)\n        while True:\n            log('pushing')\n            sess.run(enqueue_op, feed_dict = {x: np.zeros(raw_shape)})\n            log('push done')\n            sess.run(result)\n            log('pop done')\n</code></pre>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"161912688\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3009\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3009/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3009\">#3009</a><br>\nBut this issue is more related to threading, not same as mine case.</p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nUbuntu 14.04<br>\nCPU E5-2643 v3 @ 3.40GHz</p>\n<p>Installed version of CUDA and cuDNN:<br>\nNone (I used CUDA_VISIBLE_DEVICES='')</p>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/d6b25985ac219a6e58d186a2beb74d5e8d9e4533/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/d6b25985ac219a6e58d186a2beb74d5e8d9e4533\"><tt>d6b2598</tt></a></li>\n<li>The output of <code>bazel version</code><br>\nBuild label: 0.4.0<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)<br>\nBuild timestamp: 1478109254<br>\nBuild timestamp as int: 1478109254</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p><a href=\"https://gist.github.com/gowithqi/6bcb1dc50facfd992639f09e9af463a8\">https://gist.github.com/gowithqi/6bcb1dc50facfd992639f09e9af463a8</a></p>\n<p>Log<br>\nCUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true</p>\n<pre><code>2016-11-28 16:18:13.767 pushing\n2016-11-28 16:18:13.998 push done\n2016-11-28 16:18:14.112 pop done\n2016-11-28 16:18:14.112 pushing\n2016-11-28 16:18:14.337 push done\n2016-11-28 16:18:14.501 pop done\n2016-11-28 16:18:14.502 pushing\n2016-11-28 16:18:14.746 push done\n2016-11-28 16:18:14.916 pop done\n2016-11-28 16:18:14.916 pushing\n2016-11-28 16:18:15.155 push done\n2016-11-28 16:18:15.269 pop done\n</code></pre>\n<p>CUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true</p>\n<pre><code>2016-11-28 16:17:00.218 pushing\n2016-11-28 16:17:03.470 push done\n2016-11-28 16:17:07.395 pop done\n2016-11-28 16:17:07.395 pushing\n2016-11-28 16:17:10.908 push done\n2016-11-28 16:17:14.732 pop done\n2016-11-28 16:17:14.733 pushing\n2016-11-28 16:17:17.878 push done\n2016-11-28 16:17:21.788 pop done\n</code></pre>", "body_text": "Hi I am testing the speed of FIFOQueue. I create a queue of Tensor with type of tf.float32 and shape of (576, 3, 220, 220). Then I push a tensor to the queue, followed by pop the tensor from the queue.\nThe speed of single node version( sess = tf.Session())  and distributed version of tensorflow (even in single machine single process scenario, sess = tf.Session(server.target) ) differs much.\nPushing and poping take about 0.2s in single node version while 4s in  distributed version.\nI know that using distributed version of tensorflow will encounter some proto serialize overhead, but the overhead seems too much.\nThe main code is like this (I also give a link to full source code below, which can be used to reproduce the problem)\n    raw_shape = [576, 3, 220, 220]\n    shape = tf.TensorShape(raw_shape)\n    if FLAGS.cluster:    \n        server = tf.train.Server.create_local_server()\n        sess = tf.Session(server.target)\n    else:\n        sess = tf.Session()\n    with tf.device('/cpu:0'): \n        q = tf.FIFOQueue(10, tf.float32, shape)\n        rand_data = tf.zeros(shape)\n        init_op = tf.initialize_local_variables()\n        sess.run(init_op)\n        result = q.dequeue()\n        x = tf.placeholder(tf.float32, shape, 'data')\n        enqueue_op = q.enqueue(x)\n        while True:\n            log('pushing')\n            sess.run(enqueue_op, feed_dict = {x: np.zeros(raw_shape)})\n            log('push done')\n            sess.run(result)\n            log('pop done')\n\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n#3009\nBut this issue is more related to threading, not same as mine case.\nEnvironment info\nOperating System:\nUbuntu 14.04\nCPU E5-2643 v3 @ 3.40GHz\nInstalled version of CUDA and cuDNN:\nNone (I used CUDA_VISIBLE_DEVICES='')\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nd6b2598\nThe output of bazel version\nBuild label: 0.4.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\nBuild timestamp: 1478109254\nBuild timestamp as int: 1478109254\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nhttps://gist.github.com/gowithqi/6bcb1dc50facfd992639f09e9af463a8\nLog\nCUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true\n2016-11-28 16:18:13.767 pushing\n2016-11-28 16:18:13.998 push done\n2016-11-28 16:18:14.112 pop done\n2016-11-28 16:18:14.112 pushing\n2016-11-28 16:18:14.337 push done\n2016-11-28 16:18:14.501 pop done\n2016-11-28 16:18:14.502 pushing\n2016-11-28 16:18:14.746 push done\n2016-11-28 16:18:14.916 pop done\n2016-11-28 16:18:14.916 pushing\n2016-11-28 16:18:15.155 push done\n2016-11-28 16:18:15.269 pop done\n\nCUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true\n2016-11-28 16:17:00.218 pushing\n2016-11-28 16:17:03.470 push done\n2016-11-28 16:17:07.395 pop done\n2016-11-28 16:17:07.395 pushing\n2016-11-28 16:17:10.908 push done\n2016-11-28 16:17:14.732 pop done\n2016-11-28 16:17:14.733 pushing\n2016-11-28 16:17:17.878 push done\n2016-11-28 16:17:21.788 pop done", "body": "Hi I am testing the speed of FIFOQueue. I create a queue of Tensor with type of tf.float32 and shape of (576, 3, 220, 220). Then I push a tensor to the queue, followed by pop the tensor from the queue.\r\nThe speed of single node version( sess = tf.Session())  and distributed version of tensorflow (even in single machine single process scenario, sess = tf.Session(server.target) ) differs much. \r\nPushing and poping take about 0.2s in single node version while 4s in  distributed version.\r\nI know that using distributed version of tensorflow will encounter some proto serialize overhead, but the overhead seems too much.\r\n\r\nThe main code is like this (I also give a link to full source code below, which can be used to reproduce the problem)\r\n```\r\n    raw_shape = [576, 3, 220, 220]\r\n    shape = tf.TensorShape(raw_shape)\r\n    if FLAGS.cluster:    \r\n        server = tf.train.Server.create_local_server()\r\n        sess = tf.Session(server.target)\r\n    else:\r\n        sess = tf.Session()\r\n    with tf.device('/cpu:0'): \r\n        q = tf.FIFOQueue(10, tf.float32, shape)\r\n        rand_data = tf.zeros(shape)\r\n        init_op = tf.initialize_local_variables()\r\n        sess.run(init_op)\r\n        result = q.dequeue()\r\n        x = tf.placeholder(tf.float32, shape, 'data')\r\n        enqueue_op = q.enqueue(x)\r\n        while True:\r\n            log('pushing')\r\n            sess.run(enqueue_op, feed_dict = {x: np.zeros(raw_shape)})\r\n            log('push done')\r\n            sess.run(result)\r\n            log('pop done')\r\n```\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/3009 \r\nBut this issue is more related to threading, not same as mine case.\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\nCPU E5-2643 v3 @ 3.40GHz\r\n\r\nInstalled version of CUDA and cuDNN: \r\nNone (I used CUDA_VISIBLE_DEVICES='')\r\n\r\n\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`)\r\nd6b25985ac219a6e58d186a2beb74d5e8d9e4533\r\n2. The output of `bazel version`\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nhttps://gist.github.com/gowithqi/6bcb1dc50facfd992639f09e9af463a8\r\n\r\nLog\r\nCUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true\r\n```\r\n2016-11-28 16:18:13.767 pushing\r\n2016-11-28 16:18:13.998 push done\r\n2016-11-28 16:18:14.112 pop done\r\n2016-11-28 16:18:14.112 pushing\r\n2016-11-28 16:18:14.337 push done\r\n2016-11-28 16:18:14.501 pop done\r\n2016-11-28 16:18:14.502 pushing\r\n2016-11-28 16:18:14.746 push done\r\n2016-11-28 16:18:14.916 pop done\r\n2016-11-28 16:18:14.916 pushing\r\n2016-11-28 16:18:15.155 push done\r\n2016-11-28 16:18:15.269 pop done\r\n```\r\n\r\nCUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true\r\n```\r\n2016-11-28 16:17:00.218 pushing\r\n2016-11-28 16:17:03.470 push done\r\n2016-11-28 16:17:07.395 pop done\r\n2016-11-28 16:17:07.395 pushing\r\n2016-11-28 16:17:10.908 push done\r\n2016-11-28 16:17:14.732 pop done\r\n2016-11-28 16:17:14.733 pushing\r\n2016-11-28 16:17:17.878 push done\r\n2016-11-28 16:17:21.788 pop done\r\n```\r\n"}