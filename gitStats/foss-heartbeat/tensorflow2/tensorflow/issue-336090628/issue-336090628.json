{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20332", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20332/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20332/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20332/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20332", "id": 336090628, "node_id": "MDU6SXNzdWUzMzYwOTA2Mjg=", "number": 20332, "title": "An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0)", "user": {"login": "nehaljwani", "id": 1779189, "node_id": "MDQ6VXNlcjE3NzkxODk=", "avatar_url": "https://avatars0.githubusercontent.com/u/1779189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nehaljwani", "html_url": "https://github.com/nehaljwani", "followers_url": "https://api.github.com/users/nehaljwani/followers", "following_url": "https://api.github.com/users/nehaljwani/following{/other_user}", "gists_url": "https://api.github.com/users/nehaljwani/gists{/gist_id}", "starred_url": "https://api.github.com/users/nehaljwani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nehaljwani/subscriptions", "organizations_url": "https://api.github.com/users/nehaljwani/orgs", "repos_url": "https://api.github.com/users/nehaljwani/repos", "events_url": "https://api.github.com/users/nehaljwani/events{/privacy}", "received_events_url": "https://api.github.com/users/nehaljwani/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "meteorcloudy", "id": 4171702, "node_id": "MDQ6VXNlcjQxNzE3MDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/4171702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/meteorcloudy", "html_url": "https://github.com/meteorcloudy", "followers_url": "https://api.github.com/users/meteorcloudy/followers", "following_url": "https://api.github.com/users/meteorcloudy/following{/other_user}", "gists_url": "https://api.github.com/users/meteorcloudy/gists{/gist_id}", "starred_url": "https://api.github.com/users/meteorcloudy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/meteorcloudy/subscriptions", "organizations_url": "https://api.github.com/users/meteorcloudy/orgs", "repos_url": "https://api.github.com/users/meteorcloudy/repos", "events_url": "https://api.github.com/users/meteorcloudy/events{/privacy}", "received_events_url": "https://api.github.com/users/meteorcloudy/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "meteorcloudy", "id": 4171702, "node_id": "MDQ6VXNlcjQxNzE3MDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/4171702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/meteorcloudy", "html_url": "https://github.com/meteorcloudy", "followers_url": "https://api.github.com/users/meteorcloudy/followers", "following_url": "https://api.github.com/users/meteorcloudy/following{/other_user}", "gists_url": "https://api.github.com/users/meteorcloudy/gists{/gist_id}", "starred_url": "https://api.github.com/users/meteorcloudy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/meteorcloudy/subscriptions", "organizations_url": "https://api.github.com/users/meteorcloudy/orgs", "repos_url": "https://api.github.com/users/meteorcloudy/repos", "events_url": "https://api.github.com/users/meteorcloudy/events{/privacy}", "received_events_url": "https://api.github.com/users/meteorcloudy/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2018-06-27T06:17:16Z", "updated_at": "2018-09-24T08:19:10Z", "closed_at": "2018-09-24T08:19:09Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Well, I added patches</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10, 64 bit</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>:  3.5, 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 1.12.0, 1.14.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:  MSVC 2015 Update v3</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7.1</li>\n<li><strong>GPU model and memory</strong>: GeForce GT 430</li>\n<li><strong>Exact command to reproduce</strong>: ;-)</li>\n</ul>\n<h3>Description</h3>\n<p>I wanted to compile tensorflow with GPU support on Windows and I went through the ordeal (I was finally able to build it, but I'll come to that later). I always followed the stuff done in: <a href=\"https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tools/ci_build/\">https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tools/ci_build/</a></p>\n<p>This is how it went:</p>\n<p>First, I attempted the CPU build.<br>\nInitial attempt was with CMake. It took ~3hours, all went fine upto the final linking phase, there link.exe died with:</p>\n<pre><code>(Lib target) -&gt;\n  tf_core_kernels.dir\\Release\\tf_core_kernels.lib : fatal error LNK1248: image size (100039B2C) exceeds maximum allowable size (FFFFFFFF) \n</code></pre>\n<p>I was lost here, I tried tinkering with some CMake build options, but all resulted into the same error. So, I gave up CMake.</p>\n<p>Then, I attempted to build with bazel 1.12.0.<br>\nI was using the mingw64 shell, so first, I had to patch <code>tensorflow/tools/pip_package/build_pip_package.sh</code> . I also sent a PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"318680793\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/18953\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/18953/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/18953\">#18953</a></p>\n<p>Surprisingly, the build went fine after that, and then I tried to run the tests. All were successful, except <code>tensorflow/python/kernel_tests/boosted_trees:training_ops_test</code>,  probably because it was not being run serially. Even for running the tests, I had to patch <code>tensorflow/contrib/tensorboard/plugins/trace/trace.py</code> and I also sent a PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"318680816\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/18954\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/18954/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/18954\">#18954</a></p>\n<p>Now comes the real deal. The GPU build.</p>\n<p>Again, I attempted the build with CMake. It took ~3hours and then link.exe failed with the same error as before. I spent quite some time reading about it and couldn't figure it out as most solutions said 'break your huge lib into smaller libs'. I am still baffled as to why the CI job at <a href=\"http://ci.tensorflow.org/job/tf-master-win-gpu-cmake/\" rel=\"nofollow\">http://ci.tensorflow.org/job/tf-master-win-gpu-cmake/</a> doesn't die with the same problem. I am using the same compiler, same linker,<br>\nand my machine is powerful enough. So, I just gave up again on CMake.</p>\n<p>Going back to bazel now.  By this time, bazel 1.14.0 was was released. So I decided to use that instead. I had to apply the two patches as before, but life isn't so easy.</p>\n<p>Next big hurdle: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"297766287\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/17067\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/17067/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/17067\">#17067</a> .</p>\n<ul>\n<li>So I got @dtrebbien 's patch from <a href=\"https://github.com/dtrebbien/protobuf/commit/50f552646ba1de79e07562b41f3999fe036b4fd0\">https://github.com/dtrebbien/protobuf/commit/50f552646ba1de79e07562b41f3999fe036b4fd0</a> and made changes to <code>tensorflow/workspace.bzl</code> to apply it to all protobuf checkouts.</li>\n</ul>\n<p>Next big hurdle: NCCL doesn't seem to be officially supported on Windows.</p>\n<ul>\n<li>But bazel is still trying to build nccl related stuff. I had to patch stuff again, to disable all references to nccl in the BUILD files all over the place. <g-emoji class=\"g-emoji\" alias=\"crying_cat_face\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f63f.png\">\ud83d\ude3f</g-emoji></li>\n</ul>\n<p>Next big hurdle:</p>\n<ul>\n<li>Shared libraries under <code>tensorflow/contrib/rnn/python/ops/</code> failed at link phase with the error:</li>\n</ul>\n<pre><code>Creating library bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.ifso and object bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.exp\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream &amp; __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class perftools::gputools::DeviceMemory&lt;float&gt; const &amp;,int,class perftools::gputools::DeviceMemory&lt;float&gt; const &amp;,int,float,class perftools::gputools::DeviceMemory&lt;float&gt; *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11MAEBV?$DeviceMemory@M@23@H2HMPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm&lt;float&gt;::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,float,float const *,int,float const *,int,float,float *,int)\" (??R?$TensorCuBlasGemm@M@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22MPEBMH3HMPEAMH@Z)\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream &amp; __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,double,class perftools::gputools::DeviceMemory&lt;double&gt; const &amp;,int,class perftools::gputools::DeviceMemory&lt;double&gt; const &amp;,int,double,class perftools::gputools::DeviceMemory&lt;double&gt; *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11NAEBV?$DeviceMemory@N@23@H2HNPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm&lt;double&gt;::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,double,double const *,int,double const *,int,double,double *,int)\" (??R?$TensorCuBlasGemm@N@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22NPEBNH3HNPEANH@Z)\n</code></pre>\n<ul>\n<li>Then I stumbled upon <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"278302064\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15013\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/15013/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/15013\">#15013</a> and saw that another person had faced the same issue quite some time ago and the OP's solution was <code>add all the .lib from tensorflow and cuda</code> . I can't do that. So, I went looking for where this symbol comes from, and where should it be. Took me a while to realize what's happening and that's when I came across the workaround<br>\nof using intermediate interface shared object file at <a href=\"https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tensorflow.bzl#L1228-L1237\">https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tensorflow.bzl#L1228-L1237</a>. Apparently, this didn't have enough symbols for the kernels in <code>tensorflow/contrib/rnn/python/ops/</code>. So I started looking for where I could find them and ended up adding:<br>\n<code>clean_dep(\"//tensorflow/stream_executor:stream_executor_impl\"),</code> to that list and the link error went away.</li>\n</ul>\n<p>Next hurdle: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"333122041\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20088\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/20088/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/20088\">#20088</a></p>\n<ul>\n<li>The compiler doesn't like<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L46\">https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L46</a> when it was already being done at <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu.h#L189\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu.h#L189</a> . So I commented out the typdef in <code>fused_conv2d_bias_activation_op.cc</code> and tried building it. The error went away, but link.exe cried again at the end, because it didn't know where to get the symbol for <code>GetCudnnWorkspaceLimit</code> at <a href=\"https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L519\">https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L519</a> . After some searching, I found out that this symbol comes from <a href=\"https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/kernels/conv_ops.cc#L456-L471\">https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/kernels/conv_ops.cc#L456-L471</a> . So I tried making it part of the intermediary additional_deps_impl.ifso import lib, but no matter what, it always ended up bloating it with symbols &gt; 64k (it went upto ~72k, when I tried adding some deps which might have that symbol). So I gave up on this contrib kernel at this point and when I looked at the CMake build (I could be wrong here) I didn't find it building this kernel either. So I just ended up commenting out all references to <code>fused_conv</code> in the BUILD files.</li>\n</ul>\n<p>Next hurdle: Bazel had made some breaking changes in 1.13.0</p>\n<ul>\n<li>I came across at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"294430208\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/bazelbuild/bazel/issues/4583\" data-hovercard-type=\"issue\" data-hovercard-url=\"/bazelbuild/bazel/issues/4583/hovercard\" href=\"https://github.com/bazelbuild/bazel/issues/4583\">bazelbuild/bazel#4583</a> and then applied two other patches locally to get around that problem.</li>\n</ul>\n<p>Next hurdle: Tests won't run.</p>\n<ul>\n<li>Most of them want to import nccl. <g-emoji class=\"g-emoji\" alias=\"sob\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f62d.png\">\ud83d\ude2d</g-emoji> So I ended up faking it by creating a blank module and then the tests ran fine. I did have to tell bazel to ignore the <code>contrib/lite</code> subset <code>-//${PY_TEST_DIR}/tensorflow/contrib/lite/...</code>.</li>\n</ul>\n<p>For the lost souls who attempt at building tensorflow with GPU support, I hope this issue might be helpful. All the patches and the entire build can be found here: <a href=\"https://github.com/AnacondaRecipes/aggregate/blob/459dee0989e0c7cc4ab66c49d3d7605cddbb1bc3/tensorflow-gpu-base-feedstock/recipe/meta.yaml\">https://github.com/AnacondaRecipes/aggregate/blob/459dee0989e0c7cc4ab66c49d3d7605cddbb1bc3/tensorflow-gpu-base-feedstock/recipe/meta.yaml</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Well, I added patches\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 64 bit\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.8.0\nPython version:  3.5, 3.6\nBazel version (if compiling from source): 1.12.0, 1.14.0\nGCC/Compiler version (if compiling from source):  MSVC 2015 Update v3\nCUDA/cuDNN version: 9.0/7.1\nGPU model and memory: GeForce GT 430\nExact command to reproduce: ;-)\n\nDescription\nI wanted to compile tensorflow with GPU support on Windows and I went through the ordeal (I was finally able to build it, but I'll come to that later). I always followed the stuff done in: https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tools/ci_build/\nThis is how it went:\nFirst, I attempted the CPU build.\nInitial attempt was with CMake. It took ~3hours, all went fine upto the final linking phase, there link.exe died with:\n(Lib target) ->\n  tf_core_kernels.dir\\Release\\tf_core_kernels.lib : fatal error LNK1248: image size (100039B2C) exceeds maximum allowable size (FFFFFFFF) \n\nI was lost here, I tried tinkering with some CMake build options, but all resulted into the same error. So, I gave up CMake.\nThen, I attempted to build with bazel 1.12.0.\nI was using the mingw64 shell, so first, I had to patch tensorflow/tools/pip_package/build_pip_package.sh . I also sent a PR #18953\nSurprisingly, the build went fine after that, and then I tried to run the tests. All were successful, except tensorflow/python/kernel_tests/boosted_trees:training_ops_test,  probably because it was not being run serially. Even for running the tests, I had to patch tensorflow/contrib/tensorboard/plugins/trace/trace.py and I also sent a PR #18954\nNow comes the real deal. The GPU build.\nAgain, I attempted the build with CMake. It took ~3hours and then link.exe failed with the same error as before. I spent quite some time reading about it and couldn't figure it out as most solutions said 'break your huge lib into smaller libs'. I am still baffled as to why the CI job at http://ci.tensorflow.org/job/tf-master-win-gpu-cmake/ doesn't die with the same problem. I am using the same compiler, same linker,\nand my machine is powerful enough. So, I just gave up again on CMake.\nGoing back to bazel now.  By this time, bazel 1.14.0 was was released. So I decided to use that instead. I had to apply the two patches as before, but life isn't so easy.\nNext big hurdle: #17067 .\n\nSo I got @dtrebbien 's patch from https://github.com/dtrebbien/protobuf/commit/50f552646ba1de79e07562b41f3999fe036b4fd0 and made changes to tensorflow/workspace.bzl to apply it to all protobuf checkouts.\n\nNext big hurdle: NCCL doesn't seem to be officially supported on Windows.\n\nBut bazel is still trying to build nccl related stuff. I had to patch stuff again, to disable all references to nccl in the BUILD files all over the place. \ud83d\ude3f\n\nNext big hurdle:\n\nShared libraries under tensorflow/contrib/rnn/python/ops/ failed at link phase with the error:\n\nCreating library bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.ifso and object bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.exp\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class perftools::gputools::DeviceMemory<float> const &,int,class perftools::gputools::DeviceMemory<float> const &,int,float,class perftools::gputools::DeviceMemory<float> *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11MAEBV?$DeviceMemory@M@23@H2HMPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm<float>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,float,float const *,int,float const *,int,float,float *,int)\" (??R?$TensorCuBlasGemm@M@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22MPEBMH3HMPEAMH@Z)\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,double,class perftools::gputools::DeviceMemory<double> const &,int,class perftools::gputools::DeviceMemory<double> const &,int,double,class perftools::gputools::DeviceMemory<double> *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11NAEBV?$DeviceMemory@N@23@H2HNPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm<double>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,double,double const *,int,double const *,int,double,double *,int)\" (??R?$TensorCuBlasGemm@N@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22NPEBNH3HNPEANH@Z)\n\n\nThen I stumbled upon #15013 and saw that another person had faced the same issue quite some time ago and the OP's solution was add all the .lib from tensorflow and cuda . I can't do that. So, I went looking for where this symbol comes from, and where should it be. Took me a while to realize what's happening and that's when I came across the workaround\nof using intermediate interface shared object file at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tensorflow.bzl#L1228-L1237. Apparently, this didn't have enough symbols for the kernels in tensorflow/contrib/rnn/python/ops/. So I started looking for where I could find them and ended up adding:\nclean_dep(\"//tensorflow/stream_executor:stream_executor_impl\"), to that list and the link error went away.\n\nNext hurdle: #20088\n\nThe compiler doesn't like\nhttps://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L46 when it was already being done at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu.h#L189 . So I commented out the typdef in fused_conv2d_bias_activation_op.cc and tried building it. The error went away, but link.exe cried again at the end, because it didn't know where to get the symbol for GetCudnnWorkspaceLimit at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L519 . After some searching, I found out that this symbol comes from https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/kernels/conv_ops.cc#L456-L471 . So I tried making it part of the intermediary additional_deps_impl.ifso import lib, but no matter what, it always ended up bloating it with symbols > 64k (it went upto ~72k, when I tried adding some deps which might have that symbol). So I gave up on this contrib kernel at this point and when I looked at the CMake build (I could be wrong here) I didn't find it building this kernel either. So I just ended up commenting out all references to fused_conv in the BUILD files.\n\nNext hurdle: Bazel had made some breaking changes in 1.13.0\n\nI came across at bazelbuild/bazel#4583 and then applied two other patches locally to get around that problem.\n\nNext hurdle: Tests won't run.\n\nMost of them want to import nccl. \ud83d\ude2d So I ended up faking it by creating a blank module and then the tests ran fine. I did have to tell bazel to ignore the contrib/lite subset -//${PY_TEST_DIR}/tensorflow/contrib/lite/....\n\nFor the lost souls who attempt at building tensorflow with GPU support, I hope this issue might be helpful. All the patches and the entire build can be found here: https://github.com/AnacondaRecipes/aggregate/blob/459dee0989e0c7cc4ab66c49d3d7605cddbb1bc3/tensorflow-gpu-base-feedstock/recipe/meta.yaml", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Well, I added patches\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, 64 bit\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  3.5, 3.6\r\n- **Bazel version (if compiling from source)**: 1.12.0, 1.14.0\r\n- **GCC/Compiler version (if compiling from source)**:  MSVC 2015 Update v3\r\n- **CUDA/cuDNN version**: 9.0/7.1\r\n- **GPU model and memory**: GeForce GT 430\r\n- **Exact command to reproduce**: ;-)\r\n\r\n### Description\r\nI wanted to compile tensorflow with GPU support on Windows and I went through the ordeal (I was finally able to build it, but I'll come to that later). I always followed the stuff done in: https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tools/ci_build/\r\n\r\nThis is how it went:\r\n\r\nFirst, I attempted the CPU build.\r\nInitial attempt was with CMake. It took ~3hours, all went fine upto the final linking phase, there link.exe died with:\r\n```\r\n(Lib target) ->\r\n  tf_core_kernels.dir\\Release\\tf_core_kernels.lib : fatal error LNK1248: image size (100039B2C) exceeds maximum allowable size (FFFFFFFF) \r\n```\r\nI was lost here, I tried tinkering with some CMake build options, but all resulted into the same error. So, I gave up CMake.\r\n\r\nThen, I attempted to build with bazel 1.12.0.\r\nI was using the mingw64 shell, so first, I had to patch `tensorflow/tools/pip_package/build_pip_package.sh` . I also sent a PR https://github.com/tensorflow/tensorflow/pull/18953 \r\n\r\nSurprisingly, the build went fine after that, and then I tried to run the tests. All were successful, except `tensorflow/python/kernel_tests/boosted_trees:training_ops_test`,  probably because it was not being run serially. Even for running the tests, I had to patch `tensorflow/contrib/tensorboard/plugins/trace/trace.py` and I also sent a PR https://github.com/tensorflow/tensorflow/pull/18954\r\n\r\nNow comes the real deal. The GPU build.\r\n\r\nAgain, I attempted the build with CMake. It took ~3hours and then link.exe failed with the same error as before. I spent quite some time reading about it and couldn't figure it out as most solutions said 'break your huge lib into smaller libs'. I am still baffled as to why the CI job at http://ci.tensorflow.org/job/tf-master-win-gpu-cmake/ doesn't die with the same problem. I am using the same compiler, same linker, \r\nand my machine is powerful enough. So, I just gave up again on CMake.\r\n\r\nGoing back to bazel now.  By this time, bazel 1.14.0 was was released. So I decided to use that instead. I had to apply the two patches as before, but life isn't so easy. \r\n\r\nNext big hurdle: https://github.com/tensorflow/tensorflow/issues/17067 .\r\n-  So I got @dtrebbien 's patch from https://github.com/dtrebbien/protobuf/commit/50f552646ba1de79e07562b41f3999fe036b4fd0 and made changes to `tensorflow/workspace.bzl` to apply it to all protobuf checkouts.\r\n\r\nNext big hurdle: NCCL doesn't seem to be officially supported on Windows. \r\n- But bazel is still trying to build nccl related stuff. I had to patch stuff again, to disable all references to nccl in the BUILD files all over the place. :crying_cat_face: \r\n\r\nNext big hurdle:\r\n-  Shared libraries under `tensorflow/contrib/rnn/python/ops/` failed at link phase with the error:\r\n```\r\nCreating library bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.ifso and object bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.exp\r\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class perftools::gputools::DeviceMemory<float> const &,int,class perftools::gputools::DeviceMemory<float> const &,int,float,class perftools::gputools::DeviceMemory<float> *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11MAEBV?$DeviceMemory@M@23@H2HMPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm<float>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,float,float const *,int,float const *,int,float,float *,int)\" (??R?$TensorCuBlasGemm@M@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22MPEBMH3HMPEAMH@Z)\r\nblas_gemm.o : error LNK2019: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,double,class perftools::gputools::DeviceMemory<double> const &,int,class perftools::gputools::DeviceMemory<double> const &,int,double,class perftools::gputools::DeviceMemory<double> *,int)\" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11NAEBV?$DeviceMemory@N@23@H2HNPEAV623@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm<double>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,double,double const *,int,double const *,int,double,double *,int)\" (??R?$TensorCuBlasGemm@N@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22NPEBNH3HNPEANH@Z)\r\n```\r\n- Then I stumbled upon https://github.com/tensorflow/tensorflow/issues/15013 and saw that another person had faced the same issue quite some time ago and the OP's solution was `add all the .lib from tensorflow and cuda` . I can't do that. So, I went looking for where this symbol comes from, and where should it be. Took me a while to realize what's happening and that's when I came across the workaround \r\nof using intermediate interface shared object file at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tensorflow.bzl#L1228-L1237. Apparently, this didn't have enough symbols for the kernels in `tensorflow/contrib/rnn/python/ops/`. So I started looking for where I could find them and ended up adding:\r\n`clean_dep(\"//tensorflow/stream_executor:stream_executor_impl\"),` to that list and the link error went away.\r\n\r\nNext hurdle: https://github.com/tensorflow/tensorflow/issues/20088\r\n-  The compiler doesn't like\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L46 when it was already being done at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu.h#L189 . So I commented out the typdef in `fused_conv2d_bias_activation_op.cc` and tried building it. The error went away, but link.exe cried again at the end, because it didn't know where to get the symbol for `GetCudnnWorkspaceLimit` at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L519 . After some searching, I found out that this symbol comes from https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/kernels/conv_ops.cc#L456-L471 . So I tried making it part of the intermediary additional_deps_impl.ifso import lib, but no matter what, it always ended up bloating it with symbols > 64k (it went upto ~72k, when I tried adding some deps which might have that symbol). So I gave up on this contrib kernel at this point and when I looked at the CMake build (I could be wrong here) I didn't find it building this kernel either. So I just ended up commenting out all references to `fused_conv` in the BUILD files.\r\n\r\nNext hurdle: Bazel had made some breaking changes in 1.13.0\r\n-  I came across at https://github.com/bazelbuild/bazel/issues/4583 and then applied two other patches locally to get around that problem.\r\n\r\nNext hurdle: Tests won't run.\r\n- Most of them want to import nccl. :sob: So I ended up faking it by creating a blank module and then the tests ran fine. I did have to tell bazel to ignore the `contrib/lite` subset `-//${PY_TEST_DIR}/tensorflow/contrib/lite/...`.\r\n\r\nFor the lost souls who attempt at building tensorflow with GPU support, I hope this issue might be helpful. All the patches and the entire build can be found here: https://github.com/AnacondaRecipes/aggregate/blob/459dee0989e0c7cc4ab66c49d3d7605cddbb1bc3/tensorflow-gpu-base-feedstock/recipe/meta.yaml"}