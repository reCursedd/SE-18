{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/288698464", "html_url": "https://github.com/tensorflow/tensorflow/issues/8466#issuecomment-288698464", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8466", "id": 288698464, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODY5ODQ2NA==", "user": {"login": "abdasgupta", "id": 15626057, "node_id": "MDQ6VXNlcjE1NjI2MDU3", "avatar_url": "https://avatars0.githubusercontent.com/u/15626057?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abdasgupta", "html_url": "https://github.com/abdasgupta", "followers_url": "https://api.github.com/users/abdasgupta/followers", "following_url": "https://api.github.com/users/abdasgupta/following{/other_user}", "gists_url": "https://api.github.com/users/abdasgupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/abdasgupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abdasgupta/subscriptions", "organizations_url": "https://api.github.com/users/abdasgupta/orgs", "repos_url": "https://api.github.com/users/abdasgupta/repos", "events_url": "https://api.github.com/users/abdasgupta/events{/privacy}", "received_events_url": "https://api.github.com/users/abdasgupta/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-23T12:07:14Z", "updated_at": "2017-03-23T12:07:14Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=88808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skye\">@skye</a>  and  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19711421\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/antoajayraj\">@antoajayraj</a> ,</p>\n<p>First let me describe my setup - I am running tensorflow training on inception v3 model to fine tune for Flowers data set(see <a href=\"https://github.com/tensorflow/models/tree/master/inception\">https://github.com/tensorflow/models/tree/master/inception</a>), through docker in a small kubernetes cluster made of power 8 machines. I have 4GPUs at my disposal.<br>\nMy container and host OS are Ubuntu 16.04.<br>\nHost kernel is : 4.4.0-64-generic<br>\nGLibc version: Ubuntu GLIBC 2.23-0ubuntu7</p>\n<p>I was hitting the above mentioned issue with tensorflow version 1.0.0 only when I ran 1000 iterations of training or above. The above error was appearing approx after 600-900 iterations.</p>\n<p>After I upgraded to version 1.0.1, I am seeing no such error within 10000 iterations.<br>\nBut if I run 100000 iterations, the training is getting stuck at around 50k and stack trace is showing that it is executing some waitpid(-1..) and stuck there.</p>\n<p>Command to run the training in docker container:<br>\n<code>docker run -it --privileged -v /usr/local/cuda-8.0/:/usr/local/cuda-8.0/ -v /usr/lib/powerpc64le-linux-gnu/:/usr/lib/powerpc64le-linux-gnu/ -v /usr/lib/nvidia-375/:/usr/lib/nvidia-375/  tf-inception-trainer-flowers-v101 /bin/bash -c \"./run-trainer.sh 100000\"</code></p>\n<p>Image name : tf-inception-trainer-flowers-v101<br>\nThis is the same image I am using for Kubernetes. This is a locally built image with tensorflow 1.0.1 installed.</p>\n<p>Content of run-trainer.sh:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#!</span>/bin/bash</span>\n\n<span class=\"pl-k\">if</span> [ <span class=\"pl-smi\">$1</span> ]<span class=\"pl-k\">;</span>  <span class=\"pl-k\">then</span>\n  ITERATIONS=<span class=\"pl-smi\">$1</span>\n<span class=\"pl-k\">else</span>\n  <span class=\"pl-c1\">echo</span> <span class=\"pl-smi\">$0</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>: usage: ./run-traner.sh [iterations]<span class=\"pl-pds\">\"</span></span>\n  <span class=\"pl-c1\">exit</span> 1\n<span class=\"pl-k\">fi</span>\n\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz <span class=\"pl-k\">&amp;&amp;</span> \\\ntar xzf inception-v3-2016-03-01.tar.gz <span class=\"pl-k\">&amp;&amp;</span> \\\nls inception-v3\n\nmkdir /flowers-data\n\n<span class=\"pl-c1\">cd</span> /models/inception <span class=\"pl-k\">&amp;&amp;</span> \\\n<span class=\"pl-k\">export</span> LD_LIBRARY_PATH=/usr/lib/powerpc64le-linux-gnu/ <span class=\"pl-k\">&amp;&amp;</span> \\\n<span class=\"pl-k\">export</span> LD_LIBRARY_PATH=<span class=\"pl-smi\">$LD_LIBRARY_PATH</span>:/usr/local/cuda-8.0/lib64/ <span class=\"pl-k\">&amp;&amp;</span> \\\n<span class=\"pl-k\">export</span> LD_LIBRARY_PATH=<span class=\"pl-smi\">$LD_LIBRARY_PATH</span>:/usr/lib/nvidia-375/ <span class=\"pl-k\">&amp;&amp;</span> \\\nbazel build inception/download_and_preprocess_flowers <span class=\"pl-k\">&amp;&amp;</span> \\\nbazel-bin/inception/download_and_preprocess_flowers /flowers-data <span class=\"pl-k\">&amp;&amp;</span> \\\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Build the model. Note that we need to make sure the TensorFlow is ready to</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> use before this as this command will not build TensorFlow.</span>\nbazel build inception/flowers_train\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Path to the downloaded Inception-v3 model.</span>\nMODEL_PATH=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/inception-v3/model.ckpt-157585<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Directory where the flowers data resides.</span>\nFLOWERS_DATA_DIR=/flowers-data/\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Directory where to save the checkpoint and events files.</span>\nTRAIN_DIR=/flowers_train/\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the fine-tuning on the flowers data set starting from the pre-trained</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Imagenet-v3 model.</span>\nbazel-bin/inception/flowers_train \\\n  --train_dir=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${TRAIN_DIR}</span><span class=\"pl-pds\">\"</span></span> \\\n  --data_dir=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${FLOWERS_DATA_DIR}</span><span class=\"pl-pds\">\"</span></span> \\\n  --pretrained_model_checkpoint_path=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${MODEL_PATH}</span><span class=\"pl-pds\">\"</span></span> \\\n  --fine_tune=True \\\n  --initial_learning_rate=0.001 \\\n  --max_steps=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${ITERATIONS}</span><span class=\"pl-pds\">\"</span></span> \\\n  --input_queue_memory_factor=1</pre></div>\n<p>Is the behaviour expected? Am I doing something wrong? Please, guide me. Though my current purpose is served as I can run the training now till 5000 iterations through docker. It gives me about 99.8% of accuracy. And accuracy is same while the iterations go up to 50k. But the accuracy is hurt while I lower the iterations to , say 2000 (Only 93.4%)</p>\n<p>I will update my findings after I run the training on P8 bare-metal till 100000 iterations.</p>\n<p>Thank you.</p>", "body_text": "Hi @skye  and  @antoajayraj ,\nFirst let me describe my setup - I am running tensorflow training on inception v3 model to fine tune for Flowers data set(see https://github.com/tensorflow/models/tree/master/inception), through docker in a small kubernetes cluster made of power 8 machines. I have 4GPUs at my disposal.\nMy container and host OS are Ubuntu 16.04.\nHost kernel is : 4.4.0-64-generic\nGLibc version: Ubuntu GLIBC 2.23-0ubuntu7\nI was hitting the above mentioned issue with tensorflow version 1.0.0 only when I ran 1000 iterations of training or above. The above error was appearing approx after 600-900 iterations.\nAfter I upgraded to version 1.0.1, I am seeing no such error within 10000 iterations.\nBut if I run 100000 iterations, the training is getting stuck at around 50k and stack trace is showing that it is executing some waitpid(-1..) and stuck there.\nCommand to run the training in docker container:\ndocker run -it --privileged -v /usr/local/cuda-8.0/:/usr/local/cuda-8.0/ -v /usr/lib/powerpc64le-linux-gnu/:/usr/lib/powerpc64le-linux-gnu/ -v /usr/lib/nvidia-375/:/usr/lib/nvidia-375/  tf-inception-trainer-flowers-v101 /bin/bash -c \"./run-trainer.sh 100000\"\nImage name : tf-inception-trainer-flowers-v101\nThis is the same image I am using for Kubernetes. This is a locally built image with tensorflow 1.0.1 installed.\nContent of run-trainer.sh:\n#!/bin/bash\n\nif [ $1 ];  then\n  ITERATIONS=$1\nelse\n  echo $0\": usage: ./run-traner.sh [iterations]\"\n  exit 1\nfi\n\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz && \\\ntar xzf inception-v3-2016-03-01.tar.gz && \\\nls inception-v3\n\nmkdir /flowers-data\n\ncd /models/inception && \\\nexport LD_LIBRARY_PATH=/usr/lib/powerpc64le-linux-gnu/ && \\\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64/ && \\\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia-375/ && \\\nbazel build inception/download_and_preprocess_flowers && \\\nbazel-bin/inception/download_and_preprocess_flowers /flowers-data && \\\n\n# Build the model. Note that we need to make sure the TensorFlow is ready to\n# use before this as this command will not build TensorFlow.\nbazel build inception/flowers_train\n\n# Path to the downloaded Inception-v3 model.\nMODEL_PATH=\"/inception-v3/model.ckpt-157585\"\n\n# Directory where the flowers data resides.\nFLOWERS_DATA_DIR=/flowers-data/\n\n# Directory where to save the checkpoint and events files.\nTRAIN_DIR=/flowers_train/\n\n# Run the fine-tuning on the flowers data set starting from the pre-trained\n# Imagenet-v3 model.\nbazel-bin/inception/flowers_train \\\n  --train_dir=\"${TRAIN_DIR}\" \\\n  --data_dir=\"${FLOWERS_DATA_DIR}\" \\\n  --pretrained_model_checkpoint_path=\"${MODEL_PATH}\" \\\n  --fine_tune=True \\\n  --initial_learning_rate=0.001 \\\n  --max_steps=\"${ITERATIONS}\" \\\n  --input_queue_memory_factor=1\nIs the behaviour expected? Am I doing something wrong? Please, guide me. Though my current purpose is served as I can run the training now till 5000 iterations through docker. It gives me about 99.8% of accuracy. And accuracy is same while the iterations go up to 50k. But the accuracy is hurt while I lower the iterations to , say 2000 (Only 93.4%)\nI will update my findings after I run the training on P8 bare-metal till 100000 iterations.\nThank you.", "body": "Hi @skye  and  @antoajayraj ,\r\n\r\nFirst let me describe my setup - I am running tensorflow training on inception v3 model to fine tune for Flowers data set(see https://github.com/tensorflow/models/tree/master/inception), through docker in a small kubernetes cluster made of power 8 machines. I have 4GPUs at my disposal.\r\nMy container and host OS are Ubuntu 16.04.\r\nHost kernel is : 4.4.0-64-generic\r\nGLibc version: Ubuntu GLIBC 2.23-0ubuntu7\r\n\r\nI was hitting the above mentioned issue with tensorflow version 1.0.0 only when I ran 1000 iterations of training or above. The above error was appearing approx after 600-900 iterations.\r\n\r\nAfter I upgraded to version 1.0.1, I am seeing no such error within 10000 iterations.\r\nBut if I run 100000 iterations, the training is getting stuck at around 50k and stack trace is showing that it is executing some waitpid(-1..) and stuck there. \r\n\r\nCommand to run the training in docker container:\r\n`docker run -it --privileged -v /usr/local/cuda-8.0/:/usr/local/cuda-8.0/ -v /usr/lib/powerpc64le-linux-gnu/:/usr/lib/powerpc64le-linux-gnu/ -v /usr/lib/nvidia-375/:/usr/lib/nvidia-375/  tf-inception-trainer-flowers-v101 /bin/bash -c \"./run-trainer.sh 100000\"`\r\n\r\nImage name : tf-inception-trainer-flowers-v101 \r\nThis is the same image I am using for Kubernetes. This is a locally built image with tensorflow 1.0.1 installed.\r\n\r\nContent of run-trainer.sh:\r\n```bash\r\n#!/bin/bash\r\n\r\nif [ $1 ];  then\r\n  ITERATIONS=$1\r\nelse\r\n  echo $0\": usage: ./run-traner.sh [iterations]\"\r\n  exit 1\r\nfi\r\n\r\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz && \\\r\ntar xzf inception-v3-2016-03-01.tar.gz && \\\r\nls inception-v3\r\n\r\nmkdir /flowers-data\r\n\r\ncd /models/inception && \\\r\nexport LD_LIBRARY_PATH=/usr/lib/powerpc64le-linux-gnu/ && \\\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64/ && \\\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia-375/ && \\\r\nbazel build inception/download_and_preprocess_flowers && \\\r\nbazel-bin/inception/download_and_preprocess_flowers /flowers-data && \\\r\n\r\n# Build the model. Note that we need to make sure the TensorFlow is ready to\r\n# use before this as this command will not build TensorFlow.\r\nbazel build inception/flowers_train\r\n\r\n# Path to the downloaded Inception-v3 model.\r\nMODEL_PATH=\"/inception-v3/model.ckpt-157585\"\r\n\r\n# Directory where the flowers data resides.\r\nFLOWERS_DATA_DIR=/flowers-data/\r\n\r\n# Directory where to save the checkpoint and events files.\r\nTRAIN_DIR=/flowers_train/\r\n\r\n# Run the fine-tuning on the flowers data set starting from the pre-trained\r\n# Imagenet-v3 model.\r\nbazel-bin/inception/flowers_train \\\r\n  --train_dir=\"${TRAIN_DIR}\" \\\r\n  --data_dir=\"${FLOWERS_DATA_DIR}\" \\\r\n  --pretrained_model_checkpoint_path=\"${MODEL_PATH}\" \\\r\n  --fine_tune=True \\\r\n  --initial_learning_rate=0.001 \\\r\n  --max_steps=\"${ITERATIONS}\" \\\r\n  --input_queue_memory_factor=1\r\n```\r\nIs the behaviour expected? Am I doing something wrong? Please, guide me. Though my current purpose is served as I can run the training now till 5000 iterations through docker. It gives me about 99.8% of accuracy. And accuracy is same while the iterations go up to 50k. But the accuracy is hurt while I lower the iterations to , say 2000 (Only 93.4%)\r\n\r\nI will update my findings after I run the training on P8 bare-metal till 100000 iterations.\r\n\r\nThank you."}