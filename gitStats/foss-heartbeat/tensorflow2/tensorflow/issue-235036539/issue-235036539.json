{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10618", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10618/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10618/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10618/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10618", "id": 235036539, "node_id": "MDU6SXNzdWUyMzUwMzY1Mzk=", "number": 10618, "title": "crop_and_resize issue with box_index on 1.2rc2", "user": {"login": "waleedka", "id": 106472, "node_id": "MDQ6VXNlcjEwNjQ3Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/106472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/waleedka", "html_url": "https://github.com/waleedka", "followers_url": "https://api.github.com/users/waleedka/followers", "following_url": "https://api.github.com/users/waleedka/following{/other_user}", "gists_url": "https://api.github.com/users/waleedka/gists{/gist_id}", "starred_url": "https://api.github.com/users/waleedka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/waleedka/subscriptions", "organizations_url": "https://api.github.com/users/waleedka/orgs", "repos_url": "https://api.github.com/users/waleedka/repos", "events_url": "https://api.github.com/users/waleedka/events{/privacy}", "received_events_url": "https://api.github.com/users/waleedka/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "av8ramit", "id": 5588272, "node_id": "MDQ6VXNlcjU1ODgyNzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/5588272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/av8ramit", "html_url": "https://github.com/av8ramit", "followers_url": "https://api.github.com/users/av8ramit/followers", "following_url": "https://api.github.com/users/av8ramit/following{/other_user}", "gists_url": "https://api.github.com/users/av8ramit/gists{/gist_id}", "starred_url": "https://api.github.com/users/av8ramit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/av8ramit/subscriptions", "organizations_url": "https://api.github.com/users/av8ramit/orgs", "repos_url": "https://api.github.com/users/av8ramit/repos", "events_url": "https://api.github.com/users/av8ramit/events{/privacy}", "received_events_url": "https://api.github.com/users/av8ramit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "av8ramit", "id": 5588272, "node_id": "MDQ6VXNlcjU1ODgyNzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/5588272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/av8ramit", "html_url": "https://github.com/av8ramit", "followers_url": "https://api.github.com/users/av8ramit/followers", "following_url": "https://api.github.com/users/av8ramit/following{/other_user}", "gists_url": "https://api.github.com/users/av8ramit/gists{/gist_id}", "starred_url": "https://api.github.com/users/av8ramit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/av8ramit/subscriptions", "organizations_url": "https://api.github.com/users/av8ramit/orgs", "repos_url": "https://api.github.com/users/av8ramit/repos", "events_url": "https://api.github.com/users/av8ramit/events{/privacy}", "received_events_url": "https://api.github.com/users/av8ramit/received_events", "type": "User", "site_admin": false}, {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2017-06-11T01:22:04Z", "updated_at": "2018-02-24T09:38:46Z", "closed_at": "2017-09-03T18:50:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I use crop_and_resize() in a standard way as such:</p>\n<pre><code>pool = tf.image.crop_and_resize(features, boxes, box_indicies, pool_shape, method=\"bilinear\")\n</code></pre>\n<p>And it works great. Then I tried to train on multiple GPUs, and I got this error:</p>\n<pre><code>OutOfRangeError: box_index has values outside [0, batch_size)\n\t [[Node: tower_2/mask_rcnn/roi_align/CropAndResize = CropAndResize[T=DT_FLOAT, extrapolation_value=0, method=\"bilinear\", _device=\"/job:localhost/replica:0/task:0/gpu:2\"](tower_2/mask_rcnn/activation_40/Relu, tower_2/mask_rcnn/roi_align/StopGradient, tower_2/mask_rcnn/roi_align/StopGradient_1/_5871, tower_2/mask_rcnn/roi_align/CropAndResize/crop_size)]]\n\t [[Node: tower_5/mask_rcnn/mrcnn_mask/Reshape_1/_6389 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:5\", send_device_incarnation=1, tensor_name=\"edge_33119_tower_5/mask_rcnn/mrcnn_mask/Reshape_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<p>I verified that box_index is within the correct range (all zeros in my case because I use one image per batch), and verified that all the other inputs to crop_and_resize() look good. I ended up spending almost two days checking and rechecking every line of my code to make sure I didn't make a mistake somewhere. It drove me nuts. Finally, I tried downgrading TF to 1.1 and suddenly everything worked. And to make sure this is the problem, I upgraded to 1.2rc2 again and got the error again.</p>\n<h3>A few points that might help:</h3>\n<ul>\n<li>On 1.1 it works whether I use 1 GPU or 8 GPUs. On 1.2rc2 it works on 1 GPU but fails on 8GPUs.</li>\n<li>The way I do multi-GPU training is by running 8 copies of my model, one on each GPU (shared weights). My input batch size is 8 and I split the inputs by the batch dimension and feed one sample to each GPU, and then I concatenate the outputs to get a batch size of 8 again and then apply the loss function.</li>\n<li>If you're wondering why I'm using 1.2rc2, it's because 1.1 has an issue that causes Batch normalization to run on CPU and it was fixed in 1.2rc2.</li>\n</ul>\n<h3>System information</h3>\n<ul>\n<li>Ubuntu 16.04</li>\n<li>Python 3.5</li>\n<li>TensorFlow 1.2.0-rc2. Installed using</li>\n</ul>\n<pre><code>sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc2-cp35-cp35m-linux_x86_64.whl\n</code></pre>\n<ul>\n<li>On EC2 p2.8xlarge (8 GPUs)</li>\n</ul>", "body_text": "I use crop_and_resize() in a standard way as such:\npool = tf.image.crop_and_resize(features, boxes, box_indicies, pool_shape, method=\"bilinear\")\n\nAnd it works great. Then I tried to train on multiple GPUs, and I got this error:\nOutOfRangeError: box_index has values outside [0, batch_size)\n\t [[Node: tower_2/mask_rcnn/roi_align/CropAndResize = CropAndResize[T=DT_FLOAT, extrapolation_value=0, method=\"bilinear\", _device=\"/job:localhost/replica:0/task:0/gpu:2\"](tower_2/mask_rcnn/activation_40/Relu, tower_2/mask_rcnn/roi_align/StopGradient, tower_2/mask_rcnn/roi_align/StopGradient_1/_5871, tower_2/mask_rcnn/roi_align/CropAndResize/crop_size)]]\n\t [[Node: tower_5/mask_rcnn/mrcnn_mask/Reshape_1/_6389 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:5\", send_device_incarnation=1, tensor_name=\"edge_33119_tower_5/mask_rcnn/mrcnn_mask/Reshape_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nI verified that box_index is within the correct range (all zeros in my case because I use one image per batch), and verified that all the other inputs to crop_and_resize() look good. I ended up spending almost two days checking and rechecking every line of my code to make sure I didn't make a mistake somewhere. It drove me nuts. Finally, I tried downgrading TF to 1.1 and suddenly everything worked. And to make sure this is the problem, I upgraded to 1.2rc2 again and got the error again.\nA few points that might help:\n\nOn 1.1 it works whether I use 1 GPU or 8 GPUs. On 1.2rc2 it works on 1 GPU but fails on 8GPUs.\nThe way I do multi-GPU training is by running 8 copies of my model, one on each GPU (shared weights). My input batch size is 8 and I split the inputs by the batch dimension and feed one sample to each GPU, and then I concatenate the outputs to get a batch size of 8 again and then apply the loss function.\nIf you're wondering why I'm using 1.2rc2, it's because 1.1 has an issue that causes Batch normalization to run on CPU and it was fixed in 1.2rc2.\n\nSystem information\n\nUbuntu 16.04\nPython 3.5\nTensorFlow 1.2.0-rc2. Installed using\n\nsudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc2-cp35-cp35m-linux_x86_64.whl\n\n\nOn EC2 p2.8xlarge (8 GPUs)", "body": "I use crop_and_resize() in a standard way as such:\r\n```\r\npool = tf.image.crop_and_resize(features, boxes, box_indicies, pool_shape, method=\"bilinear\")\r\n```\r\n\r\nAnd it works great. Then I tried to train on multiple GPUs, and I got this error:\r\n```\r\nOutOfRangeError: box_index has values outside [0, batch_size)\r\n\t [[Node: tower_2/mask_rcnn/roi_align/CropAndResize = CropAndResize[T=DT_FLOAT, extrapolation_value=0, method=\"bilinear\", _device=\"/job:localhost/replica:0/task:0/gpu:2\"](tower_2/mask_rcnn/activation_40/Relu, tower_2/mask_rcnn/roi_align/StopGradient, tower_2/mask_rcnn/roi_align/StopGradient_1/_5871, tower_2/mask_rcnn/roi_align/CropAndResize/crop_size)]]\r\n\t [[Node: tower_5/mask_rcnn/mrcnn_mask/Reshape_1/_6389 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:5\", send_device_incarnation=1, tensor_name=\"edge_33119_tower_5/mask_rcnn/mrcnn_mask/Reshape_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\nI verified that box_index is within the correct range (all zeros in my case because I use one image per batch), and verified that all the other inputs to crop_and_resize() look good. I ended up spending almost two days checking and rechecking every line of my code to make sure I didn't make a mistake somewhere. It drove me nuts. Finally, I tried downgrading TF to 1.1 and suddenly everything worked. And to make sure this is the problem, I upgraded to 1.2rc2 again and got the error again. \r\n\r\n### A few points that might help:\r\n- On 1.1 it works whether I use 1 GPU or 8 GPUs. On 1.2rc2 it works on 1 GPU but fails on 8GPUs.\r\n- The way I do multi-GPU training is by running 8 copies of my model, one on each GPU (shared weights). My input batch size is 8 and I split the inputs by the batch dimension and feed one sample to each GPU, and then I concatenate the outputs to get a batch size of 8 again and then apply the loss function.\r\n- If you're wondering why I'm using 1.2rc2, it's because 1.1 has an issue that causes Batch normalization to run on CPU and it was fixed in 1.2rc2.\r\n\r\n### System information\r\n- Ubuntu 16.04\r\n- Python 3.5\r\n- TensorFlow 1.2.0-rc2. Installed using \r\n```\r\nsudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc2-cp35-cp35m-linux_x86_64.whl\r\n```\r\n- On EC2 p2.8xlarge (8 GPUs)\r\n"}