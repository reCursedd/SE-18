{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4314", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4314/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4314/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4314/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4314", "id": 176160066, "node_id": "MDU6SXNzdWUxNzYxNjAwNjY=", "number": 4314, "title": "Add float16 support for Elu, EluGrad ops", "user": {"login": "Fenugreek", "id": 3323801, "node_id": "MDQ6VXNlcjMzMjM4MDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3323801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Fenugreek", "html_url": "https://github.com/Fenugreek", "followers_url": "https://api.github.com/users/Fenugreek/followers", "following_url": "https://api.github.com/users/Fenugreek/following{/other_user}", "gists_url": "https://api.github.com/users/Fenugreek/gists{/gist_id}", "starred_url": "https://api.github.com/users/Fenugreek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Fenugreek/subscriptions", "organizations_url": "https://api.github.com/users/Fenugreek/orgs", "repos_url": "https://api.github.com/users/Fenugreek/repos", "events_url": "https://api.github.com/users/Fenugreek/events{/privacy}", "received_events_url": "https://api.github.com/users/Fenugreek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-09-10T05:40:04Z", "updated_at": "2016-09-14T02:13:55Z", "closed_at": "2016-09-14T02:13:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Support for <code>tf.float16</code> dtype was recently added (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"136722135\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1300\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1300/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1300\">#1300</a>) to a bunch of ops, starting with matmul, conv2D and then many more. Can we add it for elu also please?</p>\n<p>Looking at <code>tensorflow/nn_ops.cc</code>, I see that the <code>Elu</code> and <code>EluGrad</code> ops are registered for types <code>{float, double}</code> whereas for a whole bunch of other activation functions I see the type as <code>T: realnumbertype</code> or <code>{half, float, double}</code>.</p>\n<p>To help with triaging, here's the paper that motivates elu activation: <a href=\"http://arxiv.org/abs/1511.07289\" rel=\"nofollow\">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</a></p>\n<p>And FWIW <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15474222\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alexatknit\">@alexatknit</a> in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"136722135\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1300\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1300/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1300\">#1300</a> commented Re: looking forward to float16 support for elu too.</p>", "body_text": "Support for tf.float16 dtype was recently added (#1300) to a bunch of ops, starting with matmul, conv2D and then many more. Can we add it for elu also please?\nLooking at tensorflow/nn_ops.cc, I see that the Elu and EluGrad ops are registered for types {float, double} whereas for a whole bunch of other activation functions I see the type as T: realnumbertype or {half, float, double}.\nTo help with triaging, here's the paper that motivates elu activation: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\nAnd FWIW @alexatknit in #1300 commented Re: looking forward to float16 support for elu too.", "body": "Support for `tf.float16` dtype was recently added (#1300) to a bunch of ops, starting with matmul, conv2D and then many more. Can we add it for elu also please? \n\nLooking at `tensorflow/nn_ops.cc`, I see that the `Elu` and `EluGrad` ops are registered for types `{float, double}` whereas for a whole bunch of other activation functions I see the type as `T: realnumbertype` or `{half, float, double}`.\n\nTo help with triaging, here's the paper that motivates elu activation: [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](http://arxiv.org/abs/1511.07289)\n\nAnd FWIW @alexatknit in #1300 commented Re: looking forward to float16 support for elu too.\n"}