{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12413", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12413/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12413/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12413/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12413", "id": 251425417, "node_id": "MDU6SXNzdWUyNTE0MjU0MTc=", "number": 12413, "title": "FIFOQueue '_1_input_producer' is closed.", "user": {"login": "AFAgarap", "id": 11130276, "node_id": "MDQ6VXNlcjExMTMwMjc2", "avatar_url": "https://avatars1.githubusercontent.com/u/11130276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AFAgarap", "html_url": "https://github.com/AFAgarap", "followers_url": "https://api.github.com/users/AFAgarap/followers", "following_url": "https://api.github.com/users/AFAgarap/following{/other_user}", "gists_url": "https://api.github.com/users/AFAgarap/gists{/gist_id}", "starred_url": "https://api.github.com/users/AFAgarap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AFAgarap/subscriptions", "organizations_url": "https://api.github.com/users/AFAgarap/orgs", "repos_url": "https://api.github.com/users/AFAgarap/repos", "events_url": "https://api.github.com/users/AFAgarap/events{/privacy}", "received_events_url": "https://api.github.com/users/AFAgarap/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-08-19T13:31:55Z", "updated_at": "2017-08-22T13:23:10Z", "closed_at": "2017-08-22T13:23:10Z", "author_association": "NONE", "body_html": "<p>I'm using an <code>input_pipeline</code> in a <code>tf.managed_session()</code>, and encountered the following error message:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 206, in &lt;module&gt;\n    main()\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 193, in main\n    coord.join(threads)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/lib/python3/dist-packages/six.py\", line 686, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 238, in _run\n    enqueue_callable()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1063, in _single_operation_run\n    target_list_as_strings, status, None)\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.CancelledError: FIFOQueue '_1_input_producer' is closed.\n\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/limit_epochs)]]\n</code></pre>\n<p>Here's the session block I wrote:</p>\n<pre><code>sv = tf.train.Supervisor(logdir=CHECKPOINT_PATH, summary_op=None)\n\n    with sv.managed_session() as sess:\n\n        sess.run(init_op)\n\n        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\n\n        if checkpoint and checkpoint.model_checkpoint_path:\n            saver.restore(sess, checkpoint.model_checkpoint_path)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        try:\n            step = 0\n            while not coord.should_stop():\n                example_batch, label_batch = sess.run([examples, labels])\n\n                feed_dict = {x_input: example_batch, y_input: label_batch, state: current_state,\n                             learning_rate: LEARNING_RATE, p_keep: DROPOUT_P_KEEP}\n\n                summary, _, epoch_loss, next_state = sess.run([merged, optimizer, cost, states], feed_dict=feed_dict)\n\n                accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\n\n                current_state = next_state\n\n                if step % 100 == 0:\n                    print('step [{}] loss : {}, accuracy : {}'.format(step, epoch_loss, accuracy_))\n                    writer.add_summary(summary, step)\n                    saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\n\n                step += 1\n        except tf.errors.OutOfRangeError:\n            print('EOF -- training done at step {}'.format(step))\n        finally:\n            writer.close()\n            coord.request_stop()\n\n        coord.join(threads)\n\n        saver = tf.train.Saver()\n        saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\n</code></pre>\n<p>Thank you in advance for your help!</p>", "body_text": "I'm using an input_pipeline in a tf.managed_session(), and encountered the following error message:\nTraceback (most recent call last):\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 206, in <module>\n    main()\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 193, in main\n    coord.join(threads)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/lib/python3/dist-packages/six.py\", line 686, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 238, in _run\n    enqueue_callable()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1063, in _single_operation_run\n    target_list_as_strings, status, None)\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.CancelledError: FIFOQueue '_1_input_producer' is closed.\n\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/limit_epochs)]]\n\nHere's the session block I wrote:\nsv = tf.train.Supervisor(logdir=CHECKPOINT_PATH, summary_op=None)\n\n    with sv.managed_session() as sess:\n\n        sess.run(init_op)\n\n        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\n\n        if checkpoint and checkpoint.model_checkpoint_path:\n            saver.restore(sess, checkpoint.model_checkpoint_path)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        try:\n            step = 0\n            while not coord.should_stop():\n                example_batch, label_batch = sess.run([examples, labels])\n\n                feed_dict = {x_input: example_batch, y_input: label_batch, state: current_state,\n                             learning_rate: LEARNING_RATE, p_keep: DROPOUT_P_KEEP}\n\n                summary, _, epoch_loss, next_state = sess.run([merged, optimizer, cost, states], feed_dict=feed_dict)\n\n                accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\n\n                current_state = next_state\n\n                if step % 100 == 0:\n                    print('step [{}] loss : {}, accuracy : {}'.format(step, epoch_loss, accuracy_))\n                    writer.add_summary(summary, step)\n                    saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\n\n                step += 1\n        except tf.errors.OutOfRangeError:\n            print('EOF -- training done at step {}'.format(step))\n        finally:\n            writer.close()\n            coord.request_stop()\n\n        coord.join(threads)\n\n        saver = tf.train.Saver()\n        saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\n\nThank you in advance for your help!", "body": "I'm using an `input_pipeline` in a `tf.managed_session()`, and encountered the following error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 206, in <module>\r\n    main()\r\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 193, in main\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/lib/python3/dist-packages/six.py\", line 686, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 238, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1063, in _single_operation_run\r\n    target_list_as_strings, status, None)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.CancelledError: FIFOQueue '_1_input_producer' is closed.\r\n\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/limit_epochs)]]\r\n```\r\n\r\nHere's the session block I wrote:\r\n\r\n```\r\nsv = tf.train.Supervisor(logdir=CHECKPOINT_PATH, summary_op=None)\r\n\r\n    with sv.managed_session() as sess:\r\n\r\n        sess.run(init_op)\r\n\r\n        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\r\n\r\n        if checkpoint and checkpoint.model_checkpoint_path:\r\n            saver.restore(sess, checkpoint.model_checkpoint_path)\r\n\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n        try:\r\n            step = 0\r\n            while not coord.should_stop():\r\n                example_batch, label_batch = sess.run([examples, labels])\r\n\r\n                feed_dict = {x_input: example_batch, y_input: label_batch, state: current_state,\r\n                             learning_rate: LEARNING_RATE, p_keep: DROPOUT_P_KEEP}\r\n\r\n                summary, _, epoch_loss, next_state = sess.run([merged, optimizer, cost, states], feed_dict=feed_dict)\r\n\r\n                accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\r\n\r\n                current_state = next_state\r\n\r\n                if step % 100 == 0:\r\n                    print('step [{}] loss : {}, accuracy : {}'.format(step, epoch_loss, accuracy_))\r\n                    writer.add_summary(summary, step)\r\n                    saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\r\n\r\n                step += 1\r\n        except tf.errors.OutOfRangeError:\r\n            print('EOF -- training done at step {}'.format(step))\r\n        finally:\r\n            writer.close()\r\n            coord.request_stop()\r\n\r\n        coord.join(threads)\r\n\r\n        saver = tf.train.Saver()\r\n        saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\r\n```\r\n\r\nThank you in advance for your help!\r\n"}