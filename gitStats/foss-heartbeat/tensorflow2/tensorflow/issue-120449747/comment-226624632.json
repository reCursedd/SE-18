{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226624632", "html_url": "https://github.com/tensorflow/tensorflow/issues/412#issuecomment-226624632", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/412", "id": 226624632, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjYyNDYzMg==", "user": {"login": "asaydin", "id": 5859763, "node_id": "MDQ6VXNlcjU4NTk3NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5859763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asaydin", "html_url": "https://github.com/asaydin", "followers_url": "https://api.github.com/users/asaydin/followers", "following_url": "https://api.github.com/users/asaydin/following{/other_user}", "gists_url": "https://api.github.com/users/asaydin/gists{/gist_id}", "starred_url": "https://api.github.com/users/asaydin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asaydin/subscriptions", "organizations_url": "https://api.github.com/users/asaydin/orgs", "repos_url": "https://api.github.com/users/asaydin/repos", "events_url": "https://api.github.com/users/asaydin/events{/privacy}", "received_events_url": "https://api.github.com/users/asaydin/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-16T21:51:54Z", "updated_at": "2016-06-16T21:51:54Z", "author_association": "NONE", "body_html": "<p>I think I am having a similar problem to this. Gradients are apparently computed, but the weight matrix associated with the gradient does not change (i.e., they are exactly the same across mini-batches and iterations). Interestingly, this happens only with one dataset. I use the same code for two other (MNIST and an artificially generated dataset) datasets, and it works fine.</p>\n<pre><code>optimizer = tf.train.GradientDescentOptimizer(0.1)\npredict_op = tf.argmax(py_x,1)\n\ntvars = tf.trainable_variables()\ngrads = tf.gradients(cost, tvars)\ntrain_op = optimizer.apply_gradients(zip(grads, tvars))\n</code></pre>\n<p>So this is my optimization code. I used this explicit way of computing to see the values of the gradients. Otherwise I used</p>\n<p><code>train_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)</code></p>\n<p>and other optimization techniques TensorFlow offers as well. As I said, although gradients are computed(And they are non-zero), they are not applied to the weight matrix, and the weight matrix remains the same. As a result, because of the random initialization of the weights, the classifier I trained (Which is an LSTM + a softmax layer) keeps predicting with random guess (i.e., 1/n accuracy, where n is the number of classes).</p>\n<p>Not sure if this is the right place to post, but I guess the problem is similar. I can open a new issue if you want. The code is very long (and messy because of everything I tried to make it work in the last few days), so I did not post here the whole thing. If you need the other parts, I can clean them and post here.</p>", "body_text": "I think I am having a similar problem to this. Gradients are apparently computed, but the weight matrix associated with the gradient does not change (i.e., they are exactly the same across mini-batches and iterations). Interestingly, this happens only with one dataset. I use the same code for two other (MNIST and an artificially generated dataset) datasets, and it works fine.\noptimizer = tf.train.GradientDescentOptimizer(0.1)\npredict_op = tf.argmax(py_x,1)\n\ntvars = tf.trainable_variables()\ngrads = tf.gradients(cost, tvars)\ntrain_op = optimizer.apply_gradients(zip(grads, tvars))\n\nSo this is my optimization code. I used this explicit way of computing to see the values of the gradients. Otherwise I used\ntrain_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\nand other optimization techniques TensorFlow offers as well. As I said, although gradients are computed(And they are non-zero), they are not applied to the weight matrix, and the weight matrix remains the same. As a result, because of the random initialization of the weights, the classifier I trained (Which is an LSTM + a softmax layer) keeps predicting with random guess (i.e., 1/n accuracy, where n is the number of classes).\nNot sure if this is the right place to post, but I guess the problem is similar. I can open a new issue if you want. The code is very long (and messy because of everything I tried to make it work in the last few days), so I did not post here the whole thing. If you need the other parts, I can clean them and post here.", "body": "I think I am having a similar problem to this. Gradients are apparently computed, but the weight matrix associated with the gradient does not change (i.e., they are exactly the same across mini-batches and iterations). Interestingly, this happens only with one dataset. I use the same code for two other (MNIST and an artificially generated dataset) datasets, and it works fine. \n\n```\noptimizer = tf.train.GradientDescentOptimizer(0.1)\npredict_op = tf.argmax(py_x,1)\n\ntvars = tf.trainable_variables()\ngrads = tf.gradients(cost, tvars)\ntrain_op = optimizer.apply_gradients(zip(grads, tvars))\n```\n\nSo this is my optimization code. I used this explicit way of computing to see the values of the gradients. Otherwise I used\n\n`train_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)`\n\nand other optimization techniques TensorFlow offers as well. As I said, although gradients are computed(And they are non-zero), they are not applied to the weight matrix, and the weight matrix remains the same. As a result, because of the random initialization of the weights, the classifier I trained (Which is an LSTM + a softmax layer) keeps predicting with random guess (i.e., 1/n accuracy, where n is the number of classes). \n\nNot sure if this is the right place to post, but I guess the problem is similar. I can open a new issue if you want. The code is very long (and messy because of everything I tried to make it work in the last few days), so I did not post here the whole thing. If you need the other parts, I can clean them and post here. \n"}