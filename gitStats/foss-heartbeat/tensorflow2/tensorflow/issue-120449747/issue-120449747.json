{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/412", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/412/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/412/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/412/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/412", "id": 120449747, "node_id": "MDU6SXNzdWUxMjA0NDk3NDc=", "number": 412, "title": "Adding operations after gradient are applied screws with the optimizer", "user": {"login": "domluna", "id": 1907223, "node_id": "MDQ6VXNlcjE5MDcyMjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1907223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/domluna", "html_url": "https://github.com/domluna", "followers_url": "https://api.github.com/users/domluna/followers", "following_url": "https://api.github.com/users/domluna/following{/other_user}", "gists_url": "https://api.github.com/users/domluna/gists{/gist_id}", "starred_url": "https://api.github.com/users/domluna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/domluna/subscriptions", "organizations_url": "https://api.github.com/users/domluna/orgs", "repos_url": "https://api.github.com/users/domluna/repos", "events_url": "https://api.github.com/users/domluna/events{/privacy}", "received_events_url": "https://api.github.com/users/domluna/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2015-12-04T17:36:51Z", "updated_at": "2016-06-16T23:01:48Z", "closed_at": "2016-02-19T07:53:43Z", "author_association": "NONE", "body_html": "<p>I'm currently trying to add an operation after I apply the gradients. Essentially I have a row of some variables that I don't to change via gradients so I would ideally reset it after gradients are applied (I could also set that part of the gradient to 0 but w/e). I currently have this done after my forward inference pass as putting it after apply_gradients doesn't seem to work.</p>\n<p>Even adding an op like tf.Print seems to completely screw with the optimizer. What I gather from my debugging is no variables are updated.</p>\n<pre><code>train_op = self._opt.apply_gradients(...)\ntrain_op = tf.Print(train_op, ....)\n</code></pre>\n<p>If I understand Print correctly it just prints stuff and then returns have the op that was passed in, so the returned op should work the same. tf.Print works as expected it prints out what I wanted it to print.</p>", "body_text": "I'm currently trying to add an operation after I apply the gradients. Essentially I have a row of some variables that I don't to change via gradients so I would ideally reset it after gradients are applied (I could also set that part of the gradient to 0 but w/e). I currently have this done after my forward inference pass as putting it after apply_gradients doesn't seem to work.\nEven adding an op like tf.Print seems to completely screw with the optimizer. What I gather from my debugging is no variables are updated.\ntrain_op = self._opt.apply_gradients(...)\ntrain_op = tf.Print(train_op, ....)\n\nIf I understand Print correctly it just prints stuff and then returns have the op that was passed in, so the returned op should work the same. tf.Print works as expected it prints out what I wanted it to print.", "body": "I'm currently trying to add an operation after I apply the gradients. Essentially I have a row of some variables that I don't to change via gradients so I would ideally reset it after gradients are applied (I could also set that part of the gradient to 0 but w/e). I currently have this done after my forward inference pass as putting it after apply_gradients doesn't seem to work.\n\nEven adding an op like tf.Print seems to completely screw with the optimizer. What I gather from my debugging is no variables are updated.\n\n```\ntrain_op = self._opt.apply_gradients(...)\ntrain_op = tf.Print(train_op, ....)\n```\n\nIf I understand Print correctly it just prints stuff and then returns have the op that was passed in, so the returned op should work the same. tf.Print works as expected it prints out what I wanted it to print.\n"}