{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/401413077", "html_url": "https://github.com/tensorflow/tensorflow/issues/20059#issuecomment-401413077", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20059", "id": 401413077, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTQxMzA3Nw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-29T16:55:32Z", "updated_at": "2018-06-29T16:55:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>That is surprising. I'll assign this to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1072079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jsimsa\">@jsimsa</a>, since he has been working on the performance of <code>map_and_batch()</code> most recently (and since our working hypothesis is that converting parallel <code>map().batch()</code> to <code>map_and_batch()</code> with the same degree is always at worst performance-neutral, we'll need to get to the bottom of this).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1072079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jsimsa\">@jsimsa</a> The only thing I can think of here is that the <code>ParallelConcat()</code> we use here:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/b7300de4ef75bdd9373bccc4ae5b98135a70287b/tensorflow/core/kernels/data/map_and_batch_dataset_op.cc#L309-L310\">tensorflow/tensorflow/core/kernels/data/map_and_batch_dataset_op.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 309 to 310\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/b7300de4ef75bdd9373bccc4ae5b98135a70287b\">b7300de</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L309\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"309\"></td>\n          <td id=\"LC309\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> Status copy_status = ::<span class=\"pl-c1\">tensorflow::functor::DoParallelConcat</span>( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L310\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"310\"></td>\n          <td id=\"LC310\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     *<span class=\"pl-c1\">dataset</span>()-&gt;<span class=\"pl-smi\">device_</span>, tensor, offset, batch); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>...might be slower than sequential concat in some cases. For example, we might be using too many threads to perform each copy, and they could be contending. I'm not convinced that multithreading that copy is always a good idea when we'd expect to have <code>num_parallel_calls</code> copies issuing in parallel anyway.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2991890\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cipri-tom\">@cipri-tom</a> We might have some difficulty reproducing your workload without more details. As a proxy, would you be able to capture a performance trace using a tool like pprof and share the results when running each version? Also, could you try running with <code>intra_op_parallelism=1</code>, by adding a <code>tf.ConfigProto</code> to your <code>tf.Session</code> arguments? That would help to test my hypothesis about <code>ParallelConcat()</code>.</p>\n<p>Thanks!</p>", "body_text": "That is surprising. I'll assign this to @jsimsa, since he has been working on the performance of map_and_batch() most recently (and since our working hypothesis is that converting parallel map().batch() to map_and_batch() with the same degree is always at worst performance-neutral, we'll need to get to the bottom of this).\n@jsimsa The only thing I can think of here is that the ParallelConcat() we use here:\n\n  \n    \n      tensorflow/tensorflow/core/kernels/data/map_and_batch_dataset_op.cc\n    \n    \n        Lines 309 to 310\n      in\n      b7300de\n    \n    \n    \n    \n\n        \n          \n           Status copy_status = ::tensorflow::functor::DoParallelConcat( \n        \n\n        \n          \n               *dataset()->device_, tensor, offset, batch); \n        \n    \n  \n\n\n...might be slower than sequential concat in some cases. For example, we might be using too many threads to perform each copy, and they could be contending. I'm not convinced that multithreading that copy is always a good idea when we'd expect to have num_parallel_calls copies issuing in parallel anyway.\n@cipri-tom We might have some difficulty reproducing your workload without more details. As a proxy, would you be able to capture a performance trace using a tool like pprof and share the results when running each version? Also, could you try running with intra_op_parallelism=1, by adding a tf.ConfigProto to your tf.Session arguments? That would help to test my hypothesis about ParallelConcat().\nThanks!", "body": "That is surprising. I'll assign this to @jsimsa, since he has been working on the performance of `map_and_batch()` most recently (and since our working hypothesis is that converting parallel `map().batch()` to `map_and_batch()` with the same degree is always at worst performance-neutral, we'll need to get to the bottom of this).\r\n\r\n@jsimsa The only thing I can think of here is that the `ParallelConcat()` we use here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b7300de4ef75bdd9373bccc4ae5b98135a70287b/tensorflow/core/kernels/data/map_and_batch_dataset_op.cc#L309-L310\r\n\r\n...might be slower than sequential concat in some cases. For example, we might be using too many threads to perform each copy, and they could be contending. I'm not convinced that multithreading that copy is always a good idea when we'd expect to have `num_parallel_calls` copies issuing in parallel anyway.\r\n\r\n@cipri-tom We might have some difficulty reproducing your workload without more details. As a proxy, would you be able to capture a performance trace using a tool like pprof and share the results when running each version? Also, could you try running with `intra_op_parallelism=1`, by adding a `tf.ConfigProto` to your `tf.Session` arguments? That would help to test my hypothesis about `ParallelConcat()`.\r\n\r\nThanks!"}