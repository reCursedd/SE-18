{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20059", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20059/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20059/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20059/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20059", "id": 332752216, "node_id": "MDU6SXNzdWUzMzI3NTIyMTY=", "number": 20059, "title": "map_and_batch slower than map + batch", "user": {"login": "cipri-tom", "id": 2991890, "node_id": "MDQ6VXNlcjI5OTE4OTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/2991890?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cipri-tom", "html_url": "https://github.com/cipri-tom", "followers_url": "https://api.github.com/users/cipri-tom/followers", "following_url": "https://api.github.com/users/cipri-tom/following{/other_user}", "gists_url": "https://api.github.com/users/cipri-tom/gists{/gist_id}", "starred_url": "https://api.github.com/users/cipri-tom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cipri-tom/subscriptions", "organizations_url": "https://api.github.com/users/cipri-tom/orgs", "repos_url": "https://api.github.com/users/cipri-tom/repos", "events_url": "https://api.github.com/users/cipri-tom/events{/privacy}", "received_events_url": "https://api.github.com/users/cipri-tom/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-06-15T12:17:38Z", "updated_at": "2018-07-31T18:01:35Z", "closed_at": "2018-07-07T01:36:21Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Docker (tensorflow/tensorflow:1.8.0-gpu-py3)</li>\n<li><strong>TensorFlow version (use command below)</strong>: <code>v1.8.0-0-g93bc2e2072</code></li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>: GTX 1080 Ti 11 Gb</li>\n<li><strong>Exact command to reproduce</strong>: n/a</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using <code>map_and_batch</code> in my use case results in a slower input pipeline than using normal <code>map</code> followed by <code>batch</code>. <code>batch_size=512</code></p>\n<p>Here is my code. The <code>augment_data</code> and <code>padding_inputs_width</code> are <a href=\"https://github.com/cipri-tom/tf-crnn/blob/15b9aa5cce440a4e4f4df0dcffc77ce4cd9b913d/src/data_handler.py#L92\">quite heavy</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">parse_example</span>(<span class=\"pl-smi\">serialized_example</span>, <span class=\"pl-smi\">output_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    features <span class=\"pl-k\">=</span> tf.parse_single_example(serialized_example, feature_spec)\n    label <span class=\"pl-k\">=</span> features.pop(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>label<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Replace image_raw with the decoded &amp; preprocessed version</span>\n    image <span class=\"pl-k\">=</span> features.pop(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image_raw<span class=\"pl-pds\">'</span></span>)\n    image <span class=\"pl-k\">=</span> tf.image.decode_png(image, <span class=\"pl-v\">channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    image <span class=\"pl-k\">=</span> augment_data(image)\n    image, orig_width <span class=\"pl-k\">=</span> padding_inputs_width(image, output_shape, <span class=\"pl-c1\">...</span>)\n    features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> image\n    features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image_width<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> orig_width\n    <span class=\"pl-k\">return</span> features, label\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">make_input_fn</span>(<span class=\"pl-smi\">files_pattern</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">output_shape</span>):\n    shaped_parse_example <span class=\"pl-k\">=</span> partial(parse_example, <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>output_shape)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">input_fn</span>():\n        files <span class=\"pl-k\">=</span> tf.data.Dataset.list_files(files_pattern, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        ds <span class=\"pl-k\">=</span> files.apply(tf.contrib.data.parallel_interleave(\n            tf.data.TFRecordDataset,\n            <span class=\"pl-v\">cycle_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">block_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">16</span>, <span class=\"pl-v\">sloppy</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">NOTE</span>: using map_and_batch seems to decrease performance</span>\n        ds <span class=\"pl-k\">=</span> (ds.shuffle(<span class=\"pl-v\">buffer_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> small buffer since files were also shuffled</span>\n                .apply(tf.contrib.data.map_and_batch(\n                    shaped_parse_example, batch_size,\n                    <span class=\"pl-v\">num_parallel_batches</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">drop_remainder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> separate calls version, comment the above apply</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> .map(shaped_parse_example, num_parallel_calls=4)</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> .apply(tf.contrib.data.batch_and_drop_remainder(batch_size))</span>\n              )\n        features, labels <span class=\"pl-k\">=</span> ds.prefetch(<span class=\"pl-c1\">2</span>).make_one_shot_iterator().get_next()\n        <span class=\"pl-k\">return</span> features, labels\n\n    <span class=\"pl-k\">return</span> input_fn</pre></div>\n<p>While I use the same number of <em>parallel</em> stuff, I think the difference comes from the fact that the map function is heavy and when using <code>map_and_batch</code> only one thread is used for producing each batch.</p>\n<h3>How much slower ?</h3>\n<p>It is hard to quantify. With <code>map_and_batch</code> I just see lower numbers for GPU utilisation and even reaching zero at times. I tried increasing the <code>prefetch</code> to 4 to make up for this, but no improvement.</p>\n<p>Here I ran with the first input pipeline for a bit and then with the <code>map_and_batch</code>. You can see a difference of about 30%.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2991890/41467345-733f37ec-70a6-11e8-89f7-21be1d8eda66.png\"><img width=\"361\" alt=\"screen shot 2018-06-15 at 14 13 36\" src=\"https://user-images.githubusercontent.com/2991890/41467345-733f37ec-70a6-11e8-89f7-21be1d8eda66.png\" style=\"max-width:100%;\"></a></p>\n<h3>Feature request</h3>\n<p>The reason for this issue is that the documentation for <code>map_and_batch</code> says it will be done automatically in future versions. I think that in its current version, this can be a regression, as shown above. I believe (though I'm most probably wrong) that there should be a parameter in <code>map_and_batch</code> controlling the number of threads for the <code>map</code> operation, and another one for the <code>num_parallel_batches</code>. Or along those lines...</p>\n<p>Edit: Python version is 3.5.2</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Docker (tensorflow/tensorflow:1.8.0-gpu-py3)\nTensorFlow version (use command below): v1.8.0-0-g93bc2e2072\nPython version: 3.5.2\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory: GTX 1080 Ti 11 Gb\nExact command to reproduce: n/a\n\nDescribe the problem\nUsing map_and_batch in my use case results in a slower input pipeline than using normal map followed by batch. batch_size=512\nHere is my code. The augment_data and padding_inputs_width are quite heavy\ndef parse_example(serialized_example, output_shape=None):\n    features = tf.parse_single_example(serialized_example, feature_spec)\n    label = features.pop('label')\n\n    # Replace image_raw with the decoded & preprocessed version\n    image = features.pop('image_raw')\n    image = tf.image.decode_png(image, channels=1)\n    image = augment_data(image)\n    image, orig_width = padding_inputs_width(image, output_shape, ...)\n    features['image'] = image\n    features['image_width'] = orig_width\n    return features, label\n\n\ndef make_input_fn(files_pattern, batch_size, output_shape):\n    shaped_parse_example = partial(parse_example, output_shape=output_shape)\n\n    def input_fn():\n        files = tf.data.Dataset.list_files(files_pattern, shuffle=True)\n        ds = files.apply(tf.contrib.data.parallel_interleave(\n            tf.data.TFRecordDataset,\n            cycle_length=4, block_length=16, sloppy=True))\n\n        # NOTE: using map_and_batch seems to decrease performance\n        ds = (ds.shuffle(buffer_size=128) # small buffer since files were also shuffled\n                .apply(tf.contrib.data.map_and_batch(\n                    shaped_parse_example, batch_size,\n                    num_parallel_batches=4, drop_remainder=True))\n\n                # separate calls version, comment the above apply\n                # .map(shaped_parse_example, num_parallel_calls=4)\n                # .apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n              )\n        features, labels = ds.prefetch(2).make_one_shot_iterator().get_next()\n        return features, labels\n\n    return input_fn\nWhile I use the same number of parallel stuff, I think the difference comes from the fact that the map function is heavy and when using map_and_batch only one thread is used for producing each batch.\nHow much slower ?\nIt is hard to quantify. With map_and_batch I just see lower numbers for GPU utilisation and even reaching zero at times. I tried increasing the prefetch to 4 to make up for this, but no improvement.\nHere I ran with the first input pipeline for a bit and then with the map_and_batch. You can see a difference of about 30%.\n\nFeature request\nThe reason for this issue is that the documentation for map_and_batch says it will be done automatically in future versions. I think that in its current version, this can be a regression, as shown above. I believe (though I'm most probably wrong) that there should be a parameter in map_and_batch controlling the number of threads for the map operation, and another one for the num_parallel_batches. Or along those lines...\nEdit: Python version is 3.5.2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Docker (tensorflow/tensorflow:1.8.0-gpu-py3)\r\n- **TensorFlow version (use command below)**: `v1.8.0-0-g93bc2e2072`\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: GTX 1080 Ti 11 Gb\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\nUsing `map_and_batch` in my use case results in a slower input pipeline than using normal `map` followed by `batch`. `batch_size=512`\r\n\r\nHere is my code. The `augment_data` and `padding_inputs_width` are [quite heavy](https://github.com/cipri-tom/tf-crnn/blob/15b9aa5cce440a4e4f4df0dcffc77ce4cd9b913d/src/data_handler.py#L92)\r\n\r\n```python\r\ndef parse_example(serialized_example, output_shape=None):\r\n    features = tf.parse_single_example(serialized_example, feature_spec)\r\n    label = features.pop('label')\r\n\r\n    # Replace image_raw with the decoded & preprocessed version\r\n    image = features.pop('image_raw')\r\n    image = tf.image.decode_png(image, channels=1)\r\n    image = augment_data(image)\r\n    image, orig_width = padding_inputs_width(image, output_shape, ...)\r\n    features['image'] = image\r\n    features['image_width'] = orig_width\r\n    return features, label\r\n\r\n\r\ndef make_input_fn(files_pattern, batch_size, output_shape):\r\n    shaped_parse_example = partial(parse_example, output_shape=output_shape)\r\n\r\n    def input_fn():\r\n        files = tf.data.Dataset.list_files(files_pattern, shuffle=True)\r\n        ds = files.apply(tf.contrib.data.parallel_interleave(\r\n            tf.data.TFRecordDataset,\r\n            cycle_length=4, block_length=16, sloppy=True))\r\n\r\n        # NOTE: using map_and_batch seems to decrease performance\r\n        ds = (ds.shuffle(buffer_size=128) # small buffer since files were also shuffled\r\n                .apply(tf.contrib.data.map_and_batch(\r\n                    shaped_parse_example, batch_size,\r\n                    num_parallel_batches=4, drop_remainder=True))\r\n\r\n                # separate calls version, comment the above apply\r\n                # .map(shaped_parse_example, num_parallel_calls=4)\r\n                # .apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n              )\r\n        features, labels = ds.prefetch(2).make_one_shot_iterator().get_next()\r\n        return features, labels\r\n\r\n    return input_fn\r\n```\r\n\r\n\r\nWhile I use the same number of _parallel_ stuff, I think the difference comes from the fact that the map function is heavy and when using `map_and_batch` only one thread is used for producing each batch.\r\n\r\n### How much slower ?\r\n\r\nIt is hard to quantify. With `map_and_batch` I just see lower numbers for GPU utilisation and even reaching zero at times. I tried increasing the `prefetch` to 4 to make up for this, but no improvement.\r\n\r\nHere I ran with the first input pipeline for a bit and then with the `map_and_batch`. You can see a difference of about 30%.\r\n\r\n<img width=\"361\" alt=\"screen shot 2018-06-15 at 14 13 36\" src=\"https://user-images.githubusercontent.com/2991890/41467345-733f37ec-70a6-11e8-89f7-21be1d8eda66.png\">\r\n\r\n\r\n### Feature request\r\n\r\nThe reason for this issue is that the documentation for `map_and_batch` says it will be done automatically in future versions. I think that in its current version, this can be a regression, as shown above. I believe (though I'm most probably wrong) that there should be a parameter in `map_and_batch` controlling the number of threads for the `map` operation, and another one for the `num_parallel_batches`. Or along those lines...\r\n\r\nEdit: Python version is 3.5.2"}