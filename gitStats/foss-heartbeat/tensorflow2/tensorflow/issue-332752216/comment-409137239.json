{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/409137239", "html_url": "https://github.com/tensorflow/tensorflow/issues/20059#issuecomment-409137239", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20059", "id": 409137239, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTEzNzIzOQ==", "user": {"login": "cipri-tom", "id": 2991890, "node_id": "MDQ6VXNlcjI5OTE4OTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/2991890?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cipri-tom", "html_url": "https://github.com/cipri-tom", "followers_url": "https://api.github.com/users/cipri-tom/followers", "following_url": "https://api.github.com/users/cipri-tom/following{/other_user}", "gists_url": "https://api.github.com/users/cipri-tom/gists{/gist_id}", "starred_url": "https://api.github.com/users/cipri-tom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cipri-tom/subscriptions", "organizations_url": "https://api.github.com/users/cipri-tom/orgs", "repos_url": "https://api.github.com/users/cipri-tom/repos", "events_url": "https://api.github.com/users/cipri-tom/events{/privacy}", "received_events_url": "https://api.github.com/users/cipri-tom/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T08:19:54Z", "updated_at": "2018-07-31T08:19:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1072079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jsimsa\">@jsimsa</a> Thank you for the tests and the benchmarking program! Indeed, running it with various configurations doesn't reflect any troubles with either pipeline.</p>\n<p>On my side, there are no conclusive results. I still see the mentioned slow-down, but the causes are very weird and most probably tied to my system/program and not to TF.</p>\n<p>This is because I ran one very long training on a separate and more performant machine, and during the training I saw the <code>global_step/sec</code> measure fluctuating by about the same amount as I previously attributed to the difference between <code>.map().batch()</code> and <code>map_and_batch()</code>.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2991890/43446724-e32eb9fc-94a9-11e8-9899-bec46e933afb.png\"><img width=\"358\" alt=\"graph of global steps per second during continuous training\" src=\"https://user-images.githubusercontent.com/2991890/43446724-e32eb9fc-94a9-11e8-9899-bec46e933afb.png\" style=\"max-width:100%;\"></a></p>\n<p>It is interesting that the intervals of performance drop/increase are synchronised with the epochs. In other words, each lasts ~4000 steps which is the size of one epoch, and I trained for 10 epochs. If you have any suggestions for this, they would be very welcome. Otherwise, it is safe to let this issue die <g-emoji class=\"g-emoji\" alias=\"grinning\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f600.png\">\ud83d\ude00</g-emoji></p>", "body_text": "@jsimsa Thank you for the tests and the benchmarking program! Indeed, running it with various configurations doesn't reflect any troubles with either pipeline.\nOn my side, there are no conclusive results. I still see the mentioned slow-down, but the causes are very weird and most probably tied to my system/program and not to TF.\nThis is because I ran one very long training on a separate and more performant machine, and during the training I saw the global_step/sec measure fluctuating by about the same amount as I previously attributed to the difference between .map().batch() and map_and_batch().\n\nIt is interesting that the intervals of performance drop/increase are synchronised with the epochs. In other words, each lasts ~4000 steps which is the size of one epoch, and I trained for 10 epochs. If you have any suggestions for this, they would be very welcome. Otherwise, it is safe to let this issue die \ud83d\ude00", "body": "@jsimsa Thank you for the tests and the benchmarking program! Indeed, running it with various configurations doesn't reflect any troubles with either pipeline.\r\n\r\nOn my side, there are no conclusive results. I still see the mentioned slow-down, but the causes are very weird and most probably tied to my system/program and not to TF.\r\n\r\nThis is because I ran one very long training on a separate and more performant machine, and during the training I saw the `global_step/sec` measure fluctuating by about the same amount as I previously attributed to the difference between `.map().batch()` and `map_and_batch()`.\r\n\r\n<img width=\"358\" alt=\"graph of global steps per second during continuous training\" src=\"https://user-images.githubusercontent.com/2991890/43446724-e32eb9fc-94a9-11e8-9899-bec46e933afb.png\">\r\n\r\nIt is interesting that the intervals of performance drop/increase are synchronised with the epochs. In other words, each lasts ~4000 steps which is the size of one epoch, and I trained for 10 epochs. If you have any suggestions for this, they would be very welcome. Otherwise, it is safe to let this issue die \ud83d\ude00 \r\n"}