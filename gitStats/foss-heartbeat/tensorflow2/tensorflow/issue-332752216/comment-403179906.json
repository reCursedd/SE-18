{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/403179906", "html_url": "https://github.com/tensorflow/tensorflow/issues/20059#issuecomment-403179906", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20059", "id": 403179906, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzE3OTkwNg==", "user": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-07T01:36:21Z", "updated_at": "2018-07-07T01:36:21Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2991890\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cipri-tom\">@cipri-tom</a>, I evaluated the performance of <code>map().batch()</code> vs <code>map_and_batch()</code> across a wide range of configurations (varying transformation parallelism, batch size, map function cost, threadpool size) and didn't come across any configuration for which <code>map_and_batch()</code> would perform worse than <code>map().batch()</code>.</p>\n<p>This is the program that I used for my evaluation:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport time\n\nbatch_size = 1024\nnum_calls = 16\ninter_op = 16\nelement_size = 1\nnum_iters = 16\nk = 1024 * 1024\n\ndataset = tf.data.Dataset.from_tensors((np.random.rand(\n    element_size, 4 * k), np.random.rand(4 * k, 1))).repeat()\n\nchained_dataset = dataset.map(\n    tf.matmul, num_parallel_calls=num_calls).batch(batch_size=batch_size)\nchained_iterator = chained_dataset.make_one_shot_iterator()\nchained_get_next = chained_iterator.get_next()\n\nchained_deltas = []\nwith tf.Session(config=tf.ConfigProto(\n    inter_op_parallelism_threads=inter_op)) as sess:\n\n  with tf.Session(\n      config=tf.ConfigProto(\n          inter_op_parallelism_threads=inter_op)) as sess:\n    for _ in range(5):\n      sess.run(chained_get_next.op)\n    for _ in range(num_iters):\n      start = time.time()\n      sess.run(chained_get_next.op)\n      end = time.time()\n      chained_deltas.append(end - start)\n\nfused_dataset = dataset.apply(\n    tf.contrib.data.map_and_batch(\n        tf.matmul, num_parallel_calls=num_calls, batch_size=batch_size))\nfused_iterator = fused_dataset.make_one_shot_iterator()\nfused_get_next = fused_iterator.get_next()\n\nfused_deltas = []\nwith tf.Session(config=tf.ConfigProto(\n    inter_op_parallelism_threads=inter_op)) as sess:\n\n  with tf.Session(\n      config=tf.ConfigProto(\n          inter_op_parallelism_threads=inter_op)) as sess:\n\n    for _ in range(5):\n      sess.run(fused_get_next.op)\n    for _ in range(num_iters):\n      start = time.time()\n      sess.run(fused_get_next.op)\n      end = time.time()\n      fused_deltas.append(end - start)\n\nprint(\n    \"batch size: %d, num parallel calls: %d, inter-op parallelism: %d, \"\n    \"element size: %d\\nchained wall time: %f (median), %f (mean), %f \"\n    \"(stddev), %f (min), %f (max)\\n  fused wall time: %f (median), %f \"\n    \"(mean), %f (stddev), %f (min), %f (max)\\n    chained/fused: \"\n    \"   %.2fx (median),    %.2fx (mean)\" %\n    (batch_size, num_calls, inter_op, element_size, np.median(chained_deltas),\n     np.mean(chained_deltas), np.std(chained_deltas), np.min(chained_deltas),\n     np.max(chained_deltas), np.median(fused_deltas), np.mean(fused_deltas),\n     np.std(fused_deltas), np.min(fused_deltas), np.max(fused_deltas),\n     np.median(chained_deltas) / np.median(fused_deltas),\n     np.mean(chained_deltas) / np.mean(fused_deltas)))\n</code></pre>\n<p>See if you can use it as a starting point to generate an example that reproduces the issue you have encountered.</p>\n<p>As a side note, since you seem to care about performance, I recommend you build TensorFlow from source with AVX, AVX2, or FMA enabled (assuming your CPU supports these). Doing so will likely benefit the performance of your pipeline.</p>", "body_text": "Hi @cipri-tom, I evaluated the performance of map().batch() vs map_and_batch() across a wide range of configurations (varying transformation parallelism, batch size, map function cost, threadpool size) and didn't come across any configuration for which map_and_batch() would perform worse than map().batch().\nThis is the program that I used for my evaluation:\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nbatch_size = 1024\nnum_calls = 16\ninter_op = 16\nelement_size = 1\nnum_iters = 16\nk = 1024 * 1024\n\ndataset = tf.data.Dataset.from_tensors((np.random.rand(\n    element_size, 4 * k), np.random.rand(4 * k, 1))).repeat()\n\nchained_dataset = dataset.map(\n    tf.matmul, num_parallel_calls=num_calls).batch(batch_size=batch_size)\nchained_iterator = chained_dataset.make_one_shot_iterator()\nchained_get_next = chained_iterator.get_next()\n\nchained_deltas = []\nwith tf.Session(config=tf.ConfigProto(\n    inter_op_parallelism_threads=inter_op)) as sess:\n\n  with tf.Session(\n      config=tf.ConfigProto(\n          inter_op_parallelism_threads=inter_op)) as sess:\n    for _ in range(5):\n      sess.run(chained_get_next.op)\n    for _ in range(num_iters):\n      start = time.time()\n      sess.run(chained_get_next.op)\n      end = time.time()\n      chained_deltas.append(end - start)\n\nfused_dataset = dataset.apply(\n    tf.contrib.data.map_and_batch(\n        tf.matmul, num_parallel_calls=num_calls, batch_size=batch_size))\nfused_iterator = fused_dataset.make_one_shot_iterator()\nfused_get_next = fused_iterator.get_next()\n\nfused_deltas = []\nwith tf.Session(config=tf.ConfigProto(\n    inter_op_parallelism_threads=inter_op)) as sess:\n\n  with tf.Session(\n      config=tf.ConfigProto(\n          inter_op_parallelism_threads=inter_op)) as sess:\n\n    for _ in range(5):\n      sess.run(fused_get_next.op)\n    for _ in range(num_iters):\n      start = time.time()\n      sess.run(fused_get_next.op)\n      end = time.time()\n      fused_deltas.append(end - start)\n\nprint(\n    \"batch size: %d, num parallel calls: %d, inter-op parallelism: %d, \"\n    \"element size: %d\\nchained wall time: %f (median), %f (mean), %f \"\n    \"(stddev), %f (min), %f (max)\\n  fused wall time: %f (median), %f \"\n    \"(mean), %f (stddev), %f (min), %f (max)\\n    chained/fused: \"\n    \"   %.2fx (median),    %.2fx (mean)\" %\n    (batch_size, num_calls, inter_op, element_size, np.median(chained_deltas),\n     np.mean(chained_deltas), np.std(chained_deltas), np.min(chained_deltas),\n     np.max(chained_deltas), np.median(fused_deltas), np.mean(fused_deltas),\n     np.std(fused_deltas), np.min(fused_deltas), np.max(fused_deltas),\n     np.median(chained_deltas) / np.median(fused_deltas),\n     np.mean(chained_deltas) / np.mean(fused_deltas)))\n\nSee if you can use it as a starting point to generate an example that reproduces the issue you have encountered.\nAs a side note, since you seem to care about performance, I recommend you build TensorFlow from source with AVX, AVX2, or FMA enabled (assuming your CPU supports these). Doing so will likely benefit the performance of your pipeline.", "body": "Hi @cipri-tom, I evaluated the performance of `map().batch()` vs `map_and_batch()` across a wide range of configurations (varying transformation parallelism, batch size, map function cost, threadpool size) and didn't come across any configuration for which `map_and_batch()` would perform worse than `map().batch()`.\r\n\r\nThis is the program that I used for my evaluation:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nbatch_size = 1024\r\nnum_calls = 16\r\ninter_op = 16\r\nelement_size = 1\r\nnum_iters = 16\r\nk = 1024 * 1024\r\n\r\ndataset = tf.data.Dataset.from_tensors((np.random.rand(\r\n    element_size, 4 * k), np.random.rand(4 * k, 1))).repeat()\r\n\r\nchained_dataset = dataset.map(\r\n    tf.matmul, num_parallel_calls=num_calls).batch(batch_size=batch_size)\r\nchained_iterator = chained_dataset.make_one_shot_iterator()\r\nchained_get_next = chained_iterator.get_next()\r\n\r\nchained_deltas = []\r\nwith tf.Session(config=tf.ConfigProto(\r\n    inter_op_parallelism_threads=inter_op)) as sess:\r\n\r\n  with tf.Session(\r\n      config=tf.ConfigProto(\r\n          inter_op_parallelism_threads=inter_op)) as sess:\r\n    for _ in range(5):\r\n      sess.run(chained_get_next.op)\r\n    for _ in range(num_iters):\r\n      start = time.time()\r\n      sess.run(chained_get_next.op)\r\n      end = time.time()\r\n      chained_deltas.append(end - start)\r\n\r\nfused_dataset = dataset.apply(\r\n    tf.contrib.data.map_and_batch(\r\n        tf.matmul, num_parallel_calls=num_calls, batch_size=batch_size))\r\nfused_iterator = fused_dataset.make_one_shot_iterator()\r\nfused_get_next = fused_iterator.get_next()\r\n\r\nfused_deltas = []\r\nwith tf.Session(config=tf.ConfigProto(\r\n    inter_op_parallelism_threads=inter_op)) as sess:\r\n\r\n  with tf.Session(\r\n      config=tf.ConfigProto(\r\n          inter_op_parallelism_threads=inter_op)) as sess:\r\n\r\n    for _ in range(5):\r\n      sess.run(fused_get_next.op)\r\n    for _ in range(num_iters):\r\n      start = time.time()\r\n      sess.run(fused_get_next.op)\r\n      end = time.time()\r\n      fused_deltas.append(end - start)\r\n\r\nprint(\r\n    \"batch size: %d, num parallel calls: %d, inter-op parallelism: %d, \"\r\n    \"element size: %d\\nchained wall time: %f (median), %f (mean), %f \"\r\n    \"(stddev), %f (min), %f (max)\\n  fused wall time: %f (median), %f \"\r\n    \"(mean), %f (stddev), %f (min), %f (max)\\n    chained/fused: \"\r\n    \"   %.2fx (median),    %.2fx (mean)\" %\r\n    (batch_size, num_calls, inter_op, element_size, np.median(chained_deltas),\r\n     np.mean(chained_deltas), np.std(chained_deltas), np.min(chained_deltas),\r\n     np.max(chained_deltas), np.median(fused_deltas), np.mean(fused_deltas),\r\n     np.std(fused_deltas), np.min(fused_deltas), np.max(fused_deltas),\r\n     np.median(chained_deltas) / np.median(fused_deltas),\r\n     np.mean(chained_deltas) / np.mean(fused_deltas)))\r\n```\r\n\r\nSee if you can use it as a starting point to generate an example that reproduces the issue you have encountered.\r\n\r\nAs a side note, since you seem to care about performance, I recommend you build TensorFlow from source with AVX, AVX2, or FMA enabled (assuming your CPU supports these). Doing so will likely benefit the performance of your pipeline."}