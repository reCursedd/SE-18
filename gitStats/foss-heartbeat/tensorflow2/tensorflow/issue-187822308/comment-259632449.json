{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259632449", "html_url": "https://github.com/tensorflow/tensorflow/issues/5464#issuecomment-259632449", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5464", "id": 259632449, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTYzMjQ0OQ==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-10T08:42:00Z", "updated_at": "2016-11-10T08:42:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12770037\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sherrym\">@sherrym</a> - ok, the thing was that queue returns images in batches of 128 images at a time so CNN expects tensor of shape [batch_size, height, width, channels]. When I changed batch_size to 1 and trained it like that, everything worked fine. I'm wondering still if there is a way to train network with batch_size != 1 and then later freeze graph and reuse it by classifying images 1 by 1? Maybe somehow tell CNN to receive tensors of shape [None, height, width, channels] so when I freeze graph and run it from C++ it will work with only 1 example. My current solution could be to train Cifar10 model with batch_size = 128 and then in C++ create tensor with shape [128, height, width, channels], only first image would be real image and other \"images\" would be just random numbers, but that seems like waste of processing time and resources.</p>", "body_text": "@sherrym - ok, the thing was that queue returns images in batches of 128 images at a time so CNN expects tensor of shape [batch_size, height, width, channels]. When I changed batch_size to 1 and trained it like that, everything worked fine. I'm wondering still if there is a way to train network with batch_size != 1 and then later freeze graph and reuse it by classifying images 1 by 1? Maybe somehow tell CNN to receive tensors of shape [None, height, width, channels] so when I freeze graph and run it from C++ it will work with only 1 example. My current solution could be to train Cifar10 model with batch_size = 128 and then in C++ create tensor with shape [128, height, width, channels], only first image would be real image and other \"images\" would be just random numbers, but that seems like waste of processing time and resources.", "body": "@sherrym - ok, the thing was that queue returns images in batches of 128 images at a time so CNN expects tensor of shape [batch_size, height, width, channels]. When I changed batch_size to 1 and trained it like that, everything worked fine. I'm wondering still if there is a way to train network with batch_size != 1 and then later freeze graph and reuse it by classifying images 1 by 1? Maybe somehow tell CNN to receive tensors of shape [None, height, width, channels] so when I freeze graph and run it from C++ it will work with only 1 example. My current solution could be to train Cifar10 model with batch_size = 128 and then in C++ create tensor with shape [128, height, width, channels], only first image would be real image and other \"images\" would be just random numbers, but that seems like waste of processing time and resources.\n"}