{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/358044442", "html_url": "https://github.com/tensorflow/tensorflow/issues/16082#issuecomment-358044442", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16082", "id": 358044442, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODA0NDQ0Mg==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-16T17:43:13Z", "updated_at": "2018-01-16T17:43:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If you're using PySpark and wrapping your training step in an <code>RDD.mapPartitions()</code> transformation, you should be able to use <code>tf.data.Dataset.from_generator()</code> to convert the partition iterator into a generator of NumPy arrays. I dare say there might be a more efficient way to connect a Spark RDD to a <code>tf.data.Dataset</code> (e.g. with a custom reader dataset as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6932348\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yongtang\">@yongtang</a> suggested), and we'd welcome contributions of that kind.</p>", "body_text": "If you're using PySpark and wrapping your training step in an RDD.mapPartitions() transformation, you should be able to use tf.data.Dataset.from_generator() to convert the partition iterator into a generator of NumPy arrays. I dare say there might be a more efficient way to connect a Spark RDD to a tf.data.Dataset (e.g. with a custom reader dataset as @yongtang suggested), and we'd welcome contributions of that kind.", "body": "If you're using PySpark and wrapping your training step in an `RDD.mapPartitions()` transformation, you should be able to use `tf.data.Dataset.from_generator()` to convert the partition iterator into a generator of NumPy arrays. I dare say there might be a more efficient way to connect a Spark RDD to a `tf.data.Dataset` (e.g. with a custom reader dataset as @yongtang suggested), and we'd welcome contributions of that kind."}