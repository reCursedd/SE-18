{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5999", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5999/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5999/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5999/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5999", "id": 192737074, "node_id": "MDU6SXNzdWUxOTI3MzcwNzQ=", "number": 5999, "title": "distributed seq2seq used SyncReplicasOptimizerV2 question", "user": {"login": "chengdianxuezi", "id": 10277403, "node_id": "MDQ6VXNlcjEwMjc3NDAz", "avatar_url": "https://avatars1.githubusercontent.com/u/10277403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chengdianxuezi", "html_url": "https://github.com/chengdianxuezi", "followers_url": "https://api.github.com/users/chengdianxuezi/followers", "following_url": "https://api.github.com/users/chengdianxuezi/following{/other_user}", "gists_url": "https://api.github.com/users/chengdianxuezi/gists{/gist_id}", "starred_url": "https://api.github.com/users/chengdianxuezi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chengdianxuezi/subscriptions", "organizations_url": "https://api.github.com/users/chengdianxuezi/orgs", "repos_url": "https://api.github.com/users/chengdianxuezi/repos", "events_url": "https://api.github.com/users/chengdianxuezi/events{/privacy}", "received_events_url": "https://api.github.com/users/chengdianxuezi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-12-01T02:15:35Z", "updated_at": "2016-12-01T02:36:41Z", "closed_at": "2016-12-01T02:36:41Z", "author_association": "NONE", "body_html": "<p>Now I want to change seq2seq to distributed mode used SyncReplicasOptimizerV2. I've been trying for a few days\uff0cbut still can't work out. this is my code,one file is seq2seq_train.py,on file is seq2seq_model.py<br>\nI try to run in 1 ps and 2 worker</p>\n<p>the error is:<br>\nTraceback (most recent call last):<br>\nFile \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 188, in <br>\ntf.app.run()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run<br>\nsys.exit(main(sys.argv[:1] + flags_passthrough))<br>\nFile \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 82, in main<br>\ntask_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)<br>\nFile \"/home/test/replicas_seq2seq_v4/seq2seq_model.py\", line 221, in <strong>init</strong><br>\nself.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 720, in prepare_or_wait_for_session<br>\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 251, in prepare_session<br>\nself._local_init_op, msg))<br>\nRuntimeError: Init operations did not make model ready.  Init op: init, init fn: None, local_init_op: name: \"sync_replicas_3/group_deps\"<br>\nop: \"NoOp\"<br>\ninput: \"^sync_replicas_3/group_deps/NoOp\"<br>\ninput: \"^sync_replicas_3/group_deps/NoOp_1\"<br>\ninput: \"^sync_replicas_3/group_deps/NoOp_2\"<br>\ndevice: \"/job:worker/task:0\"<br>\n, error: Variables not initialized: sync_rep_local_step, sync_rep_local_step_1, sync_rep_local_step_2</p>\n<p>ps:<br>\npython seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=ps --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --max_train_data_size=50000</p>\n<p>worker1:<br>\npython seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --</p>\n<p>worker2:<br>\npython seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=1 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --</p>\n<p>seq2seq_train.py</p>\n<p>from <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import tempfile<br>\nimport math<br>\nimport os<br>\nimport random<br>\nimport sys<br>\nimport time<br>\nimport json</p>\n<p>import numpy as np<br>\nfrom six.moves import xrange  # pylint: disable=redefined-builtin<br>\nimport tensorflow as tf</p>\n<p>from tensorflow.models.rnn.translate import data_utils<br>\n#from tensorflow.models.rnn.translate import seq2seq_model<br>\nimport seq2seq_model</p>\n<h1>Flags for defining the tf.train.ClusterSpec</h1>\n<p>tf.app.flags.DEFINE_string(\"ps_hosts\", \"\",<br>\n\"Comma-separated list of hostname:port pairs\")<br>\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",<br>\n\"Comma-separated list of hostname:port pairs\")</p>\n<h1>Flags for defining the tf.train.Server</h1>\n<p>tf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")<br>\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")</p>\n<p>tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")<br>\ntf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,<br>\n\"Learning rate decays by this much.\")<br>\ntf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,<br>\n\"Clip gradients to this norm.\")<br>\ntf.app.flags.DEFINE_integer(\"batch_size\", 64,<br>\n\"Batch size to use during training.\")<br>\ntf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")<br>\ntf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")<br>\ntf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")<br>\ntf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")<br>\ntf.app.flags.DEFINE_string(\"data_dir\", \"/tmp/data\", \"Data directory\")<br>\ntf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training directory.\")<br>\ntf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,<br>\n\"Limit on the size of training data (0: no limit).\")<br>\ntf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,<br>\n\"How many training steps to do per checkpoint.\")<br>\ntf.app.flags.DEFINE_boolean(\"decode\", False,<br>\n\"Set to True for interactive decoding.\")<br>\ntf.app.flags.DEFINE_boolean(\"self_test\", False,<br>\n\"Run a self-test if this is set to True.\")</p>\n<p>FLAGS = tf.app.flags.FLAGS</p>\n<p>_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]</p>\n<p>def main(_):<br>\nif FLAGS.job_name is None or FLAGS.job_name == \"\":<br>\nraise ValueError(\"Must specify an explicit <code>job_name</code>\")<br>\nif FLAGS.task_index is None or FLAGS.task_index ==\"\":<br>\nraise ValueError(\"Must specify an explicit <code>task_index</code>\")</p>\n<p>ps_hosts = FLAGS.ps_hosts.split(\",\")<br>\nworker_hosts = FLAGS.worker_hosts.split(\",\")</p>\n<h1>Create a cluster from the parameter server and worker hosts.</h1>\n<p>cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})</p>\n<h1>Create and start a server for the local task.</h1>\n<p>server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)<br>\nnum_workers = len(worker_hosts)</p>\n<p>server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)<br>\nif FLAGS.job_name == \"ps\":<br>\nserver.join()<br>\nreturn<br>\nis_chief = (FLAGS.task_index == 0)<br>\nmodel = seq2seq_model.Seq2SeqModel(server, FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,<br>\nFLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,<br>\nFLAGS.learning_rate, FLAGS.learning_rate_decay_factor,num_workers = num_workers,<br>\ntask_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)<br>\nstep_time, loss = 0.0, 0.0<br>\ncurrent_step = 0<br>\nprevious_losses = []</p>\n<p>print(\"query -&gt; title:\")<br>\nen_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.en\"<br>\nfr_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.fr\"<br>\nen_dev = FLAGS.data_dir + \"/newstest2013.ids40000.en\"<br>\nfr_dev = FLAGS.data_dir + \"/newstest2013.ids40000.fr\"</p>\n<h1>Read data into buckets and compute their sizes.</h1>\n<p>print (\"Reading development and training data (limit: %d).\" % FLAGS.max_train_data_size)<br>\ndev_set = read_data(en_dev, fr_dev)<br>\ntrain_set = read_train_data(en_train, fr_train, num_workers, FLAGS.max_train_data_size)<br>\ntrain_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]<br>\ntrain_total_size = float(sum(train_bucket_sizes))</p>\n<p>train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]<br>\nprint(\"finish to load data\")<br>\nsess = model.session<br>\nwhile True:<br>\nrandom_number_01 = np.random.random_sample()<br>\nbucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] &gt; random_number_01])</p>\n<pre><code># Get a batch and make a step.\nstart_time = time.time()\nencoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n_, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\nstep_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\nstep_time_now = (time.time() - start_time)\nloss += step_loss / FLAGS.steps_per_checkpoint\nstep_loss_now = step_loss\ncurrent_step += 1\nprint(\"step: %d\" % current_step);\n\nif (True):\n  perplexity = math.exp(step_loss) if step_loss &lt; 300 else float('inf')\n  print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n         \"%.2f\" % (model.global_step.eval(sess), model.learning_rate.eval(sess), step_time_now, perplexity))\n  previous_losses.append(step_loss)\n\n  checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\n  model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n  if (current_step %  FLAGS.steps_per_checkpoint == 0):\n    for bucket_id in xrange(len(_buckets)):\n      if len(dev_set[bucket_id]) == 0:\n        print(\"  eval: empty bucket %d\" % (bucket_id))\n        continue\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\n      _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n      eval_ppx = math.exp(eval_loss) if eval_loss &lt; 300 else float('inf')\n      print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n    step_time, loss = 0.0, 0.0\n  sys.stdout.flush()\n</code></pre>\n<p>def read_train_data(source_path, target_path, num_workers, max_size=None):<br>\ndata_set = [[] for _ in _buckets]<br>\nwith tf.gfile.GFile(source_path, mode=\"r\") as source_file:<br>\nwith tf.gfile.GFile(target_path, mode=\"r\") as target_file:<br>\nsource, target = source_file.readline(), target_file.readline()<br>\ncounter = 0<br>\nwhile source and target and (not max_size or counter &lt; max_size):<br>\ncounter += 1<br>\nif counter % 1000000 == 0:<br>\nprint(\"  reading data line %d\" % counter)<br>\nsys.stdout.flush()<br>\nsource_ids = [int(x) for x in source.split()]<br>\ntarget_ids = [int(x) for x in target.split()]<br>\ntarget_ids.append(data_utils.EOS_ID)<br>\n# if (counter % num_workers == 0):<br>\nif (True):<br>\nfor bucket_id, (source_size, target_size) in enumerate(_buckets):<br>\nif len(source_ids) &lt; source_size and len(target_ids) &lt; target_size:<br>\ndata_set[bucket_id].append([source_ids, target_ids])<br>\nbreak<br>\nsource, target = source_file.readline(), target_file.readline()<br>\nreturn data_set</p>\n<p>def read_data(source_path, target_path, max_size=None):<br>\ndata_set = [[] for _ in _buckets]<br>\nwith tf.gfile.GFile(source_path, mode=\"r\") as source_file:<br>\nwith tf.gfile.GFile(target_path, mode=\"r\") as target_file:<br>\nsource, target = source_file.readline(), target_file.readline()<br>\ncounter = 0<br>\nwhile source and target and (not max_size or counter &lt; max_size):<br>\ncounter += 1<br>\nif counter % 100000 == 0:<br>\nprint(\"  reading data line %d\" % counter)<br>\nsys.stdout.flush()<br>\nsource_ids = [int(x) for x in source.split()]<br>\ntarget_ids = [int(x) for x in target.split()]<br>\ntarget_ids.append(data_utils.EOS_ID)<br>\nfor bucket_id, (source_size, target_size) in enumerate(_buckets):<br>\nif len(source_ids) &lt; source_size and len(target_ids) &lt; target_size:<br>\ndata_set[bucket_id].append([source_ids, target_ids])<br>\nbreak<br>\nsource, target = source_file.readline(), target_file.readline()<br>\nreturn data_set</p>\n<p>if <strong>name</strong> == \"<strong>main</strong>\":<br>\ntf.app.run()</p>\n<p>seq2seq_model.py</p>\n<h1>Copyright 2015 The TensorFlow Authors. All Rights Reserved.</h1>\n<h1></h1>\n<h1>Licensed under the Apache License, Version 2.0 (the \"License\");</h1>\n<h1>you may not use this file except in compliance with the License.</h1>\n<h1>You may obtain a copy of the License at</h1>\n<h1></h1>\n<h1><a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\">http://www.apache.org/licenses/LICENSE-2.0</a></h1>\n<h1></h1>\n<h1>Unless required by applicable law or agreed to in writing, software</h1>\n<h1>distributed under the License is distributed on an \"AS IS\" BASIS,</h1>\n<h1>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</h1>\n<h1>See the License for the specific language governing permissions and</h1>\n<h1>limitations under the License.</h1>\n<h1></h1>\n<p>\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"</p>\n<p>from <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import random</p>\n<p>import numpy as np<br>\nfrom six.moves import xrange  # pylint: disable=redefined-builtin<br>\nimport tensorflow as tf</p>\n<p>from tensorflow.models.rnn.translate import data_utils</p>\n<p>class Seq2SeqModel(object):</p>\n<p>def <strong>init</strong>(self,<br>\nserver,<br>\nsource_vocab_size,<br>\ntarget_vocab_size,<br>\nbuckets,<br>\nsize,<br>\nnum_layers,<br>\nmax_gradient_norm,<br>\nbatch_size,<br>\nlearning_rate,<br>\nlearning_rate_decay_factor,<br>\nnum_workers=2,<br>\ntask_index=0,<br>\nreplicas_to_aggregate=2,<br>\nuse_lstm=False,<br>\nnum_samples=512,<br>\nforward_only=False,<br>\ndtype=tf.float32):</p>\n<pre><code>self.graph = tf.Graph()\nself.server = server\nself.source_vocab_size = source_vocab_size\nself.target_vocab_size = target_vocab_size\nself.buckets = buckets\nself.batch_size = batch_size\nself.num_workers = num_workers \nself.task_index = task_index\nself.is_chief = (task_index == 0)\nself.num_workers = num_workers\nself.replicas_to_aggregate = replicas_to_aggregate\n\nwith self.graph.as_default():\n  with tf.device(\"/job:ps/task:0\"): \n    self.global_step = tf.Variable(0, trainable=False)\n    self.learning_rate = tf.Variable(\n        float(learning_rate), trainable=False, dtype=dtype)\n\n  output_projection = None\n  softmax_loss_function = None\n  # Sampled softmax only makes sense if we sample less than vocabulary size.\n  if num_samples &gt; 0 and num_samples &lt; self.target_vocab_size:\n    with tf.device(\"/job:ps/task:0\"): \n      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n      w = tf.transpose(w_t)\n      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n    output_projection = (w, b)\n\n    def sampled_loss(inputs, labels):\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n        labels = tf.reshape(labels, [-1, 1])\n        # We need to compute the sampled_softmax_loss using 32bit floats to\n        # avoid numerical instabilities.\n        local_w_t = tf.cast(w_t, tf.float32)\n        local_b = tf.cast(b, tf.float32)\n        local_inputs = tf.cast(inputs, tf.float32)\n        return tf.cast(\n            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n                                       num_samples, self.target_vocab_size),\n            dtype)\n    softmax_loss_function = sampled_loss\n\n  # Create the internal multi-layer cell for our RNN.\n  with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n    single_cell = tf.nn.rnn_cell.GRUCell(size)\n    if use_lstm:\n      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n    cell = single_cell\n    if num_layers &gt; 1:\n      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n  # The seq2seq function: we use embedding for the input and attention.\n  def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n    with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n      return tf.nn.seq2seq.embedding_attention_seq2seq(\n          encoder_inputs,\n          decoder_inputs,\n          cell,\n          num_encoder_symbols=source_vocab_size,\n          num_decoder_symbols=target_vocab_size,\n          embedding_size=size,\n          output_projection=output_projection,\n          feed_previous=do_decode,\n          dtype=dtype)\n\n  # Feeds for inputs.\n  with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n    self.encoder_inputs = []\n    self.decoder_inputs = []\n    self.target_weights = []\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=\"encoder{0}\".format(i)))\n    for i in xrange(buckets[-1][1] + 1):\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=\"decoder{0}\".format(i)))\n      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n                                                name=\"weight{0}\".format(i)))\n\n    # Our targets are decoder inputs shifted by one.\n    targets = [self.decoder_inputs[i + 1]\n               for i in xrange(len(self.decoder_inputs) - 1)]\n    if forward_only:\n      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          self.outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in self.outputs[b]\n          ]\n    else:\n      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    # Gradients and SGD update operation for training the model.\n  with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n    params = tf.trainable_variables()\n    self.gradient_norms = []\n    self.updates = []\n  #sync_rep_opt = tf.train.AdamOptimizer(self.learning_rate)\n    sgd_opt = tf.train.GradientDescentOptimizer(2.0)\n    sync_rep_opt = tf.train.SyncReplicasOptimizerV2(\n        sgd_opt, replicas_to_aggregate=replicas_to_aggregate,\n        total_num_replicas=num_workers)\n    for b in xrange(len(buckets)):\n      gradients = tf.gradients(self.losses[b], params)\n      clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                       max_gradient_norm)\n      self.gradient_norms.append(norm)\n      self.updates.append(sync_rep_opt.apply_gradients(\n          zip(clipped_gradients, params), global_step=self.global_step))\n\n    init_op = tf.global_variables_initializer()\n    local_init_op = sync_rep_opt.local_step_init_op\n    if self.is_chief:\n      local_init_op = sync_rep_opt.chief_init_op\n    ready_for_local_init_op = sync_rep_opt.ready_for_local_init_op\n\n    # Chief_queue_runner\n    chief_queue_runner = sync_rep_opt.get_chief_queue_runner()\n    sync_init_op = sync_rep_opt.get_init_tokens_op(num_workers)\n\n# Creates session for chief.\nsupervisor = tf.train.Supervisor(\n    graph=self.graph,\n    is_chief=self.is_chief,\n    recovery_wait_secs=1,\n    init_op=init_op,\n    local_init_op=local_init_op,\n    ready_for_local_init_op=ready_for_local_init_op)\n\nsess_config = tf.ConfigProto(\n    allow_soft_placement=True,\n    log_device_placement=True,\n    device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % self.task_index])\n\n\nself.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\n\n# Chief should execute the sync_init_op and start the chief queue runner.\nif self.is_chief:\n  self.session.run(sync_init_op)\n  supervisor.StartQueueRunners(self.session, [chief_queue_runner])\n\nif self.is_chief:\n  self.session.run(sync_init_op)\n  supervisor.StartQueueRunners(self.session, [chief_queue_runner])\n</code></pre>\n<p>def step(self, session, encoder_inputs, decoder_inputs, target_weights,<br>\nbucket_id, forward_only):<br>\n\"\"\"Run a step of the model feeding the given inputs.</p>\n<pre><code>Args:\n  session: tensorflow session to use.\n  encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n  decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n  target_weights: list of numpy float vectors to feed as target weights.\n  bucket_id: which bucket of the model to use.\n  forward_only: whether to do the backward step or only forward.\n\nReturns:\n  A triple consisting of gradient norm (or None if we did not do backward),\n  average perplexity, and the outputs.\n\nRaises:\n  ValueError: if length of encoder_inputs, decoder_inputs, or\n    target_weights disagrees with bucket size for the specified bucket_id.\n\"\"\"\n# Check if the sizes match.\nencoder_size, decoder_size = self.buckets[bucket_id]\nif len(encoder_inputs) != encoder_size:\n  raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(encoder_inputs), encoder_size))\nif len(decoder_inputs) != decoder_size:\n  raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(decoder_inputs), decoder_size))\nif len(target_weights) != decoder_size:\n  raise ValueError(\"Weights length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(target_weights), decoder_size))\n\n# Input feed: encoder inputs, decoder inputs, target_weights, as provided.\ninput_feed = {}\nfor l in xrange(encoder_size):\n  input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\nfor l in xrange(decoder_size):\n  input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n  input_feed[self.target_weights[l].name] = target_weights[l]\n\n# Since our targets are decoder inputs shifted by one, we need one more.\nlast_target = self.decoder_inputs[decoder_size].name\ninput_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\noutputs = session.run(output_feed, input_feed)\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n</code></pre>\n<p>def get_batch(self, data, bucket_id):<br>\n\"\"\"Get a random batch of data from the specified bucket, prepare for step.</p>\n<pre><code>To feed data in step(..) it must be a list of batch-major vectors, while\ndata here contains single length-major cases. So the main logic of this\nfunction is to re-index data cases to be in the proper format for feeding.\n\nArgs:\n  data: a tuple of size len(self.buckets) in which each element contains\n    lists of pairs of input and output data that we use to create a batch.\n  bucket_id: integer, which bucket to get the batch for.\n\nReturns:\n  The triple (encoder_inputs, decoder_inputs, target_weights) for\n  the constructed batch that has the proper format to call step(...) later.\n\"\"\"\nencoder_size, decoder_size = self.buckets[bucket_id]\nencoder_inputs, decoder_inputs = [], []\n\n# Get a random batch of encoder and decoder inputs from data,\n# pad them if needed, reverse encoder inputs and add GO to decoder.\nfor _ in xrange(self.batch_size):\n  encoder_input, decoder_input = random.choice(data[bucket_id])\n\n  # Encoder inputs are padded and then reversed.\n  encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n  encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n  # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n  decoder_pad_size = decoder_size - len(decoder_input) - 1\n  decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                        [data_utils.PAD_ID] * decoder_pad_size)\n\n# Now we create batch-major vectors from the data selected above.\nbatch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n# Batch encoder inputs are just re-indexed encoder_inputs.\nfor length_idx in xrange(encoder_size):\n  batch_encoder_inputs.append(\n      np.array([encoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n# Batch decoder inputs are re-indexed decoder_inputs, we create weights.\nfor length_idx in xrange(decoder_size):\n  batch_decoder_inputs.append(\n      np.array([decoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n  # Create target_weights to be 0 for targets that are padding.\n  batch_weight = np.ones(self.batch_size, dtype=np.float32)\n  for batch_idx in xrange(self.batch_size):\n    # We set weight to 0 if the corresponding target is a PAD symbol.\n    # The corresponding target is decoder_input shifted by 1 forward.\n    if length_idx &lt; decoder_size - 1:\n      target = decoder_inputs[batch_idx][length_idx + 1]\n    if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n      batch_weight[batch_idx] = 0.0\n  batch_weights.append(batch_weight)\nreturn batch_encoder_inputs, batch_decoder_inputs, batch_weights\n</code></pre>", "body_text": "Now I want to change seq2seq to distributed mode used SyncReplicasOptimizerV2. I've been trying for a few days\uff0cbut still can't work out. this is my code,one file is seq2seq_train.py,on file is seq2seq_model.py\nI try to run in 1 ps and 2 worker\nthe error is:\nTraceback (most recent call last):\nFile \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 188, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\nsys.exit(main(sys.argv[:1] + flags_passthrough))\nFile \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 82, in main\ntask_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\nFile \"/home/test/replicas_seq2seq_v4/seq2seq_model.py\", line 221, in init\nself.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 720, in prepare_or_wait_for_session\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 251, in prepare_session\nself._local_init_op, msg))\nRuntimeError: Init operations did not make model ready.  Init op: init, init fn: None, local_init_op: name: \"sync_replicas_3/group_deps\"\nop: \"NoOp\"\ninput: \"^sync_replicas_3/group_deps/NoOp\"\ninput: \"^sync_replicas_3/group_deps/NoOp_1\"\ninput: \"^sync_replicas_3/group_deps/NoOp_2\"\ndevice: \"/job:worker/task:0\"\n, error: Variables not initialized: sync_rep_local_step, sync_rep_local_step_1, sync_rep_local_step_2\nps:\npython seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=ps --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --max_train_data_size=50000\nworker1:\npython seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --\nworker2:\npython seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=1 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --\nseq2seq_train.py\nfrom future import absolute_import\nfrom future import division\nfrom future import print_function\nimport tempfile\nimport math\nimport os\nimport random\nimport sys\nimport time\nimport json\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.models.rnn.translate import data_utils\n#from tensorflow.models.rnn.translate import seq2seq_model\nimport seq2seq_model\nFlags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n\"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n\"Comma-separated list of hostname:port pairs\")\nFlags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\ntf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n\"Learning rate decays by this much.\")\ntf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n\"Clip gradients to this norm.\")\ntf.app.flags.DEFINE_integer(\"batch_size\", 64,\n\"Batch size to use during training.\")\ntf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\ntf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\ntf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\ntf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")\ntf.app.flags.DEFINE_string(\"data_dir\", \"/tmp/data\", \"Data directory\")\ntf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training directory.\")\ntf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n\"Limit on the size of training data (0: no limit).\")\ntf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\n\"How many training steps to do per checkpoint.\")\ntf.app.flags.DEFINE_boolean(\"decode\", False,\n\"Set to True for interactive decoding.\")\ntf.app.flags.DEFINE_boolean(\"self_test\", False,\n\"Run a self-test if this is set to True.\")\nFLAGS = tf.app.flags.FLAGS\n_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\ndef main(_):\nif FLAGS.job_name is None or FLAGS.job_name == \"\":\nraise ValueError(\"Must specify an explicit job_name\")\nif FLAGS.task_index is None or FLAGS.task_index ==\"\":\nraise ValueError(\"Must specify an explicit task_index\")\nps_hosts = FLAGS.ps_hosts.split(\",\")\nworker_hosts = FLAGS.worker_hosts.split(\",\")\nCreate a cluster from the parameter server and worker hosts.\ncluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\nCreate and start a server for the local task.\nserver = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\nnum_workers = len(worker_hosts)\nserver = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\nif FLAGS.job_name == \"ps\":\nserver.join()\nreturn\nis_chief = (FLAGS.task_index == 0)\nmodel = seq2seq_model.Seq2SeqModel(server, FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\nFLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\nFLAGS.learning_rate, FLAGS.learning_rate_decay_factor,num_workers = num_workers,\ntask_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\nstep_time, loss = 0.0, 0.0\ncurrent_step = 0\nprevious_losses = []\nprint(\"query -> title:\")\nen_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.en\"\nfr_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.fr\"\nen_dev = FLAGS.data_dir + \"/newstest2013.ids40000.en\"\nfr_dev = FLAGS.data_dir + \"/newstest2013.ids40000.fr\"\nRead data into buckets and compute their sizes.\nprint (\"Reading development and training data (limit: %d).\" % FLAGS.max_train_data_size)\ndev_set = read_data(en_dev, fr_dev)\ntrain_set = read_train_data(en_train, fr_train, num_workers, FLAGS.max_train_data_size)\ntrain_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\ntrain_total_size = float(sum(train_bucket_sizes))\ntrain_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]\nprint(\"finish to load data\")\nsess = model.session\nwhile True:\nrandom_number_01 = np.random.random_sample()\nbucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n# Get a batch and make a step.\nstart_time = time.time()\nencoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n_, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\nstep_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\nstep_time_now = (time.time() - start_time)\nloss += step_loss / FLAGS.steps_per_checkpoint\nstep_loss_now = step_loss\ncurrent_step += 1\nprint(\"step: %d\" % current_step);\n\nif (True):\n  perplexity = math.exp(step_loss) if step_loss < 300 else float('inf')\n  print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n         \"%.2f\" % (model.global_step.eval(sess), model.learning_rate.eval(sess), step_time_now, perplexity))\n  previous_losses.append(step_loss)\n\n  checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\n  model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n  if (current_step %  FLAGS.steps_per_checkpoint == 0):\n    for bucket_id in xrange(len(_buckets)):\n      if len(dev_set[bucket_id]) == 0:\n        print(\"  eval: empty bucket %d\" % (bucket_id))\n        continue\n      encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\n      _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n      eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n      print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n    step_time, loss = 0.0, 0.0\n  sys.stdout.flush()\n\ndef read_train_data(source_path, target_path, num_workers, max_size=None):\ndata_set = [[] for _ in _buckets]\nwith tf.gfile.GFile(source_path, mode=\"r\") as source_file:\nwith tf.gfile.GFile(target_path, mode=\"r\") as target_file:\nsource, target = source_file.readline(), target_file.readline()\ncounter = 0\nwhile source and target and (not max_size or counter < max_size):\ncounter += 1\nif counter % 1000000 == 0:\nprint(\"  reading data line %d\" % counter)\nsys.stdout.flush()\nsource_ids = [int(x) for x in source.split()]\ntarget_ids = [int(x) for x in target.split()]\ntarget_ids.append(data_utils.EOS_ID)\n# if (counter % num_workers == 0):\nif (True):\nfor bucket_id, (source_size, target_size) in enumerate(_buckets):\nif len(source_ids) < source_size and len(target_ids) < target_size:\ndata_set[bucket_id].append([source_ids, target_ids])\nbreak\nsource, target = source_file.readline(), target_file.readline()\nreturn data_set\ndef read_data(source_path, target_path, max_size=None):\ndata_set = [[] for _ in _buckets]\nwith tf.gfile.GFile(source_path, mode=\"r\") as source_file:\nwith tf.gfile.GFile(target_path, mode=\"r\") as target_file:\nsource, target = source_file.readline(), target_file.readline()\ncounter = 0\nwhile source and target and (not max_size or counter < max_size):\ncounter += 1\nif counter % 100000 == 0:\nprint(\"  reading data line %d\" % counter)\nsys.stdout.flush()\nsource_ids = [int(x) for x in source.split()]\ntarget_ids = [int(x) for x in target.split()]\ntarget_ids.append(data_utils.EOS_ID)\nfor bucket_id, (source_size, target_size) in enumerate(_buckets):\nif len(source_ids) < source_size and len(target_ids) < target_size:\ndata_set[bucket_id].append([source_ids, target_ids])\nbreak\nsource, target = source_file.readline(), target_file.readline()\nreturn data_set\nif name == \"main\":\ntf.app.run()\nseq2seq_model.py\nCopyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\nfrom future import absolute_import\nfrom future import division\nfrom future import print_function\nimport random\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.models.rnn.translate import data_utils\nclass Seq2SeqModel(object):\ndef init(self,\nserver,\nsource_vocab_size,\ntarget_vocab_size,\nbuckets,\nsize,\nnum_layers,\nmax_gradient_norm,\nbatch_size,\nlearning_rate,\nlearning_rate_decay_factor,\nnum_workers=2,\ntask_index=0,\nreplicas_to_aggregate=2,\nuse_lstm=False,\nnum_samples=512,\nforward_only=False,\ndtype=tf.float32):\nself.graph = tf.Graph()\nself.server = server\nself.source_vocab_size = source_vocab_size\nself.target_vocab_size = target_vocab_size\nself.buckets = buckets\nself.batch_size = batch_size\nself.num_workers = num_workers \nself.task_index = task_index\nself.is_chief = (task_index == 0)\nself.num_workers = num_workers\nself.replicas_to_aggregate = replicas_to_aggregate\n\nwith self.graph.as_default():\n  with tf.device(\"/job:ps/task:0\"): \n    self.global_step = tf.Variable(0, trainable=False)\n    self.learning_rate = tf.Variable(\n        float(learning_rate), trainable=False, dtype=dtype)\n\n  output_projection = None\n  softmax_loss_function = None\n  # Sampled softmax only makes sense if we sample less than vocabulary size.\n  if num_samples > 0 and num_samples < self.target_vocab_size:\n    with tf.device(\"/job:ps/task:0\"): \n      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n      w = tf.transpose(w_t)\n      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n    output_projection = (w, b)\n\n    def sampled_loss(inputs, labels):\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n        labels = tf.reshape(labels, [-1, 1])\n        # We need to compute the sampled_softmax_loss using 32bit floats to\n        # avoid numerical instabilities.\n        local_w_t = tf.cast(w_t, tf.float32)\n        local_b = tf.cast(b, tf.float32)\n        local_inputs = tf.cast(inputs, tf.float32)\n        return tf.cast(\n            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n                                       num_samples, self.target_vocab_size),\n            dtype)\n    softmax_loss_function = sampled_loss\n\n  # Create the internal multi-layer cell for our RNN.\n  with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n    single_cell = tf.nn.rnn_cell.GRUCell(size)\n    if use_lstm:\n      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n    cell = single_cell\n    if num_layers > 1:\n      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n  # The seq2seq function: we use embedding for the input and attention.\n  def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n    with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n      return tf.nn.seq2seq.embedding_attention_seq2seq(\n          encoder_inputs,\n          decoder_inputs,\n          cell,\n          num_encoder_symbols=source_vocab_size,\n          num_decoder_symbols=target_vocab_size,\n          embedding_size=size,\n          output_projection=output_projection,\n          feed_previous=do_decode,\n          dtype=dtype)\n\n  # Feeds for inputs.\n  with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n    self.encoder_inputs = []\n    self.decoder_inputs = []\n    self.target_weights = []\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=\"encoder{0}\".format(i)))\n    for i in xrange(buckets[-1][1] + 1):\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=\"decoder{0}\".format(i)))\n      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n                                                name=\"weight{0}\".format(i)))\n\n    # Our targets are decoder inputs shifted by one.\n    targets = [self.decoder_inputs[i + 1]\n               for i in xrange(len(self.decoder_inputs) - 1)]\n    if forward_only:\n      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          self.outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in self.outputs[b]\n          ]\n    else:\n      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    # Gradients and SGD update operation for training the model.\n  with tf.device(\"/job:worker/task:\"+str(self.task_index)):\n    params = tf.trainable_variables()\n    self.gradient_norms = []\n    self.updates = []\n  #sync_rep_opt = tf.train.AdamOptimizer(self.learning_rate)\n    sgd_opt = tf.train.GradientDescentOptimizer(2.0)\n    sync_rep_opt = tf.train.SyncReplicasOptimizerV2(\n        sgd_opt, replicas_to_aggregate=replicas_to_aggregate,\n        total_num_replicas=num_workers)\n    for b in xrange(len(buckets)):\n      gradients = tf.gradients(self.losses[b], params)\n      clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                       max_gradient_norm)\n      self.gradient_norms.append(norm)\n      self.updates.append(sync_rep_opt.apply_gradients(\n          zip(clipped_gradients, params), global_step=self.global_step))\n\n    init_op = tf.global_variables_initializer()\n    local_init_op = sync_rep_opt.local_step_init_op\n    if self.is_chief:\n      local_init_op = sync_rep_opt.chief_init_op\n    ready_for_local_init_op = sync_rep_opt.ready_for_local_init_op\n\n    # Chief_queue_runner\n    chief_queue_runner = sync_rep_opt.get_chief_queue_runner()\n    sync_init_op = sync_rep_opt.get_init_tokens_op(num_workers)\n\n# Creates session for chief.\nsupervisor = tf.train.Supervisor(\n    graph=self.graph,\n    is_chief=self.is_chief,\n    recovery_wait_secs=1,\n    init_op=init_op,\n    local_init_op=local_init_op,\n    ready_for_local_init_op=ready_for_local_init_op)\n\nsess_config = tf.ConfigProto(\n    allow_soft_placement=True,\n    log_device_placement=True,\n    device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % self.task_index])\n\n\nself.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\n\n# Chief should execute the sync_init_op and start the chief queue runner.\nif self.is_chief:\n  self.session.run(sync_init_op)\n  supervisor.StartQueueRunners(self.session, [chief_queue_runner])\n\nif self.is_chief:\n  self.session.run(sync_init_op)\n  supervisor.StartQueueRunners(self.session, [chief_queue_runner])\n\ndef step(self, session, encoder_inputs, decoder_inputs, target_weights,\nbucket_id, forward_only):\n\"\"\"Run a step of the model feeding the given inputs.\nArgs:\n  session: tensorflow session to use.\n  encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n  decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n  target_weights: list of numpy float vectors to feed as target weights.\n  bucket_id: which bucket of the model to use.\n  forward_only: whether to do the backward step or only forward.\n\nReturns:\n  A triple consisting of gradient norm (or None if we did not do backward),\n  average perplexity, and the outputs.\n\nRaises:\n  ValueError: if length of encoder_inputs, decoder_inputs, or\n    target_weights disagrees with bucket size for the specified bucket_id.\n\"\"\"\n# Check if the sizes match.\nencoder_size, decoder_size = self.buckets[bucket_id]\nif len(encoder_inputs) != encoder_size:\n  raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(encoder_inputs), encoder_size))\nif len(decoder_inputs) != decoder_size:\n  raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(decoder_inputs), decoder_size))\nif len(target_weights) != decoder_size:\n  raise ValueError(\"Weights length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(target_weights), decoder_size))\n\n# Input feed: encoder inputs, decoder inputs, target_weights, as provided.\ninput_feed = {}\nfor l in xrange(encoder_size):\n  input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\nfor l in xrange(decoder_size):\n  input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n  input_feed[self.target_weights[l].name] = target_weights[l]\n\n# Since our targets are decoder inputs shifted by one, we need one more.\nlast_target = self.decoder_inputs[decoder_size].name\ninput_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\noutputs = session.run(output_feed, input_feed)\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\ndef get_batch(self, data, bucket_id):\n\"\"\"Get a random batch of data from the specified bucket, prepare for step.\nTo feed data in step(..) it must be a list of batch-major vectors, while\ndata here contains single length-major cases. So the main logic of this\nfunction is to re-index data cases to be in the proper format for feeding.\n\nArgs:\n  data: a tuple of size len(self.buckets) in which each element contains\n    lists of pairs of input and output data that we use to create a batch.\n  bucket_id: integer, which bucket to get the batch for.\n\nReturns:\n  The triple (encoder_inputs, decoder_inputs, target_weights) for\n  the constructed batch that has the proper format to call step(...) later.\n\"\"\"\nencoder_size, decoder_size = self.buckets[bucket_id]\nencoder_inputs, decoder_inputs = [], []\n\n# Get a random batch of encoder and decoder inputs from data,\n# pad them if needed, reverse encoder inputs and add GO to decoder.\nfor _ in xrange(self.batch_size):\n  encoder_input, decoder_input = random.choice(data[bucket_id])\n\n  # Encoder inputs are padded and then reversed.\n  encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n  encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n  # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n  decoder_pad_size = decoder_size - len(decoder_input) - 1\n  decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                        [data_utils.PAD_ID] * decoder_pad_size)\n\n# Now we create batch-major vectors from the data selected above.\nbatch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n# Batch encoder inputs are just re-indexed encoder_inputs.\nfor length_idx in xrange(encoder_size):\n  batch_encoder_inputs.append(\n      np.array([encoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n# Batch decoder inputs are re-indexed decoder_inputs, we create weights.\nfor length_idx in xrange(decoder_size):\n  batch_decoder_inputs.append(\n      np.array([decoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n  # Create target_weights to be 0 for targets that are padding.\n  batch_weight = np.ones(self.batch_size, dtype=np.float32)\n  for batch_idx in xrange(self.batch_size):\n    # We set weight to 0 if the corresponding target is a PAD symbol.\n    # The corresponding target is decoder_input shifted by 1 forward.\n    if length_idx < decoder_size - 1:\n      target = decoder_inputs[batch_idx][length_idx + 1]\n    if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n      batch_weight[batch_idx] = 0.0\n  batch_weights.append(batch_weight)\nreturn batch_encoder_inputs, batch_decoder_inputs, batch_weights", "body": "Now I want to change seq2seq to distributed mode used SyncReplicasOptimizerV2. I've been trying for a few days\uff0cbut still can't work out. this is my code,one file is seq2seq_train.py,on file is seq2seq_model.py\r\nI try to run in 1 ps and 2 worker\r\n\r\nthe error is:\r\nTraceback (most recent call last):\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 188, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_train.py\", line 82, in main\r\n    task_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\r\n  File \"/home/test/replicas_seq2seq_v4/seq2seq_model.py\", line 221, in __init__\r\n    self.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 720, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 251, in prepare_session\r\n    self._local_init_op, msg))\r\nRuntimeError: Init operations did not make model ready.  Init op: init, init fn: None, local_init_op: name: \"sync_replicas_3/group_deps\"\r\nop: \"NoOp\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp_1\"\r\ninput: \"^sync_replicas_3/group_deps/NoOp_2\"\r\ndevice: \"/job:worker/task:0\"\r\n, error: Variables not initialized: sync_rep_local_step, sync_rep_local_step_1, sync_rep_local_step_2\r\n\r\nps:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=ps --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --max_train_data_size=50000\r\n\r\nworker1:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=0 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --\r\n\r\nworker2:\r\n python seq2seq_train.py --ps_hosts=127.0.0.1:2240 --worker_hosts=127.0.0.1:2230,127.0.0.1:2242 --job_name=worker --task_index=1 --data_dir=data1 --train_dir=result  --batch_size=128 --size=1024 --num_layers=2 --steps_per_checkpoint=5 --en_vocab_size=40000 --fr_vocab_size=40000 --learning_rate=0.5 --learning_rate_decay_factor=0.95 --max_gradient_norm=2.0 --\r\n\r\n\r\n\r\nseq2seq_train.py\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tempfile\r\nimport math\r\nimport os\r\nimport random\r\nimport sys\r\nimport time\r\nimport json\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.models.rnn.translate import data_utils\r\n#from tensorflow.models.rnn.translate import seq2seq_model\r\nimport seq2seq_model\r\n\r\n# Flags for defining the tf.train.ClusterSpec\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\r\n                           \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\r\n                           \"Comma-separated list of hostname:port pairs\")\r\n\r\n# Flags for defining the tf.train.Server\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\n\r\ntf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\r\ntf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\r\n                          \"Learning rate decays by this much.\")\r\ntf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\r\n                          \"Clip gradients to this norm.\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 64,\r\n                            \"Batch size to use during training.\")\r\ntf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\r\ntf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\r\ntf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\r\ntf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")\r\ntf.app.flags.DEFINE_string(\"data_dir\", \"/tmp/data\", \"Data directory\")\r\ntf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training directory.\")\r\ntf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\r\n                            \"Limit on the size of training data (0: no limit).\")\r\ntf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\r\n                            \"How many training steps to do per checkpoint.\")\r\ntf.app.flags.DEFINE_boolean(\"decode\", False,\r\n                            \"Set to True for interactive decoding.\")\r\ntf.app.flags.DEFINE_boolean(\"self_test\", False,\r\n                            \"Run a self-test if this is set to True.\")\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\r\n\r\ndef main(_):\r\n  if FLAGS.job_name is None or FLAGS.job_name == \"\":\r\n    raise ValueError(\"Must specify an explicit `job_name`\")\r\n  if FLAGS.task_index is None or FLAGS.task_index ==\"\":\r\n    raise ValueError(\"Must specify an explicit `task_index`\")\r\n\r\n\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n  num_workers = len(worker_hosts)\r\n\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n    return\r\n  is_chief = (FLAGS.task_index == 0)\r\n  model = seq2seq_model.Seq2SeqModel(server, FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\r\n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\r\n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,num_workers = num_workers, \r\n      task_index = FLAGS.task_index ,replicas_to_aggregate=num_workers, forward_only=False)\r\n  step_time, loss = 0.0, 0.0\r\n  current_step = 0\r\n  previous_losses = []\r\n\r\n  print(\"query -> title:\")\r\n  en_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.en\"\r\n  fr_train = FLAGS.data_dir + \"/giga-fren.release2.ids40000.fr\"\r\n  en_dev = FLAGS.data_dir + \"/newstest2013.ids40000.en\"\r\n  fr_dev = FLAGS.data_dir + \"/newstest2013.ids40000.fr\"\r\n\r\n  # Read data into buckets and compute their sizes.\r\n  print (\"Reading development and training data (limit: %d).\" % FLAGS.max_train_data_size)\r\n  dev_set = read_data(en_dev, fr_dev)\r\n  train_set = read_train_data(en_train, fr_train, num_workers, FLAGS.max_train_data_size)\r\n  train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\r\n  train_total_size = float(sum(train_bucket_sizes))\r\n\r\n  train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]\r\n  print(\"finish to load data\")\r\n  sess = model.session\r\n  while True:\r\n    random_number_01 = np.random.random_sample()\r\n    bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\r\n\r\n    # Get a batch and make a step.\r\n    start_time = time.time()\r\n    encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\r\n    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\r\n    step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\r\n    step_time_now = (time.time() - start_time)\r\n    loss += step_loss / FLAGS.steps_per_checkpoint\r\n    step_loss_now = step_loss\r\n    current_step += 1\r\n    print(\"step: %d\" % current_step);\r\n\r\n    if (True):\r\n      perplexity = math.exp(step_loss) if step_loss < 300 else float('inf')\r\n      print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\r\n             \"%.2f\" % (model.global_step.eval(sess), model.learning_rate.eval(sess), step_time_now, perplexity))\r\n      previous_losses.append(step_loss)\r\n\r\n      checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\r\n      model.saver.save(sess, checkpoint_path, global_step=model.global_step)\r\n      if (current_step %  FLAGS.steps_per_checkpoint == 0):\r\n        for bucket_id in xrange(len(_buckets)):\r\n          if len(dev_set[bucket_id]) == 0:\r\n            print(\"  eval: empty bucket %d\" % (bucket_id))\r\n            continue\r\n          encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\r\n          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\r\n          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\r\n          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\r\n        step_time, loss = 0.0, 0.0\r\n      sys.stdout.flush()\r\n\r\n\r\ndef read_train_data(source_path, target_path, num_workers, max_size=None):\r\n  data_set = [[] for _ in _buckets]\r\n  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\r\n    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\r\n      source, target = source_file.readline(), target_file.readline()\r\n      counter = 0\r\n      while source and target and (not max_size or counter < max_size):\r\n        counter += 1\r\n        if counter % 1000000 == 0:\r\n          print(\"  reading data line %d\" % counter)\r\n          sys.stdout.flush()\r\n        source_ids = [int(x) for x in source.split()]\r\n        target_ids = [int(x) for x in target.split()]\r\n        target_ids.append(data_utils.EOS_ID)\r\n        # if (counter % num_workers == 0):\r\n        if (True):\r\n          for bucket_id, (source_size, target_size) in enumerate(_buckets):\r\n            if len(source_ids) < source_size and len(target_ids) < target_size:\r\n              data_set[bucket_id].append([source_ids, target_ids])\r\n              break\r\n        source, target = source_file.readline(), target_file.readline()\r\n  return data_set\r\n\r\ndef read_data(source_path, target_path, max_size=None):\r\n  data_set = [[] for _ in _buckets]\r\n  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\r\n    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\r\n      source, target = source_file.readline(), target_file.readline()\r\n      counter = 0\r\n      while source and target and (not max_size or counter < max_size):\r\n        counter += 1\r\n        if counter % 100000 == 0:\r\n          print(\"  reading data line %d\" % counter)\r\n          sys.stdout.flush()\r\n        source_ids = [int(x) for x in source.split()]\r\n        target_ids = [int(x) for x in target.split()]\r\n        target_ids.append(data_utils.EOS_ID)\r\n        for bucket_id, (source_size, target_size) in enumerate(_buckets):\r\n          if len(source_ids) < source_size and len(target_ids) < target_size:\r\n            data_set[bucket_id].append([source_ids, target_ids])\r\n            break\r\n        source, target = source_file.readline(), target_file.readline()\r\n  return data_set\r\n\r\nif __name__ == \"__main__\":\r\n      tf.app.run()\r\n\r\nseq2seq_model.py\r\n\r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# \r\n\r\n\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport random\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.models.rnn.translate import data_utils\r\n\r\n\r\nclass Seq2SeqModel(object):\r\n\r\n  def __init__(self,\r\n               server,\r\n               source_vocab_size,\r\n               target_vocab_size,\r\n               buckets,\r\n               size,\r\n               num_layers,\r\n               max_gradient_norm,\r\n               batch_size,\r\n               learning_rate,\r\n               learning_rate_decay_factor,\r\n               num_workers=2, \r\n               task_index=0,\r\n               replicas_to_aggregate=2,\r\n               use_lstm=False,\r\n               num_samples=512,\r\n               forward_only=False,\r\n               dtype=tf.float32):\r\n\r\n    self.graph = tf.Graph()\r\n    self.server = server\r\n    self.source_vocab_size = source_vocab_size\r\n    self.target_vocab_size = target_vocab_size\r\n    self.buckets = buckets\r\n    self.batch_size = batch_size\r\n    self.num_workers = num_workers \r\n    self.task_index = task_index\r\n    self.is_chief = (task_index == 0)\r\n    self.num_workers = num_workers\r\n    self.replicas_to_aggregate = replicas_to_aggregate\r\n\r\n    with self.graph.as_default():\r\n      with tf.device(\"/job:ps/task:0\"): \r\n        self.global_step = tf.Variable(0, trainable=False)\r\n        self.learning_rate = tf.Variable(\r\n            float(learning_rate), trainable=False, dtype=dtype)\r\n  \r\n      output_projection = None\r\n      softmax_loss_function = None\r\n      # Sampled softmax only makes sense if we sample less than vocabulary size.\r\n      if num_samples > 0 and num_samples < self.target_vocab_size:\r\n        with tf.device(\"/job:ps/task:0\"): \r\n          w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\r\n          w = tf.transpose(w_t)\r\n          b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\r\n        output_projection = (w, b)\r\n  \r\n        def sampled_loss(inputs, labels):\r\n          with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n            labels = tf.reshape(labels, [-1, 1])\r\n            # We need to compute the sampled_softmax_loss using 32bit floats to\r\n            # avoid numerical instabilities.\r\n            local_w_t = tf.cast(w_t, tf.float32)\r\n            local_b = tf.cast(b, tf.float32)\r\n            local_inputs = tf.cast(inputs, tf.float32)\r\n            return tf.cast(\r\n                tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\r\n                                           num_samples, self.target_vocab_size),\r\n                dtype)\r\n        softmax_loss_function = sampled_loss\r\n  \r\n      # Create the internal multi-layer cell for our RNN.\r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        single_cell = tf.nn.rnn_cell.GRUCell(size)\r\n        if use_lstm:\r\n          single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\r\n        cell = single_cell\r\n        if num_layers > 1:\r\n          cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\r\n  \r\n      # The seq2seq function: we use embedding for the input and attention.\r\n      def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n        with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n          return tf.nn.seq2seq.embedding_attention_seq2seq(\r\n              encoder_inputs,\r\n              decoder_inputs,\r\n              cell,\r\n              num_encoder_symbols=source_vocab_size,\r\n              num_decoder_symbols=target_vocab_size,\r\n              embedding_size=size,\r\n              output_projection=output_projection,\r\n              feed_previous=do_decode,\r\n              dtype=dtype)\r\n\r\n      # Feeds for inputs.\r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        self.encoder_inputs = []\r\n        self.decoder_inputs = []\r\n        self.target_weights = []\r\n        for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\r\n          self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                    name=\"encoder{0}\".format(i)))\r\n        for i in xrange(buckets[-1][1] + 1):\r\n          self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                    name=\"decoder{0}\".format(i)))\r\n          self.target_weights.append(tf.placeholder(dtype, shape=[None],\r\n                                                    name=\"weight{0}\".format(i)))\r\n  \r\n        # Our targets are decoder inputs shifted by one.\r\n        targets = [self.decoder_inputs[i + 1]\r\n                   for i in xrange(len(self.decoder_inputs) - 1)]\r\n        if forward_only:\r\n          self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\r\n              self.encoder_inputs, self.decoder_inputs, targets,\r\n              self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\r\n              softmax_loss_function=softmax_loss_function)\r\n          # If we use output projection, we need to project outputs for decoding.\r\n          if output_projection is not None:\r\n            for b in xrange(len(buckets)):\r\n              self.outputs[b] = [\r\n                  tf.matmul(output, output_projection[0]) + output_projection[1]\r\n                  for output in self.outputs[b]\r\n              ]\r\n        else:\r\n          self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\r\n              self.encoder_inputs, self.decoder_inputs, targets,\r\n              self.target_weights, buckets,\r\n              lambda x, y: seq2seq_f(x, y, False),\r\n              softmax_loss_function=softmax_loss_function)\r\n  \r\n        # Gradients and SGD update operation for training the model.\r\n      with tf.device(\"/job:worker/task:\"+str(self.task_index)):\r\n        params = tf.trainable_variables()\r\n        self.gradient_norms = []\r\n        self.updates = []\r\n      #sync_rep_opt = tf.train.AdamOptimizer(self.learning_rate)\r\n        sgd_opt = tf.train.GradientDescentOptimizer(2.0)\r\n        sync_rep_opt = tf.train.SyncReplicasOptimizerV2(\r\n            sgd_opt, replicas_to_aggregate=replicas_to_aggregate,\r\n            total_num_replicas=num_workers)\r\n        for b in xrange(len(buckets)):\r\n          gradients = tf.gradients(self.losses[b], params)\r\n          clipped_gradients, norm = tf.clip_by_global_norm(gradients,\r\n                                                           max_gradient_norm)\r\n          self.gradient_norms.append(norm)\r\n          self.updates.append(sync_rep_opt.apply_gradients(\r\n              zip(clipped_gradients, params), global_step=self.global_step))\r\n\r\n        init_op = tf.global_variables_initializer()\r\n        local_init_op = sync_rep_opt.local_step_init_op\r\n        if self.is_chief:\r\n          local_init_op = sync_rep_opt.chief_init_op\r\n        ready_for_local_init_op = sync_rep_opt.ready_for_local_init_op\r\n\r\n        # Chief_queue_runner\r\n        chief_queue_runner = sync_rep_opt.get_chief_queue_runner()\r\n        sync_init_op = sync_rep_opt.get_init_tokens_op(num_workers)\r\n\r\n    # Creates session for chief.\r\n    supervisor = tf.train.Supervisor(\r\n        graph=self.graph,\r\n        is_chief=self.is_chief,\r\n        recovery_wait_secs=1,\r\n        init_op=init_op,\r\n        local_init_op=local_init_op,\r\n        ready_for_local_init_op=ready_for_local_init_op)\r\n\r\n    sess_config = tf.ConfigProto(\r\n        allow_soft_placement=True,\r\n        log_device_placement=True,\r\n        device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % self.task_index])\r\n\r\n\r\n    self.session = supervisor.prepare_or_wait_for_session(self.server.target, config=sess_config)\r\n\r\n    # Chief should execute the sync_init_op and start the chief queue runner.\r\n    if self.is_chief:\r\n      self.session.run(sync_init_op)\r\n      supervisor.StartQueueRunners(self.session, [chief_queue_runner])\r\n\r\n    if self.is_chief:\r\n      self.session.run(sync_init_op)\r\n      supervisor.StartQueueRunners(self.session, [chief_queue_runner])\r\n\r\n\r\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\r\n           bucket_id, forward_only):\r\n    \"\"\"Run a step of the model feeding the given inputs.\r\n\r\n    Args:\r\n      session: tensorflow session to use.\r\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\r\n      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\r\n      target_weights: list of numpy float vectors to feed as target weights.\r\n      bucket_id: which bucket of the model to use.\r\n      forward_only: whether to do the backward step or only forward.\r\n\r\n    Returns:\r\n      A triple consisting of gradient norm (or None if we did not do backward),\r\n      average perplexity, and the outputs.\r\n\r\n    Raises:\r\n      ValueError: if length of encoder_inputs, decoder_inputs, or\r\n        target_weights disagrees with bucket size for the specified bucket_id.\r\n    \"\"\"\r\n    # Check if the sizes match.\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    if len(encoder_inputs) != encoder_size:\r\n      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\r\n    if len(decoder_inputs) != decoder_size:\r\n      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\r\n    if len(target_weights) != decoder_size:\r\n      raise ValueError(\"Weights length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(target_weights), decoder_size))\r\n\r\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\r\n    input_feed = {}\r\n    for l in xrange(encoder_size):\r\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\r\n    for l in xrange(decoder_size):\r\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\r\n      input_feed[self.target_weights[l].name] = target_weights[l]\r\n\r\n    # Since our targets are decoder inputs shifted by one, we need one more.\r\n    last_target = self.decoder_inputs[decoder_size].name\r\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\r\n\r\n    # Output feed: depends on whether we do a backward step or not.\r\n    if not forward_only:\r\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\r\n                     self.gradient_norms[bucket_id],  # Gradient norm.\r\n                     self.losses[bucket_id]]  # Loss for this batch.\r\n    else:\r\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\r\n      for l in xrange(decoder_size):  # Output logits.\r\n        output_feed.append(self.outputs[bucket_id][l])\r\n\r\n    outputs = session.run(output_feed, input_feed)\r\n    if not forward_only:\r\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\r\n    else:\r\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\r\n\r\n  def get_batch(self, data, bucket_id):\r\n    \"\"\"Get a random batch of data from the specified bucket, prepare for step.\r\n\r\n    To feed data in step(..) it must be a list of batch-major vectors, while\r\n    data here contains single length-major cases. So the main logic of this\r\n    function is to re-index data cases to be in the proper format for feeding.\r\n\r\n    Args:\r\n      data: a tuple of size len(self.buckets) in which each element contains\r\n        lists of pairs of input and output data that we use to create a batch.\r\n      bucket_id: integer, which bucket to get the batch for.\r\n\r\n    Returns:\r\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\r\n      the constructed batch that has the proper format to call step(...) later.\r\n    \"\"\"\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    encoder_inputs, decoder_inputs = [], []\r\n\r\n    # Get a random batch of encoder and decoder inputs from data,\r\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\r\n    for _ in xrange(self.batch_size):\r\n      encoder_input, decoder_input = random.choice(data[bucket_id])\r\n\r\n      # Encoder inputs are padded and then reversed.\r\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\r\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\r\n\r\n      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\r\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\r\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\r\n                            [data_utils.PAD_ID] * decoder_pad_size)\r\n\r\n    # Now we create batch-major vectors from the data selected above.\r\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\r\n\r\n    # Batch encoder inputs are just re-indexed encoder_inputs.\r\n    for length_idx in xrange(encoder_size):\r\n      batch_encoder_inputs.append(\r\n          np.array([encoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\r\n    for length_idx in xrange(decoder_size):\r\n      batch_decoder_inputs.append(\r\n          np.array([decoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n      # Create target_weights to be 0 for targets that are padding.\r\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\r\n      for batch_idx in xrange(self.batch_size):\r\n        # We set weight to 0 if the corresponding target is a PAD symbol.\r\n        # The corresponding target is decoder_input shifted by 1 forward.\r\n        if length_idx < decoder_size - 1:\r\n          target = decoder_inputs[batch_idx][length_idx + 1]\r\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\r\n          batch_weight[batch_idx] = 0.0\r\n      batch_weights.append(batch_weight)\r\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\r\n\r\n"}