{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9853", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9853/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9853/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9853/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9853", "id": 228200600, "node_id": "MDU6SXNzdWUyMjgyMDA2MDA=", "number": 9853, "title": "Multiprocessing for input pipeline ", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-05-12T06:37:22Z", "updated_at": "2017-11-28T09:42:30Z", "closed_at": "2017-05-12T15:53:12Z", "author_association": "NONE", "body_html": "<p>I have asked this <a href=\"http://stackoverflow.com/questions/43889941/enqueuing-a-tf-randomshufflequeue-from-multiple-processes-using-multiprocessing\" rel=\"nofollow\">question</a> on StackOverFlow but I also feel that it can also be seen as a feature request.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\n: YES</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\n: Linux version 3.10.0-229.11.1.el7.x86_64 (<a href=\"mailto:builder@kbuilder.dev.centos.org\">builder@kbuilder.dev.centos.org</a>) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) )</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\n:  Installed from sources</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n:  v1.0.0-63-g5b4bb03-dirty' 1.0.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n:  0.4.4</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n:  CUDA 8.0<br>\n:  CuDNN 5.1</li>\n<li><strong>GPU model and memory</strong>:<br>\n: NVidia Titan X (Maxwell)</li>\n<li><strong>Exact command to reproduce</strong>:<br>\n: It is a feature request or clarification</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><strong>Basic Objective</strong><br>\nI have to train the OverFeat architecture for patch classification from scratch.</p>\n<p><strong>Problem scope</strong><br>\nInput to the graph</p>\n<p><strong>Problem description</strong><br>\nDuring the training of the OverFeat model, for each image 5 random crops and their mirrored versions (a total of 10 augmented images) are created and they are fed to the model.</p>\n<p>I previously used <code>feed_dict</code> to provide input to the model. However, it led to a very very slow  training and turned out to be impractical when handling 12 million input images (1.2 million of imagenet X 10).<br>\nHence, I want to use native Tensorflow queues to provide input to the graph.</p>\n<p>I have used <code>multiprocessing</code> module of Python to create sharded TFRecord files of Imagenet dataset.<br>\nNow, I would like to use <code>multiprocessing</code> to enqueue augmented images to a <code>tf.RandomShuffleQueue</code> from which the graph takes it input.<br>\nI could have used <code>multithreading</code> but it is still slow. Seeing the speedup in creation of TFRecord files through <code>multiprocessing</code> as opposed to <code>multithreading</code>, I am convinced that using <code>multiprocessing</code> would increase the speed of preprocessing.</p>\n<p>The problem is that, it is not clear to me that how do I use <code>multiprocessing</code> in TensorFlow. One code snippet is <a href=\"https://github.com/WeiTang114/tf-image-classification/blob/ff790fcb1ab6062979688647bb078c2765ad0e7a/input.py\">here</a>  , but it ends up using a <code>feed_dict</code> at the end of the pipeline which leads to one extra copy operation.</p>\n<p>So, I want to launch like 20 processes using <code>multiprocessing</code>, each of which will process a range of shards and enqueue the augmented images and corresponding labels to a <code>tf.RandomShuffleQueue</code><br>\nIt would be very helpful if there is a feature for using multiprocessing with TensorFlow for input pipeline and data augmentation. A good guideline in that direction would be very helpful.</p>\n<h3>Source code / logs</h3>\n<p>The source code which is used for creating the TFRecord files is <a href=\"https://gitlab.inria.fr/uujjwal/overfeat-tensorflow/blob/master/buildtfrecords.py\" rel=\"nofollow\">here</a></p>\n<p>I also wrote a basic code to try out multiprocessing (to enqueue a queue from multiple processes) but it does not work.</p>\n<pre><code>import os\nimport tensorflow as tf\nfrom pathos.multiprocessing import Pool as mp\n\ndef f(g,i):\n    #g = tf.Graph()                                                                                                  \n    with g.as_default():\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\n                         shared_name=\"shared_q\", name=\"q\")\n        a = tf.constant(\"Hello\")\n        b =\tq.enqueue(a)\n        c = q.size()\n    with tf.Session(graph=g) as sess:\n        for n in range(i):\n            sess.run(b)\n            print(sess.run(c))\n    return\n\nif __name__ == \"__main__\":\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n    p = mp(5)\n    g = tf.Graph()\n    with g.as_default():\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\n                         shared_name=\"shared_q\", name=\"q\")\n        qsz = q.size()\n\tp.starmap(f,[(g,1),(g,2)])\n        with tf.Session(graph=g) as sess:\n            for\ti in range(10):\n                print(sess.run(qsz))\n    \tos.unsetenv(\"CUDA_VISIBLE_DEVICES\")\n</code></pre>\n<p>The output is</p>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n1\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n1\n2\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n</code></pre>\n<p>As can be seen in the subprocesses, the queue is being enqueued, but the size as shown in the main module is continuously zero.</p>", "body_text": "I have asked this question on StackOverFlow but I also feel that it can also be seen as a feature request.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n: YES\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n: Linux version 3.10.0-229.11.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) )\nTensorFlow installed from (source or binary):\n:  Installed from sources\nTensorFlow version (use command below):\n:  v1.0.0-63-g5b4bb03-dirty' 1.0.1\nBazel version (if compiling from source):\n:  0.4.4\nCUDA/cuDNN version:\n:  CUDA 8.0\n:  CuDNN 5.1\nGPU model and memory:\n: NVidia Titan X (Maxwell)\nExact command to reproduce:\n: It is a feature request or clarification\n\nDescribe the problem\nBasic Objective\nI have to train the OverFeat architecture for patch classification from scratch.\nProblem scope\nInput to the graph\nProblem description\nDuring the training of the OverFeat model, for each image 5 random crops and their mirrored versions (a total of 10 augmented images) are created and they are fed to the model.\nI previously used feed_dict to provide input to the model. However, it led to a very very slow  training and turned out to be impractical when handling 12 million input images (1.2 million of imagenet X 10).\nHence, I want to use native Tensorflow queues to provide input to the graph.\nI have used multiprocessing module of Python to create sharded TFRecord files of Imagenet dataset.\nNow, I would like to use multiprocessing to enqueue augmented images to a tf.RandomShuffleQueue from which the graph takes it input.\nI could have used multithreading but it is still slow. Seeing the speedup in creation of TFRecord files through multiprocessing as opposed to multithreading, I am convinced that using multiprocessing would increase the speed of preprocessing.\nThe problem is that, it is not clear to me that how do I use multiprocessing in TensorFlow. One code snippet is here  , but it ends up using a feed_dict at the end of the pipeline which leads to one extra copy operation.\nSo, I want to launch like 20 processes using multiprocessing, each of which will process a range of shards and enqueue the augmented images and corresponding labels to a tf.RandomShuffleQueue\nIt would be very helpful if there is a feature for using multiprocessing with TensorFlow for input pipeline and data augmentation. A good guideline in that direction would be very helpful.\nSource code / logs\nThe source code which is used for creating the TFRecord files is here\nI also wrote a basic code to try out multiprocessing (to enqueue a queue from multiple processes) but it does not work.\nimport os\nimport tensorflow as tf\nfrom pathos.multiprocessing import Pool as mp\n\ndef f(g,i):\n    #g = tf.Graph()                                                                                                  \n    with g.as_default():\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\n                         shared_name=\"shared_q\", name=\"q\")\n        a = tf.constant(\"Hello\")\n        b =\tq.enqueue(a)\n        c = q.size()\n    with tf.Session(graph=g) as sess:\n        for n in range(i):\n            sess.run(b)\n            print(sess.run(c))\n    return\n\nif __name__ == \"__main__\":\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n    p = mp(5)\n    g = tf.Graph()\n    with g.as_default():\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\n                         shared_name=\"shared_q\", name=\"q\")\n        qsz = q.size()\n\tp.starmap(f,[(g,1),(g,2)])\n        with tf.Session(graph=g) as sess:\n            for\ti in range(10):\n                print(sess.run(qsz))\n    \tos.unsetenv(\"CUDA_VISIBLE_DEVICES\")\n\nThe output is\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n1\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n1\n2\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\nAs can be seen in the subprocesses, the queue is being enqueued, but the size as shown in the main module is continuously zero.", "body": "I have asked this [question](http://stackoverflow.com/questions/43889941/enqueuing-a-tf-randomshufflequeue-from-multiple-processes-using-multiprocessing) on StackOverFlow but I also feel that it can also be seen as a feature request. \r\n\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n   : YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    : Linux version 3.10.0-229.11.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) \r\n- **TensorFlow installed from (source or binary)**:\r\n    :  Installed from sources\r\n- **TensorFlow version (use command below)**:\r\n    :  v1.0.0-63-g5b4bb03-dirty' 1.0.1\r\n- **Bazel version (if compiling from source)**:\r\n    :  0.4.4\r\n- **CUDA/cuDNN version**:\r\n    :  CUDA 8.0 \r\n    :  CuDNN 5.1\r\n- **GPU model and memory**:\r\n    : NVidia Titan X (Maxwell)\r\n- **Exact command to reproduce**:\r\n    : It is a feature request or clarification\r\n\r\n\r\n### Describe the problem\r\n\r\n**Basic Objective**\r\nI have to train the OverFeat architecture for patch classification from scratch.\r\n\r\n**Problem scope**\r\nInput to the graph\r\n\r\n**Problem description**\r\nDuring the training of the OverFeat model, for each image 5 random crops and their mirrored versions (a total of 10 augmented images) are created and they are fed to the model.\r\n\r\nI previously used `feed_dict` to provide input to the model. However, it led to a very very slow  training and turned out to be impractical when handling 12 million input images (1.2 million of imagenet X 10).\r\nHence, I want to use native Tensorflow queues to provide input to the graph.\r\n\r\nI have used `multiprocessing` module of Python to create sharded TFRecord files of Imagenet dataset. \r\nNow, I would like to use `multiprocessing` to enqueue augmented images to a `tf.RandomShuffleQueue` from which the graph takes it input. \r\nI could have used `multithreading` but it is still slow. Seeing the speedup in creation of TFRecord files through `multiprocessing` as opposed to `multithreading`, I am convinced that using `multiprocessing` would increase the speed of preprocessing.\r\n\r\nThe problem is that, it is not clear to me that how do I use `multiprocessing` in TensorFlow. One code snippet is [here](https://github.com/WeiTang114/tf-image-classification/blob/ff790fcb1ab6062979688647bb078c2765ad0e7a/input.py)  , but it ends up using a `feed_dict` at the end of the pipeline which leads to one extra copy operation. \r\n\r\nSo, I want to launch like 20 processes using `multiprocessing`, each of which will process a range of shards and enqueue the augmented images and corresponding labels to a `tf.RandomShuffleQueue`\r\nIt would be very helpful if there is a feature for using multiprocessing with TensorFlow for input pipeline and data augmentation. A good guideline in that direction would be very helpful.\r\n\r\n\r\n### Source code / logs\r\nThe source code which is used for creating the TFRecord files is [here](https://gitlab.inria.fr/uujjwal/overfeat-tensorflow/blob/master/buildtfrecords.py)\r\n\r\nI also wrote a basic code to try out multiprocessing (to enqueue a queue from multiple processes) but it does not work.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom pathos.multiprocessing import Pool as mp\r\n\r\ndef f(g,i):\r\n    #g = tf.Graph()                                                                                                  \r\n    with g.as_default():\r\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\r\n                         shared_name=\"shared_q\", name=\"q\")\r\n        a = tf.constant(\"Hello\")\r\n        b =\tq.enqueue(a)\r\n        c = q.size()\r\n    with tf.Session(graph=g) as sess:\r\n        for n in range(i):\r\n            sess.run(b)\r\n            print(sess.run(c))\r\n    return\r\n\r\nif __name__ == \"__main__\":\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\n    p = mp(5)\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\r\n                         shared_name=\"shared_q\", name=\"q\")\r\n        qsz = q.size()\r\n\tp.starmap(f,[(g,1),(g,2)])\r\n        with tf.Session(graph=g) as sess:\r\n            for\ti in range(10):\r\n                print(sess.run(qsz))\r\n    \tos.unsetenv(\"CUDA_VISIBLE_DEVICES\")\r\n```\r\n\r\nThe output is \r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\r\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\r\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n1\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\r\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\r\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n1\r\n2\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\r\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\r\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n```\r\n\r\nAs can be seen in the subprocesses, the queue is being enqueued, but the size as shown in the main module is continuously zero.\r\n"}