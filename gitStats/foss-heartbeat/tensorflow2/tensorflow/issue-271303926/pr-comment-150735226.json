{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/150735226", "pull_request_review_id": 76328110, "id": 150735226, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDczNTIyNg==", "diff_hunk": "@@ -352,32 +354,45 @@ def _SvdGrad(op, grad_s, grad_u, grad_v):\n             array_ops.expand_dims(s2, -2) - array_ops.expand_dims(s2, -1)),\n         array_ops.zeros_like(s))\n     s_inv_mat = array_ops.matrix_diag(math_ops.reciprocal(s))\n+\n+    v1 = v[..., :, :m]\n+    grad_v1 = grad_v[..., :, :m]\n+\n     u_gu = math_ops.matmul(u, grad_u, adjoint_a=True)\n-    v_gv = math_ops.matmul(v, grad_v, adjoint_a=True)\n+    v_gv = math_ops.matmul(v1, grad_v1, adjoint_a=True)\n \n-    if m == n:\n-      f_u = f * u_gu\n-      f_v = f * v_gv\n-    else:\n-      dv2 = array_ops.matrix_transpose(v_gv[..., m:n, :m]) - v_gv[..., :m, m:n]", "path": "tensorflow/python/ops/linalg_grad.py", "position": 65, "original_position": 60, "commit_id": "f7a0cc05b4210ae3f1598de619da37f94ff3d2b2", "original_commit_id": "b739ee1734baf437f1925ea09d07647ec7602caf", "user": {"login": "vnavkal", "id": 5924638, "node_id": "MDQ6VXNlcjU5MjQ2Mzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5924638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vnavkal", "html_url": "https://github.com/vnavkal", "followers_url": "https://api.github.com/users/vnavkal/followers", "following_url": "https://api.github.com/users/vnavkal/following{/other_user}", "gists_url": "https://api.github.com/users/vnavkal/gists{/gist_id}", "starred_url": "https://api.github.com/users/vnavkal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vnavkal/subscriptions", "organizations_url": "https://api.github.com/users/vnavkal/orgs", "repos_url": "https://api.github.com/users/vnavkal/repos", "events_url": "https://api.github.com/users/vnavkal/events{/privacy}", "received_events_url": "https://api.github.com/users/vnavkal/received_events", "type": "User", "site_admin": false}, "body": "I should explain what happened to `dv2`.  `dv2` has two terms:\r\n1. `array_ops.matrix_transpose(v_gv[..., m:n, :m])` and\r\n2. `-v_gv[..., :m, m:n]`.\r\n\r\nWhen `grad_a_nouv` gets right-multiplied by the adjoint of `v`, the submatrix `dv2` gets right-multiplied by the adjoint of `v[..., :, m:n]`.  After right-multiplication by `v[..., :, m:n]`,\r\n1. the first term of `dv2` becomes the first term of `term2_nous`: `math_ops.matmul(grad_v1, proj_v1_perp, adjoint_a=True)`\r\n2. the second term of `dv2` becomes the second term of `term2_nous`: `-math_ops.matmul(v1t_gv2, v2, adjoint_b=True)`\r\n\r\nTo see why the first terms match, use the identity\r\n```\r\nmath_ops.matmul(v[..., :, :m], v[..., :, :m], adjoint_b=True)\r\n   + math_ops.matmul(v[..., :, m:n], v[..., :, m:n], adjoint_b=True)\r\n       == linalg_ops.eye(n, dtype=v.dtype)\r\n```\r\nwhich follows from the fact that `v` is unitary.\r\n\r\nMy motivation for separating the terms of `dv2` was to make clear what the contribution to `grad_a` is from the \"redundant\" columns of V when `full_matrices=True`.\r\n\r\nIf the math here isn't clear, just let me know and I can write it down and upload it.", "created_at": "2017-11-14T04:31:25Z", "updated_at": "2017-12-18T04:35:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14259#discussion_r150735226", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14259", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/150735226"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14259#discussion_r150735226"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14259"}}, "body_html": "<p>I should explain what happened to <code>dv2</code>.  <code>dv2</code> has two terms:</p>\n<ol>\n<li><code>array_ops.matrix_transpose(v_gv[..., m:n, :m])</code> and</li>\n<li><code>-v_gv[..., :m, m:n]</code>.</li>\n</ol>\n<p>When <code>grad_a_nouv</code> gets right-multiplied by the adjoint of <code>v</code>, the submatrix <code>dv2</code> gets right-multiplied by the adjoint of <code>v[..., :, m:n]</code>.  After right-multiplication by <code>v[..., :, m:n]</code>,</p>\n<ol>\n<li>the first term of <code>dv2</code> becomes the first term of <code>term2_nous</code>: <code>math_ops.matmul(grad_v1, proj_v1_perp, adjoint_a=True)</code></li>\n<li>the second term of <code>dv2</code> becomes the second term of <code>term2_nous</code>: <code>-math_ops.matmul(v1t_gv2, v2, adjoint_b=True)</code></li>\n</ol>\n<p>To see why the first terms match, use the identity</p>\n<pre><code>math_ops.matmul(v[..., :, :m], v[..., :, :m], adjoint_b=True)\n   + math_ops.matmul(v[..., :, m:n], v[..., :, m:n], adjoint_b=True)\n       == linalg_ops.eye(n, dtype=v.dtype)\n</code></pre>\n<p>which follows from the fact that <code>v</code> is unitary.</p>\n<p>My motivation for separating the terms of <code>dv2</code> was to make clear what the contribution to <code>grad_a</code> is from the \"redundant\" columns of V when <code>full_matrices=True</code>.</p>\n<p>If the math here isn't clear, just let me know and I can write it down and upload it.</p>", "body_text": "I should explain what happened to dv2.  dv2 has two terms:\n\narray_ops.matrix_transpose(v_gv[..., m:n, :m]) and\n-v_gv[..., :m, m:n].\n\nWhen grad_a_nouv gets right-multiplied by the adjoint of v, the submatrix dv2 gets right-multiplied by the adjoint of v[..., :, m:n].  After right-multiplication by v[..., :, m:n],\n\nthe first term of dv2 becomes the first term of term2_nous: math_ops.matmul(grad_v1, proj_v1_perp, adjoint_a=True)\nthe second term of dv2 becomes the second term of term2_nous: -math_ops.matmul(v1t_gv2, v2, adjoint_b=True)\n\nTo see why the first terms match, use the identity\nmath_ops.matmul(v[..., :, :m], v[..., :, :m], adjoint_b=True)\n   + math_ops.matmul(v[..., :, m:n], v[..., :, m:n], adjoint_b=True)\n       == linalg_ops.eye(n, dtype=v.dtype)\n\nwhich follows from the fact that v is unitary.\nMy motivation for separating the terms of dv2 was to make clear what the contribution to grad_a is from the \"redundant\" columns of V when full_matrices=True.\nIf the math here isn't clear, just let me know and I can write it down and upload it."}