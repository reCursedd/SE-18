{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11538", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11538/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11538/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11538/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11538", "id": 243270595, "node_id": "MDU6SXNzdWUyNDMyNzA1OTU=", "number": 11538, "title": "MultiRNNCell fails _like_rnncell check.", "user": {"login": "GrandathePanda", "id": 6426407, "node_id": "MDQ6VXNlcjY0MjY0MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6426407?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GrandathePanda", "html_url": "https://github.com/GrandathePanda", "followers_url": "https://api.github.com/users/GrandathePanda/followers", "following_url": "https://api.github.com/users/GrandathePanda/following{/other_user}", "gists_url": "https://api.github.com/users/GrandathePanda/gists{/gist_id}", "starred_url": "https://api.github.com/users/GrandathePanda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GrandathePanda/subscriptions", "organizations_url": "https://api.github.com/users/GrandathePanda/orgs", "repos_url": "https://api.github.com/users/GrandathePanda/repos", "events_url": "https://api.github.com/users/GrandathePanda/events{/privacy}", "received_events_url": "https://api.github.com/users/GrandathePanda/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-16T22:20:24Z", "updated_at": "2017-07-16T22:40:52Z", "closed_at": "2017-07-16T22:40:52Z", "author_association": "NONE", "body_html": "<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: MacOS Sierra 10.12.5</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary, pip3 install</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.2.0-5-g435cdfc 1.2.1</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I believe this is a bug. When initializing a MultiRNNCell like MultiRNNCell([lstm]*3) this subsequently may be passed into something like tf.nn.bidirectional_dynamic_rnn and it will pass the _like_rnncell check and everything proceeds normally. When initializing a MultiRNNCell like MultiRNNCell([lstm_factory() for _ in range num_layers]), such as in <a href=\"https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms\" rel=\"nofollow\">this code</a> the _like_rnncell check on line 393 of <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py\">this code</a> will fail. If you trace it back to where that function is defined there are 4 qualifiers, the qualifiers that fail are hasattr(cell, 'output_size') and hasattr(cell, 'state_size'). The preferred way of initializing multi cell rnns that do not share input size which lead to later dimension mismatch seem to be this way. Initially I wrote my code with the former implementation but the former initialization leads to input dimension sharing and then mismatches later on which is another issue and why I can't use that one. (Also this probably fails anywhere a _like_rnncell check is used, I know for a fact my AttentionWrapper in my decoder fails for the same reason not just in the bidirectional_dynamic_rnn)</p>\n<h3>Source code / logs</h3>\n<p>This is my encoder copied from my seq2seq model class.</p>\n<pre><code>    LSTMCell = tf.nn.rnn_cell.LSTMCell\n    MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\n    DropoutWrapper = tf.nn.rnn_cell.DropoutWrapper\n\n    def encode(\n        self,\n        num_units, peepholes, inputs,\n        num_layers, seq_len, time_major,\n        keep_prob=0.5\n    ):\n        multi_cell = MultiRNNCell(\n            [\n                (self._cell_factory(num_units, peepholes, keep_prob)\n                    for x in range(num_layers))\n            ]\n        )\n        enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=multi_cell,\n            cell_bw=multi_cell,\n            inputs=inputs,\n            sequence_length=seq_len,\n            dtype=tf.float32,\n            time_major=time_major\n        )\n        return enc_outputs, enc_state\n\n    def _cell_factory(self, num_units, peepholes, keep_prob):\n        lstm = LSTMCell(num_units=num_units, use_peepholes=peepholes)\n        dropout = DropoutWrapper(lstm, input_keep_prob=keep_prob)\n        return dropout\n</code></pre>\n<p>Trace:<br>\n<code>  File \"execute.py\", line 168, in &lt;module&gt; main() File \"execute.py\", line 91, in main steps=10000 File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 241, in train loss = self._train_model(input_fn=input_fn, hooks=hooks) File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 560, in _train_model model_fn_lib.ModeKeys.TRAIN) File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 545, in _call_model_fn features=features, labels=labels, **kwargs) File \"execute.py\", line 134, in model_wrapper keep_prob=params['encode']['keep_probability'] File \"/Users/panda/Desktop/aura_ml/model_1/model.py\", line 42, in encode time_major=time_major File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 364, in bidirectional_dynamic_rnn raise TypeError(\"cell_fw must be an instance of RNNCell\") TypeError: cell_fw must be an instance of RNNCell</code></p>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Sierra 10.12.5\nTensorFlow installed from (source or binary): binary, pip3 install\nTensorFlow version (use command below): v1.2.0-5-g435cdfc 1.2.1\nPython version: 3.5.2\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\n\nDescribe the problem\nI believe this is a bug. When initializing a MultiRNNCell like MultiRNNCell([lstm]*3) this subsequently may be passed into something like tf.nn.bidirectional_dynamic_rnn and it will pass the _like_rnncell check and everything proceeds normally. When initializing a MultiRNNCell like MultiRNNCell([lstm_factory() for _ in range num_layers]), such as in this code the _like_rnncell check on line 393 of this code will fail. If you trace it back to where that function is defined there are 4 qualifiers, the qualifiers that fail are hasattr(cell, 'output_size') and hasattr(cell, 'state_size'). The preferred way of initializing multi cell rnns that do not share input size which lead to later dimension mismatch seem to be this way. Initially I wrote my code with the former implementation but the former initialization leads to input dimension sharing and then mismatches later on which is another issue and why I can't use that one. (Also this probably fails anywhere a _like_rnncell check is used, I know for a fact my AttentionWrapper in my decoder fails for the same reason not just in the bidirectional_dynamic_rnn)\nSource code / logs\nThis is my encoder copied from my seq2seq model class.\n    LSTMCell = tf.nn.rnn_cell.LSTMCell\n    MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\n    DropoutWrapper = tf.nn.rnn_cell.DropoutWrapper\n\n    def encode(\n        self,\n        num_units, peepholes, inputs,\n        num_layers, seq_len, time_major,\n        keep_prob=0.5\n    ):\n        multi_cell = MultiRNNCell(\n            [\n                (self._cell_factory(num_units, peepholes, keep_prob)\n                    for x in range(num_layers))\n            ]\n        )\n        enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=multi_cell,\n            cell_bw=multi_cell,\n            inputs=inputs,\n            sequence_length=seq_len,\n            dtype=tf.float32,\n            time_major=time_major\n        )\n        return enc_outputs, enc_state\n\n    def _cell_factory(self, num_units, peepholes, keep_prob):\n        lstm = LSTMCell(num_units=num_units, use_peepholes=peepholes)\n        dropout = DropoutWrapper(lstm, input_keep_prob=keep_prob)\n        return dropout\n\nTrace:\n  File \"execute.py\", line 168, in <module> main() File \"execute.py\", line 91, in main steps=10000 File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 241, in train loss = self._train_model(input_fn=input_fn, hooks=hooks) File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 560, in _train_model model_fn_lib.ModeKeys.TRAIN) File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 545, in _call_model_fn features=features, labels=labels, **kwargs) File \"execute.py\", line 134, in model_wrapper keep_prob=params['encode']['keep_probability'] File \"/Users/panda/Desktop/aura_ml/model_1/model.py\", line 42, in encode time_major=time_major File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 364, in bidirectional_dynamic_rnn raise TypeError(\"cell_fw must be an instance of RNNCell\") TypeError: cell_fw must be an instance of RNNCell", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra 10.12.5\r\n- **TensorFlow installed from (source or binary)**: binary, pip3 install\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\n### Describe the problem\r\nI believe this is a bug. When initializing a MultiRNNCell like MultiRNNCell([lstm]*3) this subsequently may be passed into something like tf.nn.bidirectional_dynamic_rnn and it will pass the _like_rnncell check and everything proceeds normally. When initializing a MultiRNNCell like MultiRNNCell([lstm_factory() for _ in range num_layers]), such as in [this code](https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms) the _like_rnncell check on line 393 of [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py) will fail. If you trace it back to where that function is defined there are 4 qualifiers, the qualifiers that fail are hasattr(cell, 'output_size') and hasattr(cell, 'state_size'). The preferred way of initializing multi cell rnns that do not share input size which lead to later dimension mismatch seem to be this way. Initially I wrote my code with the former implementation but the former initialization leads to input dimension sharing and then mismatches later on which is another issue and why I can't use that one. (Also this probably fails anywhere a _like_rnncell check is used, I know for a fact my AttentionWrapper in my decoder fails for the same reason not just in the bidirectional_dynamic_rnn)\r\n\r\n\r\n### Source code / logs\r\nThis is my encoder copied from my seq2seq model class. \r\n```\r\n    LSTMCell = tf.nn.rnn_cell.LSTMCell\r\n    MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\r\n    DropoutWrapper = tf.nn.rnn_cell.DropoutWrapper\r\n\r\n    def encode(\r\n        self,\r\n        num_units, peepholes, inputs,\r\n        num_layers, seq_len, time_major,\r\n        keep_prob=0.5\r\n    ):\r\n        multi_cell = MultiRNNCell(\r\n            [\r\n                (self._cell_factory(num_units, peepholes, keep_prob)\r\n                    for x in range(num_layers))\r\n            ]\r\n        )\r\n        enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\r\n            cell_fw=multi_cell,\r\n            cell_bw=multi_cell,\r\n            inputs=inputs,\r\n            sequence_length=seq_len,\r\n            dtype=tf.float32,\r\n            time_major=time_major\r\n        )\r\n        return enc_outputs, enc_state\r\n\r\n    def _cell_factory(self, num_units, peepholes, keep_prob):\r\n        lstm = LSTMCell(num_units=num_units, use_peepholes=peepholes)\r\n        dropout = DropoutWrapper(lstm, input_keep_prob=keep_prob)\r\n        return dropout\r\n```\r\n\r\nTrace:\r\n`  File \"execute.py\", line 168, in <module>\r\n    main()\r\n  File \"execute.py\", line 91, in main\r\n    steps=10000\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 241, in train\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 560, in _train_model\r\n    model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 545, in _call_model_fn\r\n    features=features, labels=labels, **kwargs)\r\n  File \"execute.py\", line 134, in model_wrapper\r\n    keep_prob=params['encode']['keep_probability']\r\n  File \"/Users/panda/Desktop/aura_ml/model_1/model.py\", line 42, in encode\r\n    time_major=time_major\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 364, in bidirectional_dynamic_rnn\r\n    raise TypeError(\"cell_fw must be an instance of RNNCell\")\r\nTypeError: cell_fw must be an instance of RNNCell`"}