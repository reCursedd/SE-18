{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9676", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9676/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9676/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9676/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9676", "id": 226466738, "node_id": "MDU6SXNzdWUyMjY0NjY3Mzg=", "number": 9676, "title": "Bug when trying to restore training from Inception-3 checkpoint with different trainable variables", "user": {"login": "reiinakano", "id": 18363734, "node_id": "MDQ6VXNlcjE4MzYzNzM0", "avatar_url": "https://avatars0.githubusercontent.com/u/18363734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reiinakano", "html_url": "https://github.com/reiinakano", "followers_url": "https://api.github.com/users/reiinakano/followers", "following_url": "https://api.github.com/users/reiinakano/following{/other_user}", "gists_url": "https://api.github.com/users/reiinakano/gists{/gist_id}", "starred_url": "https://api.github.com/users/reiinakano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reiinakano/subscriptions", "organizations_url": "https://api.github.com/users/reiinakano/orgs", "repos_url": "https://api.github.com/users/reiinakano/repos", "events_url": "https://api.github.com/users/reiinakano/events{/privacy}", "received_events_url": "https://api.github.com/users/reiinakano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-05T03:54:57Z", "updated_at": "2017-05-05T20:51:57Z", "closed_at": "2017-05-05T20:51:57Z", "author_association": "NONE", "body_html": "<p>I have the pretty common use case of freezing the bottom layers of Inception and training only the first two layers, after which I lower the learning rate and fine tune the entire Inception model.</p>\n<p>Here is my code for running the first part</p>\n<div class=\"highlight highlight-source-python\"><pre>train_dir<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>/home/ubuntu/pynb/TF play/log-inceptionv3flowers<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    tf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n    \n    dataset <span class=\"pl-k\">=</span> get_dataset()\n    images, _, labels <span class=\"pl-k\">=</span> load_batch(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the model, use the default arg scope to configure the batch norm parameters.</span>\n    <span class=\"pl-k\">with</span> slim.arg_scope(inception.inception_v3_arg_scope()):\n        logits, _ <span class=\"pl-k\">=</span> inception.inception_v3(images, <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Specify the loss function:</span>\n    one_hot_labels <span class=\"pl-k\">=</span> slim.one_hot_encoding(labels, <span class=\"pl-c1\">5</span>)\n    tf.losses.softmax_cross_entropy(one_hot_labels, logits)\n    total_loss <span class=\"pl-k\">=</span> tf.losses.get_total_loss()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create some summaries to visualize the training process:</span>\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>losses/Total Loss<span class=\"pl-pds\">'</span></span>, total_loss)\n  \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Specify the optimizer and create the train op:</span>\n    optimizer <span class=\"pl-k\">=</span> tf.train.RMSPropOptimizer(<span class=\"pl-c1\">0.001</span>, <span class=\"pl-c1\">0.9</span>,\n                                    <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n    train_op <span class=\"pl-k\">=</span> slim.learning.create_train_op(total_loss, optimizer, <span class=\"pl-v\">variables_to_train</span><span class=\"pl-k\">=</span>get_variables_to_train())\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the training:</span>\n    final_loss <span class=\"pl-k\">=</span> slim.learning.train(\n        train_op,\n        <span class=\"pl-v\">logdir</span><span class=\"pl-k\">=</span>train_dir,\n        <span class=\"pl-v\">init_fn</span><span class=\"pl-k\">=</span>get_init_fn(),\n        <span class=\"pl-v\">number_of_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4500</span>,\n        <span class=\"pl-v\">save_summaries_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>,\n        <span class=\"pl-v\">save_interval_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>,\n        <span class=\"pl-v\">session_config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options))\n        \n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Finished training. Last batch loss <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> final_loss)</pre></div>\n<p>which runs properly, then my code for running the second part</p>\n<div class=\"highlight highlight-source-python\"><pre>train_dir<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>/home/ubuntu/pynb/TF play/log-inceptionv3flowers<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    tf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n    \n    dataset <span class=\"pl-k\">=</span> get_dataset()\n    images, _, labels <span class=\"pl-k\">=</span> load_batch(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the model, use the default arg scope to configure the batch norm parameters.</span>\n    <span class=\"pl-k\">with</span> slim.arg_scope(inception.inception_v3_arg_scope()):\n        logits, _ <span class=\"pl-k\">=</span> inception.inception_v3(images, <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Specify the loss function:</span>\n    one_hot_labels <span class=\"pl-k\">=</span> slim.one_hot_encoding(labels, <span class=\"pl-c1\">5</span>)\n    tf.losses.softmax_cross_entropy(one_hot_labels, logits)\n    total_loss <span class=\"pl-k\">=</span> tf.losses.get_total_loss()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create some summaries to visualize the training process:</span>\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>losses/Total Loss<span class=\"pl-pds\">'</span></span>, total_loss)\n  \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Specify the optimizer and create the train op:</span>\n    optimizer <span class=\"pl-k\">=</span> tf.train.RMSPropOptimizer(<span class=\"pl-c1\">0.0001</span>, <span class=\"pl-c1\">0.9</span>,\n                                    <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n    train_op <span class=\"pl-k\">=</span> slim.learning.create_train_op(total_loss, optimizer)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the training:</span>\n    final_loss <span class=\"pl-k\">=</span> slim.learning.train(\n        train_op,\n        <span class=\"pl-v\">logdir</span><span class=\"pl-k\">=</span>train_dir,\n        <span class=\"pl-v\">init_fn</span><span class=\"pl-k\">=</span>get_init_fn(),\n        <span class=\"pl-v\">number_of_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10000</span>,\n        <span class=\"pl-v\">save_summaries_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>,\n        <span class=\"pl-v\">save_interval_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>,\n        <span class=\"pl-v\">session_config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options))\n        \n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Finished training. Last batch loss <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> final_loss)</pre></div>\n<p>Notice that in the second part, I do not pass anything into <code>create_train_op</code>'s <code>variables_to_train</code> parameter. This error is then shown</p>\n<pre><code>NotFoundError (see above for traceback): Key InceptionV3/Conv2d_4a_3x3/BatchNorm/beta/RMSProp not found in checkpoint\n\t [[Node: save_1/RestoreV2_49 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2_49/tensor_names, save_1/RestoreV2_49/shape_and_slices)]]\n\t [[Node: save_1/Assign_774/_1550 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_2911_save_1/Assign_774\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n</code></pre>\n<p>I suspect that it's looking for the RMSProp variables for the InceptionV3/Conv2d_4a_3x3 layer, which is non-existent, because I didn't train that layer in the previous checkpoint. I'm not sure how to achieve what I want, as I can see no examples in the documentation about how to do this.</p>", "body_text": "I have the pretty common use case of freezing the bottom layers of Inception and training only the first two layers, after which I lower the learning rate and fine tune the entire Inception model.\nHere is my code for running the first part\ntrain_dir='/home/ubuntu/pynb/TF play/log-inceptionv3flowers'\nwith tf.Graph().as_default():\n    tf.logging.set_verbosity(tf.logging.INFO)\n    \n    dataset = get_dataset()\n    images, _, labels = load_batch(dataset, batch_size=32)\n    \n    # Create the model, use the default arg scope to configure the batch norm parameters.\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n        logits, _ = inception.inception_v3(images, num_classes=5, is_training=True)\n        \n    # Specify the loss function:\n    one_hot_labels = slim.one_hot_encoding(labels, 5)\n    tf.losses.softmax_cross_entropy(one_hot_labels, logits)\n    total_loss = tf.losses.get_total_loss()\n\n    # Create some summaries to visualize the training process:\n    tf.summary.scalar('losses/Total Loss', total_loss)\n  \n    # Specify the optimizer and create the train op:\n    optimizer = tf.train.RMSPropOptimizer(0.001, 0.9,\n                                    momentum=0.9, epsilon=1.0)\n    train_op = slim.learning.create_train_op(total_loss, optimizer, variables_to_train=get_variables_to_train())\n    \n    # Run the training:\n    final_loss = slim.learning.train(\n        train_op,\n        logdir=train_dir,\n        init_fn=get_init_fn(),\n        number_of_steps=4500,\n        save_summaries_secs=30,\n        save_interval_secs=30,\n        session_config=tf.ConfigProto(gpu_options=gpu_options))\n        \nprint('Finished training. Last batch loss %f' % final_loss)\nwhich runs properly, then my code for running the second part\ntrain_dir='/home/ubuntu/pynb/TF play/log-inceptionv3flowers'\nwith tf.Graph().as_default():\n    tf.logging.set_verbosity(tf.logging.INFO)\n    \n    dataset = get_dataset()\n    images, _, labels = load_batch(dataset, batch_size=32)\n    \n    # Create the model, use the default arg scope to configure the batch norm parameters.\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n        logits, _ = inception.inception_v3(images, num_classes=5, is_training=True)\n        \n    # Specify the loss function:\n    one_hot_labels = slim.one_hot_encoding(labels, 5)\n    tf.losses.softmax_cross_entropy(one_hot_labels, logits)\n    total_loss = tf.losses.get_total_loss()\n\n    # Create some summaries to visualize the training process:\n    tf.summary.scalar('losses/Total Loss', total_loss)\n  \n    # Specify the optimizer and create the train op:\n    optimizer = tf.train.RMSPropOptimizer(0.0001, 0.9,\n                                    momentum=0.9, epsilon=1.0)\n    train_op = slim.learning.create_train_op(total_loss, optimizer)\n    \n    # Run the training:\n    final_loss = slim.learning.train(\n        train_op,\n        logdir=train_dir,\n        init_fn=get_init_fn(),\n        number_of_steps=10000,\n        save_summaries_secs=30,\n        save_interval_secs=30,\n        session_config=tf.ConfigProto(gpu_options=gpu_options))\n        \nprint('Finished training. Last batch loss %f' % final_loss)\nNotice that in the second part, I do not pass anything into create_train_op's variables_to_train parameter. This error is then shown\nNotFoundError (see above for traceback): Key InceptionV3/Conv2d_4a_3x3/BatchNorm/beta/RMSProp not found in checkpoint\n\t [[Node: save_1/RestoreV2_49 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2_49/tensor_names, save_1/RestoreV2_49/shape_and_slices)]]\n\t [[Node: save_1/Assign_774/_1550 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_2911_save_1/Assign_774\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n\nI suspect that it's looking for the RMSProp variables for the InceptionV3/Conv2d_4a_3x3 layer, which is non-existent, because I didn't train that layer in the previous checkpoint. I'm not sure how to achieve what I want, as I can see no examples in the documentation about how to do this.", "body": "I have the pretty common use case of freezing the bottom layers of Inception and training only the first two layers, after which I lower the learning rate and fine tune the entire Inception model.\r\n\r\nHere is my code for running the first part\r\n\r\n```python\r\ntrain_dir='/home/ubuntu/pynb/TF play/log-inceptionv3flowers'\r\nwith tf.Graph().as_default():\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    \r\n    dataset = get_dataset()\r\n    images, _, labels = load_batch(dataset, batch_size=32)\r\n    \r\n    # Create the model, use the default arg scope to configure the batch norm parameters.\r\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\r\n        logits, _ = inception.inception_v3(images, num_classes=5, is_training=True)\r\n        \r\n    # Specify the loss function:\r\n    one_hot_labels = slim.one_hot_encoding(labels, 5)\r\n    tf.losses.softmax_cross_entropy(one_hot_labels, logits)\r\n    total_loss = tf.losses.get_total_loss()\r\n\r\n    # Create some summaries to visualize the training process:\r\n    tf.summary.scalar('losses/Total Loss', total_loss)\r\n  \r\n    # Specify the optimizer and create the train op:\r\n    optimizer = tf.train.RMSPropOptimizer(0.001, 0.9,\r\n                                    momentum=0.9, epsilon=1.0)\r\n    train_op = slim.learning.create_train_op(total_loss, optimizer, variables_to_train=get_variables_to_train())\r\n    \r\n    # Run the training:\r\n    final_loss = slim.learning.train(\r\n        train_op,\r\n        logdir=train_dir,\r\n        init_fn=get_init_fn(),\r\n        number_of_steps=4500,\r\n        save_summaries_secs=30,\r\n        save_interval_secs=30,\r\n        session_config=tf.ConfigProto(gpu_options=gpu_options))\r\n        \r\nprint('Finished training. Last batch loss %f' % final_loss)\r\n```\r\n\r\nwhich runs properly, then my code for running the second part\r\n\r\n```python\r\ntrain_dir='/home/ubuntu/pynb/TF play/log-inceptionv3flowers'\r\nwith tf.Graph().as_default():\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    \r\n    dataset = get_dataset()\r\n    images, _, labels = load_batch(dataset, batch_size=32)\r\n    \r\n    # Create the model, use the default arg scope to configure the batch norm parameters.\r\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\r\n        logits, _ = inception.inception_v3(images, num_classes=5, is_training=True)\r\n        \r\n    # Specify the loss function:\r\n    one_hot_labels = slim.one_hot_encoding(labels, 5)\r\n    tf.losses.softmax_cross_entropy(one_hot_labels, logits)\r\n    total_loss = tf.losses.get_total_loss()\r\n\r\n    # Create some summaries to visualize the training process:\r\n    tf.summary.scalar('losses/Total Loss', total_loss)\r\n  \r\n    # Specify the optimizer and create the train op:\r\n    optimizer = tf.train.RMSPropOptimizer(0.0001, 0.9,\r\n                                    momentum=0.9, epsilon=1.0)\r\n    train_op = slim.learning.create_train_op(total_loss, optimizer)\r\n    \r\n    # Run the training:\r\n    final_loss = slim.learning.train(\r\n        train_op,\r\n        logdir=train_dir,\r\n        init_fn=get_init_fn(),\r\n        number_of_steps=10000,\r\n        save_summaries_secs=30,\r\n        save_interval_secs=30,\r\n        session_config=tf.ConfigProto(gpu_options=gpu_options))\r\n        \r\nprint('Finished training. Last batch loss %f' % final_loss)\r\n```\r\n\r\nNotice that in the second part, I do not pass anything into `create_train_op`'s `variables_to_train` parameter. This error is then shown\r\n\r\n```\r\nNotFoundError (see above for traceback): Key InceptionV3/Conv2d_4a_3x3/BatchNorm/beta/RMSProp not found in checkpoint\r\n\t [[Node: save_1/RestoreV2_49 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2_49/tensor_names, save_1/RestoreV2_49/shape_and_slices)]]\r\n\t [[Node: save_1/Assign_774/_1550 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_2911_save_1/Assign_774\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n```\r\n\r\nI suspect that it's looking for the RMSProp variables for the InceptionV3/Conv2d_4a_3x3 layer, which is non-existent, because I didn't train that layer in the previous checkpoint. I'm not sure how to achieve what I want, as I can see no examples in the documentation about how to do this."}