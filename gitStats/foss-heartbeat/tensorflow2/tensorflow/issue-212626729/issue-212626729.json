{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8182", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8182/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8182/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8182/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8182", "id": 212626729, "node_id": "MDU6SXNzdWUyMTI2MjY3Mjk=", "number": 8182, "title": "bucket_by_sequence_length does not dequeue all items given to it from another queue", "user": {"login": "nOkuda", "id": 1238620, "node_id": "MDQ6VXNlcjEyMzg2MjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1238620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nOkuda", "html_url": "https://github.com/nOkuda", "followers_url": "https://api.github.com/users/nOkuda/followers", "following_url": "https://api.github.com/users/nOkuda/following{/other_user}", "gists_url": "https://api.github.com/users/nOkuda/gists{/gist_id}", "starred_url": "https://api.github.com/users/nOkuda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nOkuda/subscriptions", "organizations_url": "https://api.github.com/users/nOkuda/orgs", "repos_url": "https://api.github.com/users/nOkuda/repos", "events_url": "https://api.github.com/users/nOkuda/events{/privacy}", "received_events_url": "https://api.github.com/users/nOkuda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "joel-shor", "id": 6020988, "node_id": "MDQ6VXNlcjYwMjA5ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6020988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joel-shor", "html_url": "https://github.com/joel-shor", "followers_url": "https://api.github.com/users/joel-shor/followers", "following_url": "https://api.github.com/users/joel-shor/following{/other_user}", "gists_url": "https://api.github.com/users/joel-shor/gists{/gist_id}", "starred_url": "https://api.github.com/users/joel-shor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joel-shor/subscriptions", "organizations_url": "https://api.github.com/users/joel-shor/orgs", "repos_url": "https://api.github.com/users/joel-shor/repos", "events_url": "https://api.github.com/users/joel-shor/events{/privacy}", "received_events_url": "https://api.github.com/users/joel-shor/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "joel-shor", "id": 6020988, "node_id": "MDQ6VXNlcjYwMjA5ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6020988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joel-shor", "html_url": "https://github.com/joel-shor", "followers_url": "https://api.github.com/users/joel-shor/followers", "following_url": "https://api.github.com/users/joel-shor/following{/other_user}", "gists_url": "https://api.github.com/users/joel-shor/gists{/gist_id}", "starred_url": "https://api.github.com/users/joel-shor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joel-shor/subscriptions", "organizations_url": "https://api.github.com/users/joel-shor/orgs", "repos_url": "https://api.github.com/users/joel-shor/repos", "events_url": "https://api.github.com/users/joel-shor/events{/privacy}", "received_events_url": "https://api.github.com/users/joel-shor/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2017-03-08T03:18:31Z", "updated_at": "2018-01-06T19:49:58Z", "closed_at": "2018-01-06T19:49:58Z", "author_association": "NONE", "body_html": "<p>I've got some SequenceExamples gzipped in a file that I read with TFRecordReader.  I then enqueue those serialized SequenceExamples onto a RandomShuffleQueue.  Then, I dequeue the RandomShuffleQueue, get a number I need from the serialized SequenceExample, and pass both the number and the SequenceExample into bucket_by_sequence_length.</p>\n<p>Since there are 100 items stored in the file, I expected to get 20 batches of 5.  Instead, I'm getting 17 batches of 5, with many of the smallest input_length items missing.</p>\n<p>My guess is that the top queue on batch_by_sequence_length is looking at its bucket queues, finding that only very small and very large inputs remain, and so refuses to put them together for the final batch(es).  I was hoping that by setting <code>allow_smaller_final_batch</code> to <code>True</code>, I could avoid that problem, but I guess not.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>GitHub issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"189281953\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5609\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5609/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/5609\">#5609</a> (the discussion at the very bottom of this thread seems to be the only place on the web [as of 7 Mar 2017] with a discussion on how to actually use bucket_by_sequence_length)</p>\n<h3>Environment info</h3>\n<p>Operating System:  Fedora 24</p>\n<p>Installed version of CUDA and cuDNN:  8.0.44, 5.1.5<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n-rw-r--r-- 1 root root   558720 Nov 24 09:02 libcudadevrt.a<br>\nlrwxrwxrwx 1 root root       16 Nov 24 09:02 libcudart.so -&gt; libcudart.so.8.0<br>\nlrwxrwxrwx 1 root root       19 Nov 24 09:02 libcudart.so.8.0 -&gt; libcudart.so.8.0.44<br>\n-rwxr-xr-x 1 root root   415432 Nov 24 09:02 libcudart.so.8.0.44<br>\n-rw-r--r-- 1 root root   775162 Nov 24 09:02 libcudart_static.a<br>\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so<br>\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5<br>\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5.1.5<br>\n-rw-r--r-- 1 root root 69756172 Nov 24 09:05 libcudnn_static.a</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed:  <a href=\"https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl\" rel=\"nofollow\">https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl</a> &lt;- installed on 3 Mar 2017</li>\n<li>The output from <code>python3 -c \"import tensorflow; print(tensorflow.__version__)\"</code>.  1.0.0</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>Save the following code as <code>test_bucket.py</code>, download <a href=\"https://github.com/tensorflow/tensorflow/files/826517/other.gz\">other.gz</a> into the same directory, then run<br>\n<code>python3 test_bucket.py --intput_file other.gz --run_type batches</code></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-c1\">LAST_RUNNER</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_dequeue<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-c1\">CONTEXT_FEATURES</span> <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>aas_length<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64),\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>funcs_length<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)}\n<span class=\"pl-c1\">SEQUENCE_FEATURES</span> <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>aas<span class=\"pl-pds\">'</span></span>: tf.FixedLenSequenceFeature([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64),\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>funcs<span class=\"pl-pds\">'</span></span>: tf.FixedLenSequenceFeature([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)}\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">read_aas_length</span>(<span class=\"pl-smi\">serialized</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Read aas_length<span class=\"pl-pds\">\"\"\"</span></span>\n    context_parsed, _ <span class=\"pl-k\">=</span> tf.parse_single_sequence_example(\n        <span class=\"pl-v\">serialized</span><span class=\"pl-k\">=</span>serialized,\n        <span class=\"pl-v\">context_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">CONTEXT_FEATURES</span>,\n        <span class=\"pl-v\">sequence_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">SEQUENCE_FEATURES</span>)\n    <span class=\"pl-k\">return</span> context_parsed[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>aas_length<span class=\"pl-pds\">'</span></span>]\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">args</span>):\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> filename_queue is FIFOQueue</span>\n        filename_queue <span class=\"pl-k\">=</span> tf.train.string_input_producer(\n            [args.input_file],\n            <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>tfrecord_filename_queue<span class=\"pl-pds\">'</span></span>)\n        reader <span class=\"pl-k\">=</span> tf.TFRecordReader(\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>tfrecord_reader<span class=\"pl-pds\">'</span></span>,\n            <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>tf.python_io.TFRecordOptions(\n                tf.python_io.TFRecordCompressionType.<span class=\"pl-c1\">GZIP</span>))\n\n        _, next_raw <span class=\"pl-k\">=</span> reader.read(\n            filename_queue,\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>read_records<span class=\"pl-pds\">'</span></span>)\n        random_raws <span class=\"pl-k\">=</span> tf.RandomShuffleQueue(\n            <span class=\"pl-v\">capacity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">50</span>,\n            <span class=\"pl-v\">min_after_dequeue</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>,\n            <span class=\"pl-v\">dtypes</span><span class=\"pl-k\">=</span>tf.string,\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> http://stackoverflow.com/questions/42119238/tensorflow-fifoqueues-dequeuemany-and-dequeueupto-require-the-components-to-ha</span>\n            <span class=\"pl-v\">shapes</span><span class=\"pl-k\">=</span>[()],\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>randomize_records<span class=\"pl-pds\">'</span></span>)\n        enqueue_random_raws <span class=\"pl-k\">=</span> random_raws.enqueue(next_raw)\n\n        serialized_example_dq <span class=\"pl-k\">=</span> random_raws.dequeue()\n        aas_length <span class=\"pl-k\">=</span> read_aas_length(serialized_example_dq)\n        <span class=\"pl-k\">if</span> args.run_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batches<span class=\"pl-pds\">'</span></span>:\n            batch_max_lens, batches <span class=\"pl-k\">=</span> \\\n                tf.contrib.training.bucket_by_sequence_length(\n                    tf.to_int32(aas_length),\n                    [serialized_example_dq],\n                    <span class=\"pl-c1\">5</span>,\n                    [<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">400</span>, <span class=\"pl-c1\">500</span>, <span class=\"pl-c1\">1000</span>],\n                    <span class=\"pl-v\">allow_smaller_final_batch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LAST_RUNNER</span>)\n\n        init <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n        sess.run(init)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> necessary when num_epochs in string_input_producer is not None</span>\n        sess.run(tf.local_variables_initializer())\n\n        random_records_runner <span class=\"pl-k\">=</span> tf.train.QueueRunner(\n            random_raws,\n            [enqueue_random_raws] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span>)\n        coord <span class=\"pl-k\">=</span> tf.train.Coordinator()\n        tf.train.add_queue_runner(random_records_runner)\n        threads <span class=\"pl-k\">=</span> tf.train.start_queue_runners(<span class=\"pl-v\">coord</span><span class=\"pl-k\">=</span>coord)\n        working <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n        i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n        <span class=\"pl-k\">if</span> args.run_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batches<span class=\"pl-pds\">'</span></span>:\n            fetch <span class=\"pl-k\">=</span> {\n                <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_max_lens<span class=\"pl-pds\">'</span></span>: batch_max_lens,\n                <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batches<span class=\"pl-pds\">'</span></span>: batches}\n\n        <span class=\"pl-k\">while</span> working:\n            <span class=\"pl-k\">try</span>:\n                i <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n                <span class=\"pl-k\">if</span> args.run_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>batches<span class=\"pl-pds\">'</span></span>:\n                    fetched <span class=\"pl-k\">=</span> sess.run(fetch)\n                    <span class=\"pl-c1\">print</span>(i, fetched[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_max_lens<span class=\"pl-pds\">'</span></span>])\n                <span class=\"pl-k\">else</span>:\n                    <span class=\"pl-c1\">print</span>(i, sess.run(aas_length))\n            <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError <span class=\"pl-k\">as</span> err:\n                coord.request_stop()\n                working <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\n        coord.join(threads)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n    parser.add_argument(\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--input_file<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n        <span class=\"pl-v\">required</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Serialized file path... ?<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--run_type<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n        <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Choose \"batches\" to run in batches<span class=\"pl-pds\">'</span></span>)\n    args <span class=\"pl-k\">=</span> parser.parse_args()\n    main(args)</pre></div>\n<h3>What other attempted solutions have you tried?</h3>\n<p>I've tried collecting relevant data in the while loop and then feeding them into a placeholder for batch_by_sequence_length.  That yielded an error message about expecting a string but getting a _Message, or something to that effect.</p>\n<p>I've also looked at the node_def.name of the OutOfRangeError.  It only ever came up with the top queue created by bucket_by_sequence_length.</p>\n<p>Somewhat related, calling bucket_by_sequence_length but not fetching its outputs (so taking the function call out of the if block and running the code without the <code>--run_type</code> flag) still caused the list of input lengths to stop at 99 (not 100, as I would expect).</p>\n<p>I've also observed that the missing smaller numbers will show up if <code>num_epochs</code> on the TFRecordReader is set to 2, although they only appear once.</p>\n<p>There were other failed attempts, but I don't recall their details.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/826529/output_batches.txt\">output_batches.txt</a>:  Example output of not getting back enough batches.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/826530/output_list.txt\">output_list.txt</a>:  Example output of printing out all lengths (note that some lengths are less than 100, but there are no lengths in the batches less than 100).</p>", "body_text": "I've got some SequenceExamples gzipped in a file that I read with TFRecordReader.  I then enqueue those serialized SequenceExamples onto a RandomShuffleQueue.  Then, I dequeue the RandomShuffleQueue, get a number I need from the serialized SequenceExample, and pass both the number and the SequenceExample into bucket_by_sequence_length.\nSince there are 100 items stored in the file, I expected to get 20 batches of 5.  Instead, I'm getting 17 batches of 5, with many of the smallest input_length items missing.\nMy guess is that the top queue on batch_by_sequence_length is looking at its bucket queues, finding that only very small and very large inputs remain, and so refuses to put them together for the final batch(es).  I was hoping that by setting allow_smaller_final_batch to True, I could avoid that problem, but I guess not.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nGitHub issue #5609 (the discussion at the very bottom of this thread seems to be the only place on the web [as of 7 Mar 2017] with a discussion on how to actually use bucket_by_sequence_length)\nEnvironment info\nOperating System:  Fedora 24\nInstalled version of CUDA and cuDNN:  8.0.44, 5.1.5\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root   558720 Nov 24 09:02 libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Nov 24 09:02 libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Nov 24 09:02 libcudart.so.8.0 -> libcudart.so.8.0.44\n-rwxr-xr-x 1 root root   415432 Nov 24 09:02 libcudart.so.8.0.44\n-rw-r--r-- 1 root root   775162 Nov 24 09:02 libcudart_static.a\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Nov 24 09:05 libcudnn_static.a\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:  https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl <- installed on 3 Mar 2017\nThe output from python3 -c \"import tensorflow; print(tensorflow.__version__)\".  1.0.0\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nSave the following code as test_bucket.py, download other.gz into the same directory, then run\npython3 test_bucket.py --intput_file other.gz --run_type batches\nimport argparse\nimport tensorflow as tf\n\n\nLAST_RUNNER = 'batch_dequeue'\nCONTEXT_FEATURES = {\n    'aas_length': tf.FixedLenFeature([], dtype=tf.int64),\n    'funcs_length': tf.FixedLenFeature([], dtype=tf.int64)}\nSEQUENCE_FEATURES = {\n    'aas': tf.FixedLenSequenceFeature([], dtype=tf.int64),\n    'funcs': tf.FixedLenSequenceFeature([], dtype=tf.int64)}\n\n\ndef read_aas_length(serialized):\n    \"\"\"Read aas_length\"\"\"\n    context_parsed, _ = tf.parse_single_sequence_example(\n        serialized=serialized,\n        context_features=CONTEXT_FEATURES,\n        sequence_features=SEQUENCE_FEATURES)\n    return context_parsed['aas_length']\n\n\ndef main(args):\n    with tf.Session() as sess:\n        # filename_queue is FIFOQueue\n        filename_queue = tf.train.string_input_producer(\n            [args.input_file],\n            num_epochs=1,\n            name='tfrecord_filename_queue')\n        reader = tf.TFRecordReader(\n            name='tfrecord_reader',\n            options=tf.python_io.TFRecordOptions(\n                tf.python_io.TFRecordCompressionType.GZIP))\n\n        _, next_raw = reader.read(\n            filename_queue,\n            name='read_records')\n        random_raws = tf.RandomShuffleQueue(\n            capacity=50,\n            min_after_dequeue=0,\n            dtypes=tf.string,\n            # http://stackoverflow.com/questions/42119238/tensorflow-fifoqueues-dequeuemany-and-dequeueupto-require-the-components-to-ha\n            shapes=[()],\n            name='randomize_records')\n        enqueue_random_raws = random_raws.enqueue(next_raw)\n\n        serialized_example_dq = random_raws.dequeue()\n        aas_length = read_aas_length(serialized_example_dq)\n        if args.run_type == 'batches':\n            batch_max_lens, batches = \\\n                tf.contrib.training.bucket_by_sequence_length(\n                    tf.to_int32(aas_length),\n                    [serialized_example_dq],\n                    5,\n                    [100, 200, 300, 400, 500, 1000],\n                    allow_smaller_final_batch=True,\n                    name=LAST_RUNNER)\n\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        # necessary when num_epochs in string_input_producer is not None\n        sess.run(tf.local_variables_initializer())\n\n        random_records_runner = tf.train.QueueRunner(\n            random_raws,\n            [enqueue_random_raws] * 4)\n        coord = tf.train.Coordinator()\n        tf.train.add_queue_runner(random_records_runner)\n        threads = tf.train.start_queue_runners(coord=coord)\n        working = True\n        i = 0\n\n        if args.run_type == 'batches':\n            fetch = {\n                'batch_max_lens': batch_max_lens,\n                'batches': batches}\n\n        while working:\n            try:\n                i += 1\n                if args.run_type == 'batches':\n                    fetched = sess.run(fetch)\n                    print(i, fetched['batch_max_lens'])\n                else:\n                    print(i, sess.run(aas_length))\n            except tf.errors.OutOfRangeError as err:\n                coord.request_stop()\n                working = False\n\n        coord.join(threads)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--input_file',\n        type=str,\n        required=True,\n        help='Serialized file path... ?')\n    parser.add_argument(\n        '--run_type',\n        type=str,\n        default='',\n        help='Choose \"batches\" to run in batches')\n    args = parser.parse_args()\n    main(args)\nWhat other attempted solutions have you tried?\nI've tried collecting relevant data in the while loop and then feeding them into a placeholder for batch_by_sequence_length.  That yielded an error message about expecting a string but getting a _Message, or something to that effect.\nI've also looked at the node_def.name of the OutOfRangeError.  It only ever came up with the top queue created by bucket_by_sequence_length.\nSomewhat related, calling bucket_by_sequence_length but not fetching its outputs (so taking the function call out of the if block and running the code without the --run_type flag) still caused the list of input lengths to stop at 99 (not 100, as I would expect).\nI've also observed that the missing smaller numbers will show up if num_epochs on the TFRecordReader is set to 2, although they only appear once.\nThere were other failed attempts, but I don't recall their details.\nLogs or other output that would be helpful\noutput_batches.txt:  Example output of not getting back enough batches.\noutput_list.txt:  Example output of printing out all lengths (note that some lengths are less than 100, but there are no lengths in the batches less than 100).", "body": "I've got some SequenceExamples gzipped in a file that I read with TFRecordReader.  I then enqueue those serialized SequenceExamples onto a RandomShuffleQueue.  Then, I dequeue the RandomShuffleQueue, get a number I need from the serialized SequenceExample, and pass both the number and the SequenceExample into bucket_by_sequence_length.\r\n\r\nSince there are 100 items stored in the file, I expected to get 20 batches of 5.  Instead, I'm getting 17 batches of 5, with many of the smallest input_length items missing.\r\n\r\nMy guess is that the top queue on batch_by_sequence_length is looking at its bucket queues, finding that only very small and very large inputs remain, and so refuses to put them together for the final batch(es).  I was hoping that by setting `allow_smaller_final_batch` to `True`, I could avoid that problem, but I guess not.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nGitHub issue #5609 (the discussion at the very bottom of this thread seems to be the only place on the web [as of 7 Mar 2017] with a discussion on how to actually use bucket_by_sequence_length)\r\n\r\n### Environment info\r\nOperating System:  Fedora 24\r\n\r\nInstalled version of CUDA and cuDNN:  8.0.44, 5.1.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 Nov 24 09:02 libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Nov 24 09:02 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Nov 24 09:02 libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Nov 24 09:02 libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Nov 24 09:02 libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 24 09:05 libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:  https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl <- installed on 3 Mar 2017\r\n2. The output from `python3 -c \"import tensorflow; print(tensorflow.__version__)\"`.  1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nSave the following code as `test_bucket.py`, download [other.gz](https://github.com/tensorflow/tensorflow/files/826517/other.gz) into the same directory, then run \r\n```python3 test_bucket.py --intput_file other.gz --run_type batches```\r\n\r\n\r\n\r\n```Python\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\n\r\nLAST_RUNNER = 'batch_dequeue'\r\nCONTEXT_FEATURES = {\r\n    'aas_length': tf.FixedLenFeature([], dtype=tf.int64),\r\n    'funcs_length': tf.FixedLenFeature([], dtype=tf.int64)}\r\nSEQUENCE_FEATURES = {\r\n    'aas': tf.FixedLenSequenceFeature([], dtype=tf.int64),\r\n    'funcs': tf.FixedLenSequenceFeature([], dtype=tf.int64)}\r\n\r\n\r\ndef read_aas_length(serialized):\r\n    \"\"\"Read aas_length\"\"\"\r\n    context_parsed, _ = tf.parse_single_sequence_example(\r\n        serialized=serialized,\r\n        context_features=CONTEXT_FEATURES,\r\n        sequence_features=SEQUENCE_FEATURES)\r\n    return context_parsed['aas_length']\r\n\r\n\r\ndef main(args):\r\n    with tf.Session() as sess:\r\n        # filename_queue is FIFOQueue\r\n        filename_queue = tf.train.string_input_producer(\r\n            [args.input_file],\r\n            num_epochs=1,\r\n            name='tfrecord_filename_queue')\r\n        reader = tf.TFRecordReader(\r\n            name='tfrecord_reader',\r\n            options=tf.python_io.TFRecordOptions(\r\n                tf.python_io.TFRecordCompressionType.GZIP))\r\n\r\n        _, next_raw = reader.read(\r\n            filename_queue,\r\n            name='read_records')\r\n        random_raws = tf.RandomShuffleQueue(\r\n            capacity=50,\r\n            min_after_dequeue=0,\r\n            dtypes=tf.string,\r\n            # http://stackoverflow.com/questions/42119238/tensorflow-fifoqueues-dequeuemany-and-dequeueupto-require-the-components-to-ha\r\n            shapes=[()],\r\n            name='randomize_records')\r\n        enqueue_random_raws = random_raws.enqueue(next_raw)\r\n\r\n        serialized_example_dq = random_raws.dequeue()\r\n        aas_length = read_aas_length(serialized_example_dq)\r\n        if args.run_type == 'batches':\r\n            batch_max_lens, batches = \\\r\n                tf.contrib.training.bucket_by_sequence_length(\r\n                    tf.to_int32(aas_length),\r\n                    [serialized_example_dq],\r\n                    5,\r\n                    [100, 200, 300, 400, 500, 1000],\r\n                    allow_smaller_final_batch=True,\r\n                    name=LAST_RUNNER)\r\n\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n        # necessary when num_epochs in string_input_producer is not None\r\n        sess.run(tf.local_variables_initializer())\r\n\r\n        random_records_runner = tf.train.QueueRunner(\r\n            random_raws,\r\n            [enqueue_random_raws] * 4)\r\n        coord = tf.train.Coordinator()\r\n        tf.train.add_queue_runner(random_records_runner)\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        working = True\r\n        i = 0\r\n\r\n        if args.run_type == 'batches':\r\n            fetch = {\r\n                'batch_max_lens': batch_max_lens,\r\n                'batches': batches}\r\n\r\n        while working:\r\n            try:\r\n                i += 1\r\n                if args.run_type == 'batches':\r\n                    fetched = sess.run(fetch)\r\n                    print(i, fetched['batch_max_lens'])\r\n                else:\r\n                    print(i, sess.run(aas_length))\r\n            except tf.errors.OutOfRangeError as err:\r\n                coord.request_stop()\r\n                working = False\r\n\r\n        coord.join(threads)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        '--input_file',\r\n        type=str,\r\n        required=True,\r\n        help='Serialized file path... ?')\r\n    parser.add_argument(\r\n        '--run_type',\r\n        type=str,\r\n        default='',\r\n        help='Choose \"batches\" to run in batches')\r\n    args = parser.parse_args()\r\n    main(args)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nI've tried collecting relevant data in the while loop and then feeding them into a placeholder for batch_by_sequence_length.  That yielded an error message about expecting a string but getting a _Message, or something to that effect.\r\n\r\nI've also looked at the node_def.name of the OutOfRangeError.  It only ever came up with the top queue created by bucket_by_sequence_length.\r\n\r\nSomewhat related, calling bucket_by_sequence_length but not fetching its outputs (so taking the function call out of the if block and running the code without the `--run_type` flag) still caused the list of input lengths to stop at 99 (not 100, as I would expect).\r\n\r\nI've also observed that the missing smaller numbers will show up if `num_epochs` on the TFRecordReader is set to 2, although they only appear once.\r\n\r\nThere were other failed attempts, but I don't recall their details.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n[output_batches.txt](https://github.com/tensorflow/tensorflow/files/826529/output_batches.txt):  Example output of not getting back enough batches.\r\n[output_list.txt](https://github.com/tensorflow/tensorflow/files/826530/output_list.txt):  Example output of printing out all lengths (note that some lengths are less than 100, but there are no lengths in the batches less than 100).\r\n"}