{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19901", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19901/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19901/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19901/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19901", "id": 331078467, "node_id": "MDU6SXNzdWUzMzEwNzg0Njc=", "number": 19901, "title": "[TF-serving-r1.7] How to compile Tensorflow-serving r1.7 with TensorRT", "user": {"login": "oscarriddle", "id": 13745902, "node_id": "MDQ6VXNlcjEzNzQ1OTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/13745902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oscarriddle", "html_url": "https://github.com/oscarriddle", "followers_url": "https://api.github.com/users/oscarriddle/followers", "following_url": "https://api.github.com/users/oscarriddle/following{/other_user}", "gists_url": "https://api.github.com/users/oscarriddle/gists{/gist_id}", "starred_url": "https://api.github.com/users/oscarriddle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oscarriddle/subscriptions", "organizations_url": "https://api.github.com/users/oscarriddle/orgs", "repos_url": "https://api.github.com/users/oscarriddle/repos", "events_url": "https://api.github.com/users/oscarriddle/events{/privacy}", "received_events_url": "https://api.github.com/users/oscarriddle/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-06-11T07:46:17Z", "updated_at": "2018-07-10T03:46:32Z", "closed_at": "2018-07-10T03:46:32Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  CentOS 7</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: tensorflow-serving r1.7 source code</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow-serving r1.7 source code</li>\n<li><strong>Python version</strong>: Python2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc5.3</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA9.0, cuDNN7.0.5</li>\n<li><strong>GPU model and memory</strong>: TitanXP 12GB</li>\n<li><strong>TensorRT Installed Version</strong>: 4.0.4 (actually)</li>\n<li><strong>Exact command to reproduce</strong>: The way of compiling tf-serving 1.7 with TenosrRT, Please refer to my environment variables setting and compilation command as below.</li>\n</ul>\n<hr>\n<p>This issue is originally posted in repo tensorflow-serving (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"330249907\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/serving/issues/925\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/serving/issues/925/hovercard\" href=\"https://github.com/tensorflow/serving/issues/925\">tensorflow/serving#925</a>). After several days of effort, I still failed to find a way to solve this problem. Hope someone can give me a clue of how to build tensorflow-serving 1.7 with tensorrt, because there are really few docs about this. (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10539540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samikama\">@samikama</a> )</p>\n<hr>\n<p>I tried to compile the Tensorflow-serving r1.7 with TensorRT 4.0.4, and the compilation is successfully done.</p>\n<pre><code>At global scope:\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\nINFO: Elapsed time: 1452.421s, Critical Path: 479.68s\nINFO: Build completed successfully, 11375 total actions\n</code></pre>\n<p>But when I start the service and load a TFTRT optimized model, I get error:</p>\n<pre><code>2018-06-07 17:41:40.910874: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /media/disk1/fordata/web_server/project/LdaBasedClassification_623_1.7/data/cate155_tftrt_frozen/1\n2018-06-07 17:41:41.030117: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-06-07 17:41:41.283451: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:84:00.0\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\n2018-06-07 17:41:41.283514: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-06-07 17:41:41.601178: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-07 17:41:41.601253: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \n2018-06-07 17:41:41.601273: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \n2018-06-07 17:41:41.601561: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10970 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)\n2018-06-07 17:41:41.878689: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 967809 microseconds.\n2018-06-07 17:41:41.878771: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: inception_v3 version: 1} failed: Not found: Op type not registered 'TRTEngineOp' in binary running on bjpg-g180.yz02. Make sure the Op and Kernel are registered in the binary running in this process.\n\n</code></pre>\n<p>Looks like the TRTEngineOp is still not supported by this execution file.<br>\nThough I'm not 100% sure about my way of compiling Tensorflow-serving 1.7 with TRT, but I think the compilation indeed searched and found the libnvinfer.so, etc, and also checked the TensorRT version is correct. So I don't know why the binary executive file still can't support TRTEngineOp.</p>\n<p>Here is my environment variables:</p>\n<pre><code>export TENSORRT_INSTALL_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\nexport TF_TENSORRT_VERSION=4.0.4\nexport TENSORRT_LIB_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\n</code></pre>\n<p>This is my compilation command:</p>\n<pre><code>sed -i.bak 's/@org_tensorflow\\/\\/third_party\\/gpus\\/crosstool/@local_config_cuda\\/\\/crosstool:toolchain/g' tools/bazel.rc      \nbazel build  --config=cuda --action_env PYTHON_BIN_PATH=\"/home/karafuto/dlpy72/dlpy/bin/python2.7\" TENSORRT_BIN_PATH=\"/home/karafuto/TensorRT-3.0.4\"  -c opt tensorflow_serving/...\n\n</code></pre>\n<p>I'm not sure whether my procedure is correct. Really few of docs can be found that talk about how to build the tensorflow-serving 1.7 with tensorrt. Any idea will be welcome?</p>\n<p>Thanks,</p>\n<hr>\n<p>PS:  The tensorrt source code is downloaded from NVIDIA official website, which tar file is named \"TensorRT-3.0.4.Ubuntu-14.04.5.x86_64.cuda-9.0.cudnn7.0.tar.gz\". The weird thing is, I unpacked the tar file and find the actually version is 4.0.4 not 3.0.4. So in the tensorflow-serving-r1.7, I need to set the variable TF_TENSORRT_VERSION=4.0.4 to avoid version check failure.</p>\n<p>I encountered below 2 errors and solved them, so I think the bazel compilation shall indeed compiled the TensorRT. Post here as an evidence.</p>\n<hr>\n<p>This is the error when I set wrong TENSORRT_LIB_PATH, (can't find libnvinfer.so):</p>\n<pre><code>ERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 160\n\t\tauto_configure_fail(\"TensorRT library (libnvinfer) v...\")\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\n\nCuda Configuration Error: TensorRT library (libnvinfer) version is not set.\n</code></pre>\n<hr>\n<p>This is when the TF_TENSORRT_VERSION is not the same with found libnvinfer:</p>\n<pre><code>ERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 167\n\t\t_trt_lib_version(repository_ctx, trt_install_path)\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 87, in _trt_lib_version\n\t\tauto_configure_fail((\"TensorRT library version detec...)))\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\n\nCuda Configuration Error: TensorRT library version detected from /media/disk1/fordata/web_server/project/xiaolun/TensorRT-3.0.4/include/NvInfer.h (4.0.4) does not match TF_TENSORRT_VERSION (3.0.4). To fix this rerun configure again.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS 7\nTensorFlow installed from (source or binary): tensorflow-serving r1.7 source code\nTensorFlow version (use command below): tensorflow-serving r1.7 source code\nPython version: Python2.7\nBazel version (if compiling from source): 0.11.1\nGCC/Compiler version (if compiling from source): gcc5.3\nCUDA/cuDNN version: CUDA9.0, cuDNN7.0.5\nGPU model and memory: TitanXP 12GB\nTensorRT Installed Version: 4.0.4 (actually)\nExact command to reproduce: The way of compiling tf-serving 1.7 with TenosrRT, Please refer to my environment variables setting and compilation command as below.\n\n\nThis issue is originally posted in repo tensorflow-serving (tensorflow/serving#925). After several days of effort, I still failed to find a way to solve this problem. Hope someone can give me a clue of how to build tensorflow-serving 1.7 with tensorrt, because there are really few docs about this. (@samikama )\n\nI tried to compile the Tensorflow-serving r1.7 with TensorRT 4.0.4, and the compilation is successfully done.\nAt global scope:\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\nINFO: Elapsed time: 1452.421s, Critical Path: 479.68s\nINFO: Build completed successfully, 11375 total actions\n\nBut when I start the service and load a TFTRT optimized model, I get error:\n2018-06-07 17:41:40.910874: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /media/disk1/fordata/web_server/project/LdaBasedClassification_623_1.7/data/cate155_tftrt_frozen/1\n2018-06-07 17:41:41.030117: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-06-07 17:41:41.283451: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:84:00.0\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\n2018-06-07 17:41:41.283514: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-06-07 17:41:41.601178: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-07 17:41:41.601253: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \n2018-06-07 17:41:41.601273: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \n2018-06-07 17:41:41.601561: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10970 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)\n2018-06-07 17:41:41.878689: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 967809 microseconds.\n2018-06-07 17:41:41.878771: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: inception_v3 version: 1} failed: Not found: Op type not registered 'TRTEngineOp' in binary running on bjpg-g180.yz02. Make sure the Op and Kernel are registered in the binary running in this process.\n\n\nLooks like the TRTEngineOp is still not supported by this execution file.\nThough I'm not 100% sure about my way of compiling Tensorflow-serving 1.7 with TRT, but I think the compilation indeed searched and found the libnvinfer.so, etc, and also checked the TensorRT version is correct. So I don't know why the binary executive file still can't support TRTEngineOp.\nHere is my environment variables:\nexport TENSORRT_INSTALL_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\nexport TF_TENSORRT_VERSION=4.0.4\nexport TENSORRT_LIB_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\n\nThis is my compilation command:\nsed -i.bak 's/@org_tensorflow\\/\\/third_party\\/gpus\\/crosstool/@local_config_cuda\\/\\/crosstool:toolchain/g' tools/bazel.rc      \nbazel build  --config=cuda --action_env PYTHON_BIN_PATH=\"/home/karafuto/dlpy72/dlpy/bin/python2.7\" TENSORRT_BIN_PATH=\"/home/karafuto/TensorRT-3.0.4\"  -c opt tensorflow_serving/...\n\n\nI'm not sure whether my procedure is correct. Really few of docs can be found that talk about how to build the tensorflow-serving 1.7 with tensorrt. Any idea will be welcome?\nThanks,\n\nPS:  The tensorrt source code is downloaded from NVIDIA official website, which tar file is named \"TensorRT-3.0.4.Ubuntu-14.04.5.x86_64.cuda-9.0.cudnn7.0.tar.gz\". The weird thing is, I unpacked the tar file and find the actually version is 4.0.4 not 3.0.4. So in the tensorflow-serving-r1.7, I need to set the variable TF_TENSORRT_VERSION=4.0.4 to avoid version check failure.\nI encountered below 2 errors and solved them, so I think the bazel compilation shall indeed compiled the TensorRT. Post here as an evidence.\n\nThis is the error when I set wrong TENSORRT_LIB_PATH, (can't find libnvinfer.so):\nERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 160\n\t\tauto_configure_fail(\"TensorRT library (libnvinfer) v...\")\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\n\nCuda Configuration Error: TensorRT library (libnvinfer) version is not set.\n\n\nThis is when the TF_TENSORRT_VERSION is not the same with found libnvinfer:\nERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 167\n\t\t_trt_lib_version(repository_ctx, trt_install_path)\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 87, in _trt_lib_version\n\t\tauto_configure_fail((\"TensorRT library version detec...)))\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\n\nCuda Configuration Error: TensorRT library version detected from /media/disk1/fordata/web_server/project/xiaolun/TensorRT-3.0.4/include/NvInfer.h (4.0.4) does not match TF_TENSORRT_VERSION (3.0.4). To fix this rerun configure again.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  CentOS 7\r\n- **TensorFlow installed from (source or binary)**: tensorflow-serving r1.7 source code\r\n- **TensorFlow version (use command below)**: tensorflow-serving r1.7 source code\r\n- **Python version**: Python2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc5.3\r\n- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5\r\n- **GPU model and memory**: TitanXP 12GB\r\n- **TensorRT Installed Version**: 4.0.4 (actually) \r\n- **Exact command to reproduce**: The way of compiling tf-serving 1.7 with TenosrRT, Please refer to my environment variables setting and compilation command as below.\r\n\r\n****************************************\r\nThis issue is originally posted in repo tensorflow-serving (https://github.com/tensorflow/serving/issues/925). After several days of effort, I still failed to find a way to solve this problem. Hope someone can give me a clue of how to build tensorflow-serving 1.7 with tensorrt, because there are really few docs about this. (@samikama )\r\n****************************************\r\nI tried to compile the Tensorflow-serving r1.7 with TensorRT 4.0.4, and the compilation is successfully done.\r\n```\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\r\nINFO: Elapsed time: 1452.421s, Critical Path: 479.68s\r\nINFO: Build completed successfully, 11375 total actions\r\n```\r\n\r\nBut when I start the service and load a TFTRT optimized model, I get error:\r\n```\r\n2018-06-07 17:41:40.910874: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /media/disk1/fordata/web_server/project/LdaBasedClassification_623_1.7/data/cate155_tftrt_frozen/1\r\n2018-06-07 17:41:41.030117: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-06-07 17:41:41.283451: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\r\n2018-06-07 17:41:41.283514: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-06-07 17:41:41.601178: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-07 17:41:41.601253: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-06-07 17:41:41.601273: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-06-07 17:41:41.601561: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10970 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n2018-06-07 17:41:41.878689: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 967809 microseconds.\r\n2018-06-07 17:41:41.878771: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: inception_v3 version: 1} failed: Not found: Op type not registered 'TRTEngineOp' in binary running on bjpg-g180.yz02. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\n```\r\n\r\nLooks like the TRTEngineOp is still not supported by this execution file. \r\nThough I'm not 100% sure about my way of compiling Tensorflow-serving 1.7 with TRT, but I think the compilation indeed searched and found the libnvinfer.so, etc, and also checked the TensorRT version is correct. So I don't know why the binary executive file still can't support TRTEngineOp. \r\n\r\nHere is my environment variables:\r\n```\r\nexport TENSORRT_INSTALL_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\r\nexport TF_TENSORRT_VERSION=4.0.4\r\nexport TENSORRT_LIB_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\r\n```\r\nThis is my compilation command:\r\n```\r\nsed -i.bak 's/@org_tensorflow\\/\\/third_party\\/gpus\\/crosstool/@local_config_cuda\\/\\/crosstool:toolchain/g' tools/bazel.rc      \r\nbazel build  --config=cuda --action_env PYTHON_BIN_PATH=\"/home/karafuto/dlpy72/dlpy/bin/python2.7\" TENSORRT_BIN_PATH=\"/home/karafuto/TensorRT-3.0.4\"  -c opt tensorflow_serving/...\r\n\r\n```\r\nI'm not sure whether my procedure is correct. Really few of docs can be found that talk about how to build the tensorflow-serving 1.7 with tensorrt. Any idea will be welcome? \r\n\r\nThanks,\r\n****************************************\r\nPS:  The tensorrt source code is downloaded from NVIDIA official website, which tar file is named \"TensorRT-3.0.4.Ubuntu-14.04.5.x86_64.cuda-9.0.cudnn7.0.tar.gz\". The weird thing is, I unpacked the tar file and find the actually version is 4.0.4 not 3.0.4. So in the tensorflow-serving-r1.7, I need to set the variable TF_TENSORRT_VERSION=4.0.4 to avoid version check failure.\r\n\r\nI encountered below 2 errors and solved them, so I think the bazel compilation shall indeed compiled the TensorRT. Post here as an evidence.\r\n****************************************\r\nThis is the error when I set wrong TENSORRT_LIB_PATH, (can't find libnvinfer.so):\r\n```\r\nERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 160\r\n\t\tauto_configure_fail(\"TensorRT library (libnvinfer) v...\")\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: TensorRT library (libnvinfer) version is not set.\r\n```\r\n*******************************************\r\nThis is when the TF_TENSORRT_VERSION is not the same with found libnvinfer:\r\n```\r\nERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 167\r\n\t\t_trt_lib_version(repository_ctx, trt_install_path)\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 87, in _trt_lib_version\r\n\t\tauto_configure_fail((\"TensorRT library version detec...)))\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: TensorRT library version detected from /media/disk1/fordata/web_server/project/xiaolun/TensorRT-3.0.4/include/NvInfer.h (4.0.4) does not match TF_TENSORRT_VERSION (3.0.4). To fix this rerun configure again.\r\n```\r\n"}