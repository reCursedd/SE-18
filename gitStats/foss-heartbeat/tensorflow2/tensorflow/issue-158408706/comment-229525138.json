{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/229525138", "html_url": "https://github.com/tensorflow/tensorflow/issues/2641#issuecomment-229525138", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2641", "id": 229525138, "node_id": "MDEyOklzc3VlQ29tbWVudDIyOTUyNTEzOA==", "user": {"login": "ibab", "id": 890531, "node_id": "MDQ6VXNlcjg5MDUzMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/890531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ibab", "html_url": "https://github.com/ibab", "followers_url": "https://api.github.com/users/ibab/followers", "following_url": "https://api.github.com/users/ibab/following{/other_user}", "gists_url": "https://api.github.com/users/ibab/gists{/gist_id}", "starred_url": "https://api.github.com/users/ibab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ibab/subscriptions", "organizations_url": "https://api.github.com/users/ibab/orgs", "repos_url": "https://api.github.com/users/ibab/repos", "events_url": "https://api.github.com/users/ibab/events{/privacy}", "received_events_url": "https://api.github.com/users/ibab/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-30T00:05:12Z", "updated_at": "2016-06-30T00:05:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For reference, here's the gradient implementation I came up with:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Prod<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_ProdGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gradient for Prod.<span class=\"pl-pds\">\"\"\"</span></span>\n  input_shape <span class=\"pl-k\">=</span> array_ops.shape(op.inputs[<span class=\"pl-c1\">0</span>])\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Expand grad to full input shape</span>\n  output_shape_kept_dims <span class=\"pl-k\">=</span> math_ops.reduced_shape(input_shape, op.inputs[<span class=\"pl-c1\">1</span>])\n  tile_scaling <span class=\"pl-k\">=</span> _safe_shape_div(input_shape, output_shape_kept_dims)\n  grad <span class=\"pl-k\">=</span> array_ops.reshape(grad, output_shape_kept_dims)\n  grad <span class=\"pl-k\">=</span> array_ops.tile(grad, tile_scaling)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> If the list is empty, it defaults to float32</span>\n  reduced <span class=\"pl-k\">=</span> math_ops.cast(op.inputs[<span class=\"pl-c1\">1</span>], dtypes.int32)\n  idx <span class=\"pl-k\">=</span> math_ops.range(<span class=\"pl-c1\">0</span>, array_ops.rank(op.inputs[<span class=\"pl-c1\">0</span>]))\n  other, _ <span class=\"pl-k\">=</span> array_ops.listdiff(idx, reduced)\n  perm <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">0</span>, [reduced, other])\n  reduced_num <span class=\"pl-k\">=</span> math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\n  other_num <span class=\"pl-k\">=</span> math_ops.reduce_prod(array_ops.gather(input_shape, other))\n  permuted <span class=\"pl-k\">=</span> array_ops.transpose(op.inputs[<span class=\"pl-c1\">0</span>], perm)\n  permuted_shape <span class=\"pl-k\">=</span> array_ops.shape(permuted)\n  reshaped <span class=\"pl-k\">=</span> array_ops.reshape(permuted, (reduced_num, other_num))\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Calculate product, leaving out the current entry</span>\n  left <span class=\"pl-k\">=</span> math_ops.cumprod(reshaped, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">exclusive</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n  right <span class=\"pl-k\">=</span> math_ops.cumprod(reshaped, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">exclusive</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">reverse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n  y <span class=\"pl-k\">=</span> array_ops.reshape(left <span class=\"pl-k\">*</span> right, permuted_shape)\n\n  out <span class=\"pl-k\">=</span> grad <span class=\"pl-k\">*</span> array_ops.transpose(y, array_ops.invert_permutation(perm))\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reset statically known shape information</span>\n  <span class=\"pl-k\">return</span> array_ops.reshape(out, input_shape), <span class=\"pl-c1\">None</span></pre></div>\n<p>This makes the existing tests pass and also works if there are zeros in the input array.</p>", "body_text": "For reference, here's the gradient implementation I came up with:\n@ops.RegisterGradient(\"Prod\")\ndef _ProdGrad(op, grad):\n  \"\"\"Gradient for Prod.\"\"\"\n  input_shape = array_ops.shape(op.inputs[0])\n\n  # Expand grad to full input shape\n  output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\n  tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\n  grad = array_ops.reshape(grad, output_shape_kept_dims)\n  grad = array_ops.tile(grad, tile_scaling)\n\n  # If the list is empty, it defaults to float32\n  reduced = math_ops.cast(op.inputs[1], dtypes.int32)\n  idx = math_ops.range(0, array_ops.rank(op.inputs[0]))\n  other, _ = array_ops.listdiff(idx, reduced)\n  perm = array_ops.concat(0, [reduced, other])\n  reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\n  other_num = math_ops.reduce_prod(array_ops.gather(input_shape, other))\n  permuted = array_ops.transpose(op.inputs[0], perm)\n  permuted_shape = array_ops.shape(permuted)\n  reshaped = array_ops.reshape(permuted, (reduced_num, other_num))\n\n  # Calculate product, leaving out the current entry\n  left = math_ops.cumprod(reshaped, axis=0, exclusive=True)\n  right = math_ops.cumprod(reshaped, axis=0, exclusive=True, reverse=True)\n  y = array_ops.reshape(left * right, permuted_shape)\n\n  out = grad * array_ops.transpose(y, array_ops.invert_permutation(perm))\n  # Reset statically known shape information\n  return array_ops.reshape(out, input_shape), None\nThis makes the existing tests pass and also works if there are zeros in the input array.", "body": "For reference, here's the gradient implementation I came up with:\n\n``` python\n@ops.RegisterGradient(\"Prod\")\ndef _ProdGrad(op, grad):\n  \"\"\"Gradient for Prod.\"\"\"\n  input_shape = array_ops.shape(op.inputs[0])\n\n  # Expand grad to full input shape\n  output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\n  tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\n  grad = array_ops.reshape(grad, output_shape_kept_dims)\n  grad = array_ops.tile(grad, tile_scaling)\n\n  # If the list is empty, it defaults to float32\n  reduced = math_ops.cast(op.inputs[1], dtypes.int32)\n  idx = math_ops.range(0, array_ops.rank(op.inputs[0]))\n  other, _ = array_ops.listdiff(idx, reduced)\n  perm = array_ops.concat(0, [reduced, other])\n  reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\n  other_num = math_ops.reduce_prod(array_ops.gather(input_shape, other))\n  permuted = array_ops.transpose(op.inputs[0], perm)\n  permuted_shape = array_ops.shape(permuted)\n  reshaped = array_ops.reshape(permuted, (reduced_num, other_num))\n\n  # Calculate product, leaving out the current entry\n  left = math_ops.cumprod(reshaped, axis=0, exclusive=True)\n  right = math_ops.cumprod(reshaped, axis=0, exclusive=True, reverse=True)\n  y = array_ops.reshape(left * right, permuted_shape)\n\n  out = grad * array_ops.transpose(y, array_ops.invert_permutation(perm))\n  # Reset statically known shape information\n  return array_ops.reshape(out, input_shape), None\n```\n\nThis makes the existing tests pass and also works if there are zeros in the input array.\n"}