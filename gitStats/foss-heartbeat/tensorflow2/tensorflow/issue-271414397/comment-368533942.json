{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/368533942", "html_url": "https://github.com/tensorflow/tensorflow/issues/14283#issuecomment-368533942", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14283", "id": 368533942, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODUzMzk0Mg==", "user": {"login": "voegtlel", "id": 5764745, "node_id": "MDQ6VXNlcjU3NjQ3NDU=", "avatar_url": "https://avatars0.githubusercontent.com/u/5764745?v=4", "gravatar_id": "", "url": "https://api.github.com/users/voegtlel", "html_url": "https://github.com/voegtlel", "followers_url": "https://api.github.com/users/voegtlel/followers", "following_url": "https://api.github.com/users/voegtlel/following{/other_user}", "gists_url": "https://api.github.com/users/voegtlel/gists{/gist_id}", "starred_url": "https://api.github.com/users/voegtlel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/voegtlel/subscriptions", "organizations_url": "https://api.github.com/users/voegtlel/orgs", "repos_url": "https://api.github.com/users/voegtlel/repos", "events_url": "https://api.github.com/users/voegtlel/events{/privacy}", "received_events_url": "https://api.github.com/users/voegtlel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-26T15:09:47Z", "updated_at": "2018-02-27T08:29:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We have the same issue: We have a large dataset which trains several hours per epoch, but wish to evaluate in between epochs. Unfortunately there is no resource sharing for local sessions, so iterators cannot be shared between sessions.</p>\n<p>So these are the possibilities I see:</p>\n<ul>\n<li>As far as I understand, dataset.skip() always skips the first lines, thus it is not usable for this case. Also the global step is not available during graph creation in the Estimator, so it cannot be used since it is stateful.</li>\n<li>Allow local sessions to share resources, and keep the dataset iterator alive.</li>\n<li>Prevent the graph and session from being destroyed, thus keeping the iterator. This might be problematic with memory usage.</li>\n<li>Allow a way to use the main graph for both, evaluation and training (i.e. evaluation only uses a subgraph of training, there might be some <code>tf.cond</code>s involved with a placeholder for switching the mode). The problem here is that saving the graph will result in a lot of unnececssary ops. (&lt;- this is still my favorite, because it seems feasable)</li>\n<li>Allow to skip entries on first training step after the session was created (e.g. <code>iterator.skip_initial_steps</code>), thus continuing training at about the i-th step (the shuffle buffer is not exactly the same then, but at least close to). This should be optimized, such that the dataset does not actually read the data, but only skip it.</li>\n<li>Add a simple option to evaluate after exactly one epoch, thus ensure each sample was read. This unfortunately does not allow for evaluations in between, but is rather a workaround.</li>\n</ul>\n<p>Are there any other suggestions?</p>", "body_text": "We have the same issue: We have a large dataset which trains several hours per epoch, but wish to evaluate in between epochs. Unfortunately there is no resource sharing for local sessions, so iterators cannot be shared between sessions.\nSo these are the possibilities I see:\n\nAs far as I understand, dataset.skip() always skips the first lines, thus it is not usable for this case. Also the global step is not available during graph creation in the Estimator, so it cannot be used since it is stateful.\nAllow local sessions to share resources, and keep the dataset iterator alive.\nPrevent the graph and session from being destroyed, thus keeping the iterator. This might be problematic with memory usage.\nAllow a way to use the main graph for both, evaluation and training (i.e. evaluation only uses a subgraph of training, there might be some tf.conds involved with a placeholder for switching the mode). The problem here is that saving the graph will result in a lot of unnececssary ops. (<- this is still my favorite, because it seems feasable)\nAllow to skip entries on first training step after the session was created (e.g. iterator.skip_initial_steps), thus continuing training at about the i-th step (the shuffle buffer is not exactly the same then, but at least close to). This should be optimized, such that the dataset does not actually read the data, but only skip it.\nAdd a simple option to evaluate after exactly one epoch, thus ensure each sample was read. This unfortunately does not allow for evaluations in between, but is rather a workaround.\n\nAre there any other suggestions?", "body": "We have the same issue: We have a large dataset which trains several hours per epoch, but wish to evaluate in between epochs. Unfortunately there is no resource sharing for local sessions, so iterators cannot be shared between sessions.\r\n\r\nSo these are the possibilities I see:\r\n- As far as I understand, dataset.skip() always skips the first lines, thus it is not usable for this case. Also the global step is not available during graph creation in the Estimator, so it cannot be used since it is stateful.\r\n- Allow local sessions to share resources, and keep the dataset iterator alive.\r\n- Prevent the graph and session from being destroyed, thus keeping the iterator. This might be problematic with memory usage.\r\n- Allow a way to use the main graph for both, evaluation and training (i.e. evaluation only uses a subgraph of training, there might be some `tf.cond`s involved with a placeholder for switching the mode). The problem here is that saving the graph will result in a lot of unnececssary ops. (<- this is still my favorite, because it seems feasable)\r\n- Allow to skip entries on first training step after the session was created (e.g. `iterator.skip_initial_steps`), thus continuing training at about the i-th step (the shuffle buffer is not exactly the same then, but at least close to). This should be optimized, such that the dataset does not actually read the data, but only skip it.\r\n- Add a simple option to evaluate after exactly one epoch, thus ensure each sample was read. This unfortunately does not allow for evaluations in between, but is rather a workaround.\r\n\r\nAre there any other suggestions?"}