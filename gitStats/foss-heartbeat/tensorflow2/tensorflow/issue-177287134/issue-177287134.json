{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4402", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4402/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4402/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4402/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4402", "id": 177287134, "node_id": "MDU6SXNzdWUxNzcyODcxMzQ=", "number": 4402, "title": "explicit device specification of restore operation on distributed training", "user": {"login": "mbz", "id": 1122127, "node_id": "MDQ6VXNlcjExMjIxMjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1122127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbz", "html_url": "https://github.com/mbz", "followers_url": "https://api.github.com/users/mbz/followers", "following_url": "https://api.github.com/users/mbz/following{/other_user}", "gists_url": "https://api.github.com/users/mbz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbz/subscriptions", "organizations_url": "https://api.github.com/users/mbz/orgs", "repos_url": "https://api.github.com/users/mbz/repos", "events_url": "https://api.github.com/users/mbz/events{/privacy}", "received_events_url": "https://api.github.com/users/mbz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-09-15T20:53:43Z", "updated_at": "2016-09-23T04:48:06Z", "closed_at": "2016-09-23T04:48:06Z", "author_association": "NONE", "body_html": "<p>I'm trying the run the distributed training of Inception, and this is the error I'm getting, the moment workers connect to ps.</p>\n<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_1268': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0\n     [[Node: save/restore_slice_1268 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/restore_slice_1268/tensor_name, save/restore_slice_1268/shape_and_slice)]]\n\nCaused by op u'save/restore_slice_1268', defined at:\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 65, in &lt;module&gt;\n    tf.app.run()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception_distributed_train.py\", line 233, in train\n    saver = tf.train.Saver()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 861, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 519, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\n    preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n</code></pre>\n<p>I'm using the latest pip TensorFlow (0.10) on Cuda 7.5 and Cudnn v5.</p>", "body_text": "I'm trying the run the distributed training of Inception, and this is the error I'm getting, the moment workers connect to ps.\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_1268': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0\n     [[Node: save/restore_slice_1268 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/restore_slice_1268/tensor_name, save/restore_slice_1268/shape_and_slice)]]\n\nCaused by op u'save/restore_slice_1268', defined at:\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception_distributed_train.py\", line 233, in train\n    saver = tf.train.Saver()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 861, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 519, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\n    preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n\nI'm using the latest pip TensorFlow (0.10) on Cuda 7.5 and Cudnn v5.", "body": "I'm trying the run the distributed training of Inception, and this is the error I'm getting, the moment workers connect to ps. \n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_1268': Could not satisfy explicit device specification '/job:ps/task:0/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0\n     [[Node: save/restore_slice_1268 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/restore_slice_1268/tensor_name, save/restore_slice_1268/shape_and_slice)]]\n\nCaused by op u'save/restore_slice_1268', defined at:\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/mbz/exp/tf_inception/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception_distributed_train.py\", line 233, in train\n    saver = tf.train.Saver()\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 861, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 519, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\n    preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/mbz/systems/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n```\n\nI'm using the latest pip TensorFlow (0.10) on Cuda 7.5 and Cudnn v5.\n"}