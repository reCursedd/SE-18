{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239511066", "html_url": "https://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239511066", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3766", "id": 239511066, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTUxMTA2Ng==", "user": {"login": "bixiongxu", "id": 20988542, "node_id": "MDQ6VXNlcjIwOTg4NTQy", "avatar_url": "https://avatars1.githubusercontent.com/u/20988542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bixiongxu", "html_url": "https://github.com/bixiongxu", "followers_url": "https://api.github.com/users/bixiongxu/followers", "following_url": "https://api.github.com/users/bixiongxu/following{/other_user}", "gists_url": "https://api.github.com/users/bixiongxu/gists{/gist_id}", "starred_url": "https://api.github.com/users/bixiongxu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bixiongxu/subscriptions", "organizations_url": "https://api.github.com/users/bixiongxu/orgs", "repos_url": "https://api.github.com/users/bixiongxu/repos", "events_url": "https://api.github.com/users/bixiongxu/events{/privacy}", "received_events_url": "https://api.github.com/users/bixiongxu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-12T17:37:38Z", "updated_at": "2016-08-12T17:37:38Z", "author_association": "NONE", "body_html": "<p>Of course.<br>\nAlthough so far worker 0 is the only one who crashed. And small rnn worker 0 also crashes when it restores from ckpt.</p>\n<p>But for distributed training , 2 GB shouldn't be enough. I saw some fix about the protonuf  2GB crash in github. It seems being done by add a check and return. Any other plans to make it fit for larger data?</p>\n<p>Thank you!</p>\n<p>Get Outlook for iOShttps://aka.ms/o0ukef</p>\n<p>On Sat, Aug 13, 2016 at 1:22 AM +0800, \"Derek Murray\" &lt;<a href=\"mailto:notifications@github.com\">notifications@github.com</a><a href=\"mailto:notifications@github.com\">mailto:notifications@github.com</a>&gt; wrote:</p>\n<p>When you see an UnavailableError, this is most likely caused by another (remote) process crashing. Since it seems to depend on the size of your RNN, I suspect something is hitting the 2GB limit for protobuf serialization, but it's not clear what that might be. Can you capture the logs for any other crashed processes to see if there's more information in there?</p>\n<h2></h2>\n<p>You are receiving this because you authored the thread.<br>\nReply to this email directly, view it on GitHubhttps://github.com/<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"170844072\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3766\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3766/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3766\">/issues/3766</a>#issuecomment-239506773, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfiBBTlz4wMZnfK2jYPCb1mv-mLBqks5qfKvrgaJpZM4Ji93R.</p>", "body_text": "Of course.\nAlthough so far worker 0 is the only one who crashed. And small rnn worker 0 also crashes when it restores from ckpt.\nBut for distributed training , 2 GB shouldn't be enough. I saw some fix about the protonuf  2GB crash in github. It seems being done by add a check and return. Any other plans to make it fit for larger data?\nThank you!\nGet Outlook for iOShttps://aka.ms/o0ukef\nOn Sat, Aug 13, 2016 at 1:22 AM +0800, \"Derek Murray\" <notifications@github.commailto:notifications@github.com> wrote:\nWhen you see an UnavailableError, this is most likely caused by another (remote) process crashing. Since it seems to depend on the size of your RNN, I suspect something is hitting the 2GB limit for protobuf serialization, but it's not clear what that might be. Can you capture the logs for any other crashed processes to see if there's more information in there?\n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com//issues/3766#issuecomment-239506773, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfiBBTlz4wMZnfK2jYPCb1mv-mLBqks5qfKvrgaJpZM4Ji93R.", "body": "Of course.\nAlthough so far worker 0 is the only one who crashed. And small rnn worker 0 also crashes when it restores from ckpt.\n\nBut for distributed training , 2 GB shouldn't be enough. I saw some fix about the protonuf  2GB crash in github. It seems being done by add a check and return. Any other plans to make it fit for larger data?\n\nThank you!\n\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nOn Sat, Aug 13, 2016 at 1:22 AM +0800, \"Derek Murray\" <notifications@github.com<mailto:notifications@github.com>> wrote:\n\nWhen you see an UnavailableError, this is most likely caused by another (remote) process crashing. Since it seems to depend on the size of your RNN, I suspect something is hitting the 2GB limit for protobuf serialization, but it's not clear what that might be. Can you capture the logs for any other crashed processes to see if there's more information in there?\n\n## \n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239506773, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfiBBTlz4wMZnfK2jYPCb1mv-mLBqks5qfKvrgaJpZM4Ji93R.\n"}