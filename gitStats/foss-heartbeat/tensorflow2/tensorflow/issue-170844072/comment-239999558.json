{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239999558", "html_url": "https://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239999558", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3766", "id": 239999558, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTk5OTU1OA==", "user": {"login": "bixiongxu", "id": 20988542, "node_id": "MDQ6VXNlcjIwOTg4NTQy", "avatar_url": "https://avatars1.githubusercontent.com/u/20988542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bixiongxu", "html_url": "https://github.com/bixiongxu", "followers_url": "https://api.github.com/users/bixiongxu/followers", "following_url": "https://api.github.com/users/bixiongxu/following{/other_user}", "gists_url": "https://api.github.com/users/bixiongxu/gists{/gist_id}", "starred_url": "https://api.github.com/users/bixiongxu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bixiongxu/subscriptions", "organizations_url": "https://api.github.com/users/bixiongxu/orgs", "repos_url": "https://api.github.com/users/bixiongxu/repos", "events_url": "https://api.github.com/users/bixiongxu/events{/privacy}", "received_events_url": "https://api.github.com/users/bixiongxu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-16T04:41:16Z", "updated_at": "2016-08-16T05:26:13Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> I have added variable partition in my code, but still got the same exception (on same line: E tensorflow/core/client/tensor_c_api.cc:485)  on Worker 0, while Worker1 can start training...</p>\n<p>The whole model is wrapped by a variable_scope with a partitioner, so in theory it will work to solve the problem. I will keep trying and appreciate to have some suggestions from you.</p>\n<p>I extracted some code as below:</p>\n<pre><code>    with tf.variable_scope(\"soulmate\", partitioner=tf.variable_axis_size_partitioner(\n            max_shard_bytes=33554432)):\n        with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)) as dev:\n            # Create model.\n            print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n            model = create_model_fresh(False)   \n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n            print(model.global_step.name)\n            print(model.learning_rate.name)\n            # Create a \"supervisor\", which oversees the training process.\n            sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=FLAGS.train_dir,\n                                 init_op=init_op,\n                                 summary_op=None,\n                                 saver=None,\n                                 global_step=None,\n                                 recovery_wait_secs=10)\n\n            print(\"Global_step is placed on \" + model.global_step.device + \", will be initialized on \" + model.global_step.initializer.device)\n            if FLAGS.task_index == 0:\n                print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\n            else:\n                print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.task_index)\n            time_begin = time.time()    \n            with sv.managed_session(server.target) as sess:\n                print(\"Worker %d: Session initialization complete.\" % FLAGS.task_index)\n                print(\"in %f secs.\" % (time.time()-time_begin))\n\n                # This is the training loop.\n                step_time, loss = 0.0, 0.0\n                current_step = 0\n                current_global_step = 0\n                previous_losses = []\n                while not sv.should_stop():\n                    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n                    random_number_01 = np.random.random_sample()\n                    bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                                   if train_buckets_scale[i] &gt; random_number_01])\n\n                    # Get a batch and make a step.\n                    start_time = time.time()\n                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                        train_set, bucket_id)\n                    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                               target_weights, bucket_id, False)\n\n                    step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n                    loss += step_loss / FLAGS.steps_per_checkpoint\n                    current_step += 1\n\n            sv.stop()\n</code></pre>\n<p>And the stack:</p>\n<p>E tensorflow/core/client/tensor_c_api.cc:485]<br>\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.<br>\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.<br>\nTraceback (most recent call last):<br>\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 349, in <br>\ntf.app.run()<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv))<br>\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 346, in main<br>\ntrain()<br>\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 239, in train<br>\ntarget_weights, bucket_id, False)<br>\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/seq2seq_model.py\", line 228, in step<br>\noutputs = session.run(output_feed, input_feed)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run<br>\nfeed_dict_string, options, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run<br>\ntarget_list, options, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors.UnavailableError</p>", "body_text": "@mrry I have added variable partition in my code, but still got the same exception (on same line: E tensorflow/core/client/tensor_c_api.cc:485)  on Worker 0, while Worker1 can start training...\nThe whole model is wrapped by a variable_scope with a partitioner, so in theory it will work to solve the problem. I will keep trying and appreciate to have some suggestions from you.\nI extracted some code as below:\n    with tf.variable_scope(\"soulmate\", partitioner=tf.variable_axis_size_partitioner(\n            max_shard_bytes=33554432)):\n        with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)) as dev:\n            # Create model.\n            print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n            model = create_model_fresh(False)   \n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n            print(model.global_step.name)\n            print(model.learning_rate.name)\n            # Create a \"supervisor\", which oversees the training process.\n            sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=FLAGS.train_dir,\n                                 init_op=init_op,\n                                 summary_op=None,\n                                 saver=None,\n                                 global_step=None,\n                                 recovery_wait_secs=10)\n\n            print(\"Global_step is placed on \" + model.global_step.device + \", will be initialized on \" + model.global_step.initializer.device)\n            if FLAGS.task_index == 0:\n                print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\n            else:\n                print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.task_index)\n            time_begin = time.time()    \n            with sv.managed_session(server.target) as sess:\n                print(\"Worker %d: Session initialization complete.\" % FLAGS.task_index)\n                print(\"in %f secs.\" % (time.time()-time_begin))\n\n                # This is the training loop.\n                step_time, loss = 0.0, 0.0\n                current_step = 0\n                current_global_step = 0\n                previous_losses = []\n                while not sv.should_stop():\n                    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n                    random_number_01 = np.random.random_sample()\n                    bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                                   if train_buckets_scale[i] > random_number_01])\n\n                    # Get a batch and make a step.\n                    start_time = time.time()\n                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                        train_set, bucket_id)\n                    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                               target_weights, bucket_id, False)\n\n                    step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n                    loss += step_loss / FLAGS.steps_per_checkpoint\n                    current_step += 1\n\n            sv.stop()\n\nAnd the stack:\nE tensorflow/core/client/tensor_c_api.cc:485]\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.\nTraceback (most recent call last):\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 349, in \ntf.app.run()\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv))\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 346, in main\ntrain()\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 239, in train\ntarget_weights, bucket_id, False)\nFile \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/seq2seq_model.py\", line 228, in step\noutputs = session.run(output_feed, input_feed)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\nrun_metadata_ptr)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\nfeed_dict_string, options, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\ntarget_list, options, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.UnavailableError", "body": "@mrry I have added variable partition in my code, but still got the same exception (on same line: E tensorflow/core/client/tensor_c_api.cc:485)  on Worker 0, while Worker1 can start training... \n\nThe whole model is wrapped by a variable_scope with a partitioner, so in theory it will work to solve the problem. I will keep trying and appreciate to have some suggestions from you. \n\nI extracted some code as below: \n\n```\n    with tf.variable_scope(\"soulmate\", partitioner=tf.variable_axis_size_partitioner(\n            max_shard_bytes=33554432)):\n        with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)) as dev:\n            # Create model.\n            print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n            model = create_model_fresh(False)   \n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n            print(model.global_step.name)\n            print(model.learning_rate.name)\n            # Create a \"supervisor\", which oversees the training process.\n            sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=FLAGS.train_dir,\n                                 init_op=init_op,\n                                 summary_op=None,\n                                 saver=None,\n                                 global_step=None,\n                                 recovery_wait_secs=10)\n\n            print(\"Global_step is placed on \" + model.global_step.device + \", will be initialized on \" + model.global_step.initializer.device)\n            if FLAGS.task_index == 0:\n                print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\n            else:\n                print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.task_index)\n            time_begin = time.time()    \n            with sv.managed_session(server.target) as sess:\n                print(\"Worker %d: Session initialization complete.\" % FLAGS.task_index)\n                print(\"in %f secs.\" % (time.time()-time_begin))\n\n                # This is the training loop.\n                step_time, loss = 0.0, 0.0\n                current_step = 0\n                current_global_step = 0\n                previous_losses = []\n                while not sv.should_stop():\n                    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n                    random_number_01 = np.random.random_sample()\n                    bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                                   if train_buckets_scale[i] > random_number_01])\n\n                    # Get a batch and make a step.\n                    start_time = time.time()\n                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                        train_set, bucket_id)\n                    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                               target_weights, bucket_id, False)\n\n                    step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n                    loss += step_loss / FLAGS.steps_per_checkpoint\n                    current_step += 1\n\n            sv.stop()\n```\n\nAnd the stack: \n\nE tensorflow/core/client/tensor_c_api.cc:485]\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.\nTraceback (most recent call last):\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 349, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 346, in main\n    train()\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 239, in train\n    target_weights, bucket_id, False)\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/seq2seq_model.py\", line 228, in step\n    outputs = session.run(output_feed, input_feed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.UnavailableError\n"}