{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9367", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9367/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9367/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9367/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9367", "id": 223412866, "node_id": "MDU6SXNzdWUyMjM0MTI4NjY=", "number": 9367, "title": "[XLA] Ptxas Error when TF_CPP_MIN_VLOG_LEVEL=2 ", "user": {"login": "pgplus1628", "id": 1302872, "node_id": "MDQ6VXNlcjEzMDI4NzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1302872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pgplus1628", "html_url": "https://github.com/pgplus1628", "followers_url": "https://api.github.com/users/pgplus1628/followers", "following_url": "https://api.github.com/users/pgplus1628/following{/other_user}", "gists_url": "https://api.github.com/users/pgplus1628/gists{/gist_id}", "starred_url": "https://api.github.com/users/pgplus1628/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pgplus1628/subscriptions", "organizations_url": "https://api.github.com/users/pgplus1628/orgs", "repos_url": "https://api.github.com/users/pgplus1628/repos", "events_url": "https://api.github.com/users/pgplus1628/events{/privacy}", "received_events_url": "https://api.github.com/users/pgplus1628/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-04-21T15:10:33Z", "updated_at": "2017-04-25T01:31:01Z", "closed_at": "2017-04-25T01:31:01Z", "author_association": "NONE", "body_html": "<h3>System Information</h3>\n<ul>\n<li><em>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?</em>: yes</li>\n<li><em>OS Platform and Distribution (i.e. Linux Ubuntu 16.0)</em>: Linux Ubuntu 14.04</li>\n<li><em>TensorFlow installed from (source or binary)?</em>: source</li>\n<li><em>TensorFlow version</em> (use command below): <code>('v1.1.0-rc2-219-g623dd83', '1.1.0-rc2')</code></li>\n<li><em>Bazel version (if compiling from source)</em>: <code>0.4.5-jdk7</code></li>\n<li><em>CUDA/cuDNN version</em>: 7.5/5</li>\n<li><em>GPU Model and Memory</em>: GeForce GTX TitanX</li>\n<li><em>Exact command to reproduce</em>: <code>python test.py --batch_size 16 --step 20</code></li>\n</ul>\n<h3>Describe the problem clearly</h3>\n<p>To make tensorflow print the logs in VLOG(2), I set the <code>TF_CPP_MIN_VLOG_LEVEL=2</code>. After doing that, the program throws a fatal error. It seems that there's something wrong when compiling xla hlo_instruction to ptx.</p>\n<blockquote>\n<p>2017-04-21 14:35:23.158362: I tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:219] ptxas fatal   : SM version specified by .target is higher than default SM version assumed<br>\n2017-04-21 14:35:23.158423: F tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:221] Invalid PTX. See the error message above for reasons.</p>\n</blockquote>\n<h3>Source Code / Logs</h3>\n<p>Full log can be found <a href=\"https://gist.github.com/pgplus1628/b257901de5af4bdd88fd78adab084177\">here</a><br>\nReproduce with command <code>python test.py --batch_size 16 --step 20</code></p>\n<p>Code:</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2' # enable logging debug info\nimport inspect\nimport numpy as np\nimport tensorflow as tf\n\n\nflags = tf.flags\nlogging = tf.logging\nflags.DEFINE_integer(\"batch_size\", 1, \"inference batch size\")\nflags.DEFINE_integer(\"step\", 1, \"step size for infernece\")\nFLAGS = flags.FLAGS\n\n\ndef data_type():\n    return tf.float32\n\n\nclass InputData(object):\n    \n    def __init__(self, config):\n        self.batch_size = batch_size = config.batch_size\n        self.num_steps = num_steps = config.num_steps\n        self.hidden_size = hidden_size = config.hidden_size\n        self.input_data = tf.placeholder(data_type(), [batch_size, num_steps, hidden_size], name = 'input_data')\n\n\nclass Config(object):\n    num_layers = 1\n    num_steps = 20\n    hidden_size = 256\n    batch_size = 20\n    vocab_size = 10000\n    init_scale = 0.1\n    num_iter = 50\n    warm_iter = 2\n\n\nclass LSTMModel(object):\n    \"\"\" Only forward, No Embedding\n    \"\"\"\n\n    def __init__(self, config, input_):\n        self._input = input_\n        self._input_data = self._input.input_data\n        \n        batch_size = input_.batch_size\n        num_steps =  input_.num_steps\n        size = config.hidden_size\n        vocab_size = config.vocab_size\n\n        def lstm_cell():\n            if 'reuse' in inspect.getargspec(\n              tf.contrib.rnn.BasicLSTMCell.__init__).args:\n                print(\"reuse\")\n                return tf.contrib.rnn.BasicLSTMCell(\n                    size, forget_bias=0., state_is_tuple=True,\n                    reuse = tf.get_variable_scope().reuse)\n            else:\n                print(\"not reuse\")\n                return tf.contrib.rnn.BasicLSTMCell(\n                    size, forget_bias=0.0, state_is_tuple=True)\n\n        attn_cell = lstm_cell\n        cell = tf.contrib.rnn.MultiRNNCell(\n            [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n\n        self._initial_state = cell.zero_state(batch_size, data_type())\n\n        outputs = []\n        state = self._initial_state\n        with tf.variable_scope(\"RNN\"):\n            for time_step in range(num_steps):\n                if time_step &gt; 0 : tf.get_variable_scope().reuse_variables()\n                (cell_output, state) = cell(self._input.input_data[:, time_step, :], state)\n                outputs.append(cell_output)\n        \n        self._output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])\n        self._final_state = state\n        softmax_w = tf.get_variable(\n            \"softmax_w\", [size, vocab_size], dtype=data_type())\n        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\n        self._logits = tf.matmul(self._output, softmax_w) + softmax_b\n\n        return\n\n    @property\n    def initial_state(self):\n        return self._initial_state\n\n    @property\n    def logits(self):\n        return self._logits\n  \n    @property\n    def input_data(self):\n        return self._input_data\n\n\ndef run_inference(session, model, input_data, sv) :\n    # initialize with a clean state\n    state = session.run(model.initial_state)\n\n    fetches = {}\n    fetches['logit'] = model.logits\n\n    feed_dict = {}\n    feed_dict[model.input_data] = input_data\n    for i, (c, h) in enumerate(model.initial_state):\n        feed_dict[c] = state[i].c\n        feed_dict[h] = state[i].h\n\n    session.run(fetches, feed_dict)\n\n\ndef main(_):\n    # config\n    eval_config = Config()\n    eval_config.num_steps = FLAGS.step\n    eval_config.batch_size = FLAGS.batch_size\n\n    # generate random data\n    input_data = np.random.rand(eval_config.batch_size, eval_config.num_steps, eval_config.hidden_size).astype(np.float32)\n\n    with tf.Graph().as_default():\n        initializer = tf.random_uniform_initializer(-eval_config.init_scale, \n                                                    eval_config.init_scale)\n        with tf.name_scope('Inference'):\n            _input = InputData(eval_config)\n            with tf.variable_scope('Model', reuse=None, initializer=initializer):\n                model = LSTMModel(config=eval_config, input_=_input)\n\n        sv = tf.train.Supervisor()\n        sess_config = tf.ConfigProto(allow_soft_placement=True,\n                                     log_device_placement=False)\n        # enable xla\n        sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\n        # run inference\n        with sv.managed_session(config=sess_config) as session:\n            run_inference(session, model, input_data, sv)\n\nif __name__ == '__main__':\n    tf.app.run()\n</code></pre>", "body_text": "System Information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)?: yes\nOS Platform and Distribution (i.e. Linux Ubuntu 16.0): Linux Ubuntu 14.04\nTensorFlow installed from (source or binary)?: source\nTensorFlow version (use command below): ('v1.1.0-rc2-219-g623dd83', '1.1.0-rc2')\nBazel version (if compiling from source): 0.4.5-jdk7\nCUDA/cuDNN version: 7.5/5\nGPU Model and Memory: GeForce GTX TitanX\nExact command to reproduce: python test.py --batch_size 16 --step 20\n\nDescribe the problem clearly\nTo make tensorflow print the logs in VLOG(2), I set the TF_CPP_MIN_VLOG_LEVEL=2. After doing that, the program throws a fatal error. It seems that there's something wrong when compiling xla hlo_instruction to ptx.\n\n2017-04-21 14:35:23.158362: I tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:219] ptxas fatal   : SM version specified by .target is higher than default SM version assumed\n2017-04-21 14:35:23.158423: F tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:221] Invalid PTX. See the error message above for reasons.\n\nSource Code / Logs\nFull log can be found here\nReproduce with command python test.py --batch_size 16 --step 20\nCode:\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2' # enable logging debug info\nimport inspect\nimport numpy as np\nimport tensorflow as tf\n\n\nflags = tf.flags\nlogging = tf.logging\nflags.DEFINE_integer(\"batch_size\", 1, \"inference batch size\")\nflags.DEFINE_integer(\"step\", 1, \"step size for infernece\")\nFLAGS = flags.FLAGS\n\n\ndef data_type():\n    return tf.float32\n\n\nclass InputData(object):\n    \n    def __init__(self, config):\n        self.batch_size = batch_size = config.batch_size\n        self.num_steps = num_steps = config.num_steps\n        self.hidden_size = hidden_size = config.hidden_size\n        self.input_data = tf.placeholder(data_type(), [batch_size, num_steps, hidden_size], name = 'input_data')\n\n\nclass Config(object):\n    num_layers = 1\n    num_steps = 20\n    hidden_size = 256\n    batch_size = 20\n    vocab_size = 10000\n    init_scale = 0.1\n    num_iter = 50\n    warm_iter = 2\n\n\nclass LSTMModel(object):\n    \"\"\" Only forward, No Embedding\n    \"\"\"\n\n    def __init__(self, config, input_):\n        self._input = input_\n        self._input_data = self._input.input_data\n        \n        batch_size = input_.batch_size\n        num_steps =  input_.num_steps\n        size = config.hidden_size\n        vocab_size = config.vocab_size\n\n        def lstm_cell():\n            if 'reuse' in inspect.getargspec(\n              tf.contrib.rnn.BasicLSTMCell.__init__).args:\n                print(\"reuse\")\n                return tf.contrib.rnn.BasicLSTMCell(\n                    size, forget_bias=0., state_is_tuple=True,\n                    reuse = tf.get_variable_scope().reuse)\n            else:\n                print(\"not reuse\")\n                return tf.contrib.rnn.BasicLSTMCell(\n                    size, forget_bias=0.0, state_is_tuple=True)\n\n        attn_cell = lstm_cell\n        cell = tf.contrib.rnn.MultiRNNCell(\n            [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n\n        self._initial_state = cell.zero_state(batch_size, data_type())\n\n        outputs = []\n        state = self._initial_state\n        with tf.variable_scope(\"RNN\"):\n            for time_step in range(num_steps):\n                if time_step > 0 : tf.get_variable_scope().reuse_variables()\n                (cell_output, state) = cell(self._input.input_data[:, time_step, :], state)\n                outputs.append(cell_output)\n        \n        self._output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])\n        self._final_state = state\n        softmax_w = tf.get_variable(\n            \"softmax_w\", [size, vocab_size], dtype=data_type())\n        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\n        self._logits = tf.matmul(self._output, softmax_w) + softmax_b\n\n        return\n\n    @property\n    def initial_state(self):\n        return self._initial_state\n\n    @property\n    def logits(self):\n        return self._logits\n  \n    @property\n    def input_data(self):\n        return self._input_data\n\n\ndef run_inference(session, model, input_data, sv) :\n    # initialize with a clean state\n    state = session.run(model.initial_state)\n\n    fetches = {}\n    fetches['logit'] = model.logits\n\n    feed_dict = {}\n    feed_dict[model.input_data] = input_data\n    for i, (c, h) in enumerate(model.initial_state):\n        feed_dict[c] = state[i].c\n        feed_dict[h] = state[i].h\n\n    session.run(fetches, feed_dict)\n\n\ndef main(_):\n    # config\n    eval_config = Config()\n    eval_config.num_steps = FLAGS.step\n    eval_config.batch_size = FLAGS.batch_size\n\n    # generate random data\n    input_data = np.random.rand(eval_config.batch_size, eval_config.num_steps, eval_config.hidden_size).astype(np.float32)\n\n    with tf.Graph().as_default():\n        initializer = tf.random_uniform_initializer(-eval_config.init_scale, \n                                                    eval_config.init_scale)\n        with tf.name_scope('Inference'):\n            _input = InputData(eval_config)\n            with tf.variable_scope('Model', reuse=None, initializer=initializer):\n                model = LSTMModel(config=eval_config, input_=_input)\n\n        sv = tf.train.Supervisor()\n        sess_config = tf.ConfigProto(allow_soft_placement=True,\n                                     log_device_placement=False)\n        # enable xla\n        sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\n        # run inference\n        with sv.managed_session(config=sess_config) as session:\n            run_inference(session, model, input_data, sv)\n\nif __name__ == '__main__':\n    tf.app.run()", "body": "### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: Linux Ubuntu 14.04\r\n- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version* (use command below): `('v1.1.0-rc2-219-g623dd83', '1.1.0-rc2')`\r\n- *Bazel version (if compiling from source)*: `0.4.5-jdk7`\r\n- *CUDA/cuDNN version*: 7.5/5\r\n- *GPU Model and Memory*: GeForce GTX TitanX \r\n- *Exact command to reproduce*: `python test.py --batch_size 16 --step 20`\r\n\r\n### Describe the problem clearly\r\nTo make tensorflow print the logs in VLOG(2), I set the `TF_CPP_MIN_VLOG_LEVEL=2`. After doing that, the program throws a fatal error. It seems that there's something wrong when compiling xla hlo_instruction to ptx.\r\n\r\n>2017-04-21 14:35:23.158362: I tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:219] ptxas fatal   : SM version specified by .target is higher than default SM version assumed\r\n2017-04-21 14:35:23.158423: F tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:221] Invalid PTX. See the error message above for reasons.\r\n\r\n\r\n### Source Code / Logs\r\n\r\nFull log can be found [here](https://gist.github.com/pgplus1628/b257901de5af4bdd88fd78adab084177)\r\nReproduce with command `python test.py --batch_size 16 --step 20`  \r\n\r\nCode:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2' # enable logging debug info\r\nimport inspect\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nflags = tf.flags\r\nlogging = tf.logging\r\nflags.DEFINE_integer(\"batch_size\", 1, \"inference batch size\")\r\nflags.DEFINE_integer(\"step\", 1, \"step size for infernece\")\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef data_type():\r\n    return tf.float32\r\n\r\n\r\nclass InputData(object):\r\n    \r\n    def __init__(self, config):\r\n        self.batch_size = batch_size = config.batch_size\r\n        self.num_steps = num_steps = config.num_steps\r\n        self.hidden_size = hidden_size = config.hidden_size\r\n        self.input_data = tf.placeholder(data_type(), [batch_size, num_steps, hidden_size], name = 'input_data')\r\n\r\n\r\nclass Config(object):\r\n    num_layers = 1\r\n    num_steps = 20\r\n    hidden_size = 256\r\n    batch_size = 20\r\n    vocab_size = 10000\r\n    init_scale = 0.1\r\n    num_iter = 50\r\n    warm_iter = 2\r\n\r\n\r\nclass LSTMModel(object):\r\n    \"\"\" Only forward, No Embedding\r\n    \"\"\"\r\n\r\n    def __init__(self, config, input_):\r\n        self._input = input_\r\n        self._input_data = self._input.input_data\r\n        \r\n        batch_size = input_.batch_size\r\n        num_steps =  input_.num_steps\r\n        size = config.hidden_size\r\n        vocab_size = config.vocab_size\r\n\r\n        def lstm_cell():\r\n            if 'reuse' in inspect.getargspec(\r\n              tf.contrib.rnn.BasicLSTMCell.__init__).args:\r\n                print(\"reuse\")\r\n                return tf.contrib.rnn.BasicLSTMCell(\r\n                    size, forget_bias=0., state_is_tuple=True,\r\n                    reuse = tf.get_variable_scope().reuse)\r\n            else:\r\n                print(\"not reuse\")\r\n                return tf.contrib.rnn.BasicLSTMCell(\r\n                    size, forget_bias=0.0, state_is_tuple=True)\r\n\r\n        attn_cell = lstm_cell\r\n        cell = tf.contrib.rnn.MultiRNNCell(\r\n            [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)\r\n\r\n        self._initial_state = cell.zero_state(batch_size, data_type())\r\n\r\n        outputs = []\r\n        state = self._initial_state\r\n        with tf.variable_scope(\"RNN\"):\r\n            for time_step in range(num_steps):\r\n                if time_step > 0 : tf.get_variable_scope().reuse_variables()\r\n                (cell_output, state) = cell(self._input.input_data[:, time_step, :], state)\r\n                outputs.append(cell_output)\r\n        \r\n        self._output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])\r\n        self._final_state = state\r\n        softmax_w = tf.get_variable(\r\n            \"softmax_w\", [size, vocab_size], dtype=data_type())\r\n        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\r\n        self._logits = tf.matmul(self._output, softmax_w) + softmax_b\r\n\r\n        return\r\n\r\n    @property\r\n    def initial_state(self):\r\n        return self._initial_state\r\n\r\n    @property\r\n    def logits(self):\r\n        return self._logits\r\n  \r\n    @property\r\n    def input_data(self):\r\n        return self._input_data\r\n\r\n\r\ndef run_inference(session, model, input_data, sv) :\r\n    # initialize with a clean state\r\n    state = session.run(model.initial_state)\r\n\r\n    fetches = {}\r\n    fetches['logit'] = model.logits\r\n\r\n    feed_dict = {}\r\n    feed_dict[model.input_data] = input_data\r\n    for i, (c, h) in enumerate(model.initial_state):\r\n        feed_dict[c] = state[i].c\r\n        feed_dict[h] = state[i].h\r\n\r\n    session.run(fetches, feed_dict)\r\n\r\n\r\ndef main(_):\r\n    # config\r\n    eval_config = Config()\r\n    eval_config.num_steps = FLAGS.step\r\n    eval_config.batch_size = FLAGS.batch_size\r\n\r\n    # generate random data\r\n    input_data = np.random.rand(eval_config.batch_size, eval_config.num_steps, eval_config.hidden_size).astype(np.float32)\r\n\r\n    with tf.Graph().as_default():\r\n        initializer = tf.random_uniform_initializer(-eval_config.init_scale, \r\n                                                    eval_config.init_scale)\r\n        with tf.name_scope('Inference'):\r\n            _input = InputData(eval_config)\r\n            with tf.variable_scope('Model', reuse=None, initializer=initializer):\r\n                model = LSTMModel(config=eval_config, input_=_input)\r\n\r\n        sv = tf.train.Supervisor()\r\n        sess_config = tf.ConfigProto(allow_soft_placement=True,\r\n                                     log_device_placement=False)\r\n        # enable xla\r\n        sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\n        # run inference\r\n        with sv.managed_session(config=sess_config) as session:\r\n            run_inference(session, model, input_data, sv)\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n```"}