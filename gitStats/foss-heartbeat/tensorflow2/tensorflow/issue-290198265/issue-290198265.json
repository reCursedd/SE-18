{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16259", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16259/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16259/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16259/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/16259", "id": 290198265, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY0MTMxMDI3", "number": 16259, "title": "Fix two small issues of XLA", "user": {"login": "yangjunpro", "id": 407784, "node_id": "MDQ6VXNlcjQwNzc4NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/407784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yangjunpro", "html_url": "https://github.com/yangjunpro", "followers_url": "https://api.github.com/users/yangjunpro/followers", "following_url": "https://api.github.com/users/yangjunpro/following{/other_user}", "gists_url": "https://api.github.com/users/yangjunpro/gists{/gist_id}", "starred_url": "https://api.github.com/users/yangjunpro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yangjunpro/subscriptions", "organizations_url": "https://api.github.com/users/yangjunpro/orgs", "repos_url": "https://api.github.com/users/yangjunpro/repos", "events_url": "https://api.github.com/users/yangjunpro/events{/privacy}", "received_events_url": "https://api.github.com/users/yangjunpro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-01-20T15:01:24Z", "updated_at": "2018-03-16T20:48:01Z", "closed_at": "2018-03-16T20:48:01Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259", "html_url": "https://github.com/tensorflow/tensorflow/pull/16259", "diff_url": "https://github.com/tensorflow/tensorflow/pull/16259.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/16259.patch"}, "body_html": "<p>1.In Tensorflow, conventionally, INT32 ops are regarded as shape or control<br>\nops, and are registered on CPU only. XLA should follow the same rule, to<br>\navoid the potential data transfer between TF CPU ops and in GPU_XLA ops,<br>\nwhich result into performance degradation when XLA is turned-on.<br>\n2. Any OpKernel with more than 500 inputs is excluded. This is a temp<br>\nworkaround to avoid an potential issue that, the cuda drive do not accept<br>\na compiled PTX kernel with parameter space larger than 4352 bytes. The data type<br>\nof PTX kernel parameter is u64. An OpKernel with 500 inputs are more likely<br>\nto exceed the limit.</p>", "body_text": "1.In Tensorflow, conventionally, INT32 ops are regarded as shape or control\nops, and are registered on CPU only. XLA should follow the same rule, to\navoid the potential data transfer between TF CPU ops and in GPU_XLA ops,\nwhich result into performance degradation when XLA is turned-on.\n2. Any OpKernel with more than 500 inputs is excluded. This is a temp\nworkaround to avoid an potential issue that, the cuda drive do not accept\na compiled PTX kernel with parameter space larger than 4352 bytes. The data type\nof PTX kernel parameter is u64. An OpKernel with 500 inputs are more likely\nto exceed the limit.", "body": "1.In Tensorflow, conventionally, INT32 ops are regarded as shape or control\r\nops, and are registered on CPU only. XLA should follow the same rule, to\r\navoid the potential data transfer between TF CPU ops and in GPU_XLA ops,\r\nwhich result into performance degradation when XLA is turned-on.\r\n2. Any OpKernel with more than 500 inputs is excluded. This is a temp\r\nworkaround to avoid an potential issue that, the cuda drive do not accept\r\na compiled PTX kernel with parameter space larger than 4352 bytes. The data type\r\nof PTX kernel parameter is u64. An OpKernel with 500 inputs are more likely\r\nto exceed the limit."}