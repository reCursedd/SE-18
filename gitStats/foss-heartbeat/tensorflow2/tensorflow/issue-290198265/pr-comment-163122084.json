{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163122084", "pull_request_review_id": 90684940, "id": 163122084, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzEyMjA4NA==", "diff_hunk": "@@ -45,6 +45,42 @@ const char* const kXlaOutsideCompilationAttr = \"_XlaOutsideCompilation\";\n \n namespace {\n \n+// An node will not be processed by XLA even it has the XLA kernel:\n+// 1, In Tensorflow, conventionally, INT32 ops are regarded as shape or control ops, and", "path": "tensorflow/compiler/jit/mark_for_compilation_pass.cc", "position": 5, "original_position": 5, "commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "original_commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "user": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "body": "Can you say more about why you want to do this? Do you have an example where int32 copies cause a problem?\r\n\r\nI am reluctant to do this because it is something of a historical accident that the TF classic GPU device doesn't have int32 kernels, and ideally it is something that we would fix. I think a more specific motivating example would help clarify things for me here.", "created_at": "2018-01-23T01:32:32Z", "updated_at": "2018-01-23T01:36:26Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163122084", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163122084"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163122084"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259"}}, "body_html": "<p>Can you say more about why you want to do this? Do you have an example where int32 copies cause a problem?</p>\n<p>I am reluctant to do this because it is something of a historical accident that the TF classic GPU device doesn't have int32 kernels, and ideally it is something that we would fix. I think a more specific motivating example would help clarify things for me here.</p>", "body_text": "Can you say more about why you want to do this? Do you have an example where int32 copies cause a problem?\nI am reluctant to do this because it is something of a historical accident that the TF classic GPU device doesn't have int32 kernels, and ideally it is something that we would fix. I think a more specific motivating example would help clarify things for me here."}