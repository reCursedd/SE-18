{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163124441", "pull_request_review_id": 90687672, "id": 163124441, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzEyNDQ0MQ==", "diff_hunk": "@@ -45,6 +45,42 @@ const char* const kXlaOutsideCompilationAttr = \"_XlaOutsideCompilation\";\n \n namespace {\n \n+// An node will not be processed by XLA even it has the XLA kernel:\n+// 1, In Tensorflow, conventionally, INT32 ops are regarded as shape or control ops, and\n+// are registered on CPU only. XLA should follow the same rule, to avoid the potential\n+// redundant memcopy in GPU_XLA case.\n+// 2, Any OpKernel with more than 500 inputs is excluded. This is a temp workaround to avoid an", "path": "tensorflow/compiler/jit/mark_for_compilation_pass.cc", "position": 8, "original_position": 8, "commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "original_commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "> whether or not the original TensorFlow operator has many inputs does not necessarily imply generated kernel has that many inputs, although I agree it is likely to lead to that for an operator such as a concatenation.\r\n\r\nYes, that's exactly right, and why I'm also hesitant about fixing it in this way.\r\n\r\nI think the right change is to fix the lowering in the XLA GPU backend to do something sane when it tries to construct a kernel with many inputs.  Essentially you'd want to add a new \"many-args\" calling convention.  A simple implementation of this calling convention would be:\r\n\r\n  * Make the kernel take a single argument, namely `void** params`.  Then instead of referring to \"paramN\" in the kernel's body, we'd refer to params[N].\r\n  * Before running the kernel, construct the params array in memory, probably as a H to D memcpy.\r\n\r\nOne might have to introduce a new `Thunk` subclass to accomplish this.  One would also have to teach buffer assignment that HLOs with the many-args calling convention need this additional buffer for their args.\r\n\r\n#14507 is another bug report to the same effect.", "created_at": "2018-01-23T01:50:55Z", "updated_at": "2018-01-23T01:51:05Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163124441", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163124441"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163124441"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259"}}, "body_html": "<blockquote>\n<p>whether or not the original TensorFlow operator has many inputs does not necessarily imply generated kernel has that many inputs, although I agree it is likely to lead to that for an operator such as a concatenation.</p>\n</blockquote>\n<p>Yes, that's exactly right, and why I'm also hesitant about fixing it in this way.</p>\n<p>I think the right change is to fix the lowering in the XLA GPU backend to do something sane when it tries to construct a kernel with many inputs.  Essentially you'd want to add a new \"many-args\" calling convention.  A simple implementation of this calling convention would be:</p>\n<ul>\n<li>Make the kernel take a single argument, namely <code>void** params</code>.  Then instead of referring to \"paramN\" in the kernel's body, we'd refer to params[N].</li>\n<li>Before running the kernel, construct the params array in memory, probably as a H to D memcpy.</li>\n</ul>\n<p>One might have to introduce a new <code>Thunk</code> subclass to accomplish this.  One would also have to teach buffer assignment that HLOs with the many-args calling convention need this additional buffer for their args.</p>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"273288200\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/14507\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/14507/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/14507\">#14507</a> is another bug report to the same effect.</p>", "body_text": "whether or not the original TensorFlow operator has many inputs does not necessarily imply generated kernel has that many inputs, although I agree it is likely to lead to that for an operator such as a concatenation.\n\nYes, that's exactly right, and why I'm also hesitant about fixing it in this way.\nI think the right change is to fix the lowering in the XLA GPU backend to do something sane when it tries to construct a kernel with many inputs.  Essentially you'd want to add a new \"many-args\" calling convention.  A simple implementation of this calling convention would be:\n\nMake the kernel take a single argument, namely void** params.  Then instead of referring to \"paramN\" in the kernel's body, we'd refer to params[N].\nBefore running the kernel, construct the params array in memory, probably as a H to D memcpy.\n\nOne might have to introduce a new Thunk subclass to accomplish this.  One would also have to teach buffer assignment that HLOs with the many-args calling convention need this additional buffer for their args.\n#14507 is another bug report to the same effect.", "in_reply_to_id": 163122540}