{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163191956", "pull_request_review_id": 90762937, "id": 163191956, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzE5MTk1Ng==", "diff_hunk": "@@ -45,6 +45,42 @@ const char* const kXlaOutsideCompilationAttr = \"_XlaOutsideCompilation\";\n \n namespace {\n \n+// An node will not be processed by XLA even it has the XLA kernel:\n+// 1, In Tensorflow, conventionally, INT32 ops are regarded as shape or control ops, and\n+// are registered on CPU only. XLA should follow the same rule, to avoid the potential\n+// redundant memcopy in GPU_XLA case.\n+// 2, Any OpKernel with more than 500 inputs is excluded. This is a temp workaround to avoid an", "path": "tensorflow/compiler/jit/mark_for_compilation_pass.cc", "position": 8, "original_position": 8, "commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "original_commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "user": {"login": "yangjunpro", "id": 407784, "node_id": "MDQ6VXNlcjQwNzc4NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/407784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yangjunpro", "html_url": "https://github.com/yangjunpro", "followers_url": "https://api.github.com/users/yangjunpro/followers", "following_url": "https://api.github.com/users/yangjunpro/following{/other_user}", "gists_url": "https://api.github.com/users/yangjunpro/gists{/gist_id}", "starred_url": "https://api.github.com/users/yangjunpro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yangjunpro/subscriptions", "organizations_url": "https://api.github.com/users/yangjunpro/orgs", "repos_url": "https://api.github.com/users/yangjunpro/repos", "events_url": "https://api.github.com/users/yangjunpro/events{/privacy}", "received_events_url": "https://api.github.com/users/yangjunpro/received_events", "type": "User", "site_admin": false}, "body": "Justin,\r\n\r\nAs you and Hawkinsp metioned, fix 2 is actually a work-around and it could resolve our internal issue but not a fundamental solution. I agree we may solve it in a more principal way----add flexible enough support for op with many inputs althrough.\r\n\r\nAs to fix 1, May I know why \"it is something of a historical accident that the TF classic GPU device doesn't have int32 kernels\"? I am curious about this design situation. \r\nThe reason for us to fix it is that for some complex models such as Bi-LSTM, without this fix, there will be many CPU2GPU data transfer which impede XLA performance.  If you have interest, I could explain it with more details.\r\n", "created_at": "2018-01-23T09:56:22Z", "updated_at": "2018-01-23T09:56:22Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163191956", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163191956"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163191956"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259"}}, "body_html": "<p>Justin,</p>\n<p>As you and Hawkinsp metioned, fix 2 is actually a work-around and it could resolve our internal issue but not a fundamental solution. I agree we may solve it in a more principal way----add flexible enough support for op with many inputs althrough.</p>\n<p>As to fix 1, May I know why \"it is something of a historical accident that the TF classic GPU device doesn't have int32 kernels\"? I am curious about this design situation.<br>\nThe reason for us to fix it is that for some complex models such as Bi-LSTM, without this fix, there will be many CPU2GPU data transfer which impede XLA performance.  If you have interest, I could explain it with more details.</p>", "body_text": "Justin,\nAs you and Hawkinsp metioned, fix 2 is actually a work-around and it could resolve our internal issue but not a fundamental solution. I agree we may solve it in a more principal way----add flexible enough support for op with many inputs althrough.\nAs to fix 1, May I know why \"it is something of a historical accident that the TF classic GPU device doesn't have int32 kernels\"? I am curious about this design situation.\nThe reason for us to fix it is that for some complex models such as Bi-LSTM, without this fix, there will be many CPU2GPU data transfer which impede XLA performance.  If you have interest, I could explain it with more details.", "in_reply_to_id": 163122540}