{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163303883", "pull_request_review_id": 90895239, "id": 163303883, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzMwMzg4Mw==", "diff_hunk": "@@ -45,6 +45,42 @@ const char* const kXlaOutsideCompilationAttr = \"_XlaOutsideCompilation\";\n \n namespace {\n \n+// An node will not be processed by XLA even it has the XLA kernel:\n+// 1, In Tensorflow, conventionally, INT32 ops are regarded as shape or control ops, and\n+// are registered on CPU only. XLA should follow the same rule, to avoid the potential\n+// redundant memcopy in GPU_XLA case.\n+// 2, Any OpKernel with more than 500 inputs is excluded. This is a temp workaround to avoid an", "path": "tensorflow/compiler/jit/mark_for_compilation_pass.cc", "position": 8, "original_position": 8, "commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "original_commit_id": "1014b797bdb3a036da8d011e86f99f347580f948", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "> A simple implementation of this calling convention would be:\r\n\r\nPerhaps an even simpler implementation of this calling convention would be to have  void** of all of the root computation's params and outputs in GPU memory from the start.  That is, have one big array instead of one array per op with the large calling convention.\r\n\r\nOne would need to avoid creating this array if no HLOs have the large calling convention, to avoid that overhead.  But otherwise I expect it's simpler and probably more efficient if you have many ops with the large calling convention.", "created_at": "2018-01-23T16:42:37Z", "updated_at": "2018-01-23T16:42:37Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163303883", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163303883"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16259#discussion_r163303883"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16259"}}, "body_html": "<blockquote>\n<p>A simple implementation of this calling convention would be:</p>\n</blockquote>\n<p>Perhaps an even simpler implementation of this calling convention would be to have  void** of all of the root computation's params and outputs in GPU memory from the start.  That is, have one big array instead of one array per op with the large calling convention.</p>\n<p>One would need to avoid creating this array if no HLOs have the large calling convention, to avoid that overhead.  But otherwise I expect it's simpler and probably more efficient if you have many ops with the large calling convention.</p>", "body_text": "A simple implementation of this calling convention would be:\n\nPerhaps an even simpler implementation of this calling convention would be to have  void** of all of the root computation's params and outputs in GPU memory from the start.  That is, have one big array instead of one array per op with the large calling convention.\nOne would need to avoid creating this array if no HLOs have the large calling convention, to avoid that overhead.  But otherwise I expect it's simpler and probably more efficient if you have many ops with the large calling convention.", "in_reply_to_id": 163122540}