{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237294928", "html_url": "https://github.com/tensorflow/tensorflow/issues/3603#issuecomment-237294928", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3603", "id": 237294928, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzI5NDkyOA==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-03T16:47:46Z", "updated_at": "2016-08-03T16:47:46Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12414695\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rob-rowe\">@rob-rowe</a> Thanks for the nvprof output - it looks like there are two common shapes of Sum reduction ops in your trace and one of them is being evaluated very differently by Eigen in 0.10 resulting in much less GPU parallelism and about a 60x slowdown!  (3.3ms -&gt; 178ms)</p>\n<p>It would help us to make a quick repro if we knew the shapes of the operands to these ops...<br>\nDo you happen to have this information to hand?  If not, we could either get if from the GraphDef or the StepStats proto.</p>\n<p>You can save a graphdef with shape annotations using code like this:<br>\n<code>tf.train.write_graph(g.as_graph_def(add_shapes=True),  '/tmp', 'graphdef.pbtxt')</code></p>\n<p>(Since you have timelines, you already have the code to capture the stepstats proto - this can be written to a .pbtxt file.  It contains the runtime shapes of all of the Tensors.)</p>", "body_text": "@rob-rowe Thanks for the nvprof output - it looks like there are two common shapes of Sum reduction ops in your trace and one of them is being evaluated very differently by Eigen in 0.10 resulting in much less GPU parallelism and about a 60x slowdown!  (3.3ms -> 178ms)\nIt would help us to make a quick repro if we knew the shapes of the operands to these ops...\nDo you happen to have this information to hand?  If not, we could either get if from the GraphDef or the StepStats proto.\nYou can save a graphdef with shape annotations using code like this:\ntf.train.write_graph(g.as_graph_def(add_shapes=True),  '/tmp', 'graphdef.pbtxt')\n(Since you have timelines, you already have the code to capture the stepstats proto - this can be written to a .pbtxt file.  It contains the runtime shapes of all of the Tensors.)", "body": "@rob-rowe Thanks for the nvprof output - it looks like there are two common shapes of Sum reduction ops in your trace and one of them is being evaluated very differently by Eigen in 0.10 resulting in much less GPU parallelism and about a 60x slowdown!  (3.3ms -> 178ms)\n\nIt would help us to make a quick repro if we knew the shapes of the operands to these ops...  \nDo you happen to have this information to hand?  If not, we could either get if from the GraphDef or the StepStats proto.\n\nYou can save a graphdef with shape annotations using code like this:\n`tf.train.write_graph(g.as_graph_def(add_shapes=True),  '/tmp', 'graphdef.pbtxt')`\n\n(Since you have timelines, you already have the code to capture the stepstats proto - this can be written to a .pbtxt file.  It contains the runtime shapes of all of the Tensors.) \n"}