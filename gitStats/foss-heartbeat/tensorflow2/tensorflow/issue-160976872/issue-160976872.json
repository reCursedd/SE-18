{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2938", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2938/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2938/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2938/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2938", "id": 160976872, "node_id": "MDU6SXNzdWUxNjA5NzY4NzI=", "number": 2938, "title": "dynamic_rnn after reshape seems impossible", "user": {"login": "mp2893", "id": 17195131, "node_id": "MDQ6VXNlcjE3MTk1MTMx", "avatar_url": "https://avatars3.githubusercontent.com/u/17195131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mp2893", "html_url": "https://github.com/mp2893", "followers_url": "https://api.github.com/users/mp2893/followers", "following_url": "https://api.github.com/users/mp2893/following{/other_user}", "gists_url": "https://api.github.com/users/mp2893/gists{/gist_id}", "starred_url": "https://api.github.com/users/mp2893/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mp2893/subscriptions", "organizations_url": "https://api.github.com/users/mp2893/orgs", "repos_url": "https://api.github.com/users/mp2893/repos", "events_url": "https://api.github.com/users/mp2893/events{/privacy}", "received_events_url": "https://api.github.com/users/mp2893/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2016-06-17T21:19:02Z", "updated_at": "2017-10-26T15:39:47Z", "closed_at": "2016-10-26T14:03:05Z", "author_association": "NONE", "body_html": "<p>I need to multiply a 3D tensor by a 2D weight matrix, then feed it to dynamic_rnn.<br>\nBelow is the code.</p>\n<pre><code>inputSize = 1000\nembeddingSize = 100\n\nbatchX = tf.placeholder(tf.float32, [None, None, inputSize])\n#The maximum length of the sequences and the size of the mini-batch are dynamic\n#batchX is a time-major 3D tensor\n\nbatchXLenghts = tf.placeholder(tf.int32, [None,])\n#A list of integers indicating the length of each sequence in batchX\n\nmaxLength = tf.shape(batchX)[0]\nbatchSize = tf.shape(batchX)[1]\n\nwith tf.variable_scope('embedding') as scope:\n    W_emb = tf.get_variable('W_emb', [inputDimSize, embDimSize], initializer=tf.truncated_normal_initializer())\n    b_emb = tf.get_variable('b_emb', [embDimSize,], initializer=tf.constant_initializer())\n\nemb = tf.reshape(tf.matmul(tf.reshape(batchX, [maxLength*batchSize, inputDimSize]), W_emb) + b_emb, [maxLength, batchSize, embDimSize])\n#embedding step. There are two reshape operations because I am doing np.dot(3D, 2D)\n\ncell = GRUCell(embDimSize)\noutputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n\n# calculating logits, loss, etc.\n...\n...\n</code></pre>\n<p>When I run this code, I get the following error</p>\n<pre><code>Traceback (most recent call last):\n  File \"feedTestDynamicRnn.py\", line 127, in &lt;module&gt;\n    train(xFile=xFile, yFile=yFile)\n  File \"feedTestDynamicRnn.py\", line 98, in train\n    batchX, batchXLengths, batchY, logits, mean_loss = inference(options)\n  File \"feedTestDynamicRnn.py\", line 57, in inference\n    outputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 580, in dynamic_rnn\n    swap_memory=swap_memory, sequence_length=sequence_length)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 630, in _dynamic_rnn_loop\n    \"Input size (depth of inputs) must be accessible via shape inference, \"\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\n</code></pre>\n<p>When I comment out the embedding step, and feed <code>batchX</code> directly to <code>dynamic_rnn</code>, there is no problem. So it seems that <code>tf.reshape()</code> loses shape information as told by <a href=\"http://stackoverflow.com/questions/35374958/reshape-tensor-using-placeholder-value\" rel=\"nofollow\">this post</a>.</p>\n<p>The problem is, I cannot fix <code>maxLength</code> and <code>batchSize</code> to a static value, because they need to change. Also, I need to do embedding, so directly feeding <code>batchX</code> to the RNN is not an option. Is there a work around for this?</p>", "body_text": "I need to multiply a 3D tensor by a 2D weight matrix, then feed it to dynamic_rnn.\nBelow is the code.\ninputSize = 1000\nembeddingSize = 100\n\nbatchX = tf.placeholder(tf.float32, [None, None, inputSize])\n#The maximum length of the sequences and the size of the mini-batch are dynamic\n#batchX is a time-major 3D tensor\n\nbatchXLenghts = tf.placeholder(tf.int32, [None,])\n#A list of integers indicating the length of each sequence in batchX\n\nmaxLength = tf.shape(batchX)[0]\nbatchSize = tf.shape(batchX)[1]\n\nwith tf.variable_scope('embedding') as scope:\n    W_emb = tf.get_variable('W_emb', [inputDimSize, embDimSize], initializer=tf.truncated_normal_initializer())\n    b_emb = tf.get_variable('b_emb', [embDimSize,], initializer=tf.constant_initializer())\n\nemb = tf.reshape(tf.matmul(tf.reshape(batchX, [maxLength*batchSize, inputDimSize]), W_emb) + b_emb, [maxLength, batchSize, embDimSize])\n#embedding step. There are two reshape operations because I am doing np.dot(3D, 2D)\n\ncell = GRUCell(embDimSize)\noutputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n\n# calculating logits, loss, etc.\n...\n...\n\nWhen I run this code, I get the following error\nTraceback (most recent call last):\n  File \"feedTestDynamicRnn.py\", line 127, in <module>\n    train(xFile=xFile, yFile=yFile)\n  File \"feedTestDynamicRnn.py\", line 98, in train\n    batchX, batchXLengths, batchY, logits, mean_loss = inference(options)\n  File \"feedTestDynamicRnn.py\", line 57, in inference\n    outputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 580, in dynamic_rnn\n    swap_memory=swap_memory, sequence_length=sequence_length)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 630, in _dynamic_rnn_loop\n    \"Input size (depth of inputs) must be accessible via shape inference, \"\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\n\nWhen I comment out the embedding step, and feed batchX directly to dynamic_rnn, there is no problem. So it seems that tf.reshape() loses shape information as told by this post.\nThe problem is, I cannot fix maxLength and batchSize to a static value, because they need to change. Also, I need to do embedding, so directly feeding batchX to the RNN is not an option. Is there a work around for this?", "body": "I need to multiply a 3D tensor by a 2D weight matrix, then feed it to dynamic_rnn.\nBelow is the code.\n\n```\ninputSize = 1000\nembeddingSize = 100\n\nbatchX = tf.placeholder(tf.float32, [None, None, inputSize])\n#The maximum length of the sequences and the size of the mini-batch are dynamic\n#batchX is a time-major 3D tensor\n\nbatchXLenghts = tf.placeholder(tf.int32, [None,])\n#A list of integers indicating the length of each sequence in batchX\n\nmaxLength = tf.shape(batchX)[0]\nbatchSize = tf.shape(batchX)[1]\n\nwith tf.variable_scope('embedding') as scope:\n    W_emb = tf.get_variable('W_emb', [inputDimSize, embDimSize], initializer=tf.truncated_normal_initializer())\n    b_emb = tf.get_variable('b_emb', [embDimSize,], initializer=tf.constant_initializer())\n\nemb = tf.reshape(tf.matmul(tf.reshape(batchX, [maxLength*batchSize, inputDimSize]), W_emb) + b_emb, [maxLength, batchSize, embDimSize])\n#embedding step. There are two reshape operations because I am doing np.dot(3D, 2D)\n\ncell = GRUCell(embDimSize)\noutputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n\n# calculating logits, loss, etc.\n...\n...\n```\n\nWhen I run this code, I get the following error\n\n```\nTraceback (most recent call last):\n  File \"feedTestDynamicRnn.py\", line 127, in <module>\n    train(xFile=xFile, yFile=yFile)\n  File \"feedTestDynamicRnn.py\", line 98, in train\n    batchX, batchXLengths, batchY, logits, mean_loss = inference(options)\n  File \"feedTestDynamicRnn.py\", line 57, in inference\n    outputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 580, in dynamic_rnn\n    swap_memory=swap_memory, sequence_length=sequence_length)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 630, in _dynamic_rnn_loop\n    \"Input size (depth of inputs) must be accessible via shape inference, \"\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\n```\n\nWhen I comment out the embedding step, and feed `batchX` directly to `dynamic_rnn`, there is no problem. So it seems that `tf.reshape()` loses shape information as told by [this post](http://stackoverflow.com/questions/35374958/reshape-tensor-using-placeholder-value). \n\nThe problem is, I cannot fix `maxLength` and `batchSize` to a static value, because they need to change. Also, I need to do embedding, so directly feeding `batchX` to the RNN is not an option. Is there a work around for this?\n"}