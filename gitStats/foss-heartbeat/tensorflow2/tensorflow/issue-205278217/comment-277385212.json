{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/277385212", "html_url": "https://github.com/tensorflow/tensorflow/issues/7251#issuecomment-277385212", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7251", "id": 277385212, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NzM4NTIxMg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-03T22:54:51Z", "updated_at": "2017-02-03T23:21:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Running with a bit more instrumentation, see <a href=\"https://github.com/yaroslavvb/stuff/blob/master/dynamic_stitch_gpu.py\">dynamic_stitch_gpu.py</a> . There are two kinds of transfers going on. The inputs are transferred from GPU to host memory, the op runs, and then outputs are transferred from host memory to GPU memory.</p>\n<p>If you run with <code>output_partition_graphs=True</code> and look at <code>partition_graphs</code> in generated metadata <a href=\"https://github.com/yaroslavvb/stuff/blob/master/dynamic_stitch_gpu_profile.pbtxt\">pbtxt</a> you see there are ops like this</p>\n<pre><code>  node {\n    name: \"x/_0\"\n    op: \"_Send\"\n    input: \"x\"\n    device: \"/job:localhost/replica:0/task:0/gpu:0\"\n    attr {\n      key: \"T\"\n      value {\n        type: DT_FLOAT\n      }\n    }\n\n</code></pre>\n<p>This is the input <code>x</code> which is on GPU, and is transferred onto CPU because that input has <code>HostMemory</code> attribute, hence needs to be copied.</p>\n<p>Later you have ops like this</p>\n<pre><code>  node {\n    name: \"stitch-0/_4\"\n    op: \"_HostSend\"\n    input: \"stitch-0\"\n    device: \"/job:localhost/replica:0/task:0/gpu:0\"\n</code></pre>\n<p>which indicates that output was placed into Host memory, and needs to be transferred to GPU memory. To summarize, it seems like <code>dynamic_stitch</code> is not optimized for GPU, and may be faster to run it pinned  to CPU.</p>", "body_text": "Running with a bit more instrumentation, see dynamic_stitch_gpu.py . There are two kinds of transfers going on. The inputs are transferred from GPU to host memory, the op runs, and then outputs are transferred from host memory to GPU memory.\nIf you run with output_partition_graphs=True and look at partition_graphs in generated metadata pbtxt you see there are ops like this\n  node {\n    name: \"x/_0\"\n    op: \"_Send\"\n    input: \"x\"\n    device: \"/job:localhost/replica:0/task:0/gpu:0\"\n    attr {\n      key: \"T\"\n      value {\n        type: DT_FLOAT\n      }\n    }\n\n\nThis is the input x which is on GPU, and is transferred onto CPU because that input has HostMemory attribute, hence needs to be copied.\nLater you have ops like this\n  node {\n    name: \"stitch-0/_4\"\n    op: \"_HostSend\"\n    input: \"stitch-0\"\n    device: \"/job:localhost/replica:0/task:0/gpu:0\"\n\nwhich indicates that output was placed into Host memory, and needs to be transferred to GPU memory. To summarize, it seems like dynamic_stitch is not optimized for GPU, and may be faster to run it pinned  to CPU.", "body": "Running with a bit more instrumentation, see [dynamic_stitch_gpu.py](https://github.com/yaroslavvb/stuff/blob/master/dynamic_stitch_gpu.py) . There are two kinds of transfers going on. The inputs are transferred from GPU to host memory, the op runs, and then outputs are transferred from host memory to GPU memory. \r\n\r\nIf you run with `output_partition_graphs=True` and look at `partition_graphs` in generated metadata [pbtxt](https://github.com/yaroslavvb/stuff/blob/master/dynamic_stitch_gpu_profile.pbtxt) you see there are ops like this\r\n\r\n```\r\n  node {\r\n    name: \"x/_0\"\r\n    op: \"_Send\"\r\n    input: \"x\"\r\n    device: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n\r\n```\r\n\r\nThis is the input `x` which is on GPU, and is transferred onto CPU because that input has `HostMemory` attribute, hence needs to be copied.\r\n\r\nLater you have ops like this\r\n\r\n```\r\n  node {\r\n    name: \"stitch-0/_4\"\r\n    op: \"_HostSend\"\r\n    input: \"stitch-0\"\r\n    device: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n```\r\n\r\nwhich indicates that output was placed into Host memory, and needs to be transferred to GPU memory. To summarize, it seems like `dynamic_stitch` is not optimized for GPU, and may be faster to run it pinned  to CPU."}