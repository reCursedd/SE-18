{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9720", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9720/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9720/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9720/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9720", "id": 226763265, "node_id": "MDU6SXNzdWUyMjY3NjMyNjU=", "number": 9720, "title": "tf.while_loop swap_memory=True when not running train_op, does it always swap?", "user": {"login": "carlthome", "id": 1595907, "node_id": "MDQ6VXNlcjE1OTU5MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1595907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlthome", "html_url": "https://github.com/carlthome", "followers_url": "https://api.github.com/users/carlthome/followers", "following_url": "https://api.github.com/users/carlthome/following{/other_user}", "gists_url": "https://api.github.com/users/carlthome/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlthome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlthome/subscriptions", "organizations_url": "https://api.github.com/users/carlthome/orgs", "repos_url": "https://api.github.com/users/carlthome/repos", "events_url": "https://api.github.com/users/carlthome/events{/privacy}", "received_events_url": "https://api.github.com/users/carlthome/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-05-06T11:38:21Z", "updated_at": "2017-05-10T14:05:39Z", "closed_at": "2017-05-10T14:05:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>From the documentation of <code>tf.while_loop</code>:</p>\n<blockquote>\n<p>For training, TensorFlow remembers the tensors that are produced in the forward inference but needed in back propagation. These tensors can be a main source of memory consumption and often cause OOM problems when training on GPUs. When the flag swap_memory is true, we swap out these tensors from GPU to CPU. This for example allows us to train RNN models with very long sequences and large batches.</p>\n</blockquote>\n<p>But <strong>after</strong> having trained, is TensorFlow's engine smart enough to avoid swapping even if <code>swap_memory=True</code> or should <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py\">optimize_for_inference.py</a> flip swap_memory (i.e. make the equivalent modifications to the graph)?</p>\n<p>Naturally this only applies to running inference on a GPU.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2342391\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuanbyu\">@yuanbyu</a></p>", "body_text": "From the documentation of tf.while_loop:\n\nFor training, TensorFlow remembers the tensors that are produced in the forward inference but needed in back propagation. These tensors can be a main source of memory consumption and often cause OOM problems when training on GPUs. When the flag swap_memory is true, we swap out these tensors from GPU to CPU. This for example allows us to train RNN models with very long sequences and large batches.\n\nBut after having trained, is TensorFlow's engine smart enough to avoid swapping even if swap_memory=True or should optimize_for_inference.py flip swap_memory (i.e. make the equivalent modifications to the graph)?\nNaturally this only applies to running inference on a GPU.\n@yuanbyu", "body": "From the documentation of `tf.while_loop`:\r\n> For training, TensorFlow remembers the tensors that are produced in the forward inference but needed in back propagation. These tensors can be a main source of memory consumption and often cause OOM problems when training on GPUs. When the flag swap_memory is true, we swap out these tensors from GPU to CPU. This for example allows us to train RNN models with very long sequences and large batches.\r\n\r\nBut **after** having trained, is TensorFlow's engine smart enough to avoid swapping even if `swap_memory=True` or should [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) flip swap_memory (i.e. make the equivalent modifications to the graph)?\r\n\r\nNaturally this only applies to running inference on a GPU.\r\n\r\n@yuanbyu"}