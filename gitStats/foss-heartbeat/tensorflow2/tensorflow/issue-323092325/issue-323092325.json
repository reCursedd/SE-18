{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19286", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19286/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19286/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19286/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19286", "id": 323092325, "node_id": "MDU6SXNzdWUzMjMwOTIzMjU=", "number": 19286, "title": "[tf-gpu1.7][tf-serving] Error with cuDNN version incompatible issue when load a ckpt model to do the inference", "user": {"login": "oscarriddle", "id": 13745902, "node_id": "MDQ6VXNlcjEzNzQ1OTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/13745902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oscarriddle", "html_url": "https://github.com/oscarriddle", "followers_url": "https://api.github.com/users/oscarriddle/followers", "following_url": "https://api.github.com/users/oscarriddle/following{/other_user}", "gists_url": "https://api.github.com/users/oscarriddle/gists{/gist_id}", "starred_url": "https://api.github.com/users/oscarriddle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oscarriddle/subscriptions", "organizations_url": "https://api.github.com/users/oscarriddle/orgs", "repos_url": "https://api.github.com/users/oscarriddle/repos", "events_url": "https://api.github.com/users/oscarriddle/events{/privacy}", "received_events_url": "https://api.github.com/users/oscarriddle/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-05-15T07:09:55Z", "updated_at": "2018-05-17T16:18:45Z", "closed_at": "2018-05-17T16:18:45Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:  NO</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip (python 2.7)</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow-gpu==1.7.0</li>\n<li><strong>Python version</strong>:  python 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc 5.3</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA9.0, cuDNN7.0.5</li>\n<li><strong>GPU model and memory</strong>: Tesla P4, 8GB</li>\n<li><strong>Exact command to reproduce</strong>: NA</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I established a tensorflow-serving environment by gRPC, which load a model in MetaGraph/ckpt format and create a session to do inference. It worked fine under tensorflow-gpu 1.4.1, but now Error happened when I changed to tensorflow-gpu 1.7.0.</p>\n<p>Here is the log when server side starts up:</p>\n<pre><code>$ python classification_server.py -m inception_v3.ckpt-85000 -p 10000\n2018-05-15 14:44:53.703251: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-05-15 14:44:53.992419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:84:00.0\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\n2018-05-15 14:44:54.221179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:88:00.0\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\n2018-05-15 14:44:54.222130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-05-15 14:44:55.050563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-05-15 14:44:55.050618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 \n2018-05-15 14:44:55.050667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N Y \n2018-05-15 14:44:55.050677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   Y N \n2018-05-15 14:44:55.051199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2437 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)\n2018-05-15 14:44:55.098488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2437 MB memory) -&gt; physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:88:00.0, compute capability: 6.1)\nTensor(\"InceptionV3/Logits/global_pool:0\", shape=(?, 1, 1, 2048), dtype=float32)\n[Tue May 15 14:44:57 2018]server port 10000\n[Tue May 15 14:44:57 2018]config visual_model \n[Tue May 15 14:44:57 2018]server start at 10000\n\n</code></pre>\n<p>Here is the log on the server side when a client asks for image classification service:</p>\n<pre><code>2018-05-15 14:45:10.095470: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7103 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\n2018-05-15 14:45:10.096881: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo&lt;T&gt;(), &amp;algorithms) \nAborted (core dumped)\n</code></pre>\n<p>What does client side gets:</p>\n<pre><code>$ python client.py  -p 127.0.0.1:10000\nTraceback (most recent call last):\n  File \"client.py\", line 52, in &lt;module&gt;\n    run(options.port, options.feat_len)\n  File \"client.py\", line 26, in run\n    reply = stub.GetFeatureByImgs(tmp2)\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py\", line 487, in __call__\n    return _end_unary_response_blocking(state, call, False, deadline)\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py\", line 437, in _end_unary_response_blocking\n    raise _Rendezvous(state, None, None, deadline)\ngrpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Socket closed)&gt;\n</code></pre>\n<p>I checked the CUDA+cuDNN version in below way to ensure that I am using CUDA9.0 and cuDNN 7.0.5. Not sure why Tensorflow says I'm using cuDNN 7.1.3.</p>\n<pre><code>$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\nCuda compilation tools, release 9.0, V9.0.176\n</code></pre>\n<pre><code>$ python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib() + \"/python/_pywrap_tensorflow_internal.so\")' | xargs ldd\n/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\tlinux-vdso.so.1 =&gt;  (0x00007ffd3b9f7000)\n\tlibtensorflow_framework.so =&gt; /home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so (0x00007f5927b53000)\n\tlibcublas.so.9.0 =&gt; /home/web_server/xiaolun/cuda-9.0/lib64/libcublas.so.9.0 (0x00007f592440e000)\n\tlibcusolver.so.9.0 =&gt; /home/web_server/xiaolun/cuda-9.0/lib64/libcusolver.so.9.0 (0x00007f591f812000)\n\tlibcudart.so.9.0 =&gt; /home/web_server/xiaolun/cuda-9.0/lib64/libcudart.so.9.0 (0x00007f591f5a5000)\n\tlibdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f591f392000)\n\tlibpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f591f175000)\n\tlibgomp.so.1 =&gt; /home/web_server/gcc-5.3/lib64/libgomp.so.1 (0x00007f591ef56000)\n\tlibrt.so.1 =&gt; /lib64/librt.so.1 (0x00007f591ed4e000)\n\tlibm.so.6 =&gt; /lib64/libm.so.6 (0x00007f591ea4b000)\n\tlibstdc++.so.6 =&gt; /home/web_server/gcc-5.3/lib64/libstdc++.so.6 (0x00007f591e6bd000)\n\tlibgcc_s.so.1 =&gt; /home/web_server/gcc-5.3/lib64/libgcc_s.so.1 (0x00007f591e4a7000)\n\tlibc.so.6 =&gt; /lib64/libc.so.6 (0x00007f591e0e3000)\n\t/lib64/ld-linux-x86-64.so.2 (0x000055944f876000)\n\tlibcuda.so.1 =&gt; /lib64/libcuda.so.1 (0x00007f591d265000)\n\tlibcudnn.so.7 =&gt; /home/web_server/xiaolun/cuda-9.0/lib64/libcudnn.so.7 (0x00007f59093f9000)\n\tlibcufft.so.9.0 =&gt; /home/web_server/xiaolun/cuda-9.0/lib64/libcufft.so.9.0 (0x00007f5901358000)\n\tlibcurand.so.9.0 =&gt; /home/web_server/xiaolun/cuda-9.0/lib64/libcurand.so.9.0 (0x00007f58fd3f3000)\n\tlibnvidia-fatbinaryloader.so.384.90 =&gt; /lib64/libnvidia-fatbinaryloader.so.384.90 (0x00007f58fd1a1000)\n</code></pre>\n<p>Moreover, I tried to load a frozen model to do the inference in tensorflow-serving, and it worked fine.<br>\nHas anyone encountered a similar problem? Any idea will be welcome.</p>\n<p>Thanks,</p>\n<h3>Source code / logs</h3>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):  NO\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64\nTensorFlow installed from (source or binary): pip (python 2.7)\nTensorFlow version (use command below): tensorflow-gpu==1.7.0\nPython version:  python 2.7\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): gcc 5.3\nCUDA/cuDNN version: CUDA9.0, cuDNN7.0.5\nGPU model and memory: Tesla P4, 8GB\nExact command to reproduce: NA\n\nDescribe the problem\nI established a tensorflow-serving environment by gRPC, which load a model in MetaGraph/ckpt format and create a session to do inference. It worked fine under tensorflow-gpu 1.4.1, but now Error happened when I changed to tensorflow-gpu 1.7.0.\nHere is the log when server side starts up:\n$ python classification_server.py -m inception_v3.ckpt-85000 -p 10000\n2018-05-15 14:44:53.703251: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-05-15 14:44:53.992419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:84:00.0\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\n2018-05-15 14:44:54.221179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:88:00.0\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\n2018-05-15 14:44:54.222130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-05-15 14:44:55.050563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-05-15 14:44:55.050618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 \n2018-05-15 14:44:55.050667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N Y \n2018-05-15 14:44:55.050677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   Y N \n2018-05-15 14:44:55.051199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2437 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)\n2018-05-15 14:44:55.098488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2437 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:88:00.0, compute capability: 6.1)\nTensor(\"InceptionV3/Logits/global_pool:0\", shape=(?, 1, 1, 2048), dtype=float32)\n[Tue May 15 14:44:57 2018]server port 10000\n[Tue May 15 14:44:57 2018]config visual_model \n[Tue May 15 14:44:57 2018]server start at 10000\n\n\nHere is the log on the server side when a client asks for image classification service:\n2018-05-15 14:45:10.095470: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7103 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\n2018-05-15 14:45:10.096881: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \nAborted (core dumped)\n\nWhat does client side gets:\n$ python client.py  -p 127.0.0.1:10000\nTraceback (most recent call last):\n  File \"client.py\", line 52, in <module>\n    run(options.port, options.feat_len)\n  File \"client.py\", line 26, in run\n    reply = stub.GetFeatureByImgs(tmp2)\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py\", line 487, in __call__\n    return _end_unary_response_blocking(state, call, False, deadline)\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py\", line 437, in _end_unary_response_blocking\n    raise _Rendezvous(state, None, None, deadline)\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Socket closed)>\n\nI checked the CUDA+cuDNN version in below way to ensure that I am using CUDA9.0 and cuDNN 7.0.5. Not sure why Tensorflow says I'm using cuDNN 7.1.3.\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\nCuda compilation tools, release 9.0, V9.0.176\n\n$ python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib() + \"/python/_pywrap_tensorflow_internal.so\")' | xargs ldd\n/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\tlinux-vdso.so.1 =>  (0x00007ffd3b9f7000)\n\tlibtensorflow_framework.so => /home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so (0x00007f5927b53000)\n\tlibcublas.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcublas.so.9.0 (0x00007f592440e000)\n\tlibcusolver.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcusolver.so.9.0 (0x00007f591f812000)\n\tlibcudart.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcudart.so.9.0 (0x00007f591f5a5000)\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f591f392000)\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f591f175000)\n\tlibgomp.so.1 => /home/web_server/gcc-5.3/lib64/libgomp.so.1 (0x00007f591ef56000)\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f591ed4e000)\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f591ea4b000)\n\tlibstdc++.so.6 => /home/web_server/gcc-5.3/lib64/libstdc++.so.6 (0x00007f591e6bd000)\n\tlibgcc_s.so.1 => /home/web_server/gcc-5.3/lib64/libgcc_s.so.1 (0x00007f591e4a7000)\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f591e0e3000)\n\t/lib64/ld-linux-x86-64.so.2 (0x000055944f876000)\n\tlibcuda.so.1 => /lib64/libcuda.so.1 (0x00007f591d265000)\n\tlibcudnn.so.7 => /home/web_server/xiaolun/cuda-9.0/lib64/libcudnn.so.7 (0x00007f59093f9000)\n\tlibcufft.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcufft.so.9.0 (0x00007f5901358000)\n\tlibcurand.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcurand.so.9.0 (0x00007f58fd3f3000)\n\tlibnvidia-fatbinaryloader.so.384.90 => /lib64/libnvidia-fatbinaryloader.so.384.90 (0x00007f58fd1a1000)\n\nMoreover, I tried to load a frozen model to do the inference in tensorflow-serving, and it worked fine.\nHas anyone encountered a similar problem? Any idea will be welcome.\nThanks,\nSource code / logs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64\r\n- **TensorFlow installed from (source or binary)**: pip (python 2.7)\r\n- **TensorFlow version (use command below)**: tensorflow-gpu==1.7.0\r\n- **Python version**:  python 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.3\r\n- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5\r\n- **GPU model and memory**: Tesla P4, 8GB\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nI established a tensorflow-serving environment by gRPC, which load a model in MetaGraph/ckpt format and create a session to do inference. It worked fine under tensorflow-gpu 1.4.1, but now Error happened when I changed to tensorflow-gpu 1.7.0. \r\n\r\nHere is the log when server side starts up:\r\n```\r\n$ python classification_server.py -m inception_v3.ckpt-85000 -p 10000\r\n2018-05-15 14:44:53.703251: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-05-15 14:44:53.992419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\r\n2018-05-15 14:44:54.221179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:88:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\r\n2018-05-15 14:44:54.222130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1\r\n2018-05-15 14:44:55.050563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-15 14:44:55.050618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 \r\n2018-05-15 14:44:55.050667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N Y \r\n2018-05-15 14:44:55.050677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   Y N \r\n2018-05-15 14:44:55.051199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2437 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n2018-05-15 14:44:55.098488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2437 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:88:00.0, compute capability: 6.1)\r\nTensor(\"InceptionV3/Logits/global_pool:0\", shape=(?, 1, 1, 2048), dtype=float32)\r\n[Tue May 15 14:44:57 2018]server port 10000\r\n[Tue May 15 14:44:57 2018]config visual_model \r\n[Tue May 15 14:44:57 2018]server start at 10000\r\n\r\n```\r\nHere is the log on the server side when a client asks for image classification service:\r\n```\r\n2018-05-15 14:45:10.095470: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7103 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-05-15 14:45:10.096881: F tensorflow/core/kernels/conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted (core dumped)\r\n```\r\nWhat does client side gets:\r\n```\r\n$ python client.py  -p 127.0.0.1:10000\r\nTraceback (most recent call last):\r\n  File \"client.py\", line 52, in <module>\r\n    run(options.port, options.feat_len)\r\n  File \"client.py\", line 26, in run\r\n    reply = stub.GetFeatureByImgs(tmp2)\r\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py\", line 487, in __call__\r\n    return _end_unary_response_blocking(state, call, False, deadline)\r\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/grpc/_channel.py\", line 437, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Socket closed)>\r\n```\r\n\r\nI checked the CUDA+cuDNN version in below way to ensure that I am using CUDA9.0 and cuDNN 7.0.5. Not sure why Tensorflow says I'm using cuDNN 7.1.3.\r\n\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n```\r\n```\r\n$ python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib() + \"/python/_pywrap_tensorflow_internal.so\")' | xargs ldd\r\n/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\tlinux-vdso.so.1 =>  (0x00007ffd3b9f7000)\r\n\tlibtensorflow_framework.so => /home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so (0x00007f5927b53000)\r\n\tlibcublas.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcublas.so.9.0 (0x00007f592440e000)\r\n\tlibcusolver.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcusolver.so.9.0 (0x00007f591f812000)\r\n\tlibcudart.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcudart.so.9.0 (0x00007f591f5a5000)\r\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f591f392000)\r\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f591f175000)\r\n\tlibgomp.so.1 => /home/web_server/gcc-5.3/lib64/libgomp.so.1 (0x00007f591ef56000)\r\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f591ed4e000)\r\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f591ea4b000)\r\n\tlibstdc++.so.6 => /home/web_server/gcc-5.3/lib64/libstdc++.so.6 (0x00007f591e6bd000)\r\n\tlibgcc_s.so.1 => /home/web_server/gcc-5.3/lib64/libgcc_s.so.1 (0x00007f591e4a7000)\r\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f591e0e3000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x000055944f876000)\r\n\tlibcuda.so.1 => /lib64/libcuda.so.1 (0x00007f591d265000)\r\n\tlibcudnn.so.7 => /home/web_server/xiaolun/cuda-9.0/lib64/libcudnn.so.7 (0x00007f59093f9000)\r\n\tlibcufft.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcufft.so.9.0 (0x00007f5901358000)\r\n\tlibcurand.so.9.0 => /home/web_server/xiaolun/cuda-9.0/lib64/libcurand.so.9.0 (0x00007f58fd3f3000)\r\n\tlibnvidia-fatbinaryloader.so.384.90 => /lib64/libnvidia-fatbinaryloader.so.384.90 (0x00007f58fd1a1000)\r\n```\r\n\r\nMoreover, I tried to load a frozen model to do the inference in tensorflow-serving, and it worked fine.\r\nHas anyone encountered a similar problem? Any idea will be welcome.\r\n\r\nThanks,\r\n\r\n\r\n### Source code / logs\r\n\r\n"}