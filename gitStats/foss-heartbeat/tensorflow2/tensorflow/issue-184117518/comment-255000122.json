{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/255000122", "html_url": "https://github.com/tensorflow/tensorflow/issues/5084#issuecomment-255000122", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5084", "id": 255000122, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTAwMDEyMg==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-20T03:29:05Z", "updated_at": "2016-10-20T03:29:05Z", "author_association": "MEMBER", "body_html": "<p>Hi @chenqifeng22 : Could you elaborate a bit as to why you need to define this in python only? If the gradient for your custom op cannot be implemented in terms of existing ops, then you can also define a custom op for the gradient and use that. For example, the gradient of the <code>Sqrt</code> operation is the <a href=\"https://github.com/tensorflow/tensorflow/blob/c8a45a8e236776bed1d14fd71f3b6755bd63cc58/tensorflow/core/ops/math_ops.cc#L258\"><code>SqrtGrad</code></a> operation. Would something like that not work for you?</p>\n<p>Calling back into python during graph execution for any operation (and consequently gradient computation) is going to create performance problems - greatly hampering the use of multiple threads, cores and devices.</p>", "body_text": "Hi @chenqifeng22 : Could you elaborate a bit as to why you need to define this in python only? If the gradient for your custom op cannot be implemented in terms of existing ops, then you can also define a custom op for the gradient and use that. For example, the gradient of the Sqrt operation is the SqrtGrad operation. Would something like that not work for you?\nCalling back into python during graph execution for any operation (and consequently gradient computation) is going to create performance problems - greatly hampering the use of multiple threads, cores and devices.", "body": "Hi @chenqifeng22 : Could you elaborate a bit as to why you need to define this in python only? If the gradient for your custom op cannot be implemented in terms of existing ops, then you can also define a custom op for the gradient and use that. For example, the gradient of the `Sqrt` operation is the [`SqrtGrad`](https://github.com/tensorflow/tensorflow/blob/c8a45a8e236776bed1d14fd71f3b6755bd63cc58/tensorflow/core/ops/math_ops.cc#L258) operation. Would something like that not work for you?\n\nCalling back into python during graph execution for any operation (and consequently gradient computation) is going to create performance problems - greatly hampering the use of multiple threads, cores and devices.\n"}