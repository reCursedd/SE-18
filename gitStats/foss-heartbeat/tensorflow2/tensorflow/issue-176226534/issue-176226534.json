{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4321", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4321/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4321/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4321/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4321", "id": 176226534, "node_id": "MDU6SXNzdWUxNzYyMjY1MzQ=", "number": 4321, "title": "model.saver.save() freeze and get killed", "user": {"login": "todpole3", "id": 4227871, "node_id": "MDQ6VXNlcjQyMjc4NzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4227871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/todpole3", "html_url": "https://github.com/todpole3", "followers_url": "https://api.github.com/users/todpole3/followers", "following_url": "https://api.github.com/users/todpole3/following{/other_user}", "gists_url": "https://api.github.com/users/todpole3/gists{/gist_id}", "starred_url": "https://api.github.com/users/todpole3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/todpole3/subscriptions", "organizations_url": "https://api.github.com/users/todpole3/orgs", "repos_url": "https://api.github.com/users/todpole3/repos", "events_url": "https://api.github.com/users/todpole3/events{/privacy}", "received_events_url": "https://api.github.com/users/todpole3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-09-11T07:11:46Z", "updated_at": "2016-10-06T18:28:41Z", "closed_at": "2016-10-06T18:28:41Z", "author_association": "NONE", "body_html": "<p>Below is my flow for training an encoder-decoder model.</p>\n<p>The training runs alright. But the program froze and eventually got killed during the call to Saver.export_meta_graph().</p>\n<p>However, I was able to get the saved checkpoint and continue training.</p>\n<p>Does anyone know what may be the cause for this problem?</p>\n<pre><code>    for t in xrange(FLAGS.num_epochs):\n        print(\"Epoch %d\" % t)\n\n        start_time = time.time()\n\n        # shuffling training examples\n        # random.shuffle(train_set)\n\n        # progress bar\n        for _ in tqdm(xrange(FLAGS.steps_per_checkpoint)):\n            time.sleep(0.01)\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                             if train_buckets_scale[i] &gt; random_number_01])\n            formatted_example = model.get_batch(train_set, bucket_id)\n            _, step_loss, _ = model.step(sess, formatted_example, bucket_id, \n                                         forward_only=False)\n            loss += step_loss\n            current_step += 1\n\n        epoch_time = time.time() - start_time\n\n        # Once in a while, we save checkpoint, print statistics, and run evals.\n        if t % FLAGS.epochs_per_checkpoint == 0:\n\n            # Print statistics for the previous epoch.\n            loss /= FLAGS.steps_per_checkpoint\n            ppx = math.exp(loss) if loss &lt; 300 else float('inf')\n            print(\"learning rate %.4f epoch-time %.2f perplexity %.2f\" % (\n                model.learning_rate.eval(), epoch_time, ppx))\n\n            # Decrease learning rate if no improvement of loss was seen over last 3 times.\n            if len(previous_losses) &gt; 2 and loss &gt; max(previous_losses[-3:]):\n                sess.run(model.learning_rate_decay_op)\n            previous_losses.append(loss)\n\n            checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n</code></pre>", "body_text": "Below is my flow for training an encoder-decoder model.\nThe training runs alright. But the program froze and eventually got killed during the call to Saver.export_meta_graph().\nHowever, I was able to get the saved checkpoint and continue training.\nDoes anyone know what may be the cause for this problem?\n    for t in xrange(FLAGS.num_epochs):\n        print(\"Epoch %d\" % t)\n\n        start_time = time.time()\n\n        # shuffling training examples\n        # random.shuffle(train_set)\n\n        # progress bar\n        for _ in tqdm(xrange(FLAGS.steps_per_checkpoint)):\n            time.sleep(0.01)\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                             if train_buckets_scale[i] > random_number_01])\n            formatted_example = model.get_batch(train_set, bucket_id)\n            _, step_loss, _ = model.step(sess, formatted_example, bucket_id, \n                                         forward_only=False)\n            loss += step_loss\n            current_step += 1\n\n        epoch_time = time.time() - start_time\n\n        # Once in a while, we save checkpoint, print statistics, and run evals.\n        if t % FLAGS.epochs_per_checkpoint == 0:\n\n            # Print statistics for the previous epoch.\n            loss /= FLAGS.steps_per_checkpoint\n            ppx = math.exp(loss) if loss < 300 else float('inf')\n            print(\"learning rate %.4f epoch-time %.2f perplexity %.2f\" % (\n                model.learning_rate.eval(), epoch_time, ppx))\n\n            # Decrease learning rate if no improvement of loss was seen over last 3 times.\n            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n                sess.run(model.learning_rate_decay_op)\n            previous_losses.append(loss)\n\n            checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")", "body": "Below is my flow for training an encoder-decoder model.\n\nThe training runs alright. But the program froze and eventually got killed during the call to Saver.export_meta_graph(). \n\nHowever, I was able to get the saved checkpoint and continue training.\n\nDoes anyone know what may be the cause for this problem?\n\n```\n    for t in xrange(FLAGS.num_epochs):\n        print(\"Epoch %d\" % t)\n\n        start_time = time.time()\n\n        # shuffling training examples\n        # random.shuffle(train_set)\n\n        # progress bar\n        for _ in tqdm(xrange(FLAGS.steps_per_checkpoint)):\n            time.sleep(0.01)\n            random_number_01 = np.random.random_sample()\n            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                             if train_buckets_scale[i] > random_number_01])\n            formatted_example = model.get_batch(train_set, bucket_id)\n            _, step_loss, _ = model.step(sess, formatted_example, bucket_id, \n                                         forward_only=False)\n            loss += step_loss\n            current_step += 1\n\n        epoch_time = time.time() - start_time\n\n        # Once in a while, we save checkpoint, print statistics, and run evals.\n        if t % FLAGS.epochs_per_checkpoint == 0:\n\n            # Print statistics for the previous epoch.\n            loss /= FLAGS.steps_per_checkpoint\n            ppx = math.exp(loss) if loss < 300 else float('inf')\n            print(\"learning rate %.4f epoch-time %.2f perplexity %.2f\" % (\n                model.learning_rate.eval(), epoch_time, ppx))\n\n            # Decrease learning rate if no improvement of loss was seen over last 3 times.\n            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n                sess.run(model.learning_rate_decay_op)\n            previous_losses.append(loss)\n\n            checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n```\n"}