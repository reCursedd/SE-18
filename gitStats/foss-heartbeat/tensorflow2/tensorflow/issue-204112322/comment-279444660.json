{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279444660", "html_url": "https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-279444660", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7150", "id": 279444660, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTQ0NDY2MA==", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-13T16:33:06Z", "updated_at": "2017-02-13T16:33:06Z", "author_association": "MEMBER", "body_html": "<p>Are you only trying to shrink the file size, rather than reduce latency? If so, try using \"quantize_weights\" rather than \"quantize_nodes\" in your call to the graph transform tool. That will store the parameters as eight bit, but use float calculations inside the graph (expanding the weights to floats the first time the graph is run). That might help your accuracy problems.</p>", "body_text": "Are you only trying to shrink the file size, rather than reduce latency? If so, try using \"quantize_weights\" rather than \"quantize_nodes\" in your call to the graph transform tool. That will store the parameters as eight bit, but use float calculations inside the graph (expanding the weights to floats the first time the graph is run). That might help your accuracy problems.", "body": "Are you only trying to shrink the file size, rather than reduce latency? If so, try using \"quantize_weights\" rather than \"quantize_nodes\" in your call to the graph transform tool. That will store the parameters as eight bit, but use float calculations inside the graph (expanding the weights to floats the first time the graph is run). That might help your accuracy problems."}