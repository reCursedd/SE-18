{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279458213", "html_url": "https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-279458213", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7150", "id": 279458213, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTQ1ODIxMw==", "user": {"login": "bazinac", "id": 19685528, "node_id": "MDQ6VXNlcjE5Njg1NTI4", "avatar_url": "https://avatars0.githubusercontent.com/u/19685528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bazinac", "html_url": "https://github.com/bazinac", "followers_url": "https://api.github.com/users/bazinac/followers", "following_url": "https://api.github.com/users/bazinac/following{/other_user}", "gists_url": "https://api.github.com/users/bazinac/gists{/gist_id}", "starred_url": "https://api.github.com/users/bazinac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bazinac/subscriptions", "organizations_url": "https://api.github.com/users/bazinac/orgs", "repos_url": "https://api.github.com/users/bazinac/repos", "events_url": "https://api.github.com/users/bazinac/events{/privacy}", "received_events_url": "https://api.github.com/users/bazinac/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-13T17:17:11Z", "updated_at": "2017-02-13T18:01:27Z", "author_association": "NONE", "body_html": "<p>Actually that is what I did. First I stripped graph, then rounded weights (round_weights(num_steps=256)')<br>\nand then quantized weights using:</p>\n<p><code>bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/rounded.pb --out_graph=/tmp/eightbit.pb --inputs='Mul:0' --outputs=\"final_result_musoth,final_result\" --transforms='quantize_weights'</code></p>\n<p>However, after this step, it is not accuracy problem that I am hitting there - it is so that final_result_musoth layer produces super bad output when quantized (It is all the time casting almost 1 in favor of one category). I am unable to found what is causing this... With no quantization, both layers work just fine, but graph is too big. Any tips? Thanks</p>", "body_text": "Actually that is what I did. First I stripped graph, then rounded weights (round_weights(num_steps=256)')\nand then quantized weights using:\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/rounded.pb --out_graph=/tmp/eightbit.pb --inputs='Mul:0' --outputs=\"final_result_musoth,final_result\" --transforms='quantize_weights'\nHowever, after this step, it is not accuracy problem that I am hitting there - it is so that final_result_musoth layer produces super bad output when quantized (It is all the time casting almost 1 in favor of one category). I am unable to found what is causing this... With no quantization, both layers work just fine, but graph is too big. Any tips? Thanks", "body": "Actually that is what I did. First I stripped graph, then rounded weights (round_weights(num_steps=256)')\r\nand then quantized weights using:\r\n\r\n`bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/rounded.pb --out_graph=/tmp/eightbit.pb --inputs='Mul:0' --outputs=\"final_result_musoth,final_result\" --transforms='quantize_weights'`\r\n\r\nHowever, after this step, it is not accuracy problem that I am hitting there - it is so that final_result_musoth layer produces super bad output when quantized (It is all the time casting almost 1 in favor of one category). I am unable to found what is causing this... With no quantization, both layers work just fine, but graph is too big. Any tips? Thanks"}