{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312084551", "html_url": "https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-312084551", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7150", "id": 312084551, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjA4NDU1MQ==", "user": {"login": "cpgil", "id": 18169021, "node_id": "MDQ6VXNlcjE4MTY5MDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/18169021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpgil", "html_url": "https://github.com/cpgil", "followers_url": "https://api.github.com/users/cpgil/followers", "following_url": "https://api.github.com/users/cpgil/following{/other_user}", "gists_url": "https://api.github.com/users/cpgil/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpgil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpgil/subscriptions", "organizations_url": "https://api.github.com/users/cpgil/orgs", "repos_url": "https://api.github.com/users/cpgil/repos", "events_url": "https://api.github.com/users/cpgil/events{/privacy}", "received_events_url": "https://api.github.com/users/cpgil/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-29T19:47:40Z", "updated_at": "2017-06-29T19:47:40Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nI am using the latest tensorflow version (1.2.0 at this time) and I have been trying to quantize my model in order to use it in an Android application.</p>\n<p>The steps I have followed are:</p>\n<ul>\n<li>freeze_graph.py</li>\n<li>bazel-bin/tensorflow/python/tools/optimize_for_inference</li>\n<li>bazel-bin/tensorflow/tools/quantization/quantize_graph (eightbit)</li>\n</ul>\n<p>I have used the prebuilt libraries (nightly Android build artifacts) and also have built the libraries myself.</p>\n<p>I can load my model and run it, but the output I get is completely outside of the expected ranges (I expect +/-1e-3 and the values are around +/-1e6 with a very small variance).</p>\n<p>I am using a VGG-based model.</p>\n<p>I would really appreciate a little guidance. Thank you in advance.</p>", "body_text": "Hi,\nI am using the latest tensorflow version (1.2.0 at this time) and I have been trying to quantize my model in order to use it in an Android application.\nThe steps I have followed are:\n\nfreeze_graph.py\nbazel-bin/tensorflow/python/tools/optimize_for_inference\nbazel-bin/tensorflow/tools/quantization/quantize_graph (eightbit)\n\nI have used the prebuilt libraries (nightly Android build artifacts) and also have built the libraries myself.\nI can load my model and run it, but the output I get is completely outside of the expected ranges (I expect +/-1e-3 and the values are around +/-1e6 with a very small variance).\nI am using a VGG-based model.\nI would really appreciate a little guidance. Thank you in advance.", "body": "Hi,\r\nI am using the latest tensorflow version (1.2.0 at this time) and I have been trying to quantize my model in order to use it in an Android application.\r\n\r\nThe steps I have followed are:\r\n  - freeze_graph.py\r\n  - bazel-bin/tensorflow/python/tools/optimize_for_inference\r\n  - bazel-bin/tensorflow/tools/quantization/quantize_graph (eightbit)\r\n\r\nI have used the prebuilt libraries (nightly Android build artifacts) and also have built the libraries myself.\r\n\r\nI can load my model and run it, but the output I get is completely outside of the expected ranges (I expect +/-1e-3 and the values are around +/-1e6 with a very small variance).\r\n\r\nI am using a VGG-based model.\r\n\r\nI would really appreciate a little guidance. Thank you in advance.\r\n\r\n"}