{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276198640", "html_url": "https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-276198640", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7150", "id": 276198640, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjE5ODY0MA==", "user": {"login": "bazinac", "id": 19685528, "node_id": "MDQ6VXNlcjE5Njg1NTI4", "avatar_url": "https://avatars0.githubusercontent.com/u/19685528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bazinac", "html_url": "https://github.com/bazinac", "followers_url": "https://api.github.com/users/bazinac/followers", "following_url": "https://api.github.com/users/bazinac/following{/other_user}", "gists_url": "https://api.github.com/users/bazinac/gists{/gist_id}", "starred_url": "https://api.github.com/users/bazinac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bazinac/subscriptions", "organizations_url": "https://api.github.com/users/bazinac/orgs", "repos_url": "https://api.github.com/users/bazinac/repos", "events_url": "https://api.github.com/users/bazinac/events{/privacy}", "received_events_url": "https://api.github.com/users/bazinac/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-30T21:35:51Z", "updated_at": "2017-01-30T21:37:37Z", "author_association": "NONE", "body_html": "<p>Thanks for answer. I put before and after log around. runInference, all I got is:</p>\n<pre><code>\n`D/INFERNECE: before\nI/native: tensorflow_inference_jni.cc:228 End computing. Ran in 2822ms (2822ms avg over 1 runs)\nD/INFERNECE: after\nD/INFERNECE: before\nA/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0xb9583000 in tid 9986 (inference)\nApplication terminated.`\n\n</code></pre>\n<p>Regarding right data types reading - I suppose that quantization script should not change those, therefore when it works with just optimized_for_inference graph correctly and I just run quantization on it it shoud work in same manner, right?</p>", "body_text": "Thanks for answer. I put before and after log around. runInference, all I got is:\n\n`D/INFERNECE: before\nI/native: tensorflow_inference_jni.cc:228 End computing. Ran in 2822ms (2822ms avg over 1 runs)\nD/INFERNECE: after\nD/INFERNECE: before\nA/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0xb9583000 in tid 9986 (inference)\nApplication terminated.`\n\n\nRegarding right data types reading - I suppose that quantization script should not change those, therefore when it works with just optimized_for_inference graph correctly and I just run quantization on it it shoud work in same manner, right?", "body": "Thanks for answer. I put before and after log around. runInference, all I got is:\r\n```\r\n\r\n`D/INFERNECE: before\r\nI/native: tensorflow_inference_jni.cc:228 End computing. Ran in 2822ms (2822ms avg over 1 runs)\r\nD/INFERNECE: after\r\nD/INFERNECE: before\r\nA/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0xb9583000 in tid 9986 (inference)\r\nApplication terminated.`\r\n\r\n```\r\n\r\n\r\n\r\nRegarding right data types reading - I suppose that quantization script should not change those, therefore when it works with just optimized_for_inference graph correctly and I just run quantization on it it shoud work in same manner, right?"}