{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279492557", "html_url": "https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-279492557", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7150", "id": 279492557, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTQ5MjU1Nw==", "user": {"login": "bazinac", "id": 19685528, "node_id": "MDQ6VXNlcjE5Njg1NTI4", "avatar_url": "https://avatars0.githubusercontent.com/u/19685528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bazinac", "html_url": "https://github.com/bazinac", "followers_url": "https://api.github.com/users/bazinac/followers", "following_url": "https://api.github.com/users/bazinac/following{/other_user}", "gists_url": "https://api.github.com/users/bazinac/gists{/gist_id}", "starred_url": "https://api.github.com/users/bazinac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bazinac/subscriptions", "organizations_url": "https://api.github.com/users/bazinac/orgs", "repos_url": "https://api.github.com/users/bazinac/repos", "events_url": "https://api.github.com/users/bazinac/events{/privacy}", "received_events_url": "https://api.github.com/users/bazinac/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-13T19:16:45Z", "updated_at": "2017-02-13T19:25:26Z", "author_association": "NONE", "body_html": "<p>UPDATE: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a> I think this has something to do with quantize_weights.cc script, specially with this part:</p>\n<pre><code> // min_value == max_value is a tricky case. It can occur for general\n        // tensors, and of course for scalars. The quantized ops cannot deal\n        // with this case, so we set max_value to something else.\n        // It's a tricky question what is the numerically best solution to\n        // deal with this degeneracy.\n        // TODO(petewarden): Better use a tolerance than a hard comparison?\n        if (min == max) {\n          if (std::abs(min) &lt; 0.000001f) {\n            max = min + 1.0f;\n          } else if (min &gt; 0) {\n            max = 2.0f * min;\n          } else {\n            max = min / 2.0f;\n          }\n        }\n</code></pre>\n<p>There is some comparison (&lt;0.000001f) and in output of my  layer which basically works totally wrong after quantization I see interference results like this:<br>\n0.9999943<br>\n0.9999887<br>\n0.9999981<br>\n0.99999905<br>\n...<br>\nSo they basically range with difference of that threshold. Prior to quantization, those worked correctly and have shown something between 0 and 1 based on input. Does this indicate something or is it just coincidence?  Thanks</p>", "body_text": "UPDATE: @petewarden I think this has something to do with quantize_weights.cc script, specially with this part:\n // min_value == max_value is a tricky case. It can occur for general\n        // tensors, and of course for scalars. The quantized ops cannot deal\n        // with this case, so we set max_value to something else.\n        // It's a tricky question what is the numerically best solution to\n        // deal with this degeneracy.\n        // TODO(petewarden): Better use a tolerance than a hard comparison?\n        if (min == max) {\n          if (std::abs(min) < 0.000001f) {\n            max = min + 1.0f;\n          } else if (min > 0) {\n            max = 2.0f * min;\n          } else {\n            max = min / 2.0f;\n          }\n        }\n\nThere is some comparison (<0.000001f) and in output of my  layer which basically works totally wrong after quantization I see interference results like this:\n0.9999943\n0.9999887\n0.9999981\n0.99999905\n...\nSo they basically range with difference of that threshold. Prior to quantization, those worked correctly and have shown something between 0 and 1 based on input. Does this indicate something or is it just coincidence?  Thanks", "body": "UPDATE: @petewarden I think this has something to do with quantize_weights.cc script, specially with this part:\r\n```\r\n // min_value == max_value is a tricky case. It can occur for general\r\n        // tensors, and of course for scalars. The quantized ops cannot deal\r\n        // with this case, so we set max_value to something else.\r\n        // It's a tricky question what is the numerically best solution to\r\n        // deal with this degeneracy.\r\n        // TODO(petewarden): Better use a tolerance than a hard comparison?\r\n        if (min == max) {\r\n          if (std::abs(min) < 0.000001f) {\r\n            max = min + 1.0f;\r\n          } else if (min > 0) {\r\n            max = 2.0f * min;\r\n          } else {\r\n            max = min / 2.0f;\r\n          }\r\n        }\r\n```\r\n\r\nThere is some comparison (<0.000001f) and in output of my  layer which basically works totally wrong after quantization I see interference results like this:\r\n0.9999943\r\n0.9999887\r\n0.9999981\r\n0.99999905\r\n...\r\nSo they basically range with difference of that threshold. Prior to quantization, those worked correctly and have shown something between 0 and 1 based on input. Does this indicate something or is it just coincidence?  Thanks"}