{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276224842", "html_url": "https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-276224842", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7150", "id": 276224842, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjIyNDg0Mg==", "user": {"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-30T23:24:45Z", "updated_at": "2017-01-30T23:24:45Z", "author_association": "MEMBER", "body_html": "<p>To be clear, you have instantiated two unique TensorFlowInferenceInterface objects in your app, and you use one for each layer you're interested in? If you make the calls in the opposite order, does it still crash?</p>\n<p>To workaround your issue, if the input data remains the same in computing finaleResultA/B, you can and should get both output layers from a single runInference call on a single TensorFlowInferenceInterface instance. This will save you multiple seconds of duplicate computation anyway.</p>\n<p>It's possible you've run into a bug in TF, but without doublechecking that all the identifiers are correct, you're not using any of the same buffers etc, it's hard to diagnose if the problem is in TF or your app. Sometimes things can seem to work in one situation, when really there is still a problem and the other codepaths just aren't as picky about the data access. If you have a minimal reproducible example you'd be willing to share I can take a look, but otherwise I'd suggest just using a single TensorFlowInferenceInterface.</p>", "body_text": "To be clear, you have instantiated two unique TensorFlowInferenceInterface objects in your app, and you use one for each layer you're interested in? If you make the calls in the opposite order, does it still crash?\nTo workaround your issue, if the input data remains the same in computing finaleResultA/B, you can and should get both output layers from a single runInference call on a single TensorFlowInferenceInterface instance. This will save you multiple seconds of duplicate computation anyway.\nIt's possible you've run into a bug in TF, but without doublechecking that all the identifiers are correct, you're not using any of the same buffers etc, it's hard to diagnose if the problem is in TF or your app. Sometimes things can seem to work in one situation, when really there is still a problem and the other codepaths just aren't as picky about the data access. If you have a minimal reproducible example you'd be willing to share I can take a look, but otherwise I'd suggest just using a single TensorFlowInferenceInterface.", "body": "To be clear, you have instantiated two unique TensorFlowInferenceInterface objects in your app, and you use one for each layer you're interested in? If you make the calls in the opposite order, does it still crash? \r\n\r\nTo workaround your issue, if the input data remains the same in computing finaleResultA/B, you can and should get both output layers from a single runInference call on a single TensorFlowInferenceInterface instance. This will save you multiple seconds of duplicate computation anyway.\r\n\r\nIt's possible you've run into a bug in TF, but without doublechecking that all the identifiers are correct, you're not using any of the same buffers etc, it's hard to diagnose if the problem is in TF or your app. Sometimes things can seem to work in one situation, when really there is still a problem and the other codepaths just aren't as picky about the data access. If you have a minimal reproducible example you'd be willing to share I can take a look, but otherwise I'd suggest just using a single TensorFlowInferenceInterface.\r\n"}