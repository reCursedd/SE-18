{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280437815", "html_url": "https://github.com/tensorflow/tensorflow/issues/7150#issuecomment-280437815", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7150", "id": 280437815, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDQzNzgxNQ==", "user": {"login": "bazinac", "id": 19685528, "node_id": "MDQ6VXNlcjE5Njg1NTI4", "avatar_url": "https://avatars0.githubusercontent.com/u/19685528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bazinac", "html_url": "https://github.com/bazinac", "followers_url": "https://api.github.com/users/bazinac/followers", "following_url": "https://api.github.com/users/bazinac/following{/other_user}", "gists_url": "https://api.github.com/users/bazinac/gists{/gist_id}", "starred_url": "https://api.github.com/users/bazinac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bazinac/subscriptions", "organizations_url": "https://api.github.com/users/bazinac/orgs", "repos_url": "https://api.github.com/users/bazinac/repos", "events_url": "https://api.github.com/users/bazinac/events{/privacy}", "received_events_url": "https://api.github.com/users/bazinac/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-16T19:45:49Z", "updated_at": "2017-02-16T19:47:54Z", "author_association": "NONE", "body_html": "<p>Update here, I have found out that quantization does not just corrupts my second output layer, but messes with first one as well. It seems that after quantization, classifing tends to favor just one category no matter what you feed it. Therefore I think something is definititelly wrong with quantize_weights transform (as user <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25055499\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Bruczzz\">@Bruczzz</a> in issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"207769085\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7523\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7523/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7523\">#7523</a> is hitting exactly same problem). Now when he is having the same problem, then I am pretty sure there is some bug in that transformation, please <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a>  and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a> help us :).</p>\n<p>I have retrained clean inception V3 graph on few categories and this is pattern of classifing, you can see that after quantization is really obsessed with T category.:</p>\n<p>`<strong>Sample image 1 (R)</strong><br>\n<em>unquantized results:</em><br>\nR (score = 0.40234)<br>\nP (score = 0.19375)<br>\nB (score = 0.19081)<br>\nC (score = 0.06899)<br>\nF (score = 0.06185)</p>\n<p><em>quantized results:</em><br>\nT (score = 0.97295)<br>\nL (score = 0.00936)<br>\nH (score = 0.00766)<br>\nG (score = 0.00272)<br>\nS (score = 0.00128)</p>\n<p><strong>Sample image 2 (is H)</strong><br>\n<em>unquantized results:</em><br>\nH (score = 0.99536)<br>\nD (score = 0.00163)<br>\nE (score = 0.00086)<br>\nC (score = 0.00051)<br>\nU (score = 0.00024)</p>\n<p><em>quantized results:</em><br>\nT (score = 0.76606)<br>\nH (score = 0.10995)<br>\nS (score = 0.09073)<br>\nD (score = 0.01019)<br>\nF (score = 0.00469)</p>\n<p><strong>Sample image 3 ( is A)</strong><br>\n<em>unquantized results:</em><br>\nA (score = 0.91210)<br>\nX (score = 0.07343)<br>\nW (score = 0.00629)<br>\nG (score = 0.00273)<br>\nS (score = 0.00208)</p>\n<p><em>quantized results:</em><br>\nT (score = 0.69732)<br>\nG (score = 0.07490)<br>\nF (score = 0.04135)<br>\nD (score = 0.03919)<br>\nH (score = 0.02643)`</p>", "body_text": "Update here, I have found out that quantization does not just corrupts my second output layer, but messes with first one as well. It seems that after quantization, classifing tends to favor just one category no matter what you feed it. Therefore I think something is definititelly wrong with quantize_weights transform (as user @Bruczzz in issue #7523 is hitting exactly same problem). Now when he is having the same problem, then I am pretty sure there is some bug in that transformation, please @andrewharp  and @petewarden help us :).\nI have retrained clean inception V3 graph on few categories and this is pattern of classifing, you can see that after quantization is really obsessed with T category.:\n`Sample image 1 (R)\nunquantized results:\nR (score = 0.40234)\nP (score = 0.19375)\nB (score = 0.19081)\nC (score = 0.06899)\nF (score = 0.06185)\nquantized results:\nT (score = 0.97295)\nL (score = 0.00936)\nH (score = 0.00766)\nG (score = 0.00272)\nS (score = 0.00128)\nSample image 2 (is H)\nunquantized results:\nH (score = 0.99536)\nD (score = 0.00163)\nE (score = 0.00086)\nC (score = 0.00051)\nU (score = 0.00024)\nquantized results:\nT (score = 0.76606)\nH (score = 0.10995)\nS (score = 0.09073)\nD (score = 0.01019)\nF (score = 0.00469)\nSample image 3 ( is A)\nunquantized results:\nA (score = 0.91210)\nX (score = 0.07343)\nW (score = 0.00629)\nG (score = 0.00273)\nS (score = 0.00208)\nquantized results:\nT (score = 0.69732)\nG (score = 0.07490)\nF (score = 0.04135)\nD (score = 0.03919)\nH (score = 0.02643)`", "body": "Update here, I have found out that quantization does not just corrupts my second output layer, but messes with first one as well. It seems that after quantization, classifing tends to favor just one category no matter what you feed it. Therefore I think something is definititelly wrong with quantize_weights transform (as user @Bruczzz in issue #7523 is hitting exactly same problem). Now when he is having the same problem, then I am pretty sure there is some bug in that transformation, please @andrewharp  and @petewarden help us :).\r\n\r\nI have retrained clean inception V3 graph on few categories and this is pattern of classifing, you can see that after quantization is really obsessed with T category.:\r\n\r\n`**Sample image 1 (R)**\r\n_unquantized results:_\r\nR (score = 0.40234)\r\nP (score = 0.19375)\r\nB (score = 0.19081)\r\nC (score = 0.06899)\r\nF (score = 0.06185)\r\n\r\n_quantized results:_\r\nT (score = 0.97295)\r\nL (score = 0.00936)\r\nH (score = 0.00766)\r\nG (score = 0.00272)\r\nS (score = 0.00128)\r\n\r\n**Sample image 2 (is H)**\r\n_unquantized results:_\r\nH (score = 0.99536)\r\nD (score = 0.00163)\r\nE (score = 0.00086)\r\nC (score = 0.00051)\r\nU (score = 0.00024)\r\n\r\n_quantized results:_\r\nT (score = 0.76606)\r\nH (score = 0.10995)\r\nS (score = 0.09073)\r\nD (score = 0.01019)\r\nF (score = 0.00469)\r\n\r\n**Sample image 3 ( is A)**\r\n_unquantized results:_\r\nA (score = 0.91210)\r\nX (score = 0.07343)\r\nW (score = 0.00629)\r\nG (score = 0.00273)\r\nS (score = 0.00208)\r\n\r\n_quantized results:_\r\nT (score = 0.69732)\r\nG (score = 0.07490)\r\nF (score = 0.04135)\r\nD (score = 0.03919)\r\nH (score = 0.02643)` "}