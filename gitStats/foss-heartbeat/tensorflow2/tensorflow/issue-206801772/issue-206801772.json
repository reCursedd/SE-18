{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7415", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7415/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7415/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7415/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7415", "id": 206801772, "node_id": "MDU6SXNzdWUyMDY4MDE3NzI=", "number": 7415, "title": "Scale-out performance limitation for distributed session", "user": {"login": "samwhitlock", "id": 433170, "node_id": "MDQ6VXNlcjQzMzE3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/433170?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samwhitlock", "html_url": "https://github.com/samwhitlock", "followers_url": "https://api.github.com/users/samwhitlock/followers", "following_url": "https://api.github.com/users/samwhitlock/following{/other_user}", "gists_url": "https://api.github.com/users/samwhitlock/gists{/gist_id}", "starred_url": "https://api.github.com/users/samwhitlock/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samwhitlock/subscriptions", "organizations_url": "https://api.github.com/users/samwhitlock/orgs", "repos_url": "https://api.github.com/users/samwhitlock/repos", "events_url": "https://api.github.com/users/samwhitlock/events{/privacy}", "received_events_url": "https://api.github.com/users/samwhitlock/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-10T13:55:37Z", "updated_at": "2017-02-14T21:08:42Z", "closed_at": "2017-02-10T16:31:58Z", "author_association": "NONE", "body_html": "<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/433170/22829003/8b585f4e-efa0-11e6-9ddc-34a6b7e65cfb.png\"><img src=\"https://cloud.githubusercontent.com/assets/433170/22829003/8b585f4e-efa0-11e6-9ddc-34a6b7e65cfb.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>Scale-out performance for scatter-gather dataflow pipelines is limited.</p>\n<p>I use Tensorflow to build custom pipelines (i.e. I write my own OpKernels) of \"embarrassingly parallel\" problems (no coordination between \"local\" pipelines required). Typically these involve the <em>local</em> pipeline being replicated across all the machines in my cluster, and having a source and sink queue to feed input and receive output, respectively.</p>\n<p>The issue: the performance degrades linearly with the number of nodes in the cluster, compared to a perfect scale-out performance.</p>\n<p>I am not sure if this is a fundamental limitation of the distributed session, or if there is a way within Tensorflow to build such graphs better (with less coordination needed between independent <em>local</em> pipelines).</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>Not much guidance. When speaking with other folks at conferences, it seems common to sidestep the distributed session in Tensorflow for custom, non-TF solutions (e.g. MPI). I personally use ZeroMQ to facilitate these sort of scatter-gather patterns.</p>\n<h3>Environment info</h3>\n<ul>\n<li>Operating System: Linux (Ubuntu 16.04, 4.4.0-22)</li>\n<li>installed from <code>pip</code> following the default <a href=\"https://www.tensorflow.org/get_started/os_setup\" rel=\"nofollow\">download and setup instructions</a></li>\n<li>output of <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code> -&gt; <code>0.12.1</code></li>\n</ul>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<ul>\n<li><a href=\"https://gist.github.com/samwhitlock/3b099f8489909a3293b3e4239a7188d0\">here is a gist with vanilla tensorflow</a>, with the versions I specified above. My local pipelines that I stamp out across all nodes in my cluster has custom ops, which I replicated in standard Tensorflow with a while loop to imitate the delay of my custom pipeline. I'm not sure if there is a better way to simulate the delay.</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1Ht8O9OT4csfYVOxSDybojXnZ-_uV2A60CF9Jt0jhps0/edit\" rel=\"nofollow\">here is the data I collected on my cluster of 10 machines (only up to 9 to keep the source/sink queue machine unloaded)</a>. Note that the workload scales up linearly with <em>only</em> changes with the number of nodes; if the scale-out was perfect, the time should be the same regardless of the number of nodes.</li>\n</ul>\n<h3>What other attempted solutions have you tried?</h3>\n<ul>\n<li>I get around this by limitation by using ZeroMQ as a higher-performance substitute for the source/sink queues (the equivalent of <code>\"source_queue\"</code> and <code>\"sink_queue\"</code> in the example script)</li>\n</ul>", "body_text": "Scale-out performance for scatter-gather dataflow pipelines is limited.\nI use Tensorflow to build custom pipelines (i.e. I write my own OpKernels) of \"embarrassingly parallel\" problems (no coordination between \"local\" pipelines required). Typically these involve the local pipeline being replicated across all the machines in my cluster, and having a source and sink queue to feed input and receive output, respectively.\nThe issue: the performance degrades linearly with the number of nodes in the cluster, compared to a perfect scale-out performance.\nI am not sure if this is a fundamental limitation of the distributed session, or if there is a way within Tensorflow to build such graphs better (with less coordination needed between independent local pipelines).\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNot much guidance. When speaking with other folks at conferences, it seems common to sidestep the distributed session in Tensorflow for custom, non-TF solutions (e.g. MPI). I personally use ZeroMQ to facilitate these sort of scatter-gather patterns.\nEnvironment info\n\nOperating System: Linux (Ubuntu 16.04, 4.4.0-22)\ninstalled from pip following the default download and setup instructions\noutput of python -c \"import tensorflow; print(tensorflow.__version__)\" -> 0.12.1\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nhere is a gist with vanilla tensorflow, with the versions I specified above. My local pipelines that I stamp out across all nodes in my cluster has custom ops, which I replicated in standard Tensorflow with a while loop to imitate the delay of my custom pipeline. I'm not sure if there is a better way to simulate the delay.\nhere is the data I collected on my cluster of 10 machines (only up to 9 to keep the source/sink queue machine unloaded). Note that the workload scales up linearly with only changes with the number of nodes; if the scale-out was perfect, the time should be the same regardless of the number of nodes.\n\nWhat other attempted solutions have you tried?\n\nI get around this by limitation by using ZeroMQ as a higher-performance substitute for the source/sink queues (the equivalent of \"source_queue\" and \"sink_queue\" in the example script)", "body": "\r\n![image](https://cloud.githubusercontent.com/assets/433170/22829003/8b585f4e-efa0-11e6-9ddc-34a6b7e65cfb.png)\r\n\r\nScale-out performance for scatter-gather dataflow pipelines is limited.\r\n\r\nI use Tensorflow to build custom pipelines (i.e. I write my own OpKernels) of \"embarrassingly parallel\" problems (no coordination between \"local\" pipelines required). Typically these involve the _local_ pipeline being replicated across all the machines in my cluster, and having a source and sink queue to feed input and receive output, respectively.\r\n\r\nThe issue: the performance degrades linearly with the number of nodes in the cluster, compared to a perfect scale-out performance.\r\n\r\nI am not sure if this is a fundamental limitation of the distributed session, or if there is a way within Tensorflow to build such graphs better (with less coordination needed between independent _local_ pipelines).\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNot much guidance. When speaking with other folks at conferences, it seems common to sidestep the distributed session in Tensorflow for custom, non-TF solutions (e.g. MPI). I personally use ZeroMQ to facilitate these sort of scatter-gather patterns.\r\n\r\n### Environment info\r\n\r\n* Operating System: Linux (Ubuntu 16.04, 4.4.0-22)\r\n* installed from `pip` following the default [download and setup instructions](https://www.tensorflow.org/get_started/os_setup)\r\n* output of `python -c \"import tensorflow; print(tensorflow.__version__)\"` -> `0.12.1`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n* [here is a gist with vanilla tensorflow](https://gist.github.com/samwhitlock/3b099f8489909a3293b3e4239a7188d0), with the versions I specified above. My local pipelines that I stamp out across all nodes in my cluster has custom ops, which I replicated in standard Tensorflow with a while loop to imitate the delay of my custom pipeline. I'm not sure if there is a better way to simulate the delay.\r\n* [here is the data I collected on my cluster of 10 machines (only up to 9 to keep the source/sink queue machine unloaded)](https://docs.google.com/spreadsheets/d/1Ht8O9OT4csfYVOxSDybojXnZ-_uV2A60CF9Jt0jhps0/edit). Note that the workload scales up linearly with _only_ changes with the number of nodes; if the scale-out was perfect, the time should be the same regardless of the number of nodes.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n* I get around this by limitation by using ZeroMQ as a higher-performance substitute for the source/sink queues (the equivalent of `\"source_queue\"` and `\"sink_queue\"` in the example script)"}