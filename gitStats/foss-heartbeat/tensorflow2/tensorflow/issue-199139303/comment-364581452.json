{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364581452", "html_url": "https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-364581452", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6683", "id": 364581452, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDU4MTQ1Mg==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T21:59:40Z", "updated_at": "2018-02-09T22:03:24Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13804600\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pickou\">@pickou</a>  : As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=32960135\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/NLPpupil\">@NLPpupil</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2807595\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/flrngel\">@flrngel</a> point out, I don't believe there is a problem with the TensorFlow checkpointing here, but rather it's the use of <code>set</code> for the generating the vocabulary in your program that is the underlying cause of the problem.</p>\n<p>Specifically, between <code>python train_gentext.py</code> and <code>python load_gentext.py</code>, the vocabulary changes because each time <a href=\"https://github.com/pickou/multiplicative-lstm/blob/5c5c1ffceb34b541d7dbfe96f6813abdb49086ad/mlstm.py#L31\"><code>preprocess()</code></a> is invoked, it will return a different mapping from letters to integers due to its use of <code>set</code>, hence you see messed up results.</p>\n<p>I quickly verified this by adding a:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">print</span>(int_to_vocab)</pre></div>\n<p>in <code>preprocess()</code> and observed different mappings when invoking <code>train_gentext.py</code> and <code>load_gentext.py</code>.</p>\n<p>Furthermore, I verified that the variables are showing no corruption by printing the hashes of the variable contents before saving and after restoring using something like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">print_var_hashes</span>(<span class=\"pl-smi\">sess</span>):\n  <span class=\"pl-k\">import</span> hashlib\n  <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">TRAINABLE_VARIABLES</span>):                                                                              \n    val <span class=\"pl-k\">=</span> sess.run(v.value())\n    h <span class=\"pl-k\">=</span> hashlib.md5()                                                                                                                        \n    h.update(val.data)                                                                                                                       \n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%-50s</span> <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (v.name, h.hexdigest()))</pre></div>\n<p>So I think <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13804600\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pickou\">@pickou</a> 's example isn't quite right.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15844017\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xisnu\">@xisnu</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1078598\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/igorbb\">@igorbb</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8042963\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bartgras\">@bartgras</a>  : Are you seeing this after moving to TF 1.0+? Could you provide a means to reproduce the problem?</p>", "body_text": "@pickou  : As @NLPpupil and @flrngel point out, I don't believe there is a problem with the TensorFlow checkpointing here, but rather it's the use of set for the generating the vocabulary in your program that is the underlying cause of the problem.\nSpecifically, between python train_gentext.py and python load_gentext.py, the vocabulary changes because each time preprocess() is invoked, it will return a different mapping from letters to integers due to its use of set, hence you see messed up results.\nI quickly verified this by adding a:\nprint(int_to_vocab)\nin preprocess() and observed different mappings when invoking train_gentext.py and load_gentext.py.\nFurthermore, I verified that the variables are showing no corruption by printing the hashes of the variable contents before saving and after restoring using something like this:\ndef print_var_hashes(sess):\n  import hashlib\n  for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):                                                                              \n    val = sess.run(v.value())\n    h = hashlib.md5()                                                                                                                        \n    h.update(val.data)                                                                                                                       \n    print('%-50s %s' % (v.name, h.hexdigest()))\nSo I think @pickou 's example isn't quite right.\n@xisnu @igorbb @bartgras  : Are you seeing this after moving to TF 1.0+? Could you provide a means to reproduce the problem?", "body": "@pickou  : As @NLPpupil and @flrngel point out, I don't believe there is a problem with the TensorFlow checkpointing here, but rather it's the use of `set` for the generating the vocabulary in your program that is the underlying cause of the problem.\r\n\r\nSpecifically, between `python train_gentext.py` and `python load_gentext.py`, the vocabulary changes because each time [`preprocess()`](https://github.com/pickou/multiplicative-lstm/blob/5c5c1ffceb34b541d7dbfe96f6813abdb49086ad/mlstm.py#L31) is invoked, it will return a different mapping from letters to integers due to its use of `set`, hence you see messed up results.\r\n\r\nI quickly verified this by adding a:\r\n```python\r\nprint(int_to_vocab)\r\n```\r\nin `preprocess()` and observed different mappings when invoking `train_gentext.py` and `load_gentext.py`.\r\n\r\nFurthermore, I verified that the variables are showing no corruption by printing the hashes of the variable contents before saving and after restoring using something like this:\r\n\r\n```python\r\ndef print_var_hashes(sess):\r\n  import hashlib\r\n  for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):                                                                              \r\n    val = sess.run(v.value())\r\n    h = hashlib.md5()                                                                                                                        \r\n    h.update(val.data)                                                                                                                       \r\n    print('%-50s %s' % (v.name, h.hexdigest()))\r\n```\r\n\r\nSo I think @pickou 's example isn't quite right.\r\n\r\n@xisnu @igorbb @bartgras  : Are you seeing this after moving to TF 1.0+? Could you provide a means to reproduce the problem?"}