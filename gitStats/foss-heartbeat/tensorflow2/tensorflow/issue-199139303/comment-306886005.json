{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/306886005", "html_url": "https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-306886005", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6683", "id": 306886005, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjg4NjAwNQ==", "user": {"login": "igorbb", "id": 1078598, "node_id": "MDQ6VXNlcjEwNzg1OTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1078598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/igorbb", "html_url": "https://github.com/igorbb", "followers_url": "https://api.github.com/users/igorbb/followers", "following_url": "https://api.github.com/users/igorbb/following{/other_user}", "gists_url": "https://api.github.com/users/igorbb/gists{/gist_id}", "starred_url": "https://api.github.com/users/igorbb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/igorbb/subscriptions", "organizations_url": "https://api.github.com/users/igorbb/orgs", "repos_url": "https://api.github.com/users/igorbb/repos", "events_url": "https://api.github.com/users/igorbb/events{/privacy}", "received_events_url": "https://api.github.com/users/igorbb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-07T18:35:30Z", "updated_at": "2017-06-08T08:20:04Z", "author_association": "NONE", "body_html": "<p>I also can confirm the issue:<br>\nWhenever we are using the</p>\n<pre><code>    rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\n    rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\n</code></pre>\n<p>The tf.train.Saver or alternative does not save the weights of the LSTM cells properlly.<br>\nThis also seems to be the case of using</p>\n<pre><code>    #Base cell\n    base_lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) \n\n    # Add dropout to the cell\n    drop = tf.contrib.rnn.DropoutWrapper(base_lstm, output_keep_prob = keep_prob)\n\n    # Stack up multiple LSTM layers, for deep learning\n    lstms_cells = tf.contrib.rnn.MultiRNNCell([drop]*2)\n\n    # Getting an initial state of all zeros\n    initial_state = lstms_cells.zero_state(batch_size, tf.float32)\n    initial_state = tf.identity(initial_state, name=\"initial_state\")\n\n        \n    rnn_out, final_state = tf.nn.dynamic_rnn(lstms_cells, inputs, dtype=tf.float32)\n</code></pre>\n<p>So right now it seems we have no reliable way to save and load LSTM based on <code>tf.contrib.rnn.BasicLSTMCell</code></p>", "body_text": "I also can confirm the issue:\nWhenever we are using the\n    rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\n    rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\n\nThe tf.train.Saver or alternative does not save the weights of the LSTM cells properlly.\nThis also seems to be the case of using\n    #Base cell\n    base_lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) \n\n    # Add dropout to the cell\n    drop = tf.contrib.rnn.DropoutWrapper(base_lstm, output_keep_prob = keep_prob)\n\n    # Stack up multiple LSTM layers, for deep learning\n    lstms_cells = tf.contrib.rnn.MultiRNNCell([drop]*2)\n\n    # Getting an initial state of all zeros\n    initial_state = lstms_cells.zero_state(batch_size, tf.float32)\n    initial_state = tf.identity(initial_state, name=\"initial_state\")\n\n        \n    rnn_out, final_state = tf.nn.dynamic_rnn(lstms_cells, inputs, dtype=tf.float32)\n\nSo right now it seems we have no reliable way to save and load LSTM based on tf.contrib.rnn.BasicLSTMCell", "body": "I also can confirm the issue:\r\nWhenever we are using the \r\n\r\n        rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\r\n        rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\r\n    \r\nThe tf.train.Saver or alternative does not save the weights of the LSTM cells properlly.\r\nThis also seems to be the case of using\r\n\r\n        #Base cell\r\n        base_lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) \r\n\r\n        # Add dropout to the cell\r\n        drop = tf.contrib.rnn.DropoutWrapper(base_lstm, output_keep_prob = keep_prob)\r\n\r\n        # Stack up multiple LSTM layers, for deep learning\r\n        lstms_cells = tf.contrib.rnn.MultiRNNCell([drop]*2)\r\n\r\n        # Getting an initial state of all zeros\r\n        initial_state = lstms_cells.zero_state(batch_size, tf.float32)\r\n        initial_state = tf.identity(initial_state, name=\"initial_state\")\r\n\r\n            \r\n        rnn_out, final_state = tf.nn.dynamic_rnn(lstms_cells, inputs, dtype=tf.float32)\r\n\r\n\r\nSo right now it seems we have no reliable way to save and load LSTM based on `tf.contrib.rnn.BasicLSTMCell`"}