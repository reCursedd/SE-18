{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271214166", "html_url": "https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-271214166", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6683", "id": 271214166, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTIxNDE2Ng==", "user": {"login": "xisnu", "id": 15844017, "node_id": "MDQ6VXNlcjE1ODQ0MDE3", "avatar_url": "https://avatars2.githubusercontent.com/u/15844017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xisnu", "html_url": "https://github.com/xisnu", "followers_url": "https://api.github.com/users/xisnu/followers", "following_url": "https://api.github.com/users/xisnu/following{/other_user}", "gists_url": "https://api.github.com/users/xisnu/gists{/gist_id}", "starred_url": "https://api.github.com/users/xisnu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xisnu/subscriptions", "organizations_url": "https://api.github.com/users/xisnu/orgs", "repos_url": "https://api.github.com/users/xisnu/repos", "events_url": "https://api.github.com/users/xisnu/events{/privacy}", "received_events_url": "https://api.github.com/users/xisnu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-09T05:39:24Z", "updated_at": "2017-01-09T05:39:24Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5376757\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/michaelisard\">@michaelisard</a><br>\nI am extremely sorry for my delayed response. My input is taken from a H5 file which contains features extracted from online handwriting data sample. at every time step I have 16 features. Every time I read from this file the order of data is shuffled.<br>\nHere is the part where I am creating the graph.</p>\n<pre><code>graph=tf.Graph()\nwith graph.as_default():\n    print(\"Graph Creation\")\n    x=tf.placeholder(tf.float32,[None,ms,nb_features],name=\"x\")\n\n    y=tf.sparse_placeholder(tf.int32,name=\"y\")\n    seq_len=tf.placeholder(tf.int32,[None],name=\"seq_len\")\n    \n\n    f_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\n    f_stack=tf.nn.rnn_cell.MultiRNNCell([f_cell]*nb_layers)\n    \n    b_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\n    b_stack=tf.nn.rnn_cell.MultiRNNCell([b_cell]*nb_layers)\n    \n    outputs,_=tf.nn.bidirectional_dynamic_rnn(f_stack,b_stack,x,sequence_length=seq_len,dtype=tf.float32)\n\n    merge=tf.concat(2, outputs,name=\"merge\")\n\n    shape = tf.shape(x)\n    batch_s,maxtimesteps=shape[0],shape[1]\n\n    output_reshape = tf.reshape(merge, [-1, nb_hidden*2])#batch*timesteps,nb_hidden\n   \n    W = tf.Variable(tf.truncated_normal([nb_hidden*2,nb_classes],stddev=0.1),name=\"W1\")\n\n    b = tf.Variable(tf.constant(0., shape=[nb_classes]),name=\"b1\")\n\n    logits = tf.add(tf.matmul(output_reshape, W) , b,name=\"logits\") #818622,52\n      \n    logits_reshape = tf.transpose(tf.reshape(logits, [batch_s, -1, nb_classes]),[1,0,2],name=\"logits_reshape\")#534,1533,52\n\n    loss =tf.nn.ctc_loss(logits_reshape, y, seq_len,time_major=True)\n    cost = tf.reduce_mean(loss,name=\"cost\")\n\n    optimizer = tf.train.RMSPropOptimizer(lr).minimize(cost)\n\n    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits_reshape, seq_len)\n\n    actual_ed=tf.edit_distance(tf.cast(decoded[0], tf.int32),y,normalize=False)\n    ler = tf.reduce_sum(actual_ed,name=\"ler\")\n    saver=tf.train.Saver()\n    bestsaver=tf.train.Saver()\n    print(\"Network Ready\")\n</code></pre>\n<p>Just after this, I am running the training and saving it</p>\n<pre><code>with tf.Session(graph=graph,config=tf.ConfigProto(log_device_placement=False)) as session:    \n    #saver = tf.train.Saver()\n    if(sys.argv[1]==\"load\"):\n        saver.restore(session, \"Weights/model_last\")\n        print(\"Previous weights loaded\")\n    else:\n        init_op = tf.global_variables_initializer()\n        session.run(init_op)\n        print(\"New Weights Initialized\")\n    \n    best=0\n    acctest=0\n    bestloss=10000\n    #testfeed = {x:test_inpx[0],y:test_inp_sparse_y[0],seq_len:test_inpseqlen[0]}\n    testcases=[0,5,17,39,60]\n    true=[]\n    for tr in range(len(testcases)):\n        true1=label_from_sparse(test_inp_sparse_y[0],testcases[tr])\n        true.append(true1)    \n    print(\"Actual \",true)\n    for e in range(nb_epochs):\n        f=open(logfilename,\"a\")\n        totalloss=0\n        totalacc=0\n        starttime=time.time()\n\n        for b in range(trainbatch):\n            p=b+1\n            print(\"Reading Batch \",p,\"/\",trainbatch,end=\"\\r\")\n            feed = {x:inpx[b],y:inp_sparse_y[b],seq_len:inpseqlen[b]}\n            batchloss,batchacc, _ = session.run([cost,ler,optimizer], feed)\n            \n            totalloss=totalloss+batchloss\n            totalacc=totalacc+batchacc\n        avgloss=totalloss/trainbatch\n        avgacc=1-(totalacc/nctr)\n        if(avgloss&lt;bestloss):\n            bestloss=avgloss\n            print(\"Network Improvement\")\n            saver.save(session, \"Weights/model_last\")\n        \n        testloss=0\n        testacc=0\n        for t in range(testbatch):\n            testfeed = {x:test_inpx[t],y:test_inp_sparse_y[t],seq_len:test_inpseqlen[t]}\n            outcome,testbatchloss,testbatchacc=session.run([decoded[0],cost,ler],testfeed)\n            if(t==0):\n                first_batch_outcome=outcome\n            testloss=testloss+testbatchloss\n            testacc=testacc+testbatchacc\n        \n        testfile=open(\"Results.txt\",\"w\")\n        testfile.write(\"Epoch \"+str(e)+\"\\n\")\n        for tc in range(len(testcases)):\n            predicted=label_from_sparse(first_batch_outcome,testcases[tc])\n            testfile.write(str(true[tc])+\" As \"+str(predicted)+\"\\n\")\n        testfile.close()\n        \n        testloss=testloss/testbatch\n        testacc=1-(testacc/ncts)\n        \n        endtime=time.time()        \n        if(testacc&gt;best):\n            best=testacc\n            print(\"Test Result Improvement\")\n            bestsaver.save(session, \"BestWeights/model_best\")\n        timetaken=endtime-starttime\n        msg=\"Epoch \"+str(e)+\"(\"+str(timetaken)+ \" sec ) Training: Cost is \"+str(avgloss)+\" Accuracy \"+str(avgacc)+\" Testing: Loss \"+str(testloss)+\" Accuracy \"+str(testacc)+\"\\n\"\n        print(msg)\n        f.write(msg)\n        f.close()\n</code></pre>\n<p>Now, whenever I am loading the model from last save or best save, It is not showing any sign of previous training. Seems to be starting from some scratch. I also tried <code>import_meta_graph()</code> without any success. But the same strategy is working absolutely fine with a RNN model which is tested against the well known IRIS data set (hence a classification problem).</p>\n<pre><code>mygraph=tf.Graph()\n\nwith mygraph.as_default():\n    x=tf.placeholder(tf.float32,[None,4,1])\n    y=tf.placeholder(tf.float32,[None,3])\n    \n    rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\n    rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\n    \n    shape=tf.shape(rnnop)\n    batch=shape[0]\n    op=shape[1]*shape[2]\n    rnnrs=tf.reshape(rnnop,[batch,op],name=\"rnnrs\")\n    \n    w1=tf.Variable(tf.truncated_normal([12,5]),name=\"w1\")\n    b1=tf.Variable(tf.truncated_normal([5]),name=\"b1\")\n    \n    layer1=tf.add(tf.matmul(rnnrs,w1),b1)\n    layer1_op=tf.nn.tanh(layer1)\n    \n    w2=tf.Variable(tf.truncated_normal([5,6]),name=\"w2\")\n    b2=tf.Variable(tf.truncated_normal([6]),name=\"b2\")\n    \n    layer2=tf.add(tf.matmul(layer1_op,w2),b2)\n    layer2_op=tf.nn.tanh(layer2)\n    \n    w3=tf.Variable(tf.truncated_normal([6,3]),name=\"w3\")\n    b3=tf.Variable(tf.truncated_normal([3]),name=\"b3\")\n    \n    layer3=tf.add(tf.matmul(layer2_op,w3),b3)\n    prediction=layer3\n    \n    correct=tf.equal(tf.arg_max(prediction,1),tf.arg_max(y,1))\n    acc=tf.reduce_mean(tf.cast(correct,tf.float32))\n    #loss=tf.reduce_mean(tf.reduce_sum(tf.square(tf.sub(y,prediction))))\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))\n    optimizer=tf.train.MomentumOptimizer(0.01,0.9).minimize(loss)\n    saver=tf.train.Saver()\n    print(\"Network Ready\")\n\nwith tf.Session(graph=mygraph) as session:\n    feedx,feedy=loadiris(\"/media/parthosarothi/OHWR/Dataset/iris.csv\")\n    print(\"Data 0 x=\",feedx[0],\" y=\",feedy[0])\n    feed={x:feedx,y:feedy}\n    if(sys.argv[1]==\"load\"):\n        saver.restore(session,\"Weights/last\")\n        print(\"Previous Weights Loaded\")\n    else:\n        initop=tf.global_variables_initializer()\n        session.run(initop)\n        print(\"New Weights Loaded\")\n    for e in range(100):\n        l,_,p,a=session.run([loss,optimizer,prediction,acc],feed)\n        print(\"Loss is \",l,\" P \",p[0],\" y \",feedy[0],\" A \",a)\n        saver.save(session,\"Weights/last\")\n</code></pre>\n<p>I am completely in dark. Your concern is highly appreciated.</p>", "body_text": "@michaelisard\nI am extremely sorry for my delayed response. My input is taken from a H5 file which contains features extracted from online handwriting data sample. at every time step I have 16 features. Every time I read from this file the order of data is shuffled.\nHere is the part where I am creating the graph.\ngraph=tf.Graph()\nwith graph.as_default():\n    print(\"Graph Creation\")\n    x=tf.placeholder(tf.float32,[None,ms,nb_features],name=\"x\")\n\n    y=tf.sparse_placeholder(tf.int32,name=\"y\")\n    seq_len=tf.placeholder(tf.int32,[None],name=\"seq_len\")\n    \n\n    f_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\n    f_stack=tf.nn.rnn_cell.MultiRNNCell([f_cell]*nb_layers)\n    \n    b_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\n    b_stack=tf.nn.rnn_cell.MultiRNNCell([b_cell]*nb_layers)\n    \n    outputs,_=tf.nn.bidirectional_dynamic_rnn(f_stack,b_stack,x,sequence_length=seq_len,dtype=tf.float32)\n\n    merge=tf.concat(2, outputs,name=\"merge\")\n\n    shape = tf.shape(x)\n    batch_s,maxtimesteps=shape[0],shape[1]\n\n    output_reshape = tf.reshape(merge, [-1, nb_hidden*2])#batch*timesteps,nb_hidden\n   \n    W = tf.Variable(tf.truncated_normal([nb_hidden*2,nb_classes],stddev=0.1),name=\"W1\")\n\n    b = tf.Variable(tf.constant(0., shape=[nb_classes]),name=\"b1\")\n\n    logits = tf.add(tf.matmul(output_reshape, W) , b,name=\"logits\") #818622,52\n      \n    logits_reshape = tf.transpose(tf.reshape(logits, [batch_s, -1, nb_classes]),[1,0,2],name=\"logits_reshape\")#534,1533,52\n\n    loss =tf.nn.ctc_loss(logits_reshape, y, seq_len,time_major=True)\n    cost = tf.reduce_mean(loss,name=\"cost\")\n\n    optimizer = tf.train.RMSPropOptimizer(lr).minimize(cost)\n\n    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits_reshape, seq_len)\n\n    actual_ed=tf.edit_distance(tf.cast(decoded[0], tf.int32),y,normalize=False)\n    ler = tf.reduce_sum(actual_ed,name=\"ler\")\n    saver=tf.train.Saver()\n    bestsaver=tf.train.Saver()\n    print(\"Network Ready\")\n\nJust after this, I am running the training and saving it\nwith tf.Session(graph=graph,config=tf.ConfigProto(log_device_placement=False)) as session:    \n    #saver = tf.train.Saver()\n    if(sys.argv[1]==\"load\"):\n        saver.restore(session, \"Weights/model_last\")\n        print(\"Previous weights loaded\")\n    else:\n        init_op = tf.global_variables_initializer()\n        session.run(init_op)\n        print(\"New Weights Initialized\")\n    \n    best=0\n    acctest=0\n    bestloss=10000\n    #testfeed = {x:test_inpx[0],y:test_inp_sparse_y[0],seq_len:test_inpseqlen[0]}\n    testcases=[0,5,17,39,60]\n    true=[]\n    for tr in range(len(testcases)):\n        true1=label_from_sparse(test_inp_sparse_y[0],testcases[tr])\n        true.append(true1)    \n    print(\"Actual \",true)\n    for e in range(nb_epochs):\n        f=open(logfilename,\"a\")\n        totalloss=0\n        totalacc=0\n        starttime=time.time()\n\n        for b in range(trainbatch):\n            p=b+1\n            print(\"Reading Batch \",p,\"/\",trainbatch,end=\"\\r\")\n            feed = {x:inpx[b],y:inp_sparse_y[b],seq_len:inpseqlen[b]}\n            batchloss,batchacc, _ = session.run([cost,ler,optimizer], feed)\n            \n            totalloss=totalloss+batchloss\n            totalacc=totalacc+batchacc\n        avgloss=totalloss/trainbatch\n        avgacc=1-(totalacc/nctr)\n        if(avgloss<bestloss):\n            bestloss=avgloss\n            print(\"Network Improvement\")\n            saver.save(session, \"Weights/model_last\")\n        \n        testloss=0\n        testacc=0\n        for t in range(testbatch):\n            testfeed = {x:test_inpx[t],y:test_inp_sparse_y[t],seq_len:test_inpseqlen[t]}\n            outcome,testbatchloss,testbatchacc=session.run([decoded[0],cost,ler],testfeed)\n            if(t==0):\n                first_batch_outcome=outcome\n            testloss=testloss+testbatchloss\n            testacc=testacc+testbatchacc\n        \n        testfile=open(\"Results.txt\",\"w\")\n        testfile.write(\"Epoch \"+str(e)+\"\\n\")\n        for tc in range(len(testcases)):\n            predicted=label_from_sparse(first_batch_outcome,testcases[tc])\n            testfile.write(str(true[tc])+\" As \"+str(predicted)+\"\\n\")\n        testfile.close()\n        \n        testloss=testloss/testbatch\n        testacc=1-(testacc/ncts)\n        \n        endtime=time.time()        \n        if(testacc>best):\n            best=testacc\n            print(\"Test Result Improvement\")\n            bestsaver.save(session, \"BestWeights/model_best\")\n        timetaken=endtime-starttime\n        msg=\"Epoch \"+str(e)+\"(\"+str(timetaken)+ \" sec ) Training: Cost is \"+str(avgloss)+\" Accuracy \"+str(avgacc)+\" Testing: Loss \"+str(testloss)+\" Accuracy \"+str(testacc)+\"\\n\"\n        print(msg)\n        f.write(msg)\n        f.close()\n\nNow, whenever I am loading the model from last save or best save, It is not showing any sign of previous training. Seems to be starting from some scratch. I also tried import_meta_graph() without any success. But the same strategy is working absolutely fine with a RNN model which is tested against the well known IRIS data set (hence a classification problem).\nmygraph=tf.Graph()\n\nwith mygraph.as_default():\n    x=tf.placeholder(tf.float32,[None,4,1])\n    y=tf.placeholder(tf.float32,[None,3])\n    \n    rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\n    rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\n    \n    shape=tf.shape(rnnop)\n    batch=shape[0]\n    op=shape[1]*shape[2]\n    rnnrs=tf.reshape(rnnop,[batch,op],name=\"rnnrs\")\n    \n    w1=tf.Variable(tf.truncated_normal([12,5]),name=\"w1\")\n    b1=tf.Variable(tf.truncated_normal([5]),name=\"b1\")\n    \n    layer1=tf.add(tf.matmul(rnnrs,w1),b1)\n    layer1_op=tf.nn.tanh(layer1)\n    \n    w2=tf.Variable(tf.truncated_normal([5,6]),name=\"w2\")\n    b2=tf.Variable(tf.truncated_normal([6]),name=\"b2\")\n    \n    layer2=tf.add(tf.matmul(layer1_op,w2),b2)\n    layer2_op=tf.nn.tanh(layer2)\n    \n    w3=tf.Variable(tf.truncated_normal([6,3]),name=\"w3\")\n    b3=tf.Variable(tf.truncated_normal([3]),name=\"b3\")\n    \n    layer3=tf.add(tf.matmul(layer2_op,w3),b3)\n    prediction=layer3\n    \n    correct=tf.equal(tf.arg_max(prediction,1),tf.arg_max(y,1))\n    acc=tf.reduce_mean(tf.cast(correct,tf.float32))\n    #loss=tf.reduce_mean(tf.reduce_sum(tf.square(tf.sub(y,prediction))))\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))\n    optimizer=tf.train.MomentumOptimizer(0.01,0.9).minimize(loss)\n    saver=tf.train.Saver()\n    print(\"Network Ready\")\n\nwith tf.Session(graph=mygraph) as session:\n    feedx,feedy=loadiris(\"/media/parthosarothi/OHWR/Dataset/iris.csv\")\n    print(\"Data 0 x=\",feedx[0],\" y=\",feedy[0])\n    feed={x:feedx,y:feedy}\n    if(sys.argv[1]==\"load\"):\n        saver.restore(session,\"Weights/last\")\n        print(\"Previous Weights Loaded\")\n    else:\n        initop=tf.global_variables_initializer()\n        session.run(initop)\n        print(\"New Weights Loaded\")\n    for e in range(100):\n        l,_,p,a=session.run([loss,optimizer,prediction,acc],feed)\n        print(\"Loss is \",l,\" P \",p[0],\" y \",feedy[0],\" A \",a)\n        saver.save(session,\"Weights/last\")\n\nI am completely in dark. Your concern is highly appreciated.", "body": "@michaelisard \r\nI am extremely sorry for my delayed response. My input is taken from a H5 file which contains features extracted from online handwriting data sample. at every time step I have 16 features. Every time I read from this file the order of data is shuffled.\r\nHere is the part where I am creating the graph.\r\n```\r\ngraph=tf.Graph()\r\nwith graph.as_default():\r\n    print(\"Graph Creation\")\r\n    x=tf.placeholder(tf.float32,[None,ms,nb_features],name=\"x\")\r\n\r\n    y=tf.sparse_placeholder(tf.int32,name=\"y\")\r\n    seq_len=tf.placeholder(tf.int32,[None],name=\"seq_len\")\r\n    \r\n\r\n    f_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\r\n    f_stack=tf.nn.rnn_cell.MultiRNNCell([f_cell]*nb_layers)\r\n    \r\n    b_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\r\n    b_stack=tf.nn.rnn_cell.MultiRNNCell([b_cell]*nb_layers)\r\n    \r\n    outputs,_=tf.nn.bidirectional_dynamic_rnn(f_stack,b_stack,x,sequence_length=seq_len,dtype=tf.float32)\r\n\r\n    merge=tf.concat(2, outputs,name=\"merge\")\r\n\r\n    shape = tf.shape(x)\r\n    batch_s,maxtimesteps=shape[0],shape[1]\r\n\r\n    output_reshape = tf.reshape(merge, [-1, nb_hidden*2])#batch*timesteps,nb_hidden\r\n   \r\n    W = tf.Variable(tf.truncated_normal([nb_hidden*2,nb_classes],stddev=0.1),name=\"W1\")\r\n\r\n    b = tf.Variable(tf.constant(0., shape=[nb_classes]),name=\"b1\")\r\n\r\n    logits = tf.add(tf.matmul(output_reshape, W) , b,name=\"logits\") #818622,52\r\n      \r\n    logits_reshape = tf.transpose(tf.reshape(logits, [batch_s, -1, nb_classes]),[1,0,2],name=\"logits_reshape\")#534,1533,52\r\n\r\n    loss =tf.nn.ctc_loss(logits_reshape, y, seq_len,time_major=True)\r\n    cost = tf.reduce_mean(loss,name=\"cost\")\r\n\r\n    optimizer = tf.train.RMSPropOptimizer(lr).minimize(cost)\r\n\r\n    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits_reshape, seq_len)\r\n\r\n    actual_ed=tf.edit_distance(tf.cast(decoded[0], tf.int32),y,normalize=False)\r\n    ler = tf.reduce_sum(actual_ed,name=\"ler\")\r\n    saver=tf.train.Saver()\r\n    bestsaver=tf.train.Saver()\r\n    print(\"Network Ready\")\r\n```\r\nJust after this, I am running the training and saving it\r\n```\r\nwith tf.Session(graph=graph,config=tf.ConfigProto(log_device_placement=False)) as session:    \r\n    #saver = tf.train.Saver()\r\n    if(sys.argv[1]==\"load\"):\r\n        saver.restore(session, \"Weights/model_last\")\r\n        print(\"Previous weights loaded\")\r\n    else:\r\n        init_op = tf.global_variables_initializer()\r\n        session.run(init_op)\r\n        print(\"New Weights Initialized\")\r\n    \r\n    best=0\r\n    acctest=0\r\n    bestloss=10000\r\n    #testfeed = {x:test_inpx[0],y:test_inp_sparse_y[0],seq_len:test_inpseqlen[0]}\r\n    testcases=[0,5,17,39,60]\r\n    true=[]\r\n    for tr in range(len(testcases)):\r\n        true1=label_from_sparse(test_inp_sparse_y[0],testcases[tr])\r\n        true.append(true1)    \r\n    print(\"Actual \",true)\r\n    for e in range(nb_epochs):\r\n        f=open(logfilename,\"a\")\r\n        totalloss=0\r\n        totalacc=0\r\n        starttime=time.time()\r\n\r\n        for b in range(trainbatch):\r\n            p=b+1\r\n            print(\"Reading Batch \",p,\"/\",trainbatch,end=\"\\r\")\r\n            feed = {x:inpx[b],y:inp_sparse_y[b],seq_len:inpseqlen[b]}\r\n            batchloss,batchacc, _ = session.run([cost,ler,optimizer], feed)\r\n            \r\n            totalloss=totalloss+batchloss\r\n            totalacc=totalacc+batchacc\r\n        avgloss=totalloss/trainbatch\r\n        avgacc=1-(totalacc/nctr)\r\n        if(avgloss<bestloss):\r\n            bestloss=avgloss\r\n            print(\"Network Improvement\")\r\n            saver.save(session, \"Weights/model_last\")\r\n        \r\n        testloss=0\r\n        testacc=0\r\n        for t in range(testbatch):\r\n            testfeed = {x:test_inpx[t],y:test_inp_sparse_y[t],seq_len:test_inpseqlen[t]}\r\n            outcome,testbatchloss,testbatchacc=session.run([decoded[0],cost,ler],testfeed)\r\n            if(t==0):\r\n                first_batch_outcome=outcome\r\n            testloss=testloss+testbatchloss\r\n            testacc=testacc+testbatchacc\r\n        \r\n        testfile=open(\"Results.txt\",\"w\")\r\n        testfile.write(\"Epoch \"+str(e)+\"\\n\")\r\n        for tc in range(len(testcases)):\r\n            predicted=label_from_sparse(first_batch_outcome,testcases[tc])\r\n            testfile.write(str(true[tc])+\" As \"+str(predicted)+\"\\n\")\r\n        testfile.close()\r\n        \r\n        testloss=testloss/testbatch\r\n        testacc=1-(testacc/ncts)\r\n        \r\n        endtime=time.time()        \r\n        if(testacc>best):\r\n            best=testacc\r\n            print(\"Test Result Improvement\")\r\n            bestsaver.save(session, \"BestWeights/model_best\")\r\n        timetaken=endtime-starttime\r\n        msg=\"Epoch \"+str(e)+\"(\"+str(timetaken)+ \" sec ) Training: Cost is \"+str(avgloss)+\" Accuracy \"+str(avgacc)+\" Testing: Loss \"+str(testloss)+\" Accuracy \"+str(testacc)+\"\\n\"\r\n        print(msg)\r\n        f.write(msg)\r\n        f.close()\r\n```\r\nNow, whenever I am loading the model from last save or best save, It is not showing any sign of previous training. Seems to be starting from some scratch. I also tried `import_meta_graph()` without any success. But the same strategy is working absolutely fine with a RNN model which is tested against the well known IRIS data set (hence a classification problem).\r\n```\r\nmygraph=tf.Graph()\r\n\r\nwith mygraph.as_default():\r\n    x=tf.placeholder(tf.float32,[None,4,1])\r\n    y=tf.placeholder(tf.float32,[None,3])\r\n    \r\n    rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\r\n    rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\r\n    \r\n    shape=tf.shape(rnnop)\r\n    batch=shape[0]\r\n    op=shape[1]*shape[2]\r\n    rnnrs=tf.reshape(rnnop,[batch,op],name=\"rnnrs\")\r\n    \r\n    w1=tf.Variable(tf.truncated_normal([12,5]),name=\"w1\")\r\n    b1=tf.Variable(tf.truncated_normal([5]),name=\"b1\")\r\n    \r\n    layer1=tf.add(tf.matmul(rnnrs,w1),b1)\r\n    layer1_op=tf.nn.tanh(layer1)\r\n    \r\n    w2=tf.Variable(tf.truncated_normal([5,6]),name=\"w2\")\r\n    b2=tf.Variable(tf.truncated_normal([6]),name=\"b2\")\r\n    \r\n    layer2=tf.add(tf.matmul(layer1_op,w2),b2)\r\n    layer2_op=tf.nn.tanh(layer2)\r\n    \r\n    w3=tf.Variable(tf.truncated_normal([6,3]),name=\"w3\")\r\n    b3=tf.Variable(tf.truncated_normal([3]),name=\"b3\")\r\n    \r\n    layer3=tf.add(tf.matmul(layer2_op,w3),b3)\r\n    prediction=layer3\r\n    \r\n    correct=tf.equal(tf.arg_max(prediction,1),tf.arg_max(y,1))\r\n    acc=tf.reduce_mean(tf.cast(correct,tf.float32))\r\n    #loss=tf.reduce_mean(tf.reduce_sum(tf.square(tf.sub(y,prediction))))\r\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))\r\n    optimizer=tf.train.MomentumOptimizer(0.01,0.9).minimize(loss)\r\n    saver=tf.train.Saver()\r\n    print(\"Network Ready\")\r\n\r\nwith tf.Session(graph=mygraph) as session:\r\n    feedx,feedy=loadiris(\"/media/parthosarothi/OHWR/Dataset/iris.csv\")\r\n    print(\"Data 0 x=\",feedx[0],\" y=\",feedy[0])\r\n    feed={x:feedx,y:feedy}\r\n    if(sys.argv[1]==\"load\"):\r\n        saver.restore(session,\"Weights/last\")\r\n        print(\"Previous Weights Loaded\")\r\n    else:\r\n        initop=tf.global_variables_initializer()\r\n        session.run(initop)\r\n        print(\"New Weights Loaded\")\r\n    for e in range(100):\r\n        l,_,p,a=session.run([loss,optimizer,prediction,acc],feed)\r\n        print(\"Loss is \",l,\" P \",p[0],\" y \",feedy[0],\" A \",a)\r\n        saver.save(session,\"Weights/last\")\r\n```\r\nI am completely in dark. Your concern is highly appreciated."}