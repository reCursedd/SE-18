{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/440753480", "html_url": "https://github.com/tensorflow/tensorflow/pull/23790#issuecomment-440753480", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23790", "id": 440753480, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDc1MzQ4MA==", "user": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-21T17:44:25Z", "updated_at": "2018-11-21T17:44:25Z", "author_association": "MEMBER", "body_html": "<p>My impression is that <code>sharded=True</code> is mainly useful if you can't fit all of your variables in memory on a single machine.</p>\n<p>So you shard the variables among a bunch of parameter servers. But now if you go to save without sharding, you copy all of the variables to a single machine and run out of memory. Theoretically this could be serialized so each shard gets saved before the next gets copied to the machine doing the saving, but that seems very complicated and pretty slow.</p>\n<p>So instead the assumption is that a distributed filesystem handles the shards, and each parameter server can just dump whatever they have to it. This assumption is annoying if there is no distributed filesystem, but I don't think copying all the shards to a single machine helps. Maybe the discussion should be around whether <code>sharded=True</code> is a good default, or whether it needs to be clearer in the documentation that a shared filesystem is an assumption for distributed <code>Estimator</code>?</p>\n<p>FWIW <code>tf.train.Saver</code> will probably not be in TensorFlow 2.x. We haven't yet settled on an API to expose this kind of sharding behavior.</p>", "body_text": "My impression is that sharded=True is mainly useful if you can't fit all of your variables in memory on a single machine.\nSo you shard the variables among a bunch of parameter servers. But now if you go to save without sharding, you copy all of the variables to a single machine and run out of memory. Theoretically this could be serialized so each shard gets saved before the next gets copied to the machine doing the saving, but that seems very complicated and pretty slow.\nSo instead the assumption is that a distributed filesystem handles the shards, and each parameter server can just dump whatever they have to it. This assumption is annoying if there is no distributed filesystem, but I don't think copying all the shards to a single machine helps. Maybe the discussion should be around whether sharded=True is a good default, or whether it needs to be clearer in the documentation that a shared filesystem is an assumption for distributed Estimator?\nFWIW tf.train.Saver will probably not be in TensorFlow 2.x. We haven't yet settled on an API to expose this kind of sharding behavior.", "body": "My impression is that `sharded=True` is mainly useful if you can't fit all of your variables in memory on a single machine.\r\n\r\nSo you shard the variables among a bunch of parameter servers. But now if you go to save without sharding, you copy all of the variables to a single machine and run out of memory. Theoretically this could be serialized so each shard gets saved before the next gets copied to the machine doing the saving, but that seems very complicated and pretty slow.\r\n\r\nSo instead the assumption is that a distributed filesystem handles the shards, and each parameter server can just dump whatever they have to it. This assumption is annoying if there is no distributed filesystem, but I don't think copying all the shards to a single machine helps. Maybe the discussion should be around whether `sharded=True` is a good default, or whether it needs to be clearer in the documentation that a shared filesystem is an assumption for distributed `Estimator`?\r\n\r\nFWIW `tf.train.Saver` will probably not be in TensorFlow 2.x. We haven't yet settled on an API to expose this kind of sharding behavior."}