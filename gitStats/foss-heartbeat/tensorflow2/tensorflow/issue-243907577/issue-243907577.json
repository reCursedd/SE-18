{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11598", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11598/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11598/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11598/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11598", "id": 243907577, "node_id": "MDU6SXNzdWUyNDM5MDc1Nzc=", "number": 11598, "title": "Unclear about how to make BeamSearchDecoder work", "user": {"login": "zhedongzheng", "id": 16261331, "node_id": "MDQ6VXNlcjE2MjYxMzMx", "avatar_url": "https://avatars2.githubusercontent.com/u/16261331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhedongzheng", "html_url": "https://github.com/zhedongzheng", "followers_url": "https://api.github.com/users/zhedongzheng/followers", "following_url": "https://api.github.com/users/zhedongzheng/following{/other_user}", "gists_url": "https://api.github.com/users/zhedongzheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhedongzheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhedongzheng/subscriptions", "organizations_url": "https://api.github.com/users/zhedongzheng/orgs", "repos_url": "https://api.github.com/users/zhedongzheng/repos", "events_url": "https://api.github.com/users/zhedongzheng/events{/privacy}", "received_events_url": "https://api.github.com/users/zhedongzheng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 26, "created_at": "2017-07-19T02:47:53Z", "updated_at": "2018-08-30T12:10:27Z", "closed_at": "2017-07-28T12:30:47Z", "author_association": "NONE", "body_html": "<h2>UPDATE: In the latest tensorflow 1.2.1, this is no longer a problem.</h2>\n<h2>Please ignore this problem and install the latest tensorflow.</h2>\n<p>Hello, I am trying to understand the way to use BeamSearchDecoder in a seq2seq model by following the tutorial of <a href=\"https://github.com/tensorflow/nmt#beam-search\">nmt</a>. However, both the documentation and error message seem to be very unclear for starters. I have wrote the minimal code for a common seq2seq purpose with beam search:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.layers.core <span class=\"pl-k\">import</span> Dense\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> INPUTS</span>\nX <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>])\nY <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>])\nX_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>])\nY_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ENCODER         </span>\nencoder_out, encoder_state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>), \n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.contrib.layers.embed_sequence(X, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len,\n    <span class=\"pl-v\">dtype</span> <span class=\"pl-k\">=</span> tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER COMPONENTS</span>\nY_vocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\ndecoder_embedding <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([Y_vocab_size, <span class=\"pl-c1\">128</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>))\nprojection_layer <span class=\"pl-k\">=</span> Dense(Y_vocab_size)\ndecoder_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> TRAINING DECODER</span>\ntraining_helper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(\n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(decoder_embedding, Y),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> Y_seq_len,\n    <span class=\"pl-v\">time_major</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\ntraining_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">helper</span> <span class=\"pl-k\">=</span> training_helper,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> encoder_state,\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer)\ntraining_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> training_decoder,\n    <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> tf.reduce_max(Y_seq_len))\ntraining_logits <span class=\"pl-k\">=</span> training_decoder_output.rnn_output\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> PREDICTING_DECODER</span>\npredicting_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BeamSearchDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">embedding</span> <span class=\"pl-k\">=</span> decoder_embedding,\n    <span class=\"pl-v\">start_tokens</span> <span class=\"pl-k\">=</span> tf.tile(tf.constant([<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32), [<span class=\"pl-c1\">128</span>]),\n    <span class=\"pl-v\">end_token</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> tf.contrib.seq2seq.tile_batch(encoder_state, <span class=\"pl-v\">multiplier</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>),\n    <span class=\"pl-v\">beam_width</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>,\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer,\n    <span class=\"pl-v\">length_penalty_weight</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>)\npredicting_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> predicting_decoder,\n    <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> tf.reduce_max(Y_seq_len))\npredicting_logits <span class=\"pl-k\">=</span> predicting_decoder_output.sample_id\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> LOSS</span>\nmasks <span class=\"pl-k\">=</span> tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nloss <span class=\"pl-k\">=</span> tf.contrib.seq2seq.sequence_loss(<span class=\"pl-v\">logits</span> <span class=\"pl-k\">=</span> training_logits, <span class=\"pl-v\">targets</span> <span class=\"pl-k\">=</span> Y, <span class=\"pl-v\">weights</span> <span class=\"pl-k\">=</span> masks)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> BACKWARD</span>\nparams <span class=\"pl-k\">=</span> tf.trainable_variables()\ngradients <span class=\"pl-k\">=</span> tf.gradients(loss, params)\nclipped_gradients, _ <span class=\"pl-k\">=</span> tf.clip_by_global_norm(gradients, <span class=\"pl-c1\">5.0</span>)\ntrain_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer().apply_gradients(<span class=\"pl-c1\">zip</span>(clipped_gradients, params))</pre></div>\n<p>The error occurs at BeamSearchDecoder:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 48, in &lt;module&gt;\n    length_penalty_weight = 0.0)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 175, in __init__\n    initial_state, self._cell.state_size)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 319, in map_structure\n    assert_same_structure(structure[0], other, check_types=check_types)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 146, in assert_same_structure\n    % (nest1, nest2))\nValueError: The two structures don't have the same number of elements. First structure: Tensor(\"tile_batch/Reshape:0\", shape=(20, ?, 128), dtype=float32), second structure: LSTMStateTuple(c=128, h=128).\n</code></pre>", "body_text": "UPDATE: In the latest tensorflow 1.2.1, this is no longer a problem.\nPlease ignore this problem and install the latest tensorflow.\nHello, I am trying to understand the way to use BeamSearchDecoder in a seq2seq model by following the tutorial of nmt. However, both the documentation and error message seem to be very unclear for starters. I have wrote the minimal code for a common seq2seq purpose with beam search:\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\n\n# INPUTS\nX = tf.placeholder(tf.int32, [None, None])\nY = tf.placeholder(tf.int32, [None, None])\nX_seq_len = tf.placeholder(tf.int32, [None])\nY_seq_len = tf.placeholder(tf.int32, [None])\n\n# ENCODER         \nencoder_out, encoder_state = tf.nn.dynamic_rnn(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\n    sequence_length = X_seq_len,\n    dtype = tf.float32)\n\n# DECODER COMPONENTS\nY_vocab_size = 10000\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\nprojection_layer = Dense(Y_vocab_size)\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(128)\n\n# TRAINING DECODER\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\n    sequence_length = Y_seq_len,\n    time_major = False)\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\n    cell = decoder_cell,\n    helper = training_helper,\n    initial_state = encoder_state,\n    output_layer = projection_layer)\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder = training_decoder,\n    impute_finished = True,\n    maximum_iterations = tf.reduce_max(Y_seq_len))\ntraining_logits = training_decoder_output.rnn_output\n\n# PREDICTING_DECODER\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n    cell = decoder_cell,\n    embedding = decoder_embedding,\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [128]),\n    end_token = 2,\n    initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=10),\n    beam_width = 10,\n    output_layer = projection_layer,\n    length_penalty_weight = 0.0)\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder = predicting_decoder,\n    impute_finished = True,\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\npredicting_logits = predicting_decoder_output.sample_id\n\n# LOSS\nmasks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)\nloss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)\n\n# BACKWARD\nparams = tf.trainable_variables()\ngradients = tf.gradients(loss, params)\nclipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\ntrain_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))\nThe error occurs at BeamSearchDecoder:\nTraceback (most recent call last):\n  File \"test.py\", line 48, in <module>\n    length_penalty_weight = 0.0)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 175, in __init__\n    initial_state, self._cell.state_size)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 319, in map_structure\n    assert_same_structure(structure[0], other, check_types=check_types)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 146, in assert_same_structure\n    % (nest1, nest2))\nValueError: The two structures don't have the same number of elements. First structure: Tensor(\"tile_batch/Reshape:0\", shape=(20, ?, 128), dtype=float32), second structure: LSTMStateTuple(c=128, h=128).", "body": "## UPDATE: In the latest tensorflow 1.2.1, this is no longer a problem.\r\n##  Please ignore this problem and install the latest tensorflow.\r\n\r\nHello, I am trying to understand the way to use BeamSearchDecoder in a seq2seq model by following the tutorial of [nmt](https://github.com/tensorflow/nmt#beam-search). However, both the documentation and error message seem to be very unclear for starters. I have wrote the minimal code for a common seq2seq purpose with beam search:\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [None, None])\r\nY = tf.placeholder(tf.int32, [None, None])\r\nX_seq_len = tf.placeholder(tf.int32, [None])\r\nY_seq_len = tf.placeholder(tf.int32, [None])\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(128)\r\n\r\n# TRAINING DECODER\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = encoder_state,\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n# PREDICTING_DECODER\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [128]),\r\n    end_token = 2,\r\n    initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=10),\r\n    beam_width = 10,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = predicting_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.sample_id\r\n\r\n# LOSS\r\nmasks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)\r\nloss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)\r\n\r\n# BACKWARD\r\nparams = tf.trainable_variables()\r\ngradients = tf.gradients(loss, params)\r\nclipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))\r\n```\r\nThe error occurs at BeamSearchDecoder:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 48, in <module>\r\n    length_penalty_weight = 0.0)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 175, in __init__\r\n    initial_state, self._cell.state_size)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 319, in map_structure\r\n    assert_same_structure(structure[0], other, check_types=check_types)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 146, in assert_same_structure\r\n    % (nest1, nest2))\r\nValueError: The two structures don't have the same number of elements. First structure: Tensor(\"tile_batch/Reshape:0\", shape=(20, ?, 128), dtype=float32), second structure: LSTMStateTuple(c=128, h=128).\r\n```"}