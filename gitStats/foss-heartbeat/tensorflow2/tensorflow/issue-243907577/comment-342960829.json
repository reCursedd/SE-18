{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/342960829", "html_url": "https://github.com/tensorflow/tensorflow/issues/11598#issuecomment-342960829", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11598", "id": 342960829, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mjk2MDgyOQ==", "user": {"login": "georgesterpu", "id": 6018251, "node_id": "MDQ6VXNlcjYwMTgyNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6018251?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgesterpu", "html_url": "https://github.com/georgesterpu", "followers_url": "https://api.github.com/users/georgesterpu/followers", "following_url": "https://api.github.com/users/georgesterpu/following{/other_user}", "gists_url": "https://api.github.com/users/georgesterpu/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgesterpu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgesterpu/subscriptions", "organizations_url": "https://api.github.com/users/georgesterpu/orgs", "repos_url": "https://api.github.com/users/georgesterpu/repos", "events_url": "https://api.github.com/users/georgesterpu/events{/privacy}", "received_events_url": "https://api.github.com/users/georgesterpu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-08T21:10:54Z", "updated_at": "2017-11-08T21:10:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry for missing this, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>.<br>\nYes, as I don't use the decoder states, masking the outputs would do it.<br>\nI am now concerned about <code>impute_finished</code> and <code>dynamic_decode</code> during training.</p>\n<pre><code>self._decoder_train_outputs, self._final_states, self._final_seq_lens = seq2seq.dynamic_decode(\n            self._decoder_train,\n            output_time_major=False,\n            impute_finished=True,\n            swap_memory=False)\nself._train_prediction = tf.argmax(self._decoder_train_outputs.rnn_output, axis=-1, output_type=tf.int32)\n</code></pre>\n<p>While checking for some predictions, I could see that sometimes the network predicts the EOS token before the actual EOS in the ground truth. However, decoding does not stop there during training, as <code>ScheduledEmbeddingTrainingHelper</code> will keep feeding the entire label stream. This results in a decoded sequence having multiple EOS tokens, and <code>impute_finished</code> adds zeros only after the last EOS in the ground truth. This also implies that there is a possibility for an EOS prediction to be sampled for being fed again to the decoder's input, thus adding some noise during training. Is there any way of preventing this behaviour ? It does not seem that <code>ScheduledEmbeddingTrainingHelper</code> could swap a sampled EOS token with the true label at the moment, as it does not take the <code>end_token</code> argument unlike <code>GreedyEmbeddingHelper</code> or <code>BeamSearchDecoder</code>.</p>", "body_text": "Sorry for missing this, @ebrevdo.\nYes, as I don't use the decoder states, masking the outputs would do it.\nI am now concerned about impute_finished and dynamic_decode during training.\nself._decoder_train_outputs, self._final_states, self._final_seq_lens = seq2seq.dynamic_decode(\n            self._decoder_train,\n            output_time_major=False,\n            impute_finished=True,\n            swap_memory=False)\nself._train_prediction = tf.argmax(self._decoder_train_outputs.rnn_output, axis=-1, output_type=tf.int32)\n\nWhile checking for some predictions, I could see that sometimes the network predicts the EOS token before the actual EOS in the ground truth. However, decoding does not stop there during training, as ScheduledEmbeddingTrainingHelper will keep feeding the entire label stream. This results in a decoded sequence having multiple EOS tokens, and impute_finished adds zeros only after the last EOS in the ground truth. This also implies that there is a possibility for an EOS prediction to be sampled for being fed again to the decoder's input, thus adding some noise during training. Is there any way of preventing this behaviour ? It does not seem that ScheduledEmbeddingTrainingHelper could swap a sampled EOS token with the true label at the moment, as it does not take the end_token argument unlike GreedyEmbeddingHelper or BeamSearchDecoder.", "body": "Sorry for missing this, @ebrevdo.\r\nYes, as I don't use the decoder states, masking the outputs would do it.\r\nI am now concerned about `impute_finished` and `dynamic_decode` during training. \r\n\r\n```\r\nself._decoder_train_outputs, self._final_states, self._final_seq_lens = seq2seq.dynamic_decode(\r\n            self._decoder_train,\r\n            output_time_major=False,\r\n            impute_finished=True,\r\n            swap_memory=False)\r\nself._train_prediction = tf.argmax(self._decoder_train_outputs.rnn_output, axis=-1, output_type=tf.int32)\r\n```\r\n\r\nWhile checking for some predictions, I could see that sometimes the network predicts the EOS token before the actual EOS in the ground truth. However, decoding does not stop there during training, as `ScheduledEmbeddingTrainingHelper` will keep feeding the entire label stream. This results in a decoded sequence having multiple EOS tokens, and `impute_finished` adds zeros only after the last EOS in the ground truth. This also implies that there is a possibility for an EOS prediction to be sampled for being fed again to the decoder's input, thus adding some noise during training. Is there any way of preventing this behaviour ? It does not seem that `ScheduledEmbeddingTrainingHelper` could swap a sampled EOS token with the true label at the moment, as it does not take the `end_token` argument unlike `GreedyEmbeddingHelper` or `BeamSearchDecoder`."}