{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243992315", "html_url": "https://github.com/tensorflow/tensorflow/issues/3907#issuecomment-243992315", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3907", "id": 243992315, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mzk5MjMxNQ==", "user": {"login": "pfllo", "id": 5235521, "node_id": "MDQ6VXNlcjUyMzU1MjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5235521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pfllo", "html_url": "https://github.com/pfllo", "followers_url": "https://api.github.com/users/pfllo/followers", "following_url": "https://api.github.com/users/pfllo/following{/other_user}", "gists_url": "https://api.github.com/users/pfllo/gists{/gist_id}", "starred_url": "https://api.github.com/users/pfllo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pfllo/subscriptions", "organizations_url": "https://api.github.com/users/pfllo/orgs", "repos_url": "https://api.github.com/users/pfllo/repos", "events_url": "https://api.github.com/users/pfllo/events{/privacy}", "received_events_url": "https://api.github.com/users/pfllo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-01T06:55:36Z", "updated_at": "2016-09-01T06:55:55Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=592670\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/concretevitamin\">@concretevitamin</a> I'm not sure what you mean about <code>tf.scatter_{assign,add,sub}()</code>.</p>\n<p>I accumulate the gradients using the method mentioned in issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"172794790\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3994\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3994/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3994\">#3994</a>. To be more specific, in order to update variable <code>var</code>, I first create a zero initialized variable <code>accum_var</code> to hold the accumulated gradients, and I add the gradients to <code>accum_var</code> in every iteration. Finally I add <code>accum_var</code> to <code>var</code> after several iterations.</p>\n<p>This method works fine except for the situation where <code>var</code> is the word embedding matrix. In this situation, I want <code>accum_var</code> to be something like <code>IndexedSlices</code> or <code>SparseTensor</code> so that I can use functions like <code>tf.scatter_add()</code> to update the word embedding matrix efficiently. However, I'm not sure how <code>tf.scatter_{assign,add,sub}()</code> alone can solve this problem. Do you mean evaluating <code>IndexedSlices.indices</code> and <code>IndexedSlices.values</code> of word embedding gradient every iteration, accumulate them outside of the graph and use <code>tf.scatter_add()</code> to update afterwards?</p>", "body_text": "@concretevitamin I'm not sure what you mean about tf.scatter_{assign,add,sub}().\nI accumulate the gradients using the method mentioned in issue #3994. To be more specific, in order to update variable var, I first create a zero initialized variable accum_var to hold the accumulated gradients, and I add the gradients to accum_var in every iteration. Finally I add accum_var to var after several iterations.\nThis method works fine except for the situation where var is the word embedding matrix. In this situation, I want accum_var to be something like IndexedSlices or SparseTensor so that I can use functions like tf.scatter_add() to update the word embedding matrix efficiently. However, I'm not sure how tf.scatter_{assign,add,sub}() alone can solve this problem. Do you mean evaluating IndexedSlices.indices and IndexedSlices.values of word embedding gradient every iteration, accumulate them outside of the graph and use tf.scatter_add() to update afterwards?", "body": "@concretevitamin I'm not sure what you mean about `tf.scatter_{assign,add,sub}()`.\n\nI accumulate the gradients using the method mentioned in issue #3994. To be more specific, in order to update variable `var`, I first create a zero initialized variable `accum_var` to hold the accumulated gradients, and I add the gradients to `accum_var` in every iteration. Finally I add `accum_var` to `var` after several iterations. \n\nThis method works fine except for the situation where `var` is the word embedding matrix. In this situation, I want `accum_var` to be something like `IndexedSlices` or `SparseTensor` so that I can use functions like `tf.scatter_add()` to update the word embedding matrix efficiently. However, I'm not sure how `tf.scatter_{assign,add,sub}()` alone can solve this problem. Do you mean evaluating `IndexedSlices.indices` and `IndexedSlices.values` of word embedding gradient every iteration, accumulate them outside of the graph and use `tf.scatter_add()` to update afterwards?\n"}