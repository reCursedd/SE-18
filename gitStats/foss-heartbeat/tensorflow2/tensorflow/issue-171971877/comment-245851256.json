{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/245851256", "html_url": "https://github.com/tensorflow/tensorflow/issues/3907#issuecomment-245851256", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3907", "id": 245851256, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NTg1MTI1Ng==", "user": {"login": "MohammadSamragh", "id": 17733504, "node_id": "MDQ6VXNlcjE3NzMzNTA0", "avatar_url": "https://avatars0.githubusercontent.com/u/17733504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MohammadSamragh", "html_url": "https://github.com/MohammadSamragh", "followers_url": "https://api.github.com/users/MohammadSamragh/followers", "following_url": "https://api.github.com/users/MohammadSamragh/following{/other_user}", "gists_url": "https://api.github.com/users/MohammadSamragh/gists{/gist_id}", "starred_url": "https://api.github.com/users/MohammadSamragh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MohammadSamragh/subscriptions", "organizations_url": "https://api.github.com/users/MohammadSamragh/orgs", "repos_url": "https://api.github.com/users/MohammadSamragh/repos", "events_url": "https://api.github.com/users/MohammadSamragh/events{/privacy}", "received_events_url": "https://api.github.com/users/MohammadSamragh/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-09T08:25:14Z", "updated_at": "2016-09-09T08:28:53Z", "author_association": "NONE", "body_html": "<p>Sorry for my late response guys, I had to get back to my codes to see what I have done.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5235521\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pfllo\">@pfllo</a> You got my question well. Assume that <code>y=tf.matmul(input,W)</code>; if <code>W</code> is a dense matrix, then <code>tf.gradients(y,W)</code> returns a dense matrix. My question is how to make <code>tf.gradients(y,W)</code> ignore gradient computation of some indices in <code>W</code> and compute them only in some known indices. Basically I want to implement the Pruning idea in neural networks described in this <a href=\"https://arxiv.org/abs/1506.02626\" rel=\"nofollow\">paper</a>. They claim that they have implemented this using Caffe, but I'm not sure how you can implement it using TF.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> I have tried using <code>tf.gather</code> and <code>tf.gradients</code> together, I opened a Stackoverflow  question <a href=\"http://stackoverflow.com/questions/38063430/using-tensorflows-tf-gather-and-tf-gradients-together\" rel=\"nofollow\">here</a><br>\nI used the following code:</p>\n<pre><code>sparse_X=tf.gather(W,indices)\ngrads_i_want_to_compute = tf.gradient(y, sparse_W)\n</code></pre>\n<p>the problem is that <code>grads_i_want_to_compute</code> becomes a <code>None</code> object. I assume this happens because <code>sparse_W</code> is not considered as part of the graph and the gradients of <code>y</code> with respect to  <code>sparse_W</code> become <code>None</code>.</p>", "body_text": "Sorry for my late response guys, I had to get back to my codes to see what I have done.\n@pfllo You got my question well. Assume that y=tf.matmul(input,W); if W is a dense matrix, then tf.gradients(y,W) returns a dense matrix. My question is how to make tf.gradients(y,W) ignore gradient computation of some indices in W and compute them only in some known indices. Basically I want to implement the Pruning idea in neural networks described in this paper. They claim that they have implemented this using Caffe, but I'm not sure how you can implement it using TF.\n@alextp I have tried using tf.gather and tf.gradients together, I opened a Stackoverflow  question here\nI used the following code:\nsparse_X=tf.gather(W,indices)\ngrads_i_want_to_compute = tf.gradient(y, sparse_W)\n\nthe problem is that grads_i_want_to_compute becomes a None object. I assume this happens because sparse_W is not considered as part of the graph and the gradients of y with respect to  sparse_W become None.", "body": "Sorry for my late response guys, I had to get back to my codes to see what I have done.\n\n@pfllo You got my question well. Assume that `y=tf.matmul(input,W)`; if `W` is a dense matrix, then `tf.gradients(y,W)` returns a dense matrix. My question is how to make `tf.gradients(y,W)` ignore gradient computation of some indices in `W` and compute them only in some known indices. Basically I want to implement the Pruning idea in neural networks described in this [paper](https://arxiv.org/abs/1506.02626). They claim that they have implemented this using Caffe, but I'm not sure how you can implement it using TF. \n\n@alextp I have tried using `tf.gather` and `tf.gradients` together, I opened a Stackoverflow  question [here](http://stackoverflow.com/questions/38063430/using-tensorflows-tf-gather-and-tf-gradients-together)\nI used the following code:\n\n```\nsparse_X=tf.gather(W,indices)\ngrads_i_want_to_compute = tf.gradient(y, sparse_W)\n```\n\nthe problem is that `grads_i_want_to_compute` becomes a `None` object. I assume this happens because `sparse_W` is not considered as part of the graph and the gradients of `y` with respect to  `sparse_W` become `None`.\n"}