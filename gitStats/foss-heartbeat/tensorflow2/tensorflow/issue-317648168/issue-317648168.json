{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18862", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18862/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18862/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18862/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18862", "id": 317648168, "node_id": "MDU6SXNzdWUzMTc2NDgxNjg=", "number": 18862, "title": "failure when using Dataset to build a model with dilation convolutional layers", "user": {"login": "mycrazycracy", "id": 29195614, "node_id": "MDQ6VXNlcjI5MTk1NjE0", "avatar_url": "https://avatars0.githubusercontent.com/u/29195614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mycrazycracy", "html_url": "https://github.com/mycrazycracy", "followers_url": "https://api.github.com/users/mycrazycracy/followers", "following_url": "https://api.github.com/users/mycrazycracy/following{/other_user}", "gists_url": "https://api.github.com/users/mycrazycracy/gists{/gist_id}", "starred_url": "https://api.github.com/users/mycrazycracy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mycrazycracy/subscriptions", "organizations_url": "https://api.github.com/users/mycrazycracy/orgs", "repos_url": "https://api.github.com/users/mycrazycracy/repos", "events_url": "https://api.github.com/users/mycrazycracy/events{/privacy}", "received_events_url": "https://api.github.com/users/mycrazycracy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-04-25T14:16:23Z", "updated_at": "2018-05-06T01:53:48Z", "closed_at": "2018-04-27T21:21:49Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li>**OS Platform and Distribution **:  Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5.0</li>\n<li><strong>Python version</strong>: 3.6.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I happened to find this issue when I tried to add dilation convolution to my model. Here I give basic steps to reproduce the issue.</p>\n<p>First, I use placeholder to create a tiny network.</p>\n<pre><code># [batch, 1, length, feat_dim]\ninput = tf.placeholder(tf.float32, shape=(128, 1, 100, 20), name='input')\nlabel = tf.placeholder(tf.int64, shape=(128,), name='label')\ninput_value = np.random.normal(size=(128,1,100,20))\nlabel_value = np.random.randint(9852, size=(128))\n\nconv0 = tf.layers.conv2d(input,\n                         512,\n                         (1, 5),\n                         dilation_rate=(1, 1),\n                         activation=tf.nn.relu,\n                         name='conv0')\n\nconv1 = tf.layers.conv2d(conv0,\n                         512,\n                         (1, 3),\n                         dilation_rate=(1, 2),\n                         activation=tf.nn.relu,\n                         name='conv1')\nconv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])\nlogits = tf.layers.dense(conv1_squeeze, 9852, name='logits')\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=\"loss\")\ntrain_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    _ = sess.run(train_op, feed_dict={input:input_value, label:label_value})\n    print(\"Here we are\")\n</code></pre>\n<p>I add two conv layers and the second one uses a dilation convolution with rate=2. It is actually 1d conv, though I use conv2d here. A mean pooling is performed before the fully connected layer. It works.</p>\n<p>Then I introduce the Dataset to build to the input pipeline:</p>\n<pre><code>def _parse_tfrecord(example_proto):\n    dics = {'input': tf.FixedLenFeature(shape=(), dtype=tf.string),\n            'input_shape': tf.FixedLenFeature(shape=(2,), dtype=tf.int64),\n            'output': tf.FixedLenFeature(shape=(), dtype=tf.int64)}\n    parsed_example = tf.parse_single_example(example_proto, dics)\n    # the dtype of feature is 'float32'\n    parsed_example['input'] = tf.decode_raw(parsed_example['input'], tf.float32)\n    parsed_example['input'] = tf.reshape(parsed_example['input'], parsed_example['input_shape'])\n    return parsed_example\n\ndef create_variable_dataset(filenames, batch_size, feat_dim):\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n    dataset = dataset.interleave(lambda filename:\n                               tf.data.TFRecordDataset(filename).map(\n                                   _parse_tfrecord, num_parallel_calls=8).padded_batch(\n                                       batch_size,\n                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []})),\n                                cycle_length=len(filenames), block_length=1\n                               )\n\n    dataset = dataset.prefetch(5)\n    itr = dataset.make_initializable_iterator()\n    element = itr.get_next()\n    return itr, element['input'], element['output']\n\ntrain_itr, input, label = create_variable_train_dataset(['egs/egs.1.tfrecord'],\n                                                                      batch_size=64,\n                                                                      feat_dim=20)\ninput = tf.expand_dims(input, 1)\nconv0 = tf.layers.conv2d(input,\n                         512,\n                         (1, 5),\n                         dilation_rate=(1, 1),\n                         activation=tf.nn.relu,\n                         name='conv0')\n\nconv1 = tf.layers.conv2d(conv0,\n                         512,\n                         (1, 3),\n                         dilation_rate=(1, 2),\n                         activation=tf.nn.relu,\n                         name='conv1')\n\nconv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])\nlogits = tf.layers.dense(conv1_squeeze, 9852, name='logits')\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=\"loss\")\ntrain_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(train_itr.initializer)\n    _ = sess.run(train_op)\n    print(\"Here we are\")\n</code></pre>\n<p>I load to the tfrecords which are made before. The data loaded has size [length, feat_dim]. With batch, it becomes [batch, length, feat_dim]. And it also works.</p>\n<p>Now, I slightly change the input pipeline to</p>\n<pre><code>def create_variable_train_dataset(filenames, batch_size, feat_dim, shuffle_size=-1):\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n    dataset = dataset.interleave(lambda filename:\n                               tf.data.TFRecordDataset(filename).map(\n                                   _parse_tfrecord, num_parallel_calls=8).apply(\n                                   tf.contrib.data.padded_batch_and_drop_remainder(\n                                       batch_size,\n                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []}))),\n                                cycle_length=len(filenames), block_length=1\n                               )\n\n    dataset = dataset.prefetch(5)\n    itr = dataset.make_initializable_iterator()\n    element = itr.get_next()\n    return itr, element['input'], element['output']\n</code></pre>\n<p>It breaks and throws the exception:</p>\n<pre><code>2018-04-25 22:07:04.779657: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\nTraceback (most recent call last):\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _do_call\n    return fn(*args)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1329, in _run_fn\n    status, run_metadata)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 1530, in &lt;module&gt;\n    globals = debugger.run(setup['file'], None, None, is_module)\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 937, in run\n    pydev_imports.execfile(file, globals, locals)  # execute the script\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"G:/kaldi-master/egs/fisher/v3/xvector/train_model.py\", line 3, in &lt;module&gt;\n    from xvector_train import *\n  File \"G:/kaldi-master/egs/fisher/v3/xvector\\xvector_train.py\", line 48, in &lt;module&gt;\n    _ = sess.run(train_op)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1128, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1344, in _do_run\n    options, run_metadata)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1363, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\n\nCaused by op 'conv1/SpaceToBatchND', defined at:\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 1530, in &lt;module&gt;\n    globals = debugger.run(setup['file'], None, None, is_module)\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 937, in run\n    pydev_imports.execfile(file, globals, locals)  # execute the script\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"G:/kaldi-master/egs/fisher/v3/xvector/train_model.py\", line 3, in &lt;module&gt;\n    from xvector_train import *\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 971, in _find_and_load\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 955, in _find_and_load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 665, in _load_unlocked\n  File \"&lt;frozen importlib._bootstrap_external&gt;\", line 678, in exec_module\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 219, in _call_with_frames_removed\n  File \"G:/kaldi-master/egs/fisher/v3/xvector\\xvector_train.py\", line 36, in &lt;module&gt;\n    name='conv1')\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 614, in conv2d\n    return layer.apply(inputs)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 762, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 652, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 167, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 838, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 502, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 493, in _with_space_to_batch_call\n    paddings=paddings)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6670, in space_to_batch_nd\n    paddings=paddings, name=name)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): padded_shape[1]=97 is not divisible by block_shape[1]=2\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\n</code></pre>\n<p>All the examples in the tfrecords have the size [100, 20].The only thing changed is the batch function \"padded_batch_and_drop_remainder\". I don't know why it tell me the dilation operation cannot be performed because padding should be used in the operation. I felt stranger that if I change the dilation rate of the first layer (which is 1 now) to (1, 2), it works again!</p>\n<pre><code>conv0 = tf.layers.conv2d(input,\n                         512,\n                         (1, 5),\n                         dilation_rate=(1, 2),\n                         activation=tf.nn.relu,\n                         name='conv0')\n</code></pre>\n<p>Could anyone tell me what is going on here? Is anything wrong with the pipeline, or it is a bug ?<br>\nI use TF 1.5.0 in a server.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n**OS Platform and Distribution **:  Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.5.0\nPython version: 3.6.4\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nI happened to find this issue when I tried to add dilation convolution to my model. Here I give basic steps to reproduce the issue.\nFirst, I use placeholder to create a tiny network.\n# [batch, 1, length, feat_dim]\ninput = tf.placeholder(tf.float32, shape=(128, 1, 100, 20), name='input')\nlabel = tf.placeholder(tf.int64, shape=(128,), name='label')\ninput_value = np.random.normal(size=(128,1,100,20))\nlabel_value = np.random.randint(9852, size=(128))\n\nconv0 = tf.layers.conv2d(input,\n                         512,\n                         (1, 5),\n                         dilation_rate=(1, 1),\n                         activation=tf.nn.relu,\n                         name='conv0')\n\nconv1 = tf.layers.conv2d(conv0,\n                         512,\n                         (1, 3),\n                         dilation_rate=(1, 2),\n                         activation=tf.nn.relu,\n                         name='conv1')\nconv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])\nlogits = tf.layers.dense(conv1_squeeze, 9852, name='logits')\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=\"loss\")\ntrain_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    _ = sess.run(train_op, feed_dict={input:input_value, label:label_value})\n    print(\"Here we are\")\n\nI add two conv layers and the second one uses a dilation convolution with rate=2. It is actually 1d conv, though I use conv2d here. A mean pooling is performed before the fully connected layer. It works.\nThen I introduce the Dataset to build to the input pipeline:\ndef _parse_tfrecord(example_proto):\n    dics = {'input': tf.FixedLenFeature(shape=(), dtype=tf.string),\n            'input_shape': tf.FixedLenFeature(shape=(2,), dtype=tf.int64),\n            'output': tf.FixedLenFeature(shape=(), dtype=tf.int64)}\n    parsed_example = tf.parse_single_example(example_proto, dics)\n    # the dtype of feature is 'float32'\n    parsed_example['input'] = tf.decode_raw(parsed_example['input'], tf.float32)\n    parsed_example['input'] = tf.reshape(parsed_example['input'], parsed_example['input_shape'])\n    return parsed_example\n\ndef create_variable_dataset(filenames, batch_size, feat_dim):\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n    dataset = dataset.interleave(lambda filename:\n                               tf.data.TFRecordDataset(filename).map(\n                                   _parse_tfrecord, num_parallel_calls=8).padded_batch(\n                                       batch_size,\n                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []})),\n                                cycle_length=len(filenames), block_length=1\n                               )\n\n    dataset = dataset.prefetch(5)\n    itr = dataset.make_initializable_iterator()\n    element = itr.get_next()\n    return itr, element['input'], element['output']\n\ntrain_itr, input, label = create_variable_train_dataset(['egs/egs.1.tfrecord'],\n                                                                      batch_size=64,\n                                                                      feat_dim=20)\ninput = tf.expand_dims(input, 1)\nconv0 = tf.layers.conv2d(input,\n                         512,\n                         (1, 5),\n                         dilation_rate=(1, 1),\n                         activation=tf.nn.relu,\n                         name='conv0')\n\nconv1 = tf.layers.conv2d(conv0,\n                         512,\n                         (1, 3),\n                         dilation_rate=(1, 2),\n                         activation=tf.nn.relu,\n                         name='conv1')\n\nconv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])\nlogits = tf.layers.dense(conv1_squeeze, 9852, name='logits')\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=\"loss\")\ntrain_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(train_itr.initializer)\n    _ = sess.run(train_op)\n    print(\"Here we are\")\n\nI load to the tfrecords which are made before. The data loaded has size [length, feat_dim]. With batch, it becomes [batch, length, feat_dim]. And it also works.\nNow, I slightly change the input pipeline to\ndef create_variable_train_dataset(filenames, batch_size, feat_dim, shuffle_size=-1):\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n    dataset = dataset.interleave(lambda filename:\n                               tf.data.TFRecordDataset(filename).map(\n                                   _parse_tfrecord, num_parallel_calls=8).apply(\n                                   tf.contrib.data.padded_batch_and_drop_remainder(\n                                       batch_size,\n                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []}))),\n                                cycle_length=len(filenames), block_length=1\n                               )\n\n    dataset = dataset.prefetch(5)\n    itr = dataset.make_initializable_iterator()\n    element = itr.get_next()\n    return itr, element['input'], element['output']\n\nIt breaks and throws the exception:\n2018-04-25 22:07:04.779657: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\nTraceback (most recent call last):\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _do_call\n    return fn(*args)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1329, in _run_fn\n    status, run_metadata)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 1530, in <module>\n    globals = debugger.run(setup['file'], None, None, is_module)\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 937, in run\n    pydev_imports.execfile(file, globals, locals)  # execute the script\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"G:/kaldi-master/egs/fisher/v3/xvector/train_model.py\", line 3, in <module>\n    from xvector_train import *\n  File \"G:/kaldi-master/egs/fisher/v3/xvector\\xvector_train.py\", line 48, in <module>\n    _ = sess.run(train_op)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1128, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1344, in _do_run\n    options, run_metadata)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1363, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\n\nCaused by op 'conv1/SpaceToBatchND', defined at:\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 1530, in <module>\n    globals = debugger.run(setup['file'], None, None, is_module)\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 937, in run\n    pydev_imports.execfile(file, globals, locals)  # execute the script\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"G:/kaldi-master/egs/fisher/v3/xvector/train_model.py\", line 3, in <module>\n    from xvector_train import *\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"G:/kaldi-master/egs/fisher/v3/xvector\\xvector_train.py\", line 36, in <module>\n    name='conv1')\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 614, in conv2d\n    return layer.apply(inputs)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 762, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 652, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 167, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 838, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 502, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 493, in _with_space_to_batch_call\n    paddings=paddings)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6670, in space_to_batch_nd\n    paddings=paddings, name=name)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): padded_shape[1]=97 is not divisible by block_shape[1]=2\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\n\nAll the examples in the tfrecords have the size [100, 20].The only thing changed is the batch function \"padded_batch_and_drop_remainder\". I don't know why it tell me the dilation operation cannot be performed because padding should be used in the operation. I felt stranger that if I change the dilation rate of the first layer (which is 1 now) to (1, 2), it works again!\nconv0 = tf.layers.conv2d(input,\n                         512,\n                         (1, 5),\n                         dilation_rate=(1, 2),\n                         activation=tf.nn.relu,\n                         name='conv0')\n\nCould anyone tell me what is going on here? Is anything wrong with the pipeline, or it is a bug ?\nI use TF 1.5.0 in a server.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution **:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI happened to find this issue when I tried to add dilation convolution to my model. Here I give basic steps to reproduce the issue.\r\n\r\nFirst, I use placeholder to create a tiny network.\r\n```\r\n# [batch, 1, length, feat_dim]\r\ninput = tf.placeholder(tf.float32, shape=(128, 1, 100, 20), name='input')\r\nlabel = tf.placeholder(tf.int64, shape=(128,), name='label')\r\ninput_value = np.random.normal(size=(128,1,100,20))\r\nlabel_value = np.random.randint(9852, size=(128))\r\n\r\nconv0 = tf.layers.conv2d(input,\r\n                         512,\r\n                         (1, 5),\r\n                         dilation_rate=(1, 1),\r\n                         activation=tf.nn.relu,\r\n                         name='conv0')\r\n\r\nconv1 = tf.layers.conv2d(conv0,\r\n                         512,\r\n                         (1, 3),\r\n                         dilation_rate=(1, 2),\r\n                         activation=tf.nn.relu,\r\n                         name='conv1')\r\nconv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])\r\nlogits = tf.layers.dense(conv1_squeeze, 9852, name='logits')\r\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=\"loss\")\r\ntrain_op = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    _ = sess.run(train_op, feed_dict={input:input_value, label:label_value})\r\n    print(\"Here we are\")\r\n```\r\nI add two conv layers and the second one uses a dilation convolution with rate=2. It is actually 1d conv, though I use conv2d here. A mean pooling is performed before the fully connected layer. It works.\r\n\r\nThen I introduce the Dataset to build to the input pipeline:\r\n```\r\ndef _parse_tfrecord(example_proto):\r\n    dics = {'input': tf.FixedLenFeature(shape=(), dtype=tf.string),\r\n            'input_shape': tf.FixedLenFeature(shape=(2,), dtype=tf.int64),\r\n            'output': tf.FixedLenFeature(shape=(), dtype=tf.int64)}\r\n    parsed_example = tf.parse_single_example(example_proto, dics)\r\n    # the dtype of feature is 'float32'\r\n    parsed_example['input'] = tf.decode_raw(parsed_example['input'], tf.float32)\r\n    parsed_example['input'] = tf.reshape(parsed_example['input'], parsed_example['input_shape'])\r\n    return parsed_example\r\n\r\ndef create_variable_dataset(filenames, batch_size, feat_dim):\r\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\r\n    dataset = dataset.interleave(lambda filename:\r\n                               tf.data.TFRecordDataset(filename).map(\r\n                                   _parse_tfrecord, num_parallel_calls=8).padded_batch(\r\n                                       batch_size,\r\n                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []})),\r\n                                cycle_length=len(filenames), block_length=1\r\n                               )\r\n\r\n    dataset = dataset.prefetch(5)\r\n    itr = dataset.make_initializable_iterator()\r\n    element = itr.get_next()\r\n    return itr, element['input'], element['output']\r\n\r\ntrain_itr, input, label = create_variable_train_dataset(['egs/egs.1.tfrecord'],\r\n                                                                      batch_size=64,\r\n                                                                      feat_dim=20)\r\ninput = tf.expand_dims(input, 1)\r\nconv0 = tf.layers.conv2d(input,\r\n                         512,\r\n                         (1, 5),\r\n                         dilation_rate=(1, 1),\r\n                         activation=tf.nn.relu,\r\n                         name='conv0')\r\n\r\nconv1 = tf.layers.conv2d(conv0,\r\n                         512,\r\n                         (1, 3),\r\n                         dilation_rate=(1, 2),\r\n                         activation=tf.nn.relu,\r\n                         name='conv1')\r\n\r\nconv1_squeeze, _ = tf.nn.moments(tf.squeeze(conv1, [1]), axes=[1])\r\nlogits = tf.layers.dense(conv1_squeeze, 9852, name='logits')\r\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits, scope=\"loss\")\r\ntrain_op = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(train_itr.initializer)\r\n    _ = sess.run(train_op)\r\n    print(\"Here we are\")\r\n```\r\nI load to the tfrecords which are made before. The data loaded has size [length, feat_dim]. With batch, it becomes [batch, length, feat_dim]. And it also works.\r\n\r\nNow, I slightly change the input pipeline to\r\n```\r\ndef create_variable_train_dataset(filenames, batch_size, feat_dim, shuffle_size=-1):\r\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\r\n    dataset = dataset.interleave(lambda filename:\r\n                               tf.data.TFRecordDataset(filename).map(\r\n                                   _parse_tfrecord, num_parallel_calls=8).apply(\r\n                                   tf.contrib.data.padded_batch_and_drop_remainder(\r\n                                       batch_size,\r\n                                       padded_shapes=({'input': [None, feat_dim], 'input_shape': [2], 'output': []}))),\r\n                                cycle_length=len(filenames), block_length=1\r\n                               )\r\n\r\n    dataset = dataset.prefetch(5)\r\n    itr = dataset.make_initializable_iterator()\r\n    element = itr.get_next()\r\n    return itr, element['input'], element['output']\r\n```\r\nIt breaks and throws the exception:\r\n```\r\n2018-04-25 22:07:04.779657: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2\r\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 1530, in <module>\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 937, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"G:/kaldi-master/egs/fisher/v3/xvector/train_model.py\", line 3, in <module>\r\n    from xvector_train import *\r\n  File \"G:/kaldi-master/egs/fisher/v3/xvector\\xvector_train.py\", line 48, in <module>\r\n    _ = sess.run(train_op)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[1]=97 is not divisible by block_shape[1]=2\r\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\r\n\r\nCaused by op 'conv1/SpaceToBatchND', defined at:\r\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 1530, in <module>\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\pydevd.py\", line 937, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm Community Edition 2016.1\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"G:/kaldi-master/egs/fisher/v3/xvector/train_model.py\", line 3, in <module>\r\n    from xvector_train import *\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"G:/kaldi-master/egs/fisher/v3/xvector\\xvector_train.py\", line 36, in <module>\r\n    name='conv1')\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 614, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 762, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 652, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 167, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 838, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 502, in __call__\r\n    return self.call(inp, filter)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 493, in _with_space_to_batch_call\r\n    paddings=paddings)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6670, in space_to_batch_nd\r\n    paddings=paddings, name=name)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): padded_shape[1]=97 is not divisible by block_shape[1]=2\r\n\t [[Node: conv1/SpaceToBatchND = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv0/Relu, conv1/SpaceToBatchND/block_shape, conv1/strided_slice_2)]]\r\n```\r\nAll the examples in the tfrecords have the size [100, 20].The only thing changed is the batch function \"padded_batch_and_drop_remainder\". I don't know why it tell me the dilation operation cannot be performed because padding should be used in the operation. I felt stranger that if I change the dilation rate of the first layer (which is 1 now) to (1, 2), it works again!\r\n```\r\nconv0 = tf.layers.conv2d(input,\r\n                         512,\r\n                         (1, 5),\r\n                         dilation_rate=(1, 2),\r\n                         activation=tf.nn.relu,\r\n                         name='conv0')\r\n```\r\nCould anyone tell me what is going on here? Is anything wrong with the pipeline, or it is a bug ?\r\nI use TF 1.5.0 in a server."}