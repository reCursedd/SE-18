{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/386847638", "html_url": "https://github.com/tensorflow/tensorflow/issues/18862#issuecomment-386847638", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18862", "id": 386847638, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Njg0NzYzOA==", "user": {"login": "mycrazycracy", "id": 29195614, "node_id": "MDQ6VXNlcjI5MTk1NjE0", "avatar_url": "https://avatars0.githubusercontent.com/u/29195614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mycrazycracy", "html_url": "https://github.com/mycrazycracy", "followers_url": "https://api.github.com/users/mycrazycracy/followers", "following_url": "https://api.github.com/users/mycrazycracy/following{/other_user}", "gists_url": "https://api.github.com/users/mycrazycracy/gists{/gist_id}", "starred_url": "https://api.github.com/users/mycrazycracy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mycrazycracy/subscriptions", "organizations_url": "https://api.github.com/users/mycrazycracy/orgs", "repos_url": "https://api.github.com/users/mycrazycracy/repos", "events_url": "https://api.github.com/users/mycrazycracy/events{/privacy}", "received_events_url": "https://api.github.com/users/mycrazycracy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-06T01:53:48Z", "updated_at": "2018-05-06T01:53:48Z", "author_association": "NONE", "body_html": "<p>If padded_batch_and_drop_remainder is used, the shape is [128, ?, 20], which is [batch, length, dim], while padded_batch give me [?, ?, 20]. I think this is expected. The actual length is 100 in the examples and the only difference is the batch function I used.<br>\nThe shapes between input and label seem to be consistent in both conditions.<br>\nI just change padded_batch_and_drop_remainder to padded_batch in my code, and it works well now.</p>", "body_text": "If padded_batch_and_drop_remainder is used, the shape is [128, ?, 20], which is [batch, length, dim], while padded_batch give me [?, ?, 20]. I think this is expected. The actual length is 100 in the examples and the only difference is the batch function I used.\nThe shapes between input and label seem to be consistent in both conditions.\nI just change padded_batch_and_drop_remainder to padded_batch in my code, and it works well now.", "body": "If padded_batch_and_drop_remainder is used, the shape is [128, ?, 20], which is [batch, length, dim], while padded_batch give me [?, ?, 20]. I think this is expected. The actual length is 100 in the examples and the only difference is the batch function I used.\r\nThe shapes between input and label seem to be consistent in both conditions. \r\nI just change padded_batch_and_drop_remainder to padded_batch in my code, and it works well now."}