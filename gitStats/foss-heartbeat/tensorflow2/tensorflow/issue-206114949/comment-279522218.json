{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279522218", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-279522218", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 279522218, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTUyMjIxOA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-13T21:06:17Z", "updated_at": "2017-02-13T21:06:17Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>First, I used shared=True and wrote checkpoint to hdfs, and found only one parameter server had no output without any error occurred, and as you said, the ps stacks are all waiting for more work.</p>\n</blockquote>\n<p>Is it always the same PS task that fails to write a checkpoint? Does it fail on the first attempt to write a checkpoint, or does it fail non-deterministically at some point during training?</p>\n<blockquote>\n<p>Also before saving checkpoint, all workers were timeout even with 100 seconds timeout.</p>\n</blockquote>\n<p>What were the workers doing that timed out?</p>\n<blockquote>\n<p>I am pretty sure that one parameter server can't response any more.</p>\n</blockquote>\n<p>How did you confirm this? Can you try running a step on that PS task, e.g. to fetch a single variable that's held on that task?</p>\n<blockquote>\n<p>Second, each worker is not aware of other workers, they only connect to ps.</p>\n</blockquote>\n<p>Which mechanism do you use for this? (By default, all workers know about all PS tasks <em>and</em> all other worker tasks when you set up the <code>tf.train.ClusterSpec</code>.)</p>\n<blockquote>\n<p>Each step used the same timeout(2 seconds)</p>\n</blockquote>\n<p>Does this include steps that read from a queue-based input pipeline? Or steps that save a checkpoint? It's possible that, if the queues are mostly empty, this timeout could be leaving behind a lot of cancelled attempts in the queue data structures (which are more efficient in the case where timeouts are rare).</p>\n<blockquote>\n<p>Third, when the total worker count is small like 200, it works fine with or without shard=True, but when the worker count is 800 or even 400, cluster will hang with or without shard=True when the training is almost done.</p>\n</blockquote>\n<p>This is strange, because I'd expect the reliability of <code>tf.train.Saver(sharded=True).save()</code> to be a function of the number of PS tasks, and not affected by the number of workers.</p>\n<p>(Out of curiosity, why are you using multiple workers per machine?)</p>\n<blockquote>\n<p>I am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it.</p>\n</blockquote>\n<p>This is true... I don't have easy access to a dedicated cluster of hundreds of Windows machines :). Hopefully we can work with you to figure out the problem and then patch it.</p>", "body_text": "First, I used shared=True and wrote checkpoint to hdfs, and found only one parameter server had no output without any error occurred, and as you said, the ps stacks are all waiting for more work.\n\nIs it always the same PS task that fails to write a checkpoint? Does it fail on the first attempt to write a checkpoint, or does it fail non-deterministically at some point during training?\n\nAlso before saving checkpoint, all workers were timeout even with 100 seconds timeout.\n\nWhat were the workers doing that timed out?\n\nI am pretty sure that one parameter server can't response any more.\n\nHow did you confirm this? Can you try running a step on that PS task, e.g. to fetch a single variable that's held on that task?\n\nSecond, each worker is not aware of other workers, they only connect to ps.\n\nWhich mechanism do you use for this? (By default, all workers know about all PS tasks and all other worker tasks when you set up the tf.train.ClusterSpec.)\n\nEach step used the same timeout(2 seconds)\n\nDoes this include steps that read from a queue-based input pipeline? Or steps that save a checkpoint? It's possible that, if the queues are mostly empty, this timeout could be leaving behind a lot of cancelled attempts in the queue data structures (which are more efficient in the case where timeouts are rare).\n\nThird, when the total worker count is small like 200, it works fine with or without shard=True, but when the worker count is 800 or even 400, cluster will hang with or without shard=True when the training is almost done.\n\nThis is strange, because I'd expect the reliability of tf.train.Saver(sharded=True).save() to be a function of the number of PS tasks, and not affected by the number of workers.\n(Out of curiosity, why are you using multiple workers per machine?)\n\nI am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it.\n\nThis is true... I don't have easy access to a dedicated cluster of hundreds of Windows machines :). Hopefully we can work with you to figure out the problem and then patch it.", "body": "> First, I used shared=True and wrote checkpoint to hdfs, and found only one parameter server had no output without any error occurred, and as you said, the ps stacks are all waiting for more work.\r\n\r\nIs it always the same PS task that fails to write a checkpoint? Does it fail on the first attempt to write a checkpoint, or does it fail non-deterministically at some point during training?\r\n\r\n> Also before saving checkpoint, all workers were timeout even with 100 seconds timeout.\r\n\r\nWhat were the workers doing that timed out?\r\n\r\n> I am pretty sure that one parameter server can't response any more.\r\n\r\nHow did you confirm this? Can you try running a step on that PS task, e.g. to fetch a single variable that's held on that task?\r\n\r\n> Second, each worker is not aware of other workers, they only connect to ps.\r\n\r\nWhich mechanism do you use for this? (By default, all workers know about all PS tasks *and* all other worker tasks when you set up the `tf.train.ClusterSpec`.)\r\n\r\n> Each step used the same timeout(2 seconds)\r\n\r\nDoes this include steps that read from a queue-based input pipeline? Or steps that save a checkpoint? It's possible that, if the queues are mostly empty, this timeout could be leaving behind a lot of cancelled attempts in the queue data structures (which are more efficient in the case where timeouts are rare).\r\n\r\n> Third, when the total worker count is small like 200, it works fine with or without shard=True, but when the worker count is 800 or even 400, cluster will hang with or without shard=True when the training is almost done.\r\n\r\nThis is strange, because I'd expect the reliability of `tf.train.Saver(sharded=True).save()` to be a function of the number of PS tasks, and not affected by the number of workers.\r\n\r\n (Out of curiosity, why are you using multiple workers per machine?)\r\n\r\n> I am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it.\r\n\r\nThis is true... I don't have easy access to a dedicated cluster of hundreds of Windows machines :). Hopefully we can work with you to figure out the problem and then patch it."}