{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279292933", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-279292933", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 279292933, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTI5MjkzMw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-13T04:37:25Z", "updated_at": "2017-02-13T04:37:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23738439\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fesun\">@fesun</a> Thanks for sending over the thread stacks. Unfortunately, there doesn't seem to be anything informative in the parameter server stack... all threads appear to be blocked waiting for more work (or in <code>tf.Server.join()</code>).</p>\n<p>The most important observation seems to be that creating a <code>tf.train.Saver</code> with <code>sharded=True</code> is a more stable configuration than <code>sharded=False</code>, especially when the total parameter size is large. You mentioned that you saw a failure when writing with <code>sharded=True</code> to HDFS, so it's possible that there's a bug in the HDFS layer, and it might be worth instrumenting that code to see if an unhandled failure is occurring. Does \"failed to output checkpoint\" mean \"<code>saver.save()</code> returned with no exception and yet it didn't write a file it should have\"?</p>\n<p>[As an aside, it looks like I might have found a bug in <code>SaveV2</code>, which doesn't check the <a href=\"https://github.com/tensorflow/tensorflow/blob/9949045460ffb7bfc037f4b3a5396325791e0f09/tensorflow/core/kernels/save_restore_v2_ops.cc#L131\">error status from <code>writer.Add()</code></a>. This could potentially lead to a silent failure if the underlying write fails. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=592670\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/concretevitamin\">@concretevitamin</a>, can you take a look at this and confirm?]</p>\n<p>Beyond that, I don't think we know enough about your program to know where the problem might lie. There are a lot of details\u2014surrounding the setup of the parameters and the saver, the coordination between the workers, and the use of timeouts\u2014that are unclear to me. Can you create a minimal example that exhibits the same problems and share the code for that? With some code in black-and-white, we should be able to provide more help with tracking down the problem.</p>", "body_text": "@fesun Thanks for sending over the thread stacks. Unfortunately, there doesn't seem to be anything informative in the parameter server stack... all threads appear to be blocked waiting for more work (or in tf.Server.join()).\nThe most important observation seems to be that creating a tf.train.Saver with sharded=True is a more stable configuration than sharded=False, especially when the total parameter size is large. You mentioned that you saw a failure when writing with sharded=True to HDFS, so it's possible that there's a bug in the HDFS layer, and it might be worth instrumenting that code to see if an unhandled failure is occurring. Does \"failed to output checkpoint\" mean \"saver.save() returned with no exception and yet it didn't write a file it should have\"?\n[As an aside, it looks like I might have found a bug in SaveV2, which doesn't check the error status from writer.Add(). This could potentially lead to a silent failure if the underlying write fails. @concretevitamin, can you take a look at this and confirm?]\nBeyond that, I don't think we know enough about your program to know where the problem might lie. There are a lot of details\u2014surrounding the setup of the parameters and the saver, the coordination between the workers, and the use of timeouts\u2014that are unclear to me. Can you create a minimal example that exhibits the same problems and share the code for that? With some code in black-and-white, we should be able to provide more help with tracking down the problem.", "body": "@fesun Thanks for sending over the thread stacks. Unfortunately, there doesn't seem to be anything informative in the parameter server stack... all threads appear to be blocked waiting for more work (or in `tf.Server.join()`).\r\n\r\nThe most important observation seems to be that creating a `tf.train.Saver` with `sharded=True` is a more stable configuration than `sharded=False`, especially when the total parameter size is large. You mentioned that you saw a failure when writing with `sharded=True` to HDFS, so it's possible that there's a bug in the HDFS layer, and it might be worth instrumenting that code to see if an unhandled failure is occurring. Does \"failed to output checkpoint\" mean \"`saver.save()` returned with no exception and yet it didn't write a file it should have\"?\r\n\r\n[As an aside, it looks like I might have found a bug in `SaveV2`, which doesn't check the [error status from `writer.Add()`](https://github.com/tensorflow/tensorflow/blob/9949045460ffb7bfc037f4b3a5396325791e0f09/tensorflow/core/kernels/save_restore_v2_ops.cc#L131). This could potentially lead to a silent failure if the underlying write fails. @concretevitamin, can you take a look at this and confirm?]\r\n\r\nBeyond that, I don't think we know enough about your program to know where the problem might lie. There are a lot of details&mdash;surrounding the setup of the parameters and the saver, the coordination between the workers, and the use of timeouts&mdash;that are unclear to me. Can you create a minimal example that exhibits the same problems and share the code for that? With some code in black-and-white, we should be able to provide more help with tracking down the problem."}