{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279926775", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-279926775", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 279926775, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTkyNjc3NQ==", "user": {"login": "fesun", "id": 23738439, "node_id": "MDQ6VXNlcjIzNzM4NDM5", "avatar_url": "https://avatars3.githubusercontent.com/u/23738439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fesun", "html_url": "https://github.com/fesun", "followers_url": "https://api.github.com/users/fesun/followers", "following_url": "https://api.github.com/users/fesun/following{/other_user}", "gists_url": "https://api.github.com/users/fesun/gists{/gist_id}", "starred_url": "https://api.github.com/users/fesun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fesun/subscriptions", "organizations_url": "https://api.github.com/users/fesun/orgs", "repos_url": "https://api.github.com/users/fesun/repos", "events_url": "https://api.github.com/users/fesun/events{/privacy}", "received_events_url": "https://api.github.com/users/fesun/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-15T06:18:03Z", "updated_at": "2017-02-15T06:18:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a></p>\n<blockquote>\n<p><code>Second, each worker is not aware of other workers, they only connect to ps.</code><br>\nWhich mechanism do you use for this? (By default, all workers know about all PS tasks and all other worker tasks when you set up the tf.train.ClusterSpec.)</p>\n</blockquote>\n<p>A: only provide all ps servers and itself when starting worker server:<br>\n<code>cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": {task_index: worker_hosts[task_index]}})</code>, also tensorflow provide device_filters when constructing ConfigProto:<br>\n<code>tf.ConfigProto(device_filters=[\"/job:ps\", \"/job:worker/task:%d\" %  LAGS.task_index])</code></p>\n<blockquote>\n<p>Does this include steps that read from a queue-based input pipeline? Or steps that save a checkpoint? It's possible that, if the queues are mostly empty, this timeout could be leaving behind a lot of cancelled attempts in the queue data structures (which are more efficient in the case where timeouts are rare).</p>\n</blockquote>\n<p>A: the only queue I was using is file name queue, and here is the data reading code:</p>\n<pre><code>filename_queuetf.train.string_input_producer([data_file_name], num_epochs=FLAGS.iteration)\n_, batch_example = tf.TFRecordReader().read_up_to(filename_queue, batch_size)\nmy_data = user_ops.my_own_parsing_operator(batch_example)\n</code></pre>\n<p>So I am leveraging tfrecord to read data from file, parse it using my own parser after getting raw sample data. I don't think it's queue issue.</p>\n<blockquote>\n<p>(Out of curiosity, why are you using multiple workers per machine?)</p>\n</blockquote>\n<p>A: My neural network is very simple, it's not CPU bottleneck, increasing worker count will better utilize resources and increase throughput. I also tried python multi-thread version by using the same session and different session each thread, but didn't work well, multi-thread can not scale out well.</p>\n<p>I will reproduce this issue and dump tcp packet to see if I can find something. Will update later.</p>", "body_text": "@mrry\n\nSecond, each worker is not aware of other workers, they only connect to ps.\nWhich mechanism do you use for this? (By default, all workers know about all PS tasks and all other worker tasks when you set up the tf.train.ClusterSpec.)\n\nA: only provide all ps servers and itself when starting worker server:\ncluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": {task_index: worker_hosts[task_index]}}), also tensorflow provide device_filters when constructing ConfigProto:\ntf.ConfigProto(device_filters=[\"/job:ps\", \"/job:worker/task:%d\" %  LAGS.task_index])\n\nDoes this include steps that read from a queue-based input pipeline? Or steps that save a checkpoint? It's possible that, if the queues are mostly empty, this timeout could be leaving behind a lot of cancelled attempts in the queue data structures (which are more efficient in the case where timeouts are rare).\n\nA: the only queue I was using is file name queue, and here is the data reading code:\nfilename_queuetf.train.string_input_producer([data_file_name], num_epochs=FLAGS.iteration)\n_, batch_example = tf.TFRecordReader().read_up_to(filename_queue, batch_size)\nmy_data = user_ops.my_own_parsing_operator(batch_example)\n\nSo I am leveraging tfrecord to read data from file, parse it using my own parser after getting raw sample data. I don't think it's queue issue.\n\n(Out of curiosity, why are you using multiple workers per machine?)\n\nA: My neural network is very simple, it's not CPU bottleneck, increasing worker count will better utilize resources and increase throughput. I also tried python multi-thread version by using the same session and different session each thread, but didn't work well, multi-thread can not scale out well.\nI will reproduce this issue and dump tcp packet to see if I can find something. Will update later.", "body": "@mrry \r\n> `Second, each worker is not aware of other workers, they only connect to ps.`\r\nWhich mechanism do you use for this? (By default, all workers know about all PS tasks and all other worker tasks when you set up the tf.train.ClusterSpec.)\r\n\r\nA: only provide all ps servers and itself when starting worker server:\r\n`cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": {task_index: worker_hosts[task_index]}})`, also tensorflow provide device_filters when constructing ConfigProto:\r\n`tf.ConfigProto(device_filters=[\"/job:ps\", \"/job:worker/task:%d\" %  LAGS.task_index])`\r\n\r\n>Does this include steps that read from a queue-based input pipeline? Or steps that save a checkpoint? It's possible that, if the queues are mostly empty, this timeout could be leaving behind a lot of cancelled attempts in the queue data structures (which are more efficient in the case where timeouts are rare).\r\n\r\nA: the only queue I was using is file name queue, and here is the data reading code:\r\n```\r\nfilename_queuetf.train.string_input_producer([data_file_name], num_epochs=FLAGS.iteration)\r\n_, batch_example = tf.TFRecordReader().read_up_to(filename_queue, batch_size)\r\nmy_data = user_ops.my_own_parsing_operator(batch_example)\r\n```\r\nSo I am leveraging tfrecord to read data from file, parse it using my own parser after getting raw sample data. I don't think it's queue issue.\r\n\r\n>(Out of curiosity, why are you using multiple workers per machine?)\r\n\r\nA: My neural network is very simple, it's not CPU bottleneck, increasing worker count will better utilize resources and increase throughput. I also tried python multi-thread version by using the same session and different session each thread, but didn't work well, multi-thread can not scale out well.\r\n\r\nI will reproduce this issue and dump tcp packet to see if I can find something. Will update later."}