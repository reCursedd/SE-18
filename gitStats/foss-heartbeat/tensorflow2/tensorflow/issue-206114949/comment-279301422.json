{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279301422", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-279301422", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 279301422, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTMwMTQyMg==", "user": {"login": "fesun", "id": 23738439, "node_id": "MDQ6VXNlcjIzNzM4NDM5", "avatar_url": "https://avatars3.githubusercontent.com/u/23738439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fesun", "html_url": "https://github.com/fesun", "followers_url": "https://api.github.com/users/fesun/followers", "following_url": "https://api.github.com/users/fesun/following{/other_user}", "gists_url": "https://api.github.com/users/fesun/gists{/gist_id}", "starred_url": "https://api.github.com/users/fesun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fesun/subscriptions", "organizations_url": "https://api.github.com/users/fesun/orgs", "repos_url": "https://api.github.com/users/fesun/repos", "events_url": "https://api.github.com/users/fesun/events{/privacy}", "received_events_url": "https://api.github.com/users/fesun/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-13T06:03:25Z", "updated_at": "2017-02-13T06:03:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> thanks for your responses.<br>\nFirst, I used shared=True and wrote checkpoint to hdfs, and found only one parameter server had no output without any error occurred, and as you said, the ps stacks are all waiting for more work. Also before saving checkpoint, all workers were timeout even with 100 seconds timeout. I am pretty sure that one parameter server can't response any more.<br>\nSecond, each worker is not aware of other workers, they only connect to ps. Each step used the same timeout(2 seconds)<br>\nThird, when the total worker count is small like 200, it works fine with or without shard=True, but when the worker count is 800 or even 400, cluster will hang with or without shard=True when the training is almost done. I am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it.</p>", "body_text": "@mrry thanks for your responses.\nFirst, I used shared=True and wrote checkpoint to hdfs, and found only one parameter server had no output without any error occurred, and as you said, the ps stacks are all waiting for more work. Also before saving checkpoint, all workers were timeout even with 100 seconds timeout. I am pretty sure that one parameter server can't response any more.\nSecond, each worker is not aware of other workers, they only connect to ps. Each step used the same timeout(2 seconds)\nThird, when the total worker count is small like 200, it works fine with or without shard=True, but when the worker count is 800 or even 400, cluster will hang with or without shard=True when the training is almost done. I am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it.", "body": "@mrry thanks for your responses.\r\nFirst, I used shared=True and wrote checkpoint to hdfs, and found only one parameter server had no output without any error occurred, and as you said, the ps stacks are all waiting for more work. Also before saving checkpoint, all workers were timeout even with 100 seconds timeout. I am pretty sure that one parameter server can't response any more. \r\nSecond, each worker is not aware of other workers, they only connect to ps. Each step used the same timeout(2 seconds)\r\nThird, when the total worker count is small like 200, it works fine with or without shard=True, but when the worker count is 800 or even 400, cluster will hang with or without shard=True when the training is almost done. I am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it.\r\n\r\n"}