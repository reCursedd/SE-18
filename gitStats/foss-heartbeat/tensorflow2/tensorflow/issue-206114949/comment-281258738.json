{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281258738", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-281258738", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 281258738, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTI1ODczOA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T06:42:48Z", "updated_at": "2017-02-21T06:42:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23738439\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fesun\">@fesun</a> Thanks for digging into the <code>ENQUEUE_REQUEST()</code> issue. This seems like a bug that only occurs at higher pending request counts (e.g. when we have a larger cluster of workers, all contacting the same PS task), but it doesn't seem to trigger deterministically (e.g. when the number of enqueued requests drops to zero).</p>\n<p>Perhaps somebody on the gRPC team has a hunch.... <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10120821\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ctiller\">@ctiller</a>, you were super-helpful when we were writing the original async code for TensorFlow on gRPC, and I recall you mentioning that there could be a negative impact on performance if the number of enqueued requests for a method ever dropped to zero. Can you think of a reason why a request might hang indefinitely in this regime? (It would be great to find a simple repro that doesn't involve running 800 processes....) Perhaps I made some fundamentally incorrect assumption about how the async code works?</p>", "body_text": "@fesun Thanks for digging into the ENQUEUE_REQUEST() issue. This seems like a bug that only occurs at higher pending request counts (e.g. when we have a larger cluster of workers, all contacting the same PS task), but it doesn't seem to trigger deterministically (e.g. when the number of enqueued requests drops to zero).\nPerhaps somebody on the gRPC team has a hunch.... @ctiller, you were super-helpful when we were writing the original async code for TensorFlow on gRPC, and I recall you mentioning that there could be a negative impact on performance if the number of enqueued requests for a method ever dropped to zero. Can you think of a reason why a request might hang indefinitely in this regime? (It would be great to find a simple repro that doesn't involve running 800 processes....) Perhaps I made some fundamentally incorrect assumption about how the async code works?", "body": "@fesun Thanks for digging into the `ENQUEUE_REQUEST()` issue. This seems like a bug that only occurs at higher pending request counts (e.g. when we have a larger cluster of workers, all contacting the same PS task), but it doesn't seem to trigger deterministically (e.g. when the number of enqueued requests drops to zero).\r\n\r\nPerhaps somebody on the gRPC team has a hunch.... @ctiller, you were super-helpful when we were writing the original async code for TensorFlow on gRPC, and I recall you mentioning that there could be a negative impact on performance if the number of enqueued requests for a method ever dropped to zero. Can you think of a reason why a request might hang indefinitely in this regime? (It would be great to find a simple repro that doesn't involve running 800 processes....) Perhaps I made some fundamentally incorrect assumption about how the async code works?"}