{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279797882", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-279797882", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 279797882, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTc5Nzg4Mg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-14T18:47:52Z", "updated_at": "2017-02-14T18:49:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17514649\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shishaochen\">@shishaochen</a></p>\n<ol>\n<li>\n<p>Google uses stubby while open-source version uses grpc which still has some obvious performance problems (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"193706605\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6116\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6116/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6116\">#6116</a>). I've been told that Google plans to transition to gRPC internally, so that may eliminate the gap eventually.</p>\n</li>\n<li>\n<p>When I trained image models in Google and outside I have not seen communication (minus the gRPC issues) being the main bottleneck. The reason is that distributed training tends to be done using parameter server architecture which scales badly for reasons unrelated to hardware. You can actually hurt your accuracy by increasing hardware resources.</p>\n</li>\n</ol>\n<p>For asynchronous updates, imagine having 50 replicas updating parameters asynchronously. This means each replica sees parameters which are stale by 49 steps. As you increase number of workers, the staleness increases even if network communication is instant. One ImageNet experiment I've seen attained best accuracy for 50 replicas, and then accuracy went down after that. Synchronous training does better for same replica count, but exhibits similar diminishing returns as you increase number of replicas.</p>\n<p>You may have better scaling using model parallelism, but this means using larger model, and larger models take longer to train, which is too long given that some of the smaller models already get trained for 6 months. For the largest image models/data-sizes, I would say a few hundred GPU machines is an upper bound on the amount of resources you can efficiently utilize using parameter server.</p>", "body_text": "@shishaochen\n\n\nGoogle uses stubby while open-source version uses grpc which still has some obvious performance problems (#6116). I've been told that Google plans to transition to gRPC internally, so that may eliminate the gap eventually.\n\n\nWhen I trained image models in Google and outside I have not seen communication (minus the gRPC issues) being the main bottleneck. The reason is that distributed training tends to be done using parameter server architecture which scales badly for reasons unrelated to hardware. You can actually hurt your accuracy by increasing hardware resources.\n\n\nFor asynchronous updates, imagine having 50 replicas updating parameters asynchronously. This means each replica sees parameters which are stale by 49 steps. As you increase number of workers, the staleness increases even if network communication is instant. One ImageNet experiment I've seen attained best accuracy for 50 replicas, and then accuracy went down after that. Synchronous training does better for same replica count, but exhibits similar diminishing returns as you increase number of replicas.\nYou may have better scaling using model parallelism, but this means using larger model, and larger models take longer to train, which is too long given that some of the smaller models already get trained for 6 months. For the largest image models/data-sizes, I would say a few hundred GPU machines is an upper bound on the amount of resources you can efficiently utilize using parameter server.", "body": "@shishaochen \r\n1. Google uses stubby while open-source version uses grpc which still has some obvious performance problems (https://github.com/tensorflow/tensorflow/issues/6116). I've been told that Google plans to transition to gRPC internally, so that may eliminate the gap eventually.\r\n\r\n2. When I trained image models in Google and outside I have not seen communication (minus the gRPC issues) being the main bottleneck. The reason is that distributed training tends to be done using parameter server architecture which scales badly for reasons unrelated to hardware. You can actually hurt your accuracy by increasing hardware resources.\r\n\r\nFor asynchronous updates, imagine having 50 replicas updating parameters asynchronously. This means each replica sees parameters which are stale by 49 steps. As you increase number of workers, the staleness increases even if network communication is instant. One ImageNet experiment I've seen attained best accuracy for 50 replicas, and then accuracy went down after that. Synchronous training does better for same replica count, but exhibits similar diminishing returns as you increase number of replicas.\r\n\r\nYou may have better scaling using model parallelism, but this means using larger model, and larger models take longer to train, which is too long given that some of the smaller models already get trained for 6 months. For the largest image models/data-sizes, I would say a few hundred GPU machines is an upper bound on the amount of resources you can efficiently utilize using parameter server."}