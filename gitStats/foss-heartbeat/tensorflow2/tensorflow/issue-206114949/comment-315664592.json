{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/315664592", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-315664592", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 315664592, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTY2NDU5Mg==", "user": {"login": "fesun", "id": 23738439, "node_id": "MDQ6VXNlcjIzNzM4NDM5", "avatar_url": "https://avatars3.githubusercontent.com/u/23738439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fesun", "html_url": "https://github.com/fesun", "followers_url": "https://api.github.com/users/fesun/followers", "following_url": "https://api.github.com/users/fesun/following{/other_user}", "gists_url": "https://api.github.com/users/fesun/gists{/gist_id}", "starred_url": "https://api.github.com/users/fesun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fesun/subscriptions", "organizations_url": "https://api.github.com/users/fesun/orgs", "repos_url": "https://api.github.com/users/fesun/repos", "events_url": "https://api.github.com/users/fesun/events{/privacy}", "received_events_url": "https://api.github.com/users/fesun/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-17T04:04:32Z", "updated_at": "2017-07-17T04:04:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2613663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/byronyi\">@byronyi</a> The tensor size is not large, smaller than 1M, what I want to highlight is the time when tensorflow hangs. <strong><em>It happens only when some workers finished training and waiting for others to finish using while loop by checking one finish_count variable</em></strong>. The phenomenon is that sess.run() didn't return within 20 seconds timeout when some workers finished without timeout exception, at last I will call my custom saving operator(master worker call sess.run([my_saving]) without timeout setting) and all the parameter server wrote models successfully, but sess.run() never returned. Increasing those three magic numbers do solve my problem temporary, but my scenario is this: I have dozens of files to process, I start parameter servers once, restart workers for each file, when I increase magic number to 3000, with 500 workers, tensorflow hangs after about 6-7 restart, increasing to 5000, hangs after about 10 restart. Seems that each worker will occupy one request object and not reusable. I believe this grpc implementation definitely has some wrong and probably platform independent.</p>", "body_text": "@byronyi The tensor size is not large, smaller than 1M, what I want to highlight is the time when tensorflow hangs. It happens only when some workers finished training and waiting for others to finish using while loop by checking one finish_count variable. The phenomenon is that sess.run() didn't return within 20 seconds timeout when some workers finished without timeout exception, at last I will call my custom saving operator(master worker call sess.run([my_saving]) without timeout setting) and all the parameter server wrote models successfully, but sess.run() never returned. Increasing those three magic numbers do solve my problem temporary, but my scenario is this: I have dozens of files to process, I start parameter servers once, restart workers for each file, when I increase magic number to 3000, with 500 workers, tensorflow hangs after about 6-7 restart, increasing to 5000, hangs after about 10 restart. Seems that each worker will occupy one request object and not reusable. I believe this grpc implementation definitely has some wrong and probably platform independent.", "body": "@byronyi The tensor size is not large, smaller than 1M, what I want to highlight is the time when tensorflow hangs. **_It happens only when some workers finished training and waiting for others to finish using while loop by checking one finish_count variable_**. The phenomenon is that sess.run() didn't return within 20 seconds timeout when some workers finished without timeout exception, at last I will call my custom saving operator(master worker call sess.run([my_saving]) without timeout setting) and all the parameter server wrote models successfully, but sess.run() never returned. Increasing those three magic numbers do solve my problem temporary, but my scenario is this: I have dozens of files to process, I start parameter servers once, restart workers for each file, when I increase magic number to 3000, with 500 workers, tensorflow hangs after about 6-7 restart, increasing to 5000, hangs after about 10 restart. Seems that each worker will occupy one request object and not reusable. I believe this grpc implementation definitely has some wrong and probably platform independent."}