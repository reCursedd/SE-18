{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280059555", "html_url": "https://github.com/tensorflow/tensorflow/issues/7353#issuecomment-280059555", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7353", "id": 280059555, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDA1OTU1NQ==", "user": {"login": "fesun", "id": 23738439, "node_id": "MDQ6VXNlcjIzNzM4NDM5", "avatar_url": "https://avatars3.githubusercontent.com/u/23738439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fesun", "html_url": "https://github.com/fesun", "followers_url": "https://api.github.com/users/fesun/followers", "following_url": "https://api.github.com/users/fesun/following{/other_user}", "gists_url": "https://api.github.com/users/fesun/gists{/gist_id}", "starred_url": "https://api.github.com/users/fesun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fesun/subscriptions", "organizations_url": "https://api.github.com/users/fesun/orgs", "repos_url": "https://api.github.com/users/fesun/repos", "events_url": "https://api.github.com/users/fesun/events{/privacy}", "received_events_url": "https://api.github.com/users/fesun/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-15T16:26:10Z", "updated_at": "2017-02-15T16:26:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a><br>\nI changed the code in tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc:Line124 from:</p>\n<pre><code>    // number of each request type.\n    for (int i = 0; i &lt; 1000; ++i) {\n      EnqueueRecvTensorRequestRaw();\n    }\n    for (int i = 0; i &lt; 100; ++i) {\n      ENQUEUE_REQUEST(RunGraph, true);\n    }\n    for (int i = 0; i &lt; 100; ++i) {\n      ENQUEUE_REQUEST(CleanupGraph, false);\n    }\n</code></pre>\n<p>to</p>\n<pre><code>    for (int i = 0; i &lt; 5000; ++i) {\n      EnqueueRecvTensorRequestRaw();\n    }\n    for (int i = 0; i &lt; 2000; ++i) {\n      ENQUEUE_REQUEST(RunGraph, true);\n    }\n    for (int i = 0; i &lt; 2000; ++i) {\n      ENQUEUE_REQUEST(CleanupGraph, false);\n    }\n</code></pre>\n<p>This change solved the hang issue. Now with total 400 workers, no timeout error occurred and writing checkpoint finished successfully, I verified it by four running.<br>\nI don't understand why this could solve this problem, is there any other performance issues when the cluster is large?</p>\n<p>here is the skeleton of worker code:</p>\n<pre><code>build graph and create session using tf supervisor\n......\ntry:\n    while not sv.should_stop():\n        sess.run()\nexcept tf.errors.OutOfRangeError:\n    print(\"End of file\")\n\n# increase finish_count variable by one\nsess.run(increase_finish_count_op)\n\nif is_master_worker:\n    # wait for all workers finish\n    while True:\n        if sess.run(finish_count_variable) &lt; total_worker_count:\n            time.sleep(30)\n        else:\n            break\n    # write checkpoint to hdfs\n    saver.save()\n    # write exit_signal variables to notify all other workers to exit\n    sess.run(increase_exit_signal_op)\nelse:\n    while True:\n        if sess.run(exit_signal_variable) != 0:\n            time.sleep(30)\n        else:\n            break\n</code></pre>", "body_text": "@mrry\nI changed the code in tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc:Line124 from:\n    // number of each request type.\n    for (int i = 0; i < 1000; ++i) {\n      EnqueueRecvTensorRequestRaw();\n    }\n    for (int i = 0; i < 100; ++i) {\n      ENQUEUE_REQUEST(RunGraph, true);\n    }\n    for (int i = 0; i < 100; ++i) {\n      ENQUEUE_REQUEST(CleanupGraph, false);\n    }\n\nto\n    for (int i = 0; i < 5000; ++i) {\n      EnqueueRecvTensorRequestRaw();\n    }\n    for (int i = 0; i < 2000; ++i) {\n      ENQUEUE_REQUEST(RunGraph, true);\n    }\n    for (int i = 0; i < 2000; ++i) {\n      ENQUEUE_REQUEST(CleanupGraph, false);\n    }\n\nThis change solved the hang issue. Now with total 400 workers, no timeout error occurred and writing checkpoint finished successfully, I verified it by four running.\nI don't understand why this could solve this problem, is there any other performance issues when the cluster is large?\nhere is the skeleton of worker code:\nbuild graph and create session using tf supervisor\n......\ntry:\n    while not sv.should_stop():\n        sess.run()\nexcept tf.errors.OutOfRangeError:\n    print(\"End of file\")\n\n# increase finish_count variable by one\nsess.run(increase_finish_count_op)\n\nif is_master_worker:\n    # wait for all workers finish\n    while True:\n        if sess.run(finish_count_variable) < total_worker_count:\n            time.sleep(30)\n        else:\n            break\n    # write checkpoint to hdfs\n    saver.save()\n    # write exit_signal variables to notify all other workers to exit\n    sess.run(increase_exit_signal_op)\nelse:\n    while True:\n        if sess.run(exit_signal_variable) != 0:\n            time.sleep(30)\n        else:\n            break", "body": "@mrry \r\nI changed the code in tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc:Line124 from:\r\n```\r\n    // number of each request type.\r\n    for (int i = 0; i < 1000; ++i) {\r\n      EnqueueRecvTensorRequestRaw();\r\n    }\r\n    for (int i = 0; i < 100; ++i) {\r\n      ENQUEUE_REQUEST(RunGraph, true);\r\n    }\r\n    for (int i = 0; i < 100; ++i) {\r\n      ENQUEUE_REQUEST(CleanupGraph, false);\r\n    }\r\n```\r\n\r\nto\r\n\r\n```\r\n    for (int i = 0; i < 5000; ++i) {\r\n      EnqueueRecvTensorRequestRaw();\r\n    }\r\n    for (int i = 0; i < 2000; ++i) {\r\n      ENQUEUE_REQUEST(RunGraph, true);\r\n    }\r\n    for (int i = 0; i < 2000; ++i) {\r\n      ENQUEUE_REQUEST(CleanupGraph, false);\r\n    }\r\n```\r\nThis change solved the hang issue. Now with total 400 workers, no timeout error occurred and writing checkpoint finished successfully, I verified it by four running.\r\nI don't understand why this could solve this problem, is there any other performance issues when the cluster is large?\r\n\r\nhere is the skeleton of worker code:\r\n```\r\nbuild graph and create session using tf supervisor\r\n......\r\ntry:\r\n    while not sv.should_stop():\r\n        sess.run()\r\nexcept tf.errors.OutOfRangeError:\r\n    print(\"End of file\")\r\n\r\n# increase finish_count variable by one\r\nsess.run(increase_finish_count_op)\r\n\r\nif is_master_worker:\r\n    # wait for all workers finish\r\n    while True:\r\n        if sess.run(finish_count_variable) < total_worker_count:\r\n            time.sleep(30)\r\n        else:\r\n            break\r\n    # write checkpoint to hdfs\r\n    saver.save()\r\n    # write exit_signal variables to notify all other workers to exit\r\n    sess.run(increase_exit_signal_op)\r\nelse:\r\n    while True:\r\n        if sess.run(exit_signal_variable) != 0:\r\n            time.sleep(30)\r\n        else:\r\n            break\r\n```"}