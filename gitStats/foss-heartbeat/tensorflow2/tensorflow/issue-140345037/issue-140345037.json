{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1470", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1470/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1470/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1470/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1470", "id": 140345037, "node_id": "MDU6SXNzdWUxNDAzNDUwMzc=", "number": 1470, "title": "Error when run optimizer.compute_gradients", "user": {"login": "gliese581gg", "id": 15100232, "node_id": "MDQ6VXNlcjE1MTAwMjMy", "avatar_url": "https://avatars0.githubusercontent.com/u/15100232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gliese581gg", "html_url": "https://github.com/gliese581gg", "followers_url": "https://api.github.com/users/gliese581gg/followers", "following_url": "https://api.github.com/users/gliese581gg/following{/other_user}", "gists_url": "https://api.github.com/users/gliese581gg/gists{/gist_id}", "starred_url": "https://api.github.com/users/gliese581gg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gliese581gg/subscriptions", "organizations_url": "https://api.github.com/users/gliese581gg/orgs", "repos_url": "https://api.github.com/users/gliese581gg/repos", "events_url": "https://api.github.com/users/gliese581gg/events{/privacy}", "received_events_url": "https://api.github.com/users/gliese581gg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-03-12T04:15:28Z", "updated_at": "2017-04-25T04:03:35Z", "closed_at": "2016-03-14T16:57:42Z", "author_association": "NONE", "body_html": "<p>I'm trying to do training with multiple threads</p>\n<p>I tried to run optimizer.minimize() in each threads but it didn't work because threads overwrited each other's weight updates. So I'm trying to stack computed gradients in each threads and apply mean or sum of them in another thread.<br>\nSo I made a code like</p>\n<p>w1 = tf.Variable(tf.random_normal([shape], stddev=0.01))<br>\nb1 = tf.Variable(tf.constant(0.1, shape=[filters]))<br>\nrmsprop = tf.train.RMSPropOptimizer(lr,decay,0.0,eps)<br>\ngrads = rmsprop.compute_gradients(cost,[w1,b1])<br>\napply_grads = rmsprop.apply_gradients(grads)</p>\n<p>If i run apply_grads, it works fine. but when i run grads to get gradients, error occurs like below.<br>\n<code>[(&lt;tf.Tensor ~~ shape=~~ dtype=float32&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350&gt;)] of [(&lt;tf.Tensor ~~ shape=~~ dtype=float32&gt;, &lt;tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350&gt;)]  has invalid type &lt;type \"list\"&gt;, must be a string or Tensor. (Can not convert a list into a Tensor or Operation.)</code></p>\n<p>How can I get gradients for optimizer?</p>\n<p>Or is there a efficient way to do training with multiple threads?</p>", "body_text": "I'm trying to do training with multiple threads\nI tried to run optimizer.minimize() in each threads but it didn't work because threads overwrited each other's weight updates. So I'm trying to stack computed gradients in each threads and apply mean or sum of them in another thread.\nSo I made a code like\nw1 = tf.Variable(tf.random_normal([shape], stddev=0.01))\nb1 = tf.Variable(tf.constant(0.1, shape=[filters]))\nrmsprop = tf.train.RMSPropOptimizer(lr,decay,0.0,eps)\ngrads = rmsprop.compute_gradients(cost,[w1,b1])\napply_grads = rmsprop.apply_gradients(grads)\nIf i run apply_grads, it works fine. but when i run grads to get gradients, error occurs like below.\n[(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)] of [(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)]  has invalid type <type \"list\">, must be a string or Tensor. (Can not convert a list into a Tensor or Operation.)\nHow can I get gradients for optimizer?\nOr is there a efficient way to do training with multiple threads?", "body": "I'm trying to do training with multiple threads\n\nI tried to run optimizer.minimize() in each threads but it didn't work because threads overwrited each other's weight updates. So I'm trying to stack computed gradients in each threads and apply mean or sum of them in another thread.\nSo I made a code like\n\nw1 = tf.Variable(tf.random_normal([shape], stddev=0.01))\nb1 = tf.Variable(tf.constant(0.1, shape=[filters]))\nrmsprop = tf.train.RMSPropOptimizer(lr,decay,0.0,eps)\ngrads = rmsprop.compute_gradients(cost,[w1,b1])\napply_grads = rmsprop.apply_gradients(grads)\n\nIf i run apply_grads, it works fine. but when i run grads to get gradients, error occurs like below.\n`[(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)] of [(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)]  has invalid type <type \"list\">, must be a string or Tensor. (Can not convert a list into a Tensor or Operation.)`\n\nHow can I get gradients for optimizer?\n\nOr is there a efficient way to do training with multiple threads?\n"}