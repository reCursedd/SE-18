{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2577", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2577/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2577/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2577/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2577", "id": 157461978, "node_id": "MDU6SXNzdWUxNTc0NjE5Nzg=", "number": 2577, "title": "Support complex numbers in Tensor Transformation Operations", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rryan", "id": 26527, "node_id": "MDQ6VXNlcjI2NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/26527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rryan", "html_url": "https://github.com/rryan", "followers_url": "https://api.github.com/users/rryan/followers", "following_url": "https://api.github.com/users/rryan/following{/other_user}", "gists_url": "https://api.github.com/users/rryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rryan/subscriptions", "organizations_url": "https://api.github.com/users/rryan/orgs", "repos_url": "https://api.github.com/users/rryan/repos", "events_url": "https://api.github.com/users/rryan/events{/privacy}", "received_events_url": "https://api.github.com/users/rryan/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rryan", "id": 26527, "node_id": "MDQ6VXNlcjI2NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/26527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rryan", "html_url": "https://github.com/rryan", "followers_url": "https://api.github.com/users/rryan/followers", "following_url": "https://api.github.com/users/rryan/following{/other_user}", "gists_url": "https://api.github.com/users/rryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rryan/subscriptions", "organizations_url": "https://api.github.com/users/rryan/orgs", "repos_url": "https://api.github.com/users/rryan/repos", "events_url": "https://api.github.com/users/rryan/events{/privacy}", "received_events_url": "https://api.github.com/users/rryan/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-05-30T09:07:35Z", "updated_at": "2017-06-16T17:51:55Z", "closed_at": "2017-06-16T17:51:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A number of changes have been made to support complex128, but the the tensor transformation op suite seems to be missing this support (At least in tf.pack and tf.concat). See the example script below which works for float32 and float64, but not complex64 and complex128.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04.4</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ ls -l /usr/local/cuda-7.5/lib64/libcud<span class=\"pl-k\">*</span>\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -<span class=\"pl-k\">&gt;</span> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -<span class=\"pl-k\">&gt;</span> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so -<span class=\"pl-k\">&gt;</span> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so.4 -<span class=\"pl-k\">&gt;</span> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn_static.a</pre></div>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed. <strong>Python 2 GPU nightly</strong></li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\". <strong>0.8.0</strong></li>\n</ol>\n<p>If installed from sources, provide the commit hash:</p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>\n<p>Vary <strong>dtype</strong> in the following test script between <code>float32, float64, complex64, complex128</code></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Data type for this test case</span>\ndtype<span class=\"pl-k\">=</span>np.float64\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Array shapes</span>\nshape<span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">30</span>,<span class=\"pl-c1\">1</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Expected concatenated array shape</span>\ncshape<span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">30</span>,<span class=\"pl-c1\">4</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Helpful lambda</span>\nrn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">s</span>, <span class=\"pl-smi\">d</span>: tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>s, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>d)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Produce random arrays of shape depending on the type</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">r</span>(<span class=\"pl-smi\">shape</span>, <span class=\"pl-smi\">dtype</span>):\n    <span class=\"pl-k\">if</span> dtype <span class=\"pl-k\">in</span> (np.float32, np.float64):\n        <span class=\"pl-k\">return</span> rn(shape, dtype)\n    <span class=\"pl-k\">elif</span> dtype <span class=\"pl-k\">==</span> np.complex64:\n        <span class=\"pl-k\">return</span> tf.complex(rn(shape, np.float32), rn(shape, np.float32))\n    <span class=\"pl-k\">elif</span> dtype <span class=\"pl-k\">==</span> np.complex128:\n        <span class=\"pl-k\">return</span> tf.complex(rn(shape, np.float64), rn(shape, np.float64))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Unhandled dtype '<span class=\"pl-c1\">{dt}</span>'<span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-v\">dt</span><span class=\"pl-k\">=</span>dtype))\n\n<span class=\"pl-c1\">XX</span> <span class=\"pl-k\">=</span> r(shape, dtype)\n<span class=\"pl-c1\">XY</span> <span class=\"pl-k\">=</span> r(shape, dtype)\n<span class=\"pl-c1\">YX</span> <span class=\"pl-k\">=</span> r(shape, dtype)\n<span class=\"pl-c1\">YY</span> <span class=\"pl-k\">=</span> r(shape, dtype)\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n    concat <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">3</span>, [<span class=\"pl-c1\">XX</span>, <span class=\"pl-c1\">XY</span>, <span class=\"pl-c1\">YX</span>, <span class=\"pl-c1\">YY</span>],)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> S:\n    result <span class=\"pl-k\">=</span> S.run(concat)\n    <span class=\"pl-k\">assert</span> result.shape <span class=\"pl-k\">==</span> cshape</pre></div>\n</li>\n<li>\n<p>If dtype is <code>np.complex64</code> of <code>np.complex128</code>, the following is thrown:</p>\n<div class=\"highlight highlight-source-python\"><pre>tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node <span class=\"pl-s\"><span class=\"pl-pds\">'</span>concat<span class=\"pl-pds\">'</span></span>: Could <span class=\"pl-k\">not</span> satisfy explicit device specification <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/device:GPU:0<span class=\"pl-pds\">'</span></span> because no supported kernel <span class=\"pl-k\">for</span> <span class=\"pl-c1\">GPU</span> devices <span class=\"pl-k\">is</span> available.\n     [[Node: concat = Concat[N=<span class=\"pl-c1\">4</span>, T=<span class=\"pl-c1\">DT_COMPLEX64</span>, _device=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/device:GPU:0<span class=\"pl-pds\">\"</span></span>](concat<span class=\"pl-k\">/</span>concat_dim, Complex, Complex_1, Complex_2, Complex_3)]]\nCaused by op <span class=\"pl-s\"><span class=\"pl-k\">u</span><span class=\"pl-pds\">'</span>concat<span class=\"pl-pds\">'</span></span>, defined at:\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tmp/concat_fail.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">32</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    concat <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">3</span>, [<span class=\"pl-c1\">XX</span>, <span class=\"pl-c1\">XY</span>, <span class=\"pl-c1\">YX</span>, <span class=\"pl-c1\">YY</span>],)</pre></div>\n</li>\n</ol>\n<h3>What have you tried?</h3>\n<p>N/A</p>", "body_text": "A number of changes have been made to support complex128, but the the tensor transformation op suite seems to be missing this support (At least in tf.pack and tf.concat). See the example script below which works for float32 and float64, but not complex64 and complex128.\nEnvironment info\nOperating System: Ubuntu 14.04.4\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$ ls -l /usr/local/cuda-7.5/lib64/libcud*\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed. Python 2 GPU nightly\nThe output from python -c \"import tensorflow; print(tensorflow.version)\". 0.8.0\n\nIf installed from sources, provide the commit hash:\nSteps to reproduce\n\n\nVary dtype in the following test script between float32, float64, complex64, complex128\nimport numpy as np\nimport tensorflow as tf\n\n# Data type for this test case\ndtype=np.float64\n\n# Array shapes\nshape=(10,20,30,1)\n# Expected concatenated array shape\ncshape=(10,20,30,4)\n\n# Helpful lambda\nrn = lambda s, d: tf.random_normal(shape=s, dtype=d)\n\n# Produce random arrays of shape depending on the type\ndef r(shape, dtype):\n    if dtype in (np.float32, np.float64):\n        return rn(shape, dtype)\n    elif dtype == np.complex64:\n        return tf.complex(rn(shape, np.float32), rn(shape, np.float32))\n    elif dtype == np.complex128:\n        return tf.complex(rn(shape, np.float64), rn(shape, np.float64))\n    else:\n        raise ValueError(\"Unhandled dtype '{dt}'\".format(dt=dtype))\n\nXX = r(shape, dtype)\nXY = r(shape, dtype)\nYX = r(shape, dtype)\nYY = r(shape, dtype)\n\nwith tf.device('/gpu:0'):\n    concat = tf.concat(3, [XX, XY, YX, YY],)\n\nwith tf.Session() as S:\n    result = S.run(concat)\n    assert result.shape == cshape\n\n\nIf dtype is np.complex64 of np.complex128, the following is thrown:\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'concat': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: concat = Concat[N=4, T=DT_COMPLEX64, _device=\"/device:GPU:0\"](concat/concat_dim, Complex, Complex_1, Complex_2, Complex_3)]]\nCaused by op u'concat', defined at:\n  File \"tmp/concat_fail.py\", line 32, in <module>\n    concat = tf.concat(3, [XX, XY, YX, YY],)\n\n\nWhat have you tried?\nN/A", "body": "A number of changes have been made to support complex128, but the the tensor transformation op suite seems to be missing this support (At least in tf.pack and tf.concat). See the example script below which works for float32 and float64, but not complex64 and complex128.\n### Environment info\n\nOperating System: Ubuntu 14.04.4\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n``` bash\n$ ls -l /usr/local/cuda-7.5/lib64/libcud*\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. **Python 2 GPU nightly**\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\". **0.8.0**\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Vary **dtype** in the following test script between `float32, float64, complex64, complex128`\n   \n   ``` python\n   import numpy as np\n   import tensorflow as tf\n   \n   # Data type for this test case\n   dtype=np.float64\n   \n   # Array shapes\n   shape=(10,20,30,1)\n   # Expected concatenated array shape\n   cshape=(10,20,30,4)\n   \n   # Helpful lambda\n   rn = lambda s, d: tf.random_normal(shape=s, dtype=d)\n   \n   # Produce random arrays of shape depending on the type\n   def r(shape, dtype):\n       if dtype in (np.float32, np.float64):\n           return rn(shape, dtype)\n       elif dtype == np.complex64:\n           return tf.complex(rn(shape, np.float32), rn(shape, np.float32))\n       elif dtype == np.complex128:\n           return tf.complex(rn(shape, np.float64), rn(shape, np.float64))\n       else:\n           raise ValueError(\"Unhandled dtype '{dt}'\".format(dt=dtype))\n   \n   XX = r(shape, dtype)\n   XY = r(shape, dtype)\n   YX = r(shape, dtype)\n   YY = r(shape, dtype)\n   \n   with tf.device('/gpu:0'):\n       concat = tf.concat(3, [XX, XY, YX, YY],)\n   \n   with tf.Session() as S:\n       result = S.run(concat)\n       assert result.shape == cshape\n   ```\n2. If dtype is `np.complex64` of `np.complex128`, the following is thrown:\n   \n   ``` python\n   tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'concat': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n        [[Node: concat = Concat[N=4, T=DT_COMPLEX64, _device=\"/device:GPU:0\"](concat/concat_dim, Complex, Complex_1, Complex_2, Complex_3)]]\n   Caused by op u'concat', defined at:\n     File \"tmp/concat_fail.py\", line 32, in <module>\n       concat = tf.concat(3, [XX, XY, YX, YY],)\n   ```\n### What have you tried?\n\nN/A\n"}