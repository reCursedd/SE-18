{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364314690", "html_url": "https://github.com/tensorflow/tensorflow/issues/13610#issuecomment-364314690", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13610", "id": 364314690, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDMxNDY5MA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T02:17:44Z", "updated_at": "2018-02-09T02:17:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>@Lancerchaing Both the (current version of the) <code>tf.data</code> API and the old queue-based approach place the entire input pipeline on the CPU, and the GPUs are exercised by parts of the graph that come after the input pipeline. Since the overheads associated with the <code>tf.data</code> implementation are smaller, I'd recommend using it over the queue-based approach. Take a look at the <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\">TF benchmarks</a> for an example of how to use <code>tf.data</code> efficiently in a multi-GPU setup.</p>\n<p>We still plan to add support for running stages of a <code>tf.data</code> pipeline on GPU, as well as easier-to-use support for prefetching to GPU memory (currently implemented <a href=\"https://github.com/tensorflow/tensorflow/blob/926fc13f7378d14fa7980963c4fe774e5922e336/tensorflow/contrib/data/python/ops/prefetching_ops.py#L30\">here</a>, but not yet exposed via the API), which should help to achieve even better performance in future.</p>", "body_text": "@Lancerchaing Both the (current version of the) tf.data API and the old queue-based approach place the entire input pipeline on the CPU, and the GPUs are exercised by parts of the graph that come after the input pipeline. Since the overheads associated with the tf.data implementation are smaller, I'd recommend using it over the queue-based approach. Take a look at the TF benchmarks for an example of how to use tf.data efficiently in a multi-GPU setup.\nWe still plan to add support for running stages of a tf.data pipeline on GPU, as well as easier-to-use support for prefetching to GPU memory (currently implemented here, but not yet exposed via the API), which should help to achieve even better performance in future.", "body": "@Lancerchaing Both the (current version of the) `tf.data` API and the old queue-based approach place the entire input pipeline on the CPU, and the GPUs are exercised by parts of the graph that come after the input pipeline. Since the overheads associated with the `tf.data` implementation are smaller, I'd recommend using it over the queue-based approach. Take a look at the [TF benchmarks](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py) for an example of how to use `tf.data` efficiently in a multi-GPU setup.\r\n\r\nWe still plan to add support for running stages of a `tf.data` pipeline on GPU, as well as easier-to-use support for prefetching to GPU memory (currently implemented [here](https://github.com/tensorflow/tensorflow/blob/926fc13f7378d14fa7980963c4fe774e5922e336/tensorflow/contrib/data/python/ops/prefetching_ops.py#L30), but not yet exposed via the API), which should help to achieve even better performance in future."}