{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364331935", "html_url": "https://github.com/tensorflow/tensorflow/issues/13610#issuecomment-364331935", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13610", "id": 364331935, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDMzMTkzNQ==", "user": {"login": "Lancerchiang", "id": 35952525, "node_id": "MDQ6VXNlcjM1OTUyNTI1", "avatar_url": "https://avatars2.githubusercontent.com/u/35952525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lancerchiang", "html_url": "https://github.com/Lancerchiang", "followers_url": "https://api.github.com/users/Lancerchiang/followers", "following_url": "https://api.github.com/users/Lancerchiang/following{/other_user}", "gists_url": "https://api.github.com/users/Lancerchiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lancerchiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lancerchiang/subscriptions", "organizations_url": "https://api.github.com/users/Lancerchiang/orgs", "repos_url": "https://api.github.com/users/Lancerchiang/repos", "events_url": "https://api.github.com/users/Lancerchiang/events{/privacy}", "received_events_url": "https://api.github.com/users/Lancerchiang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T04:29:10Z", "updated_at": "2018-02-09T04:29:10Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> Hi thanks for the reply, I have looked into this benchmarking before and the other one here <a href=\"url\">https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py</a><br>\nI am pretty new to tensorflow, and I need to change our existing project to a multi-gpu mode (instead of starting from scratch). Do you think below is a right approach with <code>tf.data</code> API ?</p>\n<pre><code>dataset = tf.data.TextLineDataset(filenames)\ndataset = dataset.map(...) #preprocessing data\ndataset = dataset.batch(batch_size)\niterator = dataset.make_initializable_iterator()\n\nnum_batches = len(filenames) // batch_size\npredictions = []\n\nfor i in range(num_batches):\n    batch = tf.split(dataset.next(), num_gpus) #splitting batch into smaller batches for each GPU\n    for i in range(num_gpus):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % (Tower_name, i)) as scope:\n                predictions.append(inference(batch[i]))\n\nwith tf.Session() as sess:\n    sess.run(predictions)\n</code></pre>\n<p>Furthermore, do I need a runner like queue_runner in dataset API to facilitate the input pipeline \uff08I hope I can to preprocess the next batch of image while GPU is doing computation\uff09?<br>\nThanks :)</p>", "body_text": "@mrry Hi thanks for the reply, I have looked into this benchmarking before and the other one here https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py\nI am pretty new to tensorflow, and I need to change our existing project to a multi-gpu mode (instead of starting from scratch). Do you think below is a right approach with tf.data API ?\ndataset = tf.data.TextLineDataset(filenames)\ndataset = dataset.map(...) #preprocessing data\ndataset = dataset.batch(batch_size)\niterator = dataset.make_initializable_iterator()\n\nnum_batches = len(filenames) // batch_size\npredictions = []\n\nfor i in range(num_batches):\n    batch = tf.split(dataset.next(), num_gpus) #splitting batch into smaller batches for each GPU\n    for i in range(num_gpus):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % (Tower_name, i)) as scope:\n                predictions.append(inference(batch[i]))\n\nwith tf.Session() as sess:\n    sess.run(predictions)\n\nFurthermore, do I need a runner like queue_runner in dataset API to facilitate the input pipeline \uff08I hope I can to preprocess the next batch of image while GPU is doing computation\uff09?\nThanks :)", "body": "@mrry Hi thanks for the reply, I have looked into this benchmarking before and the other one here [https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py](url) \r\nI am pretty new to tensorflow, and I need to change our existing project to a multi-gpu mode (instead of starting from scratch). Do you think below is a right approach with `tf.data` API ?\r\n\r\n\r\n```\r\ndataset = tf.data.TextLineDataset(filenames)\r\ndataset = dataset.map(...) #preprocessing data\r\ndataset = dataset.batch(batch_size)\r\niterator = dataset.make_initializable_iterator()\r\n\r\nnum_batches = len(filenames) // batch_size\r\npredictions = []\r\n\r\nfor i in range(num_batches):\r\n    batch = tf.split(dataset.next(), num_gpus) #splitting batch into smaller batches for each GPU\r\n    for i in range(num_gpus):\r\n        with tf.device('/gpu:%d' % i):\r\n            with tf.name_scope('%s_%d' % (Tower_name, i)) as scope:\r\n                predictions.append(inference(batch[i]))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(predictions)\r\n```\r\n\r\nFurthermore, do I need a runner like queue_runner in dataset API to facilitate the input pipeline \uff08I hope I can to preprocess the next batch of image while GPU is doing computation\uff09? \r\nThanks :)"}