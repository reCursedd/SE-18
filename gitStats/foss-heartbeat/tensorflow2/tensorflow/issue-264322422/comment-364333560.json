{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364333560", "html_url": "https://github.com/tensorflow/tensorflow/issues/13610#issuecomment-364333560", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13610", "id": 364333560, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDMzMzU2MA==", "user": {"login": "ed-alertedh", "id": 24605895, "node_id": "MDQ6VXNlcjI0NjA1ODk1", "avatar_url": "https://avatars1.githubusercontent.com/u/24605895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ed-alertedh", "html_url": "https://github.com/ed-alertedh", "followers_url": "https://api.github.com/users/ed-alertedh/followers", "following_url": "https://api.github.com/users/ed-alertedh/following{/other_user}", "gists_url": "https://api.github.com/users/ed-alertedh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ed-alertedh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ed-alertedh/subscriptions", "organizations_url": "https://api.github.com/users/ed-alertedh/orgs", "repos_url": "https://api.github.com/users/ed-alertedh/repos", "events_url": "https://api.github.com/users/ed-alertedh/events{/privacy}", "received_events_url": "https://api.github.com/users/ed-alertedh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T04:43:18Z", "updated_at": "2018-02-09T04:46:57Z", "author_association": "NONE", "body_html": "<p>I'm looking forward to GPU prefetching as currently using StagingArea to manually pipeline data gets a little messy.</p>\n<p>For the single-machine use-case I'm looking at using a feedable iterator to switch between training data and test data - is there any chance this could be supported on GPU so that it could be combined with prefetching? Trying to share the same prefetch pipeline between two data sources, while possible, would require very careful coordination to ensure the right data comes out at the right time. It would also be difficult to implement with existing high-level API's.</p>\n<p>In crude diagram form, this is the approach I have in mind:</p>\n<pre><code>    CPU                                    | GPU\n    train_data -&gt; map -&gt; prefetch&lt;send&gt; -&gt; | -&gt; prefetch&lt;recv&gt; -&gt; string_handle\\\n                                           |                                     -&gt; feedable_iterator\n    test_data  -&gt; map -&gt; prefetch&lt;send&gt; -&gt; | -&gt; prefetch&lt;recv&gt; -&gt; string_handle/\n</code></pre>\n<p>edit: this is what \"sharing the same prefetch pipeline\" would look like using the same diagram convention:</p>\n<pre><code>        CPU                                                                      | GPU\n    train_data -&gt; map -&gt; string_handle\\                                          | \n                                       -&gt; feedable_iterator -&gt; prefetch&lt;send&gt; -&gt; | -&gt; prefetch&lt;recv&gt;\n    test_data  -&gt; map -&gt; string_handle/                                          |\n</code></pre>", "body_text": "I'm looking forward to GPU prefetching as currently using StagingArea to manually pipeline data gets a little messy.\nFor the single-machine use-case I'm looking at using a feedable iterator to switch between training data and test data - is there any chance this could be supported on GPU so that it could be combined with prefetching? Trying to share the same prefetch pipeline between two data sources, while possible, would require very careful coordination to ensure the right data comes out at the right time. It would also be difficult to implement with existing high-level API's.\nIn crude diagram form, this is the approach I have in mind:\n    CPU                                    | GPU\n    train_data -> map -> prefetch<send> -> | -> prefetch<recv> -> string_handle\\\n                                           |                                     -> feedable_iterator\n    test_data  -> map -> prefetch<send> -> | -> prefetch<recv> -> string_handle/\n\nedit: this is what \"sharing the same prefetch pipeline\" would look like using the same diagram convention:\n        CPU                                                                      | GPU\n    train_data -> map -> string_handle\\                                          | \n                                       -> feedable_iterator -> prefetch<send> -> | -> prefetch<recv>\n    test_data  -> map -> string_handle/                                          |", "body": "I'm looking forward to GPU prefetching as currently using StagingArea to manually pipeline data gets a little messy.\r\n\r\nFor the single-machine use-case I'm looking at using a feedable iterator to switch between training data and test data - is there any chance this could be supported on GPU so that it could be combined with prefetching? Trying to share the same prefetch pipeline between two data sources, while possible, would require very careful coordination to ensure the right data comes out at the right time. It would also be difficult to implement with existing high-level API's.\r\n\r\nIn crude diagram form, this is the approach I have in mind:\r\n```\r\n    CPU                                    | GPU\r\n    train_data -> map -> prefetch<send> -> | -> prefetch<recv> -> string_handle\\\r\n                                           |                                     -> feedable_iterator\r\n    test_data  -> map -> prefetch<send> -> | -> prefetch<recv> -> string_handle/\r\n```\r\n\r\nedit: this is what \"sharing the same prefetch pipeline\" would look like using the same diagram convention:\r\n```\r\n        CPU                                                                      | GPU\r\n    train_data -> map -> string_handle\\                                          | \r\n                                       -> feedable_iterator -> prefetch<send> -> | -> prefetch<recv>\r\n    test_data  -> map -> string_handle/                                          |\r\n```"}