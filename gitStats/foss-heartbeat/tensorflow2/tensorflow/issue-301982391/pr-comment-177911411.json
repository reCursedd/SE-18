{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/177911411", "pull_request_review_id": 107876270, "id": 177911411, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NzkxMTQxMQ==", "diff_hunk": "@@ -0,0 +1,160 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"AdaMax for TensorFlow.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import resource_variable_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.training import adam\n+from tensorflow.python.training import training_ops\n+\n+\n+class AdaMaxOptimizer(adam.AdamOptimizer):\n+  \"\"\"Optimizer that implements the AdaMax algorithm.\n+\n+  See [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n+  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n+  \"\"\"\n+\n+  def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n+               use_locking=False, name=\"AdaMax\"):\n+    \"\"\"Construct a new AdaMax optimizer.\n+\n+    Initialization:\n+\n+    ```\n+    m_0 <- 0 (Initialize initial 1st moment vector)\n+    v_0 <- 0 (Initialize the exponentially weighted infinity norm)\n+    t <- 0 (Initialize timestep)\n+    ```\n+\n+    The update rule for `variable` with gradient `g` uses an optimization\n+    described at the end of section7.1 of the paper:\n+\n+    ```\n+    t <- t + 1\n+\n+    m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n+    v_t <- max(beta2 * v_{t-1}, abs(g))\n+    variable <- variable - learning_rate / (1 - beta1^t) * m_t / (v_t + epsilon)\n+    ```\n+\n+    Similar to AdamOptimizer, the epsilon is added for numerical stability\n+    (especially to get rid of division by zero when v_t = 0).\n+\n+    Contrast to AdamOptimizer, the sparse implementation of this algorithm", "path": "tensorflow/contrib/opt/python/training/adamax.py", "position": 66, "original_position": 64, "commit_id": "708e640f67b3f8298aad27e4e106eb8fa9f9dc60", "original_commit_id": "20424e92417b520d7ea8c7323eee46538d2b909f", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "body": "I think the formula of AdaMax has been added in its docstring.", "created_at": "2018-03-28T22:42:48Z", "updated_at": "2018-04-14T10:40:04Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17395#discussion_r177911411", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17395", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/177911411"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17395#discussion_r177911411"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17395"}}, "body_html": "<p>I think the formula of AdaMax has been added in its docstring.</p>", "body_text": "I think the formula of AdaMax has been added in its docstring.", "in_reply_to_id": 175832450}