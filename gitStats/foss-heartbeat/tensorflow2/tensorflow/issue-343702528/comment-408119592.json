{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/408119592", "html_url": "https://github.com/tensorflow/tensorflow/issues/21058#issuecomment-408119592", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21058", "id": 408119592, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODExOTU5Mg==", "user": {"login": "xiao1228", "id": 11752039, "node_id": "MDQ6VXNlcjExNzUyMDM5", "avatar_url": "https://avatars3.githubusercontent.com/u/11752039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiao1228", "html_url": "https://github.com/xiao1228", "followers_url": "https://api.github.com/users/xiao1228/followers", "following_url": "https://api.github.com/users/xiao1228/following{/other_user}", "gists_url": "https://api.github.com/users/xiao1228/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiao1228/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiao1228/subscriptions", "organizations_url": "https://api.github.com/users/xiao1228/orgs", "repos_url": "https://api.github.com/users/xiao1228/repos", "events_url": "https://api.github.com/users/xiao1228/events{/privacy}", "received_events_url": "https://api.github.com/users/xiao1228/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-26T14:37:51Z", "updated_at": "2018-07-26T14:37:51Z", "author_association": "NONE", "body_html": "<p>Yea, everything seems to work fine with<code> 'updates_collections':None</code> both in training and eval. However, as other issues mentioned (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"133980206\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1122\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1122/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1122\">#1122</a>), this may take longer time to train as it is not efficient. But if I am using slim.learning.create_train_op with update_ops =tf.get_collection( GraphKeys.UPDATE_OPS ) like the original code, my eval results was the same for different images. Another reason might also due to the batch_norm_decay, I see in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"247356255\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11965\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11965/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/11965\">#11965</a>, they also mentioned that. The original value was 0.997 for imagenet, and I changed it to 0.9. With values like 0.997 it may requires more steps to see the changes in eval results, however we don't know what is the roughly the step size for cifar10. I use decay as 0.997 with <code>update_ops =tf.get_collection( GraphKeys.UPDATE_OPS )</code>  (original code) and ran it up to 10k steps still output the same prediction for different images. But after I changed it to 0.9 with <code> 'updates_collections':None</code>  , in the first 50 steps or less I already can see that eval predictions give different labels.</p>", "body_text": "Yea, everything seems to work fine with 'updates_collections':None both in training and eval. However, as other issues mentioned (#1122), this may take longer time to train as it is not efficient. But if I am using slim.learning.create_train_op with update_ops =tf.get_collection( GraphKeys.UPDATE_OPS ) like the original code, my eval results was the same for different images. Another reason might also due to the batch_norm_decay, I see in #11965, they also mentioned that. The original value was 0.997 for imagenet, and I changed it to 0.9. With values like 0.997 it may requires more steps to see the changes in eval results, however we don't know what is the roughly the step size for cifar10. I use decay as 0.997 with update_ops =tf.get_collection( GraphKeys.UPDATE_OPS )  (original code) and ran it up to 10k steps still output the same prediction for different images. But after I changed it to 0.9 with  'updates_collections':None  , in the first 50 steps or less I already can see that eval predictions give different labels.", "body": "Yea, everything seems to work fine with` 'updates_collections':None` both in training and eval. However, as other issues mentioned (https://github.com/tensorflow/tensorflow/issues/1122), this may take longer time to train as it is not efficient. But if I am using slim.learning.create_train_op with update_ops =tf.get_collection( GraphKeys.UPDATE_OPS ) like the original code, my eval results was the same for different images. Another reason might also due to the batch_norm_decay, I see in https://github.com/tensorflow/tensorflow/issues/11965, they also mentioned that. The original value was 0.997 for imagenet, and I changed it to 0.9. With values like 0.997 it may requires more steps to see the changes in eval results, however we don't know what is the roughly the step size for cifar10. I use decay as 0.997 with `update_ops =tf.get_collection( GraphKeys.UPDATE_OPS )`  (original code) and ran it up to 10k steps still output the same prediction for different images. But after I changed it to 0.9 with ` 'updates_collections':None`  , in the first 50 steps or less I already can see that eval predictions give different labels. "}