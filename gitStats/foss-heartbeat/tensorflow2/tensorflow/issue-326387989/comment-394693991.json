{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/394693991", "html_url": "https://github.com/tensorflow/tensorflow/issues/19551#issuecomment-394693991", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19551", "id": 394693991, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDY5Mzk5MQ==", "user": {"login": "DavidWiesner", "id": 243115, "node_id": "MDQ6VXNlcjI0MzExNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/243115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidWiesner", "html_url": "https://github.com/DavidWiesner", "followers_url": "https://api.github.com/users/DavidWiesner/followers", "following_url": "https://api.github.com/users/DavidWiesner/following{/other_user}", "gists_url": "https://api.github.com/users/DavidWiesner/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidWiesner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidWiesner/subscriptions", "organizations_url": "https://api.github.com/users/DavidWiesner/orgs", "repos_url": "https://api.github.com/users/DavidWiesner/repos", "events_url": "https://api.github.com/users/DavidWiesner/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidWiesner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-05T12:40:41Z", "updated_at": "2018-06-05T12:40:41Z", "author_association": "NONE", "body_html": "<p>I got the same problem when using the <code>MovingAverageOptimizer</code></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">mode</span>):\n    layer <span class=\"pl-k\">=</span> tf.layers.Dense(<span class=\"pl-c1\">1</span>)\n    logits <span class=\"pl-k\">=</span> layer(features)\n\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">PREDICT</span>:\n        predictions <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>logits<span class=\"pl-pds\">\"</span></span>: logits}\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(mode, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions)\n\n    loss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(\n            <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>tf.reshape(logits, []))\n\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">EVAL</span>:\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(mode, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss)\n\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.02</span>)\n        optimizer <span class=\"pl-k\">=</span> tf.contrib.opt.MovingAverageOptimizer(optimizer, <span class=\"pl-c1\">0.9</span>)\n        train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss)\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(mode, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss, <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">input_fn</span>():\n    features <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensors([[<span class=\"pl-c1\">1</span>.]]).repeat(<span class=\"pl-c1\">100</span>)\n    labels <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensors(<span class=\"pl-c1\">1</span>.).repeat(<span class=\"pl-c1\">100</span>)\n    <span class=\"pl-k\">return</span> tf.data.Dataset.zip((features, labels))\n\ndistribution <span class=\"pl-k\">=</span> tf.contrib.distribute.MirroredStrategy(<span class=\"pl-v\">num_gpus</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\nconfig <span class=\"pl-k\">=</span> tf.estimator.RunConfig(<span class=\"pl-v\">train_distribute</span><span class=\"pl-k\">=</span>distribution)\nclassifier <span class=\"pl-k\">=</span> tf.estimator.Estimator(<span class=\"pl-v\">model_fn</span><span class=\"pl-k\">=</span>model_fn, <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\nclassifier.train(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>input_fn)</pre></div>\n<p>The error with some context</p>\n<pre><code>AssertionError                            Traceback (most recent call last)\n&lt;ipython-input-114-62e57e2880d4&gt; in &lt;module&gt;()\n     27 config = tf.estimator.RunConfig(train_distribute=distribution)\n     28 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\n---&gt; 29 classifier.train(input_fn=input_fn)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    361 \n    362     saving_listeners = _check_listeners_type(saving_listeners)\n--&gt; 363     loss = self._train_model(input_fn, hooks, saving_listeners)\n    364     logging.info('Loss for final step: %s.', loss)\n    365     return self\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\n    839   def _train_model(self, input_fn, hooks, saving_listeners):\n    840     if self._distribution:\n--&gt; 841       return self._train_model_distributed(input_fn, hooks, saving_listeners)\n    842     else:\n    843       return self._train_model_default(input_fn, hooks, saving_listeners)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\n    882             labels,  # although this will be None it seems\n    883             model_fn_lib.ModeKeys.TRAIN,\n--&gt; 884             self.config)\n    885 \n    886         # TODO(anjalisridhar): Figure out how to resolve the folowing scaffold\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py in call_for_each_tower(self, fn, *args, **kwargs)\n    754     \"\"\"\n    755     _require_cross_tower_context(self)\n--&gt; 756     return self._call_for_each_tower(fn, *args, **kwargs)\n    757 \n    758   def _call_for_each_tower(self, fn, *args, **kwargs):\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in _call_for_each_tower(self, fn, *args, **kwargs)\n    252       for t in threads:\n    253         t.should_run.set()\n--&gt; 254       coord.join(threads)\n    255 \n    256     return values.regroup({t.device: t.main_result for t in threads})\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\n    387       self._registered_threads = set()\n    388       if self._exc_info_to_raise:\n--&gt; 389         six.reraise(*self._exc_info_to_raise)\n    390       elif stragglers:\n    391         if ignore_live_threads:\n\n/usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)\n    691             if value.__traceback__ is not tb:\n    692                 raise value.with_traceback(tb)\n--&gt; 693             raise value\n    694         finally:\n    695             value = None\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)\n    295     \"\"\"\n    296     try:\n--&gt; 297       yield\n    298     except:  # pylint: disable=bare-except\n    299       self.request_stop(ex=sys.exc_info())\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in run(self)\n    463                 self._captured_var_scope, reuse=self.tower_id &gt; 0), \\\n    464             variable_scope.variable_creator_scope(self.variable_creator_fn):\n--&gt; 465           self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    466           self.done = True\n    467       finally:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\n    829 \n    830     logging.info('Calling model_fn.')\n--&gt; 831     model_fn_results = self._model_fn(features=features, **kwargs)\n    832     logging.info('Done calling model_fn.')\n    833 \n\n&lt;ipython-input-114-62e57e2880d4&gt; in model_fn(features, labels, mode)\n     16         optimizer = tf.train.GradientDescentOptimizer(0.02)\n     17         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\n---&gt; 18         train_op = optimizer.minimize(loss)\n     19         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n     20 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    422 \n    423     return self.apply_gradients(grads_and_vars, global_step=global_step,\n--&gt; 424                                 name=name)\n    425 \n    426   def compute_gradients(self, loss, var_list=None,\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\n     97     if self._sequential_update:\n     98       with ops.control_dependencies([train_op]):\n---&gt; 99         ma_op = self._ema.apply(var_list)\n    100     else:\n    101       ma_op = self._ema.apply(var_list)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in apply(self, var_list)\n    423         zero_debias = self._averages[var] in zero_debias_true\n    424         updates.append(assign_moving_average(\n--&gt; 425             self._averages[var], var, decay, zero_debias=zero_debias))\n    426       return control_flow_ops.group(*updates, name=scope)\n    427 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)\n     82   with ops.name_scope(name, \"AssignMovingAvg\",\n     83                       [variable, value, decay]) as scope:\n---&gt; 84     with ops.colocate_with(variable):\n     85       decay = ops.convert_to_tensor(1.0 - decay, name=\"decay\")\n     86       if decay.dtype != variable.dtype.base_dtype:\n\n/usr/lib/python3.5/contextlib.py in __enter__(self)\n     57     def __enter__(self):\n     58         try:\n---&gt; 59             return next(self.gen)\n     60         except StopIteration:\n     61             raise RuntimeError(\"generator didn't yield\") from None\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\n   4184   def _colocate_with_for_gradient(self, op, gradient_uid,\n   4185                                   ignore_existing=False):\n-&gt; 4186     with self.colocate_with(op, ignore_existing):\n   4187       if gradient_uid is not None and self._control_flow_context is not None:\n   4188         try:\n\n/usr/lib/python3.5/contextlib.py in __enter__(self)\n     57     def __enter__(self):\n     58         try:\n---&gt; 59             return next(self.gen)\n     60         except StopIteration:\n     61             raise RuntimeError(\"generator didn't yield\") from None\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)\n   4237     if op is not None and not isinstance(op, Operation):\n   4238       # We always want to colocate with the reference op.\n-&gt; 4239       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n   4240 \n   4241     # By default, colocate_with resets the device function stack,\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)\n   1260   else:\n   1261     return internal_convert_to_tensor(\n-&gt; 1262         value, dtype=dtype, name=name, as_ref=as_ref)\n   1263 \n   1264 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\n   1102 \n   1103     if ret is None:\n-&gt; 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   1105 \n   1106     if ret is NotImplemented:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py in _tensor_conversion(var, dtype, name, as_ref)\n    241   # Try to avoid assignments to and other mutations of MirroredVariable\n    242   # state except through a DistributionStrategy.update() call.\n--&gt; 243   assert not as_ref\n    244   return ops.internal_convert_to_tensor(\n    245       var.get(), dtype=dtype, name=name, as_ref=as_ref)\n\nAssertionError: \n\n</code></pre>", "body_text": "I got the same problem when using the MovingAverageOptimizer\ndef model_fn(features, labels, mode):\n    layer = tf.layers.Dense(1)\n    logits = layer(features)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        predictions = {\"logits\": logits}\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n    loss = tf.losses.mean_squared_error(\n            labels=labels, predictions=tf.reshape(logits, []))\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.GradientDescentOptimizer(0.02)\n        optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\n        train_op = optimizer.minimize(loss)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\ndef input_fn():\n    features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\n    labels = tf.data.Dataset.from_tensors(1.).repeat(100)\n    return tf.data.Dataset.zip((features, labels))\n\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\nclassifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\nclassifier.train(input_fn=input_fn)\nThe error with some context\nAssertionError                            Traceback (most recent call last)\n<ipython-input-114-62e57e2880d4> in <module>()\n     27 config = tf.estimator.RunConfig(train_distribute=distribution)\n     28 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\n---> 29 classifier.train(input_fn=input_fn)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    361 \n    362     saving_listeners = _check_listeners_type(saving_listeners)\n--> 363     loss = self._train_model(input_fn, hooks, saving_listeners)\n    364     logging.info('Loss for final step: %s.', loss)\n    365     return self\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\n    839   def _train_model(self, input_fn, hooks, saving_listeners):\n    840     if self._distribution:\n--> 841       return self._train_model_distributed(input_fn, hooks, saving_listeners)\n    842     else:\n    843       return self._train_model_default(input_fn, hooks, saving_listeners)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\n    882             labels,  # although this will be None it seems\n    883             model_fn_lib.ModeKeys.TRAIN,\n--> 884             self.config)\n    885 \n    886         # TODO(anjalisridhar): Figure out how to resolve the folowing scaffold\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py in call_for_each_tower(self, fn, *args, **kwargs)\n    754     \"\"\"\n    755     _require_cross_tower_context(self)\n--> 756     return self._call_for_each_tower(fn, *args, **kwargs)\n    757 \n    758   def _call_for_each_tower(self, fn, *args, **kwargs):\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in _call_for_each_tower(self, fn, *args, **kwargs)\n    252       for t in threads:\n    253         t.should_run.set()\n--> 254       coord.join(threads)\n    255 \n    256     return values.regroup({t.device: t.main_result for t in threads})\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\n    387       self._registered_threads = set()\n    388       if self._exc_info_to_raise:\n--> 389         six.reraise(*self._exc_info_to_raise)\n    390       elif stragglers:\n    391         if ignore_live_threads:\n\n/usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)\n    691             if value.__traceback__ is not tb:\n    692                 raise value.with_traceback(tb)\n--> 693             raise value\n    694         finally:\n    695             value = None\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)\n    295     \"\"\"\n    296     try:\n--> 297       yield\n    298     except:  # pylint: disable=bare-except\n    299       self.request_stop(ex=sys.exc_info())\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in run(self)\n    463                 self._captured_var_scope, reuse=self.tower_id > 0), \\\n    464             variable_scope.variable_creator_scope(self.variable_creator_fn):\n--> 465           self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    466           self.done = True\n    467       finally:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\n    829 \n    830     logging.info('Calling model_fn.')\n--> 831     model_fn_results = self._model_fn(features=features, **kwargs)\n    832     logging.info('Done calling model_fn.')\n    833 \n\n<ipython-input-114-62e57e2880d4> in model_fn(features, labels, mode)\n     16         optimizer = tf.train.GradientDescentOptimizer(0.02)\n     17         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\n---> 18         train_op = optimizer.minimize(loss)\n     19         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n     20 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    422 \n    423     return self.apply_gradients(grads_and_vars, global_step=global_step,\n--> 424                                 name=name)\n    425 \n    426   def compute_gradients(self, loss, var_list=None,\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\n     97     if self._sequential_update:\n     98       with ops.control_dependencies([train_op]):\n---> 99         ma_op = self._ema.apply(var_list)\n    100     else:\n    101       ma_op = self._ema.apply(var_list)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in apply(self, var_list)\n    423         zero_debias = self._averages[var] in zero_debias_true\n    424         updates.append(assign_moving_average(\n--> 425             self._averages[var], var, decay, zero_debias=zero_debias))\n    426       return control_flow_ops.group(*updates, name=scope)\n    427 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)\n     82   with ops.name_scope(name, \"AssignMovingAvg\",\n     83                       [variable, value, decay]) as scope:\n---> 84     with ops.colocate_with(variable):\n     85       decay = ops.convert_to_tensor(1.0 - decay, name=\"decay\")\n     86       if decay.dtype != variable.dtype.base_dtype:\n\n/usr/lib/python3.5/contextlib.py in __enter__(self)\n     57     def __enter__(self):\n     58         try:\n---> 59             return next(self.gen)\n     60         except StopIteration:\n     61             raise RuntimeError(\"generator didn't yield\") from None\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\n   4184   def _colocate_with_for_gradient(self, op, gradient_uid,\n   4185                                   ignore_existing=False):\n-> 4186     with self.colocate_with(op, ignore_existing):\n   4187       if gradient_uid is not None and self._control_flow_context is not None:\n   4188         try:\n\n/usr/lib/python3.5/contextlib.py in __enter__(self)\n     57     def __enter__(self):\n     58         try:\n---> 59             return next(self.gen)\n     60         except StopIteration:\n     61             raise RuntimeError(\"generator didn't yield\") from None\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)\n   4237     if op is not None and not isinstance(op, Operation):\n   4238       # We always want to colocate with the reference op.\n-> 4239       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n   4240 \n   4241     # By default, colocate_with resets the device function stack,\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)\n   1260   else:\n   1261     return internal_convert_to_tensor(\n-> 1262         value, dtype=dtype, name=name, as_ref=as_ref)\n   1263 \n   1264 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\n   1102 \n   1103     if ret is None:\n-> 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   1105 \n   1106     if ret is NotImplemented:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py in _tensor_conversion(var, dtype, name, as_ref)\n    241   # Try to avoid assignments to and other mutations of MirroredVariable\n    242   # state except through a DistributionStrategy.update() call.\n--> 243   assert not as_ref\n    244   return ops.internal_convert_to_tensor(\n    245       var.get(), dtype=dtype, name=name, as_ref=as_ref)\n\nAssertionError:", "body": "I got the same problem when using the `MovingAverageOptimizer`\r\n```python\r\ndef model_fn(features, labels, mode):\r\n    layer = tf.layers.Dense(1)\r\n    logits = layer(features)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\"logits\": logits}\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.mean_squared_error(\r\n            labels=labels, predictions=tf.reshape(logits, []))\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.GradientDescentOptimizer(0.02)\r\n        optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\r\n        train_op = optimizer.minimize(loss)\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\ndef input_fn():\r\n    features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\r\n    labels = tf.data.Dataset.from_tensors(1.).repeat(100)\r\n    return tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nclassifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\nclassifier.train(input_fn=input_fn)\r\n```\r\nThe error with some context\r\n```\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-114-62e57e2880d4> in <module>()\r\n     27 config = tf.estimator.RunConfig(train_distribute=distribution)\r\n     28 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n---> 29 classifier.train(input_fn=input_fn)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    361 \r\n    362     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 363     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    364     logging.info('Loss for final step: %s.', loss)\r\n    365     return self\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    839   def _train_model(self, input_fn, hooks, saving_listeners):\r\n    840     if self._distribution:\r\n--> 841       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n    842     else:\r\n    843       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n    882             labels,  # although this will be None it seems\r\n    883             model_fn_lib.ModeKeys.TRAIN,\r\n--> 884             self.config)\r\n    885 \r\n    886         # TODO(anjalisridhar): Figure out how to resolve the folowing scaffold\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py in call_for_each_tower(self, fn, *args, **kwargs)\r\n    754     \"\"\"\r\n    755     _require_cross_tower_context(self)\r\n--> 756     return self._call_for_each_tower(fn, *args, **kwargs)\r\n    757 \r\n    758   def _call_for_each_tower(self, fn, *args, **kwargs):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in _call_for_each_tower(self, fn, *args, **kwargs)\r\n    252       for t in threads:\r\n    253         t.should_run.set()\r\n--> 254       coord.join(threads)\r\n    255 \r\n    256     return values.regroup({t.device: t.main_result for t in threads})\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\r\n    387       self._registered_threads = set()\r\n    388       if self._exc_info_to_raise:\r\n--> 389         six.reraise(*self._exc_info_to_raise)\r\n    390       elif stragglers:\r\n    391         if ignore_live_threads:\r\n\r\n/usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)\r\n    295     \"\"\"\r\n    296     try:\r\n--> 297       yield\r\n    298     except:  # pylint: disable=bare-except\r\n    299       self.request_stop(ex=sys.exc_info())\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in run(self)\r\n    463                 self._captured_var_scope, reuse=self.tower_id > 0), \\\r\n    464             variable_scope.variable_creator_scope(self.variable_creator_fn):\r\n--> 465           self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    466           self.done = True\r\n    467       finally:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n    829 \r\n    830     logging.info('Calling model_fn.')\r\n--> 831     model_fn_results = self._model_fn(features=features, **kwargs)\r\n    832     logging.info('Done calling model_fn.')\r\n    833 \r\n\r\n<ipython-input-114-62e57e2880d4> in model_fn(features, labels, mode)\r\n     16         optimizer = tf.train.GradientDescentOptimizer(0.02)\r\n     17         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\r\n---> 18         train_op = optimizer.minimize(loss)\r\n     19         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n     20 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    422 \r\n    423     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n--> 424                                 name=name)\r\n    425 \r\n    426   def compute_gradients(self, loss, var_list=None,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\r\n     97     if self._sequential_update:\r\n     98       with ops.control_dependencies([train_op]):\r\n---> 99         ma_op = self._ema.apply(var_list)\r\n    100     else:\r\n    101       ma_op = self._ema.apply(var_list)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in apply(self, var_list)\r\n    423         zero_debias = self._averages[var] in zero_debias_true\r\n    424         updates.append(assign_moving_average(\r\n--> 425             self._averages[var], var, decay, zero_debias=zero_debias))\r\n    426       return control_flow_ops.group(*updates, name=scope)\r\n    427 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)\r\n     82   with ops.name_scope(name, \"AssignMovingAvg\",\r\n     83                       [variable, value, decay]) as scope:\r\n---> 84     with ops.colocate_with(variable):\r\n     85       decay = ops.convert_to_tensor(1.0 - decay, name=\"decay\")\r\n     86       if decay.dtype != variable.dtype.base_dtype:\r\n\r\n/usr/lib/python3.5/contextlib.py in __enter__(self)\r\n     57     def __enter__(self):\r\n     58         try:\r\n---> 59             return next(self.gen)\r\n     60         except StopIteration:\r\n     61             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\r\n   4184   def _colocate_with_for_gradient(self, op, gradient_uid,\r\n   4185                                   ignore_existing=False):\r\n-> 4186     with self.colocate_with(op, ignore_existing):\r\n   4187       if gradient_uid is not None and self._control_flow_context is not None:\r\n   4188         try:\r\n\r\n/usr/lib/python3.5/contextlib.py in __enter__(self)\r\n     57     def __enter__(self):\r\n     58         try:\r\n---> 59             return next(self.gen)\r\n     60         except StopIteration:\r\n     61             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)\r\n   4237     if op is not None and not isinstance(op, Operation):\r\n   4238       # We always want to colocate with the reference op.\r\n-> 4239       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n   4240 \r\n   4241     # By default, colocate_with resets the device function stack,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)\r\n   1260   else:\r\n   1261     return internal_convert_to_tensor(\r\n-> 1262         value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1263 \r\n   1264 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1102 \r\n   1103     if ret is None:\r\n-> 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1105 \r\n   1106     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py in _tensor_conversion(var, dtype, name, as_ref)\r\n    241   # Try to avoid assignments to and other mutations of MirroredVariable\r\n    242   # state except through a DistributionStrategy.update() call.\r\n--> 243   assert not as_ref\r\n    244   return ops.internal_convert_to_tensor(\r\n    245       var.get(), dtype=dtype, name=name, as_ref=as_ref)\r\n\r\nAssertionError: \r\n\r\n```\r\n"}