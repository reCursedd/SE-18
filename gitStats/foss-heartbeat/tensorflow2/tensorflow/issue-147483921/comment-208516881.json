{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/208516881", "html_url": "https://github.com/tensorflow/tensorflow/issues/1850#issuecomment-208516881", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1850", "id": 208516881, "node_id": "MDEyOklzc3VlQ29tbWVudDIwODUxNjg4MQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-11T19:30:15Z", "updated_at": "2016-04-11T19:30:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Valentin :).</p>\n<p>So it sounds like you have three processes: a client (on machine A or B? or a third machine?), and two TensorFlow servers (one on machine A and one on machine B). If you connect to machine A and all of the ops are on machine A then all of the master&lt;-&gt;worker traffic will be local; whereas if you connect to machine B and all of the ops are on machine A then all of the master&lt;-&gt;worker traffic will cross the network. (I'm not sure about the client&lt;-&gt;master traffic, because you didn't mention where the client is running. You might want to try putting the client in the same process as the machine A master, as I would expect that to give the best performance. That's the configuration we typically use for distributed TF.)</p>\n<p>In the latter case, it's possible that 30% of machine B is being consumed by proxying RPCs between the client and machine A. You might be seeing latency because the \"queue runners\" that prefetch input data run sequentially and now have two network round trips per record, and you're now \"I/O bound\" (not actually bound by the disk, but by the steps that have to run to populate the input queues). You could try adding prefetching threads to the input pipeline to see if that improves things. (I'm assuming you're using a version of <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py\"><code>cifar10_multi_gpu_train.py</code></a>, so no data is being fed or fetched in each training step.)</p>\n<p>The input pipeline \"protocol\" is very chatty on the network, and we've been relying on most of the interactions happening in-process. Thanks for digging into this: it might give us good motivation to improve the batching etc.!</p>", "body_text": "Hi Valentin :).\nSo it sounds like you have three processes: a client (on machine A or B? or a third machine?), and two TensorFlow servers (one on machine A and one on machine B). If you connect to machine A and all of the ops are on machine A then all of the master<->worker traffic will be local; whereas if you connect to machine B and all of the ops are on machine A then all of the master<->worker traffic will cross the network. (I'm not sure about the client<->master traffic, because you didn't mention where the client is running. You might want to try putting the client in the same process as the machine A master, as I would expect that to give the best performance. That's the configuration we typically use for distributed TF.)\nIn the latter case, it's possible that 30% of machine B is being consumed by proxying RPCs between the client and machine A. You might be seeing latency because the \"queue runners\" that prefetch input data run sequentially and now have two network round trips per record, and you're now \"I/O bound\" (not actually bound by the disk, but by the steps that have to run to populate the input queues). You could try adding prefetching threads to the input pipeline to see if that improves things. (I'm assuming you're using a version of cifar10_multi_gpu_train.py, so no data is being fed or fetched in each training step.)\nThe input pipeline \"protocol\" is very chatty on the network, and we've been relying on most of the interactions happening in-process. Thanks for digging into this: it might give us good motivation to improve the batching etc.!", "body": "Hi Valentin :).\n\nSo it sounds like you have three processes: a client (on machine A or B? or a third machine?), and two TensorFlow servers (one on machine A and one on machine B). If you connect to machine A and all of the ops are on machine A then all of the master<->worker traffic will be local; whereas if you connect to machine B and all of the ops are on machine A then all of the master<->worker traffic will cross the network. (I'm not sure about the client<->master traffic, because you didn't mention where the client is running. You might want to try putting the client in the same process as the machine A master, as I would expect that to give the best performance. That's the configuration we typically use for distributed TF.)\n\nIn the latter case, it's possible that 30% of machine B is being consumed by proxying RPCs between the client and machine A. You might be seeing latency because the \"queue runners\" that prefetch input data run sequentially and now have two network round trips per record, and you're now \"I/O bound\" (not actually bound by the disk, but by the steps that have to run to populate the input queues). You could try adding prefetching threads to the input pipeline to see if that improves things. (I'm assuming you're using a version of [`cifar10_multi_gpu_train.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py), so no data is being fed or fetched in each training step.)\n\nThe input pipeline \"protocol\" is very chatty on the network, and we've been relying on most of the interactions happening in-process. Thanks for digging into this: it might give us good motivation to improve the batching etc.!\n"}