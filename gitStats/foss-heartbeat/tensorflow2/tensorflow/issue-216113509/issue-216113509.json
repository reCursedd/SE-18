{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8626", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8626/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8626/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8626/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8626", "id": 216113509, "node_id": "MDU6SXNzdWUyMTYxMTM1MDk=", "number": 8626, "title": "The reuse flag of tf.variable_scope doesn't work with tf.contrib.layers ?", "user": {"login": "roytseng-tw", "id": 5027936, "node_id": "MDQ6VXNlcjUwMjc5MzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/5027936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roytseng-tw", "html_url": "https://github.com/roytseng-tw", "followers_url": "https://api.github.com/users/roytseng-tw/followers", "following_url": "https://api.github.com/users/roytseng-tw/following{/other_user}", "gists_url": "https://api.github.com/users/roytseng-tw/gists{/gist_id}", "starred_url": "https://api.github.com/users/roytseng-tw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roytseng-tw/subscriptions", "organizations_url": "https://api.github.com/users/roytseng-tw/orgs", "repos_url": "https://api.github.com/users/roytseng-tw/repos", "events_url": "https://api.github.com/users/roytseng-tw/events{/privacy}", "received_events_url": "https://api.github.com/users/roytseng-tw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-03-22T15:50:44Z", "updated_at": "2017-06-16T20:54:05Z", "closed_at": "2017-06-16T20:54:00Z", "author_association": "NONE", "body_html": "<p>For example, I defined a simple network</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.layers <span class=\"pl-k\">as</span> tcl\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">model</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>foo<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span>reuse) <span class=\"pl-k\">as</span> scope:\n        tcl.conv2d(x, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>)</pre></div>\n<p>And then, I try to call the model function twice <strong>without</strong> setting reuse=True for second call</p>\n<div class=\"highlight highlight-source-python\"><pre>x1 <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, (<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">3</span>))\nx2 <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, (<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">3</span>))\nmodel(x1)\nmodel(x2)</pre></div>\n<p>When using with tf.get_variable(), this will cause a error. However, it's not the case for tf.contrib.layers?<br>\nI tried to print out all the nodes in the graph with following codes</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> tf.get_default_graph().as_graph_def().node:\n    <span class=\"pl-c1\">print</span>(n.name)</pre></div>\n<p>I got this result:</p>\n<pre><code>Placeholder\nPlaceholder_1\nfoo/Conv/weights/Initializer/random_uniform/shape\nfoo/Conv/weights/Initializer/random_uniform/min\nfoo/Conv/weights/Initializer/random_uniform/max\nfoo/Conv/weights/Initializer/random_uniform/RandomUniform\nfoo/Conv/weights/Initializer/random_uniform/sub\nfoo/Conv/weights/Initializer/random_uniform/mul\nfoo/Conv/weights/Initializer/random_uniform\nfoo/Conv/weights\nfoo/Conv/weights/Assign\nfoo/Conv/weights/read\nfoo/Conv/biases/Initializer/Const\nfoo/Conv/biases\nfoo/Conv/biases/Assign\nfoo/Conv/biases/read\nfoo/Conv/convolution/Shape\nfoo/Conv/convolution/dilation_rate\nfoo/Conv/convolution/ExpandDims/dim\nfoo/Conv/convolution/ExpandDims\nfoo/Conv/convolution/ExpandDims_1/dim\nfoo/Conv/convolution/ExpandDims_1\nfoo/Conv/convolution/Conv2D\nfoo/Conv/convolution/Squeeze\nfoo/Conv/BiasAdd\nfoo/Conv/Relu\nfoo_1/Conv/convolution/Shape\nfoo_1/Conv/convolution/dilation_rate\nfoo_1/Conv/convolution/ExpandDims/dim\nfoo_1/Conv/convolution/ExpandDims\nfoo_1/Conv/convolution/ExpandDims_1/dim\nfoo_1/Conv/convolution/ExpandDims_1\nfoo_1/Conv/convolution/Conv2D\nfoo_1/Conv/convolution/Squeeze\nfoo_1/Conv/BiasAdd\nfoo_1/Conv/Relu\n</code></pre>\n<p>It seems that there is only one copy of weights, instead of two.<br>\nSo, even if <code>reuse</code> is not set to <code>True</code>, the weights can still be shared ?</p>\n<p>Is this the correct behavior?<br>\nOr did I miss anything?</p>\n<p>I'm using Python3, Tensorflow 1.0.1, Ubuntu 16.04.</p>", "body_text": "For example, I defined a simple network\nimport tensorflow as tf\nimport tensorflow.contrib.layers as tcl\n\ndef model(x, reuse=None):\n    with tf.variable_scope('foo', reuse=reuse) as scope:\n        tcl.conv2d(x, 3, 3, 1)\nAnd then, I try to call the model function twice without setting reuse=True for second call\nx1 = tf.placeholder(tf.float32, (10,10,3))\nx2 = tf.placeholder(tf.float32, (10,10,3))\nmodel(x1)\nmodel(x2)\nWhen using with tf.get_variable(), this will cause a error. However, it's not the case for tf.contrib.layers?\nI tried to print out all the nodes in the graph with following codes\nfor n in tf.get_default_graph().as_graph_def().node:\n    print(n.name)\nI got this result:\nPlaceholder\nPlaceholder_1\nfoo/Conv/weights/Initializer/random_uniform/shape\nfoo/Conv/weights/Initializer/random_uniform/min\nfoo/Conv/weights/Initializer/random_uniform/max\nfoo/Conv/weights/Initializer/random_uniform/RandomUniform\nfoo/Conv/weights/Initializer/random_uniform/sub\nfoo/Conv/weights/Initializer/random_uniform/mul\nfoo/Conv/weights/Initializer/random_uniform\nfoo/Conv/weights\nfoo/Conv/weights/Assign\nfoo/Conv/weights/read\nfoo/Conv/biases/Initializer/Const\nfoo/Conv/biases\nfoo/Conv/biases/Assign\nfoo/Conv/biases/read\nfoo/Conv/convolution/Shape\nfoo/Conv/convolution/dilation_rate\nfoo/Conv/convolution/ExpandDims/dim\nfoo/Conv/convolution/ExpandDims\nfoo/Conv/convolution/ExpandDims_1/dim\nfoo/Conv/convolution/ExpandDims_1\nfoo/Conv/convolution/Conv2D\nfoo/Conv/convolution/Squeeze\nfoo/Conv/BiasAdd\nfoo/Conv/Relu\nfoo_1/Conv/convolution/Shape\nfoo_1/Conv/convolution/dilation_rate\nfoo_1/Conv/convolution/ExpandDims/dim\nfoo_1/Conv/convolution/ExpandDims\nfoo_1/Conv/convolution/ExpandDims_1/dim\nfoo_1/Conv/convolution/ExpandDims_1\nfoo_1/Conv/convolution/Conv2D\nfoo_1/Conv/convolution/Squeeze\nfoo_1/Conv/BiasAdd\nfoo_1/Conv/Relu\n\nIt seems that there is only one copy of weights, instead of two.\nSo, even if reuse is not set to True, the weights can still be shared ?\nIs this the correct behavior?\nOr did I miss anything?\nI'm using Python3, Tensorflow 1.0.1, Ubuntu 16.04.", "body": "For example, I defined a simple network\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.layers as tcl\r\n\r\ndef model(x, reuse=None):\r\n    with tf.variable_scope('foo', reuse=reuse) as scope:\r\n        tcl.conv2d(x, 3, 3, 1)\r\n```\r\nAnd then, I try to call the model function twice **without** setting reuse=True for second call\r\n```python\r\nx1 = tf.placeholder(tf.float32, (10,10,3))\r\nx2 = tf.placeholder(tf.float32, (10,10,3))\r\nmodel(x1)\r\nmodel(x2)\r\n```\r\nWhen using with tf.get_variable(), this will cause a error. However, it's not the case for tf.contrib.layers?\r\nI tried to print out all the nodes in the graph with following codes\r\n```python\r\nfor n in tf.get_default_graph().as_graph_def().node:\r\n    print(n.name)\r\n```\r\nI got this result:\r\n```\r\nPlaceholder\r\nPlaceholder_1\r\nfoo/Conv/weights/Initializer/random_uniform/shape\r\nfoo/Conv/weights/Initializer/random_uniform/min\r\nfoo/Conv/weights/Initializer/random_uniform/max\r\nfoo/Conv/weights/Initializer/random_uniform/RandomUniform\r\nfoo/Conv/weights/Initializer/random_uniform/sub\r\nfoo/Conv/weights/Initializer/random_uniform/mul\r\nfoo/Conv/weights/Initializer/random_uniform\r\nfoo/Conv/weights\r\nfoo/Conv/weights/Assign\r\nfoo/Conv/weights/read\r\nfoo/Conv/biases/Initializer/Const\r\nfoo/Conv/biases\r\nfoo/Conv/biases/Assign\r\nfoo/Conv/biases/read\r\nfoo/Conv/convolution/Shape\r\nfoo/Conv/convolution/dilation_rate\r\nfoo/Conv/convolution/ExpandDims/dim\r\nfoo/Conv/convolution/ExpandDims\r\nfoo/Conv/convolution/ExpandDims_1/dim\r\nfoo/Conv/convolution/ExpandDims_1\r\nfoo/Conv/convolution/Conv2D\r\nfoo/Conv/convolution/Squeeze\r\nfoo/Conv/BiasAdd\r\nfoo/Conv/Relu\r\nfoo_1/Conv/convolution/Shape\r\nfoo_1/Conv/convolution/dilation_rate\r\nfoo_1/Conv/convolution/ExpandDims/dim\r\nfoo_1/Conv/convolution/ExpandDims\r\nfoo_1/Conv/convolution/ExpandDims_1/dim\r\nfoo_1/Conv/convolution/ExpandDims_1\r\nfoo_1/Conv/convolution/Conv2D\r\nfoo_1/Conv/convolution/Squeeze\r\nfoo_1/Conv/BiasAdd\r\nfoo_1/Conv/Relu\r\n```\r\nIt seems that there is only one copy of weights, instead of two.\r\nSo, even if ```reuse``` is not set to ```True```, the weights can still be shared ?\r\n\r\nIs this the correct behavior?\r\nOr did I miss anything?\r\n\r\nI'm using Python3, Tensorflow 1.0.1, Ubuntu 16.04."}