{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13197", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13197/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13197/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13197/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13197", "id": 259316053, "node_id": "MDU6SXNzdWUyNTkzMTYwNTM=", "number": 13197, "title": "\"In-Graph Replication\" Multi-GPU training in local/single machine not working.", "user": {"login": "Rajesh5688", "id": 2963478, "node_id": "MDQ6VXNlcjI5NjM0Nzg=", "avatar_url": "https://avatars3.githubusercontent.com/u/2963478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rajesh5688", "html_url": "https://github.com/Rajesh5688", "followers_url": "https://api.github.com/users/Rajesh5688/followers", "following_url": "https://api.github.com/users/Rajesh5688/following{/other_user}", "gists_url": "https://api.github.com/users/Rajesh5688/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rajesh5688/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rajesh5688/subscriptions", "organizations_url": "https://api.github.com/users/Rajesh5688/orgs", "repos_url": "https://api.github.com/users/Rajesh5688/repos", "events_url": "https://api.github.com/users/Rajesh5688/events{/privacy}", "received_events_url": "https://api.github.com/users/Rajesh5688/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-20T22:03:58Z", "updated_at": "2017-09-25T22:34:05Z", "closed_at": "2017-09-25T22:34:04Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Wrote a minimal version custom code using TensorFlow API's (see attachment)</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Installed through pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.3.0-rc2-20-g0787eee', '1.3.0')</li>\n<li><strong>Python version</strong>: Python 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: Cuda Version 8.0, cuDNN version 1.6</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX Titan X GPU 12 GB memory</li>\n<li><strong>Exact command to reproduce</strong>: sh run_dist_tf_exp.sh (attached as zip file)<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1319333/distributed_tf_issue.zip\">distributed_tf_issue.zip</a></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>We are trying to get single machine multi-gpu training with Distributed Tensor Flow for increasing model throughput. Our set-up is as follows, We have a single compute machine running Ubuntu 16.04 with 8 GPU's and we would like to enable \"Data Parallelism\" by training model on multiple(4 GPU's) device present in the local machine to increase throughput.</p>\n<p>I will explain three scenarios below which uses minimal code that just runs a LinearClassifier (see attached code),</p>\n<p>Scenario 1:<br>\ntf_config set to run single worker and single parameter server config below,</p>\n<pre><code>tf_config = {\n    \"cluster\": {\n        'ps': ['127.0.0.1:9000'],\n        'worker': ['127.0.0.1:9001']\n    }\n  }\n</code></pre>\n<p>Does not run, execution just freezes. Also tried the suggestion that came in logs to use <code>cloud</code> as environment in tf_config, still didn't work. Read similar issue written <a href=\"https://github.com/tensorflow/tensorflow/issues/8796\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8796/hovercard\">here</a> commented out the line in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336\">Experiment.py</a> that checks if Environment is not LOCAL. By doing so it was successfully running training and worker exited after finished with training_steps. Should that line be commented for local distributed training runs?</p>\n<p>Scenario 2:<br>\ntf_config set to run two worker and single parameter server config below,</p>\n<pre><code>tf_config = {\n    \"cluster\": {\n        'ps': ['127.0.0.1:9000'],\n        'worker': ['127.0.0.1:9001', '127.0.0.1:9002']\n    }\n  }\n</code></pre>\n<p>Had commented out [Experiment.py] (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336</a>) line as explained in Scenario 1, Did not work, the script was just hanging forever.</p>\n<p>Scenario 3:<br>\ntf_config set to run one worker and a single parameter server config similar to Scenario 1.</p>\n<p>Change done here was <code>learn_runner.run(..., schedule='continuous_train_and_eval)</code>, In continuous_train_and_eval mode the training just froze. Is continuous_train_and_eval not supported with distributed training? Seems to have some problem. Only change here compared to the working Scenario 1 setup with Experiment.py line commented out was changing the schedule to continuous_train_and eval.</p>\n<h3>Source code / logs</h3>\n<p>Following modification was done to Experiment.py in run function, Had to comment out LOCAL environment check.</p>\n<pre><code>    if (#config.environment != run_config.Environment.LOCAL and\n        config.environment != run_config.Environment.GOOGLE and\n        config.cluster_spec and config.master):\n      self._start_server()\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Wrote a minimal version custom code using TensorFlow API's (see attachment)\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Installed through pip\nTensorFlow version (use command below): ('v1.3.0-rc2-20-g0787eee', '1.3.0')\nPython version: Python 2.7.12\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: Cuda Version 8.0, cuDNN version 1.6\nGPU model and memory: GeForce GTX Titan X GPU 12 GB memory\nExact command to reproduce: sh run_dist_tf_exp.sh (attached as zip file)\ndistributed_tf_issue.zip\n\nDescribe the problem\nWe are trying to get single machine multi-gpu training with Distributed Tensor Flow for increasing model throughput. Our set-up is as follows, We have a single compute machine running Ubuntu 16.04 with 8 GPU's and we would like to enable \"Data Parallelism\" by training model on multiple(4 GPU's) device present in the local machine to increase throughput.\nI will explain three scenarios below which uses minimal code that just runs a LinearClassifier (see attached code),\nScenario 1:\ntf_config set to run single worker and single parameter server config below,\ntf_config = {\n    \"cluster\": {\n        'ps': ['127.0.0.1:9000'],\n        'worker': ['127.0.0.1:9001']\n    }\n  }\n\nDoes not run, execution just freezes. Also tried the suggestion that came in logs to use cloud as environment in tf_config, still didn't work. Read similar issue written here commented out the line in Experiment.py that checks if Environment is not LOCAL. By doing so it was successfully running training and worker exited after finished with training_steps. Should that line be commented for local distributed training runs?\nScenario 2:\ntf_config set to run two worker and single parameter server config below,\ntf_config = {\n    \"cluster\": {\n        'ps': ['127.0.0.1:9000'],\n        'worker': ['127.0.0.1:9001', '127.0.0.1:9002']\n    }\n  }\n\nHad commented out [Experiment.py] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336) line as explained in Scenario 1, Did not work, the script was just hanging forever.\nScenario 3:\ntf_config set to run one worker and a single parameter server config similar to Scenario 1.\nChange done here was learn_runner.run(..., schedule='continuous_train_and_eval), In continuous_train_and_eval mode the training just froze. Is continuous_train_and_eval not supported with distributed training? Seems to have some problem. Only change here compared to the working Scenario 1 setup with Experiment.py line commented out was changing the schedule to continuous_train_and eval.\nSource code / logs\nFollowing modification was done to Experiment.py in run function, Had to comment out LOCAL environment check.\n    if (#config.environment != run_config.Environment.LOCAL and\n        config.environment != run_config.Environment.GOOGLE and\n        config.cluster_spec and config.master):\n      self._start_server()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Wrote a minimal version custom code using TensorFlow API's (see attachment)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Installed through pip \r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Cuda Version 8.0, cuDNN version 1.6\r\n- **GPU model and memory**: GeForce GTX Titan X GPU 12 GB memory\r\n- **Exact command to reproduce**: sh run_dist_tf_exp.sh (attached as zip file)\r\n[distributed_tf_issue.zip](https://github.com/tensorflow/tensorflow/files/1319333/distributed_tf_issue.zip)\r\n\r\n\r\n### Describe the problem\r\nWe are trying to get single machine multi-gpu training with Distributed Tensor Flow for increasing model throughput. Our set-up is as follows, We have a single compute machine running Ubuntu 16.04 with 8 GPU's and we would like to enable \"Data Parallelism\" by training model on multiple(4 GPU's) device present in the local machine to increase throughput. \r\n\r\nI will explain three scenarios below which uses minimal code that just runs a LinearClassifier (see attached code), \r\n\r\nScenario 1:\r\ntf_config set to run single worker and single parameter server config below,\r\n```\r\ntf_config = {\r\n    \"cluster\": {\r\n        'ps': ['127.0.0.1:9000'],\r\n        'worker': ['127.0.0.1:9001']\r\n    }\r\n  }\r\n```\r\nDoes not run, execution just freezes. Also tried the suggestion that came in logs to use `cloud` as environment in tf_config, still didn't work. Read similar issue written [here](https://github.com/tensorflow/tensorflow/issues/8796) commented out the line in [Experiment.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336) that checks if Environment is not LOCAL. By doing so it was successfully running training and worker exited after finished with training_steps. Should that line be commented for local distributed training runs?\r\n\r\nScenario 2:\r\ntf_config set to run two worker and single parameter server config below,\r\n```\r\ntf_config = {\r\n    \"cluster\": {\r\n        'ps': ['127.0.0.1:9000'],\r\n        'worker': ['127.0.0.1:9001', '127.0.0.1:9002']\r\n    }\r\n  }\r\n```\r\nHad commented out [Experiment.py] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336) line as explained in Scenario 1, Did not work, the script was just hanging forever.\r\n\r\nScenario 3:\r\ntf_config set to run one worker and a single parameter server config similar to Scenario 1.\r\n\r\nChange done here was `learn_runner.run(..., schedule='continuous_train_and_eval)`, In continuous_train_and_eval mode the training just froze. Is continuous_train_and_eval not supported with distributed training? Seems to have some problem. Only change here compared to the working Scenario 1 setup with Experiment.py line commented out was changing the schedule to continuous_train_and eval.\r\n\r\n\r\n### Source code / logs\r\nFollowing modification was done to Experiment.py in run function, Had to comment out LOCAL environment check.\r\n```\r\n    if (#config.environment != run_config.Environment.LOCAL and\r\n        config.environment != run_config.Environment.GOOGLE and\r\n        config.cluster_spec and config.master):\r\n      self._start_server()\r\n```"}