{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226722933", "html_url": "https://github.com/tensorflow/tensorflow/issues/2901#issuecomment-226722933", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2901", "id": 226722933, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjcyMjkzMw==", "user": {"login": "MammadTavakoli", "id": 12212802, "node_id": "MDQ6VXNlcjEyMjEyODAy", "avatar_url": "https://avatars2.githubusercontent.com/u/12212802?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MammadTavakoli", "html_url": "https://github.com/MammadTavakoli", "followers_url": "https://api.github.com/users/MammadTavakoli/followers", "following_url": "https://api.github.com/users/MammadTavakoli/following{/other_user}", "gists_url": "https://api.github.com/users/MammadTavakoli/gists{/gist_id}", "starred_url": "https://api.github.com/users/MammadTavakoli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MammadTavakoli/subscriptions", "organizations_url": "https://api.github.com/users/MammadTavakoli/orgs", "repos_url": "https://api.github.com/users/MammadTavakoli/repos", "events_url": "https://api.github.com/users/MammadTavakoli/events{/privacy}", "received_events_url": "https://api.github.com/users/MammadTavakoli/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-17T09:24:28Z", "updated_at": "2016-06-17T09:24:28Z", "author_association": "NONE", "body_html": "<p>Yes , this file exists.</p>\n<p>This is my code:</p>\n<p>`import tensorflow as tf<br>\nimport input_data<br>\nimport os</p>\n<p>checkpoint_dir='./ckpt_dir/'</p>\n<p>mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)</p>\n<p>x = tf.placeholder(tf.float32, shape = [None , 784])<br>\ny_ = tf.placeholder(tf.float32, [None, 10])</p>\n<p>sess = tf.InteractiveSession()</p>\n<p>def load_model(sess, saver, checkpoint_dir ):<br>\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)<br>\nif ckpt and ckpt.model_checkpoint_path:<br>\nprint(ckpt.model_checkpoint_path)<br>\nsaver.restore(sess, ckpt.model_checkpoint_path)<br>\nelse:<br>\nif not os.path.exists(checkpoint_dir):<br>\nos.makedirs(checkpoint_dir)<br>\nsess.run(init)<br>\nreturn</p>\n<p>def weight_variable(shape):<br>\ninitial = tf.truncated_normal(shape, stddev = 0.1)<br>\nreturn tf.Variable(initial)</p>\n<p>def bias_variable(shape):<br>\ninitial = tf.constant(0.1, shape= shape)<br>\nreturn tf.Variable(initial)</p>\n<p>def conv2d(x, W):<br>\nreturn tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = \"SAME\")</p>\n<p>def max_pool_2x2(x):<br>\nreturn tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],<br>\npadding = \"SAME\")</p>\n<p>W_conv1 = weight_variable([5, 5, 1, 32])<br>\nb_conv1 = bias_variable([32])</p>\n<p>x_image = tf.reshape(x, [-1, 28, 28, 1])</p>\n<h1></h1>\n<p>h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1))<br>\nh_pool1 = max_pool_2x2(h_conv1)</p>\n<h1></h1>\n<p>W_conv2 = weight_variable([5, 5, 32, 64])<br>\nb_conv2 = bias_variable([64])</p>\n<p>h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2))<br>\nh_pool2 = max_pool_2x2(h_conv2)</p>\n<p>W_fc1 = weight_variable([7_7_64, 1024])<br>\nb_fc1 = bias_variable([1024])</p>\n<p>h_pool2_flat = tf.reshape(h_pool2, [-1, 7_7_64])<br>\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</p>\n<h1></h1>\n<p>keep_prob = tf.placeholder(tf.float32)<br>\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</p>\n<h1></h1>\n<p>W_fc2 = weight_variable([1024, 10])<br>\nb_fc2 = bias_variable([10])</p>\n<p>y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) +b_fc2)</p>\n<h1></h1>\n<p>cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices = [1]))<br>\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</p>\n<p>correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))<br>\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</p>\n<p>init = tf.initialize_all_variables()</p>\n<p>saver = tf.train.Saver()</p>\n<p>load_model(sess, saver, checkpoint_dir)</p>\n<p>for i in range(1):<br>\nbatch = mnist.train.next_batch(50)<br>\nif i%10 == 0:<br>\ntrain_accuracy = accuracy.eval(feed_dict = {x : batch[0] , y_ : batch[1], keep_prob : 1.0})<br>\nprint(\"step %d, training accuracy %g\"%(i, train_accuracy))</p>\n<pre><code>train_step.run(feed_dict = {x : batch[0], y_ : batch[1], keep_prob : 0.5})\n</code></pre>\n<p>print(\"test accuracy %g\"%accuracy.eval(feed_dict={<br>\nx: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))</p>\n<p>tf.scalar_summary(\"accuracy\", accuracy)</p>\n<p>saver.save(sess,checkpoint_dir+'model.ckpt')`</p>", "body_text": "Yes , this file exists.\nThis is my code:\n`import tensorflow as tf\nimport input_data\nimport os\ncheckpoint_dir='./ckpt_dir/'\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\nx = tf.placeholder(tf.float32, shape = [None , 784])\ny_ = tf.placeholder(tf.float32, [None, 10])\nsess = tf.InteractiveSession()\ndef load_model(sess, saver, checkpoint_dir ):\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt and ckpt.model_checkpoint_path:\nprint(ckpt.model_checkpoint_path)\nsaver.restore(sess, ckpt.model_checkpoint_path)\nelse:\nif not os.path.exists(checkpoint_dir):\nos.makedirs(checkpoint_dir)\nsess.run(init)\nreturn\ndef weight_variable(shape):\ninitial = tf.truncated_normal(shape, stddev = 0.1)\nreturn tf.Variable(initial)\ndef bias_variable(shape):\ninitial = tf.constant(0.1, shape= shape)\nreturn tf.Variable(initial)\ndef conv2d(x, W):\nreturn tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = \"SAME\")\ndef max_pool_2x2(x):\nreturn tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],\npadding = \"SAME\")\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1))\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2))\nh_pool2 = max_pool_2x2(h_conv2)\nW_fc1 = weight_variable([7_7_64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7_7_64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) +b_fc2)\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices = [1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\ninit = tf.initialize_all_variables()\nsaver = tf.train.Saver()\nload_model(sess, saver, checkpoint_dir)\nfor i in range(1):\nbatch = mnist.train.next_batch(50)\nif i%10 == 0:\ntrain_accuracy = accuracy.eval(feed_dict = {x : batch[0] , y_ : batch[1], keep_prob : 1.0})\nprint(\"step %d, training accuracy %g\"%(i, train_accuracy))\ntrain_step.run(feed_dict = {x : batch[0], y_ : batch[1], keep_prob : 0.5})\n\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={\nx: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\ntf.scalar_summary(\"accuracy\", accuracy)\nsaver.save(sess,checkpoint_dir+'model.ckpt')`", "body": "Yes , this file exists.\n\nThis is my code:\n\n`import tensorflow as tf\nimport input_data\nimport os \n\ncheckpoint_dir='./ckpt_dir/'                \n\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)\n\nx = tf.placeholder(tf.float32, shape = [None , 784])\ny_ = tf.placeholder(tf.float32, [None, 10])\n\nsess = tf.InteractiveSession()\n\ndef load_model(sess, saver, checkpoint_dir ):  \n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            print(ckpt.model_checkpoint_path)  \n            saver.restore(sess, ckpt.model_checkpoint_path)  \n        else:\n            if not os.path.exists(checkpoint_dir):\n                os.makedirs(checkpoint_dir)\n            sess.run(init)\n        return\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev = 0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape= shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = \"SAME\")\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1],\n                          padding = \"SAME\")\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\n#\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1))\nh_pool1 = max_pool_2x2(h_conv1)\n\n#\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2))\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7_7_64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7_7_64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n#\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n#\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) +b_fc2)\n\n#\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ \\* tf.log(y_conv), reduction_indices = [1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ninit = tf.initialize_all_variables()\n\nsaver = tf.train.Saver() \n\nload_model(sess, saver, checkpoint_dir)\n\nfor i in range(1):\n    batch = mnist.train.next_batch(50)\n    if i%10 == 0:\n        train_accuracy = accuracy.eval(feed_dict = {x : batch[0] , y_ : batch[1], keep_prob : 1.0})\n        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n\n```\ntrain_step.run(feed_dict = {x : batch[0], y_ : batch[1], keep_prob : 0.5})\n```\n\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))        \n\ntf.scalar_summary(\"accuracy\", accuracy)\n\nsaver.save(sess,checkpoint_dir+'model.ckpt')`\n"}