{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271066575", "html_url": "https://github.com/tensorflow/tensorflow/issues/6087#issuecomment-271066575", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6087", "id": 271066575, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTA2NjU3NQ==", "user": {"login": "ffmpbgrnn", "id": 4504507, "node_id": "MDQ6VXNlcjQ1MDQ1MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/4504507?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ffmpbgrnn", "html_url": "https://github.com/ffmpbgrnn", "followers_url": "https://api.github.com/users/ffmpbgrnn/followers", "following_url": "https://api.github.com/users/ffmpbgrnn/following{/other_user}", "gists_url": "https://api.github.com/users/ffmpbgrnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/ffmpbgrnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ffmpbgrnn/subscriptions", "organizations_url": "https://api.github.com/users/ffmpbgrnn/orgs", "repos_url": "https://api.github.com/users/ffmpbgrnn/repos", "events_url": "https://api.github.com/users/ffmpbgrnn/events{/privacy}", "received_events_url": "https://api.github.com/users/ffmpbgrnn/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-07T06:28:52Z", "updated_at": "2017-01-07T06:28:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2342391\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuanbyu\">@yuanbyu</a> , it seems slim.batch_norm still cannot be used inside while_loop. Here is the snippet to reproduce the error: <code>tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'control_dependency' has inputs from different frames. The input 'update_barrier' is in frame 'iteration/while/iteration/while/'. The input 'Mean' is in frame ''.</code>, I am using commit\u00a0<code>2255bc2c1ada8a300adfc8a9a1e68d59c5df4296</code>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.framework <span class=\"pl-k\">import</span> ops\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> control_flow_ops\n\nslim <span class=\"pl-k\">=</span> tf.contrib.slim\n\nsess <span class=\"pl-k\">=</span> tf.Session()\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>state<span class=\"pl-pds\">'</span></span>):\n    x <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(),\n                             <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">1</span>),\n                             <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    update_x <span class=\"pl-k\">=</span> tf.assign(x, x<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">iter_fun</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">y</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> comment the line below, the program will run without any error</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> but I need control_dependencies, or at least some way to replace it...</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> with tf.control_dependencies([update_x]):</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> y = y + x</span>\n    y <span class=\"pl-k\">=</span> slim.batch_norm(y)\n    <span class=\"pl-k\">return</span> (i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, y)\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>iteration<span class=\"pl-pds\">'</span></span>):\n    num_iterations <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n    initial_i <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n    initial_y <span class=\"pl-k\">=</span> tf.constant((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    _, result <span class=\"pl-k\">=</span> tf.while_loop(\n        <span class=\"pl-v\">cond</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">i</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">_</span>: i <span class=\"pl-k\">&lt;</span> num_iterations,\n        <span class=\"pl-v\">body</span><span class=\"pl-k\">=</span>iter_fun,\n        <span class=\"pl-v\">loop_vars</span><span class=\"pl-k\">=</span>(initial_i, initial_y))\n\ntotal_loss <span class=\"pl-k\">=</span> tf.reduce_mean(result)\nbatchnorm_updates <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>(tf.get_collection(ops.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>))\n<span class=\"pl-k\">if</span> batchnorm_updates:\n    <span class=\"pl-k\">with</span> tf.control_dependencies(batchnorm_updates):\n        barrier <span class=\"pl-k\">=</span> tf.no_op(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>update_barrier<span class=\"pl-pds\">'</span></span>)\n    total_loss <span class=\"pl-k\">=</span> control_flow_ops.with_dependencies([barrier], total_loss)\n\nopt <span class=\"pl-k\">=</span> tf.train.AdamOptimizer()\ngradients <span class=\"pl-k\">=</span> opt.compute_gradients(total_loss)\napply_gradient_op <span class=\"pl-k\">=</span> opt.apply_gradients(gradients)\n\ninit_op <span class=\"pl-k\">=</span> tf.global_variables_initializer()\nsess.run(init_op)\n<span class=\"pl-c1\">print</span>(sess.run(apply_gradient_op))\n<span class=\"pl-c1\">print</span>(sess.run(total_loss))</pre></div>", "body_text": "@yuanbyu , it seems slim.batch_norm still cannot be used inside while_loop. Here is the snippet to reproduce the error: tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'control_dependency' has inputs from different frames. The input 'update_barrier' is in frame 'iteration/while/iteration/while/'. The input 'Mean' is in frame ''., I am using commit\u00a02255bc2c1ada8a300adfc8a9a1e68d59c5df4296.\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\nsess = tf.Session()\n\nwith tf.variable_scope('state'):\n    x = tf.get_variable('x', shape=(),\n                             initializer=tf.constant_initializer(1),\n                             dtype=tf.float32)\n    update_x = tf.assign(x, x+1)\n\ndef iter_fun(i, y):\n    # comment the line below, the program will run without any error\n    # but I need control_dependencies, or at least some way to replace it...\n    # with tf.control_dependencies([update_x]):\n        # y = y + x\n    y = slim.batch_norm(y)\n    return (i+1, y)\n\nwith tf.variable_scope('iteration'):\n    num_iterations = 5\n    initial_i = tf.constant(0, dtype=tf.int32)\n    initial_y = tf.constant((1, 2, 3, 4), dtype=tf.float32)\n    _, result = tf.while_loop(\n        cond=lambda i, *_: i < num_iterations,\n        body=iter_fun,\n        loop_vars=(initial_i, initial_y))\n\ntotal_loss = tf.reduce_mean(result)\nbatchnorm_updates = set(tf.get_collection(ops.GraphKeys.UPDATE_OPS))\nif batchnorm_updates:\n    with tf.control_dependencies(batchnorm_updates):\n        barrier = tf.no_op(name='update_barrier')\n    total_loss = control_flow_ops.with_dependencies([barrier], total_loss)\n\nopt = tf.train.AdamOptimizer()\ngradients = opt.compute_gradients(total_loss)\napply_gradient_op = opt.apply_gradients(gradients)\n\ninit_op = tf.global_variables_initializer()\nsess.run(init_op)\nprint(sess.run(apply_gradient_op))\nprint(sess.run(total_loss))", "body": "@yuanbyu , it seems slim.batch_norm still cannot be used inside while_loop. Here is the snippet to reproduce the error: `tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'control_dependency' has inputs from different frames. The input 'update_barrier' is in frame 'iteration/while/iteration/while/'. The input 'Mean' is in frame ''.`, I am using commit\u00a0`2255bc2c1ada8a300adfc8a9a1e68d59c5df4296`.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import control_flow_ops\r\n\r\nslim = tf.contrib.slim\r\n\r\nsess = tf.Session()\r\n\r\nwith tf.variable_scope('state'):\r\n    x = tf.get_variable('x', shape=(),\r\n                             initializer=tf.constant_initializer(1),\r\n                             dtype=tf.float32)\r\n    update_x = tf.assign(x, x+1)\r\n\r\ndef iter_fun(i, y):\r\n    # comment the line below, the program will run without any error\r\n    # but I need control_dependencies, or at least some way to replace it...\r\n    # with tf.control_dependencies([update_x]):\r\n        # y = y + x\r\n    y = slim.batch_norm(y)\r\n    return (i+1, y)\r\n\r\nwith tf.variable_scope('iteration'):\r\n    num_iterations = 5\r\n    initial_i = tf.constant(0, dtype=tf.int32)\r\n    initial_y = tf.constant((1, 2, 3, 4), dtype=tf.float32)\r\n    _, result = tf.while_loop(\r\n        cond=lambda i, *_: i < num_iterations,\r\n        body=iter_fun,\r\n        loop_vars=(initial_i, initial_y))\r\n\r\ntotal_loss = tf.reduce_mean(result)\r\nbatchnorm_updates = set(tf.get_collection(ops.GraphKeys.UPDATE_OPS))\r\nif batchnorm_updates:\r\n    with tf.control_dependencies(batchnorm_updates):\r\n        barrier = tf.no_op(name='update_barrier')\r\n    total_loss = control_flow_ops.with_dependencies([barrier], total_loss)\r\n\r\nopt = tf.train.AdamOptimizer()\r\ngradients = opt.compute_gradients(total_loss)\r\napply_gradient_op = opt.apply_gradients(gradients)\r\n\r\ninit_op = tf.global_variables_initializer()\r\nsess.run(init_op)\r\nprint(sess.run(apply_gradient_op))\r\nprint(sess.run(total_loss))\r\n```"}