{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9329", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9329/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9329/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9329/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9329", "id": 223002495, "node_id": "MDU6SXNzdWUyMjMwMDI0OTU=", "number": 9329, "title": "Getting Error - Exception: No data provided for \"activation_2\"  Need data for each key in: ['input2', 'aux_input', 'input1']", "user": {"login": "ddegraw", "id": 27475048, "node_id": "MDQ6VXNlcjI3NDc1MDQ4", "avatar_url": "https://avatars0.githubusercontent.com/u/27475048?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ddegraw", "html_url": "https://github.com/ddegraw", "followers_url": "https://api.github.com/users/ddegraw/followers", "following_url": "https://api.github.com/users/ddegraw/following{/other_user}", "gists_url": "https://api.github.com/users/ddegraw/gists{/gist_id}", "starred_url": "https://api.github.com/users/ddegraw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ddegraw/subscriptions", "organizations_url": "https://api.github.com/users/ddegraw/orgs", "repos_url": "https://api.github.com/users/ddegraw/repos", "events_url": "https://api.github.com/users/ddegraw/events{/privacy}", "received_events_url": "https://api.github.com/users/ddegraw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-20T09:23:05Z", "updated_at": "2017-04-20T16:35:10Z", "closed_at": "2017-04-20T16:35:10Z", "author_association": "NONE", "body_html": "<p>I am trying to concatenate auxiliary inputs with a siamese lstm.  I have verified the siamese lstm works fine, but cannot add in the auxiliary inputs.  Pleae see code below.</p>\n<p>embedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)</p>\n<p>shared_lstm = Bidirectional(LSTM(num_lstm))</p>\n<p>sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input1')<br>\nembedded_sequences_1 = embedding_layer(sequence_1_input)<br>\nx1 = shared_lstm(embedded_sequences_1)</p>\n<p>sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name='input2')<br>\nembedded_sequences_2 = embedding_layer(sequence_2_input)<br>\ny1 = shared_lstm(embedded_sequences_2)</p>\n<p>merged_lstm = merge([x1,y1], mode='concat')<br>\nmerged_lstm = Dropout(rate_drop_dense)(merged_lstm)<br>\nmerged_lstm = BatchNormalization()(merged_lstm)</p>\n<p>auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(merged_lstm)</p>\n<p>auxiliary_input = Input(shape=(aux_train.shape[1],),name='aux_input')<br>\nfoo = Activation('linear')(auxiliary_input)</p>\n<p>merged = merge([merged_lstm,foo],mode='concat')<br>\nmerged = Dropout(rate_drop_dense)(merged)<br>\nmerged = BatchNormalization()(merged)<br>\nmerged = Dense(num_dense, activation=act)(merged)<br>\nmerged = Dropout(rate_drop_dense)(merged)<br>\nmerged = BatchNormalization()(merged)</p>\n<p>final = Dense(1, activation='sigmoid', name='main_output')(merged)</p>\n<p>train = aux_train.as_matrix()</p>\n<p>model = Model(input=[sequence_1_input,sequence_2_input,auxiliary_input], output=[final,auxiliary_output])<br>\nmodel.compile(loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'}, optimizer='nadam', metrics=['acc'], loss_weights=[1., 0.2])</p>\n<p>hist = model.fit({'input1': data_1, 'input2': data_2, 'aux_input': train}, {'main_output':labels, 'aux_output':labels}, validation_split=VALIDATION_SPLIT, nb_epoch=50, batch_size=1024, shuffle=True,class_weight=class_weight,callbacks=[early_stopping, model_checkpoint])</p>", "body_text": "I am trying to concatenate auxiliary inputs with a siamese lstm.  I have verified the siamese lstm works fine, but cannot add in the auxiliary inputs.  Pleae see code below.\nembedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\nshared_lstm = Bidirectional(LSTM(num_lstm))\nsequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input1')\nembedded_sequences_1 = embedding_layer(sequence_1_input)\nx1 = shared_lstm(embedded_sequences_1)\nsequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name='input2')\nembedded_sequences_2 = embedding_layer(sequence_2_input)\ny1 = shared_lstm(embedded_sequences_2)\nmerged_lstm = merge([x1,y1], mode='concat')\nmerged_lstm = Dropout(rate_drop_dense)(merged_lstm)\nmerged_lstm = BatchNormalization()(merged_lstm)\nauxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(merged_lstm)\nauxiliary_input = Input(shape=(aux_train.shape[1],),name='aux_input')\nfoo = Activation('linear')(auxiliary_input)\nmerged = merge([merged_lstm,foo],mode='concat')\nmerged = Dropout(rate_drop_dense)(merged)\nmerged = BatchNormalization()(merged)\nmerged = Dense(num_dense, activation=act)(merged)\nmerged = Dropout(rate_drop_dense)(merged)\nmerged = BatchNormalization()(merged)\nfinal = Dense(1, activation='sigmoid', name='main_output')(merged)\ntrain = aux_train.as_matrix()\nmodel = Model(input=[sequence_1_input,sequence_2_input,auxiliary_input], output=[final,auxiliary_output])\nmodel.compile(loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'}, optimizer='nadam', metrics=['acc'], loss_weights=[1., 0.2])\nhist = model.fit({'input1': data_1, 'input2': data_2, 'aux_input': train}, {'main_output':labels, 'aux_output':labels}, validation_split=VALIDATION_SPLIT, nb_epoch=50, batch_size=1024, shuffle=True,class_weight=class_weight,callbacks=[early_stopping, model_checkpoint])", "body": "I am trying to concatenate auxiliary inputs with a siamese lstm.  I have verified the siamese lstm works fine, but cannot add in the auxiliary inputs.  Pleae see code below.\r\n\r\nembedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\r\n                            \r\nshared_lstm = Bidirectional(LSTM(num_lstm))\r\n\r\nsequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input1')\r\nembedded_sequences_1 = embedding_layer(sequence_1_input)\r\nx1 = shared_lstm(embedded_sequences_1)\r\n\r\nsequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name='input2')\r\nembedded_sequences_2 = embedding_layer(sequence_2_input)\r\ny1 = shared_lstm(embedded_sequences_2)\r\n\r\nmerged_lstm = merge([x1,y1], mode='concat')\r\nmerged_lstm = Dropout(rate_drop_dense)(merged_lstm)\r\nmerged_lstm = BatchNormalization()(merged_lstm)\r\n\r\nauxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(merged_lstm)\r\n\r\nauxiliary_input = Input(shape=(aux_train.shape[1],),name='aux_input')\r\nfoo = Activation('linear')(auxiliary_input)\r\n\r\nmerged = merge([merged_lstm,foo],mode='concat')\r\nmerged = Dropout(rate_drop_dense)(merged)\r\nmerged = BatchNormalization()(merged)\r\nmerged = Dense(num_dense, activation=act)(merged)\r\nmerged = Dropout(rate_drop_dense)(merged)\r\nmerged = BatchNormalization()(merged)\r\n\r\nfinal = Dense(1, activation='sigmoid', name='main_output')(merged)\r\n\r\ntrain = aux_train.as_matrix()\r\n\r\nmodel = Model(input=[sequence_1_input,sequence_2_input,auxiliary_input], output=[final,auxiliary_output])\r\nmodel.compile(loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'}, optimizer='nadam', metrics=['acc'], loss_weights=[1., 0.2])\r\n\r\nhist = model.fit({'input1': data_1, 'input2': data_2, 'aux_input': train}, {'main_output':labels, 'aux_output':labels}, validation_split=VALIDATION_SPLIT, nb_epoch=50, batch_size=1024, shuffle=True,class_weight=class_weight,callbacks=[early_stopping, model_checkpoint])"}