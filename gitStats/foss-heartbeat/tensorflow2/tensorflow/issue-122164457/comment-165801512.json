{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/165801512", "html_url": "https://github.com/tensorflow/tensorflow/issues/512#issuecomment-165801512", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/512", "id": 165801512, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTgwMTUxMg==", "user": {"login": "ogrisel", "id": 89061, "node_id": "MDQ6VXNlcjg5MDYx", "avatar_url": "https://avatars0.githubusercontent.com/u/89061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ogrisel", "html_url": "https://github.com/ogrisel", "followers_url": "https://api.github.com/users/ogrisel/followers", "following_url": "https://api.github.com/users/ogrisel/following{/other_user}", "gists_url": "https://api.github.com/users/ogrisel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ogrisel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ogrisel/subscriptions", "organizations_url": "https://api.github.com/users/ogrisel/orgs", "repos_url": "https://api.github.com/users/ogrisel/repos", "events_url": "https://api.github.com/users/ogrisel/events{/privacy}", "received_events_url": "https://api.github.com/users/ogrisel/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-18T15:15:51Z", "updated_at": "2015-12-18T15:16:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>But for very short sequences (sequences with a lot of padding in a mini-batch that also comprises much longer sequences), this will result in an increase in the effective softmax attention temperature. I think this equivalent to having an additional implicit hyperparameter that changes for different input sequence lengths. The network might be somewhat robust to it as long as the input sequences do not vary too much but it is probably misleading for the user to have something that does not follow the mathematical description of most papers in the literature. That might be the source of hard to debug confusion when trying to reproduce the results of experiments done by other researchers with other tools.</p>", "body_text": "But for very short sequences (sequences with a lot of padding in a mini-batch that also comprises much longer sequences), this will result in an increase in the effective softmax attention temperature. I think this equivalent to having an additional implicit hyperparameter that changes for different input sequence lengths. The network might be somewhat robust to it as long as the input sequences do not vary too much but it is probably misleading for the user to have something that does not follow the mathematical description of most papers in the literature. That might be the source of hard to debug confusion when trying to reproduce the results of experiments done by other researchers with other tools.", "body": "But for very short sequences (sequences with a lot of padding in a mini-batch that also comprises much longer sequences), this will result in an increase in the effective softmax attention temperature. I think this equivalent to having an additional implicit hyperparameter that changes for different input sequence lengths. The network might be somewhat robust to it as long as the input sequences do not vary too much but it is probably misleading for the user to have something that does not follow the mathematical description of most papers in the literature. That might be the source of hard to debug confusion when trying to reproduce the results of experiments done by other researchers with other tools.\n"}