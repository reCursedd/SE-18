{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/512", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/512/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/512/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/512/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/512", "id": 122164457, "node_id": "MDU6SXNzdWUxMjIxNjQ0NTc=", "number": 512, "title": "Possible bug in attention seq2seq", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2015-12-15T00:31:22Z", "updated_at": "2017-02-02T08:31:15Z", "closed_at": "2016-06-06T20:34:15Z", "author_association": "NONE", "body_html": "<p>source file is seq2seq.py</p>\n<pre><code>def attention(query):\n  \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n  ds = []  # Results of attention reads will be stored here.\n  for a in xrange(num_heads):\n    with vs.variable_scope(\"Attention_%d\" % a): \n      y = rnn_cell.linear(query, attention_vec_size, True)\n      y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n      # Attention mask is a softmax of v^T * tanh(...).\n      s = math_ops.reduce_sum(\n          v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3]) \n      a = nn_ops.softmax(s)\n      # Now calculate the attention-weighted vector d.\n      d = math_ops.reduce_sum(\n          array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n          [1, 2]) \n      ds.append(array_ops.reshape(d, [-1, attn_size]))\n  return ds\n</code></pre>\n<p>at line 440, you compute the energies, ok\u2026 then at line 441 you normalize the energies\u2026 you need to do a -FLT_MAX mask on only the part of the encoder sequence (i.e., you don\u2019t want to pay attention to \u201cempty\u201d part of the sequence)... I don't see the masking op? Or am i missing something?</p>\n<p>I can wrap an op patch for this if nobody is working on this (let me know).</p>\n<p>Thanks all!</p>", "body_text": "source file is seq2seq.py\ndef attention(query):\n  \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n  ds = []  # Results of attention reads will be stored here.\n  for a in xrange(num_heads):\n    with vs.variable_scope(\"Attention_%d\" % a): \n      y = rnn_cell.linear(query, attention_vec_size, True)\n      y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n      # Attention mask is a softmax of v^T * tanh(...).\n      s = math_ops.reduce_sum(\n          v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3]) \n      a = nn_ops.softmax(s)\n      # Now calculate the attention-weighted vector d.\n      d = math_ops.reduce_sum(\n          array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n          [1, 2]) \n      ds.append(array_ops.reshape(d, [-1, attn_size]))\n  return ds\n\nat line 440, you compute the energies, ok\u2026 then at line 441 you normalize the energies\u2026 you need to do a -FLT_MAX mask on only the part of the encoder sequence (i.e., you don\u2019t want to pay attention to \u201cempty\u201d part of the sequence)... I don't see the masking op? Or am i missing something?\nI can wrap an op patch for this if nobody is working on this (let me know).\nThanks all!", "body": "source file is seq2seq.py\n\n```\ndef attention(query):\n  \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n  ds = []  # Results of attention reads will be stored here.\n  for a in xrange(num_heads):\n    with vs.variable_scope(\"Attention_%d\" % a): \n      y = rnn_cell.linear(query, attention_vec_size, True)\n      y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n      # Attention mask is a softmax of v^T * tanh(...).\n      s = math_ops.reduce_sum(\n          v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3]) \n      a = nn_ops.softmax(s)\n      # Now calculate the attention-weighted vector d.\n      d = math_ops.reduce_sum(\n          array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n          [1, 2]) \n      ds.append(array_ops.reshape(d, [-1, attn_size]))\n  return ds\n```\n\nat line 440, you compute the energies, ok\u2026 then at line 441 you normalize the energies\u2026 you need to do a -FLT_MAX mask on only the part of the encoder sequence (i.e., you don\u2019t want to pay attention to \u201cempty\u201d part of the sequence)... I don't see the masking op? Or am i missing something?\n\nI can wrap an op patch for this if nobody is working on this (let me know).\n\nThanks all!\n"}