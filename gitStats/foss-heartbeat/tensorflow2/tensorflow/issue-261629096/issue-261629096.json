{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13380", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13380/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13380/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13380/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13380", "id": 261629096, "node_id": "MDU6SXNzdWUyNjE2MjkwOTY=", "number": 13380, "title": "'train_x, train_y = sess.run([train_x, train_y])'  leads machine run slowly", "user": {"login": "LaiPiXiong", "id": 15033269, "node_id": "MDQ6VXNlcjE1MDMzMjY5", "avatar_url": "https://avatars2.githubusercontent.com/u/15033269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LaiPiXiong", "html_url": "https://github.com/LaiPiXiong", "followers_url": "https://api.github.com/users/LaiPiXiong/followers", "following_url": "https://api.github.com/users/LaiPiXiong/following{/other_user}", "gists_url": "https://api.github.com/users/LaiPiXiong/gists{/gist_id}", "starred_url": "https://api.github.com/users/LaiPiXiong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LaiPiXiong/subscriptions", "organizations_url": "https://api.github.com/users/LaiPiXiong/orgs", "repos_url": "https://api.github.com/users/LaiPiXiong/repos", "events_url": "https://api.github.com/users/LaiPiXiong/events{/privacy}", "received_events_url": "https://api.github.com/users/LaiPiXiong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-09-29T12:42:38Z", "updated_at": "2017-10-03T09:42:14Z", "closed_at": "2017-10-02T17:27:09Z", "author_association": "NONE", "body_html": "<p>Hi Team,<br>\nMy program run without error, but it seems not work, while my GPU shows running. The main code are as follows:<br>\n`def main(file_name, batch_size, iter_times):<br>\nx = tf.placeholder('float', (batch_size, 32, 32, 3) )<br>\ny = tf.placeholder('float', shape = [batch_size, 10] )</p>\n<pre><code>predictions, _, _, _ = inference_op(x, keep_prob = 0.5)\npredictions = tf.cast(predictions, tf.float32)\n\ncost = -tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits (labels = y, logits = predictions))\ncorrect_prediction = tf.equal(y, predictions)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\ntrain = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nfor i in xrange(iter_times):\n    train_x, train_y = read_data.fetch_data(file_name, batch_size) \n    train_x, train_y = sess.run([train_x, train_y]) \n    if i % 10 == 0 :\n        train_accuracy = accuracy.eval(feed_dict = {x: train_x, y: train_y})\n        print \"%d step accuarcy is %f\" % (i, sess.run(train_accuracy))\n    sess.run(train)\n</code></pre>\n<p>main('train.tfrecords', 5, 2000)`</p>\n<p>This net is based on  VGG (without the 3rd,4th,5th hidden layers) and the dataset is SVHN. I don't know how to deal with this phenomenon......</p>", "body_text": "Hi Team,\nMy program run without error, but it seems not work, while my GPU shows running. The main code are as follows:\n`def main(file_name, batch_size, iter_times):\nx = tf.placeholder('float', (batch_size, 32, 32, 3) )\ny = tf.placeholder('float', shape = [batch_size, 10] )\npredictions, _, _, _ = inference_op(x, keep_prob = 0.5)\npredictions = tf.cast(predictions, tf.float32)\n\ncost = -tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits (labels = y, logits = predictions))\ncorrect_prediction = tf.equal(y, predictions)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\ntrain = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nfor i in xrange(iter_times):\n    train_x, train_y = read_data.fetch_data(file_name, batch_size) \n    train_x, train_y = sess.run([train_x, train_y]) \n    if i % 10 == 0 :\n        train_accuracy = accuracy.eval(feed_dict = {x: train_x, y: train_y})\n        print \"%d step accuarcy is %f\" % (i, sess.run(train_accuracy))\n    sess.run(train)\n\nmain('train.tfrecords', 5, 2000)`\nThis net is based on  VGG (without the 3rd,4th,5th hidden layers) and the dataset is SVHN. I don't know how to deal with this phenomenon......", "body": "Hi Team,\r\n     My program run without error, but it seems not work, while my GPU shows running. The main code are as follows:\r\n`def main(file_name, batch_size, iter_times):\r\n    x = tf.placeholder('float', (batch_size, 32, 32, 3) )\r\n    y = tf.placeholder('float', shape = [batch_size, 10] )\r\n\r\n    predictions, _, _, _ = inference_op(x, keep_prob = 0.5)\r\n    predictions = tf.cast(predictions, tf.float32)\r\n\r\n    cost = -tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits (labels = y, logits = predictions))\r\n    correct_prediction = tf.equal(y, predictions)\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\r\n    train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\r\n    init = tf.global_variables_initializer()\r\n    sess = tf.Session()\r\n    sess.run(init)\r\n    for i in xrange(iter_times):\r\n        train_x, train_y = read_data.fetch_data(file_name, batch_size) \r\n        train_x, train_y = sess.run([train_x, train_y]) \r\n        if i % 10 == 0 :\r\n            train_accuracy = accuracy.eval(feed_dict = {x: train_x, y: train_y})\r\n            print \"%d step accuarcy is %f\" % (i, sess.run(train_accuracy))\r\n        sess.run(train)\r\n\r\nmain('train.tfrecords', 5, 2000)`\r\n\r\nThis net is based on  VGG (without the 3rd,4th,5th hidden layers) and the dataset is SVHN. I don't know how to deal with this phenomenon......\r\n    "}