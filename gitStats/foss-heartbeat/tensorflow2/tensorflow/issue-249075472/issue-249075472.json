{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12148", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12148/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12148/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12148/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12148", "id": 249075472, "node_id": "MDU6SXNzdWUyNDkwNzU0NzI=", "number": 12148, "title": "Tensorflow Experiment shape mismatch between train set and test set", "user": {"login": "nikky78", "id": 29562101, "node_id": "MDQ6VXNlcjI5NTYyMTAx", "avatar_url": "https://avatars1.githubusercontent.com/u/29562101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikky78", "html_url": "https://github.com/nikky78", "followers_url": "https://api.github.com/users/nikky78/followers", "following_url": "https://api.github.com/users/nikky78/following{/other_user}", "gists_url": "https://api.github.com/users/nikky78/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikky78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikky78/subscriptions", "organizations_url": "https://api.github.com/users/nikky78/orgs", "repos_url": "https://api.github.com/users/nikky78/repos", "events_url": "https://api.github.com/users/nikky78/events{/privacy}", "received_events_url": "https://api.github.com/users/nikky78/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-09T15:44:41Z", "updated_at": "2018-01-27T01:31:16Z", "closed_at": "2017-08-09T20:06:32Z", "author_association": "NONE", "body_html": "<p>Hi guys,</p>\n<p>I am not sure if this is a bug or my mistake but it looks like when the experiment evaluates with the testing set it expects it to be of the same shape as the training set.</p>\n<p>First let me show you how I feed the experiment with the train set and eval set:</p>\n<pre><code>def input_fun(data):\n        x, y = data\n        x, y = np.reshape(x,(1, -1, n_inputs)), np.reshape(y,(1, -1, n_outputs))\n        return tf.constant(x, dtype = tf.float32), tf.constant(y, dtype = tf.float32)\n\n    def get_train_inputs():\n        return input_fun(train_set)\n\n    def get_test_inputs():\n        return input_fun(test_set)\n</code></pre>\n<p>when I print my train set and eval set, I have :</p>\n<blockquote>\n<p>(&lt;tf.Tensor 'Const_22:0' shape=(1, 1000, 1) dtype=float32&gt;, &lt;tf.Tensor 'Const_23:0' shape=(1, 1000, 1) dtype=float32&gt;)<br>\n(&lt;tf.Tensor 'Const_24:0' shape=(1, 100, 1) dtype=float32&gt;, &lt;tf.Tensor 'Const_25:0' shape=(1, 100, 1) dtype=float32&gt;)</p>\n</blockquote>\n<p>Then I build my model:</p>\n<pre><code>def model_fn(x, y, mode, params):\n       predict = prediction(x)\n       loss = None\n       train_op = None\n       eval_metric_ops = None\n    \n       if mode == learn.ModeKeys.TRAIN or mode == learn.ModeKeys.EVAL:\n          loss = model_loss(y, predict, mode)\n          #eval_metric_ops = { \"rmse\":tf.metrics.root_mean_squared_error(tf.cast(y,tf.float32), predict) }\n    \n       if mode == learn.ModeKeys.TRAIN:\n          global_step = tf.train.get_global_step()\n          #train_op = model_train_op(loss, params['learning_rate'], global_step, mode)\n    \n          learning_rate = tf.train.exponential_decay(learning_rate = params[\"learning_rate\"], \n                                                  global_step = tf.contrib.framework.get_global_step(), \n                                                  decay_steps = 20, \n                                                  decay_rate = 0.96, \n                                                  staircase = True)\n\n           train_op = tf.contrib.layers.optimize_loss(loss = loss,\n                                              global_step = tf.contrib.framework.get_global_step(),\n                                              learning_rate = learning_rate,\n                                              optimizer = \"Adam\")\n    \n       predictions = {\"predictions\": predict}\n    \n       return model_fn_lib.ModelFnOps(\n          mode = mode, \n          predictions = predictions,\n          loss = loss, \n          train_op = train_op,\n       )\n</code></pre>\n<p>Then I define the experiment:</p>\n<pre><code>def experiment_fn(output_dir):\n        model_params = {'learning_rate': 0.01}\n        trainingConfig = tf.contrib.learn.RunConfig(save_checkpoints_steps = 4, save_summary_steps = 2)\n        export_strategy = saved_model_export_utils.make_export_strategy(serving_input_fn=serving_input_fn, exports_to_keep=None)\n        hooks = [\n            #tf.train.LoggingTensorHook({'loss'}, every_n_iter = 2),\n            tf.train.StepCounterHook(every_n_steps = 2, output_dir = output_dir),\n            tf.train.CheckpointSaverHook(output_dir, save_steps = 10, checkpoint_basename = 'model.ckpt'),\n            tf.train.SummarySaverHook(\n                save_steps = 10, \n                output_dir = output_dir, \n                #summary_op = ['loss'],\n                scaffold= tf.train.Scaffold(),\n                summary_op=tf.summary.merge_all()\n            )\n        ]\n        return learn.Experiment(\n            estimator = learn.Estimator(model_fn = model_fn, \n                                params = model_params, \n                                model_dir = output_dir, \n                                config = trainingConfig),\n            train_input_fn = get_train_inputs,\n            eval_input_fn = get_test_inputs,\n            #eval_metrics = model_eval_metrics(),\n            train_steps = 100,\n            #train_monitors = hooks,\n            eval_hooks = hooks,\n            export_strategies = export_strategy\n        )\n</code></pre>\n<p>Eventually I run this line:</p>\n<pre><code>learn_runner.run(experiment_fn = experiment_fn, \n                    output_dir = outdir)\n</code></pre>\n<p>The results are as followed:</p>\n<blockquote>\n<p>Monitors are deprecated. Please use tf.train.SessionRunHook.<br>\nINFO:tensorflow:Create CheckpointSaverHook.<br>\nINFO:tensorflow:Saving checkpoints for 1 into .\\model.ckpt.<br>\nINFO:tensorflow:step = 1, loss = 0.043889</p>\n</blockquote>\n<p>It works fine for the training set, but when it evaluates on the test set I get the following error:</p>\n<blockquote>\n<p>ValueError: Features are incompatible with given information. Given features: Tensor(\"Const:0\", shape=(1, 100, 1), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(1), Dimension(1000), Dimension(1)]), is_sparse=False).</p>\n</blockquote>\n<p>Do you know where it comes from ?</p>\n<p>Thanks!</p>", "body_text": "Hi guys,\nI am not sure if this is a bug or my mistake but it looks like when the experiment evaluates with the testing set it expects it to be of the same shape as the training set.\nFirst let me show you how I feed the experiment with the train set and eval set:\ndef input_fun(data):\n        x, y = data\n        x, y = np.reshape(x,(1, -1, n_inputs)), np.reshape(y,(1, -1, n_outputs))\n        return tf.constant(x, dtype = tf.float32), tf.constant(y, dtype = tf.float32)\n\n    def get_train_inputs():\n        return input_fun(train_set)\n\n    def get_test_inputs():\n        return input_fun(test_set)\n\nwhen I print my train set and eval set, I have :\n\n(<tf.Tensor 'Const_22:0' shape=(1, 1000, 1) dtype=float32>, <tf.Tensor 'Const_23:0' shape=(1, 1000, 1) dtype=float32>)\n(<tf.Tensor 'Const_24:0' shape=(1, 100, 1) dtype=float32>, <tf.Tensor 'Const_25:0' shape=(1, 100, 1) dtype=float32>)\n\nThen I build my model:\ndef model_fn(x, y, mode, params):\n       predict = prediction(x)\n       loss = None\n       train_op = None\n       eval_metric_ops = None\n    \n       if mode == learn.ModeKeys.TRAIN or mode == learn.ModeKeys.EVAL:\n          loss = model_loss(y, predict, mode)\n          #eval_metric_ops = { \"rmse\":tf.metrics.root_mean_squared_error(tf.cast(y,tf.float32), predict) }\n    \n       if mode == learn.ModeKeys.TRAIN:\n          global_step = tf.train.get_global_step()\n          #train_op = model_train_op(loss, params['learning_rate'], global_step, mode)\n    \n          learning_rate = tf.train.exponential_decay(learning_rate = params[\"learning_rate\"], \n                                                  global_step = tf.contrib.framework.get_global_step(), \n                                                  decay_steps = 20, \n                                                  decay_rate = 0.96, \n                                                  staircase = True)\n\n           train_op = tf.contrib.layers.optimize_loss(loss = loss,\n                                              global_step = tf.contrib.framework.get_global_step(),\n                                              learning_rate = learning_rate,\n                                              optimizer = \"Adam\")\n    \n       predictions = {\"predictions\": predict}\n    \n       return model_fn_lib.ModelFnOps(\n          mode = mode, \n          predictions = predictions,\n          loss = loss, \n          train_op = train_op,\n       )\n\nThen I define the experiment:\ndef experiment_fn(output_dir):\n        model_params = {'learning_rate': 0.01}\n        trainingConfig = tf.contrib.learn.RunConfig(save_checkpoints_steps = 4, save_summary_steps = 2)\n        export_strategy = saved_model_export_utils.make_export_strategy(serving_input_fn=serving_input_fn, exports_to_keep=None)\n        hooks = [\n            #tf.train.LoggingTensorHook({'loss'}, every_n_iter = 2),\n            tf.train.StepCounterHook(every_n_steps = 2, output_dir = output_dir),\n            tf.train.CheckpointSaverHook(output_dir, save_steps = 10, checkpoint_basename = 'model.ckpt'),\n            tf.train.SummarySaverHook(\n                save_steps = 10, \n                output_dir = output_dir, \n                #summary_op = ['loss'],\n                scaffold= tf.train.Scaffold(),\n                summary_op=tf.summary.merge_all()\n            )\n        ]\n        return learn.Experiment(\n            estimator = learn.Estimator(model_fn = model_fn, \n                                params = model_params, \n                                model_dir = output_dir, \n                                config = trainingConfig),\n            train_input_fn = get_train_inputs,\n            eval_input_fn = get_test_inputs,\n            #eval_metrics = model_eval_metrics(),\n            train_steps = 100,\n            #train_monitors = hooks,\n            eval_hooks = hooks,\n            export_strategies = export_strategy\n        )\n\nEventually I run this line:\nlearn_runner.run(experiment_fn = experiment_fn, \n                    output_dir = outdir)\n\nThe results are as followed:\n\nMonitors are deprecated. Please use tf.train.SessionRunHook.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Saving checkpoints for 1 into .\\model.ckpt.\nINFO:tensorflow:step = 1, loss = 0.043889\n\nIt works fine for the training set, but when it evaluates on the test set I get the following error:\n\nValueError: Features are incompatible with given information. Given features: Tensor(\"Const:0\", shape=(1, 100, 1), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(1), Dimension(1000), Dimension(1)]), is_sparse=False).\n\nDo you know where it comes from ?\nThanks!", "body": "Hi guys,\r\n\r\nI am not sure if this is a bug or my mistake but it looks like when the experiment evaluates with the testing set it expects it to be of the same shape as the training set.\r\n\r\nFirst let me show you how I feed the experiment with the train set and eval set:\r\n\r\n```\r\ndef input_fun(data):\r\n        x, y = data\r\n        x, y = np.reshape(x,(1, -1, n_inputs)), np.reshape(y,(1, -1, n_outputs))\r\n        return tf.constant(x, dtype = tf.float32), tf.constant(y, dtype = tf.float32)\r\n\r\n    def get_train_inputs():\r\n        return input_fun(train_set)\r\n\r\n    def get_test_inputs():\r\n        return input_fun(test_set)\r\n```\r\n\r\nwhen I print my train set and eval set, I have :\r\n   \r\n> (<tf.Tensor 'Const_22:0' shape=(1, 1000, 1) dtype=float32>, <tf.Tensor 'Const_23:0' shape=(1, 1000, 1) dtype=float32>)\r\n>     (<tf.Tensor 'Const_24:0' shape=(1, 100, 1) dtype=float32>, <tf.Tensor 'Const_25:0' shape=(1, 100, 1) dtype=float32>)\r\n\r\nThen I build my model: \r\n\r\n```\r\ndef model_fn(x, y, mode, params):\r\n       predict = prediction(x)\r\n       loss = None\r\n       train_op = None\r\n       eval_metric_ops = None\r\n    \r\n       if mode == learn.ModeKeys.TRAIN or mode == learn.ModeKeys.EVAL:\r\n          loss = model_loss(y, predict, mode)\r\n          #eval_metric_ops = { \"rmse\":tf.metrics.root_mean_squared_error(tf.cast(y,tf.float32), predict) }\r\n    \r\n       if mode == learn.ModeKeys.TRAIN:\r\n          global_step = tf.train.get_global_step()\r\n          #train_op = model_train_op(loss, params['learning_rate'], global_step, mode)\r\n    \r\n          learning_rate = tf.train.exponential_decay(learning_rate = params[\"learning_rate\"], \r\n                                                  global_step = tf.contrib.framework.get_global_step(), \r\n                                                  decay_steps = 20, \r\n                                                  decay_rate = 0.96, \r\n                                                  staircase = True)\r\n\r\n           train_op = tf.contrib.layers.optimize_loss(loss = loss,\r\n                                              global_step = tf.contrib.framework.get_global_step(),\r\n                                              learning_rate = learning_rate,\r\n                                              optimizer = \"Adam\")\r\n    \r\n       predictions = {\"predictions\": predict}\r\n    \r\n       return model_fn_lib.ModelFnOps(\r\n          mode = mode, \r\n          predictions = predictions,\r\n          loss = loss, \r\n          train_op = train_op,\r\n       )\r\n```\r\n\r\nThen I define the experiment:\r\n\r\n```\r\ndef experiment_fn(output_dir):\r\n        model_params = {'learning_rate': 0.01}\r\n        trainingConfig = tf.contrib.learn.RunConfig(save_checkpoints_steps = 4, save_summary_steps = 2)\r\n        export_strategy = saved_model_export_utils.make_export_strategy(serving_input_fn=serving_input_fn, exports_to_keep=None)\r\n        hooks = [\r\n            #tf.train.LoggingTensorHook({'loss'}, every_n_iter = 2),\r\n            tf.train.StepCounterHook(every_n_steps = 2, output_dir = output_dir),\r\n            tf.train.CheckpointSaverHook(output_dir, save_steps = 10, checkpoint_basename = 'model.ckpt'),\r\n            tf.train.SummarySaverHook(\r\n                save_steps = 10, \r\n                output_dir = output_dir, \r\n                #summary_op = ['loss'],\r\n                scaffold= tf.train.Scaffold(),\r\n                summary_op=tf.summary.merge_all()\r\n            )\r\n        ]\r\n        return learn.Experiment(\r\n            estimator = learn.Estimator(model_fn = model_fn, \r\n                                params = model_params, \r\n                                model_dir = output_dir, \r\n                                config = trainingConfig),\r\n            train_input_fn = get_train_inputs,\r\n            eval_input_fn = get_test_inputs,\r\n            #eval_metrics = model_eval_metrics(),\r\n            train_steps = 100,\r\n            #train_monitors = hooks,\r\n            eval_hooks = hooks,\r\n            export_strategies = export_strategy\r\n        )\r\n```\r\n\r\nEventually I run this line:\r\n\r\n ```\r\nlearn_runner.run(experiment_fn = experiment_fn, \r\n                     output_dir = outdir)\r\n```\r\n\r\nThe results are as followed:\r\n\r\n> Monitors are deprecated. Please use tf.train.SessionRunHook.\r\n>     INFO:tensorflow:Create CheckpointSaverHook.\r\n>     INFO:tensorflow:Saving checkpoints for 1 into .\\model.ckpt.\r\n>     INFO:tensorflow:step = 1, loss = 0.043889\r\n\r\n\r\nIt works fine for the training set, but when it evaluates on the test set I get the following error:\r\n\r\n> ValueError: Features are incompatible with given information. Given features: Tensor(\"Const:0\", shape=(1, 100, 1), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(1), Dimension(1000), Dimension(1)]), is_sparse=False).\r\n\r\nDo you know where it comes from ?\r\n\r\nThanks!"}