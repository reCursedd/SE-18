{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/221593400", "html_url": "https://github.com/tensorflow/tensorflow/pull/2440#issuecomment-221593400", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2440", "id": 221593400, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTU5MzQwMA==", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-25T14:27:28Z", "updated_at": "2016-05-25T14:27:28Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=575738\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cg31\">@cg31</a> No, it's focused on the C++ core needed to load and run pre-trained inference models, which doesn't include the generated cc ops and headers. In this it's similar to the Android libraries we build with Bazel. We haven't defined this core subset of files explicitly before, so I'm hoping to document what we're doing a bit more.</p>\n<p>It would also be possible to add the cc ops to the makefile, but I'm trying to keep it as minimal as possible since my focus is porting to resource-limited platforms.</p>", "body_text": "@cg31 No, it's focused on the C++ core needed to load and run pre-trained inference models, which doesn't include the generated cc ops and headers. In this it's similar to the Android libraries we build with Bazel. We haven't defined this core subset of files explicitly before, so I'm hoping to document what we're doing a bit more.\nIt would also be possible to add the cc ops to the makefile, but I'm trying to keep it as minimal as possible since my focus is porting to resource-limited platforms.", "body": "@cg31 No, it's focused on the C++ core needed to load and run pre-trained inference models, which doesn't include the generated cc ops and headers. In this it's similar to the Android libraries we build with Bazel. We haven't defined this core subset of files explicitly before, so I'm hoping to document what we're doing a bit more.\n\nIt would also be possible to add the cc ops to the makefile, but I'm trying to keep it as minimal as possible since my focus is porting to resource-limited platforms.\n"}