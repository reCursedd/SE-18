{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/376583934", "html_url": "https://github.com/tensorflow/tensorflow/issues/14000#issuecomment-376583934", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14000", "id": 376583934, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjU4MzkzNA==", "user": {"login": "jiayiliu", "id": 1511514, "node_id": "MDQ6VXNlcjE1MTE1MTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1511514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiayiliu", "html_url": "https://github.com/jiayiliu", "followers_url": "https://api.github.com/users/jiayiliu/followers", "following_url": "https://api.github.com/users/jiayiliu/following{/other_user}", "gists_url": "https://api.github.com/users/jiayiliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiayiliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiayiliu/subscriptions", "organizations_url": "https://api.github.com/users/jiayiliu/orgs", "repos_url": "https://api.github.com/users/jiayiliu/repos", "events_url": "https://api.github.com/users/jiayiliu/events{/privacy}", "received_events_url": "https://api.github.com/users/jiayiliu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-27T16:13:57Z", "updated_at": "2018-03-30T05:45:23Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=726075\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nealwu\">@nealwu</a></p>\n<p>I tried with the following code by train and test with different orders and the same order.<br>\nI don't think the restore process corrects the order for me...<br>\nDid I miss something in your code?  Thanks</p>\n<p>PS I am using TF ver 1.6 with Cuda 9.</p>\n<pre><code>python test.py -o channels_first\npython test.py --test -o channels_first # get the same accuracy\npython test.py --test -o channels_last # get different accuracy \n</code></pre>\n<p>Sample code</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom argparse import ArgumentParser\n\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pylab as plt\n\nsave_path = \"log/test_save.ckpt\"\nsample_size = 1024\nbatch_size = 128\nniter = 10\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--test\", dest='train_flag', action='store_false', default=True)\n    parser.add_argument(\"--order\", \"-o\", default=\"channels_first\", choices=[\"channels_first\", \"channels_last\"])\n    args = parser.parse_args()\n\n    n_batch = int(sample_size/batch_size)\n\n    np.random.seed(123)\n    label = np.random.randint(0, 10, size=sample_size)\n    if args.order == \"channels_first\":\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 1, 28, 28)\n        plt.imshow(img[0,0,:,:])\n        plt.savefig(\"img_first.png\")\n    else:\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 28, 28, 1)\n        plt.imshow(img[0, :, :, 0])\n        plt.savefig(\"img_last.png\")\n\n    img_shape = list(img.shape)\n    img_shape[0] = None\n    tf_input = tf.placeholder(tf.float32, shape=img_shape)\n    tf_label = tf.placeholder(tf.int32, shape=[None])\n\n    v = tf.layers.conv2d(tf_input, 6, 5, data_format=args.order, activation=tf.nn.relu)\n    if args.order == \"channels_last\":  # force to load channels_first model - just demonstration purpose.\n        v = tf.transpose(v, [0,3,1,2])\n    v = tf.layers.flatten(v)\n    v = tf.layers.dense(v, 10)\n\n    tf_sparse_labels = tf.stop_gradient(tf.one_hot(tf_label, 10))\n    tf_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_sparse_labels, logits=v)\n\n    opt = tf.train.AdamOptimizer()\n    opt_op = opt.minimize(tf_loss)\n\n    tf_pred = tf.argmax(v, axis=-1, output_type=tf.int32)\n    tf_accu = tf.reduce_mean(tf.cast(tf.equal(tf_pred, tf_label), tf.float32))\n\n    saver = tf.train.Saver()\n\n    with tf.Session(graph=tf.get_default_graph()) as sess:\n        if args.train_flag:\n            print(\"training\")\n            sess.run(tf.global_variables_initializer())\n            for _ in range(niter):\n                for i in range(n_batch):\n                    start = i * batch_size\n                    end = start + batch_size\n                    feed_dict = {tf_input: img[start:end], tf_label:label[start:end]}\n                    sess.run([opt_op], feed_dict=feed_dict)\n\n                accu = sess.run(tf_accu, feed_dict={tf_input: img, tf_label: label})\n                print(accu)\n            saver.save(sess, save_path)\n        else:\n            print(\"testing \")\n            saver.restore(sess, save_path)\n            feed_dict = {tf_input: img, tf_label: label}\n            accu = sess.run(tf_accu, feed_dict=feed_dict)\n            print(accu)\n</code></pre>", "body_text": "Hi @nealwu\nI tried with the following code by train and test with different orders and the same order.\nI don't think the restore process corrects the order for me...\nDid I miss something in your code?  Thanks\nPS I am using TF ver 1.6 with Cuda 9.\npython test.py -o channels_first\npython test.py --test -o channels_first # get the same accuracy\npython test.py --test -o channels_last # get different accuracy \n\nSample code\nimport tensorflow as tf\nimport numpy as np\nfrom argparse import ArgumentParser\n\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pylab as plt\n\nsave_path = \"log/test_save.ckpt\"\nsample_size = 1024\nbatch_size = 128\nniter = 10\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--test\", dest='train_flag', action='store_false', default=True)\n    parser.add_argument(\"--order\", \"-o\", default=\"channels_first\", choices=[\"channels_first\", \"channels_last\"])\n    args = parser.parse_args()\n\n    n_batch = int(sample_size/batch_size)\n\n    np.random.seed(123)\n    label = np.random.randint(0, 10, size=sample_size)\n    if args.order == \"channels_first\":\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 1, 28, 28)\n        plt.imshow(img[0,0,:,:])\n        plt.savefig(\"img_first.png\")\n    else:\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 28, 28, 1)\n        plt.imshow(img[0, :, :, 0])\n        plt.savefig(\"img_last.png\")\n\n    img_shape = list(img.shape)\n    img_shape[0] = None\n    tf_input = tf.placeholder(tf.float32, shape=img_shape)\n    tf_label = tf.placeholder(tf.int32, shape=[None])\n\n    v = tf.layers.conv2d(tf_input, 6, 5, data_format=args.order, activation=tf.nn.relu)\n    if args.order == \"channels_last\":  # force to load channels_first model - just demonstration purpose.\n        v = tf.transpose(v, [0,3,1,2])\n    v = tf.layers.flatten(v)\n    v = tf.layers.dense(v, 10)\n\n    tf_sparse_labels = tf.stop_gradient(tf.one_hot(tf_label, 10))\n    tf_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_sparse_labels, logits=v)\n\n    opt = tf.train.AdamOptimizer()\n    opt_op = opt.minimize(tf_loss)\n\n    tf_pred = tf.argmax(v, axis=-1, output_type=tf.int32)\n    tf_accu = tf.reduce_mean(tf.cast(tf.equal(tf_pred, tf_label), tf.float32))\n\n    saver = tf.train.Saver()\n\n    with tf.Session(graph=tf.get_default_graph()) as sess:\n        if args.train_flag:\n            print(\"training\")\n            sess.run(tf.global_variables_initializer())\n            for _ in range(niter):\n                for i in range(n_batch):\n                    start = i * batch_size\n                    end = start + batch_size\n                    feed_dict = {tf_input: img[start:end], tf_label:label[start:end]}\n                    sess.run([opt_op], feed_dict=feed_dict)\n\n                accu = sess.run(tf_accu, feed_dict={tf_input: img, tf_label: label})\n                print(accu)\n            saver.save(sess, save_path)\n        else:\n            print(\"testing \")\n            saver.restore(sess, save_path)\n            feed_dict = {tf_input: img, tf_label: label}\n            accu = sess.run(tf_accu, feed_dict=feed_dict)\n            print(accu)", "body": "Hi @nealwu \r\n\r\nI tried with the following code by train and test with different orders and the same order.\r\nI don't think the restore process corrects the order for me...\r\nDid I miss something in your code?  Thanks\r\n\r\nPS I am using TF ver 1.6 with Cuda 9.\r\n```\r\npython test.py -o channels_first\r\npython test.py --test -o channels_first # get the same accuracy\r\npython test.py --test -o channels_last # get different accuracy \r\n```\r\n\r\nSample code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom argparse import ArgumentParser\r\n\r\n\r\nimport matplotlib\r\nmatplotlib.use('Agg')\r\nimport matplotlib.pylab as plt\r\n\r\nsave_path = \"log/test_save.ckpt\"\r\nsample_size = 1024\r\nbatch_size = 128\r\nniter = 10\r\n\r\nif __name__ == \"__main__\":\r\n    parser = ArgumentParser()\r\n    parser.add_argument(\"--test\", dest='train_flag', action='store_false', default=True)\r\n    parser.add_argument(\"--order\", \"-o\", default=\"channels_first\", choices=[\"channels_first\", \"channels_last\"])\r\n    args = parser.parse_args()\r\n\r\n    n_batch = int(sample_size/batch_size)\r\n\r\n    np.random.seed(123)\r\n    label = np.random.randint(0, 10, size=sample_size)\r\n    if args.order == \"channels_first\":\r\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 1, 28, 28)\r\n        plt.imshow(img[0,0,:,:])\r\n        plt.savefig(\"img_first.png\")\r\n    else:\r\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 28, 28, 1)\r\n        plt.imshow(img[0, :, :, 0])\r\n        plt.savefig(\"img_last.png\")\r\n\r\n    img_shape = list(img.shape)\r\n    img_shape[0] = None\r\n    tf_input = tf.placeholder(tf.float32, shape=img_shape)\r\n    tf_label = tf.placeholder(tf.int32, shape=[None])\r\n\r\n    v = tf.layers.conv2d(tf_input, 6, 5, data_format=args.order, activation=tf.nn.relu)\r\n    if args.order == \"channels_last\":  # force to load channels_first model - just demonstration purpose.\r\n        v = tf.transpose(v, [0,3,1,2])\r\n    v = tf.layers.flatten(v)\r\n    v = tf.layers.dense(v, 10)\r\n\r\n    tf_sparse_labels = tf.stop_gradient(tf.one_hot(tf_label, 10))\r\n    tf_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_sparse_labels, logits=v)\r\n\r\n    opt = tf.train.AdamOptimizer()\r\n    opt_op = opt.minimize(tf_loss)\r\n\r\n    tf_pred = tf.argmax(v, axis=-1, output_type=tf.int32)\r\n    tf_accu = tf.reduce_mean(tf.cast(tf.equal(tf_pred, tf_label), tf.float32))\r\n\r\n    saver = tf.train.Saver()\r\n\r\n    with tf.Session(graph=tf.get_default_graph()) as sess:\r\n        if args.train_flag:\r\n            print(\"training\")\r\n            sess.run(tf.global_variables_initializer())\r\n            for _ in range(niter):\r\n                for i in range(n_batch):\r\n                    start = i * batch_size\r\n                    end = start + batch_size\r\n                    feed_dict = {tf_input: img[start:end], tf_label:label[start:end]}\r\n                    sess.run([opt_op], feed_dict=feed_dict)\r\n\r\n                accu = sess.run(tf_accu, feed_dict={tf_input: img, tf_label: label})\r\n                print(accu)\r\n            saver.save(sess, save_path)\r\n        else:\r\n            print(\"testing \")\r\n            saver.restore(sess, save_path)\r\n            feed_dict = {tf_input: img, tf_label: label}\r\n            accu = sess.run(tf_accu, feed_dict=feed_dict)\r\n            print(accu)\r\n```"}