{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421171177", "html_url": "https://github.com/tensorflow/tensorflow/issues/22048#issuecomment-421171177", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22048", "id": 421171177, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTE3MTE3Nw==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-13T22:20:03Z", "updated_at": "2018-09-13T22:20:03Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">Not quite, as you will find that you can reproduce this issue even with\neager execution enabled. Dynamic graphs would help if we only needed to\nsupport the case where the l2norm is a scalar (that is when there is no\naxis argument).\n\nThe underlying issue is that an op like tf.maximum, which we use here to\npick which coordinates of the tensor need to be divided by the norm,\nproduces a gradient of 0 wrt the inputs it did not use to compute the\noutput. At the same time, an op like tf.sqrt() produces a gradient of\nupstream_gradient * 1/2sqrt(input). If 1/2sqrt(input) is inf or NaN,\nmultiplying it by 0 (which is the upstream gradient for the coordinates\nwhich were not used) will result in a NaN, which is what you're seeing here.\n\nWe are looking into fixing this overall issue but it's tricky to do so\nwithout slowing down all operations whose gradients boil down up\nupstream_gradient * f(x) when f(x) can be inf or NaN.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Thu, Sep 13, 2018 at 2:42 PM Octavian Ganea ***@***.***&gt; wrote:\n Yes, I think it's called \"dynamic graphs\" :)\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"356706826\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/22048\" href=\"https://github.com/tensorflow/tensorflow/issues/22048#issuecomment-421161926\">#22048 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxbP6PBJ4r1skKHmNNETdqcrB2jP9ks5uatE8gaJpZM4WYiLF\">https://github.com/notifications/unsubscribe-auth/AAATxbP6PBJ4r1skKHmNNETdqcrB2jP9ks5uatE8gaJpZM4WYiLF</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "Not quite, as you will find that you can reproduce this issue even with\neager execution enabled. Dynamic graphs would help if we only needed to\nsupport the case where the l2norm is a scalar (that is when there is no\naxis argument).\n\nThe underlying issue is that an op like tf.maximum, which we use here to\npick which coordinates of the tensor need to be divided by the norm,\nproduces a gradient of 0 wrt the inputs it did not use to compute the\noutput. At the same time, an op like tf.sqrt() produces a gradient of\nupstream_gradient * 1/2sqrt(input). If 1/2sqrt(input) is inf or NaN,\nmultiplying it by 0 (which is the upstream gradient for the coordinates\nwhich were not used) will result in a NaN, which is what you're seeing here.\n\nWe are looking into fixing this overall issue but it's tricky to do so\nwithout slowing down all operations whose gradients boil down up\nupstream_gradient * f(x) when f(x) can be inf or NaN.\n\u2026\nOn Thu, Sep 13, 2018 at 2:42 PM Octavian Ganea ***@***.***> wrote:\n Yes, I think it's called \"dynamic graphs\" :)\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n <#22048 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxbP6PBJ4r1skKHmNNETdqcrB2jP9ks5uatE8gaJpZM4WYiLF>\n .\n\n\n-- \n - Alex", "body": "Not quite, as you will find that you can reproduce this issue even with\neager execution enabled. Dynamic graphs would help if we only needed to\nsupport the case where the l2norm is a scalar (that is when there is no\naxis argument).\n\nThe underlying issue is that an op like tf.maximum, which we use here to\npick which coordinates of the tensor need to be divided by the norm,\nproduces a gradient of 0 wrt the inputs it did not use to compute the\noutput. At the same time, an op like tf.sqrt() produces a gradient of\nupstream_gradient * 1/2sqrt(input). If 1/2sqrt(input) is inf or NaN,\nmultiplying it by 0 (which is the upstream gradient for the coordinates\nwhich were not used) will result in a NaN, which is what you're seeing here.\n\nWe are looking into fixing this overall issue but it's tricky to do so\nwithout slowing down all operations whose gradients boil down up\nupstream_gradient * f(x) when f(x) can be inf or NaN.\n\nOn Thu, Sep 13, 2018 at 2:42 PM Octavian Ganea <notifications@github.com>\nwrote:\n\n> Yes, I think it's called \"dynamic graphs\" :)\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22048#issuecomment-421161926>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbP6PBJ4r1skKHmNNETdqcrB2jP9ks5uatE8gaJpZM4WYiLF>\n> .\n>\n\n\n-- \n - Alex\n"}