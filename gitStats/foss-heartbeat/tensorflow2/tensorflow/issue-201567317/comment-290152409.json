{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290152409", "html_url": "https://github.com/tensorflow/tensorflow/issues/6926#issuecomment-290152409", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6926", "id": 290152409, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDE1MjQwOQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-29T16:53:14Z", "updated_at": "2017-03-29T16:53:14Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=178520\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dazraf\">@dazraf</a> : You're welcome to send in a PR, though I'd like to make sure I understand the proposed flow. (FYI: The PR would operate somewhere under <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/maven\"><code>tensorflow/java/maven</code></a>).</p>\n<p>To help me understand the proposed flow, consider the following scenario: User X authors a library libX that depends on TensorFlow. X does not know what platforms libX will be used on, since that choice would be made by applications that depend on libX. In this case, what happens? Does User X have to define platform-specific classifiers for libX and force libX clients to select one? Does this keep propagating? Or are end applications required to select the TensorFlow classifier even though they have no direct dependency on TensorFlow (perhaps they have to add the <code>os-maven-plugin</code> for OS but still need to explicitly choose between CPU and GPU) - which may be fine, but is more than a 4 line addition to their dependencies? (I'm guessing this is why the thread I linked to above discourages use of classifiers for different \"flavors\", but perhaps I'm misinterpreting)</p>\n<p>Note that even as-is, everything is in the maven \"world\" - builds can be fully controlled through maven with the GPU caveat. It seems to me that including GPU support in the <code>.jar</code> for <code>libtensorflow_jni</code> is somewhat orthogonal to the use of classifiers?</p>\n<p>I would be tempted to keep the dependency as simple as possible (e.g., \"add these 4 lines, that's it\"), even if advanced users (for whom the size of the distro is a real concern) have to do something extra.</p>\n<p>Looking at the <a href=\"http://netty.io/wiki/forked-tomcat-native.html#artifacts\" rel=\"nofollow\">netty link you pointed to</a>, this seems similar to the approach used by netty where they provide the <code>netty-tcnative-boringssl-static</code> artifact for the common simple use case and suggest that more nuanced users use a different artifact/classifier. Is your proposal intended to address these \"advanced\" users without affecting the simpler 4-line-addition-to-pom.xml flow?</p>", "body_text": "@dazraf : You're welcome to send in a PR, though I'd like to make sure I understand the proposed flow. (FYI: The PR would operate somewhere under tensorflow/java/maven).\nTo help me understand the proposed flow, consider the following scenario: User X authors a library libX that depends on TensorFlow. X does not know what platforms libX will be used on, since that choice would be made by applications that depend on libX. In this case, what happens? Does User X have to define platform-specific classifiers for libX and force libX clients to select one? Does this keep propagating? Or are end applications required to select the TensorFlow classifier even though they have no direct dependency on TensorFlow (perhaps they have to add the os-maven-plugin for OS but still need to explicitly choose between CPU and GPU) - which may be fine, but is more than a 4 line addition to their dependencies? (I'm guessing this is why the thread I linked to above discourages use of classifiers for different \"flavors\", but perhaps I'm misinterpreting)\nNote that even as-is, everything is in the maven \"world\" - builds can be fully controlled through maven with the GPU caveat. It seems to me that including GPU support in the .jar for libtensorflow_jni is somewhat orthogonal to the use of classifiers?\nI would be tempted to keep the dependency as simple as possible (e.g., \"add these 4 lines, that's it\"), even if advanced users (for whom the size of the distro is a real concern) have to do something extra.\nLooking at the netty link you pointed to, this seems similar to the approach used by netty where they provide the netty-tcnative-boringssl-static artifact for the common simple use case and suggest that more nuanced users use a different artifact/classifier. Is your proposal intended to address these \"advanced\" users without affecting the simpler 4-line-addition-to-pom.xml flow?", "body": "@dazraf : You're welcome to send in a PR, though I'd like to make sure I understand the proposed flow. (FYI: The PR would operate somewhere under [`tensorflow/java/maven`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/maven)).\r\n\r\nTo help me understand the proposed flow, consider the following scenario: User X authors a library libX that depends on TensorFlow. X does not know what platforms libX will be used on, since that choice would be made by applications that depend on libX. In this case, what happens? Does User X have to define platform-specific classifiers for libX and force libX clients to select one? Does this keep propagating? Or are end applications required to select the TensorFlow classifier even though they have no direct dependency on TensorFlow (perhaps they have to add the `os-maven-plugin` for OS but still need to explicitly choose between CPU and GPU) - which may be fine, but is more than a 4 line addition to their dependencies? (I'm guessing this is why the thread I linked to above discourages use of classifiers for different \"flavors\", but perhaps I'm misinterpreting)\r\n\r\nNote that even as-is, everything is in the maven \"world\" - builds can be fully controlled through maven with the GPU caveat. It seems to me that including GPU support in the `.jar` for `libtensorflow_jni` is somewhat orthogonal to the use of classifiers?\r\n\r\nI would be tempted to keep the dependency as simple as possible (e.g., \"add these 4 lines, that's it\"), even if advanced users (for whom the size of the distro is a real concern) have to do something extra.\r\n\r\nLooking at the [netty link you pointed to](http://netty.io/wiki/forked-tomcat-native.html#artifacts), this seems similar to the approach used by netty where they provide the `netty-tcnative-boringssl-static` artifact for the common simple use case and suggest that more nuanced users use a different artifact/classifier. Is your proposal intended to address these \"advanced\" users without affecting the simpler 4-line-addition-to-pom.xml flow?"}