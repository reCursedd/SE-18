{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23606", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23606/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23606/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23606/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23606", "id": 378917283, "node_id": "MDU6SXNzdWUzNzg5MTcyODM=", "number": 23606, "title": "V100 PCIE FP16 PS/worker training data corruption bug occurs when distributed over gRPC", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "dubey", "id": 2314265, "node_id": "MDQ6VXNlcjIzMTQyNjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/2314265?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dubey", "html_url": "https://github.com/dubey", "followers_url": "https://api.github.com/users/dubey/followers", "following_url": "https://api.github.com/users/dubey/following{/other_user}", "gists_url": "https://api.github.com/users/dubey/gists{/gist_id}", "starred_url": "https://api.github.com/users/dubey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dubey/subscriptions", "organizations_url": "https://api.github.com/users/dubey/orgs", "repos_url": "https://api.github.com/users/dubey/repos", "events_url": "https://api.github.com/users/dubey/events{/privacy}", "received_events_url": "https://api.github.com/users/dubey/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "dubey", "id": 2314265, "node_id": "MDQ6VXNlcjIzMTQyNjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/2314265?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dubey", "html_url": "https://github.com/dubey", "followers_url": "https://api.github.com/users/dubey/followers", "following_url": "https://api.github.com/users/dubey/following{/other_user}", "gists_url": "https://api.github.com/users/dubey/gists{/gist_id}", "starred_url": "https://api.github.com/users/dubey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dubey/subscriptions", "organizations_url": "https://api.github.com/users/dubey/orgs", "repos_url": "https://api.github.com/users/dubey/repos", "events_url": "https://api.github.com/users/dubey/events{/privacy}", "received_events_url": "https://api.github.com/users/dubey/received_events", "type": "User", "site_admin": false}, {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-11-08T21:08:07Z", "updated_at": "2018-11-13T12:58:21Z", "closed_at": "2018-11-08T21:28:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): <a href=\"https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.12_compatible\">tensorflow/benchmarks:cnn_tf_v1.12_compatible</a></li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04-hwe</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')</li>\n<li>Python version: 2.7.12</li>\n<li>Bazel version (if compiling from source): N/A</li>\n<li>GCC/Compiler version (if compiling from source): N/A</li>\n<li>CUDA/cuDNN version: 9.0/7.2.1.38-1+cuda9.0</li>\n<li>GPU model and memory: V100-PCIE-32GB, NVIDIA driver version 384.145, with <code>intel_iommu=on</code></li>\n</ul>\n<pre><code>$ cat /proc/cmdline\nBOOT_IMAGE=/boot/vmlinuz-4.15.0-38-generic root=/dev/mapper/vgroot-lvroot ro intel_iommu=on\n</code></pre>\n<pre><code>$ nvidia-smi topo -m\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n</code></pre>\n<p><strong>Describe the current behavior</strong><br>\nFP-16 multi-GPU training with CPU as local parameter server is converging in single process mode, but diverging loss value (nan) adding another loopback PS process with grpc. Consistent behaviours for both ResNet-50 and VGG16.</p>\n<p><strong>Describe the expected behavior</strong><br>\nThe same training procedure should yield exactly same result using only 1 worker with/without 1 PS. Not sure why adding SendRecvOps causes data corruption. See other info below.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<p>ResNet50 local with <a href=\"https://gist.github.com/byronyi/4f0821a4792b479bf884ff81171c4011\">output</a>:</p>\n<pre><code>$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=resnet50 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n</code></pre>\n<p>VGG16 local with <a href=\"https://gist.github.com/byronyi/0dc379b8feac762bc8d04cab2b240060\">output</a>:</p>\n<pre><code>$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=vgg16 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n</code></pre>\n<p>Distributed using the same PS command with <a href=\"https://gist.github.com/byronyi/ea508b150d1e55b0ac41502c48685b7c\">output</a>:</p>\n<pre><code>CUDA_VISIBLE_DEVICES= \\\npython benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --ps_hosts=localhost:5000 \\\n  --worker_hosts=localhost:5001 \\\n  --job_name=ps \\\n  --local_parameter_device=cpu \\\n  --task_index=0 \\\n  --server_protocol=grpc\n</code></pre>\n<p>ResNet50 distributed with <a href=\"https://gist.github.com/byronyi/de05ccb7b481932ce7be91f889f41aee\">output</a>:</p>\n<pre><code>$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --ps_hosts=localhost:5000 \\\n  --worker_hosts=localhost:5001 \\\n  --job_name=worker \\\n  --task_index=0 \\\n  --server_protocol=grpc \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=resnet50 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n</code></pre>\n<p>VGG16 distributed with <a href=\"https://gist.github.com/byronyi/1113a5cd8048f729a2c73df5bfd97017\">output</a>:</p>\n<pre><code>$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --ps_hosts=localhost:5000 \\\n  --worker_hosts=localhost:5001 \\\n  --job_name=worker \\\n  --task_index=0 \\\n  --server_protocol=grpc \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=vgg16 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n</code></pre>\n<p><strong>Other info / logs</strong><br>\nKernel reports <code>[DMA Write] PTE Write access is not set</code> for one of the GPU.<br>\nDetailed log could be found <a href=\"https://gist.github.com/dd1e75702116d941d95dfa1fe17b8e27\">here</a>.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): tensorflow/benchmarks:cnn_tf_v1.12_compatible\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04-hwe\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')\nPython version: 2.7.12\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 9.0/7.2.1.38-1+cuda9.0\nGPU model and memory: V100-PCIE-32GB, NVIDIA driver version 384.145, with intel_iommu=on\n\n$ cat /proc/cmdline\nBOOT_IMAGE=/boot/vmlinuz-4.15.0-38-generic root=/dev/mapper/vgroot-lvroot ro intel_iommu=on\n\n$ nvidia-smi topo -m\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nDescribe the current behavior\nFP-16 multi-GPU training with CPU as local parameter server is converging in single process mode, but diverging loss value (nan) adding another loopback PS process with grpc. Consistent behaviours for both ResNet-50 and VGG16.\nDescribe the expected behavior\nThe same training procedure should yield exactly same result using only 1 worker with/without 1 PS. Not sure why adding SendRecvOps causes data corruption. See other info below.\nCode to reproduce the issue\nResNet50 local with output:\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=resnet50 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n\nVGG16 local with output:\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=vgg16 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n\nDistributed using the same PS command with output:\nCUDA_VISIBLE_DEVICES= \\\npython benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --ps_hosts=localhost:5000 \\\n  --worker_hosts=localhost:5001 \\\n  --job_name=ps \\\n  --local_parameter_device=cpu \\\n  --task_index=0 \\\n  --server_protocol=grpc\n\nResNet50 distributed with output:\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --ps_hosts=localhost:5000 \\\n  --worker_hosts=localhost:5001 \\\n  --job_name=worker \\\n  --task_index=0 \\\n  --server_protocol=grpc \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=resnet50 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n\nVGG16 distributed with output:\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\n  --ps_hosts=localhost:5000 \\\n  --worker_hosts=localhost:5001 \\\n  --job_name=worker \\\n  --task_index=0 \\\n  --server_protocol=grpc \\\n  --variable_update=parameter_server \\\n  --local_parameter_device=cpu \\\n  --model=vgg16 \\\n  --num_gpus=4 \\\n  --use_fp16 \\\n  --batch_size=256\n\nOther info / logs\nKernel reports [DMA Write] PTE Write access is not set for one of the GPU.\nDetailed log could be found here.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [tensorflow/benchmarks:cnn_tf_v1.12_compatible](https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.12_compatible)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04-hwe\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0/7.2.1.38-1+cuda9.0\r\n- GPU model and memory: V100-PCIE-32GB, NVIDIA driver version 384.145, with `intel_iommu=on`\r\n\r\n```\r\n$ cat /proc/cmdline\r\nBOOT_IMAGE=/boot/vmlinuz-4.15.0-38-generic root=/dev/mapper/vgroot-lvroot ro intel_iommu=on\r\n```\r\n```\r\n$ nvidia-smi topo -m\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\r\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\r\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\r\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\r\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n**Describe the current behavior**\r\nFP-16 multi-GPU training with CPU as local parameter server is converging in single process mode, but diverging loss value (nan) adding another loopback PS process with grpc. Consistent behaviours for both ResNet-50 and VGG16. \r\n\r\n**Describe the expected behavior**\r\nThe same training procedure should yield exactly same result using only 1 worker with/without 1 PS. Not sure why adding SendRecvOps causes data corruption. See other info below.\r\n\r\n**Code to reproduce the issue**\r\n\r\nResNet50 local with [output](https://gist.github.com/byronyi/4f0821a4792b479bf884ff81171c4011):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=resnet50 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\nVGG16 local with [output](https://gist.github.com/byronyi/0dc379b8feac762bc8d04cab2b240060):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=vgg16 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\nDistributed using the same PS command with [output](https://gist.github.com/byronyi/ea508b150d1e55b0ac41502c48685b7c):\r\n```\r\nCUDA_VISIBLE_DEVICES= \\\r\npython benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --ps_hosts=localhost:5000 \\\r\n  --worker_hosts=localhost:5001 \\\r\n  --job_name=ps \\\r\n  --local_parameter_device=cpu \\\r\n  --task_index=0 \\\r\n  --server_protocol=grpc\r\n```\r\n\r\nResNet50 distributed with [output](https://gist.github.com/byronyi/de05ccb7b481932ce7be91f889f41aee):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --ps_hosts=localhost:5000 \\\r\n  --worker_hosts=localhost:5001 \\\r\n  --job_name=worker \\\r\n  --task_index=0 \\\r\n  --server_protocol=grpc \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=resnet50 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\nVGG16 distributed with [output](https://gist.github.com/byronyi/1113a5cd8048f729a2c73df5bfd97017):\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\\r\n  --ps_hosts=localhost:5000 \\\r\n  --worker_hosts=localhost:5001 \\\r\n  --job_name=worker \\\r\n  --task_index=0 \\\r\n  --server_protocol=grpc \\\r\n  --variable_update=parameter_server \\\r\n  --local_parameter_device=cpu \\\r\n  --model=vgg16 \\\r\n  --num_gpus=4 \\\r\n  --use_fp16 \\\r\n  --batch_size=256\r\n```\r\n\r\n**Other info / logs**\r\nKernel reports `[DMA Write] PTE Write access is not set` for one of the GPU.\r\nDetailed log could be found [here](https://gist.github.com/dd1e75702116d941d95dfa1fe17b8e27)."}