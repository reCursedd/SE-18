{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/438196430", "html_url": "https://github.com/tensorflow/tensorflow/issues/23606#issuecomment-438196430", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23606", "id": 438196430, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODE5NjQzMA==", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-13T09:27:33Z", "updated_at": "2018-11-13T09:27:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>After turning off IOMMU, I have tested P2P connectivity using <a href=\"https://github.com/NVIDIA/cuda-samples/blob/master/Samples/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu\">p2pBandwidthLatencyTest</a> in NVIDIA's CUDA samples. It does report dubious results, showing 1/10 bandwidth and 5000x higher latency in P2P writes when crossing a single PCIe switch.</p>\n<pre><code>$ nvidia-smi topo -m\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n$ ./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest\n[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\nDevice: 0, Tesla V100-PCIE-32GB, pciBusID: 1b, pciDeviceID: 0, pciDomainID:0\nDevice: 1, Tesla V100-PCIE-32GB, pciBusID: 1e, pciDeviceID: 0, pciDomainID:0\nDevice: 2, Tesla V100-PCIE-32GB, pciBusID: 3d, pciDeviceID: 0, pciDomainID:0\nDevice: 3, Tesla V100-PCIE-32GB, pciBusID: 41, pciDeviceID: 0, pciDomainID:0\nDevice=0 CAN Access Peer Device=1\nDevice=0 CAN Access Peer Device=2\nDevice=0 CAN Access Peer Device=3\nDevice=1 CAN Access Peer Device=0\nDevice=1 CAN Access Peer Device=2\nDevice=1 CAN Access Peer Device=3\nDevice=2 CAN Access Peer Device=0\nDevice=2 CAN Access Peer Device=1\nDevice=2 CAN Access Peer Device=3\nDevice=3 CAN Access Peer Device=0\nDevice=3 CAN Access Peer Device=1\nDevice=3 CAN Access Peer Device=2\n \n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\nSo you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n \nP2P Connectivity Matrix\n     D\\D     0     1     2     3\n     0             1     1     1     1\n     1             1     1     1     1\n     2             1     1     1     1\n     3             1     1     1     1\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 714.12   9.62  10.70  10.97\n     1   9.73 715.43  10.73  10.97\n     2  10.97  10.70 716.74   9.65\n     3  11.02  10.71   9.68 718.06\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 714.12   0.72  10.05   7.09\n     1   0.72 724.72   9.53   7.09\n     2   9.66   7.09 723.38   0.72\n     3   9.41   7.09   0.72 723.38\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 741.95  10.08  15.32  13.81\n     1  10.09 744.76  13.96  13.79\n     2  15.33  13.86 741.22  10.09\n     3  13.84  13.71  10.09 746.18\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 741.22   1.29  18.86  14.17\n     1   1.30 749.04  14.15  14.17\n     2  18.86  14.18 745.47   1.30\n     3  14.15  14.17   1.30 745.47\nP2P=Disabled Latency Matrix (us)\n   GPU     0      1      2      3\n     0   3.61  19.62  19.47  19.91\n     1  17.79   3.70  17.81  18.39\n     2  18.49  18.52   3.61  18.50\n     3  17.77  17.85  17.57   3.65\n \n   CPU     0      1      2      3\n     0   4.84  11.82  11.91  11.81\n     1  11.83   4.78  11.83  12.15\n     2  11.84  11.68   4.72  11.68\n     3  11.82  11.71  11.67   4.65\nP2P=Enabled Latency (P2P Writes) Matrix (us)\n   GPU     0      1      2      3\n     0   3.66 49262.93  10.84  10.89\n     1 49259.48   3.76  10.75  10.82\n     2  10.75  10.87   3.69 49259.81\n     3  10.72  10.80 49262.71   3.80\n \n   CPU     0      1      2      3\n     0   4.79   3.86   3.88   3.83\n     1   3.95   4.90   3.92   3.84\n     2   3.92   3.88   4.82   3.86\n     3   4.07   4.03   4.02   4.82\n \nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n</code></pre>\n<p>The hardware model involved is <a href=\"https://www.supermicro.com/products/system/4U/4029/SYS-4029GP-TRT2.cfm\" rel=\"nofollow\">SuperMicro 4029GP-TRT2</a>. We suspect it is a hardware/firmware related issue and we have reported this issue to the manufacturer. Will let you know if there is any updates from them.</p>", "body_text": "After turning off IOMMU, I have tested P2P connectivity using p2pBandwidthLatencyTest in NVIDIA's CUDA samples. It does report dubious results, showing 1/10 bandwidth and 5000x higher latency in P2P writes when crossing a single PCIe switch.\n$ nvidia-smi topo -m\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n$ ./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest\n[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\nDevice: 0, Tesla V100-PCIE-32GB, pciBusID: 1b, pciDeviceID: 0, pciDomainID:0\nDevice: 1, Tesla V100-PCIE-32GB, pciBusID: 1e, pciDeviceID: 0, pciDomainID:0\nDevice: 2, Tesla V100-PCIE-32GB, pciBusID: 3d, pciDeviceID: 0, pciDomainID:0\nDevice: 3, Tesla V100-PCIE-32GB, pciBusID: 41, pciDeviceID: 0, pciDomainID:0\nDevice=0 CAN Access Peer Device=1\nDevice=0 CAN Access Peer Device=2\nDevice=0 CAN Access Peer Device=3\nDevice=1 CAN Access Peer Device=0\nDevice=1 CAN Access Peer Device=2\nDevice=1 CAN Access Peer Device=3\nDevice=2 CAN Access Peer Device=0\nDevice=2 CAN Access Peer Device=1\nDevice=2 CAN Access Peer Device=3\nDevice=3 CAN Access Peer Device=0\nDevice=3 CAN Access Peer Device=1\nDevice=3 CAN Access Peer Device=2\n \n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\nSo you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n \nP2P Connectivity Matrix\n     D\\D     0     1     2     3\n     0             1     1     1     1\n     1             1     1     1     1\n     2             1     1     1     1\n     3             1     1     1     1\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 714.12   9.62  10.70  10.97\n     1   9.73 715.43  10.73  10.97\n     2  10.97  10.70 716.74   9.65\n     3  11.02  10.71   9.68 718.06\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 714.12   0.72  10.05   7.09\n     1   0.72 724.72   9.53   7.09\n     2   9.66   7.09 723.38   0.72\n     3   9.41   7.09   0.72 723.38\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 741.95  10.08  15.32  13.81\n     1  10.09 744.76  13.96  13.79\n     2  15.33  13.86 741.22  10.09\n     3  13.84  13.71  10.09 746.18\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n   D\\D     0      1      2      3\n     0 741.22   1.29  18.86  14.17\n     1   1.30 749.04  14.15  14.17\n     2  18.86  14.18 745.47   1.30\n     3  14.15  14.17   1.30 745.47\nP2P=Disabled Latency Matrix (us)\n   GPU     0      1      2      3\n     0   3.61  19.62  19.47  19.91\n     1  17.79   3.70  17.81  18.39\n     2  18.49  18.52   3.61  18.50\n     3  17.77  17.85  17.57   3.65\n \n   CPU     0      1      2      3\n     0   4.84  11.82  11.91  11.81\n     1  11.83   4.78  11.83  12.15\n     2  11.84  11.68   4.72  11.68\n     3  11.82  11.71  11.67   4.65\nP2P=Enabled Latency (P2P Writes) Matrix (us)\n   GPU     0      1      2      3\n     0   3.66 49262.93  10.84  10.89\n     1 49259.48   3.76  10.75  10.82\n     2  10.75  10.87   3.69 49259.81\n     3  10.72  10.80 49262.71   3.80\n \n   CPU     0      1      2      3\n     0   4.79   3.86   3.88   3.83\n     1   3.95   4.90   3.92   3.84\n     2   3.92   3.88   4.82   3.86\n     3   4.07   4.03   4.02   4.82\n \nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\nThe hardware model involved is SuperMicro 4029GP-TRT2. We suspect it is a hardware/firmware related issue and we have reported this issue to the manufacturer. Will let you know if there is any updates from them.", "body": "After turning off IOMMU, I have tested P2P connectivity using [p2pBandwidthLatencyTest](https://github.com/NVIDIA/cuda-samples/blob/master/Samples/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu) in NVIDIA's CUDA samples. It does report dubious results, showing 1/10 bandwidth and 5000x higher latency in P2P writes when crossing a single PCIe switch. \r\n\r\n```\r\n$ nvidia-smi topo -m\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\r\nGPU0\t X \tPIX\tNODE\tNODE\t0-9,20-29\r\nGPU1\tPIX\t X \tNODE\tNODE\t0-9,20-29\r\nGPU2\tNODE\tNODE\t X \tPIX\t0-9,20-29\r\nGPU3\tNODE\tNODE\tPIX\t X \t0-9,20-29\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n$ ./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest\r\n[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\r\nDevice: 0, Tesla V100-PCIE-32GB, pciBusID: 1b, pciDeviceID: 0, pciDomainID:0\r\nDevice: 1, Tesla V100-PCIE-32GB, pciBusID: 1e, pciDeviceID: 0, pciDomainID:0\r\nDevice: 2, Tesla V100-PCIE-32GB, pciBusID: 3d, pciDeviceID: 0, pciDomainID:0\r\nDevice: 3, Tesla V100-PCIE-32GB, pciBusID: 41, pciDeviceID: 0, pciDomainID:0\r\nDevice=0 CAN Access Peer Device=1\r\nDevice=0 CAN Access Peer Device=2\r\nDevice=0 CAN Access Peer Device=3\r\nDevice=1 CAN Access Peer Device=0\r\nDevice=1 CAN Access Peer Device=2\r\nDevice=1 CAN Access Peer Device=3\r\nDevice=2 CAN Access Peer Device=0\r\nDevice=2 CAN Access Peer Device=1\r\nDevice=2 CAN Access Peer Device=3\r\nDevice=3 CAN Access Peer Device=0\r\nDevice=3 CAN Access Peer Device=1\r\nDevice=3 CAN Access Peer Device=2\r\n \r\n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\r\nSo you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\r\n \r\nP2P Connectivity Matrix\r\n     D\\D     0     1     2     3\r\n     0             1     1     1     1\r\n     1             1     1     1     1\r\n     2             1     1     1     1\r\n     3             1     1     1     1\r\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 714.12   9.62  10.70  10.97\r\n     1   9.73 715.43  10.73  10.97\r\n     2  10.97  10.70 716.74   9.65\r\n     3  11.02  10.71   9.68 718.06\r\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 714.12   0.72  10.05   7.09\r\n     1   0.72 724.72   9.53   7.09\r\n     2   9.66   7.09 723.38   0.72\r\n     3   9.41   7.09   0.72 723.38\r\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 741.95  10.08  15.32  13.81\r\n     1  10.09 744.76  13.96  13.79\r\n     2  15.33  13.86 741.22  10.09\r\n     3  13.84  13.71  10.09 746.18\r\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3\r\n     0 741.22   1.29  18.86  14.17\r\n     1   1.30 749.04  14.15  14.17\r\n     2  18.86  14.18 745.47   1.30\r\n     3  14.15  14.17   1.30 745.47\r\nP2P=Disabled Latency Matrix (us)\r\n   GPU     0      1      2      3\r\n     0   3.61  19.62  19.47  19.91\r\n     1  17.79   3.70  17.81  18.39\r\n     2  18.49  18.52   3.61  18.50\r\n     3  17.77  17.85  17.57   3.65\r\n \r\n   CPU     0      1      2      3\r\n     0   4.84  11.82  11.91  11.81\r\n     1  11.83   4.78  11.83  12.15\r\n     2  11.84  11.68   4.72  11.68\r\n     3  11.82  11.71  11.67   4.65\r\nP2P=Enabled Latency (P2P Writes) Matrix (us)\r\n   GPU     0      1      2      3\r\n     0   3.66 49262.93  10.84  10.89\r\n     1 49259.48   3.76  10.75  10.82\r\n     2  10.75  10.87   3.69 49259.81\r\n     3  10.72  10.80 49262.71   3.80\r\n \r\n   CPU     0      1      2      3\r\n     0   4.79   3.86   3.88   3.83\r\n     1   3.95   4.90   3.92   3.84\r\n     2   3.92   3.88   4.82   3.86\r\n     3   4.07   4.03   4.02   4.82\r\n \r\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\r\n```\r\n\r\nThe hardware model involved is [SuperMicro 4029GP-TRT2](https://www.supermicro.com/products/system/4U/4029/SYS-4029GP-TRT2.cfm). We suspect it is a hardware/firmware related issue and we have reported this issue to the manufacturer. Will let you know if there is any updates from them."}