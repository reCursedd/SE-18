{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387812598", "html_url": "https://github.com/tensorflow/tensorflow/issues/19069#issuecomment-387812598", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19069", "id": 387812598, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzgxMjU5OA==", "user": {"login": "marhlder", "id": 2690031, "node_id": "MDQ6VXNlcjI2OTAwMzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/2690031?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marhlder", "html_url": "https://github.com/marhlder", "followers_url": "https://api.github.com/users/marhlder/followers", "following_url": "https://api.github.com/users/marhlder/following{/other_user}", "gists_url": "https://api.github.com/users/marhlder/gists{/gist_id}", "starred_url": "https://api.github.com/users/marhlder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marhlder/subscriptions", "organizations_url": "https://api.github.com/users/marhlder/orgs", "repos_url": "https://api.github.com/users/marhlder/repos", "events_url": "https://api.github.com/users/marhlder/events{/privacy}", "received_events_url": "https://api.github.com/users/marhlder/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T17:22:08Z", "updated_at": "2018-05-14T10:23:31Z", "author_association": "NONE", "body_html": "<p>Ok, I managed to cook up a hack/workaround, along with the previously mentioned changes to MirroredStrategy, for the IndexedSlices issue and now my code is running/training. It will use up a lot of memory for IndexedSlices with large dense shapes and it uses non-public API (I feel dirty), but it works and it is kinda similar to what happens in /tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py.</p>\n<p>The hack consists of the following changes to tensorflow/tensorflow/contrib/distribute/python/cross_tower_ops.py:</p>\n<pre><code>from tensorflow.python.ops import gradients_impl\n\ndef choose_the_best(devices, session_config=None):\n  \"\"\"Find the best subclass of CrossTowerOps given a tensorflow session.\n\n  Args:\n    devices: a list of devices passed for distribute strategy.\n    session_config: a tensorflow session config or None. If None, it will make\n      deciesion based on all local devices.\n\n  Returns:\n    a subclass of CrossTowerOps.\n  \"\"\"\n\n  def default_accumulation_fn(inputs):\n\n    first_element = inputs[0]\n    if isinstance(first_element, ops.IndexedSlices):\n      return math_ops.add_n(list(map(gradients_impl._IndexedSlicesToTensor, inputs)))\n    else:\n      return math_ops.add_n(inputs)\n\n  requested_devices = set([device_util.canonicalize(d) for d in devices])\n  machine_devices = device_lib.list_local_devices(session_config=session_config)\n  using_devices = []\n  for d in machine_devices:\n    if device_util.canonicalize(d.name) in requested_devices:\n      using_devices.append(d)\n    else:\n      logging.info(\n        \"Device is available but not used by distribute strategy: %s\", d.name)\n\n  if len(using_devices) != len(requested_devices):\n    logging.warning(\"Not all devices in distribute strategy are visible by \"\n                    \"TensorFlow sessions.\")\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\n\n  if any([d.device_type.lower() != \"gpu\" for d in using_devices]):\n    logging.warning(\"Not all devices in DistributionStrategy are visible to \"\n                    \"TensorFlow session.\")\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\n\n  device_links = [[] for _ in range(len(using_devices))]\n  for i, device in enumerate(using_devices):\n    for link in device.locality.links.link:\n      device_links[i].append(link.device_id)\n\n  return _choose_all_reduce_algorithm(device_links)\n\n</code></pre>\n<p>I hope you will find it useful!</p>\n<p>EDIT: I think this might only work because I am testing on a machine with only one GPU. I will need to make changes to AllReduceCrossTowerOps as well I guess. Not sure if I would have the same problems with IndexedSlices if I ran my code on an actual multi GPU machine.</p>", "body_text": "Ok, I managed to cook up a hack/workaround, along with the previously mentioned changes to MirroredStrategy, for the IndexedSlices issue and now my code is running/training. It will use up a lot of memory for IndexedSlices with large dense shapes and it uses non-public API (I feel dirty), but it works and it is kinda similar to what happens in /tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py.\nThe hack consists of the following changes to tensorflow/tensorflow/contrib/distribute/python/cross_tower_ops.py:\nfrom tensorflow.python.ops import gradients_impl\n\ndef choose_the_best(devices, session_config=None):\n  \"\"\"Find the best subclass of CrossTowerOps given a tensorflow session.\n\n  Args:\n    devices: a list of devices passed for distribute strategy.\n    session_config: a tensorflow session config or None. If None, it will make\n      deciesion based on all local devices.\n\n  Returns:\n    a subclass of CrossTowerOps.\n  \"\"\"\n\n  def default_accumulation_fn(inputs):\n\n    first_element = inputs[0]\n    if isinstance(first_element, ops.IndexedSlices):\n      return math_ops.add_n(list(map(gradients_impl._IndexedSlicesToTensor, inputs)))\n    else:\n      return math_ops.add_n(inputs)\n\n  requested_devices = set([device_util.canonicalize(d) for d in devices])\n  machine_devices = device_lib.list_local_devices(session_config=session_config)\n  using_devices = []\n  for d in machine_devices:\n    if device_util.canonicalize(d.name) in requested_devices:\n      using_devices.append(d)\n    else:\n      logging.info(\n        \"Device is available but not used by distribute strategy: %s\", d.name)\n\n  if len(using_devices) != len(requested_devices):\n    logging.warning(\"Not all devices in distribute strategy are visible by \"\n                    \"TensorFlow sessions.\")\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\n\n  if any([d.device_type.lower() != \"gpu\" for d in using_devices]):\n    logging.warning(\"Not all devices in DistributionStrategy are visible to \"\n                    \"TensorFlow session.\")\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\n\n  device_links = [[] for _ in range(len(using_devices))]\n  for i, device in enumerate(using_devices):\n    for link in device.locality.links.link:\n      device_links[i].append(link.device_id)\n\n  return _choose_all_reduce_algorithm(device_links)\n\n\nI hope you will find it useful!\nEDIT: I think this might only work because I am testing on a machine with only one GPU. I will need to make changes to AllReduceCrossTowerOps as well I guess. Not sure if I would have the same problems with IndexedSlices if I ran my code on an actual multi GPU machine.", "body": "Ok, I managed to cook up a hack/workaround, along with the previously mentioned changes to MirroredStrategy, for the IndexedSlices issue and now my code is running/training. It will use up a lot of memory for IndexedSlices with large dense shapes and it uses non-public API (I feel dirty), but it works and it is kinda similar to what happens in /tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py.\r\n\r\nThe hack consists of the following changes to tensorflow/tensorflow/contrib/distribute/python/cross_tower_ops.py:\r\n\r\n```\r\nfrom tensorflow.python.ops import gradients_impl\r\n\r\ndef choose_the_best(devices, session_config=None):\r\n  \"\"\"Find the best subclass of CrossTowerOps given a tensorflow session.\r\n\r\n  Args:\r\n    devices: a list of devices passed for distribute strategy.\r\n    session_config: a tensorflow session config or None. If None, it will make\r\n      deciesion based on all local devices.\r\n\r\n  Returns:\r\n    a subclass of CrossTowerOps.\r\n  \"\"\"\r\n\r\n  def default_accumulation_fn(inputs):\r\n\r\n    first_element = inputs[0]\r\n    if isinstance(first_element, ops.IndexedSlices):\r\n      return math_ops.add_n(list(map(gradients_impl._IndexedSlicesToTensor, inputs)))\r\n    else:\r\n      return math_ops.add_n(inputs)\r\n\r\n  requested_devices = set([device_util.canonicalize(d) for d in devices])\r\n  machine_devices = device_lib.list_local_devices(session_config=session_config)\r\n  using_devices = []\r\n  for d in machine_devices:\r\n    if device_util.canonicalize(d.name) in requested_devices:\r\n      using_devices.append(d)\r\n    else:\r\n      logging.info(\r\n        \"Device is available but not used by distribute strategy: %s\", d.name)\r\n\r\n  if len(using_devices) != len(requested_devices):\r\n    logging.warning(\"Not all devices in distribute strategy are visible by \"\r\n                    \"TensorFlow sessions.\")\r\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\r\n\r\n  if any([d.device_type.lower() != \"gpu\" for d in using_devices]):\r\n    logging.warning(\"Not all devices in DistributionStrategy are visible to \"\r\n                    \"TensorFlow session.\")\r\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\r\n\r\n  device_links = [[] for _ in range(len(using_devices))]\r\n  for i, device in enumerate(using_devices):\r\n    for link in device.locality.links.link:\r\n      device_links[i].append(link.device_id)\r\n\r\n  return _choose_all_reduce_algorithm(device_links)\r\n\r\n```\r\n\r\nI hope you will find it useful!\r\n\r\nEDIT: I think this might only work because I am testing on a machine with only one GPU. I will need to make changes to AllReduceCrossTowerOps as well I guess. Not sure if I would have the same problems with IndexedSlices if I ran my code on an actual multi GPU machine."}