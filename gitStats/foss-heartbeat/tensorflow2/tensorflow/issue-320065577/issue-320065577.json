{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19069", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19069/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19069/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19069/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19069", "id": 320065577, "node_id": "MDU6SXNzdWUzMjAwNjU1Nzc=", "number": 19069, "title": "Variable initialization under estimator + dynamic_rnn + MirroredStrategy (DistributionStrategy)", "user": {"login": "marhlder", "id": 2690031, "node_id": "MDQ6VXNlcjI2OTAwMzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/2690031?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marhlder", "html_url": "https://github.com/marhlder", "followers_url": "https://api.github.com/users/marhlder/followers", "following_url": "https://api.github.com/users/marhlder/following{/other_user}", "gists_url": "https://api.github.com/users/marhlder/gists{/gist_id}", "starred_url": "https://api.github.com/users/marhlder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marhlder/subscriptions", "organizations_url": "https://api.github.com/users/marhlder/orgs", "repos_url": "https://api.github.com/users/marhlder/repos", "events_url": "https://api.github.com/users/marhlder/events{/privacy}", "received_events_url": "https://api.github.com/users/marhlder/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, {"login": "josh11b", "id": 15258583, "node_id": "MDQ6VXNlcjE1MjU4NTgz", "avatar_url": "https://avatars0.githubusercontent.com/u/15258583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/josh11b", "html_url": "https://github.com/josh11b", "followers_url": "https://api.github.com/users/josh11b/followers", "following_url": "https://api.github.com/users/josh11b/following{/other_user}", "gists_url": "https://api.github.com/users/josh11b/gists{/gist_id}", "starred_url": "https://api.github.com/users/josh11b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/josh11b/subscriptions", "organizations_url": "https://api.github.com/users/josh11b/orgs", "repos_url": "https://api.github.com/users/josh11b/repos", "events_url": "https://api.github.com/users/josh11b/events{/privacy}", "received_events_url": "https://api.github.com/users/josh11b/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-05-03T20:19:54Z", "updated_at": "2018-09-21T12:12:23Z", "closed_at": "2018-05-25T21:18:09Z", "author_association": "NONE", "body_html": "<p><strong>System information</strong><br>\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes<br>\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 and Windows 10<br>\nTensorFlow installed from (source or binary): Binary (pip)<br>\nTensorFlow version (use command below): 1.8.0<br>\nPython version: 3.6<br>\nBazel version (if compiling from source): N/A<br>\nGCC/Compiler version (if compiling from source): N/A<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: GTX 1080 8 GiB (Windows) or two 8 GiB Tesla M60 (Ubuntu 16.04)<br>\nExact command to reproduce: See python code (below)</p>\n<p><strong>Describe the problem</strong><br>\nI am trying to transition my TensorFlow sequence2sequence codebase (RNN) into using the Estimator API, using the new tf.contrib.distribute.DistributionStrategy API. My provided runnable code snippet (below) works for a tf.contrib.distribute.OneDeviceStrategy, but not for a tf.contrib.distribute.MirroredStrategy.</p>\n<p>The problem lies with initialization of the variables associated with RNN layers and the way these variables are handled/initialized using dynamic_rnn.<br>\nThe first tower seems to work fine, as it correctly uses the default callable initializer. The problem seems to be that the following towers are initialized directly with the tensors resulting from the initializers called on the first tower, which the dynamic_rnn (tf.nn.bidirectional_dynamic_rnn) API does not like.</p>\n<p>I have found three possible issues:</p>\n<ul>\n<li>(Possibly major?) Unable to initialize RNN parameters/variables, beyond the first tower, using tf.nn.bidirectional_dynamic_rnn and tf.contrib.rnn.LSTMCell together with a tf.contrib.distribute.MirroredStrategy.</li>\n<li>(Minor) Optimizer API, such as tf.train.AdamOptimizer, seems to return a tensor containing the updated global_step tensor instead of a no_op, as it normally does, when running under a distribution strategy. (Seems this is an easy fix, e.g. by using tf.group (see below))</li>\n<li>(Minor) Error message: Would it not be more meaningful if the below error message \"... use a lambda as the initializer\" said something along the lines of: \"... use a callable, e.g. an Initializer (tf.keras.initializers.Initializer) or a lambda, as the initializer\" ?</li>\n</ul>\n<p>Is there something i might have missed?</p>\n<p><strong>Source code / logs</strong></p>\n<pre><code>import tensorflow as tf\n\nclass RNNModel(object):\n\n  def __init__(self, hparams):\n    return\n\n  def __call__(self, features, labels, mode, params):\n\n    inputs = features[0]\n\n    print(inputs)\n\n    cell_fw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\n      300)\n    cell_bw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\n      300)\n\n    (outputs, output_states) = tf.nn.bidirectional_dynamic_rnn(\n      cell_fw,\n      cell_bw,\n      inputs,\n      sequence_length=tf.convert_to_tensor([2, 2]),\n      initial_state_fw=None,\n      initial_state_bw=None,\n      dtype=tf.float32,\n      parallel_iterations=None,\n      swap_memory=False,\n      time_major=False,\n      scope=None\n    )\n\n    model_output = tf.concat(outputs, axis=-1)\n    mock_loss = 10 - tf.reduce_sum(model_output)\n\n    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mock_loss,\n                                                                         global_step=tf.train.get_or_create_global_step())\n    train_op = tf.group(train_op)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      return tf.estimator.EstimatorSpec(mode, loss=mock_loss, train_op=train_op)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n      return tf.estimator.EstimatorSpec(mode, loss=mock_loss, eval_metric_ops=None)\n    elif mode == tf.estimator.ModeKeys.PREDICT:\n      predictions = {\n        'mock': 1,\n      }\n      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n\ndef train(hparams):\n  model = RNNModel(hparams)\n\n  distribution_strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\n  #distribution_strategy = tf.contrib.distribute.OneDeviceStrategy(tf.DeviceSpec(device_type=\"GPU\", device_index=0))\n\n  checkpointing_config = tf.estimator.RunConfig(\n    save_checkpoints_secs=20 * 60,  # Save checkpoints every 20 minutes.\n    keep_checkpoint_max=10,  # Retain the 10 most recent checkpoints.\n    train_distribute=distribution_strategy\n  )\n\n  estimator = tf.estimator.Estimator(\n    model_fn=model,\n    model_dir=\"out/\",\n    params=hparams,\n    config=checkpointing_config,\n  )\n\n  def create_mock_dataset():\n    mock_data = tf.convert_to_tensor([[[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]],\n                                      [[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]]])\n\n    mock_data_set = tf.data.Dataset.from_tensors(mock_data)\n    mock_data_set = mock_data_set.map(lambda mock_data: (((mock_data,)), ()) )\n\n    return mock_data_set\n\n  num_train_steps = 100\n  estimator.train(create_mock_dataset, hooks=None,\n                  steps=num_train_steps)\n\nif __name__ == '__main__':\n  train({\"mock\": \"mock\"})\n</code></pre>\n<p>The code produces the following  output:</p>\n<pre><code>Traceback (most recent call last):\nFile \"C:\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"C:\\Python36\\lib\\runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 105, in \ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\n_sys.exit(main(argv))\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 96, in main\nrun_main(FLAGS, default_hparams, train_fn, inference_fn, hparams_creator)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 88, in run_main\ntrain_fn(hparams)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\train_estimator.py\", line 470, in s2s_train\nestimator.train(input_fn=train_input_fn, hooks=train_hooks,steps=num_train_steps)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 363, in train\nloss = self._train_model(input_fn, hooks, saving_listeners)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 841, in _train_model\nreturn self._train_model_distributed(input_fn, hooks, saving_listeners)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 884, in _train_model_distributed\nself.config)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 756, in call_for_each_tower\nreturn self._call_for_each_tower(fn, *args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 254, in _call_for_each_tower\ncoord.join(threads)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 389, in join\nsix.reraise(*self._exc_info_to_raise)\nFile \"C:\\Python36\\lib\\site-packages\\six.py\", line 693, in reraise\nraise value\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\nyield\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 465, in run\nself.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 831, in _call_model_fn\nmodel_fn_results = self._model_fn(features=features, **kwargs)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\s2s_base_model.py\", line 157, in call\nres = self.build_graph(hparams, features, labels)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 83, in build_graph\nencoded_outputs, encoded_state = self._build_encoder(hparams, features)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 202, in _build_encoder\nreturn self._build_flat_encoder(hparams, features)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 250, in _build_flat_encoder\nnum_bi_residual_layers=num_bi_residual_layers))\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 768, in _build_bidirectional_rnn\ntime_major=self.time_major)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 412, in bidirectional_dynamic_rnn\ntime_major=time_major, scope=fw_scope)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 627, in dynamic_rnn\ndtype=dtype)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 824, in _dynamic_rnn_loop\nswap_memory=swap_memory)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3224, in while_loop\nresult = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2956, in BuildLoop\npred, body, original_loop_vars, loop_vars, shape_invariants)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2893, in _BuildLoop\nbody_result = body(*packed_vars_for_body)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3194, in \nbody = lambda i, lv: (i + 1, orig_body(*lv))\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 793, in _time_step\nskip_conditionals=True)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 248, in _rnn_step\nnew_output, new_state = call_cell()\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 781, in \ncall_cell = lambda: cell(input_t, state)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 232, in call\nreturn super(RNNCell, self).call(inputs, state)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 717, in call\noutputs = self.call(inputs, *args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1292, in call\ncur_inp, new_state = cell(cur_inp, cur_state)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1099, in call\noutput, new_state = self._cell(inputs, state, scope=scope)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 339, in call\n*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 699, in call\nself.build(input_shapes)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 767, in build\npartitioner=maybe_partitioner)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 546, in add_variable\npartitioner=partitioner)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable.py\", line 436, in _add_variable_with_custom_getter\n**kwargs_for_getter)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1317, in get_variable\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1079, in get_variable\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 417, in get_variable\nreturn custom_getter(**custom_getter_kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1720, in wrapped_custom_getter\n*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 235, in _rnn_get_variable\nvariable = getter(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 575, in disable_partitioned_variables\nreturn getter(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\nuse_resource=use_resource, constraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 786, in _get_single_variable\nuse_resource=use_resource)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2220, in variable\nuse_resource=use_resource)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\shared_variable_creator.py\", line 69, in create_new_variable\nv = next_creator(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 568, in creator_with_resource_vars\nreturn self._create_variable(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 118, in _create_variable\nv = next_creator(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2210, in \nprevious_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2182, in default_variable_creator\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 282, in init\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 421, in _init_from_args\n\"initializer.\" % name)\nValueError: Initializer for variable encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/replica_1/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.\n</code></pre>", "body_text": "System information\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 and Windows 10\nTensorFlow installed from (source or binary): Binary (pip)\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: GTX 1080 8 GiB (Windows) or two 8 GiB Tesla M60 (Ubuntu 16.04)\nExact command to reproduce: See python code (below)\nDescribe the problem\nI am trying to transition my TensorFlow sequence2sequence codebase (RNN) into using the Estimator API, using the new tf.contrib.distribute.DistributionStrategy API. My provided runnable code snippet (below) works for a tf.contrib.distribute.OneDeviceStrategy, but not for a tf.contrib.distribute.MirroredStrategy.\nThe problem lies with initialization of the variables associated with RNN layers and the way these variables are handled/initialized using dynamic_rnn.\nThe first tower seems to work fine, as it correctly uses the default callable initializer. The problem seems to be that the following towers are initialized directly with the tensors resulting from the initializers called on the first tower, which the dynamic_rnn (tf.nn.bidirectional_dynamic_rnn) API does not like.\nI have found three possible issues:\n\n(Possibly major?) Unable to initialize RNN parameters/variables, beyond the first tower, using tf.nn.bidirectional_dynamic_rnn and tf.contrib.rnn.LSTMCell together with a tf.contrib.distribute.MirroredStrategy.\n(Minor) Optimizer API, such as tf.train.AdamOptimizer, seems to return a tensor containing the updated global_step tensor instead of a no_op, as it normally does, when running under a distribution strategy. (Seems this is an easy fix, e.g. by using tf.group (see below))\n(Minor) Error message: Would it not be more meaningful if the below error message \"... use a lambda as the initializer\" said something along the lines of: \"... use a callable, e.g. an Initializer (tf.keras.initializers.Initializer) or a lambda, as the initializer\" ?\n\nIs there something i might have missed?\nSource code / logs\nimport tensorflow as tf\n\nclass RNNModel(object):\n\n  def __init__(self, hparams):\n    return\n\n  def __call__(self, features, labels, mode, params):\n\n    inputs = features[0]\n\n    print(inputs)\n\n    cell_fw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\n      300)\n    cell_bw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\n      300)\n\n    (outputs, output_states) = tf.nn.bidirectional_dynamic_rnn(\n      cell_fw,\n      cell_bw,\n      inputs,\n      sequence_length=tf.convert_to_tensor([2, 2]),\n      initial_state_fw=None,\n      initial_state_bw=None,\n      dtype=tf.float32,\n      parallel_iterations=None,\n      swap_memory=False,\n      time_major=False,\n      scope=None\n    )\n\n    model_output = tf.concat(outputs, axis=-1)\n    mock_loss = 10 - tf.reduce_sum(model_output)\n\n    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mock_loss,\n                                                                         global_step=tf.train.get_or_create_global_step())\n    train_op = tf.group(train_op)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      return tf.estimator.EstimatorSpec(mode, loss=mock_loss, train_op=train_op)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n      return tf.estimator.EstimatorSpec(mode, loss=mock_loss, eval_metric_ops=None)\n    elif mode == tf.estimator.ModeKeys.PREDICT:\n      predictions = {\n        'mock': 1,\n      }\n      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n\ndef train(hparams):\n  model = RNNModel(hparams)\n\n  distribution_strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\n  #distribution_strategy = tf.contrib.distribute.OneDeviceStrategy(tf.DeviceSpec(device_type=\"GPU\", device_index=0))\n\n  checkpointing_config = tf.estimator.RunConfig(\n    save_checkpoints_secs=20 * 60,  # Save checkpoints every 20 minutes.\n    keep_checkpoint_max=10,  # Retain the 10 most recent checkpoints.\n    train_distribute=distribution_strategy\n  )\n\n  estimator = tf.estimator.Estimator(\n    model_fn=model,\n    model_dir=\"out/\",\n    params=hparams,\n    config=checkpointing_config,\n  )\n\n  def create_mock_dataset():\n    mock_data = tf.convert_to_tensor([[[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]],\n                                      [[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]]])\n\n    mock_data_set = tf.data.Dataset.from_tensors(mock_data)\n    mock_data_set = mock_data_set.map(lambda mock_data: (((mock_data,)), ()) )\n\n    return mock_data_set\n\n  num_train_steps = 100\n  estimator.train(create_mock_dataset, hooks=None,\n                  steps=num_train_steps)\n\nif __name__ == '__main__':\n  train({\"mock\": \"mock\"})\n\nThe code produces the following  output:\nTraceback (most recent call last):\nFile \"C:\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"C:\\Python36\\lib\\runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 105, in \ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\n_sys.exit(main(argv))\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 96, in main\nrun_main(FLAGS, default_hparams, train_fn, inference_fn, hparams_creator)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 88, in run_main\ntrain_fn(hparams)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\train_estimator.py\", line 470, in s2s_train\nestimator.train(input_fn=train_input_fn, hooks=train_hooks,steps=num_train_steps)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 363, in train\nloss = self._train_model(input_fn, hooks, saving_listeners)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 841, in _train_model\nreturn self._train_model_distributed(input_fn, hooks, saving_listeners)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 884, in _train_model_distributed\nself.config)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 756, in call_for_each_tower\nreturn self._call_for_each_tower(fn, *args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 254, in _call_for_each_tower\ncoord.join(threads)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 389, in join\nsix.reraise(*self._exc_info_to_raise)\nFile \"C:\\Python36\\lib\\site-packages\\six.py\", line 693, in reraise\nraise value\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\nyield\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 465, in run\nself.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 831, in _call_model_fn\nmodel_fn_results = self._model_fn(features=features, **kwargs)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\s2s_base_model.py\", line 157, in call\nres = self.build_graph(hparams, features, labels)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 83, in build_graph\nencoded_outputs, encoded_state = self._build_encoder(hparams, features)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 202, in _build_encoder\nreturn self._build_flat_encoder(hparams, features)\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 250, in _build_flat_encoder\nnum_bi_residual_layers=num_bi_residual_layers))\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 768, in _build_bidirectional_rnn\ntime_major=self.time_major)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 412, in bidirectional_dynamic_rnn\ntime_major=time_major, scope=fw_scope)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 627, in dynamic_rnn\ndtype=dtype)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 824, in _dynamic_rnn_loop\nswap_memory=swap_memory)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3224, in while_loop\nresult = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2956, in BuildLoop\npred, body, original_loop_vars, loop_vars, shape_invariants)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2893, in _BuildLoop\nbody_result = body(*packed_vars_for_body)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3194, in \nbody = lambda i, lv: (i + 1, orig_body(*lv))\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 793, in _time_step\nskip_conditionals=True)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 248, in _rnn_step\nnew_output, new_state = call_cell()\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 781, in \ncall_cell = lambda: cell(input_t, state)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 232, in call\nreturn super(RNNCell, self).call(inputs, state)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 717, in call\noutputs = self.call(inputs, *args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1292, in call\ncur_inp, new_state = cell(cur_inp, cur_state)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1099, in call\noutput, new_state = self._cell(inputs, state, scope=scope)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 339, in call\n*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 699, in call\nself.build(input_shapes)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 767, in build\npartitioner=maybe_partitioner)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 546, in add_variable\npartitioner=partitioner)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable.py\", line 436, in _add_variable_with_custom_getter\n**kwargs_for_getter)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1317, in get_variable\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1079, in get_variable\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 417, in get_variable\nreturn custom_getter(**custom_getter_kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1720, in wrapped_custom_getter\n*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 235, in _rnn_get_variable\nvariable = getter(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 575, in disable_partitioned_variables\nreturn getter(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\nuse_resource=use_resource, constraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 786, in _get_single_variable\nuse_resource=use_resource)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2220, in variable\nuse_resource=use_resource)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\shared_variable_creator.py\", line 69, in create_new_variable\nv = next_creator(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 568, in creator_with_resource_vars\nreturn self._create_variable(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 118, in _create_variable\nv = next_creator(*args, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2210, in \nprevious_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2182, in default_variable_creator\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 282, in init\nconstraint=constraint)\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 421, in _init_from_args\n\"initializer.\" % name)\nValueError: Initializer for variable encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/replica_1/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 and Windows 10\r\nTensorFlow installed from (source or binary): Binary (pip)\r\nTensorFlow version (use command below): 1.8.0\r\nPython version: 3.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: GTX 1080 8 GiB (Windows) or two 8 GiB Tesla M60 (Ubuntu 16.04)\r\nExact command to reproduce: See python code (below)\r\n\r\n**Describe the problem**\r\nI am trying to transition my TensorFlow sequence2sequence codebase (RNN) into using the Estimator API, using the new tf.contrib.distribute.DistributionStrategy API. My provided runnable code snippet (below) works for a tf.contrib.distribute.OneDeviceStrategy, but not for a tf.contrib.distribute.MirroredStrategy.\r\n\r\nThe problem lies with initialization of the variables associated with RNN layers and the way these variables are handled/initialized using dynamic_rnn. \r\nThe first tower seems to work fine, as it correctly uses the default callable initializer. The problem seems to be that the following towers are initialized directly with the tensors resulting from the initializers called on the first tower, which the dynamic_rnn (tf.nn.bidirectional_dynamic_rnn) API does not like. \r\n\r\nI have found three possible issues:\r\n\r\n- (Possibly major?) Unable to initialize RNN parameters/variables, beyond the first tower, using tf.nn.bidirectional_dynamic_rnn and tf.contrib.rnn.LSTMCell together with a tf.contrib.distribute.MirroredStrategy.\r\n-  (Minor) Optimizer API, such as tf.train.AdamOptimizer, seems to return a tensor containing the updated global_step tensor instead of a no_op, as it normally does, when running under a distribution strategy. (Seems this is an easy fix, e.g. by using tf.group (see below))\r\n- (Minor) Error message: Would it not be more meaningful if the below error message \"... use a lambda as the initializer\" said something along the lines of: \"... use a callable, e.g. an Initializer (tf.keras.initializers.Initializer) or a lambda, as the initializer\" ?\r\n\r\nIs there something i might have missed?\r\n \r\n**Source code / logs**\r\n\r\n    import tensorflow as tf\r\n\r\n    class RNNModel(object):\r\n    \r\n      def __init__(self, hparams):\r\n        return\r\n    \r\n      def __call__(self, features, labels, mode, params):\r\n    \r\n        inputs = features[0]\r\n    \r\n        print(inputs)\r\n    \r\n        cell_fw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\r\n          300)\r\n        cell_bw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\r\n          300)\r\n    \r\n        (outputs, output_states) = tf.nn.bidirectional_dynamic_rnn(\r\n          cell_fw,\r\n          cell_bw,\r\n          inputs,\r\n          sequence_length=tf.convert_to_tensor([2, 2]),\r\n          initial_state_fw=None,\r\n          initial_state_bw=None,\r\n          dtype=tf.float32,\r\n          parallel_iterations=None,\r\n          swap_memory=False,\r\n          time_major=False,\r\n          scope=None\r\n        )\r\n    \r\n        model_output = tf.concat(outputs, axis=-1)\r\n        mock_loss = 10 - tf.reduce_sum(model_output)\r\n    \r\n        train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mock_loss,\r\n                                                                             global_step=tf.train.get_or_create_global_step())\r\n        train_op = tf.group(train_op)\r\n    \r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n          return tf.estimator.EstimatorSpec(mode, loss=mock_loss, train_op=train_op)\r\n        elif mode == tf.estimator.ModeKeys.EVAL:\r\n          return tf.estimator.EstimatorSpec(mode, loss=mock_loss, eval_metric_ops=None)\r\n        elif mode == tf.estimator.ModeKeys.PREDICT:\r\n          predictions = {\r\n            'mock': 1,\r\n          }\r\n          return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n    \r\n    \r\n    def train(hparams):\r\n      model = RNNModel(hparams)\r\n    \r\n      distribution_strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\n      #distribution_strategy = tf.contrib.distribute.OneDeviceStrategy(tf.DeviceSpec(device_type=\"GPU\", device_index=0))\r\n    \r\n      checkpointing_config = tf.estimator.RunConfig(\r\n        save_checkpoints_secs=20 * 60,  # Save checkpoints every 20 minutes.\r\n        keep_checkpoint_max=10,  # Retain the 10 most recent checkpoints.\r\n        train_distribute=distribution_strategy\r\n      )\r\n    \r\n      estimator = tf.estimator.Estimator(\r\n        model_fn=model,\r\n        model_dir=\"out/\",\r\n        params=hparams,\r\n        config=checkpointing_config,\r\n      )\r\n    \r\n      def create_mock_dataset():\r\n        mock_data = tf.convert_to_tensor([[[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]],\r\n                                          [[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]]])\r\n    \r\n        mock_data_set = tf.data.Dataset.from_tensors(mock_data)\r\n        mock_data_set = mock_data_set.map(lambda mock_data: (((mock_data,)), ()) )\r\n    \r\n        return mock_data_set\r\n    \r\n      num_train_steps = 100\r\n      estimator.train(create_mock_dataset, hooks=None,\r\n                      steps=num_train_steps)\r\n    \r\n    if __name__ == '__main__':\r\n      train({\"mock\": \"mock\"})\r\n\r\nThe code produces the following  output:\r\n```\r\nTraceback (most recent call last):\r\nFile \"C:\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n\"main\", mod_spec)\r\nFile \"C:\\Python36\\lib\\runpy.py\", line 85, in _run_code\r\nexec(code, run_globals)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 105, in \r\ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n_sys.exit(main(argv))\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 96, in main\r\nrun_main(FLAGS, default_hparams, train_fn, inference_fn, hparams_creator)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 88, in run_main\r\ntrain_fn(hparams)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\train_estimator.py\", line 470, in s2s_train\r\nestimator.train(input_fn=train_input_fn, hooks=train_hooks,steps=num_train_steps)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 363, in train\r\nloss = self._train_model(input_fn, hooks, saving_listeners)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 841, in _train_model\r\nreturn self._train_model_distributed(input_fn, hooks, saving_listeners)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 884, in _train_model_distributed\r\nself.config)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 756, in call_for_each_tower\r\nreturn self._call_for_each_tower(fn, *args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 254, in _call_for_each_tower\r\ncoord.join(threads)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 389, in join\r\nsix.reraise(*self._exc_info_to_raise)\r\nFile \"C:\\Python36\\lib\\site-packages\\six.py\", line 693, in reraise\r\nraise value\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\r\nyield\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 465, in run\r\nself.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 831, in _call_model_fn\r\nmodel_fn_results = self._model_fn(features=features, **kwargs)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\s2s_base_model.py\", line 157, in call\r\nres = self.build_graph(hparams, features, labels)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 83, in build_graph\r\nencoded_outputs, encoded_state = self._build_encoder(hparams, features)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 202, in _build_encoder\r\nreturn self._build_flat_encoder(hparams, features)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 250, in _build_flat_encoder\r\nnum_bi_residual_layers=num_bi_residual_layers))\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 768, in _build_bidirectional_rnn\r\ntime_major=self.time_major)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 412, in bidirectional_dynamic_rnn\r\ntime_major=time_major, scope=fw_scope)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 627, in dynamic_rnn\r\ndtype=dtype)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 824, in _dynamic_rnn_loop\r\nswap_memory=swap_memory)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3224, in while_loop\r\nresult = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2956, in BuildLoop\r\npred, body, original_loop_vars, loop_vars, shape_invariants)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2893, in _BuildLoop\r\nbody_result = body(*packed_vars_for_body)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3194, in \r\nbody = lambda i, lv: (i + 1, orig_body(*lv))\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 793, in _time_step\r\nskip_conditionals=True)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 248, in _rnn_step\r\nnew_output, new_state = call_cell()\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 781, in \r\ncall_cell = lambda: cell(input_t, state)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 232, in call\r\nreturn super(RNNCell, self).call(inputs, state)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 717, in call\r\noutputs = self.call(inputs, *args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1292, in call\r\ncur_inp, new_state = cell(cur_inp, cur_state)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1099, in call\r\noutput, new_state = self._cell(inputs, state, scope=scope)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 339, in call\r\n*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 699, in call\r\nself.build(input_shapes)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 767, in build\r\npartitioner=maybe_partitioner)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 546, in add_variable\r\npartitioner=partitioner)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable.py\", line 436, in _add_variable_with_custom_getter\r\n**kwargs_for_getter)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1317, in get_variable\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1079, in get_variable\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 417, in get_variable\r\nreturn custom_getter(**custom_getter_kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1720, in wrapped_custom_getter\r\n*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 235, in _rnn_get_variable\r\nvariable = getter(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 575, in disable_partitioned_variables\r\nreturn getter(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\r\nuse_resource=use_resource, constraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 786, in _get_single_variable\r\nuse_resource=use_resource)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2220, in variable\r\nuse_resource=use_resource)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \r\nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\shared_variable_creator.py\", line 69, in create_new_variable\r\nv = next_creator(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \r\nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 568, in creator_with_resource_vars\r\nreturn self._create_variable(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 118, in _create_variable\r\nv = next_creator(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2210, in \r\nprevious_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2182, in default_variable_creator\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 282, in init\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 421, in _init_from_args\r\n\"initializer.\" % name)\r\nValueError: Initializer for variable encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/replica_1/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.\r\n```"}