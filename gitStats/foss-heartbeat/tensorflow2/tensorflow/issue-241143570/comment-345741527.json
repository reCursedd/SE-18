{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345741527", "html_url": "https://github.com/tensorflow/tensorflow/issues/11339#issuecomment-345741527", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11339", "id": 345741527, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTc0MTUyNw==", "user": {"login": "rryan", "id": 26527, "node_id": "MDQ6VXNlcjI2NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/26527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rryan", "html_url": "https://github.com/rryan", "followers_url": "https://api.github.com/users/rryan/followers", "following_url": "https://api.github.com/users/rryan/following{/other_user}", "gists_url": "https://api.github.com/users/rryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rryan/subscriptions", "organizations_url": "https://api.github.com/users/rryan/orgs", "repos_url": "https://api.github.com/users/rryan/repos", "events_url": "https://api.github.com/users/rryan/events{/privacy}", "received_events_url": "https://api.github.com/users/rryan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-20T16:04:57Z", "updated_at": "2017-11-20T16:08:30Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26527\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rryan\">@rryan</a> ,<br>\nI can see that there are two alternatives for processing raw audio waveforms (to obtain spectrograms and mfccs, for example):</p>\n<p>audio_ops.audio_spectrogram(), audio_ops.mfcc()<br>\ncontrib.signal.stft(), some processing, contrib.signal.mfcc_from_log_mel...()<br>\nWhat are the intended / recommended use cases of these two approaches ? It looks confusing to me.</p>\n</blockquote>\n<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6018251\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/georgesterpu\">@georgesterpu</a>,</p>\n<p>You can think of <code>audio_ops.audio_spectrogram</code> and <code>audio_ops.mfcc</code> as \"fused\" ops (like fused batch-norm or fused LSTM cells that TensorFlow has) for the ops in <code>tf.contrib.signal</code>. I think the original motivation of them was that a fused op makes it easier to provide mobile support. I think long term it would be nice if we removed them and provided automatic fusing via XLA, or unified the API to match <code>tf.contrib.signal</code> API, and provided <code>fused</code> keyword arguments to <code>tf.contrib.signal</code> functions, like we do for <code>tf.layers.batch_normalization</code>.</p>\n<p>The benefits of <code>tf.contrib.signal</code> are:</p>\n<ul>\n<li>Support for backpropagation in training. If you are building machine learning models that generate audio this is crucial. For example, you could include an MFCC loss in a network that generates spectrograms (such as <a href=\"https://google.github.io/tacotron/publications/tacotron/index.html\" rel=\"nofollow\">Tacotron</a>) that ensures the MFCCs of your generated spectrogram match those of the target spectrogram.</li>\n<li>GPU/TPU support -- another crucial feature for training. It's so fast on GPU that you can just compute them as part of your network and feed raw waveforms into your model. We compute all our features on the fly on my team at Google. My team is adding TPU support for all of these right now (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"250521351\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12314\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/12314/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/12314\">#12314</a>).</li>\n<li>Flexibility and experimentation -- you can change the computation of the MFCC or spectrogram to find out what combinations of operations (e.g. different compression options, different frequency warpings, etc.) work best for your use case. There is no \"one true\" way of computing a spectrogram. There are dozens of different choices that go into it, and machine learning practitioners need the flexibility to experiment.</li>\n</ul>", "body_text": "Hi @rryan ,\nI can see that there are two alternatives for processing raw audio waveforms (to obtain spectrograms and mfccs, for example):\naudio_ops.audio_spectrogram(), audio_ops.mfcc()\ncontrib.signal.stft(), some processing, contrib.signal.mfcc_from_log_mel...()\nWhat are the intended / recommended use cases of these two approaches ? It looks confusing to me.\n\nHi @georgesterpu,\nYou can think of audio_ops.audio_spectrogram and audio_ops.mfcc as \"fused\" ops (like fused batch-norm or fused LSTM cells that TensorFlow has) for the ops in tf.contrib.signal. I think the original motivation of them was that a fused op makes it easier to provide mobile support. I think long term it would be nice if we removed them and provided automatic fusing via XLA, or unified the API to match tf.contrib.signal API, and provided fused keyword arguments to tf.contrib.signal functions, like we do for tf.layers.batch_normalization.\nThe benefits of tf.contrib.signal are:\n\nSupport for backpropagation in training. If you are building machine learning models that generate audio this is crucial. For example, you could include an MFCC loss in a network that generates spectrograms (such as Tacotron) that ensures the MFCCs of your generated spectrogram match those of the target spectrogram.\nGPU/TPU support -- another crucial feature for training. It's so fast on GPU that you can just compute them as part of your network and feed raw waveforms into your model. We compute all our features on the fly on my team at Google. My team is adding TPU support for all of these right now (#12314).\nFlexibility and experimentation -- you can change the computation of the MFCC or spectrogram to find out what combinations of operations (e.g. different compression options, different frequency warpings, etc.) work best for your use case. There is no \"one true\" way of computing a spectrogram. There are dozens of different choices that go into it, and machine learning practitioners need the flexibility to experiment.", "body": "> Hi @rryan ,\r\n> I can see that there are two alternatives for processing raw audio waveforms (to obtain spectrograms and mfccs, for example):\r\n> \r\n> audio_ops.audio_spectrogram(), audio_ops.mfcc()\r\n> contrib.signal.stft(), some processing, contrib.signal.mfcc_from_log_mel...()\r\n> What are the intended / recommended use cases of these two approaches ? It looks confusing to me.\r\n\r\nHi @georgesterpu,\r\n\r\nYou can think of `audio_ops.audio_spectrogram` and `audio_ops.mfcc` as \"fused\" ops (like fused batch-norm or fused LSTM cells that TensorFlow has) for the ops in `tf.contrib.signal`. I think the original motivation of them was that a fused op makes it easier to provide mobile support. I think long term it would be nice if we removed them and provided automatic fusing via XLA, or unified the API to match `tf.contrib.signal` API, and provided `fused` keyword arguments to `tf.contrib.signal` functions, like we do for `tf.layers.batch_normalization`.\r\n\r\nThe benefits of `tf.contrib.signal` are:\r\n* Support for backpropagation in training. If you are building machine learning models that generate audio this is crucial. For example, you could include an MFCC loss in a network that generates spectrograms (such as [Tacotron](https://google.github.io/tacotron/publications/tacotron/index.html)) that ensures the MFCCs of your generated spectrogram match those of the target spectrogram.\r\n* GPU/TPU support -- another crucial feature for training. It's so fast on GPU that you can just compute them as part of your network and feed raw waveforms into your model. We compute all our features on the fly on my team at Google. My team is adding TPU support for all of these right now (#12314).\r\n* Flexibility and experimentation -- you can change the computation of the MFCC or spectrogram to find out what combinations of operations (e.g. different compression options, different frequency warpings, etc.) work best for your use case. There is no \"one true\" way of computing a spectrogram. There are dozens of different choices that go into it, and machine learning practitioners need the flexibility to experiment."}