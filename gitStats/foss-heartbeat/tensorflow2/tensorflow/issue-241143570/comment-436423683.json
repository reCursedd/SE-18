{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436423683", "html_url": "https://github.com/tensorflow/tensorflow/issues/11339#issuecomment-436423683", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11339", "id": 436423683, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjQyMzY4Mw==", "user": {"login": "vadel", "id": 23345525, "node_id": "MDQ6VXNlcjIzMzQ1NTI1", "avatar_url": "https://avatars1.githubusercontent.com/u/23345525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadel", "html_url": "https://github.com/vadel", "followers_url": "https://api.github.com/users/vadel/followers", "following_url": "https://api.github.com/users/vadel/following{/other_user}", "gists_url": "https://api.github.com/users/vadel/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadel/subscriptions", "organizations_url": "https://api.github.com/users/vadel/orgs", "repos_url": "https://api.github.com/users/vadel/repos", "events_url": "https://api.github.com/users/vadel/events{/privacy}", "received_events_url": "https://api.github.com/users/vadel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-06T21:58:18Z", "updated_at": "2018-11-07T19:16:06Z", "author_association": "NONE", "body_html": "<p>Hi, I am trying to update the feature extraction pipeline of an speech command recognition model replacing  the function <code>audio_ops.audio_spectrogram()</code> by  <code>tf.contrib.signal.stft()</code>. I assumed that they were equivalent, but  I am obtaining different results with the same input audio. Could someone explain the relation between the two methods, or whether it is possible to obtain the same results using <code>tf.contrib.signal.stft()</code>?</p>\n<p>My code:</p>\n<ol>\n<li><code>audio_ops</code> method:</li>\n</ol>\n<pre><code>#WAV audio loader\nwav_filename_placeholder_ = tf.placeholder(tf.string, [], name='wav_filename')\nwav_loader = io_ops.read_file(wav_filename_placeholder_)\nsample_rate = 16000\ndesired_samples = 16000 #1 sec audio\nwav_decoder = audio_ops.decode_wav(wav_loader, desired_channels=1, desired_samples=desired_samples)\n\n#Computing the spectrograms\nspectrogram = audio_ops.audio_spectrogram(wav_decoder.audio,\n                                              window_size=320,\n                                              stride=160,\n                                              magnitude_squared=False)\nwith tf.Session() as sess:\n    feed_dict={wav_filename_placeholder_:\"/&lt;folder_path&gt;/audio_sample.wav\"}\n    #Get the input audio and the spectrogram\n    audio_ops_wav_decoder_audio, audio_ops_spectrogram = sess.run([wav_decoder.audio, spectrogram], feed_dict)\n</code></pre>\n<ol start=\"2\">\n<li><code>tf.contrib.signal</code> method</li>\n</ol>\n<pre><code>#Input WAV audio (will be initialized with the same audio signal: wav_decoder.audio )\nsignals = tf.placeholder(tf.float32, [None, None])\n\n#Compute the spectrograms and get the absolute values\nstfts = tf.contrib.signal.stft(signals, \n                               frame_length=320, \n                               frame_step=160, \n                               fft_length=512, \n                               window_fn=None)\nmagnitude_spectrograms = tf.abs(stfts)\nwith tf.Session() as sess:\n    feed_dict = {signals : audio_ops_wav_decoder_audio.reshape(1,16000)}\n    tf_original, tf_stfts, tf_spectrogram, = sess.run([signals, stfts, magnitude_spectrograms], feed_dict)\n</code></pre>\n<p>Thank you in advance</p>", "body_text": "Hi, I am trying to update the feature extraction pipeline of an speech command recognition model replacing  the function audio_ops.audio_spectrogram() by  tf.contrib.signal.stft(). I assumed that they were equivalent, but  I am obtaining different results with the same input audio. Could someone explain the relation between the two methods, or whether it is possible to obtain the same results using tf.contrib.signal.stft()?\nMy code:\n\naudio_ops method:\n\n#WAV audio loader\nwav_filename_placeholder_ = tf.placeholder(tf.string, [], name='wav_filename')\nwav_loader = io_ops.read_file(wav_filename_placeholder_)\nsample_rate = 16000\ndesired_samples = 16000 #1 sec audio\nwav_decoder = audio_ops.decode_wav(wav_loader, desired_channels=1, desired_samples=desired_samples)\n\n#Computing the spectrograms\nspectrogram = audio_ops.audio_spectrogram(wav_decoder.audio,\n                                              window_size=320,\n                                              stride=160,\n                                              magnitude_squared=False)\nwith tf.Session() as sess:\n    feed_dict={wav_filename_placeholder_:\"/<folder_path>/audio_sample.wav\"}\n    #Get the input audio and the spectrogram\n    audio_ops_wav_decoder_audio, audio_ops_spectrogram = sess.run([wav_decoder.audio, spectrogram], feed_dict)\n\n\ntf.contrib.signal method\n\n#Input WAV audio (will be initialized with the same audio signal: wav_decoder.audio )\nsignals = tf.placeholder(tf.float32, [None, None])\n\n#Compute the spectrograms and get the absolute values\nstfts = tf.contrib.signal.stft(signals, \n                               frame_length=320, \n                               frame_step=160, \n                               fft_length=512, \n                               window_fn=None)\nmagnitude_spectrograms = tf.abs(stfts)\nwith tf.Session() as sess:\n    feed_dict = {signals : audio_ops_wav_decoder_audio.reshape(1,16000)}\n    tf_original, tf_stfts, tf_spectrogram, = sess.run([signals, stfts, magnitude_spectrograms], feed_dict)\n\nThank you in advance", "body": "Hi, I am trying to update the feature extraction pipeline of an speech command recognition model replacing  the function `audio_ops.audio_spectrogram()` by  `tf.contrib.signal.stft()`. I assumed that they were equivalent, but  I am obtaining different results with the same input audio. Could someone explain the relation between the two methods, or whether it is possible to obtain the same results using `tf.contrib.signal.stft()`? \r\n\r\nMy code:\r\n\r\n1) `audio_ops` method:\r\n```\r\n#WAV audio loader\r\nwav_filename_placeholder_ = tf.placeholder(tf.string, [], name='wav_filename')\r\nwav_loader = io_ops.read_file(wav_filename_placeholder_)\r\nsample_rate = 16000\r\ndesired_samples = 16000 #1 sec audio\r\nwav_decoder = audio_ops.decode_wav(wav_loader, desired_channels=1, desired_samples=desired_samples)\r\n\r\n#Computing the spectrograms\r\nspectrogram = audio_ops.audio_spectrogram(wav_decoder.audio,\r\n                                              window_size=320,\r\n                                              stride=160,\r\n                                              magnitude_squared=False)\r\nwith tf.Session() as sess:\r\n    feed_dict={wav_filename_placeholder_:\"/<folder_path>/audio_sample.wav\"}\r\n    #Get the input audio and the spectrogram\r\n    audio_ops_wav_decoder_audio, audio_ops_spectrogram = sess.run([wav_decoder.audio, spectrogram], feed_dict)\r\n```\r\n\r\n2) `tf.contrib.signal` method\r\n```\r\n#Input WAV audio (will be initialized with the same audio signal: wav_decoder.audio )\r\nsignals = tf.placeholder(tf.float32, [None, None])\r\n\r\n#Compute the spectrograms and get the absolute values\r\nstfts = tf.contrib.signal.stft(signals, \r\n                               frame_length=320, \r\n                               frame_step=160, \r\n                               fft_length=512, \r\n                               window_fn=None)\r\nmagnitude_spectrograms = tf.abs(stfts)\r\nwith tf.Session() as sess:\r\n    feed_dict = {signals : audio_ops_wav_decoder_audio.reshape(1,16000)}\r\n    tf_original, tf_stfts, tf_spectrogram, = sess.run([signals, stfts, magnitude_spectrograms], feed_dict)\r\n```\r\n\r\nThank you in advance"}