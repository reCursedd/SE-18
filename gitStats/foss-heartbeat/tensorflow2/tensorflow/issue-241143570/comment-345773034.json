{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345773034", "html_url": "https://github.com/tensorflow/tensorflow/issues/11339#issuecomment-345773034", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11339", "id": 345773034, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTc3MzAzNA==", "user": {"login": "rryan", "id": 26527, "node_id": "MDQ6VXNlcjI2NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/26527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rryan", "html_url": "https://github.com/rryan", "followers_url": "https://api.github.com/users/rryan/followers", "following_url": "https://api.github.com/users/rryan/following{/other_user}", "gists_url": "https://api.github.com/users/rryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rryan/subscriptions", "organizations_url": "https://api.github.com/users/rryan/orgs", "repos_url": "https://api.github.com/users/rryan/repos", "events_url": "https://api.github.com/users/rryan/events{/privacy}", "received_events_url": "https://api.github.com/users/rryan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-20T17:46:21Z", "updated_at": "2017-11-20T17:46:21Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8589835\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jundengdeng\">@jundengdeng</a>, yup -- the main benefit of a \"fused\" op is that it avoids the compute and memory overhead of the TensorFlow runtime executing a graph of fine-grained ops. For this reason, on CPU the fused ops are likely to perform better (they don't support GPU/TPU).</p>\n<p>We are working on making these faster, but as I said, on GPU they are plenty fast for use on the fly (no preprocessing). My suggestion would be to use them on GPU for training instead of running them on CPU in a preprocessing queue / tf.data pipeline.</p>\n<p>We are working on making them faster. My hope is that XLA will be able to automatically compile <code>tf.contrib.signal</code> into something that is as fast as a hand-written fused op on CPU.</p>", "body_text": "@jundengdeng, yup -- the main benefit of a \"fused\" op is that it avoids the compute and memory overhead of the TensorFlow runtime executing a graph of fine-grained ops. For this reason, on CPU the fused ops are likely to perform better (they don't support GPU/TPU).\nWe are working on making these faster, but as I said, on GPU they are plenty fast for use on the fly (no preprocessing). My suggestion would be to use them on GPU for training instead of running them on CPU in a preprocessing queue / tf.data pipeline.\nWe are working on making them faster. My hope is that XLA will be able to automatically compile tf.contrib.signal into something that is as fast as a hand-written fused op on CPU.", "body": "@jundengdeng, yup -- the main benefit of a \"fused\" op is that it avoids the compute and memory overhead of the TensorFlow runtime executing a graph of fine-grained ops. For this reason, on CPU the fused ops are likely to perform better (they don't support GPU/TPU).\r\n\r\nWe are working on making these faster, but as I said, on GPU they are plenty fast for use on the fly (no preprocessing). My suggestion would be to use them on GPU for training instead of running them on CPU in a preprocessing queue / tf.data pipeline.\r\n\r\nWe are working on making them faster. My hope is that XLA will be able to automatically compile `tf.contrib.signal` into something that is as fast as a hand-written fused op on CPU."}