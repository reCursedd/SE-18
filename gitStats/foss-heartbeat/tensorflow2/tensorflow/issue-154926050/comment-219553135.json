{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219553135", "html_url": "https://github.com/tensorflow/tensorflow/issues/2376#issuecomment-219553135", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2376", "id": 219553135, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTU1MzEzNQ==", "user": {"login": "zxzhijia", "id": 11791424, "node_id": "MDQ6VXNlcjExNzkxNDI0", "avatar_url": "https://avatars3.githubusercontent.com/u/11791424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zxzhijia", "html_url": "https://github.com/zxzhijia", "followers_url": "https://api.github.com/users/zxzhijia/followers", "following_url": "https://api.github.com/users/zxzhijia/following{/other_user}", "gists_url": "https://api.github.com/users/zxzhijia/gists{/gist_id}", "starred_url": "https://api.github.com/users/zxzhijia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zxzhijia/subscriptions", "organizations_url": "https://api.github.com/users/zxzhijia/orgs", "repos_url": "https://api.github.com/users/zxzhijia/repos", "events_url": "https://api.github.com/users/zxzhijia/events{/privacy}", "received_events_url": "https://api.github.com/users/zxzhijia/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-16T21:26:07Z", "updated_at": "2016-05-16T21:33:50Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> tensorflow.examples.tutorials.mnist <span class=\"pl-k\">import</span> input_data\nmnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MNIST_data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>])\ny_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>])\nW <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">784</span>,<span class=\"pl-c1\">10</span>]))\nb <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">10</span>]))\n\nsess.run(tf.initialize_all_variables())\ny <span class=\"pl-k\">=</span> tf.nn.softmax(tf.matmul(x,W) <span class=\"pl-k\">+</span> b)\ncross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(<span class=\"pl-k\">-</span>tf.reduce_sum(y_ <span class=\"pl-k\">*</span> tf.log(y), <span class=\"pl-v\">reduction_indices</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>]))\ntrain_step <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.5</span>).minimize(cross_entropy)\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n  batch <span class=\"pl-k\">=</span> mnist.train.next_batch(<span class=\"pl-c1\">50</span>)\n  train_step.run(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>]})\ncorrect_prediction <span class=\"pl-k\">=</span> tf.equal(tf.argmax(y,<span class=\"pl-c1\">1</span>), tf.argmax(y_,<span class=\"pl-c1\">1</span>))\naccuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n<span class=\"pl-c1\">print</span>(accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: mnist.test.images, y_: mnist.test.labels}))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">weight_variable</span>(<span class=\"pl-smi\">shape</span>):\n  initial <span class=\"pl-k\">=</span> tf.truncated_normal(shape, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n  <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bias_variable</span>(<span class=\"pl-smi\">shape</span>):\n  initial <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape)\n  <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">conv2d</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">W</span>):\n  <span class=\"pl-k\">return</span> tf.nn.conv2d(x, W, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">max_pool_2x2</span>(<span class=\"pl-smi\">x</span>):\n  <span class=\"pl-k\">return</span> tf.nn.max_pool(x, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>],\n                        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\nW_conv1 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>])\nb_conv1 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">32</span>])\n\nx_image <span class=\"pl-k\">=</span> tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">1</span>])\n\nh_conv1 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(x_image, W_conv1) <span class=\"pl-k\">+</span> b_conv1)\nh_pool1 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv1)\n\nW_conv2 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>])\nb_conv2 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">64</span>])\n\nh_conv2 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(h_pool1, W_conv2) <span class=\"pl-k\">+</span> b_conv2)\nh_pool2 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv2)\n\nW_fc1 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1024</span>])\nb_fc1 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">1024</span>])\n\nh_pool2_flat <span class=\"pl-k\">=</span> tf.reshape(h_pool2, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">7</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">7</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">64</span>])\nh_fc1 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) <span class=\"pl-k\">+</span> b_fc1)\n\nkeep_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\nh_fc1_drop <span class=\"pl-k\">=</span> tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">10</span>])\nb_fc2 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">10</span>])\n\ny_conv<span class=\"pl-k\">=</span>tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) <span class=\"pl-k\">+</span> b_fc2)\n\ncross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(<span class=\"pl-k\">-</span>tf.reduce_sum(y_ <span class=\"pl-k\">*</span> tf.log(y_conv), <span class=\"pl-v\">reduction_indices</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>]))\ntrain_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-4</span>).minimize(cross_entropy)\ncorrect_prediction <span class=\"pl-k\">=</span> tf.equal(tf.argmax(y_conv,<span class=\"pl-c1\">1</span>), tf.argmax(y_,<span class=\"pl-c1\">1</span>))\naccuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.initialize_all_variables())\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2000</span>):\n  batch <span class=\"pl-k\">=</span> mnist.train.next_batch(<span class=\"pl-c1\">50</span>)\n  <span class=\"pl-k\">if</span> i<span class=\"pl-k\">%</span><span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n    train_accuracy <span class=\"pl-k\">=</span> accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n        x:batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>], keep_prob: <span class=\"pl-c1\">1.0</span>})\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>step <span class=\"pl-c1\">%d</span>, training accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(i, train_accuracy))\n  train_step.run(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>], keep_prob: <span class=\"pl-c1\">0.5</span>})\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class=\"pl-c1\">1.0</span>}))</pre></div>", "body_text": "from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\nsess.run(tf.initialize_all_variables())\ny = tf.nn.softmax(tf.matmul(x,W) + b)\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\nfor i in range(1000):\n  batch = mnist.train.next_batch(50)\n  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1,28,28,1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.initialize_all_variables())\nfor i in range(2000):\n  batch = mnist.train.next_batch(50)\n  if i%100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))", "body": "``` python\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\nsess.run(tf.initialize_all_variables())\ny = tf.nn.softmax(tf.matmul(x,W) + b)\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\nfor i in range(1000):\n  batch = mnist.train.next_batch(50)\n  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1,28,28,1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.initialize_all_variables())\nfor i in range(2000):\n  batch = mnist.train.next_batch(50)\n  if i%100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n```\n"}