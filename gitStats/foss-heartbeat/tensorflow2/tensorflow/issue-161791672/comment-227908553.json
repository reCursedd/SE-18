{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/227908553", "html_url": "https://github.com/tensorflow/tensorflow/issues/3000#issuecomment-227908553", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3000", "id": 227908553, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNzkwODU1Mw==", "user": {"login": "zmimlitz", "id": 10762452, "node_id": "MDQ6VXNlcjEwNzYyNDUy", "avatar_url": "https://avatars1.githubusercontent.com/u/10762452?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zmimlitz", "html_url": "https://github.com/zmimlitz", "followers_url": "https://api.github.com/users/zmimlitz/followers", "following_url": "https://api.github.com/users/zmimlitz/following{/other_user}", "gists_url": "https://api.github.com/users/zmimlitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/zmimlitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zmimlitz/subscriptions", "organizations_url": "https://api.github.com/users/zmimlitz/orgs", "repos_url": "https://api.github.com/users/zmimlitz/repos", "events_url": "https://api.github.com/users/zmimlitz/events{/privacy}", "received_events_url": "https://api.github.com/users/zmimlitz/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-22T23:34:39Z", "updated_at": "2016-06-22T23:34:39Z", "author_association": "NONE", "body_html": "<p>Wow that seemed to solved the problem.  Data is now being returned after the batch creation returns.   However at least at the beginning the batch method is not releasing as much as it consumed and train_op is still leaking, but the amount to which they do so decays out after the first few iterations to a (mostly) even use function.  The graph below shows 10 iterations.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/10762452/16287145/e0206aca-3895-11e6-944d-87586d853244.png\"><img src=\"https://cloud.githubusercontent.com/assets/10762452/16287145/e0206aca-3895-11e6-944d-87586d853244.png\" alt=\"memory_leak_plot_2\" style=\"max-width:100%;\"></a></p>\n<p>Over 400 iterations in use memory increased by less than 100MB, was formally more like 5GB.  Thank you, that was a silly thing for me to do wrong.</p>", "body_text": "Wow that seemed to solved the problem.  Data is now being returned after the batch creation returns.   However at least at the beginning the batch method is not releasing as much as it consumed and train_op is still leaking, but the amount to which they do so decays out after the first few iterations to a (mostly) even use function.  The graph below shows 10 iterations.\n\nOver 400 iterations in use memory increased by less than 100MB, was formally more like 5GB.  Thank you, that was a silly thing for me to do wrong.", "body": "Wow that seemed to solved the problem.  Data is now being returned after the batch creation returns.   However at least at the beginning the batch method is not releasing as much as it consumed and train_op is still leaking, but the amount to which they do so decays out after the first few iterations to a (mostly) even use function.  The graph below shows 10 iterations.\n\n![memory_leak_plot_2](https://cloud.githubusercontent.com/assets/10762452/16287145/e0206aca-3895-11e6-944d-87586d853244.png)\n\nOver 400 iterations in use memory increased by less than 100MB, was formally more like 5GB.  Thank you, that was a silly thing for me to do wrong.\n"}