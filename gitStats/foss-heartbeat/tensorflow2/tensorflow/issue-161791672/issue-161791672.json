{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3000", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3000/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3000/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3000/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3000", "id": 161791672, "node_id": "MDU6SXNzdWUxNjE3OTE2NzI=", "number": 3000, "title": "Memory Leak Converting Numpy ndarry to Tensor", "user": {"login": "zmimlitz", "id": 10762452, "node_id": "MDQ6VXNlcjEwNzYyNDUy", "avatar_url": "https://avatars1.githubusercontent.com/u/10762452?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zmimlitz", "html_url": "https://github.com/zmimlitz", "followers_url": "https://api.github.com/users/zmimlitz/followers", "following_url": "https://api.github.com/users/zmimlitz/following{/other_user}", "gists_url": "https://api.github.com/users/zmimlitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/zmimlitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zmimlitz/subscriptions", "organizations_url": "https://api.github.com/users/zmimlitz/orgs", "repos_url": "https://api.github.com/users/zmimlitz/repos", "events_url": "https://api.github.com/users/zmimlitz/events{/privacy}", "received_events_url": "https://api.github.com/users/zmimlitz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-06-22T21:32:02Z", "updated_at": "2016-06-23T17:40:43Z", "closed_at": "2016-06-23T17:40:43Z", "author_association": "NONE", "body_html": "<p>I'm working on a small convolution network to identify faces which takes png images and loads them into numpy arrays through PIL.  Once all the images are loaded, they are passed into the tensorflow model through <code>train_op.train(feed_dict={input:train_images})</code>.  The problem is that each iteration of the training loop copies a new batch from the total loaded images before passing the batch into train_op and that memory is never released.</p>\n<p>I've looked and looked and found several related issues to memory not being released but nothing that I thought really fit my problem or had a working solution.  I am aware that this process of pre-loading images is not ideal, but I have other obstacles to using a queue structure to read the images directly to tensors and I would like to address this as a memory leak.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04<br>\nPython: 2.7.6<br>\nTensorflow: 0.8.0<br>\nCUDA: 7.5<br>\ncuDNN: 4.0.7</p>\n<h3>Steps to reproduce</h3>\n<p>Below is a short python script that replicates the problem by loading an image (pass a filepath as argument) and passing it to <code>tf.image.random_flip_left_right()</code> while printing out the memory in use by on the computer.</p>\n<pre><code>import tensorflow as tf\nimport psutil\nimport numpy as np\nimport Image\nimport sys\nimport gc\n\ndef printMemUsed(discript):\n    print(\"%s:\\t%d\" % (discript, psutil.virtual_memory().used))\n\ndef main(file):\n    sess = tf.InteractiveSession()\n    im = Image.open(file)\n    arr = np.array(im)\n    printMemUsed(\"After Array Creation\")\n    arr = flipArr(arr)\n    printMemUsed(\"After Tensor Conversion\")\n    del arr\n    printMemUsed(\"After Array Deletion\")\n\ndef flipArr(arr):\n    tensor = tf.image.random_flip_left_right(arr)\n    arr = tensor.eval()\n    return arr  \n\nif __name__ == '__main__':\n    main(sys.argv[1])\n    printMemUsed(\"After Scope Lost\")\n    gc.collect()\n    printMemUsed(\"After gc Collect\")\n</code></pre>\n<p>For me this program prints:</p>\n<blockquote>\n<p>After Array Creation:   1196838912<br>\nAfter Tensor Conversion:        1273106432<br>\nAfter Array Deletion:   1273106432<br>\nAfter Scope Lost:       1273106432<br>\nAfter gc Collect:       1273106432</p>\n</blockquote>\n<h3>What have you tried?</h3>\n<ol>\n<li>As shown in the example, gc.collect() and del do nothing to free the memory, it is not released when the variables pass out of scope (loop or method), only when the program exits, or in my case when I run out of RAM causing the machine to hang until manually restarted.</li>\n<li>Again I recognize that there are other (perhaps better) ways to get images into tensorflow, but at least for the start I'd like to address why the memory is not released.</li>\n</ol>\n<h3>Other Comments</h3>\n<p>The plot below shows memory usage over the first 5 training rounds of my full model.  The orange points mark the top of each loop.  The first jump is the creation of the batch, which passes all the images through <code>tf.image.random_flip_left_right()</code> as in the example above, the second jump from calling <code>train_op.train(feed_dict={input:train_images})</code>, passing in the newly created batch.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/10762452/16284315/75e71fec-3885-11e6-915e-a6d9f6610119.png\"><img src=\"https://cloud.githubusercontent.com/assets/10762452/16284315/75e71fec-3885-11e6-915e-a6d9f6610119.png\" alt=\"memory_leak_plot\" style=\"max-width:100%;\"></a></p>", "body_text": "I'm working on a small convolution network to identify faces which takes png images and loads them into numpy arrays through PIL.  Once all the images are loaded, they are passed into the tensorflow model through train_op.train(feed_dict={input:train_images}).  The problem is that each iteration of the training loop copies a new batch from the total loaded images before passing the batch into train_op and that memory is never released.\nI've looked and looked and found several related issues to memory not being released but nothing that I thought really fit my problem or had a working solution.  I am aware that this process of pre-loading images is not ideal, but I have other obstacles to using a queue structure to read the images directly to tensors and I would like to address this as a memory leak.\nEnvironment info\nOperating System: Ubuntu 14.04\nPython: 2.7.6\nTensorflow: 0.8.0\nCUDA: 7.5\ncuDNN: 4.0.7\nSteps to reproduce\nBelow is a short python script that replicates the problem by loading an image (pass a filepath as argument) and passing it to tf.image.random_flip_left_right() while printing out the memory in use by on the computer.\nimport tensorflow as tf\nimport psutil\nimport numpy as np\nimport Image\nimport sys\nimport gc\n\ndef printMemUsed(discript):\n    print(\"%s:\\t%d\" % (discript, psutil.virtual_memory().used))\n\ndef main(file):\n    sess = tf.InteractiveSession()\n    im = Image.open(file)\n    arr = np.array(im)\n    printMemUsed(\"After Array Creation\")\n    arr = flipArr(arr)\n    printMemUsed(\"After Tensor Conversion\")\n    del arr\n    printMemUsed(\"After Array Deletion\")\n\ndef flipArr(arr):\n    tensor = tf.image.random_flip_left_right(arr)\n    arr = tensor.eval()\n    return arr  \n\nif __name__ == '__main__':\n    main(sys.argv[1])\n    printMemUsed(\"After Scope Lost\")\n    gc.collect()\n    printMemUsed(\"After gc Collect\")\n\nFor me this program prints:\n\nAfter Array Creation:   1196838912\nAfter Tensor Conversion:        1273106432\nAfter Array Deletion:   1273106432\nAfter Scope Lost:       1273106432\nAfter gc Collect:       1273106432\n\nWhat have you tried?\n\nAs shown in the example, gc.collect() and del do nothing to free the memory, it is not released when the variables pass out of scope (loop or method), only when the program exits, or in my case when I run out of RAM causing the machine to hang until manually restarted.\nAgain I recognize that there are other (perhaps better) ways to get images into tensorflow, but at least for the start I'd like to address why the memory is not released.\n\nOther Comments\nThe plot below shows memory usage over the first 5 training rounds of my full model.  The orange points mark the top of each loop.  The first jump is the creation of the batch, which passes all the images through tf.image.random_flip_left_right() as in the example above, the second jump from calling train_op.train(feed_dict={input:train_images}), passing in the newly created batch.", "body": "I'm working on a small convolution network to identify faces which takes png images and loads them into numpy arrays through PIL.  Once all the images are loaded, they are passed into the tensorflow model through `train_op.train(feed_dict={input:train_images})`.  The problem is that each iteration of the training loop copies a new batch from the total loaded images before passing the batch into train_op and that memory is never released.\n\nI've looked and looked and found several related issues to memory not being released but nothing that I thought really fit my problem or had a working solution.  I am aware that this process of pre-loading images is not ideal, but I have other obstacles to using a queue structure to read the images directly to tensors and I would like to address this as a memory leak.\n### Environment info\n\nOperating System: Ubuntu 14.04\nPython: 2.7.6\nTensorflow: 0.8.0\nCUDA: 7.5\ncuDNN: 4.0.7\n### Steps to reproduce\n\nBelow is a short python script that replicates the problem by loading an image (pass a filepath as argument) and passing it to `tf.image.random_flip_left_right()` while printing out the memory in use by on the computer.  \n\n```\nimport tensorflow as tf\nimport psutil\nimport numpy as np\nimport Image\nimport sys\nimport gc\n\ndef printMemUsed(discript):\n    print(\"%s:\\t%d\" % (discript, psutil.virtual_memory().used))\n\ndef main(file):\n    sess = tf.InteractiveSession()\n    im = Image.open(file)\n    arr = np.array(im)\n    printMemUsed(\"After Array Creation\")\n    arr = flipArr(arr)\n    printMemUsed(\"After Tensor Conversion\")\n    del arr\n    printMemUsed(\"After Array Deletion\")\n\ndef flipArr(arr):\n    tensor = tf.image.random_flip_left_right(arr)\n    arr = tensor.eval()\n    return arr  \n\nif __name__ == '__main__':\n    main(sys.argv[1])\n    printMemUsed(\"After Scope Lost\")\n    gc.collect()\n    printMemUsed(\"After gc Collect\")\n```\n\nFor me this program prints:\n\n> After Array Creation:   1196838912\n> After Tensor Conversion:        1273106432\n> After Array Deletion:   1273106432\n> After Scope Lost:       1273106432\n> After gc Collect:       1273106432\n### What have you tried?\n1. As shown in the example, gc.collect() and del do nothing to free the memory, it is not released when the variables pass out of scope (loop or method), only when the program exits, or in my case when I run out of RAM causing the machine to hang until manually restarted.\n2. Again I recognize that there are other (perhaps better) ways to get images into tensorflow, but at least for the start I'd like to address why the memory is not released.\n### Other Comments\n\nThe plot below shows memory usage over the first 5 training rounds of my full model.  The orange points mark the top of each loop.  The first jump is the creation of the batch, which passes all the images through `tf.image.random_flip_left_right()` as in the example above, the second jump from calling `train_op.train(feed_dict={input:train_images})`, passing in the newly created batch.\n\n![memory_leak_plot](https://cloud.githubusercontent.com/assets/10762452/16284315/75e71fec-3885-11e6-915e-a6d9f6610119.png)\n"}