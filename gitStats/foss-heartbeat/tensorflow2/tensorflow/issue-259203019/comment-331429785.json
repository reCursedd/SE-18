{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331429785", "html_url": "https://github.com/tensorflow/tensorflow/issues/13187#issuecomment-331429785", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13187", "id": 331429785, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTQyOTc4NQ==", "user": {"login": "balaji-in-git", "id": 31880073, "node_id": "MDQ6VXNlcjMxODgwMDcz", "avatar_url": "https://avatars0.githubusercontent.com/u/31880073?v=4", "gravatar_id": "", "url": "https://api.github.com/users/balaji-in-git", "html_url": "https://github.com/balaji-in-git", "followers_url": "https://api.github.com/users/balaji-in-git/followers", "following_url": "https://api.github.com/users/balaji-in-git/following{/other_user}", "gists_url": "https://api.github.com/users/balaji-in-git/gists{/gist_id}", "starred_url": "https://api.github.com/users/balaji-in-git/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/balaji-in-git/subscriptions", "organizations_url": "https://api.github.com/users/balaji-in-git/orgs", "repos_url": "https://api.github.com/users/balaji-in-git/repos", "events_url": "https://api.github.com/users/balaji-in-git/events{/privacy}", "received_events_url": "https://api.github.com/users/balaji-in-git/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-22T12:10:54Z", "updated_at": "2017-09-22T12:15:54Z", "author_association": "NONE", "body_html": "<p>Each TFRecord is a sparse structure of feature shape  1000 222215 viz. basically I am making batches of 1000 features. TFRecordWriter works faster as compared to burning a 1000 examples of (1x222215)  . My labels are on one-hot with 58565 labels and 1000 rows again to match 1000 features.<br>\nUsing SparseFeature and parse_example is faster compared to using parse_single_example and/or VarLenFeature. But not significantly fast. (I didn't do decode_csv, it would have been worse anyway.)<br>\nWith this structure, I am able to train 400000 examples trained in batches of 1000 (each TFRecord) and 5 epochs in 861-900 secs. Is this the best I can get. I am ONLY printing the tensors. No matmuls, no optimizers.<br>\nI use 4 producer queues (string_input_producer), to read 8 TFRecord files (2 each) and shuffle_batch with 1-2 threads. If I use 4 threads, as is the case of shuffle_batch_join which creates as many number of threads to enqueue as the tensors in the list viz., 4 in my case, I get Timedout error waiting for notification. So I believe producers don't get enough time if I do that.<br>\nNow the question back, is this the best I can get on the said hardware?</p>", "body_text": "Each TFRecord is a sparse structure of feature shape  1000 222215 viz. basically I am making batches of 1000 features. TFRecordWriter works faster as compared to burning a 1000 examples of (1x222215)  . My labels are on one-hot with 58565 labels and 1000 rows again to match 1000 features.\nUsing SparseFeature and parse_example is faster compared to using parse_single_example and/or VarLenFeature. But not significantly fast. (I didn't do decode_csv, it would have been worse anyway.)\nWith this structure, I am able to train 400000 examples trained in batches of 1000 (each TFRecord) and 5 epochs in 861-900 secs. Is this the best I can get. I am ONLY printing the tensors. No matmuls, no optimizers.\nI use 4 producer queues (string_input_producer), to read 8 TFRecord files (2 each) and shuffle_batch with 1-2 threads. If I use 4 threads, as is the case of shuffle_batch_join which creates as many number of threads to enqueue as the tensors in the list viz., 4 in my case, I get Timedout error waiting for notification. So I believe producers don't get enough time if I do that.\nNow the question back, is this the best I can get on the said hardware?", "body": "Each TFRecord is a sparse structure of feature shape  1000 222215 viz. basically I am making batches of 1000 features. TFRecordWriter works faster as compared to burning a 1000 examples of (1x222215)  . My labels are on one-hot with 58565 labels and 1000 rows again to match 1000 features.\r\nUsing SparseFeature and parse_example is faster compared to using parse_single_example and/or VarLenFeature. But not significantly fast. (I didn't do decode_csv, it would have been worse anyway.)\r\nWith this structure, I am able to train 400000 examples trained in batches of 1000 (each TFRecord) and 5 epochs in 861-900 secs. Is this the best I can get. I am ONLY printing the tensors. No matmuls, no optimizers.\r\nI use 4 producer queues (string_input_producer), to read 8 TFRecord files (2 each) and shuffle_batch with 1-2 threads. If I use 4 threads, as is the case of shuffle_batch_join which creates as many number of threads to enqueue as the tensors in the list viz., 4 in my case, I get Timedout error waiting for notification. So I believe producers don't get enough time if I do that.\r\nNow the question back, is this the best I can get on the said hardware?"}