{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/332539174", "html_url": "https://github.com/tensorflow/tensorflow/issues/13187#issuecomment-332539174", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13187", "id": 332539174, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjUzOTE3NA==", "user": {"login": "balaji-in-git", "id": 31880073, "node_id": "MDQ6VXNlcjMxODgwMDcz", "avatar_url": "https://avatars0.githubusercontent.com/u/31880073?v=4", "gravatar_id": "", "url": "https://api.github.com/users/balaji-in-git", "html_url": "https://github.com/balaji-in-git", "followers_url": "https://api.github.com/users/balaji-in-git/followers", "following_url": "https://api.github.com/users/balaji-in-git/following{/other_user}", "gists_url": "https://api.github.com/users/balaji-in-git/gists{/gist_id}", "starred_url": "https://api.github.com/users/balaji-in-git/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/balaji-in-git/subscriptions", "organizations_url": "https://api.github.com/users/balaji-in-git/orgs", "repos_url": "https://api.github.com/users/balaji-in-git/repos", "events_url": "https://api.github.com/users/balaji-in-git/events{/privacy}", "received_events_url": "https://api.github.com/users/balaji-in-git/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-27T14:27:06Z", "updated_at": "2017-09-27T14:27:06Z", "author_association": "NONE", "body_html": "<p>I reduced the number of labels in the training examples from about 70000, which is the super set to about 1500 which is the number labels that's in the training set. Now the dry run, (WITHOUT matmuls, cost optimizers etc.) runs very fast. About 0.5 mn training examples pipelined and get enqueued/dequeued in about 17 secs and 5 epochs. That's about 1 million training examples in about 8-9 secs. I am not sure how &amp; why this correlates internally, the features are SparseFeature and label is FixedLenFeature.<br>\nNow the big question, if I have to train with all the 70000 labels, will the performance again take a beating.</p>", "body_text": "I reduced the number of labels in the training examples from about 70000, which is the super set to about 1500 which is the number labels that's in the training set. Now the dry run, (WITHOUT matmuls, cost optimizers etc.) runs very fast. About 0.5 mn training examples pipelined and get enqueued/dequeued in about 17 secs and 5 epochs. That's about 1 million training examples in about 8-9 secs. I am not sure how & why this correlates internally, the features are SparseFeature and label is FixedLenFeature.\nNow the big question, if I have to train with all the 70000 labels, will the performance again take a beating.", "body": "I reduced the number of labels in the training examples from about 70000, which is the super set to about 1500 which is the number labels that's in the training set. Now the dry run, (WITHOUT matmuls, cost optimizers etc.) runs very fast. About 0.5 mn training examples pipelined and get enqueued/dequeued in about 17 secs and 5 epochs. That's about 1 million training examples in about 8-9 secs. I am not sure how & why this correlates internally, the features are SparseFeature and label is FixedLenFeature.\r\nNow the big question, if I have to train with all the 70000 labels, will the performance again take a beating."}