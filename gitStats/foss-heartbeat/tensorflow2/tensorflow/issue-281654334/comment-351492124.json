{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/351492124", "html_url": "https://github.com/tensorflow/tensorflow/issues/15332#issuecomment-351492124", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15332", "id": 351492124, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTQ5MjEyNA==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-13T19:13:01Z", "updated_at": "2017-12-13T19:13:01Z", "author_association": "MEMBER", "body_html": "<p>Writing summaries in evaluate almost always does the wrong thing. The step for summaries is constant during evaluation, so you'll see only the output of the last batch (or commonly, you'd see a garbled mess of measurements all for a single step).</p>\n<p>While it is possible to do the right thing manually, we have seen that this rarely happens. Instead, you should use <code>tf.metrics.mean</code> (or some other aggregating function) to aggregate results in variables, which are summarized if you return them as part of the eval results.</p>\n<p>Maybe I have misunderstood your proposal, but if I understood it correctly, this is not something we'd like to do.</p>", "body_text": "Writing summaries in evaluate almost always does the wrong thing. The step for summaries is constant during evaluation, so you'll see only the output of the last batch (or commonly, you'd see a garbled mess of measurements all for a single step).\nWhile it is possible to do the right thing manually, we have seen that this rarely happens. Instead, you should use tf.metrics.mean (or some other aggregating function) to aggregate results in variables, which are summarized if you return them as part of the eval results.\nMaybe I have misunderstood your proposal, but if I understood it correctly, this is not something we'd like to do.", "body": "Writing summaries in evaluate almost always does the wrong thing. The step for summaries is constant during evaluation, so you'll see only the output of the last batch (or commonly, you'd see a garbled mess of measurements all for a single step).\r\n\r\nWhile it is possible to do the right thing manually, we have seen that this rarely happens. Instead, you should use `tf.metrics.mean` (or some other aggregating function) to aggregate results in variables, which are summarized if you return them as part of the eval results.\r\n\r\nMaybe I have misunderstood your proposal, but if I understood it correctly, this is not something we'd like to do."}