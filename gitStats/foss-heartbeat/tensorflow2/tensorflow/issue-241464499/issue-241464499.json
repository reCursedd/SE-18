{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11383", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11383/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11383/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11383/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11383", "id": 241464499, "node_id": "MDU6SXNzdWUyNDE0NjQ0OTk=", "number": 11383, "title": "Feeding TensorArray when running a session using feed_dict", "user": {"login": "mishajw", "id": 9575592, "node_id": "MDQ6VXNlcjk1NzU1OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/9575592?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mishajw", "html_url": "https://github.com/mishajw", "followers_url": "https://api.github.com/users/mishajw/followers", "following_url": "https://api.github.com/users/mishajw/following{/other_user}", "gists_url": "https://api.github.com/users/mishajw/gists{/gist_id}", "starred_url": "https://api.github.com/users/mishajw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mishajw/subscriptions", "organizations_url": "https://api.github.com/users/mishajw/orgs", "repos_url": "https://api.github.com/users/mishajw/repos", "events_url": "https://api.github.com/users/mishajw/events{/privacy}", "received_events_url": "https://api.github.com/users/mishajw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-07-08T16:37:35Z", "updated_at": "2017-08-10T16:18:50Z", "closed_at": "2017-07-14T06:11:09Z", "author_association": "NONE", "body_html": "<p>Currently there is no way to define a placeholder for a <code>tf.TensorArray</code>. A solution is to zero-pad a <code>tf.Tensor</code> and then unstack and slice it into a <code>tf.TensorArray</code>. However, this adds unpadding overhead which would have to be done for every batch.</p>\n<p>A possible change would be to add a new placeholder type for feeding a <code>tf.TensorArray</code> to the graph, for example called <code>tf.tensor_array_placeholder</code>.</p>\n<p>This could work as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Define an input TensorArray with three elements</span>\nta_input <span class=\"pl-k\">=</span> tf.tensor_array_placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Take the second element from the TensorArray</span>\nta_second <span class=\"pl-k\">=</span> ta.read(<span class=\"pl-c1\">1</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Sum its values</span>\nresult <span class=\"pl-k\">=</span> tf.reduce_sum(ta_second)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    run_result <span class=\"pl-k\">=</span> sess.run(\n        [result],\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Feed raw values into ta_input. These could also be NumPy arrays</span>\n        { ta_input: [[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>], [<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">9</span>]] })\n\n   <span class=\"pl-c1\">print</span>(run_result)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Should print 9</span></pre></div>", "body_text": "Currently there is no way to define a placeholder for a tf.TensorArray. A solution is to zero-pad a tf.Tensor and then unstack and slice it into a tf.TensorArray. However, this adds unpadding overhead which would have to be done for every batch.\nA possible change would be to add a new placeholder type for feeding a tf.TensorArray to the graph, for example called tf.tensor_array_placeholder.\nThis could work as follows:\n# Define an input TensorArray with three elements\nta_input = tf.tensor_array_placeholder(dtype=tf.float32, size=3)\n# Take the second element from the TensorArray\nta_second = ta.read(1)\n# Sum its values\nresult = tf.reduce_sum(ta_second)\n\nwith tf.Session() as sess:\n    run_result = sess.run(\n        [result],\n        # Feed raw values into ta_input. These could also be NumPy arrays\n        { ta_input: [[1, 2, 3], [4, 5], [6, 7, 8, 9]] })\n\n   print(run_result)  # Should print 9", "body": "Currently there is no way to define a placeholder for a `tf.TensorArray`. A solution is to zero-pad a `tf.Tensor` and then unstack and slice it into a `tf.TensorArray`. However, this adds unpadding overhead which would have to be done for every batch.\r\n\r\nA possible change would be to add a new placeholder type for feeding a `tf.TensorArray` to the graph, for example called `tf.tensor_array_placeholder`.\r\n\r\nThis could work as follows:\r\n```python\r\n# Define an input TensorArray with three elements\r\nta_input = tf.tensor_array_placeholder(dtype=tf.float32, size=3)\r\n# Take the second element from the TensorArray\r\nta_second = ta.read(1)\r\n# Sum its values\r\nresult = tf.reduce_sum(ta_second)\r\n\r\nwith tf.Session() as sess:\r\n    run_result = sess.run(\r\n        [result],\r\n        # Feed raw values into ta_input. These could also be NumPy arrays\r\n        { ta_input: [[1, 2, 3], [4, 5], [6, 7, 8, 9]] })\r\n\r\n   print(run_result)  # Should print 9\r\n```"}