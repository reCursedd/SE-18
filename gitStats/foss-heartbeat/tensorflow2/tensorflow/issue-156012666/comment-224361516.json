{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/224361516", "html_url": "https://github.com/tensorflow/tensorflow/issues/2441#issuecomment-224361516", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2441", "id": 224361516, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDM2MTUxNg==", "user": {"login": "cgel", "id": 11093686, "node_id": "MDQ6VXNlcjExMDkzNjg2", "avatar_url": "https://avatars0.githubusercontent.com/u/11093686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cgel", "html_url": "https://github.com/cgel", "followers_url": "https://api.github.com/users/cgel/followers", "following_url": "https://api.github.com/users/cgel/following{/other_user}", "gists_url": "https://api.github.com/users/cgel/gists{/gist_id}", "starred_url": "https://api.github.com/users/cgel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cgel/subscriptions", "organizations_url": "https://api.github.com/users/cgel/orgs", "repos_url": "https://api.github.com/users/cgel/repos", "events_url": "https://api.github.com/users/cgel/events{/privacy}", "received_events_url": "https://api.github.com/users/cgel/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-07T17:55:31Z", "updated_at": "2016-06-07T17:55:31Z", "author_association": "NONE", "body_html": "<p>That is not convincing. Of course the gradient of <code>tf.add_n()</code> runs as a single op, but the backpropagation should continue independently throughout the nodes in each separate GPU.<br>\nThis is evidenced by the fact that all GPUs run at the same level -- 30% or so.</p>", "body_text": "That is not convincing. Of course the gradient of tf.add_n() runs as a single op, but the backpropagation should continue independently throughout the nodes in each separate GPU.\nThis is evidenced by the fact that all GPUs run at the same level -- 30% or so.", "body": "That is not convincing. Of course the gradient of `tf.add_n()` runs as a single op, but the backpropagation should continue independently throughout the nodes in each separate GPU.\nThis is evidenced by the fact that all GPUs run at the same level -- 30% or so.\n"}