{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3159", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3159/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3159/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3159/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3159", "id": 163523399, "node_id": "MDU6SXNzdWUxNjM1MjMzOTk=", "number": 3159, "title": "How to remember the unit position of Dropout", "user": {"login": "333caowei", "id": 4569055, "node_id": "MDQ6VXNlcjQ1NjkwNTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4569055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/333caowei", "html_url": "https://github.com/333caowei", "followers_url": "https://api.github.com/users/333caowei/followers", "following_url": "https://api.github.com/users/333caowei/following{/other_user}", "gists_url": "https://api.github.com/users/333caowei/gists{/gist_id}", "starred_url": "https://api.github.com/users/333caowei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/333caowei/subscriptions", "organizations_url": "https://api.github.com/users/333caowei/orgs", "repos_url": "https://api.github.com/users/333caowei/repos", "events_url": "https://api.github.com/users/333caowei/events{/privacy}", "received_events_url": "https://api.github.com/users/333caowei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-07-02T15:38:55Z", "updated_at": "2016-07-06T16:21:16Z", "closed_at": "2016-07-06T16:21:16Z", "author_association": "NONE", "body_html": "<p>I am using dropout in my neural network:</p>\n<pre><code>W   = tf.get_variable(\"W\", [hidden_unit, 50]) \n\ndef RNN_L1(x,  initial_state,  real_length):\n                x = tf.transpose(x, [1, 0, 2]) \n                x = tf.reshape(x, [-1, word_dim]) \n\n                lstm_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\n\n                x = tf.split(0, sequence_length, x)\n                outputs, _ = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\n\n                return outputs\n\n\n\noutputs_L1  =  RNN_L1(x_vec,      self.initial_stateL1,     self.real_length)\noutputs_L1  =  tf.pack(outputs_L1)\n\n\ntensor_shape = outputs_L1.get_shape()\n\nfor step_index in range(tensor_shape[0]):\n                output_relu= tf.matmul(outputs_L1[step_index,  :,  :], W) + B\n                output_relu= tf.nn.relu(output_relu)\n                output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)\n</code></pre>\n<p><strong>outputs_L1</strong> is the output of LSTM which has 3D-Tensor[sentence length, batchsize, hidden unit]. So, you can see the \"for loop\" code, I use \"for loop\" to do dropout for each <strong>outputs_L1[step_index,  :,  :]</strong>. But in this way, each outputs_L1[step_index,  :,  :] has it's own position of dropout.</p>\n<p>For example:<br>\n<strong>outputs_L1[1,  :,  :]</strong> and <strong>outputs_L1[2,  :,  :]</strong> have different position of dropout at<br>\n<code>output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)</code>.</p>\n<p>What I want is that they have same dropout position in \"for loop\". Can you help me?</p>", "body_text": "I am using dropout in my neural network:\nW   = tf.get_variable(\"W\", [hidden_unit, 50]) \n\ndef RNN_L1(x,  initial_state,  real_length):\n                x = tf.transpose(x, [1, 0, 2]) \n                x = tf.reshape(x, [-1, word_dim]) \n\n                lstm_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\n\n                x = tf.split(0, sequence_length, x)\n                outputs, _ = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\n\n                return outputs\n\n\n\noutputs_L1  =  RNN_L1(x_vec,      self.initial_stateL1,     self.real_length)\noutputs_L1  =  tf.pack(outputs_L1)\n\n\ntensor_shape = outputs_L1.get_shape()\n\nfor step_index in range(tensor_shape[0]):\n                output_relu= tf.matmul(outputs_L1[step_index,  :,  :], W) + B\n                output_relu= tf.nn.relu(output_relu)\n                output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)\n\noutputs_L1 is the output of LSTM which has 3D-Tensor[sentence length, batchsize, hidden unit]. So, you can see the \"for loop\" code, I use \"for loop\" to do dropout for each outputs_L1[step_index,  :,  :]. But in this way, each outputs_L1[step_index,  :,  :] has it's own position of dropout.\nFor example:\noutputs_L1[1,  :,  :] and outputs_L1[2,  :,  :] have different position of dropout at\noutput_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob).\nWhat I want is that they have same dropout position in \"for loop\". Can you help me?", "body": "I am using dropout in my neural network:\n\n```\nW   = tf.get_variable(\"W\", [hidden_unit, 50]) \n\ndef RNN_L1(x,  initial_state,  real_length):\n                x = tf.transpose(x, [1, 0, 2]) \n                x = tf.reshape(x, [-1, word_dim]) \n\n                lstm_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\n\n                x = tf.split(0, sequence_length, x)\n                outputs, _ = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\n\n                return outputs\n\n\n\noutputs_L1  =  RNN_L1(x_vec,      self.initial_stateL1,     self.real_length)\noutputs_L1  =  tf.pack(outputs_L1)\n\n\ntensor_shape = outputs_L1.get_shape()\n\nfor step_index in range(tensor_shape[0]):\n                output_relu= tf.matmul(outputs_L1[step_index,  :,  :], W) + B\n                output_relu= tf.nn.relu(output_relu)\n                output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)\n```\n\n**outputs_L1** is the output of LSTM which has 3D-Tensor[sentence length, batchsize, hidden unit]. So, you can see the \"for loop\" code, I use \"for loop\" to do dropout for each **outputs_L1[step_index,  :,  :]**. But in this way, each outputs_L1[step_index,  :,  :] has it's own position of dropout. \n\nFor example:\n**outputs_L1[1,  :,  :]** and **outputs_L1[2,  :,  :]** have different position of dropout at \n`output_relu = tf.nn.dropout(output_relu, self.dropout_keep_prob)`.\n\nWhat I want is that they have same dropout position in \"for loop\". Can you help me?\n"}