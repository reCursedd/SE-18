{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5383", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5383/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5383/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5383/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5383", "id": 187126718, "node_id": "MDU6SXNzdWUxODcxMjY3MTg=", "number": 5383, "title": "GraphDef cannot be larger than 2GB using input pipelines", "user": {"login": "ischlag", "id": 7107156, "node_id": "MDQ6VXNlcjcxMDcxNTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/7107156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ischlag", "html_url": "https://github.com/ischlag", "followers_url": "https://api.github.com/users/ischlag/followers", "following_url": "https://api.github.com/users/ischlag/following{/other_user}", "gists_url": "https://api.github.com/users/ischlag/gists{/gist_id}", "starred_url": "https://api.github.com/users/ischlag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ischlag/subscriptions", "organizations_url": "https://api.github.com/users/ischlag/orgs", "repos_url": "https://api.github.com/users/ischlag/repos", "events_url": "https://api.github.com/users/ischlag/events{/privacy}", "received_events_url": "https://api.github.com/users/ischlag/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-11-03T16:48:33Z", "updated_at": "2016-11-03T18:30:34Z", "closed_at": "2016-11-03T18:30:21Z", "author_association": "NONE", "body_html": "<p><em>ValueError: GraphDef cannot be larger than 2GB.</em></p>\n<p>I'm not loading my data into a constant. I'm using an input pipeline to prefetch small batches. My graph works with cifar10 but if I use my SVHN input pipeline I get the error posted above. Both input pipeline scripts are virtually the same. How can it be that my data becomes part of the graph?</p>\n<h3>Environment info</h3>\n<p>CUDA 8.0.27<br>\nccuDNN 5.1.5<br>\nTensorflow 0.11 (manually compiled)<br>\nbazel 0.3.2<br>\nNvidia 1080 GTX</p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>cifar10 input pipeline works, but svhn gives 2GB error</p>\n<p>A working but slim version of the main script</p>\n<pre><code>import tensorflow as tf\n\nfrom libs.components import conv2d, batch_norm, flatten, dense\nfrom datasets.cifar10 import cifar10_data\nfrom datasets.svhn import svhn_data\n\n## ----------------------------------------------------------------------------\n## CONFIGURATION\nBATCH_SIZE = 128\nLOGS_PATH = \"/home/chuck/MyStuff/cnn_test/logs/InitialTests/4/\"\nEPOCHS = 300  # max number of epochs if the network never converges\nlearning_rate = 0.01\n\n## ----------------------------------------------------------------------------\n## DATA INPUT\n#data = cifar10_data(batch_size=BATCH_SIZE)\ndata = svhn_data(batch_size=BATCH_SIZE)\n\nwith tf.device('/cpu:0'):\n  train_image_batch, train_label_batch = data.build_train_data_tensor(shuffle=True)\n  test_image_batch, test_label_batch = data.build_test_data_tensor(shuffle=False)\n\nNUMBER_OF_CLASSES = data.NUMBER_OF_CLASSES\nIMG_SIZE = data.IMAGE_SIZE\nNUM_CHANNELS = data.NUM_OF_CHANNELS\nTRAIN_SET_SIZE = data.TRAIN_SET_SIZE\nTEST_SET_SIZE = data.TEST_SET_SIZE\nTRAIN_BATCHES_PER_EPOCH = int(TRAIN_SET_SIZE / BATCH_SIZE)  # only used for training\n\n## ----------------------------------------------------------------------------\n## MODEL STRUCTURE\nis_training = tf.placeholder(tf.bool, name='is_training')\n\n\ndef conv_block(data, n_filter, scope, stride=1):\n  \"\"\"An ConvBlock is a repetitive composition used in the model.\"\"\"\n  with tf.variable_scope(scope):\n    conv = conv2d(data, n_filter, \"conv\",\n                  k_h=3, k_w=3,\n                  stride_h=stride, stride_w=stride,\n                  initializer=tf.random_normal_initializer(stddev=0.01),\n                  bias=True,\n                  padding='SAME')\n    norm = batch_norm(conv, n_filter, is_training, scope=\"bn\")\n    relu = tf.nn.relu(norm)\n\n    conv2 = conv2d(relu, n_filter, \"conv2\",\n                   k_h=3, k_w=3,\n                   stride_h=stride, stride_w=stride,\n                   initializer=tf.random_normal_initializer(stddev=0.01),\n                   bias=True,\n                   padding='SAME')\n    norm2 = batch_norm(conv2, n_filter, is_training, scope=\"bn2\")\n    relu2 = tf.nn.relu(norm2)\n\n    conv3 = conv2d(relu2, n_filter, \"conv3\",\n                   k_h=3, k_w=3,\n                   stride_h=stride, stride_w=stride,\n                   initializer=tf.random_normal_initializer(stddev=0.01),\n                   bias=True,\n                   padding='SAME')\n    norm3 = batch_norm(conv3, n_filter, is_training, scope=\"bn3\")\n    relu3 = tf.nn.relu(norm3)\n\n    return relu3\n\n\ndef model(x):\n  \"\"\"Defines the CNN architecture and returns its output tensor.\"\"\"\n  block1 = conv_block(x, 64, \"block1\")\n  pool1 = tf.nn.max_pool(block1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n\n  block2 = conv_block(pool1, 128, \"block2\")\n  pool2 = tf.nn.max_pool(block2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n\n  block3 = conv_block(pool2, 256, \"block3\")\n  pool3 = tf.nn.max_pool(block3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n\n  flat = flatten(pool3)\n\n  dense1 = dense(flat, 4096, is_training, tf.nn.relu, scope=\"dense1\", dropout=True,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  dense2 = dense(dense1, 4096, is_training, tf.nn.relu, scope=\"dense2\", dropout=True,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  dense3 = dense(dense2, 4096, is_training, tf.nn.relu, scope=\"dense3\", dropout=True,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  dense6 = dense(dense3, NUMBER_OF_CLASSES, is_training, tf.nn.relu, scope=\"dense6\", dropout=False,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  return dense6\n\n\n## ----------------------------------------------------------------------------\n## LOSS AND ACCURACY\nwith tf.variable_scope(\"model\"):\n  batch_size = tf.placeholder(tf.float32, name=\"batch_size\")\n\n  input_image_batch = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_SIZE, IMG_SIZE, NUM_CHANNELS],\n                                     name=\"input_image_batch\")\n  input_label_batch = tf.placeholder(tf.float32, shape=[None, NUMBER_OF_CLASSES], name=\"input_label_batch\")\n\n  logits = model(input_image_batch)\n\nwith tf.variable_scope(\"loss\"):\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n    logits,\n    tf.cast(input_label_batch, tf.float32),\n    name=\"cross-entropy\")\n  loss = tf.reduce_mean(cross_entropy, name='loss')\n\nwith tf.variable_scope(\"accuracy\"):\n  top_1_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 1)\n  top_n_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 3)\n\npredictions = tf.argmax(logits, 1)\nlabel_batch_id = tf.argmax(input_label_batch, 1)\n\n## ----------------------------------------------------------------------------\n## OPTIMIZER\nglobal_step = tf.get_variable('global_step', [],\n                              initializer=tf.constant_initializer(0),\n                              trainable=False)\nlr = tf.placeholder(tf.float32, name=\"learning_rate\")\ntrain_op = tf.train.MomentumOptimizer(lr, 0.9).minimize(loss, global_step=global_step)\n\n## ----------------------------------------------------------------------------\n## SUMMARIES\nsummary_op = tf.merge_all_summaries()\n\n## ----------------------------------------------------------------------------\n## INITIALIZATION\ninit_op = tf.initialize_all_variables()\nwriter = tf.train.SummaryWriter(LOGS_PATH, graph=tf.get_default_graph())\nsaver = tf.train.Saver()\nsess = tf.Session()\n\n# initialize queue threads\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n# initialize variables\nsess.run(init_op)\n\n## ----------------------------------------------------------------------------\n## HELPER FUNCTIONS\ndef next_feed_dic(image_batch, label_batch, train=True):\n  \"\"\"Fetches a mini-batch of images and labels and builds a feed-dictonary\"\"\"\n  with tf.device('/cpu:0'):\n    curr_image_batch, curr_label_batch = sess.run([image_batch, label_batch])\n\n    feed_dict = {\n      input_image_batch: curr_image_batch,\n      input_label_batch: curr_label_batch,\n      batch_size: curr_image_batch.shape[0],\n      is_training.name: train,\n      lr.name: learning_rate\n    }\n    return feed_dict\n\n## ----------------------------------------------------------------------------\n## PERFORM TRAINING\n\n# train cycles\nfor j in range(EPOCHS):\n  print(\"epoch \", j)\n  print(\"epoch.batches curr_loss (avg_loss)\")\n  for i in range(TRAIN_BATCHES_PER_EPOCH):\n    feed_dict = next_feed_dic(train_image_batch, train_label_batch, train=True)\n    _, curr_loss, summary, step = sess.run([train_op, loss, summary_op, global_step], feed_dict=feed_dict)\n\n    if i % 30 == 0:\n      print(\"{:3d}.{:03d} {:.5f}\".format(j, i, curr_loss))\n\nprint(\"done\")\n</code></pre>\n<p>The cifar10 input pipeline (this one works fine)</p>\n<pre><code>import tensorflow as tf\n\nfrom utils import cifar10\nfrom tensorflow.python.framework import ops\n\nclass cifar10_data:\n  \"\"\"\n  Downloads the CIFAR10 dataset and creates an input pipeline ready to be fed into a model.\n\n  - Reshapes flat images into 32x32\n  - converts [0 1] to [-1 1]\n  - shuffles the input\n  - builds batches\n  \"\"\"\n  NUM_THREADS = 8\n  NUMBER_OF_CLASSES = 10\n\n  TRAIN_SET_SIZE = 50000\n  TEST_SET_SIZE =  10000\n  IMAGE_SIZE = 32\n  NUM_OF_CHANNELS = 3\n\n  def __init__(self, batch_size):\n    \"\"\" Downloads the cifar10 data if necessary. \"\"\"\n    self.batch_size = batch_size\n    cifar10.maybe_download_and_extract()\n\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = cifar10.load_training_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = cifar10.load_test_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\n\n    images = ops.convert_to_tensor(raw_images)\n    targets = ops.convert_to_tensor(raw_targets)\n\n    set_size, width, height, channels = raw_images.shape\n\n    images = tf.reshape(images, [set_size, width, height, channels])\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\n\n    # Data Augmentation\n    if augmentation:\n      # TODO\n      # make sure after further preprocessing it is [0 1]\n      pass\n\n    # convert the given [0, 1] to [-1, 1]\n    image = tf.sub(image, 0.5)\n    image = tf.mul(image, 2.0)\n\n    images_batch, labels_batch = tf.train.batch([image, label], batch_size=self.batch_size, num_threads=self.NUM_THREADS)\n\n    return images_batch, labels_batch\n</code></pre>\n<p>The not working SVHN input pipeline (virtually the same input pipeline script)</p>\n<pre><code>import tensorflow as tf\n\nfrom utils import svhn\nfrom tensorflow.python.framework import ops\n\nclass svhn_data:\n  \"\"\"\n  Downloads the SVHN dataset and creates an input pipeline ready to be fed into a model.\n\n  - Reshapes flat images into 32x32\n  - converts [0 1] to [-1 1]\n  - shuffles the input\n  - builds batches\n  \"\"\"\n  NUM_THREADS = 8\n  NUMBER_OF_CLASSES = 100\n\n  TRAIN_SET_SIZE = 73257\n  TEST_SET_SIZE =  26032\n  IMAGE_SIZE = 32\n  NUM_OF_CHANNELS = 3\n\n  def __init__(self, batch_size):\n    \"\"\" Downloads the cifar100 data if necessary. \"\"\"\n    self.batch_size = batch_size\n    svhn.download_data()\n\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = svhn.load_training_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = svhn.load_test_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\n\n    images = ops.convert_to_tensor(raw_images)\n    targets = ops.convert_to_tensor(raw_targets)\n\n    set_size, width, height, channels = raw_images.shape\n\n    images = tf.reshape(images, [set_size, width, height, channels])\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\n\n    # Data Augmentation\n    if augmentation:\n      # TODO\n      # make sure after further preprocessing it is [0 1]\n      pass\n\n    # convert the given [0, 1] to [-1, 1]\n    image = tf.sub(image, 0.5)\n    image = tf.mul(image, 2.0)\n\n    images_batch, labels_batch = tf.train.batch([image, label],\n                                                batch_size=self.batch_size,\n                                                num_threads=self.NUM_THREADS)\n\n    return images_batch, labels_batch\n</code></pre>", "body_text": "ValueError: GraphDef cannot be larger than 2GB.\nI'm not loading my data into a constant. I'm using an input pipeline to prefetch small batches. My graph works with cifar10 but if I use my SVHN input pipeline I get the error posted above. Both input pipeline scripts are virtually the same. How can it be that my data becomes part of the graph?\nEnvironment info\nCUDA 8.0.27\nccuDNN 5.1.5\nTensorflow 0.11 (manually compiled)\nbazel 0.3.2\nNvidia 1080 GTX\nWhat other attempted solutions have you tried?\ncifar10 input pipeline works, but svhn gives 2GB error\nA working but slim version of the main script\nimport tensorflow as tf\n\nfrom libs.components import conv2d, batch_norm, flatten, dense\nfrom datasets.cifar10 import cifar10_data\nfrom datasets.svhn import svhn_data\n\n## ----------------------------------------------------------------------------\n## CONFIGURATION\nBATCH_SIZE = 128\nLOGS_PATH = \"/home/chuck/MyStuff/cnn_test/logs/InitialTests/4/\"\nEPOCHS = 300  # max number of epochs if the network never converges\nlearning_rate = 0.01\n\n## ----------------------------------------------------------------------------\n## DATA INPUT\n#data = cifar10_data(batch_size=BATCH_SIZE)\ndata = svhn_data(batch_size=BATCH_SIZE)\n\nwith tf.device('/cpu:0'):\n  train_image_batch, train_label_batch = data.build_train_data_tensor(shuffle=True)\n  test_image_batch, test_label_batch = data.build_test_data_tensor(shuffle=False)\n\nNUMBER_OF_CLASSES = data.NUMBER_OF_CLASSES\nIMG_SIZE = data.IMAGE_SIZE\nNUM_CHANNELS = data.NUM_OF_CHANNELS\nTRAIN_SET_SIZE = data.TRAIN_SET_SIZE\nTEST_SET_SIZE = data.TEST_SET_SIZE\nTRAIN_BATCHES_PER_EPOCH = int(TRAIN_SET_SIZE / BATCH_SIZE)  # only used for training\n\n## ----------------------------------------------------------------------------\n## MODEL STRUCTURE\nis_training = tf.placeholder(tf.bool, name='is_training')\n\n\ndef conv_block(data, n_filter, scope, stride=1):\n  \"\"\"An ConvBlock is a repetitive composition used in the model.\"\"\"\n  with tf.variable_scope(scope):\n    conv = conv2d(data, n_filter, \"conv\",\n                  k_h=3, k_w=3,\n                  stride_h=stride, stride_w=stride,\n                  initializer=tf.random_normal_initializer(stddev=0.01),\n                  bias=True,\n                  padding='SAME')\n    norm = batch_norm(conv, n_filter, is_training, scope=\"bn\")\n    relu = tf.nn.relu(norm)\n\n    conv2 = conv2d(relu, n_filter, \"conv2\",\n                   k_h=3, k_w=3,\n                   stride_h=stride, stride_w=stride,\n                   initializer=tf.random_normal_initializer(stddev=0.01),\n                   bias=True,\n                   padding='SAME')\n    norm2 = batch_norm(conv2, n_filter, is_training, scope=\"bn2\")\n    relu2 = tf.nn.relu(norm2)\n\n    conv3 = conv2d(relu2, n_filter, \"conv3\",\n                   k_h=3, k_w=3,\n                   stride_h=stride, stride_w=stride,\n                   initializer=tf.random_normal_initializer(stddev=0.01),\n                   bias=True,\n                   padding='SAME')\n    norm3 = batch_norm(conv3, n_filter, is_training, scope=\"bn3\")\n    relu3 = tf.nn.relu(norm3)\n\n    return relu3\n\n\ndef model(x):\n  \"\"\"Defines the CNN architecture and returns its output tensor.\"\"\"\n  block1 = conv_block(x, 64, \"block1\")\n  pool1 = tf.nn.max_pool(block1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n\n  block2 = conv_block(pool1, 128, \"block2\")\n  pool2 = tf.nn.max_pool(block2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n\n  block3 = conv_block(pool2, 256, \"block3\")\n  pool3 = tf.nn.max_pool(block3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n\n  flat = flatten(pool3)\n\n  dense1 = dense(flat, 4096, is_training, tf.nn.relu, scope=\"dense1\", dropout=True,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  dense2 = dense(dense1, 4096, is_training, tf.nn.relu, scope=\"dense2\", dropout=True,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  dense3 = dense(dense2, 4096, is_training, tf.nn.relu, scope=\"dense3\", dropout=True,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  dense6 = dense(dense3, NUMBER_OF_CLASSES, is_training, tf.nn.relu, scope=\"dense6\", dropout=False,\n                 initializer=tf.random_normal_initializer(stddev=0.01))\n  return dense6\n\n\n## ----------------------------------------------------------------------------\n## LOSS AND ACCURACY\nwith tf.variable_scope(\"model\"):\n  batch_size = tf.placeholder(tf.float32, name=\"batch_size\")\n\n  input_image_batch = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_SIZE, IMG_SIZE, NUM_CHANNELS],\n                                     name=\"input_image_batch\")\n  input_label_batch = tf.placeholder(tf.float32, shape=[None, NUMBER_OF_CLASSES], name=\"input_label_batch\")\n\n  logits = model(input_image_batch)\n\nwith tf.variable_scope(\"loss\"):\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n    logits,\n    tf.cast(input_label_batch, tf.float32),\n    name=\"cross-entropy\")\n  loss = tf.reduce_mean(cross_entropy, name='loss')\n\nwith tf.variable_scope(\"accuracy\"):\n  top_1_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 1)\n  top_n_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 3)\n\npredictions = tf.argmax(logits, 1)\nlabel_batch_id = tf.argmax(input_label_batch, 1)\n\n## ----------------------------------------------------------------------------\n## OPTIMIZER\nglobal_step = tf.get_variable('global_step', [],\n                              initializer=tf.constant_initializer(0),\n                              trainable=False)\nlr = tf.placeholder(tf.float32, name=\"learning_rate\")\ntrain_op = tf.train.MomentumOptimizer(lr, 0.9).minimize(loss, global_step=global_step)\n\n## ----------------------------------------------------------------------------\n## SUMMARIES\nsummary_op = tf.merge_all_summaries()\n\n## ----------------------------------------------------------------------------\n## INITIALIZATION\ninit_op = tf.initialize_all_variables()\nwriter = tf.train.SummaryWriter(LOGS_PATH, graph=tf.get_default_graph())\nsaver = tf.train.Saver()\nsess = tf.Session()\n\n# initialize queue threads\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n# initialize variables\nsess.run(init_op)\n\n## ----------------------------------------------------------------------------\n## HELPER FUNCTIONS\ndef next_feed_dic(image_batch, label_batch, train=True):\n  \"\"\"Fetches a mini-batch of images and labels and builds a feed-dictonary\"\"\"\n  with tf.device('/cpu:0'):\n    curr_image_batch, curr_label_batch = sess.run([image_batch, label_batch])\n\n    feed_dict = {\n      input_image_batch: curr_image_batch,\n      input_label_batch: curr_label_batch,\n      batch_size: curr_image_batch.shape[0],\n      is_training.name: train,\n      lr.name: learning_rate\n    }\n    return feed_dict\n\n## ----------------------------------------------------------------------------\n## PERFORM TRAINING\n\n# train cycles\nfor j in range(EPOCHS):\n  print(\"epoch \", j)\n  print(\"epoch.batches curr_loss (avg_loss)\")\n  for i in range(TRAIN_BATCHES_PER_EPOCH):\n    feed_dict = next_feed_dic(train_image_batch, train_label_batch, train=True)\n    _, curr_loss, summary, step = sess.run([train_op, loss, summary_op, global_step], feed_dict=feed_dict)\n\n    if i % 30 == 0:\n      print(\"{:3d}.{:03d} {:.5f}\".format(j, i, curr_loss))\n\nprint(\"done\")\n\nThe cifar10 input pipeline (this one works fine)\nimport tensorflow as tf\n\nfrom utils import cifar10\nfrom tensorflow.python.framework import ops\n\nclass cifar10_data:\n  \"\"\"\n  Downloads the CIFAR10 dataset and creates an input pipeline ready to be fed into a model.\n\n  - Reshapes flat images into 32x32\n  - converts [0 1] to [-1 1]\n  - shuffles the input\n  - builds batches\n  \"\"\"\n  NUM_THREADS = 8\n  NUMBER_OF_CLASSES = 10\n\n  TRAIN_SET_SIZE = 50000\n  TEST_SET_SIZE =  10000\n  IMAGE_SIZE = 32\n  NUM_OF_CHANNELS = 3\n\n  def __init__(self, batch_size):\n    \"\"\" Downloads the cifar10 data if necessary. \"\"\"\n    self.batch_size = batch_size\n    cifar10.maybe_download_and_extract()\n\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = cifar10.load_training_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = cifar10.load_test_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\n\n    images = ops.convert_to_tensor(raw_images)\n    targets = ops.convert_to_tensor(raw_targets)\n\n    set_size, width, height, channels = raw_images.shape\n\n    images = tf.reshape(images, [set_size, width, height, channels])\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\n\n    # Data Augmentation\n    if augmentation:\n      # TODO\n      # make sure after further preprocessing it is [0 1]\n      pass\n\n    # convert the given [0, 1] to [-1, 1]\n    image = tf.sub(image, 0.5)\n    image = tf.mul(image, 2.0)\n\n    images_batch, labels_batch = tf.train.batch([image, label], batch_size=self.batch_size, num_threads=self.NUM_THREADS)\n\n    return images_batch, labels_batch\n\nThe not working SVHN input pipeline (virtually the same input pipeline script)\nimport tensorflow as tf\n\nfrom utils import svhn\nfrom tensorflow.python.framework import ops\n\nclass svhn_data:\n  \"\"\"\n  Downloads the SVHN dataset and creates an input pipeline ready to be fed into a model.\n\n  - Reshapes flat images into 32x32\n  - converts [0 1] to [-1 1]\n  - shuffles the input\n  - builds batches\n  \"\"\"\n  NUM_THREADS = 8\n  NUMBER_OF_CLASSES = 100\n\n  TRAIN_SET_SIZE = 73257\n  TEST_SET_SIZE =  26032\n  IMAGE_SIZE = 32\n  NUM_OF_CHANNELS = 3\n\n  def __init__(self, batch_size):\n    \"\"\" Downloads the cifar100 data if necessary. \"\"\"\n    self.batch_size = batch_size\n    svhn.download_data()\n\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = svhn.load_training_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\n    images, _, targets = svhn.load_test_data()\n    return self.__build_generic_data_tensor(images,\n                                            targets,\n                                            shuffle,\n                                            augmentation)\n\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\n\n    images = ops.convert_to_tensor(raw_images)\n    targets = ops.convert_to_tensor(raw_targets)\n\n    set_size, width, height, channels = raw_images.shape\n\n    images = tf.reshape(images, [set_size, width, height, channels])\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\n\n    # Data Augmentation\n    if augmentation:\n      # TODO\n      # make sure after further preprocessing it is [0 1]\n      pass\n\n    # convert the given [0, 1] to [-1, 1]\n    image = tf.sub(image, 0.5)\n    image = tf.mul(image, 2.0)\n\n    images_batch, labels_batch = tf.train.batch([image, label],\n                                                batch_size=self.batch_size,\n                                                num_threads=self.NUM_THREADS)\n\n    return images_batch, labels_batch", "body": "_ValueError: GraphDef cannot be larger than 2GB._\r\n\r\nI'm not loading my data into a constant. I'm using an input pipeline to prefetch small batches. My graph works with cifar10 but if I use my SVHN input pipeline I get the error posted above. Both input pipeline scripts are virtually the same. How can it be that my data becomes part of the graph?\r\n\r\n### Environment info\r\nCUDA 8.0.27\r\nccuDNN 5.1.5\r\nTensorflow 0.11 (manually compiled)\r\nbazel 0.3.2\r\nNvidia 1080 GTX\r\n\r\n### What other attempted solutions have you tried?\r\ncifar10 input pipeline works, but svhn gives 2GB error\r\n\r\nA working but slim version of the main script\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom libs.components import conv2d, batch_norm, flatten, dense\r\nfrom datasets.cifar10 import cifar10_data\r\nfrom datasets.svhn import svhn_data\r\n\r\n## ----------------------------------------------------------------------------\r\n## CONFIGURATION\r\nBATCH_SIZE = 128\r\nLOGS_PATH = \"/home/chuck/MyStuff/cnn_test/logs/InitialTests/4/\"\r\nEPOCHS = 300  # max number of epochs if the network never converges\r\nlearning_rate = 0.01\r\n\r\n## ----------------------------------------------------------------------------\r\n## DATA INPUT\r\n#data = cifar10_data(batch_size=BATCH_SIZE)\r\ndata = svhn_data(batch_size=BATCH_SIZE)\r\n\r\nwith tf.device('/cpu:0'):\r\n  train_image_batch, train_label_batch = data.build_train_data_tensor(shuffle=True)\r\n  test_image_batch, test_label_batch = data.build_test_data_tensor(shuffle=False)\r\n\r\nNUMBER_OF_CLASSES = data.NUMBER_OF_CLASSES\r\nIMG_SIZE = data.IMAGE_SIZE\r\nNUM_CHANNELS = data.NUM_OF_CHANNELS\r\nTRAIN_SET_SIZE = data.TRAIN_SET_SIZE\r\nTEST_SET_SIZE = data.TEST_SET_SIZE\r\nTRAIN_BATCHES_PER_EPOCH = int(TRAIN_SET_SIZE / BATCH_SIZE)  # only used for training\r\n\r\n## ----------------------------------------------------------------------------\r\n## MODEL STRUCTURE\r\nis_training = tf.placeholder(tf.bool, name='is_training')\r\n\r\n\r\ndef conv_block(data, n_filter, scope, stride=1):\r\n  \"\"\"An ConvBlock is a repetitive composition used in the model.\"\"\"\r\n  with tf.variable_scope(scope):\r\n    conv = conv2d(data, n_filter, \"conv\",\r\n                  k_h=3, k_w=3,\r\n                  stride_h=stride, stride_w=stride,\r\n                  initializer=tf.random_normal_initializer(stddev=0.01),\r\n                  bias=True,\r\n                  padding='SAME')\r\n    norm = batch_norm(conv, n_filter, is_training, scope=\"bn\")\r\n    relu = tf.nn.relu(norm)\r\n\r\n    conv2 = conv2d(relu, n_filter, \"conv2\",\r\n                   k_h=3, k_w=3,\r\n                   stride_h=stride, stride_w=stride,\r\n                   initializer=tf.random_normal_initializer(stddev=0.01),\r\n                   bias=True,\r\n                   padding='SAME')\r\n    norm2 = batch_norm(conv2, n_filter, is_training, scope=\"bn2\")\r\n    relu2 = tf.nn.relu(norm2)\r\n\r\n    conv3 = conv2d(relu2, n_filter, \"conv3\",\r\n                   k_h=3, k_w=3,\r\n                   stride_h=stride, stride_w=stride,\r\n                   initializer=tf.random_normal_initializer(stddev=0.01),\r\n                   bias=True,\r\n                   padding='SAME')\r\n    norm3 = batch_norm(conv3, n_filter, is_training, scope=\"bn3\")\r\n    relu3 = tf.nn.relu(norm3)\r\n\r\n    return relu3\r\n\r\n\r\ndef model(x):\r\n  \"\"\"Defines the CNN architecture and returns its output tensor.\"\"\"\r\n  block1 = conv_block(x, 64, \"block1\")\r\n  pool1 = tf.nn.max_pool(block1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\r\n\r\n  block2 = conv_block(pool1, 128, \"block2\")\r\n  pool2 = tf.nn.max_pool(block2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\r\n\r\n  block3 = conv_block(pool2, 256, \"block3\")\r\n  pool3 = tf.nn.max_pool(block3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\r\n\r\n  flat = flatten(pool3)\r\n\r\n  dense1 = dense(flat, 4096, is_training, tf.nn.relu, scope=\"dense1\", dropout=True,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  dense2 = dense(dense1, 4096, is_training, tf.nn.relu, scope=\"dense2\", dropout=True,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  dense3 = dense(dense2, 4096, is_training, tf.nn.relu, scope=\"dense3\", dropout=True,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  dense6 = dense(dense3, NUMBER_OF_CLASSES, is_training, tf.nn.relu, scope=\"dense6\", dropout=False,\r\n                 initializer=tf.random_normal_initializer(stddev=0.01))\r\n  return dense6\r\n\r\n\r\n## ----------------------------------------------------------------------------\r\n## LOSS AND ACCURACY\r\nwith tf.variable_scope(\"model\"):\r\n  batch_size = tf.placeholder(tf.float32, name=\"batch_size\")\r\n\r\n  input_image_batch = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_SIZE, IMG_SIZE, NUM_CHANNELS],\r\n                                     name=\"input_image_batch\")\r\n  input_label_batch = tf.placeholder(tf.float32, shape=[None, NUMBER_OF_CLASSES], name=\"input_label_batch\")\r\n\r\n  logits = model(input_image_batch)\r\n\r\nwith tf.variable_scope(\"loss\"):\r\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n    logits,\r\n    tf.cast(input_label_batch, tf.float32),\r\n    name=\"cross-entropy\")\r\n  loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\nwith tf.variable_scope(\"accuracy\"):\r\n  top_1_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 1)\r\n  top_n_correct = tf.nn.in_top_k(logits, tf.argmax(input_label_batch, 1), 3)\r\n\r\npredictions = tf.argmax(logits, 1)\r\nlabel_batch_id = tf.argmax(input_label_batch, 1)\r\n\r\n## ----------------------------------------------------------------------------\r\n## OPTIMIZER\r\nglobal_step = tf.get_variable('global_step', [],\r\n                              initializer=tf.constant_initializer(0),\r\n                              trainable=False)\r\nlr = tf.placeholder(tf.float32, name=\"learning_rate\")\r\ntrain_op = tf.train.MomentumOptimizer(lr, 0.9).minimize(loss, global_step=global_step)\r\n\r\n## ----------------------------------------------------------------------------\r\n## SUMMARIES\r\nsummary_op = tf.merge_all_summaries()\r\n\r\n## ----------------------------------------------------------------------------\r\n## INITIALIZATION\r\ninit_op = tf.initialize_all_variables()\r\nwriter = tf.train.SummaryWriter(LOGS_PATH, graph=tf.get_default_graph())\r\nsaver = tf.train.Saver()\r\nsess = tf.Session()\r\n\r\n# initialize queue threads\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n# initialize variables\r\nsess.run(init_op)\r\n\r\n## ----------------------------------------------------------------------------\r\n## HELPER FUNCTIONS\r\ndef next_feed_dic(image_batch, label_batch, train=True):\r\n  \"\"\"Fetches a mini-batch of images and labels and builds a feed-dictonary\"\"\"\r\n  with tf.device('/cpu:0'):\r\n    curr_image_batch, curr_label_batch = sess.run([image_batch, label_batch])\r\n\r\n    feed_dict = {\r\n      input_image_batch: curr_image_batch,\r\n      input_label_batch: curr_label_batch,\r\n      batch_size: curr_image_batch.shape[0],\r\n      is_training.name: train,\r\n      lr.name: learning_rate\r\n    }\r\n    return feed_dict\r\n\r\n## ----------------------------------------------------------------------------\r\n## PERFORM TRAINING\r\n\r\n# train cycles\r\nfor j in range(EPOCHS):\r\n  print(\"epoch \", j)\r\n  print(\"epoch.batches curr_loss (avg_loss)\")\r\n  for i in range(TRAIN_BATCHES_PER_EPOCH):\r\n    feed_dict = next_feed_dic(train_image_batch, train_label_batch, train=True)\r\n    _, curr_loss, summary, step = sess.run([train_op, loss, summary_op, global_step], feed_dict=feed_dict)\r\n\r\n    if i % 30 == 0:\r\n      print(\"{:3d}.{:03d} {:.5f}\".format(j, i, curr_loss))\r\n\r\nprint(\"done\")\r\n```\r\n\r\nThe cifar10 input pipeline (this one works fine)\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom utils import cifar10\r\nfrom tensorflow.python.framework import ops\r\n\r\nclass cifar10_data:\r\n  \"\"\"\r\n  Downloads the CIFAR10 dataset and creates an input pipeline ready to be fed into a model.\r\n\r\n  - Reshapes flat images into 32x32\r\n  - converts [0 1] to [-1 1]\r\n  - shuffles the input\r\n  - builds batches\r\n  \"\"\"\r\n  NUM_THREADS = 8\r\n  NUMBER_OF_CLASSES = 10\r\n\r\n  TRAIN_SET_SIZE = 50000\r\n  TEST_SET_SIZE =  10000\r\n  IMAGE_SIZE = 32\r\n  NUM_OF_CHANNELS = 3\r\n\r\n  def __init__(self, batch_size):\r\n    \"\"\" Downloads the cifar10 data if necessary. \"\"\"\r\n    self.batch_size = batch_size\r\n    cifar10.maybe_download_and_extract()\r\n\r\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = cifar10.load_training_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = cifar10.load_test_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\r\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\r\n\r\n    images = ops.convert_to_tensor(raw_images)\r\n    targets = ops.convert_to_tensor(raw_targets)\r\n\r\n    set_size, width, height, channels = raw_images.shape\r\n\r\n    images = tf.reshape(images, [set_size, width, height, channels])\r\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\r\n\r\n    # Data Augmentation\r\n    if augmentation:\r\n      # TODO\r\n      # make sure after further preprocessing it is [0 1]\r\n      pass\r\n\r\n    # convert the given [0, 1] to [-1, 1]\r\n    image = tf.sub(image, 0.5)\r\n    image = tf.mul(image, 2.0)\r\n\r\n    images_batch, labels_batch = tf.train.batch([image, label], batch_size=self.batch_size, num_threads=self.NUM_THREADS)\r\n\r\n    return images_batch, labels_batch\r\n```\r\n\r\nThe not working SVHN input pipeline (virtually the same input pipeline script)\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom utils import svhn\r\nfrom tensorflow.python.framework import ops\r\n\r\nclass svhn_data:\r\n  \"\"\"\r\n  Downloads the SVHN dataset and creates an input pipeline ready to be fed into a model.\r\n\r\n  - Reshapes flat images into 32x32\r\n  - converts [0 1] to [-1 1]\r\n  - shuffles the input\r\n  - builds batches\r\n  \"\"\"\r\n  NUM_THREADS = 8\r\n  NUMBER_OF_CLASSES = 100\r\n\r\n  TRAIN_SET_SIZE = 73257\r\n  TEST_SET_SIZE =  26032\r\n  IMAGE_SIZE = 32\r\n  NUM_OF_CHANNELS = 3\r\n\r\n  def __init__(self, batch_size):\r\n    \"\"\" Downloads the cifar100 data if necessary. \"\"\"\r\n    self.batch_size = batch_size\r\n    svhn.download_data()\r\n\r\n  def build_train_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = svhn.load_training_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def build_test_data_tensor(self, shuffle=False, augmentation=False):\r\n    images, _, targets = svhn.load_test_data()\r\n    return self.__build_generic_data_tensor(images,\r\n                                            targets,\r\n                                            shuffle,\r\n                                            augmentation)\r\n\r\n  def __build_generic_data_tensor(self, raw_images, raw_targets, shuffle, augmentation):\r\n    \"\"\" Creates the input pipeline and performs some preprocessing. \"\"\"\r\n\r\n    images = ops.convert_to_tensor(raw_images)\r\n    targets = ops.convert_to_tensor(raw_targets)\r\n\r\n    set_size, width, height, channels = raw_images.shape\r\n\r\n    images = tf.reshape(images, [set_size, width, height, channels])\r\n    image, label = tf.train.slice_input_producer([images, targets], shuffle=shuffle)\r\n\r\n    # Data Augmentation\r\n    if augmentation:\r\n      # TODO\r\n      # make sure after further preprocessing it is [0 1]\r\n      pass\r\n\r\n    # convert the given [0, 1] to [-1, 1]\r\n    image = tf.sub(image, 0.5)\r\n    image = tf.mul(image, 2.0)\r\n\r\n    images_batch, labels_batch = tf.train.batch([image, label],\r\n                                                batch_size=self.batch_size,\r\n                                                num_threads=self.NUM_THREADS)\r\n\r\n    return images_batch, labels_batch\r\n```"}