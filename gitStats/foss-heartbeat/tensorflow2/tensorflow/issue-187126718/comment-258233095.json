{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/258233095", "html_url": "https://github.com/tensorflow/tensorflow/issues/5383#issuecomment-258233095", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5383", "id": 258233095, "node_id": "MDEyOklzc3VlQ29tbWVudDI1ODIzMzA5NQ==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-03T18:30:21Z", "updated_at": "2016-11-03T18:30:34Z", "author_association": "MEMBER", "body_html": "<p>Note in the future this question is best asked on StackOverflow given that it concerns best usage. In fact, to get more help on feeding large data, that is the best place to look (or some of the documentation on the TensorFlow web page).</p>\n<p>The key problem here is that tf.convert_to_tensor() creates tf.constant's behind the scenes. What you need to do is to create a placeholder that represents a batch that will be less than 2GB (likely you want way less than that). Then you can feed parts of your giant numpy array dataset at a time. For a simple reproducer of what's going on with your example look at this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\nsess<span class=\"pl-k\">=</span>tf.Session()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> make a np array that is 1.1 GB</span>\nsize_in_gb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1.1</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> so that 2 of them will exceed 2GB</span>\nfloat32_bytes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\nnelements <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(size_in_gb <span class=\"pl-k\">*</span> <span class=\"pl-c1\">float</span>(<span class=\"pl-c1\">1</span><span class=\"pl-k\">&lt;&lt;</span><span class=\"pl-c1\">30</span>) <span class=\"pl-k\">/</span> float32_bytes)\nfoo <span class=\"pl-k\">=</span> np.ones(nelements, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> convert np array to tensors (<span class=\"pl-k\">NOTE</span>: this implicitly creates tf.constant's)</span>\nt1 <span class=\"pl-k\">=</span> tf.convert_to_tensor(foo)\nt2 <span class=\"pl-k\">=</span> tf.convert_to_tensor(foo)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This bakes the graphdef and throws an error when it exceeds 2GB</span>\nsess.run(t1 <span class=\"pl-k\">+</span> t2)</pre></div>", "body_text": "Note in the future this question is best asked on StackOverflow given that it concerns best usage. In fact, to get more help on feeding large data, that is the best place to look (or some of the documentation on the TensorFlow web page).\nThe key problem here is that tf.convert_to_tensor() creates tf.constant's behind the scenes. What you need to do is to create a placeholder that represents a batch that will be less than 2GB (likely you want way less than that). Then you can feed parts of your giant numpy array dataset at a time. For a simple reproducer of what's going on with your example look at this:\nimport tensorflow as tf\nimport numpy as np\nsess=tf.Session()\n\n# make a np array that is 1.1 GB\nsize_in_gb = 1.1 # so that 2 of them will exceed 2GB\nfloat32_bytes = 4\nnelements = int(size_in_gb * float(1<<30) / float32_bytes)\nfoo = np.ones(nelements, dtype=np.float32)\n\n# convert np array to tensors (NOTE: this implicitly creates tf.constant's)\nt1 = tf.convert_to_tensor(foo)\nt2 = tf.convert_to_tensor(foo)\n# This bakes the graphdef and throws an error when it exceeds 2GB\nsess.run(t1 + t2)", "body": "Note in the future this question is best asked on StackOverflow given that it concerns best usage. In fact, to get more help on feeding large data, that is the best place to look (or some of the documentation on the TensorFlow web page).\n\nThe key problem here is that tf.convert_to_tensor() creates tf.constant's behind the scenes. What you need to do is to create a placeholder that represents a batch that will be less than 2GB (likely you want way less than that). Then you can feed parts of your giant numpy array dataset at a time. For a simple reproducer of what's going on with your example look at this:\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nsess=tf.Session()\n\n# make a np array that is 1.1 GB\nsize_in_gb = 1.1 # so that 2 of them will exceed 2GB\nfloat32_bytes = 4\nnelements = int(size_in_gb * float(1<<30) / float32_bytes)\nfoo = np.ones(nelements, dtype=np.float32)\n\n# convert np array to tensors (NOTE: this implicitly creates tf.constant's)\nt1 = tf.convert_to_tensor(foo)\nt2 = tf.convert_to_tensor(foo)\n# This bakes the graphdef and throws an error when it exceeds 2GB\nsess.run(t1 + t2)\n```\n"}