{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12667", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12667/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12667/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12667/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12667", "id": 253511156, "node_id": "MDU6SXNzdWUyNTM1MTExNTY=", "number": 12667, "title": "Distributed Training Randomly Stops During the Training Process", "user": {"login": "lsy643", "id": 7875729, "node_id": "MDQ6VXNlcjc4NzU3Mjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7875729?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lsy643", "html_url": "https://github.com/lsy643", "followers_url": "https://api.github.com/users/lsy643/followers", "following_url": "https://api.github.com/users/lsy643/following{/other_user}", "gists_url": "https://api.github.com/users/lsy643/gists{/gist_id}", "starred_url": "https://api.github.com/users/lsy643/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lsy643/subscriptions", "organizations_url": "https://api.github.com/users/lsy643/orgs", "repos_url": "https://api.github.com/users/lsy643/repos", "events_url": "https://api.github.com/users/lsy643/events{/privacy}", "received_events_url": "https://api.github.com/users/lsy643/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-08-29T02:32:00Z", "updated_at": "2017-12-15T18:38:30Z", "closed_at": "2017-12-15T18:38:30Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.3.0-rc2-20-g0787eee 1.3.0</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>: 6.0</li>\n<li><strong>GPU model and memory</strong>:  Tesla K80, 12G</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>In my distributed training program, there are one server and two workers, which all run in separately nvidia-docker container. At the beginning, the cluster works just fine, but running normally after several hours, the two workers just stop.</p>\n<p>My training process:</p>\n<ol>\n<li>I create three nvidia-docker containers, one for parameter server, two for workers</li>\n<li>In every container, I run the <code>train_replica</code> function below after defining all necessary parts such as cluster_spec, inference function, data batch and so on.</li>\n<li>It works correctly at the beginning</li>\n<li>It stops several hours later</li>\n</ol>\n<h3>Source code / logs</h3>\n<p>My trainer function:</p>\n<pre lang=\"{python}\"><code>def train_replica(cluster_spec,\n                  get_data_batch,\n                  inference_fn,\n                  get_init_fn,\n                  get_learning_rate,\n                  get_optimizer,\n                  get_train_variables,\n                  replica_param,\n                  train_param,\n                  ):\n    job_name = replica_param['job_name']\n    task_index = replica_param['task_index']\n    sync_replicas = train_param['sync_replicas']\n    log_dir = train_param['log_dir']\n    assert job_name in ['ps', 'worker']\n    server = tf.train.Server(cluster_spec, job_name=job_name,\n                             task_index=task_index, config=get_ps_session_config())\n    if job_name == 'ps':\n        server.join()\n    else:\n        is_chief = (task_index == 0)\n        device_setter = tf.train.replica_device_setter(cluster=cluster_spec)\n        with tf.Graph().as_default():\n            with tf.device(device_setter):\n                global_step = create_global_step()\n                learning_rate = get_learning_rate(global_step)\n                data_batch = get_data_batch()\n                _ = inference_fn(data_batch)\n                total_loss, task_loss = get_losses()\n                optimizer = get_optimizer(learning_rate)\n                if sync_replicas:\n                    optimizer = tf.train.SyncReplicasOptimizer(\n                        opt=optimizer,\n                        replicas_to_aggregate=cluster_spec.num_tasks('worker'),\n                        total_num_replicas=cluster_spec.num_tasks('worker'),\n                        name='sync_replica_optimizer'\n                    )\n                train_op = slim.learning.create_train_op(\n                    total_loss=total_loss,\n                    optimizer=optimizer,\n                    global_step=global_step,\n                    variables_to_train=get_train_variables(),\n                    clip_gradient_norm=train_param['clip_norm'],\n                    gradient_multipliers=train_param['gradient_multipliers'],\n                )\n                init_fn = get_init_fn() if get_init_fn is not None else None\n                scaffold = tf.train.Scaffold(\n                    init_op=tf.global_variables_initializer())\n                scaffold._init_fn = init_fn\n                hooks = [tf.train.StopAtStepHook(train_param['train_steps'])]\n                if sync_replicas is True:\n                    hooks.append(optimizer.make_session_run_hook(is_chief))\n                chief_only_hooks = [tf.train.LoggingTensorHook([total_loss, task_loss], 100)]\n                step_ind = 0\n                with tf.train.MonitoredTrainingSession(\n                        master=server.target,\n                        is_chief=is_chief,\n                        checkpoint_dir=log_dir,\n                        scaffold=scaffold,\n                        hooks=hooks,\n                        chief_only_hooks=chief_only_hooks,\n                        config=get_worker_session_config(task_index)) as session:\n                    while not session.should_stop():\n                        session.run(train_op)\n                        step_ind += 1\n                        if step_ind % 1000 == 0:\n                            tf.logging.debug('Training Step At {s}'.format(s=step_ind))\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0\nPython version: 3.5.2\nCUDA/cuDNN version: 6.0\nGPU model and memory:  Tesla K80, 12G\n\nDescribe the problem\nIn my distributed training program, there are one server and two workers, which all run in separately nvidia-docker container. At the beginning, the cluster works just fine, but running normally after several hours, the two workers just stop.\nMy training process:\n\nI create three nvidia-docker containers, one for parameter server, two for workers\nIn every container, I run the train_replica function below after defining all necessary parts such as cluster_spec, inference function, data batch and so on.\nIt works correctly at the beginning\nIt stops several hours later\n\nSource code / logs\nMy trainer function:\ndef train_replica(cluster_spec,\n                  get_data_batch,\n                  inference_fn,\n                  get_init_fn,\n                  get_learning_rate,\n                  get_optimizer,\n                  get_train_variables,\n                  replica_param,\n                  train_param,\n                  ):\n    job_name = replica_param['job_name']\n    task_index = replica_param['task_index']\n    sync_replicas = train_param['sync_replicas']\n    log_dir = train_param['log_dir']\n    assert job_name in ['ps', 'worker']\n    server = tf.train.Server(cluster_spec, job_name=job_name,\n                             task_index=task_index, config=get_ps_session_config())\n    if job_name == 'ps':\n        server.join()\n    else:\n        is_chief = (task_index == 0)\n        device_setter = tf.train.replica_device_setter(cluster=cluster_spec)\n        with tf.Graph().as_default():\n            with tf.device(device_setter):\n                global_step = create_global_step()\n                learning_rate = get_learning_rate(global_step)\n                data_batch = get_data_batch()\n                _ = inference_fn(data_batch)\n                total_loss, task_loss = get_losses()\n                optimizer = get_optimizer(learning_rate)\n                if sync_replicas:\n                    optimizer = tf.train.SyncReplicasOptimizer(\n                        opt=optimizer,\n                        replicas_to_aggregate=cluster_spec.num_tasks('worker'),\n                        total_num_replicas=cluster_spec.num_tasks('worker'),\n                        name='sync_replica_optimizer'\n                    )\n                train_op = slim.learning.create_train_op(\n                    total_loss=total_loss,\n                    optimizer=optimizer,\n                    global_step=global_step,\n                    variables_to_train=get_train_variables(),\n                    clip_gradient_norm=train_param['clip_norm'],\n                    gradient_multipliers=train_param['gradient_multipliers'],\n                )\n                init_fn = get_init_fn() if get_init_fn is not None else None\n                scaffold = tf.train.Scaffold(\n                    init_op=tf.global_variables_initializer())\n                scaffold._init_fn = init_fn\n                hooks = [tf.train.StopAtStepHook(train_param['train_steps'])]\n                if sync_replicas is True:\n                    hooks.append(optimizer.make_session_run_hook(is_chief))\n                chief_only_hooks = [tf.train.LoggingTensorHook([total_loss, task_loss], 100)]\n                step_ind = 0\n                with tf.train.MonitoredTrainingSession(\n                        master=server.target,\n                        is_chief=is_chief,\n                        checkpoint_dir=log_dir,\n                        scaffold=scaffold,\n                        hooks=hooks,\n                        chief_only_hooks=chief_only_hooks,\n                        config=get_worker_session_config(task_index)) as session:\n                    while not session.should_stop():\n                        session.run(train_op)\n                        step_ind += 1\n                        if step_ind % 1000 == 0:\n                            tf.logging.debug('Training Step At {s}'.format(s=step_ind))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 \r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: 6.0\r\n- **GPU model and memory**:  Tesla K80, 12G\r\n\r\n\r\n### Describe the problem\r\nIn my distributed training program, there are one server and two workers, which all run in separately nvidia-docker container. At the beginning, the cluster works just fine, but running normally after several hours, the two workers just stop.\r\n\r\nMy training process:\r\n1. I create three nvidia-docker containers, one for parameter server, two for workers\r\n2. In every container, I run the `train_replica` function below after defining all necessary parts such as cluster_spec, inference function, data batch and so on.\r\n3. It works correctly at the beginning\r\n4. It stops several hours later\r\n\r\n\r\n### Source code / logs\r\nMy trainer function:\r\n```{python}\r\ndef train_replica(cluster_spec,\r\n                  get_data_batch,\r\n                  inference_fn,\r\n                  get_init_fn,\r\n                  get_learning_rate,\r\n                  get_optimizer,\r\n                  get_train_variables,\r\n                  replica_param,\r\n                  train_param,\r\n                  ):\r\n    job_name = replica_param['job_name']\r\n    task_index = replica_param['task_index']\r\n    sync_replicas = train_param['sync_replicas']\r\n    log_dir = train_param['log_dir']\r\n    assert job_name in ['ps', 'worker']\r\n    server = tf.train.Server(cluster_spec, job_name=job_name,\r\n                             task_index=task_index, config=get_ps_session_config())\r\n    if job_name == 'ps':\r\n        server.join()\r\n    else:\r\n        is_chief = (task_index == 0)\r\n        device_setter = tf.train.replica_device_setter(cluster=cluster_spec)\r\n        with tf.Graph().as_default():\r\n            with tf.device(device_setter):\r\n                global_step = create_global_step()\r\n                learning_rate = get_learning_rate(global_step)\r\n                data_batch = get_data_batch()\r\n                _ = inference_fn(data_batch)\r\n                total_loss, task_loss = get_losses()\r\n                optimizer = get_optimizer(learning_rate)\r\n                if sync_replicas:\r\n                    optimizer = tf.train.SyncReplicasOptimizer(\r\n                        opt=optimizer,\r\n                        replicas_to_aggregate=cluster_spec.num_tasks('worker'),\r\n                        total_num_replicas=cluster_spec.num_tasks('worker'),\r\n                        name='sync_replica_optimizer'\r\n                    )\r\n                train_op = slim.learning.create_train_op(\r\n                    total_loss=total_loss,\r\n                    optimizer=optimizer,\r\n                    global_step=global_step,\r\n                    variables_to_train=get_train_variables(),\r\n                    clip_gradient_norm=train_param['clip_norm'],\r\n                    gradient_multipliers=train_param['gradient_multipliers'],\r\n                )\r\n                init_fn = get_init_fn() if get_init_fn is not None else None\r\n                scaffold = tf.train.Scaffold(\r\n                    init_op=tf.global_variables_initializer())\r\n                scaffold._init_fn = init_fn\r\n                hooks = [tf.train.StopAtStepHook(train_param['train_steps'])]\r\n                if sync_replicas is True:\r\n                    hooks.append(optimizer.make_session_run_hook(is_chief))\r\n                chief_only_hooks = [tf.train.LoggingTensorHook([total_loss, task_loss], 100)]\r\n                step_ind = 0\r\n                with tf.train.MonitoredTrainingSession(\r\n                        master=server.target,\r\n                        is_chief=is_chief,\r\n                        checkpoint_dir=log_dir,\r\n                        scaffold=scaffold,\r\n                        hooks=hooks,\r\n                        chief_only_hooks=chief_only_hooks,\r\n                        config=get_worker_session_config(task_index)) as session:\r\n                    while not session.should_stop():\r\n                        session.run(train_op)\r\n                        step_ind += 1\r\n                        if step_ind % 1000 == 0:\r\n                            tf.logging.debug('Training Step At {s}'.format(s=step_ind))\r\n```\r\n"}