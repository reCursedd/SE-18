{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23473", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23473/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23473/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23473/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23473", "id": 377022417, "node_id": "MDU6SXNzdWUzNzcwMjI0MTc=", "number": 23473, "title": "tf.matmul fails with CUBLAS_STATUS_NOT_SUPPORTED for large matrices when using CUDA 9.1 ", "user": {"login": "malcolmst", "id": 23696107, "node_id": "MDQ6VXNlcjIzNjk2MTA3", "avatar_url": "https://avatars1.githubusercontent.com/u/23696107?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malcolmst", "html_url": "https://github.com/malcolmst", "followers_url": "https://api.github.com/users/malcolmst/followers", "following_url": "https://api.github.com/users/malcolmst/following{/other_user}", "gists_url": "https://api.github.com/users/malcolmst/gists{/gist_id}", "starred_url": "https://api.github.com/users/malcolmst/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malcolmst/subscriptions", "organizations_url": "https://api.github.com/users/malcolmst/orgs", "repos_url": "https://api.github.com/users/malcolmst/repos", "events_url": "https://api.github.com/users/malcolmst/events{/privacy}", "received_events_url": "https://api.github.com/users/malcolmst/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-11-03T04:34:33Z", "updated_at": "2018-11-22T18:51:11Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I am hitting this issue on multiple development machines with different GPUs, and all versions of Tensorflow &gt;= 1.9.0. I have a CUDA 9.1 requirement on the host machine, so downgrading to CUDA 9.0 is not an option for me.</p>\n<p>See <a href=\"https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250\" rel=\"nofollow\">https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250</a> for a possibly related issue, though that involved Tensorflow 1.8.0.</p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): <em>Yes, a minimum repro is below</em></li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): <em>Linux Ubuntu 16.04</em></li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: <em>n/a</em></li>\n<li>TensorFlow installed from (source or binary): <em>source</em></li>\n<li>TensorFlow version (use command below): <em>1.9.0 to 1.11.0 (does not reproduce in 1.8.0)</em></li>\n<li>Python version: <em>2.7</em></li>\n<li>Bazel version (if compiling from source): <em>Bazel 0.10.0 for  Tensorflow 1.9.0, Bazel 0.18.0 for Tensorflow 1.10.0 and newer</em></li>\n<li>GCC/Compiler version (if compiling from source): <em>GCC 5.4.0</em></li>\n<li>CUDA/cuDNN version: <em>CUDA 9.1.85 / cuDNN 7.0.5</em></li>\n<li>GPU model and memory: <em>GTX 1080 w/ 7400 MB, GTX 1060 w/ 5600 MB</em></li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>tf.matmul is failing to multiply matrices above a certain size, with error:<br>\n<code>failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED</code></p>\n<p>I have confirmed using <code>nvidia-smi</code> that the GPU is nowhere close to running out of memory.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>The matrix multiplication should complete successfully.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<p>This is borrowed from the stackoverflow link above:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto()\nconfig.gpu_options.allow_growth<span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>\ntf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config).close()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">calc</span>():\n    N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">15</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> works for N &lt;= 14</span>\n    a <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>\n    b <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n    X <span class=\"pl-k\">=</span> np.random.rand(N, <span class=\"pl-c1\">11520</span>, b, <span class=\"pl-c1\">1</span>).astype(np.float32)\n    <span class=\"pl-c1\">print</span>(X.nbytes<span class=\"pl-k\">*</span><span class=\"pl-c1\">1e-6</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MB<span class=\"pl-pds\">\"</span></span>)\n    W <span class=\"pl-k\">=</span> np.random.rand(N, <span class=\"pl-c1\">11520</span>, a, b).astype(np.float32)\n    <span class=\"pl-c1\">print</span>(W.nbytes<span class=\"pl-k\">*</span><span class=\"pl-c1\">1e-6</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MB<span class=\"pl-pds\">\"</span></span>)\n    X_ <span class=\"pl-k\">=</span> tf.constant(X, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X-constant<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    W_ <span class=\"pl-k\">=</span> tf.constant(W, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W-constant<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    <span class=\"pl-k\">return</span> tf.matmul(W_, X_, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mymatmul<span class=\"pl-pds\">\"</span></span>)\n\ntf.reset_default_graph()\na <span class=\"pl-k\">=</span> calc()\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(tf.global_variables_initializer())\nb <span class=\"pl-k\">=</span> sess.run(a)\nsess.close()\n<span class=\"pl-c1\">print</span>(b.shape)</pre></div>\n<p><strong>Other info / logs</strong></p>\n<p>I found a workaround for this issue is to patch <code>CUDABlas::DoBlasGemmBatchedInternal</code> in <code>tensorflow/stream_extractor/cuda/cuda_blas.cc</code> to disable the <code>#if CUDA_VERSION &gt;= 9010</code> block which calls <code>wrap::cublasGemmBatchedEx</code>. Downgrading to Tensorflow &lt;= 1.8.0 also resolves the issue for me (that codeblock was added in Tensorflow 1.9.0).</p>\n<p>Running the above code (N=15) with Tensorflow 1.11.0:</p>\n<pre><code>2018-11-02 23:13:48.059128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-11-02 23:13:48.059860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.40GiB\n2018-11-02 23:13:48.059884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:13:48.282603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:13:48.282631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:13:48.282636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:13:48.282790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(5.529599999999999, 'MB')\n(88.47359999999999, 'MB')\n2018-11-02 23:13:48.963544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:13:48.963579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:13:48.963585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:13:48.963588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:13:48.963722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-11-02 23:13:51.146858: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED\n2018-11-02 23:13:51.146900: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details\nTraceback (most recent call last):\n  File \"./tf_matmul.py\", line 25, in &lt;module&gt;\n    b = sess.run(a)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 887, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\n    run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\n\nCaused by op u'mymatmul', defined at:\n  File \"./tf_matmul.py\", line 22, in &lt;module&gt;\n    a = calc()\n  File \"./tf_matmul.py\", line 19, in calc\n    return tf.matmul(W_, X_, name=\"mymatmul\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 2015, in matmul\n    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1245, in batch_mat_mul\n    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\n</code></pre>\n<p>Reducing the matrix size (N=14):</p>\n<pre><code>2018-11-02 23:18:29.127555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-11-02 23:18:29.128409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\n2018-11-02 23:18:29.128441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:18:29.482292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:18:29.482346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:18:29.482361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:18:29.482670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(5.160959999999999, 'MB')\n(82.57535999999999, 'MB')\n2018-11-02 23:18:30.200524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:18:30.200586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:18:30.200598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:18:30.200605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:18:30.200837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(14, 11520, 16, 1)\n\n</code></pre>\n<p>After patching <code>CUDABlas::DoBlasGemmBatchedInternal</code> (N=15):</p>\n<pre><code>2018-11-02 23:28:05.667458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-11-02 23:28:05.668033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\n2018-11-02 23:28:05.668046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:28:05.866043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:28:05.866069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:28:05.866074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:28:05.866218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(5.529599999999999, 'MB')\n(88.47359999999999, 'MB')\n2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(15, 11520, 16, 1)\n\n</code></pre>\n<p>Patch for workaround:</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-c1\">diff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc</span>\nindex ab7091b..50579b6 100644\n<span class=\"pl-md\">--- a/tensorflow/stream_executor/cuda/cuda_blas.cc</span>\n<span class=\"pl-mi1\">+++ b/tensorflow/stream_executor/cuda/cuda_blas.cc</span>\n<span class=\"pl-mdr\">@@ -18,6 +18,8 @@</span> limitations under the License.\n \n #define SE_CUDA_DATA_HALF CUDA_R_16F\n \n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>#define USE_CUBLAS_GEMM_BATCHED_EX false</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span></span>\n #include \"tensorflow/stream_executor/cuda/cuda_blas.h\"\n \n // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As\n<span class=\"pl-mdr\">@@ -2482,7 +2484,7 @@</span> port::Status CUDABlas::DoBlasGemmBatchedInternal(\n \n   cudaDataType_t data_type = CUDADataType&lt;T&gt;::type;\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>#if CUDA_VERSION &gt;= 9010</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>#if CUDA_VERSION &gt;= 9010 &amp;&amp; USE_CUBLAS_GEMM_BATCHED_EX</span>\n   int cc_major, cc_minor;\n   if (stream-&gt;parent()-&gt;GetDeviceDescription().cuda_compute_capability(\n           &amp;cc_major, &amp;cc_minor) &amp;&amp;</pre></div>", "body_text": "I am hitting this issue on multiple development machines with different GPUs, and all versions of Tensorflow >= 1.9.0. I have a CUDA 9.1 requirement on the host machine, so downgrading to CUDA 9.0 is not an option for me.\nSee https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250 for a possibly related issue, though that involved Tensorflow 1.8.0.\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a minimum repro is below\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.9.0 to 1.11.0 (does not reproduce in 1.8.0)\nPython version: 2.7\nBazel version (if compiling from source): Bazel 0.10.0 for  Tensorflow 1.9.0, Bazel 0.18.0 for Tensorflow 1.10.0 and newer\nGCC/Compiler version (if compiling from source): GCC 5.4.0\nCUDA/cuDNN version: CUDA 9.1.85 / cuDNN 7.0.5\nGPU model and memory: GTX 1080 w/ 7400 MB, GTX 1060 w/ 5600 MB\n\nDescribe the current behavior\ntf.matmul is failing to multiply matrices above a certain size, with error:\nfailed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED\nI have confirmed using nvidia-smi that the GPU is nowhere close to running out of memory.\nDescribe the expected behavior\nThe matrix multiplication should complete successfully.\nCode to reproduce the issue\nThis is borrowed from the stackoverflow link above:\nimport tensorflow as tf\nimport numpy as np\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\ntf.Session(config=config).close()\n\ndef calc():\n    N = 15 # works for N <= 14\n    a = 16\n    b = 8\n    X = np.random.rand(N, 11520, b, 1).astype(np.float32)\n    print(X.nbytes*1e-6, \"MB\")\n    W = np.random.rand(N, 11520, a, b).astype(np.float32)\n    print(W.nbytes*1e-6, \"MB\")\n    X_ = tf.constant(X, name=\"X-constant\", dtype=tf.float32)\n    W_ = tf.constant(W, name=\"W-constant\", dtype=tf.float32)\n\n    return tf.matmul(W_, X_, name=\"mymatmul\")\n\ntf.reset_default_graph()\na = calc()\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nb = sess.run(a)\nsess.close()\nprint(b.shape)\nOther info / logs\nI found a workaround for this issue is to patch CUDABlas::DoBlasGemmBatchedInternal in tensorflow/stream_extractor/cuda/cuda_blas.cc to disable the #if CUDA_VERSION >= 9010 block which calls wrap::cublasGemmBatchedEx. Downgrading to Tensorflow <= 1.8.0 also resolves the issue for me (that codeblock was added in Tensorflow 1.9.0).\nRunning the above code (N=15) with Tensorflow 1.11.0:\n2018-11-02 23:13:48.059128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-11-02 23:13:48.059860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.40GiB\n2018-11-02 23:13:48.059884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:13:48.282603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:13:48.282631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:13:48.282636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:13:48.282790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(5.529599999999999, 'MB')\n(88.47359999999999, 'MB')\n2018-11-02 23:13:48.963544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:13:48.963579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:13:48.963585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:13:48.963588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:13:48.963722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-11-02 23:13:51.146858: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED\n2018-11-02 23:13:51.146900: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details\nTraceback (most recent call last):\n  File \"./tf_matmul.py\", line 25, in <module>\n    b = sess.run(a)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 887, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\n    run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\n\nCaused by op u'mymatmul', defined at:\n  File \"./tf_matmul.py\", line 22, in <module>\n    a = calc()\n  File \"./tf_matmul.py\", line 19, in calc\n    return tf.matmul(W_, X_, name=\"mymatmul\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 2015, in matmul\n    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1245, in batch_mat_mul\n    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\n\nReducing the matrix size (N=14):\n2018-11-02 23:18:29.127555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-11-02 23:18:29.128409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\n2018-11-02 23:18:29.128441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:18:29.482292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:18:29.482346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:18:29.482361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:18:29.482670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(5.160959999999999, 'MB')\n(82.57535999999999, 'MB')\n2018-11-02 23:18:30.200524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:18:30.200586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:18:30.200598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:18:30.200605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:18:30.200837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(14, 11520, 16, 1)\n\n\nAfter patching CUDABlas::DoBlasGemmBatchedInternal (N=15):\n2018-11-02 23:28:05.667458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-11-02 23:28:05.668033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\n2018-11-02 23:28:05.668046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:28:05.866043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:28:05.866069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:28:05.866074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:28:05.866218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(5.529599999999999, 'MB')\n(88.47359999999999, 'MB')\n2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n(15, 11520, 16, 1)\n\n\nPatch for workaround:\ndiff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc\nindex ab7091b..50579b6 100644\n--- a/tensorflow/stream_executor/cuda/cuda_blas.cc\n+++ b/tensorflow/stream_executor/cuda/cuda_blas.cc\n@@ -18,6 +18,8 @@ limitations under the License.\n \n #define SE_CUDA_DATA_HALF CUDA_R_16F\n \n+#define USE_CUBLAS_GEMM_BATCHED_EX false\n+\n #include \"tensorflow/stream_executor/cuda/cuda_blas.h\"\n \n // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As\n@@ -2482,7 +2484,7 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(\n \n   cudaDataType_t data_type = CUDADataType<T>::type;\n \n-#if CUDA_VERSION >= 9010\n+#if CUDA_VERSION >= 9010 && USE_CUBLAS_GEMM_BATCHED_EX\n   int cc_major, cc_minor;\n   if (stream->parent()->GetDeviceDescription().cuda_compute_capability(\n           &cc_major, &cc_minor) &&", "body": "I am hitting this issue on multiple development machines with different GPUs, and all versions of Tensorflow >= 1.9.0. I have a CUDA 9.1 requirement on the host machine, so downgrading to CUDA 9.0 is not an option for me. \r\n\r\nSee https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250 for a possibly related issue, though that involved Tensorflow 1.8.0.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes, a minimum repro is below_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Linux Ubuntu 16.04_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: _n/a_\r\n- TensorFlow installed from (source or binary): _source_\r\n- TensorFlow version (use command below): _1.9.0 to 1.11.0 (does not reproduce in 1.8.0)_\r\n- Python version: _2.7_\r\n- Bazel version (if compiling from source): _Bazel 0.10.0 for  Tensorflow 1.9.0, Bazel 0.18.0 for Tensorflow 1.10.0 and newer_\r\n- GCC/Compiler version (if compiling from source): _GCC 5.4.0_\r\n- CUDA/cuDNN version: _CUDA 9.1.85 / cuDNN 7.0.5_\r\n- GPU model and memory: _GTX 1080 w/ 7400 MB, GTX 1060 w/ 5600 MB_\r\n\r\n**Describe the current behavior**\r\n\r\ntf.matmul is failing to multiply matrices above a certain size, with error:\r\n`failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED`\r\n\r\nI have confirmed using `nvidia-smi` that the GPU is nowhere close to running out of memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe matrix multiplication should complete successfully.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis is borrowed from the stackoverflow link above:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\ntf.Session(config=config).close()\r\n\r\ndef calc():\r\n    N = 15 # works for N <= 14\r\n    a = 16\r\n    b = 8\r\n    X = np.random.rand(N, 11520, b, 1).astype(np.float32)\r\n    print(X.nbytes*1e-6, \"MB\")\r\n    W = np.random.rand(N, 11520, a, b).astype(np.float32)\r\n    print(W.nbytes*1e-6, \"MB\")\r\n    X_ = tf.constant(X, name=\"X-constant\", dtype=tf.float32)\r\n    W_ = tf.constant(W, name=\"W-constant\", dtype=tf.float32)\r\n\r\n    return tf.matmul(W_, X_, name=\"mymatmul\")\r\n\r\ntf.reset_default_graph()\r\na = calc()\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nb = sess.run(a)\r\nsess.close()\r\nprint(b.shape)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nI found a workaround for this issue is to patch `CUDABlas::DoBlasGemmBatchedInternal` in `tensorflow/stream_extractor/cuda/cuda_blas.cc` to disable the `#if CUDA_VERSION >= 9010` block which calls `wrap::cublasGemmBatchedEx`. Downgrading to Tensorflow <= 1.8.0 also resolves the issue for me (that codeblock was added in Tensorflow 1.9.0).\r\n\r\nRunning the above code (N=15) with Tensorflow 1.11.0:\r\n```\r\n2018-11-02 23:13:48.059128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-02 23:13:48.059860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.40GiB\r\n2018-11-02 23:13:48.059884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:13:48.282603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:13:48.282631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:13:48.282636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:13:48.282790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(5.529599999999999, 'MB')\r\n(88.47359999999999, 'MB')\r\n2018-11-02 23:13:48.963544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:13:48.963579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:13:48.963585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:13:48.963588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:13:48.963722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-11-02 23:13:51.146858: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED\r\n2018-11-02 23:13:51.146900: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details\r\nTraceback (most recent call last):\r\n  File \"./tf_matmul.py\", line 25, in <module>\r\n    b = sess.run(a)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\r\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\r\n\r\nCaused by op u'mymatmul', defined at:\r\n  File \"./tf_matmul.py\", line 22, in <module>\r\n    a = calc()\r\n  File \"./tf_matmul.py\", line 19, in calc\r\n    return tf.matmul(W_, X_, name=\"mymatmul\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 2015, in matmul\r\n    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1245, in batch_mat_mul\r\n    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\r\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\r\n```\r\n\r\nReducing the matrix size (N=14):\r\n```\r\n2018-11-02 23:18:29.127555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-02 23:18:29.128409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\r\n2018-11-02 23:18:29.128441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:18:29.482292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:18:29.482346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:18:29.482361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:18:29.482670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(5.160959999999999, 'MB')\r\n(82.57535999999999, 'MB')\r\n2018-11-02 23:18:30.200524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:18:30.200586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:18:30.200598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:18:30.200605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:18:30.200837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(14, 11520, 16, 1)\r\n\r\n```\r\n\r\nAfter patching `CUDABlas::DoBlasGemmBatchedInternal` (N=15):\r\n\r\n```\r\n2018-11-02 23:28:05.667458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-02 23:28:05.668033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\r\n2018-11-02 23:28:05.668046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:28:05.866043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:28:05.866069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:28:05.866074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:28:05.866218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(5.529599999999999, 'MB')\r\n(88.47359999999999, 'MB')\r\n2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(15, 11520, 16, 1)\r\n\r\n```\r\n\r\nPatch for workaround:\r\n```patch\r\ndiff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc\r\nindex ab7091b..50579b6 100644\r\n--- a/tensorflow/stream_executor/cuda/cuda_blas.cc\r\n+++ b/tensorflow/stream_executor/cuda/cuda_blas.cc\r\n@@ -18,6 +18,8 @@ limitations under the License.\r\n \r\n #define SE_CUDA_DATA_HALF CUDA_R_16F\r\n \r\n+#define USE_CUBLAS_GEMM_BATCHED_EX false\r\n+\r\n #include \"tensorflow/stream_executor/cuda/cuda_blas.h\"\r\n \r\n // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As\r\n@@ -2482,7 +2484,7 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(\r\n \r\n   cudaDataType_t data_type = CUDADataType<T>::type;\r\n \r\n-#if CUDA_VERSION >= 9010\r\n+#if CUDA_VERSION >= 9010 && USE_CUBLAS_GEMM_BATCHED_EX\r\n   int cc_major, cc_minor;\r\n   if (stream->parent()->GetDeviceDescription().cuda_compute_capability(\r\n           &cc_major, &cc_minor) &&\r\n```"}