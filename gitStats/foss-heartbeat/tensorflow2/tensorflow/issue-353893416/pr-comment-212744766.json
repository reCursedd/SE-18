{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/212744766", "pull_request_review_id": 149447613, "id": 212744766, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjc0NDc2Ng==", "diff_hunk": "@@ -0,0 +1,305 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ============================================================================\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import gen_nn_ops\n+from tensorflow.python.ops import init_ops\n+from tensorflow.python.ops import logging_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.ops import variable_scope\n+from tensorflow.python.ops import variables\n+from tensorflow.python.training import optimizer\n+from tensorflow.python.training import session_run_hook\n+\n+\n+GLOBAL_VARIABLE_NAME = 'global_center_variable'\n+GRAD_VARIABLE_NAME = 'grad_variable'\n+\n+class AGNCustomGetter(object):\n+  \"\"\"Custom_getter class is used to do:\n+  1. Change trainable variables to local collection and place them at worker\n+    device\n+  2. Generate global variables(global center variables)\n+  3. Generate grad variables(gradients) which record the gradients sum\n+    and place them at worker device\n+    Notice that the class should be used with tf.replica_device_setter,\n+    so that the global center variables and global step variable can be placed\n+    at ps device.\n+  \"\"\"\n+  def __init__(self, worker_device):\n+    \"\"\"\n+      Args:\n+        worker_device: put the grad_variables on worker device\n+    \"\"\"\n+    self._worker_device = worker_device\n+    self._global_map = {}\n+    self._grad_map = {}\n+\n+  def __call__(self, getter, name, trainable, collections, *args, **kwargs):\n+    if trainable:\n+      with ops.device(self._worker_device):\n+        local_var = getter(\n+            name,\n+            trainable=True,\n+            collections=[ops.GraphKeys.LOCAL_VARIABLES],\n+            *args,\n+            **kwargs)\n+      if kwargs['reuse'] == True:\n+        return local_var\n+      global_center_variable = getter(\n+          name='%s/%s' % (GLOBAL_VARIABLE_NAME, name),\n+          trainable=False,\n+          collections=[ops.GraphKeys.GLOBAL_VARIABLES],\n+          *args,\n+          **kwargs)\n+\n+      with ops.device(self._worker_device):\n+        grad_variable = getter(\n+            name='%s/%s' % (GRAD_VARIABLE_NAME, name),\n+            trainable=False,\n+            collections=[ops.GraphKeys.LOCAL_VARIABLES],\n+            *args,\n+            **kwargs)\n+      if kwargs['partitioner'] is None:\n+        self._grad_map[local_var] = grad_variable\n+        self._global_map[local_var] = global_center_variable\n+      else:\n+        v_list = list(local_var)\n+        for i in range(len(v_list)):\n+          self._grad_map[v_list[i]] = list(grad_variable)[i]\n+          self._global_map[v_list[i]] = list(global_center_variable)[i]\n+      return local_var\n+    else:\n+      return getter(name,\n+                    trainable=trainable,\n+                    collections=collections,\n+                    *args,\n+                    **kwargs)\n+\n+class AGNOptimizer(optimizer.Optimizer):\n+  \"\"\"Wrapper that implements the Accumulated GradientNormalization algorithm.\n+  Reference:\n+    Accumulated Gradient Normalization: Joeri Hermans ACML2017\n+    https://arxiv.org/abs/1710.02368\n+  \"\"\"\n+\n+  def __init__(self,\n+               optimizer,\n+               num_worker,\n+               custom_getter,\n+               communication_period=10,\n+               use_locking=True,\n+               name='AGNOptimizer'):\n+    \"\"\"Construct a new AGN optimizer.\n+\n+    Args:\n+      optimizer: input optimizer, can be sgd/momentum/adam etc.\n+      num_worker: The number of workers\n+      custom_getter: The AGNCustomGetter\n+      communication_period: An int point value to controls the frequency\n+        of the communication between every worker and the ps.\n+      use_locking: If True use locks for update operations.\n+      name: Optional name prefix for the operations created when applying\n+        gradients. Defaults to \"AGNOptimizer\".\n+\n+    \"\"\"\n+    super(AGNOptimizer, self).__init__(use_locking, name)\n+    self._opt = optimizer\n+    self._num_worker = num_worker\n+    self._period = communication_period\n+    self._global_map = custom_getter._global_map\n+    self._grad_map = custom_getter._grad_map\n+    self._local_step = variable_scope.get_variable(\n+        initializer=0,\n+        trainable=False,\n+        collections=[ops.GraphKeys.LOCAL_VARIABLES],\n+        name='local_step')\n+    self._opt._prepare()\n+\n+  def compute_gradients(self,", "path": "tensorflow/contrib/opt/python/training/agn_optimizer.py", "position": null, "original_position": 137, "commit_id": "40aee739c3d5c7aee63020f36b83aded09044efb", "original_commit_id": "44dc83c18dfb8fff5525422e6c08a468aca4fb65", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "No need to override this method", "created_at": "2018-08-24T20:20:06Z", "updated_at": "2018-08-28T17:11:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21859#discussion_r212744766", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21859", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/212744766"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21859#discussion_r212744766"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21859"}}, "body_html": "<p>No need to override this method</p>", "body_text": "No need to override this method"}