{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5786", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5786/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5786/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5786/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5786", "id": 191018655, "node_id": "MDU6SXNzdWUxOTEwMTg2NTU=", "number": 5786, "title": "Gradients and variables was not shared in Adam optimizers when using bucketing", "user": {"login": "Syndrome777", "id": 6788909, "node_id": "MDQ6VXNlcjY3ODg5MDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/6788909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Syndrome777", "html_url": "https://github.com/Syndrome777", "followers_url": "https://api.github.com/users/Syndrome777/followers", "following_url": "https://api.github.com/users/Syndrome777/following{/other_user}", "gists_url": "https://api.github.com/users/Syndrome777/gists{/gist_id}", "starred_url": "https://api.github.com/users/Syndrome777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Syndrome777/subscriptions", "organizations_url": "https://api.github.com/users/Syndrome777/orgs", "repos_url": "https://api.github.com/users/Syndrome777/repos", "events_url": "https://api.github.com/users/Syndrome777/events{/privacy}", "received_events_url": "https://api.github.com/users/Syndrome777/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2016-11-22T14:16:01Z", "updated_at": "2018-02-07T23:38:26Z", "closed_at": "2018-02-07T23:38:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>All,</p>\n<p>I used bucketing-like technology for seq2seq task:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> For different length in encoder and decoder</span>\nmodel_map <span class=\"pl-k\">=</span> {}\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> encoder_shape:\n    <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> decoder_shape:\n        <span class=\"pl-k\">with</span> variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                 <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span> <span class=\"pl-k\">if</span> tt <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>):\n            model <span class=\"pl-k\">=</span> Seq2SeqModel()\n            model.build(encoder[:i], decoder[:j])\n            model_map[i<span class=\"pl-k\">*</span><span class=\"pl-c1\">100</span><span class=\"pl-k\">+</span>j] <span class=\"pl-k\">=</span> model</pre></div>\n<p>And get shared model's parameters:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> tf.all_variables():\n    <span class=\"pl-c1\">print</span> t.name, t.get_shape() </pre></div>\n<pre><code>Print: \nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding:0 (50000, 256)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias:0 (1600,)\n</code></pre>\n<p>Model's optimizer is like below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>every model have an optimizer</span>\nparams <span class=\"pl-k\">=</span> tf.trainable_variables()\nopt <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-3</span>)\ngradients <span class=\"pl-k\">=</span> tf.gradients(<span class=\"pl-c1\">self</span>.loss, params)\n<span class=\"pl-c1\">self</span>.optimizer <span class=\"pl-k\">=</span> opt.apply_gradients(<span class=\"pl-c1\">zip</span>(gradients, params))</pre></div>\n<p>But I find that the optimizers don't share gradient:</p>\n<pre><code>embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam:0 (50000, 256)\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_1:0 (50000, 256)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_1:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam:0 (1600,)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_1:0 (1600,)\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_2:0 (50000, 256)\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_3:0 (50000, 256)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_2:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_3:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_2:0 (1600,)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_3:0 (1600,)\n</code></pre>\n<p>With the growth of the number of buckets, the GPU memory will grow too. And meanwhile I get a larger model in tf.train.Saver.save().</p>\n<p>So is it possible to share gradient in bucketing?</p>", "body_text": "All,\nI used bucketing-like technology for seq2seq task:\n# For different length in encoder and decoder\nmodel_map = {}\nfor i in encoder_shape:\n    for j in decoder_shape:\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n                                 reuse=True if tt > 0 else None):\n            model = Seq2SeqModel()\n            model.build(encoder[:i], decoder[:j])\n            model_map[i*100+j] = model\nAnd get shared model's parameters:\nfor t in tf.all_variables():\n    print t.name, t.get_shape() \nPrint: \nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding:0 (50000, 256)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias:0 (1600,)\n\nModel's optimizer is like below:\n#every model have an optimizer\nparams = tf.trainable_variables()\nopt = tf.train.AdamOptimizer(1e-3)\ngradients = tf.gradients(self.loss, params)\nself.optimizer = opt.apply_gradients(zip(gradients, params))\nBut I find that the optimizers don't share gradient:\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam:0 (50000, 256)\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_1:0 (50000, 256)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_1:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam:0 (1600,)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_1:0 (1600,)\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_2:0 (50000, 256)\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_3:0 (50000, 256)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_2:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_3:0 (1056, 1600)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_2:0 (1600,)\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_3:0 (1600,)\n\nWith the growth of the number of buckets, the GPU memory will grow too. And meanwhile I get a larger model in tf.train.Saver.save().\nSo is it possible to share gradient in bucketing?", "body": "All,\r\n\r\nI used bucketing-like technology for seq2seq task:\r\n\r\n```python\r\n# For different length in encoder and decoder\r\nmodel_map = {}\r\nfor i in encoder_shape:\r\n    for j in decoder_shape:\r\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(),\r\n                                 reuse=True if tt > 0 else None):\r\n            model = Seq2SeqModel()\r\n            model.build(encoder[:i], decoder[:j])\r\n            model_map[i*100+j] = model\r\n```\r\nAnd get shared model's parameters:\r\n\r\n```python\r\nfor t in tf.all_variables():\r\n    print t.name, t.get_shape() \r\n```\r\n\r\n```\r\nPrint: \r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias:0 (1600,)\r\n```\r\nModel's optimizer is like below:\r\n\r\n```python\r\n#every model have an optimizer\r\nparams = tf.trainable_variables()\r\nopt = tf.train.AdamOptimizer(1e-3)\r\ngradients = tf.gradients(self.loss, params)\r\nself.optimizer = opt.apply_gradients(zip(gradients, params))\r\n```\r\nBut I find that the optimizers don't share gradient:\r\n```\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_1:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_1:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam:0 (1600,)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_1:0 (1600,)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_2:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding/Adam_3:0 (50000, 256)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_2:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix/Adam_3:0 (1056, 1600)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_2:0 (1600,)\r\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Bias/Adam_3:0 (1600,)\r\n```\r\nWith the growth of the number of buckets, the GPU memory will grow too. And meanwhile I get a larger model in tf.train.Saver.save().\r\n\r\nSo is it possible to share gradient in bucketing?\r\n\r\n\r\n\r\n\r\n\r\n"}