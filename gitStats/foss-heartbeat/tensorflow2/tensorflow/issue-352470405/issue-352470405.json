{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21755", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21755/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21755/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21755/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21755", "id": 352470405, "node_id": "MDU6SXNzdWUzNTI0NzA0MDU=", "number": 21755, "title": "Getting Shifted values of Close Price in Tensorflow training, validation and Testing. Python", "user": {"login": "JafferWilson", "id": 13446197, "node_id": "MDQ6VXNlcjEzNDQ2MTk3", "avatar_url": "https://avatars3.githubusercontent.com/u/13446197?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JafferWilson", "html_url": "https://github.com/JafferWilson", "followers_url": "https://api.github.com/users/JafferWilson/followers", "following_url": "https://api.github.com/users/JafferWilson/following{/other_user}", "gists_url": "https://api.github.com/users/JafferWilson/gists{/gist_id}", "starred_url": "https://api.github.com/users/JafferWilson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JafferWilson/subscriptions", "organizations_url": "https://api.github.com/users/JafferWilson/orgs", "repos_url": "https://api.github.com/users/JafferWilson/repos", "events_url": "https://api.github.com/users/JafferWilson/events{/privacy}", "received_events_url": "https://api.github.com/users/JafferWilson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-08-21T10:02:55Z", "updated_at": "2018-08-29T11:34:32Z", "closed_at": "2018-08-23T18:51:32Z", "author_association": "NONE", "body_html": "<p>I am trying to experiment with the Tensorflow. I am supplying the sequence of 5 out of which 4 are input and the 5th value is the output from the close price.<br>\nThe graphs are plotted according to the respective values of the output as the original and the predicted for the training, validation and Testing.</p>\n<p>But the graph shows shifted output. See the graphs:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13446197/44395357-44371b00-a557-11e8-8e22-94c4484fcbac.jpg\"><img src=\"https://user-images.githubusercontent.com/13446197/44395357-44371b00-a557-11e8-8e22-94c4484fcbac.jpg\" alt=\"file\" style=\"max-width:100%;\"></a></p>\n<p>I do not understand what is the problem as I am trying to get the prediction one step ahead of the given input.</p>\n<p>Here is the training code that I am trying to run:</p>\n<pre><code>index_in_epoch = 0;\nperm_array  = np.arange(x_train.shape[0])\nnp.random.shuffle(perm_array)\n\n# function to get the next batch\ndef get_next_batch(batch_size):\n    global index_in_epoch, x_train, perm_array   \n    start = index_in_epoch\n    index_in_epoch += batch_size\n\n    if index_in_epoch &gt; x_train.shape[0]:\n        np.random.shuffle(perm_array) # shuffle permutation array\n        start = 0 # start next epoch\n        index_in_epoch = batch_size\n\n    end = index_in_epoch\n    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n\n# parameters\nn_steps = seq_len-1 \nn_inputs = x_train.shape[2]#4\nn_neurons = 500\nn_outputs = y_train.shape[1]#4\nn_layers = 2\nlearning_rate = 0.0001\nbatch_size =10\nn_epochs = 1000#200 \ntrain_set_size = x_train.shape[0]\ntest_set_size = x_test.shape[0]\n\ntf.reset_default_graph()\n\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.float32, [None, n_outputs])\n\nlayers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \n                                 activation=tf.nn.leaky_relu, use_peepholes = True)\n         for layer in range(n_layers)]\n\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n\nstacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \nstacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\noutputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\noutputs = outputs[:,n_steps-1,:] # keep only last output of sequence\n\nloss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \ntraining_op = optimizer.minimize(loss)\n\nsaver = tf.train.Saver()\ndisplay = 40\nwith tf.Session() as sess: \n    sess.run(tf.global_variables_initializer())\n    plt.ion()\n    fig = plt.figure()\n    fig.set_size_inches(30,10)\n    ax1 = fig.add_subplot(231)\n    line1, = ax1.plot(y_train[:display],color='blue', label='train original',marker=\".\")\n    line2, = ax1.plot(y_train[:display],color='red', label='train Prediction',marker=\".\")\n    ax2 = fig.add_subplot(232)\n    line3, = ax2.plot(y_valid[:display],color='blue', label='valid original',marker=\".\")\n    line4, = ax2.plot(y_valid[:display],color='red', label='valid Prediction',marker=\".\")\n    ax3 = fig.add_subplot(233)\n    line5, = ax3.plot(y_test[:display],color='blue', label='valid original',marker=\".\")\n    line6, = ax3.plot(y_test[:display],color='red', label='valid Prediction',marker=\".\")\n    ax4 = fig.add_subplot(234)\n    candlestick2_ohlc(ax4,df_train_candle.o.values[:display],df_train_candle.h.values[:display],df_train_candle.l.values[:display],df_train_candle.c.values[:display],width=0.6)\n    ax5 = fig.add_subplot(235)\n    candlestick2_ohlc(ax5,df_valid_candle.o.values[:display],df_valid_candle.h.values[:display],df_valid_candle.l.values[:display],df_valid_candle.c.values[:display],width=0.6)\n    ax6 = fig.add_subplot(236)\n    candlestick2_ohlc(ax6,df_test_candle.o.values[:display],df_test_candle.h.values[:display],df_test_candle.l.values[:display],df_test_candle.c.values[:display],width=0.6)\n    ax1.set_title('Training')\n    ax2.set_title('Validation')\n    ax3.set_title('Testing')\n    ax1.set_xlabel('time')\n    ax2.set_xlabel('time')\n    ax3.set_xlabel('time')\n    ax1.set_ylabel(\"Train Reversal\")\n    ax2.set_ylabel(\"Valid Reversal\")\n    ax3.set_ylabel(\"Test Reversal\")\n\n    ax4.set_title('Training Candles')\n    ax4.set_xlabel('time')\n    ax4.set_ylabel(\"Train OHLC\")\n    ax5.set_title('Validation Candles')\n    ax5.set_xlabel('time')\n    ax4.set_ylabel(\"Valid OHLC\")\n    ax6.set_title('Testing Candles')\n    ax6.set_xlabel('time')\n    ax4.set_ylabel(\"Test OHLC\")\n\n\n    plt.show()\n    #if(tf.train.checkpoint_exists(tf.train.latest_checkpoint(\"modelsOHLC\"))):\n     #   saver.restore(sess, tf.train.latest_checkpoint(\"modelsOHLC\"))\n      #  print(tf.train.latest_checkpoint(\"modelsOHLC\") + \"Session Loaded for Testing\")\n    for iteration in range(int(n_epochs*train_set_size/batch_size)):\n        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \n        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \n        if iteration % int(1*train_set_size/batch_size) == 0:\n            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) \n            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \n            mse_test = loss.eval(feed_dict={X: x_test, y: y_test})\n            y_train_pred = sess.run(outputs, feed_dict={X: x_train})\n            y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\n            y_test_pred = sess.run(outputs, feed_dict={X: x_test})\n            line2.set_ydata(y_train_pred[:display])\n            line4.set_ydata(y_valid_pred[:display])\n            line6.set_ydata(y_test_pred[:display])\n            ax1.set_title(\"Training Loss: \"+str(mse_train))\n            ax2.set_title(\"Validation Loss: \"+str(mse_valid))\n            ax3.set_title(\"Testing Loss: \"+str(mse_test))\n            plt.pause(0.01)\n            print('%.2f epochs: MSE train/valid/test = %.10f/%.10f/%.10f'%(\n                iteration*batch_size/train_set_size, mse_train, mse_valid,mse_test))\n            save_path = saver.save(sess, \"modelsOHLC\\\\model\"+str(iteration)+\".ckpt\")\n</code></pre>\n<p>The input data for the above is here</p>\n<p><a href=\"https://gist.github.com/JafferWilson/c17a4d70b4aaf839b76bde13f7e32141\">Input file</a></p>\n<p>You can access the Jupyter version of the code from here: <a href=\"https://gist.github.com/JafferWilson/2f7a2374e7b5ea4f92c2edda8b9f7691\">Jupyter Version of the Code<br>\n</a><br>\nPlease guys let me know what I have missed due to which the graph is getting shifted. I am in an ambiguous situation, please let me know what I can do to correct it.</p>\n<p>I have opened a question on Stackoverflow too, here is the <a href=\"https://stackoverflow.com/questions/51945064/getting-shifted-values-of-close-price-in-tensorflow-training-validation-and-tes\" rel=\"nofollow\">link for it</a>.</p>", "body_text": "I am trying to experiment with the Tensorflow. I am supplying the sequence of 5 out of which 4 are input and the 5th value is the output from the close price.\nThe graphs are plotted according to the respective values of the output as the original and the predicted for the training, validation and Testing.\nBut the graph shows shifted output. See the graphs:\n\nI do not understand what is the problem as I am trying to get the prediction one step ahead of the given input.\nHere is the training code that I am trying to run:\nindex_in_epoch = 0;\nperm_array  = np.arange(x_train.shape[0])\nnp.random.shuffle(perm_array)\n\n# function to get the next batch\ndef get_next_batch(batch_size):\n    global index_in_epoch, x_train, perm_array   \n    start = index_in_epoch\n    index_in_epoch += batch_size\n\n    if index_in_epoch > x_train.shape[0]:\n        np.random.shuffle(perm_array) # shuffle permutation array\n        start = 0 # start next epoch\n        index_in_epoch = batch_size\n\n    end = index_in_epoch\n    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n\n# parameters\nn_steps = seq_len-1 \nn_inputs = x_train.shape[2]#4\nn_neurons = 500\nn_outputs = y_train.shape[1]#4\nn_layers = 2\nlearning_rate = 0.0001\nbatch_size =10\nn_epochs = 1000#200 \ntrain_set_size = x_train.shape[0]\ntest_set_size = x_test.shape[0]\n\ntf.reset_default_graph()\n\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.float32, [None, n_outputs])\n\nlayers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \n                                 activation=tf.nn.leaky_relu, use_peepholes = True)\n         for layer in range(n_layers)]\n\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n\nstacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \nstacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\noutputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\noutputs = outputs[:,n_steps-1,:] # keep only last output of sequence\n\nloss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \ntraining_op = optimizer.minimize(loss)\n\nsaver = tf.train.Saver()\ndisplay = 40\nwith tf.Session() as sess: \n    sess.run(tf.global_variables_initializer())\n    plt.ion()\n    fig = plt.figure()\n    fig.set_size_inches(30,10)\n    ax1 = fig.add_subplot(231)\n    line1, = ax1.plot(y_train[:display],color='blue', label='train original',marker=\".\")\n    line2, = ax1.plot(y_train[:display],color='red', label='train Prediction',marker=\".\")\n    ax2 = fig.add_subplot(232)\n    line3, = ax2.plot(y_valid[:display],color='blue', label='valid original',marker=\".\")\n    line4, = ax2.plot(y_valid[:display],color='red', label='valid Prediction',marker=\".\")\n    ax3 = fig.add_subplot(233)\n    line5, = ax3.plot(y_test[:display],color='blue', label='valid original',marker=\".\")\n    line6, = ax3.plot(y_test[:display],color='red', label='valid Prediction',marker=\".\")\n    ax4 = fig.add_subplot(234)\n    candlestick2_ohlc(ax4,df_train_candle.o.values[:display],df_train_candle.h.values[:display],df_train_candle.l.values[:display],df_train_candle.c.values[:display],width=0.6)\n    ax5 = fig.add_subplot(235)\n    candlestick2_ohlc(ax5,df_valid_candle.o.values[:display],df_valid_candle.h.values[:display],df_valid_candle.l.values[:display],df_valid_candle.c.values[:display],width=0.6)\n    ax6 = fig.add_subplot(236)\n    candlestick2_ohlc(ax6,df_test_candle.o.values[:display],df_test_candle.h.values[:display],df_test_candle.l.values[:display],df_test_candle.c.values[:display],width=0.6)\n    ax1.set_title('Training')\n    ax2.set_title('Validation')\n    ax3.set_title('Testing')\n    ax1.set_xlabel('time')\n    ax2.set_xlabel('time')\n    ax3.set_xlabel('time')\n    ax1.set_ylabel(\"Train Reversal\")\n    ax2.set_ylabel(\"Valid Reversal\")\n    ax3.set_ylabel(\"Test Reversal\")\n\n    ax4.set_title('Training Candles')\n    ax4.set_xlabel('time')\n    ax4.set_ylabel(\"Train OHLC\")\n    ax5.set_title('Validation Candles')\n    ax5.set_xlabel('time')\n    ax4.set_ylabel(\"Valid OHLC\")\n    ax6.set_title('Testing Candles')\n    ax6.set_xlabel('time')\n    ax4.set_ylabel(\"Test OHLC\")\n\n\n    plt.show()\n    #if(tf.train.checkpoint_exists(tf.train.latest_checkpoint(\"modelsOHLC\"))):\n     #   saver.restore(sess, tf.train.latest_checkpoint(\"modelsOHLC\"))\n      #  print(tf.train.latest_checkpoint(\"modelsOHLC\") + \"Session Loaded for Testing\")\n    for iteration in range(int(n_epochs*train_set_size/batch_size)):\n        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \n        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \n        if iteration % int(1*train_set_size/batch_size) == 0:\n            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) \n            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \n            mse_test = loss.eval(feed_dict={X: x_test, y: y_test})\n            y_train_pred = sess.run(outputs, feed_dict={X: x_train})\n            y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\n            y_test_pred = sess.run(outputs, feed_dict={X: x_test})\n            line2.set_ydata(y_train_pred[:display])\n            line4.set_ydata(y_valid_pred[:display])\n            line6.set_ydata(y_test_pred[:display])\n            ax1.set_title(\"Training Loss: \"+str(mse_train))\n            ax2.set_title(\"Validation Loss: \"+str(mse_valid))\n            ax3.set_title(\"Testing Loss: \"+str(mse_test))\n            plt.pause(0.01)\n            print('%.2f epochs: MSE train/valid/test = %.10f/%.10f/%.10f'%(\n                iteration*batch_size/train_set_size, mse_train, mse_valid,mse_test))\n            save_path = saver.save(sess, \"modelsOHLC\\\\model\"+str(iteration)+\".ckpt\")\n\nThe input data for the above is here\nInput file\nYou can access the Jupyter version of the code from here: Jupyter Version of the Code\n\nPlease guys let me know what I have missed due to which the graph is getting shifted. I am in an ambiguous situation, please let me know what I can do to correct it.\nI have opened a question on Stackoverflow too, here is the link for it.", "body": "I am trying to experiment with the Tensorflow. I am supplying the sequence of 5 out of which 4 are input and the 5th value is the output from the close price.\r\nThe graphs are plotted according to the respective values of the output as the original and the predicted for the training, validation and Testing.\r\n\r\nBut the graph shows shifted output. See the graphs:\r\n![file](https://user-images.githubusercontent.com/13446197/44395357-44371b00-a557-11e8-8e22-94c4484fcbac.jpg)\r\n\r\nI do not understand what is the problem as I am trying to get the prediction one step ahead of the given input.\r\n\r\nHere is the training code that I am trying to run:\r\n\r\n```\r\nindex_in_epoch = 0;\r\nperm_array  = np.arange(x_train.shape[0])\r\nnp.random.shuffle(perm_array)\r\n\r\n# function to get the next batch\r\ndef get_next_batch(batch_size):\r\n    global index_in_epoch, x_train, perm_array   \r\n    start = index_in_epoch\r\n    index_in_epoch += batch_size\r\n\r\n    if index_in_epoch > x_train.shape[0]:\r\n        np.random.shuffle(perm_array) # shuffle permutation array\r\n        start = 0 # start next epoch\r\n        index_in_epoch = batch_size\r\n\r\n    end = index_in_epoch\r\n    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\r\n\r\n# parameters\r\nn_steps = seq_len-1 \r\nn_inputs = x_train.shape[2]#4\r\nn_neurons = 500\r\nn_outputs = y_train.shape[1]#4\r\nn_layers = 2\r\nlearning_rate = 0.0001\r\nbatch_size =10\r\nn_epochs = 1000#200 \r\ntrain_set_size = x_train.shape[0]\r\ntest_set_size = x_test.shape[0]\r\n\r\ntf.reset_default_graph()\r\n\r\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\r\ny = tf.placeholder(tf.float32, [None, n_outputs])\r\n\r\nlayers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \r\n                                 activation=tf.nn.leaky_relu, use_peepholes = True)\r\n         for layer in range(n_layers)]\r\n\r\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\r\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\r\n\r\nstacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \r\nstacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\r\noutputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\r\noutputs = outputs[:,n_steps-1,:] # keep only last output of sequence\r\n\r\nloss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \r\ntraining_op = optimizer.minimize(loss)\r\n\r\nsaver = tf.train.Saver()\r\ndisplay = 40\r\nwith tf.Session() as sess: \r\n    sess.run(tf.global_variables_initializer())\r\n    plt.ion()\r\n    fig = plt.figure()\r\n    fig.set_size_inches(30,10)\r\n    ax1 = fig.add_subplot(231)\r\n    line1, = ax1.plot(y_train[:display],color='blue', label='train original',marker=\".\")\r\n    line2, = ax1.plot(y_train[:display],color='red', label='train Prediction',marker=\".\")\r\n    ax2 = fig.add_subplot(232)\r\n    line3, = ax2.plot(y_valid[:display],color='blue', label='valid original',marker=\".\")\r\n    line4, = ax2.plot(y_valid[:display],color='red', label='valid Prediction',marker=\".\")\r\n    ax3 = fig.add_subplot(233)\r\n    line5, = ax3.plot(y_test[:display],color='blue', label='valid original',marker=\".\")\r\n    line6, = ax3.plot(y_test[:display],color='red', label='valid Prediction',marker=\".\")\r\n    ax4 = fig.add_subplot(234)\r\n    candlestick2_ohlc(ax4,df_train_candle.o.values[:display],df_train_candle.h.values[:display],df_train_candle.l.values[:display],df_train_candle.c.values[:display],width=0.6)\r\n    ax5 = fig.add_subplot(235)\r\n    candlestick2_ohlc(ax5,df_valid_candle.o.values[:display],df_valid_candle.h.values[:display],df_valid_candle.l.values[:display],df_valid_candle.c.values[:display],width=0.6)\r\n    ax6 = fig.add_subplot(236)\r\n    candlestick2_ohlc(ax6,df_test_candle.o.values[:display],df_test_candle.h.values[:display],df_test_candle.l.values[:display],df_test_candle.c.values[:display],width=0.6)\r\n    ax1.set_title('Training')\r\n    ax2.set_title('Validation')\r\n    ax3.set_title('Testing')\r\n    ax1.set_xlabel('time')\r\n    ax2.set_xlabel('time')\r\n    ax3.set_xlabel('time')\r\n    ax1.set_ylabel(\"Train Reversal\")\r\n    ax2.set_ylabel(\"Valid Reversal\")\r\n    ax3.set_ylabel(\"Test Reversal\")\r\n\r\n    ax4.set_title('Training Candles')\r\n    ax4.set_xlabel('time')\r\n    ax4.set_ylabel(\"Train OHLC\")\r\n    ax5.set_title('Validation Candles')\r\n    ax5.set_xlabel('time')\r\n    ax4.set_ylabel(\"Valid OHLC\")\r\n    ax6.set_title('Testing Candles')\r\n    ax6.set_xlabel('time')\r\n    ax4.set_ylabel(\"Test OHLC\")\r\n\r\n\r\n    plt.show()\r\n    #if(tf.train.checkpoint_exists(tf.train.latest_checkpoint(\"modelsOHLC\"))):\r\n     #   saver.restore(sess, tf.train.latest_checkpoint(\"modelsOHLC\"))\r\n      #  print(tf.train.latest_checkpoint(\"modelsOHLC\") + \"Session Loaded for Testing\")\r\n    for iteration in range(int(n_epochs*train_set_size/batch_size)):\r\n        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \r\n        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \r\n        if iteration % int(1*train_set_size/batch_size) == 0:\r\n            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) \r\n            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \r\n            mse_test = loss.eval(feed_dict={X: x_test, y: y_test})\r\n            y_train_pred = sess.run(outputs, feed_dict={X: x_train})\r\n            y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\r\n            y_test_pred = sess.run(outputs, feed_dict={X: x_test})\r\n            line2.set_ydata(y_train_pred[:display])\r\n            line4.set_ydata(y_valid_pred[:display])\r\n            line6.set_ydata(y_test_pred[:display])\r\n            ax1.set_title(\"Training Loss: \"+str(mse_train))\r\n            ax2.set_title(\"Validation Loss: \"+str(mse_valid))\r\n            ax3.set_title(\"Testing Loss: \"+str(mse_test))\r\n            plt.pause(0.01)\r\n            print('%.2f epochs: MSE train/valid/test = %.10f/%.10f/%.10f'%(\r\n                iteration*batch_size/train_set_size, mse_train, mse_valid,mse_test))\r\n            save_path = saver.save(sess, \"modelsOHLC\\\\model\"+str(iteration)+\".ckpt\")\r\n```\r\n\r\nThe input data for the above is here\r\n\r\n[Input file](https://gist.github.com/JafferWilson/c17a4d70b4aaf839b76bde13f7e32141)\r\n\r\nYou can access the Jupyter version of the code from here: [Jupyter Version of the Code\r\n](https://gist.github.com/JafferWilson/2f7a2374e7b5ea4f92c2edda8b9f7691)\r\nPlease guys let me know what I have missed due to which the graph is getting shifted. I am in an ambiguous situation, please let me know what I can do to correct it.   \r\n\r\nI have opened a question on Stackoverflow too, here is the [link for it](https://stackoverflow.com/questions/51945064/getting-shifted-values-of-close-price-in-tensorflow-training-validation-and-tes)."}