{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10845", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10845/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10845/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10845/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10845", "id": 237200816, "node_id": "MDU6SXNzdWUyMzcyMDA4MTY=", "number": 10845, "title": "Keras Dropout layer changes results with dropout=0.0", "user": {"login": "ribx", "id": 4598953, "node_id": "MDQ6VXNlcjQ1OTg5NTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4598953?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ribx", "html_url": "https://github.com/ribx", "followers_url": "https://api.github.com/users/ribx/followers", "following_url": "https://api.github.com/users/ribx/following{/other_user}", "gists_url": "https://api.github.com/users/ribx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ribx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ribx/subscriptions", "organizations_url": "https://api.github.com/users/ribx/orgs", "repos_url": "https://api.github.com/users/ribx/repos", "events_url": "https://api.github.com/users/ribx/events{/privacy}", "received_events_url": "https://api.github.com/users/ribx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-20T12:59:07Z", "updated_at": "2017-06-28T10:12:18Z", "closed_at": "2017-06-28T10:12:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am using the current version pypi 1.2.0, but also found this \"problem\" in the master I compiled about two weeks ago. I am running Gentoo linux and tensorflow is installed in an virtualenv.</p>\n<p>Maybe I am misunderstanding the concept of a dropout layer, but when I add a Dropout-Layer with 0% dropout, it still alters my results:</p>\n<p>from reuters classification example:</p>\n<pre><code>model = Sequential()\nmodel.add(Dense(128, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.0))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n</code></pre>\n<p>After 3 epochs I get:</p>\n<pre><code>Epoch 3/3\n  32/8083 [..............................] - ETA: 0s - loss: 0.7321 - acc: 0.8125\n 512/8083 [&gt;.............................] - ETA: 0s - loss: 0.8597 - acc: 0.8145\n 928/8083 [==&gt;...........................] - ETA: 0s - loss: 0.8689 - acc: 0.8017\n1344/8083 [===&gt;..........................] - ETA: 0s - loss: 0.9041 - acc: 0.7954\n1792/8083 [=====&gt;........................] - ETA: 0s - loss: 0.8979 - acc: 0.7969\n2304/8083 [=======&gt;......................] - ETA: 0s - loss: 0.8896 - acc: 0.7969\n2784/8083 [=========&gt;....................] - ETA: 0s - loss: 0.8764 - acc: 0.7989\n3168/8083 [==========&gt;...................] - ETA: 0s - loss: 0.8678 - acc: 0.8018\n3648/8083 [============&gt;.................] - ETA: 0s - loss: 0.8666 - acc: 0.8021\n4096/8083 [==============&gt;...............] - ETA: 0s - loss: 0.8656 - acc: 0.8013\n4576/8083 [===============&gt;..............] - ETA: 0s - loss: 0.8536 - acc: 0.8018\n5120/8083 [==================&gt;...........] - ETA: 0s - loss: 0.8409 - acc: 0.8033\n5664/8083 [====================&gt;.........] - ETA: 0s - loss: 0.8369 - acc: 0.8054\n6048/8083 [=====================&gt;........] - ETA: 0s - loss: 0.8385 - acc: 0.8042\n6592/8083 [=======================&gt;......] - ETA: 0s - loss: 0.8431 - acc: 0.8025\n7136/8083 [=========================&gt;....] - ETA: 0s - loss: 0.8448 - acc: 0.8028\n7680/8083 [===========================&gt;..] - ETA: 0s - loss: 0.8490 - acc: 0.8025\n8083/8083 [==============================] - 0s - loss: 0.8473 - acc: 0.8032 - val_loss: 0.9853 - val_acc: 0.7920\n  32/2246 [..............................] - ETA: 0s\n1568/2246 [===================&gt;..........] - ETA: 0s\nTest score: 0.934106262688\nTest accuracy: 0.777382012467\n</code></pre>\n<p>and without a dropout layer:</p>\n<pre><code>Epoch 3/3\n  32/8083 [..............................] - ETA: 0s - loss: 0.5419 - acc: 0.8750\n 544/8083 [=&gt;............................] - ETA: 0s - loss: 0.4974 - acc: 0.8842\n1088/8083 [===&gt;..........................] - ETA: 0s - loss: 0.5429 - acc: 0.8722\n1664/8083 [=====&gt;........................] - ETA: 0s - loss: 0.5568 - acc: 0.8762\n2208/8083 [=======&gt;......................] - ETA: 0s - loss: 0.5523 - acc: 0.8773\n2752/8083 [=========&gt;....................] - ETA: 0s - loss: 0.5494 - acc: 0.8790\n3296/8083 [===========&gt;..................] - ETA: 0s - loss: 0.5437 - acc: 0.8799\n3808/8083 [=============&gt;................] - ETA: 0s - loss: 0.5420 - acc: 0.8792\n4352/8083 [===============&gt;..............] - ETA: 0s - loss: 0.5446 - acc: 0.8750\n4896/8083 [=================&gt;............] - ETA: 0s - loss: 0.5405 - acc: 0.8754\n5440/8083 [===================&gt;..........] - ETA: 0s - loss: 0.5381 - acc: 0.8756\n5984/8083 [=====================&gt;........] - ETA: 0s - loss: 0.5392 - acc: 0.8755\n6528/8083 [=======================&gt;......] - ETA: 0s - loss: 0.5459 - acc: 0.8738\n7104/8083 [=========================&gt;....] - ETA: 0s - loss: 0.5482 - acc: 0.8740\n7648/8083 [===========================&gt;..] - ETA: 0s - loss: 0.5527 - acc: 0.8730\n8083/8083 [==============================] - 0s - loss: 0.5525 - acc: 0.8727 - val_loss: 0.9100 - val_acc: 0.7898\n  32/2246 [..............................] - ETA: 0s\n1664/2246 [=====================&gt;........] - ETA: 0s\nTest score: 0.883166806993\nTest accuracy: 0.792074799644\n</code></pre>\n<p>While the validation and test results are quite similar, the model without dropout overfits much more.</p>\n<p>Also, if I set dropout to 1.0, the model should not be able to learn anything (as everything is dropped):</p>\n<pre><code>Epoch 3/3\n  32/8083 [..............................] - ETA: 0s - loss: 0.8313 - acc: 0.7812\n 576/8083 [=&gt;............................] - ETA: 0s - loss: 0.7561 - acc: 0.8351\n1120/8083 [===&gt;..........................] - ETA: 0s - loss: 0.8669 - acc: 0.8000\n1632/8083 [=====&gt;........................] - ETA: 0s - loss: 0.8805 - acc: 0.7978\n2176/8083 [=======&gt;......................] - ETA: 0s - loss: 0.8780 - acc: 0.7973\n2720/8083 [=========&gt;....................] - ETA: 0s - loss: 0.8742 - acc: 0.7978\n3264/8083 [===========&gt;..................] - ETA: 0s - loss: 0.8693 - acc: 0.7990\n3808/8083 [=============&gt;................] - ETA: 0s - loss: 0.8616 - acc: 0.7996\n4352/8083 [===============&gt;..............] - ETA: 0s - loss: 0.8565 - acc: 0.8001\n4896/8083 [=================&gt;............] - ETA: 0s - loss: 0.8480 - acc: 0.8025\n5440/8083 [===================&gt;..........] - ETA: 0s - loss: 0.8499 - acc: 0.8007\n5984/8083 [=====================&gt;........] - ETA: 0s - loss: 0.8492 - acc: 0.8025\n6560/8083 [=======================&gt;......] - ETA: 0s - loss: 0.8498 - acc: 0.8023\n7136/8083 [=========================&gt;....] - ETA: 0s - loss: 0.8505 - acc: 0.8014\n7648/8083 [===========================&gt;..] - ETA: 0s - loss: 0.8511 - acc: 0.8010\n8083/8083 [==============================] - 0s - loss: 0.8484 - acc: 0.8021 - val_loss: 0.9544 - val_acc: 0.7909\n  32/2246 [..............................] - ETA: 0s\n1536/2246 [===================&gt;..........] - ETA: 0s\nTest score: 0.92991881655\nTest accuracy: 0.780498664292\n</code></pre>", "body_text": "I am using the current version pypi 1.2.0, but also found this \"problem\" in the master I compiled about two weeks ago. I am running Gentoo linux and tensorflow is installed in an virtualenv.\nMaybe I am misunderstanding the concept of a dropout layer, but when I add a Dropout-Layer with 0% dropout, it still alters my results:\nfrom reuters classification example:\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.0))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nAfter 3 epochs I get:\nEpoch 3/3\n  32/8083 [..............................] - ETA: 0s - loss: 0.7321 - acc: 0.8125\n 512/8083 [>.............................] - ETA: 0s - loss: 0.8597 - acc: 0.8145\n 928/8083 [==>...........................] - ETA: 0s - loss: 0.8689 - acc: 0.8017\n1344/8083 [===>..........................] - ETA: 0s - loss: 0.9041 - acc: 0.7954\n1792/8083 [=====>........................] - ETA: 0s - loss: 0.8979 - acc: 0.7969\n2304/8083 [=======>......................] - ETA: 0s - loss: 0.8896 - acc: 0.7969\n2784/8083 [=========>....................] - ETA: 0s - loss: 0.8764 - acc: 0.7989\n3168/8083 [==========>...................] - ETA: 0s - loss: 0.8678 - acc: 0.8018\n3648/8083 [============>.................] - ETA: 0s - loss: 0.8666 - acc: 0.8021\n4096/8083 [==============>...............] - ETA: 0s - loss: 0.8656 - acc: 0.8013\n4576/8083 [===============>..............] - ETA: 0s - loss: 0.8536 - acc: 0.8018\n5120/8083 [==================>...........] - ETA: 0s - loss: 0.8409 - acc: 0.8033\n5664/8083 [====================>.........] - ETA: 0s - loss: 0.8369 - acc: 0.8054\n6048/8083 [=====================>........] - ETA: 0s - loss: 0.8385 - acc: 0.8042\n6592/8083 [=======================>......] - ETA: 0s - loss: 0.8431 - acc: 0.8025\n7136/8083 [=========================>....] - ETA: 0s - loss: 0.8448 - acc: 0.8028\n7680/8083 [===========================>..] - ETA: 0s - loss: 0.8490 - acc: 0.8025\n8083/8083 [==============================] - 0s - loss: 0.8473 - acc: 0.8032 - val_loss: 0.9853 - val_acc: 0.7920\n  32/2246 [..............................] - ETA: 0s\n1568/2246 [===================>..........] - ETA: 0s\nTest score: 0.934106262688\nTest accuracy: 0.777382012467\n\nand without a dropout layer:\nEpoch 3/3\n  32/8083 [..............................] - ETA: 0s - loss: 0.5419 - acc: 0.8750\n 544/8083 [=>............................] - ETA: 0s - loss: 0.4974 - acc: 0.8842\n1088/8083 [===>..........................] - ETA: 0s - loss: 0.5429 - acc: 0.8722\n1664/8083 [=====>........................] - ETA: 0s - loss: 0.5568 - acc: 0.8762\n2208/8083 [=======>......................] - ETA: 0s - loss: 0.5523 - acc: 0.8773\n2752/8083 [=========>....................] - ETA: 0s - loss: 0.5494 - acc: 0.8790\n3296/8083 [===========>..................] - ETA: 0s - loss: 0.5437 - acc: 0.8799\n3808/8083 [=============>................] - ETA: 0s - loss: 0.5420 - acc: 0.8792\n4352/8083 [===============>..............] - ETA: 0s - loss: 0.5446 - acc: 0.8750\n4896/8083 [=================>............] - ETA: 0s - loss: 0.5405 - acc: 0.8754\n5440/8083 [===================>..........] - ETA: 0s - loss: 0.5381 - acc: 0.8756\n5984/8083 [=====================>........] - ETA: 0s - loss: 0.5392 - acc: 0.8755\n6528/8083 [=======================>......] - ETA: 0s - loss: 0.5459 - acc: 0.8738\n7104/8083 [=========================>....] - ETA: 0s - loss: 0.5482 - acc: 0.8740\n7648/8083 [===========================>..] - ETA: 0s - loss: 0.5527 - acc: 0.8730\n8083/8083 [==============================] - 0s - loss: 0.5525 - acc: 0.8727 - val_loss: 0.9100 - val_acc: 0.7898\n  32/2246 [..............................] - ETA: 0s\n1664/2246 [=====================>........] - ETA: 0s\nTest score: 0.883166806993\nTest accuracy: 0.792074799644\n\nWhile the validation and test results are quite similar, the model without dropout overfits much more.\nAlso, if I set dropout to 1.0, the model should not be able to learn anything (as everything is dropped):\nEpoch 3/3\n  32/8083 [..............................] - ETA: 0s - loss: 0.8313 - acc: 0.7812\n 576/8083 [=>............................] - ETA: 0s - loss: 0.7561 - acc: 0.8351\n1120/8083 [===>..........................] - ETA: 0s - loss: 0.8669 - acc: 0.8000\n1632/8083 [=====>........................] - ETA: 0s - loss: 0.8805 - acc: 0.7978\n2176/8083 [=======>......................] - ETA: 0s - loss: 0.8780 - acc: 0.7973\n2720/8083 [=========>....................] - ETA: 0s - loss: 0.8742 - acc: 0.7978\n3264/8083 [===========>..................] - ETA: 0s - loss: 0.8693 - acc: 0.7990\n3808/8083 [=============>................] - ETA: 0s - loss: 0.8616 - acc: 0.7996\n4352/8083 [===============>..............] - ETA: 0s - loss: 0.8565 - acc: 0.8001\n4896/8083 [=================>............] - ETA: 0s - loss: 0.8480 - acc: 0.8025\n5440/8083 [===================>..........] - ETA: 0s - loss: 0.8499 - acc: 0.8007\n5984/8083 [=====================>........] - ETA: 0s - loss: 0.8492 - acc: 0.8025\n6560/8083 [=======================>......] - ETA: 0s - loss: 0.8498 - acc: 0.8023\n7136/8083 [=========================>....] - ETA: 0s - loss: 0.8505 - acc: 0.8014\n7648/8083 [===========================>..] - ETA: 0s - loss: 0.8511 - acc: 0.8010\n8083/8083 [==============================] - 0s - loss: 0.8484 - acc: 0.8021 - val_loss: 0.9544 - val_acc: 0.7909\n  32/2246 [..............................] - ETA: 0s\n1536/2246 [===================>..........] - ETA: 0s\nTest score: 0.92991881655\nTest accuracy: 0.780498664292", "body": "I am using the current version pypi 1.2.0, but also found this \"problem\" in the master I compiled about two weeks ago. I am running Gentoo linux and tensorflow is installed in an virtualenv.\r\n\r\nMaybe I am misunderstanding the concept of a dropout layer, but when I add a Dropout-Layer with 0% dropout, it still alters my results:\r\n\r\nfrom reuters classification example:\r\n```\r\nmodel = Sequential()\r\nmodel.add(Dense(128, input_shape=(max_words,)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.0))\r\nmodel.add(Dense(num_classes))\r\nmodel.add(Activation('softmax'))\r\n```\r\nAfter 3 epochs I get:\r\n```\r\nEpoch 3/3\r\n  32/8083 [..............................] - ETA: 0s - loss: 0.7321 - acc: 0.8125\r\n 512/8083 [>.............................] - ETA: 0s - loss: 0.8597 - acc: 0.8145\r\n 928/8083 [==>...........................] - ETA: 0s - loss: 0.8689 - acc: 0.8017\r\n1344/8083 [===>..........................] - ETA: 0s - loss: 0.9041 - acc: 0.7954\r\n1792/8083 [=====>........................] - ETA: 0s - loss: 0.8979 - acc: 0.7969\r\n2304/8083 [=======>......................] - ETA: 0s - loss: 0.8896 - acc: 0.7969\r\n2784/8083 [=========>....................] - ETA: 0s - loss: 0.8764 - acc: 0.7989\r\n3168/8083 [==========>...................] - ETA: 0s - loss: 0.8678 - acc: 0.8018\r\n3648/8083 [============>.................] - ETA: 0s - loss: 0.8666 - acc: 0.8021\r\n4096/8083 [==============>...............] - ETA: 0s - loss: 0.8656 - acc: 0.8013\r\n4576/8083 [===============>..............] - ETA: 0s - loss: 0.8536 - acc: 0.8018\r\n5120/8083 [==================>...........] - ETA: 0s - loss: 0.8409 - acc: 0.8033\r\n5664/8083 [====================>.........] - ETA: 0s - loss: 0.8369 - acc: 0.8054\r\n6048/8083 [=====================>........] - ETA: 0s - loss: 0.8385 - acc: 0.8042\r\n6592/8083 [=======================>......] - ETA: 0s - loss: 0.8431 - acc: 0.8025\r\n7136/8083 [=========================>....] - ETA: 0s - loss: 0.8448 - acc: 0.8028\r\n7680/8083 [===========================>..] - ETA: 0s - loss: 0.8490 - acc: 0.8025\r\n8083/8083 [==============================] - 0s - loss: 0.8473 - acc: 0.8032 - val_loss: 0.9853 - val_acc: 0.7920\r\n  32/2246 [..............................] - ETA: 0s\r\n1568/2246 [===================>..........] - ETA: 0s\r\nTest score: 0.934106262688\r\nTest accuracy: 0.777382012467\r\n```\r\nand without a dropout layer:\r\n```\r\nEpoch 3/3\r\n  32/8083 [..............................] - ETA: 0s - loss: 0.5419 - acc: 0.8750\r\n 544/8083 [=>............................] - ETA: 0s - loss: 0.4974 - acc: 0.8842\r\n1088/8083 [===>..........................] - ETA: 0s - loss: 0.5429 - acc: 0.8722\r\n1664/8083 [=====>........................] - ETA: 0s - loss: 0.5568 - acc: 0.8762\r\n2208/8083 [=======>......................] - ETA: 0s - loss: 0.5523 - acc: 0.8773\r\n2752/8083 [=========>....................] - ETA: 0s - loss: 0.5494 - acc: 0.8790\r\n3296/8083 [===========>..................] - ETA: 0s - loss: 0.5437 - acc: 0.8799\r\n3808/8083 [=============>................] - ETA: 0s - loss: 0.5420 - acc: 0.8792\r\n4352/8083 [===============>..............] - ETA: 0s - loss: 0.5446 - acc: 0.8750\r\n4896/8083 [=================>............] - ETA: 0s - loss: 0.5405 - acc: 0.8754\r\n5440/8083 [===================>..........] - ETA: 0s - loss: 0.5381 - acc: 0.8756\r\n5984/8083 [=====================>........] - ETA: 0s - loss: 0.5392 - acc: 0.8755\r\n6528/8083 [=======================>......] - ETA: 0s - loss: 0.5459 - acc: 0.8738\r\n7104/8083 [=========================>....] - ETA: 0s - loss: 0.5482 - acc: 0.8740\r\n7648/8083 [===========================>..] - ETA: 0s - loss: 0.5527 - acc: 0.8730\r\n8083/8083 [==============================] - 0s - loss: 0.5525 - acc: 0.8727 - val_loss: 0.9100 - val_acc: 0.7898\r\n  32/2246 [..............................] - ETA: 0s\r\n1664/2246 [=====================>........] - ETA: 0s\r\nTest score: 0.883166806993\r\nTest accuracy: 0.792074799644\r\n```\r\n\r\nWhile the validation and test results are quite similar, the model without dropout overfits much more. \r\n\r\nAlso, if I set dropout to 1.0, the model should not be able to learn anything (as everything is dropped):\r\n```\r\nEpoch 3/3\r\n  32/8083 [..............................] - ETA: 0s - loss: 0.8313 - acc: 0.7812\r\n 576/8083 [=>............................] - ETA: 0s - loss: 0.7561 - acc: 0.8351\r\n1120/8083 [===>..........................] - ETA: 0s - loss: 0.8669 - acc: 0.8000\r\n1632/8083 [=====>........................] - ETA: 0s - loss: 0.8805 - acc: 0.7978\r\n2176/8083 [=======>......................] - ETA: 0s - loss: 0.8780 - acc: 0.7973\r\n2720/8083 [=========>....................] - ETA: 0s - loss: 0.8742 - acc: 0.7978\r\n3264/8083 [===========>..................] - ETA: 0s - loss: 0.8693 - acc: 0.7990\r\n3808/8083 [=============>................] - ETA: 0s - loss: 0.8616 - acc: 0.7996\r\n4352/8083 [===============>..............] - ETA: 0s - loss: 0.8565 - acc: 0.8001\r\n4896/8083 [=================>............] - ETA: 0s - loss: 0.8480 - acc: 0.8025\r\n5440/8083 [===================>..........] - ETA: 0s - loss: 0.8499 - acc: 0.8007\r\n5984/8083 [=====================>........] - ETA: 0s - loss: 0.8492 - acc: 0.8025\r\n6560/8083 [=======================>......] - ETA: 0s - loss: 0.8498 - acc: 0.8023\r\n7136/8083 [=========================>....] - ETA: 0s - loss: 0.8505 - acc: 0.8014\r\n7648/8083 [===========================>..] - ETA: 0s - loss: 0.8511 - acc: 0.8010\r\n8083/8083 [==============================] - 0s - loss: 0.8484 - acc: 0.8021 - val_loss: 0.9544 - val_acc: 0.7909\r\n  32/2246 [..............................] - ETA: 0s\r\n1536/2246 [===================>..........] - ETA: 0s\r\nTest score: 0.92991881655\r\nTest accuracy: 0.780498664292\r\n```"}