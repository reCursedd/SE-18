{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22581", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22581/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22581/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22581/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22581", "id": 364741874, "node_id": "MDU6SXNzdWUzNjQ3NDE4NzQ=", "number": 22581, "title": "Calling `tf.image.non_max_suppression()` in parallel `tf.while_loop()` causes crash", "user": {"login": "hyabe", "id": 34485234, "node_id": "MDQ6VXNlcjM0NDg1MjM0", "avatar_url": "https://avatars3.githubusercontent.com/u/34485234?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hyabe", "html_url": "https://github.com/hyabe", "followers_url": "https://api.github.com/users/hyabe/followers", "following_url": "https://api.github.com/users/hyabe/following{/other_user}", "gists_url": "https://api.github.com/users/hyabe/gists{/gist_id}", "starred_url": "https://api.github.com/users/hyabe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hyabe/subscriptions", "organizations_url": "https://api.github.com/users/hyabe/orgs", "repos_url": "https://api.github.com/users/hyabe/repos", "events_url": "https://api.github.com/users/hyabe/events{/privacy}", "received_events_url": "https://api.github.com/users/hyabe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-09-28T05:01:36Z", "updated_at": "2018-10-23T02:10:34Z", "closed_at": "2018-10-16T20:42:45Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: YES</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: CentOS 7.2 (and also Windows 10)</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (via <code>pip install tensorflow-gpu</code>)</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.11.0-0-gc19e29306c 1.11.0</li>\n<li><strong>Python version</strong>: 3.6.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7.1.4</li>\n<li><strong>GPU model and memory</strong>: NVIDIA GeForce GTX 1070 (8 GiB)</li>\n<li><strong>Exact command to reproduce</strong>: <code>python nmsstest.py</code>; see below for the content of <code>nmstest.py</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The code in the following section, which calls <code>tf.image.non_max_suppression()</code> in <code>tf.while_loop()</code> many times, crashes abnormally.<br>\nCrash reason (and loop count) varies from time to time, for example:</p>\n<ul>\n<li><code>F tensorflow/core/common_runtime/bfc_allocator.cc:384] Check failed: h != kInvalidChunkHandle</code> at loop <code>i == 140</code></li>\n<li><code>F tensorflow/core/common_runtime/bfc_allocator.cc:462] Check failed: c-&gt;in_use() &amp;&amp; (c-&gt;bin_num == kInvalidBinNum)</code> at loop <code>i == 76</code></li>\n<li><code>Bus error</code> at loop <code>i == 34</code></li>\n<li>heap corruption reported by libc (see the following section) at loop <code>i == 35</code></li>\n<li>sometimes it crashes sliently without any logs (on Windows)</li>\n</ul>\n<p>I notice that:</p>\n<ul>\n<li>it's since TensorFlow 1.11.0rc0; TF 1.10.1 was okay</li>\n<li>it also reproduces on Windows</li>\n<li>it also reproduces on CPU version (<code>pip install tensorflow</code>)</li>\n<li>it does <strong>not</strong> reproduce if <code>num_threads=1</code>; calling <code>tf.image.non_max_suppression()</code> in parallel seems the trigger</li>\n<li>even when I gave a fixed seed to <code>tf.random_uniform()</code>, crash cause varied</li>\n</ul>\n<h3>Source code / logs</h3>\n<p>The code to reproduce the problem is as follows:</p>\n<pre><code>import tensorflow as tf\n\nif __name__ == '__main__':\n    # crashes iff num_threads &gt; 1 on TensorFlow &gt;= 1.11.rc0\n    num_threads = 10\n\n    top_k = 1\n    batch_size = 32\n    num_boxes = 10000\n    boxes_op = tf.random_uniform((batch_size,num_boxes,4), 0, 1)\n    scores_op = tf.random_uniform((batch_size,num_boxes), 0, 1)\n    indices_op = tf.while_loop(\n        (lambda b, ta: True),\n        (lambda b, ta: (b+1, ta.write(b, tf.image.non_max_suppression(boxes_op[b], scores_op[b], top_k, iou_threshold=0.3)))),\n        (tf.constant(0),\n         tf.TensorArray(tf.int32, size=batch_size, infer_shape=False, element_shape=(top_k,))),\n        back_prop=False,\n        parallel_iterations=num_threads,\n        maximum_iterations=batch_size)[1].stack()\n\n    with tf.Session() as session:\n        for i in range(1000):\n            indices = session.run(indices_op)\n            print(f'#{i}: {indices.shape}')\n</code></pre>\n<pre><code>$ python nmstest.py\n2018-09-28 13:18:42.760545: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-09-28 13:18:43.123328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-28 13:18:43.124208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\npciBusID: 0000:01:00.0\ntotalMemory: 7.93GiB freeMemory: 7.83GiB\n2018-09-28 13:18:43.124232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-09-28 13:18:43.351813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-28 13:18:43.351847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-09-28 13:18:43.351859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-09-28 13:18:43.352078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7558 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\n#0: (32, 1)\n#1: (32, 1)\n#2: (32, 1)\n#3: (32, 1)\n#4: (32, 1)\n#5: (32, 1)\n#6: (32, 1)\n#7: (32, 1)\n#8: (32, 1)\n#9: (32, 1)\n#10: (32, 1)\n#11: (32, 1)\n#12: (32, 1)\n#13: (32, 1)\n#14: (32, 1)\n#15: (32, 1)\n#16: (32, 1)\n#17: (32, 1)\n#18: (32, 1)\n#19: (32, 1)\n#20: (32, 1)\n#21: (32, 1)\n#22: (32, 1)\n#23: (32, 1)\n#24: (32, 1)\n#25: (32, 1)\n#26: (32, 1)\n#27: (32, 1)\n#28: (32, 1)\n#29: (32, 1)\n#30: (32, 1)\n#31: (32, 1)\n#32: (32, 1)\n#33: (32, 1)\n#34: (32, 1)\n#35: (32, 1)\n*** Error in `python': malloc(): smallbin double linked list corrupted: 0x00007fb6c001c960 ***\n======= Backtrace: =========\n/lib64/libc.so.6(+0x7f5e4)[0x7fb9816b75e4]\n/lib64/libc.so.6(+0x82d00)[0x7fb9816bad00]\n/lib64/libc.so.6(__libc_malloc+0x4c)[0x7fb9816bd84c]\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(_Znwm+0x16)[0x7fb936087084]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorENS_19AllocatorAttributesE+0x48)[0x7fb93f0c0608]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE+0xc5)[0x7fb93f0c0785]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow7MergeOp7ComputeEPNS_15OpKernelContextE+0xa4)[0x7fb9423854b4]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x37d)[0x7fb93f23ac9d]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x8d)[0x7fb93f23b1dd]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63a4bc)[0x7fb93f2844bc]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63ae2a)[0x7fb93f284e2a]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x21a)[0x7fb93f2f296a]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7fb93f2f1a12]\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(+0xb8678)[0x7fb9360a2678]\n/lib64/libpthread.so.0(+0x7e25)[0x7fb981a0ce25]\n/lib64/libc.so.6(clone+0x6d)[0x7fb981736bad]\n======= Memory map: ========\n200000000-200200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n200200000-200400000 ---p 00000000 00:00 0 \n200400000-200404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n200404000-200600000 ---p 00000000 00:00 0 \n200600000-200a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n200a00000-201200000 ---p 00000000 00:00 0 \n201200000-201204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201204000-201400000 ---p 00000000 00:00 0 \n201400000-201800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201800000-201804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201804000-201a00000 ---p 00000000 00:00 0 \n201a00000-201e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201e00000-201e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201e04000-202000000 ---p 00000000 00:00 0 \n202000000-202400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202400000-202404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202404000-202600000 ---p 00000000 00:00 0 \n202600000-202a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202a00000-202a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202a04000-202c00000 ---p 00000000 00:00 0 \n202c00000-203000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203000000-203004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203004000-203200000 ---p 00000000 00:00 0 \n203200000-203600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203600000-203604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203604000-203800000 ---p 00000000 00:00 0 \n203800000-203c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203c00000-203c04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203c04000-203e00000 ---p 00000000 00:00 0 \n203e00000-204200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204200000-204204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204204000-204400000 ---p 00000000 00:00 0 \n204400000-204800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204800000-204804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204804000-204a00000 ---p 00000000 00:00 0 \n204a00000-204e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204e00000-204e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204e04000-205000000 ---p 00000000 00:00 0 \n205000000-205400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205400000-205404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205404000-205600000 ---p 00000000 00:00 0 \n205600000-205a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205a00000-205a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205a04000-205c00000 ---p 00000000 00:00 0 \n205c00000-206000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206000000-206004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206004000-206200000 ---p 00000000 00:00 0 \n206200000-206600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206600000-206604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206604000-206800000 ---p 00000000 00:00 0 \n206800000-206c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206c00000-206e00000 ---p 00000000 00:00 0 \n206e00000-207000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n207000000-300200000 ---p 00000000 00:00 0 \n10000000000-10204000000 ---p 00000000 00:00 0 \n5596eb5dd000-5596eb89c000 r-xp 00000000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\n5596eba9c000-5596eba9f000 r--p 002bf000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\n5596eba9f000-5596ebb02000 rw-p 002c2000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\n5596ebb02000-5596ebb33000 rw-p 00000000 00:00 0 \n5596eda67000-5597016ee000 rw-p 00000000 00:00 0                          [heap]\n7fb698000000-7fb69803a000 rw-p 00000000 00:00 0 \n7fb69803a000-7fb69c000000 ---p 00000000 00:00 0 \n7fb69c000000-7fb69c021000 rw-p 00000000 00:00 0 \n7fb69c021000-7fb6a0000000 ---p 00000000 00:00 0 \n7fb6a0000000-7fb6a0036000 rw-p 00000000 00:00 0 \n7fb6a0036000-7fb6a4000000 ---p 00000000 00:00 0 \n7fb6a4000000-7fb6a4049000 rw-p 00000000 00:00 0 \n7fb6a4049000-7fb6a8000000 ---p 00000000 00:00 0 \n7fb6a8000000-7fb6a8039000 rw-p 00000000 00:00 0 \n7fb6a8039000-7fb6ac000000 ---p 00000000 00:00 0 \n7fb6b0000000-7fb6b0036000 rw-p 00000000 00:00 0 \n7fb6b0036000-7fb6b4000000 ---p 00000000 00:00 0 \n7fb6b4000000-7fb6b403b000 rw-p 00000000 00:00 0 \n7fb6b403b000-7fb6b8000000 ---p 00000000 00:00 0 \n7fb6b8000000-7fb6b8037000 rw-p 00000000 00:00 0 \n7fb6b8037000-7fb6bc000000 ---p 00000000 00:00 0 \n7fb6c0000000-7fb6c003c000 rw-p 00000000 00:00 0 \n7fb6c003c000-7fb6c4000000 ---p 00000000 00:00 0 \n7fb6c5400000-7fb6c5a00000 rw-p 00000000 00:00 0 \n7fb6c5ad4000-7fb6c5ad5000 ---p 00000000 00:00 0 \n7fb6c5ad5000-7fb6c6326000 rw-p 00000000 00:00 0 \n7fb6c6326000-7fb6c6327000 ---p 00000000 00:00 0 \n7fb6c6327000-7fb6c6b78000 rw-p 00000000 00:00 0 \n7fb6c6b78000-7fb6c6b79000 ---p 00000000 00:00 0 \n7fb6c6b79000-7fb6d6000000 rw-p 00000000 00:00 0 \n7fb6d6000000-7fb8ae800000 ---p 00000000 00:00 0 \n7fb8ae800000-7fb8aea00000 rw-s 00000000 00:04 31968                      /dev/zero (deleted)\n7fb8aea00000-7fb8aec00000 rw-s 00000000 00:04 29039                      /dev/zero (deleted)\n7fb8aec00000-7fb8b0000000 ---p 00000000 00:00 0 \n7fb8b0000000-7fb8b0021000 rw-p 00000000 00:00 0 \n7fb8b0021000-7fb8b4000000 ---p 00000000 00:00 0 \n7fb8b4000000-7fb8b4021000 rw-p 00000000 00:00 0 \n7fb8b4021000-7fb8b8000000 ---p 00000000 00:00 0 \n7fb8b8000000-7fb8bc000000 ---p 00000000 00:00 0 \n7fb8bc000000-7fb8bc021000 rw-p 00000000 00:00 0 \n7fb8bc021000-7fb8c0000000 ---p 00000000 00:00 0 \n7fb8c0000000-7fb8c0001000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0001000-7fb8c0002000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0002000-7fb8c0003000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0003000-7fb8c0004000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0004000-7fb8c0005000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0005000-7fb8c0006000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0006000-7fb8c0007000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0007000-7fb8c0008000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0008000-7fb8c0009000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0009000-7fb8c000a000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000a000-7fb8c000b000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000b000-7fb8c000c000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000c000-7fb8c000d000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000d000-7fb8c000e000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000e000-7fb8c000f000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000f000-7fb8c0010000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0010000-7fb8d0000000 ---p 00000000 00:00 0 \n7fb8d0000000-7fb8d0021000 rw-p 00000000 00:00 0 \n7fb8d0021000-7fb8d4000000 ---p 00000000 00:00 0 \n7fb8d4200000-7fb8d4600000 rw-p 00000000 00:00 0 \n7fb8d470a000-7fb8d470b000 ---p 00000000 00:00 0 \n7fb8d470b000-7fb8d4f5c000 rw-p 00000000 00:00 0 \n7fb8d4f5c000-7fb8d4f5d000 ---p 00000000 00:00 0 \n7fb8d4f5d000-7fb8d57ae000 rw-p 00000000 00:00 0 \n7fb8d57ae000-7fb8d57af000 ---p 00000000 00:00 0 \n7fb8d57af000-7fb8d6000000 rw-p 00000000 00:00 0 \n7fb8d6000000-7fb8d6200000 ---p 00000000 00:00 0 \n7fb8d6200000-7fb8d6400000 rw-s 00000000 00:04 31965                      /dev/zero (deleted)\n7fb8d6400000-7fb8d6600000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\n7fb8d6600000-7fb8d6800000 rw-s 00000000 00:04 31966                      /dev/zero (deleted)\n7fb8d6800000-7fb8d6a00000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\n7fb8d6a00000-7fb8d6c00000 ---p 00000000 00:00 0 \n7fb8d6c00000-7fb8d6ed6000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\n7fb8d6ed6000-7fb8d8000000 ---p 00000000 00:00 0 \n7fb8d8000000-7fb8d8021000 rw-p 00000000 00:00 0 \n7fb8d8021000-7fb8dc000000 ---p 00000000 00:00 0 \n7fb8dc200000-7fb8dc600000 rw-p 00000000 00:00 0 \n7fb8dc70a000-7fb8dc70b000 ---p 00000000 00:00 0 \n7fb8dc70b000-7fb8dcf5c000 rw-p 00000000 00:00 0 \n7fb8dcf5c000-7fb8dcf5d000 ---p 00000000 00:00 0 \n7fb8dcf5d000-7fb8dd7ae000 rw-p 00000000 00:00 0 \n7fb8dd7ae000-7fb8dd7af000 ---p 00000000 00:00 0 \n7fb8dd7af000-7fb8de000000 rw-p 00000000 00:00 0 \n7fb8de000000-7fb8e4000000 ---p 00000000 00:00 0 \n7fb8e4000000-7fb8e4021000 rw-p 00000000 00:00 0 \n7fb8e4021000-7fb8e8000000 ---p 00000000 00:00 0 \n7fb8e8000000-7fb8e8021000 rw-p 00000000 00:00 0 \n7fb8e8021000-7fb8ec000000 ---p 00000000 00:00 0 \n7fb8ec000000-7fb8ec021000 rw-p 00000000 00:00 0 \n7fb8ec021000-7fb8f0000000 ---p 00000000 00:00 0 \n7fb8f0000000-7fb8f0021000 rw-p 00000000 00:00 0 \n7fb8f0021000-7fb8f4000000 ---p 00000000 00:00 0 \n7fb8f4200000-7fb8f4400000 rw-p 00000000 00:00 0 \n7fb8f45c2000-7fb8f45c3000 ---p 00000000 00:00 0 \n7fb8f45c3000-7fb8f4e14000 rw-p 00000000 00:00 0 \n7fb8f4e14000-7fb8f4e15000 ---p 00000000 00:00 0 \n7fb8f4e15000-7fb8f5666000 rw-p 00000000 00:00 0 \n7fb8f5666000-7fb8f5667000 ---p 00000000 00:00 0 \n7fb8f5667000-7fb8f5eb8000 rw-p 00000000 00:00 0 \n7fb8f5eb8000-7fb8f5eb9000 ---p 00000000 00:00 0 \n7fb8f5eb9000-7fb8f670a000 rw-p 00000000 00:00 0 \n7fb8f670a000-7fb8f670b000 ---p 00000000 00:00 0 \n7fb8f670b000-7fb8f6f5c000 rw-p 00000000 00:00 0 \n7fb8f6f5c000-7fb8f6f5d000 ---p 00000000 00:00 0 \n7fb8f6f5d000-7fb8f77ae000 rw-p 00000000 00:00 0 \n7fb8f77ae000-7fb8f77af00Aborted\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.2 (and also Windows 10)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): binary (via pip install tensorflow-gpu)\nTensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\nPython version: 3.6.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 9.0/7.1.4\nGPU model and memory: NVIDIA GeForce GTX 1070 (8 GiB)\nExact command to reproduce: python nmsstest.py; see below for the content of nmstest.py\n\nDescribe the problem\nThe code in the following section, which calls tf.image.non_max_suppression() in tf.while_loop() many times, crashes abnormally.\nCrash reason (and loop count) varies from time to time, for example:\n\nF tensorflow/core/common_runtime/bfc_allocator.cc:384] Check failed: h != kInvalidChunkHandle at loop i == 140\nF tensorflow/core/common_runtime/bfc_allocator.cc:462] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) at loop i == 76\nBus error at loop i == 34\nheap corruption reported by libc (see the following section) at loop i == 35\nsometimes it crashes sliently without any logs (on Windows)\n\nI notice that:\n\nit's since TensorFlow 1.11.0rc0; TF 1.10.1 was okay\nit also reproduces on Windows\nit also reproduces on CPU version (pip install tensorflow)\nit does not reproduce if num_threads=1; calling tf.image.non_max_suppression() in parallel seems the trigger\neven when I gave a fixed seed to tf.random_uniform(), crash cause varied\n\nSource code / logs\nThe code to reproduce the problem is as follows:\nimport tensorflow as tf\n\nif __name__ == '__main__':\n    # crashes iff num_threads > 1 on TensorFlow >= 1.11.rc0\n    num_threads = 10\n\n    top_k = 1\n    batch_size = 32\n    num_boxes = 10000\n    boxes_op = tf.random_uniform((batch_size,num_boxes,4), 0, 1)\n    scores_op = tf.random_uniform((batch_size,num_boxes), 0, 1)\n    indices_op = tf.while_loop(\n        (lambda b, ta: True),\n        (lambda b, ta: (b+1, ta.write(b, tf.image.non_max_suppression(boxes_op[b], scores_op[b], top_k, iou_threshold=0.3)))),\n        (tf.constant(0),\n         tf.TensorArray(tf.int32, size=batch_size, infer_shape=False, element_shape=(top_k,))),\n        back_prop=False,\n        parallel_iterations=num_threads,\n        maximum_iterations=batch_size)[1].stack()\n\n    with tf.Session() as session:\n        for i in range(1000):\n            indices = session.run(indices_op)\n            print(f'#{i}: {indices.shape}')\n\n$ python nmstest.py\n2018-09-28 13:18:42.760545: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-09-28 13:18:43.123328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-28 13:18:43.124208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\npciBusID: 0000:01:00.0\ntotalMemory: 7.93GiB freeMemory: 7.83GiB\n2018-09-28 13:18:43.124232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n2018-09-28 13:18:43.351813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-28 13:18:43.351847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n2018-09-28 13:18:43.351859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n2018-09-28 13:18:43.352078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7558 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\n#0: (32, 1)\n#1: (32, 1)\n#2: (32, 1)\n#3: (32, 1)\n#4: (32, 1)\n#5: (32, 1)\n#6: (32, 1)\n#7: (32, 1)\n#8: (32, 1)\n#9: (32, 1)\n#10: (32, 1)\n#11: (32, 1)\n#12: (32, 1)\n#13: (32, 1)\n#14: (32, 1)\n#15: (32, 1)\n#16: (32, 1)\n#17: (32, 1)\n#18: (32, 1)\n#19: (32, 1)\n#20: (32, 1)\n#21: (32, 1)\n#22: (32, 1)\n#23: (32, 1)\n#24: (32, 1)\n#25: (32, 1)\n#26: (32, 1)\n#27: (32, 1)\n#28: (32, 1)\n#29: (32, 1)\n#30: (32, 1)\n#31: (32, 1)\n#32: (32, 1)\n#33: (32, 1)\n#34: (32, 1)\n#35: (32, 1)\n*** Error in `python': malloc(): smallbin double linked list corrupted: 0x00007fb6c001c960 ***\n======= Backtrace: =========\n/lib64/libc.so.6(+0x7f5e4)[0x7fb9816b75e4]\n/lib64/libc.so.6(+0x82d00)[0x7fb9816bad00]\n/lib64/libc.so.6(__libc_malloc+0x4c)[0x7fb9816bd84c]\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(_Znwm+0x16)[0x7fb936087084]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorENS_19AllocatorAttributesE+0x48)[0x7fb93f0c0608]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE+0xc5)[0x7fb93f0c0785]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow7MergeOp7ComputeEPNS_15OpKernelContextE+0xa4)[0x7fb9423854b4]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x37d)[0x7fb93f23ac9d]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x8d)[0x7fb93f23b1dd]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63a4bc)[0x7fb93f2844bc]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63ae2a)[0x7fb93f284e2a]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x21a)[0x7fb93f2f296a]\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7fb93f2f1a12]\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(+0xb8678)[0x7fb9360a2678]\n/lib64/libpthread.so.0(+0x7e25)[0x7fb981a0ce25]\n/lib64/libc.so.6(clone+0x6d)[0x7fb981736bad]\n======= Memory map: ========\n200000000-200200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n200200000-200400000 ---p 00000000 00:00 0 \n200400000-200404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n200404000-200600000 ---p 00000000 00:00 0 \n200600000-200a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n200a00000-201200000 ---p 00000000 00:00 0 \n201200000-201204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201204000-201400000 ---p 00000000 00:00 0 \n201400000-201800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201800000-201804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201804000-201a00000 ---p 00000000 00:00 0 \n201a00000-201e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201e00000-201e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n201e04000-202000000 ---p 00000000 00:00 0 \n202000000-202400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202400000-202404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202404000-202600000 ---p 00000000 00:00 0 \n202600000-202a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202a00000-202a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n202a04000-202c00000 ---p 00000000 00:00 0 \n202c00000-203000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203000000-203004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203004000-203200000 ---p 00000000 00:00 0 \n203200000-203600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203600000-203604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203604000-203800000 ---p 00000000 00:00 0 \n203800000-203c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203c00000-203c04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n203c04000-203e00000 ---p 00000000 00:00 0 \n203e00000-204200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204200000-204204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204204000-204400000 ---p 00000000 00:00 0 \n204400000-204800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204800000-204804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204804000-204a00000 ---p 00000000 00:00 0 \n204a00000-204e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204e00000-204e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n204e04000-205000000 ---p 00000000 00:00 0 \n205000000-205400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205400000-205404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205404000-205600000 ---p 00000000 00:00 0 \n205600000-205a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205a00000-205a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n205a04000-205c00000 ---p 00000000 00:00 0 \n205c00000-206000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206000000-206004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206004000-206200000 ---p 00000000 00:00 0 \n206200000-206600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206600000-206604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206604000-206800000 ---p 00000000 00:00 0 \n206800000-206c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n206c00000-206e00000 ---p 00000000 00:00 0 \n206e00000-207000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\n207000000-300200000 ---p 00000000 00:00 0 \n10000000000-10204000000 ---p 00000000 00:00 0 \n5596eb5dd000-5596eb89c000 r-xp 00000000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\n5596eba9c000-5596eba9f000 r--p 002bf000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\n5596eba9f000-5596ebb02000 rw-p 002c2000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\n5596ebb02000-5596ebb33000 rw-p 00000000 00:00 0 \n5596eda67000-5597016ee000 rw-p 00000000 00:00 0                          [heap]\n7fb698000000-7fb69803a000 rw-p 00000000 00:00 0 \n7fb69803a000-7fb69c000000 ---p 00000000 00:00 0 \n7fb69c000000-7fb69c021000 rw-p 00000000 00:00 0 \n7fb69c021000-7fb6a0000000 ---p 00000000 00:00 0 \n7fb6a0000000-7fb6a0036000 rw-p 00000000 00:00 0 \n7fb6a0036000-7fb6a4000000 ---p 00000000 00:00 0 \n7fb6a4000000-7fb6a4049000 rw-p 00000000 00:00 0 \n7fb6a4049000-7fb6a8000000 ---p 00000000 00:00 0 \n7fb6a8000000-7fb6a8039000 rw-p 00000000 00:00 0 \n7fb6a8039000-7fb6ac000000 ---p 00000000 00:00 0 \n7fb6b0000000-7fb6b0036000 rw-p 00000000 00:00 0 \n7fb6b0036000-7fb6b4000000 ---p 00000000 00:00 0 \n7fb6b4000000-7fb6b403b000 rw-p 00000000 00:00 0 \n7fb6b403b000-7fb6b8000000 ---p 00000000 00:00 0 \n7fb6b8000000-7fb6b8037000 rw-p 00000000 00:00 0 \n7fb6b8037000-7fb6bc000000 ---p 00000000 00:00 0 \n7fb6c0000000-7fb6c003c000 rw-p 00000000 00:00 0 \n7fb6c003c000-7fb6c4000000 ---p 00000000 00:00 0 \n7fb6c5400000-7fb6c5a00000 rw-p 00000000 00:00 0 \n7fb6c5ad4000-7fb6c5ad5000 ---p 00000000 00:00 0 \n7fb6c5ad5000-7fb6c6326000 rw-p 00000000 00:00 0 \n7fb6c6326000-7fb6c6327000 ---p 00000000 00:00 0 \n7fb6c6327000-7fb6c6b78000 rw-p 00000000 00:00 0 \n7fb6c6b78000-7fb6c6b79000 ---p 00000000 00:00 0 \n7fb6c6b79000-7fb6d6000000 rw-p 00000000 00:00 0 \n7fb6d6000000-7fb8ae800000 ---p 00000000 00:00 0 \n7fb8ae800000-7fb8aea00000 rw-s 00000000 00:04 31968                      /dev/zero (deleted)\n7fb8aea00000-7fb8aec00000 rw-s 00000000 00:04 29039                      /dev/zero (deleted)\n7fb8aec00000-7fb8b0000000 ---p 00000000 00:00 0 \n7fb8b0000000-7fb8b0021000 rw-p 00000000 00:00 0 \n7fb8b0021000-7fb8b4000000 ---p 00000000 00:00 0 \n7fb8b4000000-7fb8b4021000 rw-p 00000000 00:00 0 \n7fb8b4021000-7fb8b8000000 ---p 00000000 00:00 0 \n7fb8b8000000-7fb8bc000000 ---p 00000000 00:00 0 \n7fb8bc000000-7fb8bc021000 rw-p 00000000 00:00 0 \n7fb8bc021000-7fb8c0000000 ---p 00000000 00:00 0 \n7fb8c0000000-7fb8c0001000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0001000-7fb8c0002000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0002000-7fb8c0003000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0003000-7fb8c0004000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0004000-7fb8c0005000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0005000-7fb8c0006000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0006000-7fb8c0007000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0007000-7fb8c0008000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0008000-7fb8c0009000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0009000-7fb8c000a000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000a000-7fb8c000b000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000b000-7fb8c000c000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000c000-7fb8c000d000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000d000-7fb8c000e000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000e000-7fb8c000f000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c000f000-7fb8c0010000 rw-s 00000000 00:05 25688                      /dev/nvidia0\n7fb8c0010000-7fb8d0000000 ---p 00000000 00:00 0 \n7fb8d0000000-7fb8d0021000 rw-p 00000000 00:00 0 \n7fb8d0021000-7fb8d4000000 ---p 00000000 00:00 0 \n7fb8d4200000-7fb8d4600000 rw-p 00000000 00:00 0 \n7fb8d470a000-7fb8d470b000 ---p 00000000 00:00 0 \n7fb8d470b000-7fb8d4f5c000 rw-p 00000000 00:00 0 \n7fb8d4f5c000-7fb8d4f5d000 ---p 00000000 00:00 0 \n7fb8d4f5d000-7fb8d57ae000 rw-p 00000000 00:00 0 \n7fb8d57ae000-7fb8d57af000 ---p 00000000 00:00 0 \n7fb8d57af000-7fb8d6000000 rw-p 00000000 00:00 0 \n7fb8d6000000-7fb8d6200000 ---p 00000000 00:00 0 \n7fb8d6200000-7fb8d6400000 rw-s 00000000 00:04 31965                      /dev/zero (deleted)\n7fb8d6400000-7fb8d6600000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\n7fb8d6600000-7fb8d6800000 rw-s 00000000 00:04 31966                      /dev/zero (deleted)\n7fb8d6800000-7fb8d6a00000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\n7fb8d6a00000-7fb8d6c00000 ---p 00000000 00:00 0 \n7fb8d6c00000-7fb8d6ed6000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\n7fb8d6ed6000-7fb8d8000000 ---p 00000000 00:00 0 \n7fb8d8000000-7fb8d8021000 rw-p 00000000 00:00 0 \n7fb8d8021000-7fb8dc000000 ---p 00000000 00:00 0 \n7fb8dc200000-7fb8dc600000 rw-p 00000000 00:00 0 \n7fb8dc70a000-7fb8dc70b000 ---p 00000000 00:00 0 \n7fb8dc70b000-7fb8dcf5c000 rw-p 00000000 00:00 0 \n7fb8dcf5c000-7fb8dcf5d000 ---p 00000000 00:00 0 \n7fb8dcf5d000-7fb8dd7ae000 rw-p 00000000 00:00 0 \n7fb8dd7ae000-7fb8dd7af000 ---p 00000000 00:00 0 \n7fb8dd7af000-7fb8de000000 rw-p 00000000 00:00 0 \n7fb8de000000-7fb8e4000000 ---p 00000000 00:00 0 \n7fb8e4000000-7fb8e4021000 rw-p 00000000 00:00 0 \n7fb8e4021000-7fb8e8000000 ---p 00000000 00:00 0 \n7fb8e8000000-7fb8e8021000 rw-p 00000000 00:00 0 \n7fb8e8021000-7fb8ec000000 ---p 00000000 00:00 0 \n7fb8ec000000-7fb8ec021000 rw-p 00000000 00:00 0 \n7fb8ec021000-7fb8f0000000 ---p 00000000 00:00 0 \n7fb8f0000000-7fb8f0021000 rw-p 00000000 00:00 0 \n7fb8f0021000-7fb8f4000000 ---p 00000000 00:00 0 \n7fb8f4200000-7fb8f4400000 rw-p 00000000 00:00 0 \n7fb8f45c2000-7fb8f45c3000 ---p 00000000 00:00 0 \n7fb8f45c3000-7fb8f4e14000 rw-p 00000000 00:00 0 \n7fb8f4e14000-7fb8f4e15000 ---p 00000000 00:00 0 \n7fb8f4e15000-7fb8f5666000 rw-p 00000000 00:00 0 \n7fb8f5666000-7fb8f5667000 ---p 00000000 00:00 0 \n7fb8f5667000-7fb8f5eb8000 rw-p 00000000 00:00 0 \n7fb8f5eb8000-7fb8f5eb9000 ---p 00000000 00:00 0 \n7fb8f5eb9000-7fb8f670a000 rw-p 00000000 00:00 0 \n7fb8f670a000-7fb8f670b000 ---p 00000000 00:00 0 \n7fb8f670b000-7fb8f6f5c000 rw-p 00000000 00:00 0 \n7fb8f6f5c000-7fb8f6f5d000 ---p 00000000 00:00 0 \n7fb8f6f5d000-7fb8f77ae000 rw-p 00000000 00:00 0 \n7fb8f77ae000-7fb8f77af00Aborted", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.2 (and also Windows 10)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary (via `pip install tensorflow-gpu`)\r\n- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.1.4\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1070 (8 GiB)\r\n- **Exact command to reproduce**: `python nmsstest.py`; see below for the content of `nmstest.py`\r\n\r\n### Describe the problem\r\nThe code in the following section, which calls `tf.image.non_max_suppression()` in `tf.while_loop()` many times, crashes abnormally.\r\nCrash reason (and loop count) varies from time to time, for example:\r\n* `F tensorflow/core/common_runtime/bfc_allocator.cc:384] Check failed: h != kInvalidChunkHandle` at loop `i == 140`\r\n* `F tensorflow/core/common_runtime/bfc_allocator.cc:462] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)` at loop `i == 76`\r\n* `Bus error` at loop `i == 34`\r\n* heap corruption reported by libc (see the following section) at loop `i == 35`\r\n* sometimes it crashes sliently without any logs (on Windows)\r\n\r\nI notice that:\r\n* it's since TensorFlow 1.11.0rc0; TF 1.10.1 was okay\r\n* it also reproduces on Windows\r\n* it also reproduces on CPU version (`pip install tensorflow`)\r\n* it does **not** reproduce if `num_threads=1`; calling `tf.image.non_max_suppression()` in parallel seems the trigger\r\n* even when I gave a fixed seed to `tf.random_uniform()`, crash cause varied\r\n\r\n### Source code / logs\r\n\r\nThe code to reproduce the problem is as follows:\r\n```\r\nimport tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    # crashes iff num_threads > 1 on TensorFlow >= 1.11.rc0\r\n    num_threads = 10\r\n\r\n    top_k = 1\r\n    batch_size = 32\r\n    num_boxes = 10000\r\n    boxes_op = tf.random_uniform((batch_size,num_boxes,4), 0, 1)\r\n    scores_op = tf.random_uniform((batch_size,num_boxes), 0, 1)\r\n    indices_op = tf.while_loop(\r\n        (lambda b, ta: True),\r\n        (lambda b, ta: (b+1, ta.write(b, tf.image.non_max_suppression(boxes_op[b], scores_op[b], top_k, iou_threshold=0.3)))),\r\n        (tf.constant(0),\r\n         tf.TensorArray(tf.int32, size=batch_size, infer_shape=False, element_shape=(top_k,))),\r\n        back_prop=False,\r\n        parallel_iterations=num_threads,\r\n        maximum_iterations=batch_size)[1].stack()\r\n\r\n    with tf.Session() as session:\r\n        for i in range(1000):\r\n            indices = session.run(indices_op)\r\n            print(f'#{i}: {indices.shape}')\r\n```\r\n\r\n```\r\n$ python nmstest.py\r\n2018-09-28 13:18:42.760545: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-28 13:18:43.123328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-28 13:18:43.124208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.83GiB\r\n2018-09-28 13:18:43.124232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-28 13:18:43.351813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-28 13:18:43.351847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-09-28 13:18:43.351859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-09-28 13:18:43.352078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7558 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n#0: (32, 1)\r\n#1: (32, 1)\r\n#2: (32, 1)\r\n#3: (32, 1)\r\n#4: (32, 1)\r\n#5: (32, 1)\r\n#6: (32, 1)\r\n#7: (32, 1)\r\n#8: (32, 1)\r\n#9: (32, 1)\r\n#10: (32, 1)\r\n#11: (32, 1)\r\n#12: (32, 1)\r\n#13: (32, 1)\r\n#14: (32, 1)\r\n#15: (32, 1)\r\n#16: (32, 1)\r\n#17: (32, 1)\r\n#18: (32, 1)\r\n#19: (32, 1)\r\n#20: (32, 1)\r\n#21: (32, 1)\r\n#22: (32, 1)\r\n#23: (32, 1)\r\n#24: (32, 1)\r\n#25: (32, 1)\r\n#26: (32, 1)\r\n#27: (32, 1)\r\n#28: (32, 1)\r\n#29: (32, 1)\r\n#30: (32, 1)\r\n#31: (32, 1)\r\n#32: (32, 1)\r\n#33: (32, 1)\r\n#34: (32, 1)\r\n#35: (32, 1)\r\n*** Error in `python': malloc(): smallbin double linked list corrupted: 0x00007fb6c001c960 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x7f5e4)[0x7fb9816b75e4]\r\n/lib64/libc.so.6(+0x82d00)[0x7fb9816bad00]\r\n/lib64/libc.so.6(__libc_malloc+0x4c)[0x7fb9816bd84c]\r\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(_Znwm+0x16)[0x7fb936087084]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorENS_19AllocatorAttributesE+0x48)[0x7fb93f0c0608]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE+0xc5)[0x7fb93f0c0785]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow7MergeOp7ComputeEPNS_15OpKernelContextE+0xa4)[0x7fb9423854b4]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x37d)[0x7fb93f23ac9d]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x8d)[0x7fb93f23b1dd]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63a4bc)[0x7fb93f2844bc]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63ae2a)[0x7fb93f284e2a]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x21a)[0x7fb93f2f296a]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7fb93f2f1a12]\r\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(+0xb8678)[0x7fb9360a2678]\r\n/lib64/libpthread.so.0(+0x7e25)[0x7fb981a0ce25]\r\n/lib64/libc.so.6(clone+0x6d)[0x7fb981736bad]\r\n======= Memory map: ========\r\n200000000-200200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n200200000-200400000 ---p 00000000 00:00 0 \r\n200400000-200404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n200404000-200600000 ---p 00000000 00:00 0 \r\n200600000-200a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n200a00000-201200000 ---p 00000000 00:00 0 \r\n201200000-201204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201204000-201400000 ---p 00000000 00:00 0 \r\n201400000-201800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201800000-201804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201804000-201a00000 ---p 00000000 00:00 0 \r\n201a00000-201e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201e00000-201e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201e04000-202000000 ---p 00000000 00:00 0 \r\n202000000-202400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202400000-202404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202404000-202600000 ---p 00000000 00:00 0 \r\n202600000-202a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202a00000-202a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202a04000-202c00000 ---p 00000000 00:00 0 \r\n202c00000-203000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203000000-203004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203004000-203200000 ---p 00000000 00:00 0 \r\n203200000-203600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203600000-203604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203604000-203800000 ---p 00000000 00:00 0 \r\n203800000-203c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203c00000-203c04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203c04000-203e00000 ---p 00000000 00:00 0 \r\n203e00000-204200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204200000-204204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204204000-204400000 ---p 00000000 00:00 0 \r\n204400000-204800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204800000-204804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204804000-204a00000 ---p 00000000 00:00 0 \r\n204a00000-204e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204e00000-204e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204e04000-205000000 ---p 00000000 00:00 0 \r\n205000000-205400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205400000-205404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205404000-205600000 ---p 00000000 00:00 0 \r\n205600000-205a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205a00000-205a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205a04000-205c00000 ---p 00000000 00:00 0 \r\n205c00000-206000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206000000-206004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206004000-206200000 ---p 00000000 00:00 0 \r\n206200000-206600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206600000-206604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206604000-206800000 ---p 00000000 00:00 0 \r\n206800000-206c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206c00000-206e00000 ---p 00000000 00:00 0 \r\n206e00000-207000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n207000000-300200000 ---p 00000000 00:00 0 \r\n10000000000-10204000000 ---p 00000000 00:00 0 \r\n5596eb5dd000-5596eb89c000 r-xp 00000000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\r\n5596eba9c000-5596eba9f000 r--p 002bf000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\r\n5596eba9f000-5596ebb02000 rw-p 002c2000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\r\n5596ebb02000-5596ebb33000 rw-p 00000000 00:00 0 \r\n5596eda67000-5597016ee000 rw-p 00000000 00:00 0                          [heap]\r\n7fb698000000-7fb69803a000 rw-p 00000000 00:00 0 \r\n7fb69803a000-7fb69c000000 ---p 00000000 00:00 0 \r\n7fb69c000000-7fb69c021000 rw-p 00000000 00:00 0 \r\n7fb69c021000-7fb6a0000000 ---p 00000000 00:00 0 \r\n7fb6a0000000-7fb6a0036000 rw-p 00000000 00:00 0 \r\n7fb6a0036000-7fb6a4000000 ---p 00000000 00:00 0 \r\n7fb6a4000000-7fb6a4049000 rw-p 00000000 00:00 0 \r\n7fb6a4049000-7fb6a8000000 ---p 00000000 00:00 0 \r\n7fb6a8000000-7fb6a8039000 rw-p 00000000 00:00 0 \r\n7fb6a8039000-7fb6ac000000 ---p 00000000 00:00 0 \r\n7fb6b0000000-7fb6b0036000 rw-p 00000000 00:00 0 \r\n7fb6b0036000-7fb6b4000000 ---p 00000000 00:00 0 \r\n7fb6b4000000-7fb6b403b000 rw-p 00000000 00:00 0 \r\n7fb6b403b000-7fb6b8000000 ---p 00000000 00:00 0 \r\n7fb6b8000000-7fb6b8037000 rw-p 00000000 00:00 0 \r\n7fb6b8037000-7fb6bc000000 ---p 00000000 00:00 0 \r\n7fb6c0000000-7fb6c003c000 rw-p 00000000 00:00 0 \r\n7fb6c003c000-7fb6c4000000 ---p 00000000 00:00 0 \r\n7fb6c5400000-7fb6c5a00000 rw-p 00000000 00:00 0 \r\n7fb6c5ad4000-7fb6c5ad5000 ---p 00000000 00:00 0 \r\n7fb6c5ad5000-7fb6c6326000 rw-p 00000000 00:00 0 \r\n7fb6c6326000-7fb6c6327000 ---p 00000000 00:00 0 \r\n7fb6c6327000-7fb6c6b78000 rw-p 00000000 00:00 0 \r\n7fb6c6b78000-7fb6c6b79000 ---p 00000000 00:00 0 \r\n7fb6c6b79000-7fb6d6000000 rw-p 00000000 00:00 0 \r\n7fb6d6000000-7fb8ae800000 ---p 00000000 00:00 0 \r\n7fb8ae800000-7fb8aea00000 rw-s 00000000 00:04 31968                      /dev/zero (deleted)\r\n7fb8aea00000-7fb8aec00000 rw-s 00000000 00:04 29039                      /dev/zero (deleted)\r\n7fb8aec00000-7fb8b0000000 ---p 00000000 00:00 0 \r\n7fb8b0000000-7fb8b0021000 rw-p 00000000 00:00 0 \r\n7fb8b0021000-7fb8b4000000 ---p 00000000 00:00 0 \r\n7fb8b4000000-7fb8b4021000 rw-p 00000000 00:00 0 \r\n7fb8b4021000-7fb8b8000000 ---p 00000000 00:00 0 \r\n7fb8b8000000-7fb8bc000000 ---p 00000000 00:00 0 \r\n7fb8bc000000-7fb8bc021000 rw-p 00000000 00:00 0 \r\n7fb8bc021000-7fb8c0000000 ---p 00000000 00:00 0 \r\n7fb8c0000000-7fb8c0001000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0001000-7fb8c0002000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0002000-7fb8c0003000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0003000-7fb8c0004000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0004000-7fb8c0005000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0005000-7fb8c0006000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0006000-7fb8c0007000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0007000-7fb8c0008000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0008000-7fb8c0009000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0009000-7fb8c000a000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000a000-7fb8c000b000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000b000-7fb8c000c000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000c000-7fb8c000d000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000d000-7fb8c000e000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000e000-7fb8c000f000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000f000-7fb8c0010000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0010000-7fb8d0000000 ---p 00000000 00:00 0 \r\n7fb8d0000000-7fb8d0021000 rw-p 00000000 00:00 0 \r\n7fb8d0021000-7fb8d4000000 ---p 00000000 00:00 0 \r\n7fb8d4200000-7fb8d4600000 rw-p 00000000 00:00 0 \r\n7fb8d470a000-7fb8d470b000 ---p 00000000 00:00 0 \r\n7fb8d470b000-7fb8d4f5c000 rw-p 00000000 00:00 0 \r\n7fb8d4f5c000-7fb8d4f5d000 ---p 00000000 00:00 0 \r\n7fb8d4f5d000-7fb8d57ae000 rw-p 00000000 00:00 0 \r\n7fb8d57ae000-7fb8d57af000 ---p 00000000 00:00 0 \r\n7fb8d57af000-7fb8d6000000 rw-p 00000000 00:00 0 \r\n7fb8d6000000-7fb8d6200000 ---p 00000000 00:00 0 \r\n7fb8d6200000-7fb8d6400000 rw-s 00000000 00:04 31965                      /dev/zero (deleted)\r\n7fb8d6400000-7fb8d6600000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\r\n7fb8d6600000-7fb8d6800000 rw-s 00000000 00:04 31966                      /dev/zero (deleted)\r\n7fb8d6800000-7fb8d6a00000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\r\n7fb8d6a00000-7fb8d6c00000 ---p 00000000 00:00 0 \r\n7fb8d6c00000-7fb8d6ed6000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\r\n7fb8d6ed6000-7fb8d8000000 ---p 00000000 00:00 0 \r\n7fb8d8000000-7fb8d8021000 rw-p 00000000 00:00 0 \r\n7fb8d8021000-7fb8dc000000 ---p 00000000 00:00 0 \r\n7fb8dc200000-7fb8dc600000 rw-p 00000000 00:00 0 \r\n7fb8dc70a000-7fb8dc70b000 ---p 00000000 00:00 0 \r\n7fb8dc70b000-7fb8dcf5c000 rw-p 00000000 00:00 0 \r\n7fb8dcf5c000-7fb8dcf5d000 ---p 00000000 00:00 0 \r\n7fb8dcf5d000-7fb8dd7ae000 rw-p 00000000 00:00 0 \r\n7fb8dd7ae000-7fb8dd7af000 ---p 00000000 00:00 0 \r\n7fb8dd7af000-7fb8de000000 rw-p 00000000 00:00 0 \r\n7fb8de000000-7fb8e4000000 ---p 00000000 00:00 0 \r\n7fb8e4000000-7fb8e4021000 rw-p 00000000 00:00 0 \r\n7fb8e4021000-7fb8e8000000 ---p 00000000 00:00 0 \r\n7fb8e8000000-7fb8e8021000 rw-p 00000000 00:00 0 \r\n7fb8e8021000-7fb8ec000000 ---p 00000000 00:00 0 \r\n7fb8ec000000-7fb8ec021000 rw-p 00000000 00:00 0 \r\n7fb8ec021000-7fb8f0000000 ---p 00000000 00:00 0 \r\n7fb8f0000000-7fb8f0021000 rw-p 00000000 00:00 0 \r\n7fb8f0021000-7fb8f4000000 ---p 00000000 00:00 0 \r\n7fb8f4200000-7fb8f4400000 rw-p 00000000 00:00 0 \r\n7fb8f45c2000-7fb8f45c3000 ---p 00000000 00:00 0 \r\n7fb8f45c3000-7fb8f4e14000 rw-p 00000000 00:00 0 \r\n7fb8f4e14000-7fb8f4e15000 ---p 00000000 00:00 0 \r\n7fb8f4e15000-7fb8f5666000 rw-p 00000000 00:00 0 \r\n7fb8f5666000-7fb8f5667000 ---p 00000000 00:00 0 \r\n7fb8f5667000-7fb8f5eb8000 rw-p 00000000 00:00 0 \r\n7fb8f5eb8000-7fb8f5eb9000 ---p 00000000 00:00 0 \r\n7fb8f5eb9000-7fb8f670a000 rw-p 00000000 00:00 0 \r\n7fb8f670a000-7fb8f670b000 ---p 00000000 00:00 0 \r\n7fb8f670b000-7fb8f6f5c000 rw-p 00000000 00:00 0 \r\n7fb8f6f5c000-7fb8f6f5d000 ---p 00000000 00:00 0 \r\n7fb8f6f5d000-7fb8f77ae000 rw-p 00000000 00:00 0 \r\n7fb8f77ae000-7fb8f77af00Aborted\r\n```"}