{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335821819", "html_url": "https://github.com/tensorflow/tensorflow/issues/13461#issuecomment-335821819", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13461", "id": 335821819, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTgyMTgxOQ==", "user": {"login": "DarcyCode", "id": 22005397, "node_id": "MDQ6VXNlcjIyMDA1Mzk3", "avatar_url": "https://avatars3.githubusercontent.com/u/22005397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DarcyCode", "html_url": "https://github.com/DarcyCode", "followers_url": "https://api.github.com/users/DarcyCode/followers", "following_url": "https://api.github.com/users/DarcyCode/following{/other_user}", "gists_url": "https://api.github.com/users/DarcyCode/gists{/gist_id}", "starred_url": "https://api.github.com/users/DarcyCode/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DarcyCode/subscriptions", "organizations_url": "https://api.github.com/users/DarcyCode/orgs", "repos_url": "https://api.github.com/users/DarcyCode/repos", "events_url": "https://api.github.com/users/DarcyCode/events{/privacy}", "received_events_url": "https://api.github.com/users/DarcyCode/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-11T14:06:53Z", "updated_at": "2017-11-03T07:35:31Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> Thanks for answering my questions. I define a CNN with BN on a computer(no-GPU) with CPU-version tensorflow r1.3. Code is below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nX <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">LENGTH</span>],<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>X<span class=\"pl-pds\">'</span></span>)\nY <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">MAX_CAPTCHA</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">CHAR_SET_LEN</span>],<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Y<span class=\"pl-pds\">'</span></span>)\nkeep_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32,<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>keep_prob<span class=\"pl-pds\">'</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> dropout</span>\ntrain_phase <span class=\"pl-k\">=</span> tf.placeholder(tf.bool, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_phase<span class=\"pl-pds\">'</span></span>)\n \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">batch_norm</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">beta</span>, <span class=\"pl-smi\">gamma</span>, <span class=\"pl-smi\">train_phase</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, <span class=\"pl-smi\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-smi\">eps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>):\n    <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n        batch_mean, batch_var <span class=\"pl-k\">=</span> tf.nn.moments(x, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>moments<span class=\"pl-pds\">'</span></span>)\n        ema <span class=\"pl-k\">=</span> tf.train.ExponentialMovingAverage(<span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span>decay) \n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">mean_var_with_update</span>():\n            ema_apply_op <span class=\"pl-k\">=</span> ema.apply([batch_mean, batch_var])\n            <span class=\"pl-k\">with</span> tf.control_dependencies([ema_apply_op]):\n                <span class=\"pl-k\">return</span> tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var <span class=\"pl-k\">=</span> tf.cond(train_phase, mean_var_with_update, <span class=\"pl-k\">lambda</span>: (ema.average(batch_mean), ema.average(batch_var)))\n        normed <span class=\"pl-k\">=</span> tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\n    <span class=\"pl-k\">return</span> normed\n \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">crack_captcha_cnn</span>(<span class=\"pl-smi\">w_alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-smi\">b_alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>):\n    x <span class=\"pl-k\">=</span> tf.reshape(X, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">IMAGE_HEIGHT</span>, <span class=\"pl-c1\">IMAGE_WIDTH</span>, <span class=\"pl-c1\">1</span>])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> 4 conv layer</span>\n    w_c1 <span class=\"pl-k\">=</span> tf.Variable(w_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>]))\n    b_c1 <span class=\"pl-k\">=</span> tf.Variable(b_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">32</span>]))\n    conv1 <span class=\"pl-k\">=</span> tf.nn.bias_add(tf.nn.conv2d(x, w_c1, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>), b_c1)\n    conv1 <span class=\"pl-k\">=</span> batch_norm(conv1, tf.constant(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">32</span>]), tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">32</span>], <span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.02</span>), train_phase, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_1<span class=\"pl-pds\">'</span></span>)\n    conv1 <span class=\"pl-k\">=</span> tf.nn.relu(conv1)\n    conv1 <span class=\"pl-k\">=</span> tf.nn.max_pool(conv1, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    conv1 <span class=\"pl-k\">=</span> tf.nn.dropout(conv1, keep_prob)\n \n    w_c2 <span class=\"pl-k\">=</span> tf.Variable(w_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>]))\n    b_c2 <span class=\"pl-k\">=</span> tf.Variable(b_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">64</span>]))\n    conv2 <span class=\"pl-k\">=</span> tf.nn.bias_add(tf.nn.conv2d(conv1, w_c2, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>), b_c2)\n    conv2 <span class=\"pl-k\">=</span> batch_norm(conv2, tf.constant(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>]), tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>], <span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.02</span>), train_phase, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_2<span class=\"pl-pds\">'</span></span>)\n    conv2 <span class=\"pl-k\">=</span> tf.nn.relu(conv2)\n    conv2 <span class=\"pl-k\">=</span> tf.nn.max_pool(conv2, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    conv2 <span class=\"pl-k\">=</span> tf.nn.dropout(conv2, keep_prob)\n \n    w_c3 <span class=\"pl-k\">=</span> tf.Variable(w_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>]))\n    b_c3 <span class=\"pl-k\">=</span> tf.Variable(b_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">64</span>]))\n    conv3 <span class=\"pl-k\">=</span> tf.nn.bias_add(tf.nn.conv2d(conv2, w_c3, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>), b_c3)\n    conv3 <span class=\"pl-k\">=</span> batch_norm(conv3, tf.constant(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>]), tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>], <span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.02</span>), train_phase, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_3<span class=\"pl-pds\">'</span></span>)\n    conv3 <span class=\"pl-k\">=</span> tf.nn.relu(conv3)\n    conv3 <span class=\"pl-k\">=</span> tf.nn.max_pool(conv3, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    conv3 <span class=\"pl-k\">=</span> tf.nn.dropout(conv3, keep_prob)\n\n    w_c4 <span class=\"pl-k\">=</span> tf.Variable(w_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>]))\n    b_c4 <span class=\"pl-k\">=</span> tf.Variable(b_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">64</span>]))\n    conv4 <span class=\"pl-k\">=</span> tf.nn.bias_add(tf.nn.conv2d(conv3, w_c4, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>), b_c4)\n    conv4 <span class=\"pl-k\">=</span> batch_norm(conv4, tf.constant(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>]), tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>], <span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.02</span>), train_phase, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_4<span class=\"pl-pds\">'</span></span>)\n    conv4 <span class=\"pl-k\">=</span> tf.nn.relu(conv4)\n    conv4 <span class=\"pl-k\">=</span> tf.nn.max_pool(conv4, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    conv4 <span class=\"pl-k\">=</span> tf.nn.dropout(conv4, keep_prob)\n     \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Fully connected layer</span>\n    w_d <span class=\"pl-k\">=</span> tf.Variable(w_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">LENGTH</span><span class=\"pl-k\">/</span><span class=\"pl-c1\">4</span>), <span class=\"pl-c1\">1024</span>]))\n    b_d <span class=\"pl-k\">=</span> tf.Variable(b_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">1024</span>]))\n    dense <span class=\"pl-k\">=</span> tf.reshape(conv4, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, w_d.get_shape().as_list()[<span class=\"pl-c1\">0</span>]])\n    dense <span class=\"pl-k\">=</span> tf.nn.relu(tf.add(tf.matmul(dense, w_d), b_d))\n    dense <span class=\"pl-k\">=</span> tf.nn.dropout(dense, keep_prob)\n \n    w_out <span class=\"pl-k\">=</span> tf.Variable(w_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">MAX_CAPTCHA</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">CHAR_SET_LEN</span>]))\n    b_out <span class=\"pl-k\">=</span> tf.Variable(b_alpha<span class=\"pl-k\">*</span>tf.random_normal([<span class=\"pl-c1\">MAX_CAPTCHA</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">CHAR_SET_LEN</span>]))\n    out <span class=\"pl-k\">=</span> tf.add(tf.matmul(dense, w_out), b_out, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>out<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> out\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train_crack_captcha_cnn</span>():\n    output <span class=\"pl-k\">=</span> crack_captcha_cnn()\n    loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>output, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>Y))\n    optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>).minimize(loss)\n \n    predict <span class=\"pl-k\">=</span> tf.reshape(output, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">MAX_CAPTCHA</span>, <span class=\"pl-c1\">CHAR_SET_LEN</span>],<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>predict<span class=\"pl-pds\">'</span></span>)\n    max_idx_p <span class=\"pl-k\">=</span> tf.argmax(predict, <span class=\"pl-c1\">2</span>)\n    max_idx_l <span class=\"pl-k\">=</span> tf.argmax(tf.reshape(Y, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">MAX_CAPTCHA</span>, <span class=\"pl-c1\">CHAR_SET_LEN</span>]), <span class=\"pl-c1\">2</span>)\n    correct_pred <span class=\"pl-k\">=</span> tf.equal(max_idx_p, max_idx_l)\n    accuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_pred, tf.float32), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>)\n \n    saver <span class=\"pl-k\">=</span> tf.train.Saver()\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.global_variables_initializer())\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> tf.global_variables():\n            <span class=\"pl-c1\">print</span>(i)\n        <span class=\"pl-k\">return</span></pre></div>\n<p>When print tf.global_variables(), it prints</p>\n<pre lang=\"&lt;tf.Variable\" data-meta=\"'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref&gt;\"><code>&lt;tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_1/bn_1/moments/Squeeze/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_1/bn_1/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_2/bn_2/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_2/bn_2/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_3/bn_3/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_3/bn_3/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_4/bn_4/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_4/bn_4/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref&gt;\n&lt;tf.Variable 'beta1_power:0' shape=() dtype=float32_ref&gt;\n&lt;tf.Variable 'beta2_power:0' shape=() dtype=float32_ref&gt;\n</code></pre>\n<p>However, when i define the same net on a computer(a GPU) with GPU-version tensorflow r1.2, it prints:</p>\n<pre><code>&lt;tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_1/bn_1/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_1/bn_1/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_2/bn_2/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_2/bn_2/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_3/bn_3/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_3/bn_3/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_4/bn_4/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'bn_4/bn_4/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref&gt;\n&lt;tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref&gt;\n</code></pre>\n<p>How could I solve this problem? Thanks again!</p>", "body_text": "@asimshankar Thanks for answering my questions. I define a CNN with BN on a computer(no-GPU) with CPU-version tensorflow r1.3. Code is below:\nimport tensorflow as tf\n\nX = tf.placeholder(tf.float32, [None, LENGTH],name='X')\nY = tf.placeholder(tf.float32, [None, MAX_CAPTCHA*CHAR_SET_LEN],name='Y')\nkeep_prob = tf.placeholder(tf.float32,name='keep_prob') # dropout\ntrain_phase = tf.placeholder(tf.bool, name='train_phase')\n \ndef batch_norm(x, beta, gamma, train_phase, scope='bn', decay=0.9, eps=1e-5):\n    with tf.variable_scope(scope):\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=decay) \n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = tf.cond(train_phase, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\n    return normed\n \ndef crack_captcha_cnn(w_alpha=0.01, b_alpha=0.1):\n    x = tf.reshape(X, shape=[-1, IMAGE_HEIGHT, IMAGE_WIDTH, 1])\n    # 4 conv layer\n    w_c1 = tf.Variable(w_alpha*tf.random_normal([5, 5, 1, 32]))\n    b_c1 = tf.Variable(b_alpha*tf.random_normal([32]))\n    conv1 = tf.nn.bias_add(tf.nn.conv2d(x, w_c1, strides=[1, 1, 1, 1], padding='SAME'), b_c1)\n    conv1 = batch_norm(conv1, tf.constant(0.0, shape=[32]), tf.random_normal(shape=[32], mean=1.0, stddev=0.02), train_phase, scope='bn_1')\n    conv1 = tf.nn.relu(conv1)\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n    conv1 = tf.nn.dropout(conv1, keep_prob)\n \n    w_c2 = tf.Variable(w_alpha*tf.random_normal([5, 5, 32, 64]))\n    b_c2 = tf.Variable(b_alpha*tf.random_normal([64]))\n    conv2 = tf.nn.bias_add(tf.nn.conv2d(conv1, w_c2, strides=[1, 1, 1, 1], padding='SAME'), b_c2)\n    conv2 = batch_norm(conv2, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_2')\n    conv2 = tf.nn.relu(conv2)\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n    conv2 = tf.nn.dropout(conv2, keep_prob)\n \n    w_c3 = tf.Variable(w_alpha*tf.random_normal([3, 3, 64, 64]))\n    b_c3 = tf.Variable(b_alpha*tf.random_normal([64]))\n    conv3 = tf.nn.bias_add(tf.nn.conv2d(conv2, w_c3, strides=[1, 1, 1, 1], padding='SAME'), b_c3)\n    conv3 = batch_norm(conv3, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_3')\n    conv3 = tf.nn.relu(conv3)\n    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n    conv3 = tf.nn.dropout(conv3, keep_prob)\n\n    w_c4 = tf.Variable(w_alpha*tf.random_normal([3, 3, 64, 64]))\n    b_c4 = tf.Variable(b_alpha*tf.random_normal([64]))\n    conv4 = tf.nn.bias_add(tf.nn.conv2d(conv3, w_c4, strides=[1, 1, 1, 1], padding='SAME'), b_c4)\n    conv4 = batch_norm(conv4, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_4')\n    conv4 = tf.nn.relu(conv4)\n    conv4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n    conv4 = tf.nn.dropout(conv4, keep_prob)\n     \n    # Fully connected layer\n    w_d = tf.Variable(w_alpha*tf.random_normal([int(LENGTH/4), 1024]))\n    b_d = tf.Variable(b_alpha*tf.random_normal([1024]))\n    dense = tf.reshape(conv4, [-1, w_d.get_shape().as_list()[0]])\n    dense = tf.nn.relu(tf.add(tf.matmul(dense, w_d), b_d))\n    dense = tf.nn.dropout(dense, keep_prob)\n \n    w_out = tf.Variable(w_alpha*tf.random_normal([1024, MAX_CAPTCHA*CHAR_SET_LEN]))\n    b_out = tf.Variable(b_alpha*tf.random_normal([MAX_CAPTCHA*CHAR_SET_LEN]))\n    out = tf.add(tf.matmul(dense, w_out), b_out, name='out')\n    return out\n\ndef train_crack_captcha_cnn():\n    output = crack_captcha_cnn()\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=Y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n \n    predict = tf.reshape(output, [-1, MAX_CAPTCHA, CHAR_SET_LEN],name='predict')\n    max_idx_p = tf.argmax(predict, 2)\n    max_idx_l = tf.argmax(tf.reshape(Y, [-1, MAX_CAPTCHA, CHAR_SET_LEN]), 2)\n    correct_pred = tf.equal(max_idx_p, max_idx_l)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n \n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in tf.global_variables():\n            print(i)\n        return\nWhen print tf.global_variables(), it prints\n<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\n<tf.Variable 'bn_1/bn_1/moments/Squeeze/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\n<tf.Variable 'bn_1/bn_1/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\n<tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n<tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_2/bn_2/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_2/bn_2/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n<tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_3/bn_3/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_3/bn_3/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n<tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_4/bn_4/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_4/bn_4/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref>\n<tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>\n<tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref>\n<tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref>\n<tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>\n<tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>\n\nHowever, when i define the same net on a computer(a GPU) with GPU-version tensorflow r1.2, it prints:\n<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\n<tf.Variable 'bn_1/bn_1/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\n<tf.Variable 'bn_1/bn_1/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\n<tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n<tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_2/bn_2/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_2/bn_2/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n<tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_3/bn_3/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_3/bn_3/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n<tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_4/bn_4/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'bn_4/bn_4/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\n<tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref>\n<tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>\n<tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref>\n<tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref>\n\nHow could I solve this problem? Thanks again!", "body": "@asimshankar Thanks for answering my questions. I define a CNN with BN on a computer(no-GPU) with CPU-version tensorflow r1.3. Code is below:\r\n``` python\r\nimport tensorflow as tf\r\n\r\nX = tf.placeholder(tf.float32, [None, LENGTH],name='X')\r\nY = tf.placeholder(tf.float32, [None, MAX_CAPTCHA*CHAR_SET_LEN],name='Y')\r\nkeep_prob = tf.placeholder(tf.float32,name='keep_prob') # dropout\r\ntrain_phase = tf.placeholder(tf.bool, name='train_phase')\r\n \r\ndef batch_norm(x, beta, gamma, train_phase, scope='bn', decay=0.9, eps=1e-5):\r\n    with tf.variable_scope(scope):\r\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\r\n        ema = tf.train.ExponentialMovingAverage(decay=decay) \r\n        def mean_var_with_update():\r\n            ema_apply_op = ema.apply([batch_mean, batch_var])\r\n            with tf.control_dependencies([ema_apply_op]):\r\n                return tf.identity(batch_mean), tf.identity(batch_var)\r\n        mean, var = tf.cond(train_phase, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\r\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\r\n    return normed\r\n \r\ndef crack_captcha_cnn(w_alpha=0.01, b_alpha=0.1):\r\n    x = tf.reshape(X, shape=[-1, IMAGE_HEIGHT, IMAGE_WIDTH, 1])\r\n    # 4 conv layer\r\n    w_c1 = tf.Variable(w_alpha*tf.random_normal([5, 5, 1, 32]))\r\n    b_c1 = tf.Variable(b_alpha*tf.random_normal([32]))\r\n    conv1 = tf.nn.bias_add(tf.nn.conv2d(x, w_c1, strides=[1, 1, 1, 1], padding='SAME'), b_c1)\r\n    conv1 = batch_norm(conv1, tf.constant(0.0, shape=[32]), tf.random_normal(shape=[32], mean=1.0, stddev=0.02), train_phase, scope='bn_1')\r\n    conv1 = tf.nn.relu(conv1)\r\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv1 = tf.nn.dropout(conv1, keep_prob)\r\n \r\n    w_c2 = tf.Variable(w_alpha*tf.random_normal([5, 5, 32, 64]))\r\n    b_c2 = tf.Variable(b_alpha*tf.random_normal([64]))\r\n    conv2 = tf.nn.bias_add(tf.nn.conv2d(conv1, w_c2, strides=[1, 1, 1, 1], padding='SAME'), b_c2)\r\n    conv2 = batch_norm(conv2, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_2')\r\n    conv2 = tf.nn.relu(conv2)\r\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv2 = tf.nn.dropout(conv2, keep_prob)\r\n \r\n    w_c3 = tf.Variable(w_alpha*tf.random_normal([3, 3, 64, 64]))\r\n    b_c3 = tf.Variable(b_alpha*tf.random_normal([64]))\r\n    conv3 = tf.nn.bias_add(tf.nn.conv2d(conv2, w_c3, strides=[1, 1, 1, 1], padding='SAME'), b_c3)\r\n    conv3 = batch_norm(conv3, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_3')\r\n    conv3 = tf.nn.relu(conv3)\r\n    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv3 = tf.nn.dropout(conv3, keep_prob)\r\n\r\n    w_c4 = tf.Variable(w_alpha*tf.random_normal([3, 3, 64, 64]))\r\n    b_c4 = tf.Variable(b_alpha*tf.random_normal([64]))\r\n    conv4 = tf.nn.bias_add(tf.nn.conv2d(conv3, w_c4, strides=[1, 1, 1, 1], padding='SAME'), b_c4)\r\n    conv4 = batch_norm(conv4, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_4')\r\n    conv4 = tf.nn.relu(conv4)\r\n    conv4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv4 = tf.nn.dropout(conv4, keep_prob)\r\n     \r\n    # Fully connected layer\r\n    w_d = tf.Variable(w_alpha*tf.random_normal([int(LENGTH/4), 1024]))\r\n    b_d = tf.Variable(b_alpha*tf.random_normal([1024]))\r\n    dense = tf.reshape(conv4, [-1, w_d.get_shape().as_list()[0]])\r\n    dense = tf.nn.relu(tf.add(tf.matmul(dense, w_d), b_d))\r\n    dense = tf.nn.dropout(dense, keep_prob)\r\n \r\n    w_out = tf.Variable(w_alpha*tf.random_normal([1024, MAX_CAPTCHA*CHAR_SET_LEN]))\r\n    b_out = tf.Variable(b_alpha*tf.random_normal([MAX_CAPTCHA*CHAR_SET_LEN]))\r\n    out = tf.add(tf.matmul(dense, w_out), b_out, name='out')\r\n    return out\r\n\r\ndef train_crack_captcha_cnn():\r\n    output = crack_captcha_cnn()\r\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=Y))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\r\n \r\n    predict = tf.reshape(output, [-1, MAX_CAPTCHA, CHAR_SET_LEN],name='predict')\r\n    max_idx_p = tf.argmax(predict, 2)\r\n    max_idx_l = tf.argmax(tf.reshape(Y, [-1, MAX_CAPTCHA, CHAR_SET_LEN]), 2)\r\n    correct_pred = tf.equal(max_idx_p, max_idx_l)\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\r\n \r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for i in tf.global_variables():\r\n            print(i)\r\n        return\r\n```\r\nWhen print tf.global_variables(), it prints\r\n```<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\r\n<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/Squeeze/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref>\r\n<tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>\r\n<tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref>\r\n<tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref>\r\n<tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>\r\n<tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>\r\n```\r\n\r\nHowever, when i define the same net on a computer(a GPU) with GPU-version tensorflow r1.2, it prints:\r\n\r\n```\r\n<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\r\n<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref>\r\n<tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>\r\n<tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref>\r\n<tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref>\r\n```\r\nHow could I solve this problem? Thanks again!"}