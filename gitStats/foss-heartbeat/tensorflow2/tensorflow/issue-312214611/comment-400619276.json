{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/400619276", "html_url": "https://github.com/tensorflow/tensorflow/issues/18311#issuecomment-400619276", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18311", "id": 400619276, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDYxOTI3Ng==", "user": {"login": "roya0045", "id": 12854129, "node_id": "MDQ6VXNlcjEyODU0MTI5", "avatar_url": "https://avatars1.githubusercontent.com/u/12854129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roya0045", "html_url": "https://github.com/roya0045", "followers_url": "https://api.github.com/users/roya0045/followers", "following_url": "https://api.github.com/users/roya0045/following{/other_user}", "gists_url": "https://api.github.com/users/roya0045/gists{/gist_id}", "starred_url": "https://api.github.com/users/roya0045/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roya0045/subscriptions", "organizations_url": "https://api.github.com/users/roya0045/orgs", "repos_url": "https://api.github.com/users/roya0045/repos", "events_url": "https://api.github.com/users/roya0045/events{/privacy}", "received_events_url": "https://api.github.com/users/roya0045/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-27T10:15:42Z", "updated_at": "2018-06-27T10:15:42Z", "author_association": "NONE", "body_html": "<p>I'm sure it can be done but I'm not sure how. I'm not familiar with c++ and the code base for tensorflow.</p>\n<p>The workaround I have found so far are implemented in <a href=\"https://github.com/roya0045/Dendritic_layer-TF-Keras-/blob/02cf170356fb0109bbcb565786cb656085be7245/tf_dendritic.py#L237\"> my latest projet</a></p>\n<p>The only issue is that I have not tested if those 4 methods break backpropagation (get the weights first, train on random data, get the weight after and just subtract the initial weight to see if it worked.)</p>\n<p>I wanted to test them but I haven't had much time to do so, if you want you could just set up a minimal network with a for loop for each version , train it and compare the weights to see if those workarounds are valid, I'd be interested in the results.</p>", "body_text": "I'm sure it can be done but I'm not sure how. I'm not familiar with c++ and the code base for tensorflow.\nThe workaround I have found so far are implemented in  my latest projet\nThe only issue is that I have not tested if those 4 methods break backpropagation (get the weights first, train on random data, get the weight after and just subtract the initial weight to see if it worked.)\nI wanted to test them but I haven't had much time to do so, if you want you could just set up a minimal network with a for loop for each version , train it and compare the weights to see if those workarounds are valid, I'd be interested in the results.", "body": "I'm sure it can be done but I'm not sure how. I'm not familiar with c++ and the code base for tensorflow.\r\n\r\nThe workaround I have found so far are implemented in [ my latest projet](https://github.com/roya0045/Dendritic_layer-TF-Keras-/blob/02cf170356fb0109bbcb565786cb656085be7245/tf_dendritic.py#L237)\r\n\r\nThe only issue is that I have not tested if those 4 methods break backpropagation (get the weights first, train on random data, get the weight after and just subtract the initial weight to see if it worked.)\r\n\r\nI wanted to test them but I haven't had much time to do so, if you want you could just set up a minimal network with a for loop for each version , train it and compare the weights to see if those workarounds are valid, I'd be interested in the results."}