{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4978", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4978/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4978/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4978/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4978", "id": 183179861, "node_id": "MDU6SXNzdWUxODMxNzk4NjE=", "number": 4978, "title": "contrib.layers and multigpu: variable scope issues", "user": {"login": "MInner", "id": 5229267, "node_id": "MDQ6VXNlcjUyMjkyNjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5229267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MInner", "html_url": "https://github.com/MInner", "followers_url": "https://api.github.com/users/MInner/followers", "following_url": "https://api.github.com/users/MInner/following{/other_user}", "gists_url": "https://api.github.com/users/MInner/gists{/gist_id}", "starred_url": "https://api.github.com/users/MInner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MInner/subscriptions", "organizations_url": "https://api.github.com/users/MInner/orgs", "repos_url": "https://api.github.com/users/MInner/repos", "events_url": "https://api.github.com/users/MInner/events{/privacy}", "received_events_url": "https://api.github.com/users/MInner/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-10-15T01:07:31Z", "updated_at": "2016-10-17T03:12:35Z", "closed_at": "2016-10-17T03:12:35Z", "author_association": "NONE", "body_html": "<p>I guess, issues regarding stuff in <code>tf.contrib</code> working nicely with tensorflow itself are also welcome here? Otherwise, I'll move it to the <code>tflearn</code> issue tracker.</p>\n<p><strong>tl;dr</strong>: using contrib.layers in multigpu setting (multiple \"towers\" in separate name scopes) leads to scope errors; using ordinary <code>tf.get_variable(..)</code> works just fine</p>\n<p>tensorflow 0.10.0<br>\ncuda 7.5</p>\n<p>basic fully connected network to classify mnist:</p>\n<pre><code>def build_fc_dnn():\n    X = tf.placeholder(tf.float32, [None, 784])\n    y = tf.placeholder(tf.float32, [None, 10])\n\n    depth = 5\n\n    last_layer = X\n    argkw = dict(num_outputs=20, activation_fn=tf.nn.relu)\n    for _ in range(depth):\n        last_layer = tf.contrib.layers.fully_connected(last_layer, **argkw)\n    logits = tf.contrib.layers.fully_connected(last_layer, num_outputs=10)\n\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits, y)\n    y_train_eq_pred = tf.equal(tf.argmax(y,1), tf.argmax(logits,1))\n    acc = tf.reduce_mean(tf.cast(y_train_eq_pred, tf.float32))\n\n    return X, y, loss, acc\n</code></pre>\n<p>and the code that builds graph (analogues to cifar multigpu example):</p>\n<pre><code>    ...\n    tower_vars = []\n    for tower_id in range(len(device_id_list)):\n        with tf.device('/gpu:%d' % tower_id):\n            with tf.name_scope('tower_%d' % tower_id) as scope:\n                X, y, loss, acc = build_model()\n                tower_vars.append([gd.compute_gradients(loss), loss, acc, (X, y)])\n                tf.get_variable_scope().reuse_variables()\n\n    tower_grads, tower_loss, tower_acc, xy_pairs = zip(*tower_vars)\n    train_op = gd.apply_gradients(average_gradients(tower_grads))\n    loss = tf.reduce_mean(tower_loss)\n    acc = tf.reduce_mean(tower_acc)\n    init_op = tf.initialize_all_variables()\n</code></pre>\n<p>if I do that, I get an error: <code>ValueError: Variable fully_connected_6/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?</code></p>\n<p>If I look into <code>tflearn</code> docs (<code>tf.contrib.layers</code> is basically <code>tflearn</code>, right?), they suggest building multigpu code as follows:</p>\n<pre><code>with tf.device('/gpu:0'):\n    # Force all Variables to reside on the CPU.\n    with tf.arg_ops([tflearn.variables.variable], device='/cpu:0'):\n        model1 = my_model(placeholder_X)\n# Reuse Variables for the next model\ntf.get_variable_scope().reuse_variables()\nwith tf.device('/gpu:1'):\n    with tf.arg_ops([tflearn.variables.variable], device='/cpu:0'):\n        model2 = my_model(placeholder_X)\n</code></pre>\n<p>those docs are probably somewhat outdated, because <code>tf.arg_ops</code> is not there anymore; the closest thing I could find is the following:</p>\n<pre><code>    arg_scope = tf.contrib.framework.arg_scope\n    create_var_op = tf.contrib.framework.python.ops.variables.variable\n    ...\n    tower_vars = []\n    for tower_id in range(len(device_id_list)):\n        with tf.device('/gpu:%d' % tower_id):\n            with tf.name_scope('tower_%d' % tower_id) as scope:\n            variables_on_cpu = arg_scope([create_var_op], device='/cpu:0')\n            with variables_on_cpu:\n                 X, y, loss, acc = build_model()\n                 tower_vars.append([gd.compute_gradients(loss), loss, acc, (X, y)])\n                 tf.get_variable_scope().reuse_variables()\n\n    tower_grads, tower_loss, tower_acc, xy_pairs = zip(*tower_vars)\n    train_op = gd.apply_gradients(average_gradients(tower_grads))\n    loss = tf.reduce_mean(tower_loss)\n    acc = tf.reduce_mean(tower_acc)\n    init_op = tf.initialize_all_variables()\n</code></pre>\n<p>but it also does not help (same error).</p>\n<p><a href=\"https://github.com/tensorflow/serving/issues/136\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/serving/issues/136/hovercard\">issue</a> with similar error message<br>\n<a href=\"https://gist.github.com/MInner/58fa334574138f1ce1359258f27af19a\">full</a> code that reproduces error</p>", "body_text": "I guess, issues regarding stuff in tf.contrib working nicely with tensorflow itself are also welcome here? Otherwise, I'll move it to the tflearn issue tracker.\ntl;dr: using contrib.layers in multigpu setting (multiple \"towers\" in separate name scopes) leads to scope errors; using ordinary tf.get_variable(..) works just fine\ntensorflow 0.10.0\ncuda 7.5\nbasic fully connected network to classify mnist:\ndef build_fc_dnn():\n    X = tf.placeholder(tf.float32, [None, 784])\n    y = tf.placeholder(tf.float32, [None, 10])\n\n    depth = 5\n\n    last_layer = X\n    argkw = dict(num_outputs=20, activation_fn=tf.nn.relu)\n    for _ in range(depth):\n        last_layer = tf.contrib.layers.fully_connected(last_layer, **argkw)\n    logits = tf.contrib.layers.fully_connected(last_layer, num_outputs=10)\n\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits, y)\n    y_train_eq_pred = tf.equal(tf.argmax(y,1), tf.argmax(logits,1))\n    acc = tf.reduce_mean(tf.cast(y_train_eq_pred, tf.float32))\n\n    return X, y, loss, acc\n\nand the code that builds graph (analogues to cifar multigpu example):\n    ...\n    tower_vars = []\n    for tower_id in range(len(device_id_list)):\n        with tf.device('/gpu:%d' % tower_id):\n            with tf.name_scope('tower_%d' % tower_id) as scope:\n                X, y, loss, acc = build_model()\n                tower_vars.append([gd.compute_gradients(loss), loss, acc, (X, y)])\n                tf.get_variable_scope().reuse_variables()\n\n    tower_grads, tower_loss, tower_acc, xy_pairs = zip(*tower_vars)\n    train_op = gd.apply_gradients(average_gradients(tower_grads))\n    loss = tf.reduce_mean(tower_loss)\n    acc = tf.reduce_mean(tower_acc)\n    init_op = tf.initialize_all_variables()\n\nif I do that, I get an error: ValueError: Variable fully_connected_6/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\nIf I look into tflearn docs (tf.contrib.layers is basically tflearn, right?), they suggest building multigpu code as follows:\nwith tf.device('/gpu:0'):\n    # Force all Variables to reside on the CPU.\n    with tf.arg_ops([tflearn.variables.variable], device='/cpu:0'):\n        model1 = my_model(placeholder_X)\n# Reuse Variables for the next model\ntf.get_variable_scope().reuse_variables()\nwith tf.device('/gpu:1'):\n    with tf.arg_ops([tflearn.variables.variable], device='/cpu:0'):\n        model2 = my_model(placeholder_X)\n\nthose docs are probably somewhat outdated, because tf.arg_ops is not there anymore; the closest thing I could find is the following:\n    arg_scope = tf.contrib.framework.arg_scope\n    create_var_op = tf.contrib.framework.python.ops.variables.variable\n    ...\n    tower_vars = []\n    for tower_id in range(len(device_id_list)):\n        with tf.device('/gpu:%d' % tower_id):\n            with tf.name_scope('tower_%d' % tower_id) as scope:\n            variables_on_cpu = arg_scope([create_var_op], device='/cpu:0')\n            with variables_on_cpu:\n                 X, y, loss, acc = build_model()\n                 tower_vars.append([gd.compute_gradients(loss), loss, acc, (X, y)])\n                 tf.get_variable_scope().reuse_variables()\n\n    tower_grads, tower_loss, tower_acc, xy_pairs = zip(*tower_vars)\n    train_op = gd.apply_gradients(average_gradients(tower_grads))\n    loss = tf.reduce_mean(tower_loss)\n    acc = tf.reduce_mean(tower_acc)\n    init_op = tf.initialize_all_variables()\n\nbut it also does not help (same error).\nissue with similar error message\nfull code that reproduces error", "body": "I guess, issues regarding stuff in `tf.contrib` working nicely with tensorflow itself are also welcome here? Otherwise, I'll move it to the `tflearn` issue tracker.\n\n**tl;dr**: using contrib.layers in multigpu setting (multiple \"towers\" in separate name scopes) leads to scope errors; using ordinary `tf.get_variable(..)` works just fine\n\ntensorflow 0.10.0\ncuda 7.5\n\nbasic fully connected network to classify mnist: \n\n```\ndef build_fc_dnn():\n    X = tf.placeholder(tf.float32, [None, 784])\n    y = tf.placeholder(tf.float32, [None, 10])\n\n    depth = 5\n\n    last_layer = X\n    argkw = dict(num_outputs=20, activation_fn=tf.nn.relu)\n    for _ in range(depth):\n        last_layer = tf.contrib.layers.fully_connected(last_layer, **argkw)\n    logits = tf.contrib.layers.fully_connected(last_layer, num_outputs=10)\n\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits, y)\n    y_train_eq_pred = tf.equal(tf.argmax(y,1), tf.argmax(logits,1))\n    acc = tf.reduce_mean(tf.cast(y_train_eq_pred, tf.float32))\n\n    return X, y, loss, acc\n```\n\nand the code that builds graph (analogues to cifar multigpu example):\n\n```\n    ...\n    tower_vars = []\n    for tower_id in range(len(device_id_list)):\n        with tf.device('/gpu:%d' % tower_id):\n            with tf.name_scope('tower_%d' % tower_id) as scope:\n                X, y, loss, acc = build_model()\n                tower_vars.append([gd.compute_gradients(loss), loss, acc, (X, y)])\n                tf.get_variable_scope().reuse_variables()\n\n    tower_grads, tower_loss, tower_acc, xy_pairs = zip(*tower_vars)\n    train_op = gd.apply_gradients(average_gradients(tower_grads))\n    loss = tf.reduce_mean(tower_loss)\n    acc = tf.reduce_mean(tower_acc)\n    init_op = tf.initialize_all_variables()\n```\n\nif I do that, I get an error: `ValueError: Variable fully_connected_6/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?`\n\nIf I look into `tflearn` docs (`tf.contrib.layers` is basically `tflearn`, right?), they suggest building multigpu code as follows:\n\n```\nwith tf.device('/gpu:0'):\n    # Force all Variables to reside on the CPU.\n    with tf.arg_ops([tflearn.variables.variable], device='/cpu:0'):\n        model1 = my_model(placeholder_X)\n# Reuse Variables for the next model\ntf.get_variable_scope().reuse_variables()\nwith tf.device('/gpu:1'):\n    with tf.arg_ops([tflearn.variables.variable], device='/cpu:0'):\n        model2 = my_model(placeholder_X)\n```\n\nthose docs are probably somewhat outdated, because `tf.arg_ops` is not there anymore; the closest thing I could find is the following:\n\n```\n    arg_scope = tf.contrib.framework.arg_scope\n    create_var_op = tf.contrib.framework.python.ops.variables.variable\n    ...\n    tower_vars = []\n    for tower_id in range(len(device_id_list)):\n        with tf.device('/gpu:%d' % tower_id):\n            with tf.name_scope('tower_%d' % tower_id) as scope:\n            variables_on_cpu = arg_scope([create_var_op], device='/cpu:0')\n            with variables_on_cpu:\n                 X, y, loss, acc = build_model()\n                 tower_vars.append([gd.compute_gradients(loss), loss, acc, (X, y)])\n                 tf.get_variable_scope().reuse_variables()\n\n    tower_grads, tower_loss, tower_acc, xy_pairs = zip(*tower_vars)\n    train_op = gd.apply_gradients(average_gradients(tower_grads))\n    loss = tf.reduce_mean(tower_loss)\n    acc = tf.reduce_mean(tower_acc)\n    init_op = tf.initialize_all_variables()\n```\n\nbut it also does not help (same error).\n\n[issue](https://github.com/tensorflow/serving/issues/136) with similar error message\n[full](https://gist.github.com/MInner/58fa334574138f1ce1359258f27af19a) code that reproduces error\n"}