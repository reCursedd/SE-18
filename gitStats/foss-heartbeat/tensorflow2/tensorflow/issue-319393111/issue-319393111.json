{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19014", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19014/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19014/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19014/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19014", "id": 319393111, "node_id": "MDU6SXNzdWUzMTkzOTMxMTE=", "number": 19014, "title": "How to quantize Mobilenet v2 ? ", "user": {"login": "WenguoLi", "id": 31765154, "node_id": "MDQ6VXNlcjMxNzY1MTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/31765154?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WenguoLi", "html_url": "https://github.com/WenguoLi", "followers_url": "https://api.github.com/users/WenguoLi/followers", "following_url": "https://api.github.com/users/WenguoLi/following{/other_user}", "gists_url": "https://api.github.com/users/WenguoLi/gists{/gist_id}", "starred_url": "https://api.github.com/users/WenguoLi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WenguoLi/subscriptions", "organizations_url": "https://api.github.com/users/WenguoLi/orgs", "repos_url": "https://api.github.com/users/WenguoLi/repos", "events_url": "https://api.github.com/users/WenguoLi/events{/privacy}", "received_events_url": "https://api.github.com/users/WenguoLi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 21, "created_at": "2018-05-02T02:50:28Z", "updated_at": "2018-11-15T15:05:16Z", "closed_at": "2018-05-05T01:35:44Z", "author_association": "NONE", "body_html": "<h2>System information</h2>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04</li>\n<li>TensorFlow installed from (source or binary):Source</li>\n<li>TensorFlow version (use command below):1.8.0rc0</li>\n<li>Python version: 2.7.12</li>\n<li>Bazel version (if compiling from source): 0.12.0</li>\n<li>GCC/Compiler version (if compiling from source): 5.4.0</li>\n<li>CUDA/cuDNN version:cuda-9.0/7.0</li>\n<li>GPU model and memory:GeForce GTX 1080/8105MiB</li>\n<li>Phone: xiaomi5 (Snapdragon 820)</li>\n</ul>\n<h2>Describe the problem</h2>\n<p>Using <strong>tf.contrib.quantize.create_training_graph()</strong> and <strong>tf.contrib.quantize.create_eval_graph()</strong>,  I<br>\nquantized training  Mobilenet v2,  and  export inference graph,  but  when toco pb to tflite,  I encountered the following error:</p>\n<p>2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.</p>\n<p>This layer MobilenetV2/expanded_conv_2/add  cannot be quantized ?</p>\n<p>Who can explain why ?</p>\n<h2>Source code / logs</h2>\n<p>bazel run --config=opt \\</p>\n<blockquote>\n<p>//tensorflow/contrib/lite/toco:toco -- <br>\n--input_file=${TRAIN_DIR}/frozen_graph.pb <br>\n--output_file=${TRAIN_DIR}/tflite_model.tflite <br>\n--input_format=TENSORFLOW_GRAPHDEF <br>\n--output_format=TFLITE <br>\n--inference_type=QUANTIZED_UINT8 <br>\n--input_shape=1,224,224,3 <br>\n--input_array=input <br>\n--output_array=MobilenetV2/Predictions/Reshape_1 <br>\n--std_value=127 <br>\n--mean_value=128 <br>\n--dump_graphviz=${TRAIN_DIR}</p>\n</blockquote>\n<pre><code>INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/frozen_graph.pb' '--output_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/tflite_model.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=MobilenetV2/Predictions/Reshape_1' '--std_value=127' '--mean_value=128' '--dump_graphviz=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan'\n2018-05-02 10:26:26.887728: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1207 operators, 1752 arrays (0 quantized)\n2018-05-02 10:26:26.934465: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1207 operators, 1752 arrays (0 quantized)\n2018-05-02 10:26:27.211718: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 120 operators, 228 arrays (1 quantized)\n2018-05-02 10:26:27.214013: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 120 operators, 228 arrays (1 quantized)\n2018-05-02 10:26:27.215023: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 67 operators, 175 arrays (1 quantized)\n2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\nTensorFlow installed from (source or binary):Source\nTensorFlow version (use command below):1.8.0rc0\nPython version: 2.7.12\nBazel version (if compiling from source): 0.12.0\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version:cuda-9.0/7.0\nGPU model and memory:GeForce GTX 1080/8105MiB\nPhone: xiaomi5 (Snapdragon 820)\n\nDescribe the problem\nUsing tf.contrib.quantize.create_training_graph() and tf.contrib.quantize.create_eval_graph(),  I\nquantized training  Mobilenet v2,  and  export inference graph,  but  when toco pb to tflite,  I encountered the following error:\n2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\nThis layer MobilenetV2/expanded_conv_2/add  cannot be quantized ?\nWho can explain why ?\nSource code / logs\nbazel run --config=opt \\\n\n//tensorflow/contrib/lite/toco:toco -- \n--input_file=${TRAIN_DIR}/frozen_graph.pb \n--output_file=${TRAIN_DIR}/tflite_model.tflite \n--input_format=TENSORFLOW_GRAPHDEF \n--output_format=TFLITE \n--inference_type=QUANTIZED_UINT8 \n--input_shape=1,224,224,3 \n--input_array=input \n--output_array=MobilenetV2/Predictions/Reshape_1 \n--std_value=127 \n--mean_value=128 \n--dump_graphviz=${TRAIN_DIR}\n\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/frozen_graph.pb' '--output_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/tflite_model.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=MobilenetV2/Predictions/Reshape_1' '--std_value=127' '--mean_value=128' '--dump_graphviz=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan'\n2018-05-02 10:26:26.887728: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1207 operators, 1752 arrays (0 quantized)\n2018-05-02 10:26:26.934465: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1207 operators, 1752 arrays (0 quantized)\n2018-05-02 10:26:27.211718: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 120 operators, 228 arrays (1 quantized)\n2018-05-02 10:26:27.214013: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 120 operators, 228 arrays (1 quantized)\n2018-05-02 10:26:27.215023: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 67 operators, 175 arrays (1 quantized)\n2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.", "body": "## System information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):1.8.0rc0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): 0.12.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version:cuda-9.0/7.0\r\n- GPU model and memory:GeForce GTX 1080/8105MiB\r\n- Phone: xiaomi5 (Snapdragon 820)\r\n\r\n## Describe the problem\r\nUsing **tf.contrib.quantize.create_training_graph()** and **tf.contrib.quantize.create_eval_graph()**,  I \r\nquantized training  Mobilenet v2,  and  export inference graph,  but  when toco pb to tflite,  I encountered the following error:\r\n\r\n2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\n\r\nThis layer MobilenetV2/expanded_conv_2/add  cannot be quantized ? \r\n\r\nWho can explain why ? \r\n\r\n## Source code / logs\r\n\r\nbazel run --config=opt \\\r\n>   //tensorflow/contrib/lite/toco:toco -- \\\r\n>   --input_file=${TRAIN_DIR}/frozen_graph.pb \\\r\n>   --output_file=${TRAIN_DIR}/tflite_model.tflite \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF \\\r\n>   --output_format=TFLITE \\\r\n>   --inference_type=QUANTIZED_UINT8 \\\r\n>   --input_shape=1,224,224,3 \\\r\n>   --input_array=input \\\r\n>   --output_array=MobilenetV2/Predictions/Reshape_1 \\\r\n>   --std_value=127 \\\r\n>   --mean_value=128 \\\r\n>   --dump_graphviz=${TRAIN_DIR}\r\n\r\n```\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/frozen_graph.pb' '--output_file=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan/tflite_model.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--input_shape=1,224,224,3' '--input_array=input' '--output_array=MobilenetV2/Predictions/Reshape_1' '--std_value=127' '--mean_value=128' '--dump_graphviz=/home/apuser/tmp/models/flowers/mobilenet_v2_1.0_224_quan'\r\n2018-05-02 10:26:26.887728: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1207 operators, 1752 arrays (0 quantized)\r\n2018-05-02 10:26:26.934465: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1207 operators, 1752 arrays (0 quantized)\r\n2018-05-02 10:26:27.211718: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 120 operators, 228 arrays (1 quantized)\r\n2018-05-02 10:26:27.214013: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 120 operators, 228 arrays (1 quantized)\r\n2018-05-02 10:26:27.215023: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 67 operators, 175 arrays (1 quantized)\r\n2018-05-02 10:26:27.215975: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array MobilenetV2/expanded_conv_2/add, which is an input to the Conv operator producing the output array MobilenetV2/expanded_conv_3/expand/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\n```\r\n\r\n"}