{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/405696749", "html_url": "https://github.com/tensorflow/tensorflow/issues/19014#issuecomment-405696749", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19014", "id": 405696749, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTY5Njc0OQ==", "user": {"login": "G-mel", "id": 7696807, "node_id": "MDQ6VXNlcjc2OTY4MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7696807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/G-mel", "html_url": "https://github.com/G-mel", "followers_url": "https://api.github.com/users/G-mel/followers", "following_url": "https://api.github.com/users/G-mel/following{/other_user}", "gists_url": "https://api.github.com/users/G-mel/gists{/gist_id}", "starred_url": "https://api.github.com/users/G-mel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/G-mel/subscriptions", "organizations_url": "https://api.github.com/users/G-mel/orgs", "repos_url": "https://api.github.com/users/G-mel/repos", "events_url": "https://api.github.com/users/G-mel/events{/privacy}", "received_events_url": "https://api.github.com/users/G-mel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-17T19:16:25Z", "updated_at": "2018-07-17T19:16:25Z", "author_association": "NONE", "body_html": "<p>I also kept on getting similar errors with using a MobileNetV2 backbone for my network.</p>\n<p>To solve my issue, I made sure to create the training quantization nodes (<code>tf.contrib.quantize.create_training_graph(quant_delay=quant_delay)</code>) in my training graph with MobileNetV2 training scope:</p>\n<pre><code>with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n    &lt;create mobilenetv2 based network&gt;\n</code></pre>\n<p>Where when I created my eval graph with quantization nodes (<code>tf.contrib.quantize.create_eval_graph()</code>), I made sure to load my MobileNetv2 model <strong>without</strong> the above training scope. This solved all of my errors of having nodes without having min/max information.</p>\n<p>I separated my training and evaluation into different scripts/functions in order to make it easier to separate the two graphs.</p>\n<p>I then <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py\">froze</a> my saved checkpoint weights from training with the graph from evaluation, then optimized them for <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py\">inference</a>, and finally converted to TFLite with <a href=\"https://github.com/tensorflow/tensorflow/blob/bfcfad55b7b3fa4a1093fa748d4241f9457b2a84/tensorflow/contrib/lite/python/tflite_convert.py\">TOCO</a> to get a quantized tflite file.</p>\n<p>Hope this helps.</p>", "body_text": "I also kept on getting similar errors with using a MobileNetV2 backbone for my network.\nTo solve my issue, I made sure to create the training quantization nodes (tf.contrib.quantize.create_training_graph(quant_delay=quant_delay)) in my training graph with MobileNetV2 training scope:\nwith tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n    <create mobilenetv2 based network>\n\nWhere when I created my eval graph with quantization nodes (tf.contrib.quantize.create_eval_graph()), I made sure to load my MobileNetv2 model without the above training scope. This solved all of my errors of having nodes without having min/max information.\nI separated my training and evaluation into different scripts/functions in order to make it easier to separate the two graphs.\nI then froze my saved checkpoint weights from training with the graph from evaluation, then optimized them for inference, and finally converted to TFLite with TOCO to get a quantized tflite file.\nHope this helps.", "body": "I also kept on getting similar errors with using a MobileNetV2 backbone for my network.\r\n\r\nTo solve my issue, I made sure to create the training quantization nodes (`tf.contrib.quantize.create_training_graph(quant_delay=quant_delay)`) in my training graph with MobileNetV2 training scope:\r\n```\r\nwith tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\r\n    <create mobilenetv2 based network>\r\n```\r\n\r\nWhere when I created my eval graph with quantization nodes (`tf.contrib.quantize.create_eval_graph()`), I made sure to load my MobileNetv2 model **without** the above training scope. This solved all of my errors of having nodes without having min/max information. \r\n\r\nI separated my training and evaluation into different scripts/functions in order to make it easier to separate the two graphs. \r\n\r\nI then [froze](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) my saved checkpoint weights from training with the graph from evaluation, then optimized them for [inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py), and finally converted to TFLite with [TOCO](https://github.com/tensorflow/tensorflow/blob/bfcfad55b7b3fa4a1093fa748d4241f9457b2a84/tensorflow/contrib/lite/python/tflite_convert.py) to get a quantized tflite file.\r\n\r\nHope this helps."}