{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9713", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9713/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9713/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9713/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9713", "id": 226736840, "node_id": "MDU6SXNzdWUyMjY3MzY4NDA=", "number": 9713, "title": "Exception during running the graph: Unable to get element from the feed as bytes.", "user": {"login": "chris-boson", "id": 6893229, "node_id": "MDQ6VXNlcjY4OTMyMjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/6893229?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chris-boson", "html_url": "https://github.com/chris-boson", "followers_url": "https://api.github.com/users/chris-boson/followers", "following_url": "https://api.github.com/users/chris-boson/following{/other_user}", "gists_url": "https://api.github.com/users/chris-boson/gists{/gist_id}", "starred_url": "https://api.github.com/users/chris-boson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chris-boson/subscriptions", "organizations_url": "https://api.github.com/users/chris-boson/orgs", "repos_url": "https://api.github.com/users/chris-boson/repos", "events_url": "https://api.github.com/users/chris-boson/events{/privacy}", "received_events_url": "https://api.github.com/users/chris-boson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-06T04:11:10Z", "updated_at": "2017-05-06T19:12:33Z", "closed_at": "2017-05-06T19:05:08Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac OS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1.0</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:<br>\ngcloud ml-engine local predict <br>\n--model-dir=$MODEL_DIR <br>\n--json-instances=\"$DATA_DIR/test2.json\" \\</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm using a beam pipeline to preprocess my text to integers bag of words, similar to this example <a href=\"https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/reddit_tft/reddit.py\">https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/reddit_tft/reddit.py</a></p>\n<pre><code>  words = tft.map(tf.string_split, inputs[name])\n  result[name + '_bow'] = tft.string_to_int(\n      words, frequency_threshold=frequency_threshold)\n</code></pre>\n<p>Preprocessing and training seem to work fine. I train a simple linear model and point to the transform function and run an experiment.</p>\n<p>When I try to run a prediction I get an error, no idea what I'm doing wrong.</p>\n<h3>Source code / logs</h3>\n<p>WARNING:root:MetaGraph has multiple signatures 2. Support for multiple signatures is<br>\nlimited. By default we select named signatures.<br>\nERROR:root:Exception during running the graph: Unable to get element from the feed a<br>\ns bytes.<br>\nTraceback (most recent call last):<br>\nFile \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 136, in <br>\nmain()<br>\nFile \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 131, in mai<br>\nn<br>\ninstances=instances)<br>\nFile \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin<br>\ne_sdk/prediction/prediction_lib.py\", line 656, in local_predict<br>\n_, predictions = model.predict(instances)<br>\nFile \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin<br>\ne_sdk/prediction/prediction_lib.py\", line 553, in predict<br>\noutputs = self._client.predict(columns, stats)<br>\nFile \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin<br>\ne_sdk/prediction/prediction_lib.py\", line 382, in predict<br>\n\"Exception during running the graph: \" + str(e))<br>\nprediction_lib.PredictionError: (4, 'Exception during running the graph: Unable to g<br>\net element from the feed as bytes.')</p>\n<pre><code>def feature_columns(vocab_size=100000):\n    result = []\n    for key in TEXT_COLUMNS:\n        column = tf.contrib.layers.sparse_column_with_integerized_feature(key, vocab_size, combiner='sum')\n    result.append(column)\nreturn result\n\nmodel_fn = tf.contrib.learn.LinearClassifier(\n      feature_columns=feature_columns(),\n      n_classes=15,\n      model_dir=output_dir\n    )  \n\ndef get_transformed_reader_input_fn(transformed_metadata,\n                                    transformed_data_paths,\n                                    batch_size,\n                                    mode):\n  \"\"\"Wrap the get input features function to provide the runtime arguments.\"\"\"\n  return input_fn_maker.build_training_input_fn(\n      metadata=transformed_metadata,\n      file_pattern=(\n          transformed_data_paths[0] if len(transformed_data_paths) == 1\n          else transformed_data_paths),\n      training_batch_size=batch_size,\n      label_keys=[LABEL_COLUMN],\n      reader=gzip_reader_fn,\n      key_feature_name='key',\n      reader_num_threads=4,\n      queue_capacity=batch_size * 2,\n      randomize_input=(mode != tf.contrib.learn.ModeKeys.EVAL),\n      num_epochs=(1 if mode == tf.contrib.learn.ModeKeys.EVAL else None))\n\n\ntransformed_metadata = metadata_io.read_metadata(\n    args.transformed_metadata_path)\nraw_metadata = metadata_io.read_metadata(args.raw_metadata_path)\n\ntrain_input_fn = get_transformed_reader_input_fn(\n    transformed_metadata, args.train_data_paths, args.batch_size,\n    tf.contrib.learn.ModeKeys.TRAIN)\n\neval_input_fn = get_transformed_reader_input_fn(\n    transformed_metadata, args.eval_data_paths, args.batch_size,\n    tf.contrib.learn.ModeKeys.EVAL)\n\nserving_input_fn = input_fn_maker.build_parsing_transforming_serving_input_fn(\n        raw_metadata,\n        args.transform_savedmodel,\n        raw_label_keys=[],\n        raw_feature_keys=model.TEXT_COLUMNS)\n\nexport_strategy = tf.contrib.learn.utils.make_export_strategy(\n    serving_input_fn,\n    default_output_alternative_key=None,\n    exports_to_keep=5,\n    as_text=True)\n\nreturn Experiment(\n    estimator=model_fn,\n    train_input_fn=train_input_fn,\n    eval_input_fn=eval_input_fn,\n    export_strategies=export_strategy,\n    eval_metrics=model.get_eval_metrics(),\n    train_monitors=[],\n    train_steps=args.train_steps,\n    eval_steps=args.eval_steps,\n    min_eval_frequency=1\n)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.1.0\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\ngcloud ml-engine local predict \n--model-dir=$MODEL_DIR \n--json-instances=\"$DATA_DIR/test2.json\" \\\n\nDescribe the problem\nI'm using a beam pipeline to preprocess my text to integers bag of words, similar to this example https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/reddit_tft/reddit.py\n  words = tft.map(tf.string_split, inputs[name])\n  result[name + '_bow'] = tft.string_to_int(\n      words, frequency_threshold=frequency_threshold)\n\nPreprocessing and training seem to work fine. I train a simple linear model and point to the transform function and run an experiment.\nWhen I try to run a prediction I get an error, no idea what I'm doing wrong.\nSource code / logs\nWARNING:root:MetaGraph has multiple signatures 2. Support for multiple signatures is\nlimited. By default we select named signatures.\nERROR:root:Exception during running the graph: Unable to get element from the feed a\ns bytes.\nTraceback (most recent call last):\nFile \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 136, in \nmain()\nFile \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 131, in mai\nn\ninstances=instances)\nFile \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\ne_sdk/prediction/prediction_lib.py\", line 656, in local_predict\n_, predictions = model.predict(instances)\nFile \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\ne_sdk/prediction/prediction_lib.py\", line 553, in predict\noutputs = self._client.predict(columns, stats)\nFile \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\ne_sdk/prediction/prediction_lib.py\", line 382, in predict\n\"Exception during running the graph: \" + str(e))\nprediction_lib.PredictionError: (4, 'Exception during running the graph: Unable to g\net element from the feed as bytes.')\ndef feature_columns(vocab_size=100000):\n    result = []\n    for key in TEXT_COLUMNS:\n        column = tf.contrib.layers.sparse_column_with_integerized_feature(key, vocab_size, combiner='sum')\n    result.append(column)\nreturn result\n\nmodel_fn = tf.contrib.learn.LinearClassifier(\n      feature_columns=feature_columns(),\n      n_classes=15,\n      model_dir=output_dir\n    )  \n\ndef get_transformed_reader_input_fn(transformed_metadata,\n                                    transformed_data_paths,\n                                    batch_size,\n                                    mode):\n  \"\"\"Wrap the get input features function to provide the runtime arguments.\"\"\"\n  return input_fn_maker.build_training_input_fn(\n      metadata=transformed_metadata,\n      file_pattern=(\n          transformed_data_paths[0] if len(transformed_data_paths) == 1\n          else transformed_data_paths),\n      training_batch_size=batch_size,\n      label_keys=[LABEL_COLUMN],\n      reader=gzip_reader_fn,\n      key_feature_name='key',\n      reader_num_threads=4,\n      queue_capacity=batch_size * 2,\n      randomize_input=(mode != tf.contrib.learn.ModeKeys.EVAL),\n      num_epochs=(1 if mode == tf.contrib.learn.ModeKeys.EVAL else None))\n\n\ntransformed_metadata = metadata_io.read_metadata(\n    args.transformed_metadata_path)\nraw_metadata = metadata_io.read_metadata(args.raw_metadata_path)\n\ntrain_input_fn = get_transformed_reader_input_fn(\n    transformed_metadata, args.train_data_paths, args.batch_size,\n    tf.contrib.learn.ModeKeys.TRAIN)\n\neval_input_fn = get_transformed_reader_input_fn(\n    transformed_metadata, args.eval_data_paths, args.batch_size,\n    tf.contrib.learn.ModeKeys.EVAL)\n\nserving_input_fn = input_fn_maker.build_parsing_transforming_serving_input_fn(\n        raw_metadata,\n        args.transform_savedmodel,\n        raw_label_keys=[],\n        raw_feature_keys=model.TEXT_COLUMNS)\n\nexport_strategy = tf.contrib.learn.utils.make_export_strategy(\n    serving_input_fn,\n    default_output_alternative_key=None,\n    exports_to_keep=5,\n    as_text=True)\n\nreturn Experiment(\n    estimator=model_fn,\n    train_input_fn=train_input_fn,\n    eval_input_fn=eval_input_fn,\n    export_strategies=export_strategy,\n    eval_metrics=model.get_eval_metrics(),\n    train_monitors=[],\n    train_steps=args.train_steps,\n    eval_steps=args.eval_steps,\n    min_eval_frequency=1\n)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\ngcloud ml-engine local predict \\\r\n    --model-dir=$MODEL_DIR \\\r\n    --json-instances=\"$DATA_DIR/test2.json\" \\\r\n\r\n### Describe the problem\r\nI'm using a beam pipeline to preprocess my text to integers bag of words, similar to this example https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/reddit_tft/reddit.py\r\n\r\n      words = tft.map(tf.string_split, inputs[name])\r\n      result[name + '_bow'] = tft.string_to_int(\r\n          words, frequency_threshold=frequency_threshold)\r\n\r\nPreprocessing and training seem to work fine. I train a simple linear model and point to the transform function and run an experiment. \r\n\r\nWhen I try to run a prediction I get an error, no idea what I'm doing wrong.\r\n\r\n### Source code / logs\r\nWARNING:root:MetaGraph has multiple signatures 2. Support for multiple signatures is\r\n limited. By default we select named signatures.\r\nERROR:root:Exception during running the graph: Unable to get element from the feed a\r\ns bytes.\r\nTraceback (most recent call last):\r\n  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 136, in <mo\r\ndule>\r\n    main()\r\n  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 131, in mai\r\nn\r\n    instances=instances)\r\n  File \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\r\ne_sdk/prediction/prediction_lib.py\", line 656, in local_predict\r\n    _, predictions = model.predict(instances)\r\n  File \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\r\ne_sdk/prediction/prediction_lib.py\", line 553, in predict\r\n    outputs = self._client.predict(columns, stats)\r\n  File \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\r\ne_sdk/prediction/prediction_lib.py\", line 382, in predict\r\n    \"Exception during running the graph: \" + str(e))\r\nprediction_lib.PredictionError: (4, 'Exception during running the graph: Unable to g\r\net element from the feed as bytes.')\r\n\r\n    def feature_columns(vocab_size=100000):\r\n        result = []\r\n        for key in TEXT_COLUMNS:\r\n            column = tf.contrib.layers.sparse_column_with_integerized_feature(key, vocab_size, combiner='sum')\r\n        result.append(column)\r\n    return result\r\n\r\n    model_fn = tf.contrib.learn.LinearClassifier(\r\n          feature_columns=feature_columns(),\r\n          n_classes=15,\r\n          model_dir=output_dir\r\n        )  \r\n\r\n    def get_transformed_reader_input_fn(transformed_metadata,\r\n                                        transformed_data_paths,\r\n                                        batch_size,\r\n                                        mode):\r\n      \"\"\"Wrap the get input features function to provide the runtime arguments.\"\"\"\r\n      return input_fn_maker.build_training_input_fn(\r\n          metadata=transformed_metadata,\r\n          file_pattern=(\r\n              transformed_data_paths[0] if len(transformed_data_paths) == 1\r\n              else transformed_data_paths),\r\n          training_batch_size=batch_size,\r\n          label_keys=[LABEL_COLUMN],\r\n          reader=gzip_reader_fn,\r\n          key_feature_name='key',\r\n          reader_num_threads=4,\r\n          queue_capacity=batch_size * 2,\r\n          randomize_input=(mode != tf.contrib.learn.ModeKeys.EVAL),\r\n          num_epochs=(1 if mode == tf.contrib.learn.ModeKeys.EVAL else None))\r\n\r\n\r\n    transformed_metadata = metadata_io.read_metadata(\r\n        args.transformed_metadata_path)\r\n    raw_metadata = metadata_io.read_metadata(args.raw_metadata_path)\r\n\r\n    train_input_fn = get_transformed_reader_input_fn(\r\n        transformed_metadata, args.train_data_paths, args.batch_size,\r\n        tf.contrib.learn.ModeKeys.TRAIN)\r\n\r\n    eval_input_fn = get_transformed_reader_input_fn(\r\n        transformed_metadata, args.eval_data_paths, args.batch_size,\r\n        tf.contrib.learn.ModeKeys.EVAL)\r\n\r\n    serving_input_fn = input_fn_maker.build_parsing_transforming_serving_input_fn(\r\n            raw_metadata,\r\n            args.transform_savedmodel,\r\n            raw_label_keys=[],\r\n            raw_feature_keys=model.TEXT_COLUMNS)\r\n\r\n    export_strategy = tf.contrib.learn.utils.make_export_strategy(\r\n        serving_input_fn,\r\n        default_output_alternative_key=None,\r\n        exports_to_keep=5,\r\n        as_text=True)\r\n\r\n    return Experiment(\r\n        estimator=model_fn,\r\n        train_input_fn=train_input_fn,\r\n        eval_input_fn=eval_input_fn,\r\n        export_strategies=export_strategy,\r\n        eval_metrics=model.get_eval_metrics(),\r\n        train_monitors=[],\r\n        train_steps=args.train_steps,\r\n        eval_steps=args.eval_steps,\r\n        min_eval_frequency=1\r\n    )\r\n\r\n"}