{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/116588567", "pull_request_review_id": 38220541, "id": 116588567, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNjU4ODU2Nw==", "diff_hunk": "@@ -0,0 +1,298 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Nadam.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import numpy as np\n+\n+from tensorflow.python.client import session\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import resource_variable_ops\n+from tensorflow.python.ops import variables\n+from tensorflow.python.platform import test\n+from tensorflow.contrib.opt.python.training import nadam_optimizer\n+\n+\n+def nadam_update_numpy(param,\n+                       g_t,\n+                       t,\n+                       m,\n+                       v,\n+                       alpha=0.001,\n+                       beta1=0.9,\n+                       beta2=0.999,\n+                       epsilon=1e-8):\n+  alpha_t = alpha * np.sqrt(1 - beta2**t) / (1 - beta1**t)\n+\n+  m_t = beta1 * m + (1 - beta1) * g_t\n+  v_t = beta2 * v + (1 - beta2) * g_t * g_t\n+\n+  m_bar = (1 - beta1) * g_t + beta1 * m_t\n+\n+  param_t = param - alpha_t * m_bar / (np.sqrt(v_t) + epsilon)\n+  return param_t, m_t, v_t\n+\n+\n+class NadamOptimizerTest(test.TestCase):\n+\n+  def doTestSparse(self, use_resource=False):\n+    for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+      with self.test_session():\n+        # Initialize variables for numpy implementation.\n+        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n+        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n+        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n+        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n+\n+        if use_resource:\n+          var0 = resource_variable_ops.ResourceVariable(var0_np)\n+          var1 = resource_variable_ops.ResourceVariable(var1_np)\n+        else:\n+          var0 = variables.Variable(var0_np)\n+          var1 = variables.Variable(var1_np)\n+        grads0_np_indices = np.array([0, 1], dtype=np.int32)\n+        grads0 = ops.IndexedSlices(\n+            constant_op.constant(grads0_np),\n+            constant_op.constant(grads0_np_indices), constant_op.constant([2]))\n+        grads1_np_indices = np.array([0, 1], dtype=np.int32)\n+        grads1 = ops.IndexedSlices(\n+            constant_op.constant(grads1_np),\n+            constant_op.constant(grads1_np_indices), constant_op.constant([2]))\n+        opt = nadam_optimizer.NadamOptimizer()\n+        update = opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n+        variables.global_variables_initializer().run()\n+\n+        # Fetch params to validate initial values\n+        self.assertAllClose([1.0, 2.0], var0.eval())\n+        self.assertAllClose([3.0, 4.0], var1.eval())\n+\n+        beta1_power, beta2_power = opt._get_beta_accumulators()\n+\n+        # Run 3 steps of Nadam\n+        for t in range(1, 4):\n+          self.assertAllCloseAccordingToType(0.9**t, beta1_power.eval())\n+          self.assertAllCloseAccordingToType(0.999**t, beta2_power.eval())\n+          update.run()\n+\n+          var0_np, m0, v0 = nadam_update_numpy(var0_np, grads0_np, t, m0, v0)\n+          var1_np, m1, v1 = nadam_update_numpy(var1_np, grads1_np, t, m1, v1)\n+\n+          # Validate updated params\n+          self.assertAllCloseAccordingToType(var0_np, var0.eval())\n+          self.assertAllCloseAccordingToType(var1_np, var1.eval())\n+\n+  def testSparse(self):\n+    self.doTestSparse(use_resource=False)\n+\n+  def testResourceSparse(self):\n+    self.doTestSparse(use_resource=True)\n+\n+  def testSparseDevicePlacement(self):\n+    for index_dtype in [dtypes.int32, dtypes.int64]:\n+      with self.test_session(force_gpu=test.is_gpu_available()):\n+        # If a GPU is available, tests that all optimizer ops can be placed on\n+        # it (i.e. they have GPU kernels).\n+        var = variables.Variable([[1.0], [2.0]])\n+        indices = constant_op.constant([0, 1], dtype=index_dtype)\n+        gathered_sum = math_ops.reduce_sum(array_ops.gather(var, indices))\n+        optimizer = nadam_optimizer.NadamOptimizer(3.0)\n+        minimize_op = optimizer.minimize(gathered_sum)\n+        variables.global_variables_initializer().run()\n+        minimize_op.run()\n+\n+  def testSparseRepeatedIndices(self):\n+    for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+      with self.test_session():\n+        repeated_index_update_var = variables.Variable(\n+            [[1.0], [2.0]], dtype=dtype)\n+        aggregated_update_var = variables.Variable(\n+            [[1.0], [2.0]], dtype=dtype)\n+        grad_repeated_index = ops.IndexedSlices(\n+            constant_op.constant(\n+                [0.1, 0.1], shape=[2, 1], dtype=dtype),\n+            constant_op.constant([1, 1]),\n+            constant_op.constant([2, 1]))\n+        grad_aggregated = ops.IndexedSlices(\n+            constant_op.constant(\n+                [0.2], shape=[1, 1], dtype=dtype),\n+            constant_op.constant([1]),\n+            constant_op.constant([2, 1]))\n+        repeated_update = nadam_optimizer.NadamOptimizer().apply_gradients(\n+            [(grad_repeated_index, repeated_index_update_var)])\n+        aggregated_update = nadam_optimizer.NadamOptimizer().apply_gradients(\n+            [(grad_aggregated, aggregated_update_var)])\n+        variables.global_variables_initializer().run()\n+        self.assertAllClose(aggregated_update_var.eval(),\n+                            repeated_index_update_var.eval())\n+        for _ in range(3):\n+          repeated_update.run()\n+          aggregated_update.run()\n+          self.assertAllClose(aggregated_update_var.eval(),\n+                              repeated_index_update_var.eval())\n+\n+  def doTestBasic(self, use_resource=False):\n+    for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+      with self.test_session():\n+        # Initialize variables for numpy implementation.\n+        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n+        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n+        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n+        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n+\n+        if use_resource:\n+          var0 = resource_variable_ops.ResourceVariable(var0_np)\n+          var1 = resource_variable_ops.ResourceVariable(var1_np)\n+        else:\n+          var0 = variables.Variable(var0_np)\n+          var1 = variables.Variable(var1_np)\n+        grads0 = constant_op.constant(grads0_np)\n+        grads1 = constant_op.constant(grads1_np)\n+        opt = nadam_optimizer.NadamOptimizer()\n+        update = opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n+        variables.global_variables_initializer().run()\n+\n+        # Fetch params to validate initial values\n+        self.assertAllClose([1.0, 2.0], var0.eval())\n+        self.assertAllClose([3.0, 4.0], var1.eval())\n+\n+        beta1_power, beta2_power = opt._get_beta_accumulators()\n+\n+        # Run 3 steps of Nadam\n+        for t in range(1, 4):\n+          self.assertAllCloseAccordingToType(0.9**t, beta1_power.eval())\n+          self.assertAllCloseAccordingToType(0.999**t, beta2_power.eval())\n+          update.run()\n+\n+          var0_np, m0, v0 = nadam_update_numpy(var0_np, grads0_np, t, m0, v0)\n+          var1_np, m1, v1 = nadam_update_numpy(var1_np, grads1_np, t, m1, v1)\n+\n+          # Validate updated params\n+          self.assertAllCloseAccordingToType(var0_np, var0.eval())\n+          self.assertAllCloseAccordingToType(var1_np, var1.eval())\n+\n+  def testBasic(self):\n+    self.doTestBasic(use_resource=False)\n+\n+  def testResourceBasic(self):\n+    self.doTestBasic(use_resource=True)\n+\n+  def testTensorLearningRate(self):\n+    for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+      with self.test_session():\n+        # Initialize variables for numpy implementation.\n+        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n+        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n+        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n+        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n+\n+        var0 = variables.Variable(var0_np)\n+        var1 = variables.Variable(var1_np)\n+        grads0 = constant_op.constant(grads0_np)\n+        grads1 = constant_op.constant(grads1_np)\n+        opt = nadam_optimizer.NadamOptimizer(constant_op.constant(0.001))\n+        update = opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n+        variables.global_variables_initializer().run()\n+\n+        # Fetch params to validate initial values\n+        self.assertAllClose([1.0, 2.0], var0.eval())\n+        self.assertAllClose([3.0, 4.0], var1.eval())\n+\n+        beta1_power, beta2_power = opt._get_beta_accumulators()\n+\n+        # Run 3 steps of Nadam\n+        for t in range(1, 4):\n+          self.assertAllCloseAccordingToType(0.9**t, beta1_power.eval())\n+          self.assertAllCloseAccordingToType(0.999**t, beta2_power.eval())\n+          update.run()\n+\n+          var0_np, m0, v0 = nadam_update_numpy(var0_np, grads0_np, t, m0, v0)\n+          var1_np, m1, v1 = nadam_update_numpy(var1_np, grads1_np, t, m1, v1)\n+\n+          # Validate updated params\n+          self.assertAllCloseAccordingToType(var0_np, var0.eval())\n+          self.assertAllCloseAccordingToType(var1_np, var1.eval())\n+\n+  def testSharing(self):\n+    for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+      with self.test_session():\n+        # Initialize variables for numpy implementation.\n+        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n+        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n+        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n+        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n+\n+        var0 = variables.Variable(var0_np)\n+        var1 = variables.Variable(var1_np)\n+        grads0 = constant_op.constant(grads0_np)\n+        grads1 = constant_op.constant(grads1_np)\n+        opt = nadam_optimizer.NadamOptimizer()\n+        update1 = opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n+        update2 = opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n+        variables.global_variables_initializer().run()\n+\n+        beta1_power, beta2_power = opt._get_beta_accumulators()\n+\n+        # Fetch params to validate initial values\n+        self.assertAllClose([1.0, 2.0], var0.eval())\n+        self.assertAllClose([3.0, 4.0], var1.eval())\n+\n+        # Run 3 steps of intertwined Nadam1 and Nadam2.\n+        for t in range(1, 4):\n+          self.assertAllCloseAccordingToType(0.9**t, beta1_power.eval())\n+          self.assertAllCloseAccordingToType(0.999**t, beta2_power.eval())\n+          if t % 2 == 0:\n+            update1.run()\n+          else:\n+            update2.run()\n+\n+          var0_np, m0, v0 = nadam_update_numpy(var0_np, grads0_np, t, m0, v0)\n+          var1_np, m1, v1 = nadam_update_numpy(var1_np, grads1_np, t, m1, v1)\n+\n+          # Validate updated params\n+          self.assertAllCloseAccordingToType(var0_np, var0.eval())\n+          self.assertAllCloseAccordingToType(var1_np, var1.eval())\n+\n+  def testTwoSessions(self):", "path": "tensorflow/contrib/opt/python/training/nadam_optimizer_test.py", "position": null, "original_position": 278, "commit_id": "2551f999d5b8e8ddcfad0f2ae506b6fbff1b5cd4", "original_commit_id": "a3c05f19311b373c3bcaf9c836af12e3eb0a34f7", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "Remove this test?", "created_at": "2017-05-15T20:09:39Z", "updated_at": "2017-05-15T23:01:10Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9889#discussion_r116588567", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9889", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/116588567"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9889#discussion_r116588567"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9889"}}, "body_html": "<p>Remove this test?</p>", "body_text": "Remove this test?"}