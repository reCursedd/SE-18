{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/343565848", "html_url": "https://github.com/tensorflow/tensorflow/issues/14446#issuecomment-343565848", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14446", "id": 343565848, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzU2NTg0OA==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-10T19:35:22Z", "updated_at": "2017-11-10T19:35:22Z", "author_association": "MEMBER", "body_html": "<p>Hey there.</p>\n<p>As you say, LLVM's APIs are not stable.  TF and XLA target LLVM built at head, not a particular LLVM release.  So if you want to build XLA, you also have to build LLVM.</p>\n<p>This should happen automatically and transparently for you as part of the TF+XLA build.</p>\n<p>The LLVM revision we check out is specified here: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L570\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L570</a></p>\n<p>Unfortunately it looks like we updated XLA to use the new fastmath API and pushed that change upstream without the corresponding change to workspace.bzl.  :(</p>\n<p>Until we upstream our next set of changes, you can either roll back to before <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/82cd76bd36217e7739e3763d50a31ef77ab832fa/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/82cd76bd36217e7739e3763d50a31ef77ab832fa\"><tt>82cd76b</tt></a>, or you can change that section in workspace.bzl to:</p>\n<pre><code>  temp_workaround_http_archive(\n      name = \"llvm\",\n      urls = [\n          \"https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/618cf290880ae9cd87b4bbf6c9b1759476f422eb.tar.gz\",\n          \"https://github.com/llvm-mirror/llvm/archive/618cf290880ae9cd87b4bbf6c9b1759476f422eb.tar.gz\",\n      ],\n      sha256 = \"ec2e032e58372c614c41b539c0309baa91843c30d7a9c6dee647dcd24be02e3c\",\n      strip_prefix = \"llvm-618cf290880ae9cd87b4bbf6c9b1759476f422eb\",\n      build_file = str(Label(\"//third_party/llvm:llvm.BUILD\")),\n      repository = tf_repo_name,\n  )\n</code></pre>\n<p>I'm sorry about this.  :-/  I don't know enough about our OSS-releasing process to know if this is avoidable on our end in some way.</p>", "body_text": "Hey there.\nAs you say, LLVM's APIs are not stable.  TF and XLA target LLVM built at head, not a particular LLVM release.  So if you want to build XLA, you also have to build LLVM.\nThis should happen automatically and transparently for you as part of the TF+XLA build.\nThe LLVM revision we check out is specified here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L570\nUnfortunately it looks like we updated XLA to use the new fastmath API and pushed that change upstream without the corresponding change to workspace.bzl.  :(\nUntil we upstream our next set of changes, you can either roll back to before 82cd76b, or you can change that section in workspace.bzl to:\n  temp_workaround_http_archive(\n      name = \"llvm\",\n      urls = [\n          \"https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/618cf290880ae9cd87b4bbf6c9b1759476f422eb.tar.gz\",\n          \"https://github.com/llvm-mirror/llvm/archive/618cf290880ae9cd87b4bbf6c9b1759476f422eb.tar.gz\",\n      ],\n      sha256 = \"ec2e032e58372c614c41b539c0309baa91843c30d7a9c6dee647dcd24be02e3c\",\n      strip_prefix = \"llvm-618cf290880ae9cd87b4bbf6c9b1759476f422eb\",\n      build_file = str(Label(\"//third_party/llvm:llvm.BUILD\")),\n      repository = tf_repo_name,\n  )\n\nI'm sorry about this.  :-/  I don't know enough about our OSS-releasing process to know if this is avoidable on our end in some way.", "body": "Hey there.\r\n\r\nAs you say, LLVM's APIs are not stable.  TF and XLA target LLVM built at head, not a particular LLVM release.  So if you want to build XLA, you also have to build LLVM.\r\n\r\nThis should happen automatically and transparently for you as part of the TF+XLA build.\r\n\r\nThe LLVM revision we check out is specified here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L570\r\n\r\nUnfortunately it looks like we updated XLA to use the new fastmath API and pushed that change upstream without the corresponding change to workspace.bzl.  :(\r\n\r\nUntil we upstream our next set of changes, you can either roll back to before 82cd76bd36, or you can change that section in workspace.bzl to:\r\n\r\n```\r\n  temp_workaround_http_archive(\r\n      name = \"llvm\",\r\n      urls = [\r\n          \"https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/618cf290880ae9cd87b4bbf6c9b1759476f422eb.tar.gz\",\r\n          \"https://github.com/llvm-mirror/llvm/archive/618cf290880ae9cd87b4bbf6c9b1759476f422eb.tar.gz\",\r\n      ],\r\n      sha256 = \"ec2e032e58372c614c41b539c0309baa91843c30d7a9c6dee647dcd24be02e3c\",\r\n      strip_prefix = \"llvm-618cf290880ae9cd87b4bbf6c9b1759476f422eb\",\r\n      build_file = str(Label(\"//third_party/llvm:llvm.BUILD\")),\r\n      repository = tf_repo_name,\r\n  )\r\n```\r\n\r\nI'm sorry about this.  :-/  I don't know enough about our OSS-releasing process to know if this is avoidable on our end in some way."}