{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/348915891", "html_url": "https://github.com/tensorflow/tensorflow/issues/15079#issuecomment-348915891", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15079", "id": 348915891, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODkxNTg5MQ==", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-04T10:05:37Z", "updated_at": "2017-12-04T10:05:37Z", "author_association": "MEMBER", "body_html": "<p>Oh, I checked the API of <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\" rel=\"nofollow\"><code>tf.nn.softmax_cross_entropy_with_logits</code></a>.</p>\n<blockquote>\n<p>NOTE: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.</p>\n<p>If using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.</p>\n</blockquote>\n<p>In all,</p>\n<ul>\n<li><code> tf.nn.softmax_cross_entropy_with_logits</code>\uff1a <code>label</code> must be valid probability distribution;</li>\n<li><code> tf.nn.sparse_softmax_cross_entropy_with_logits</code>: <code>label</code> must be class id.</li>\n</ul>\n<p>Hence, if necessary, we should check that label must be nonnegative and their sum equals one.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nlogits <span class=\"pl-k\">=</span> np.array([[<span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">0.0</span>]])\nlabels <span class=\"pl-k\">=</span> np.array([[<span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">0.0</span>]])    <span class=\"pl-c\"><span class=\"pl-c\">#</span> invalid</span>\nloss <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-c1\">print</span>(sess.run(loss))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 5.13077533]</span></pre></div>", "body_text": "Oh, I checked the API of tf.nn.softmax_cross_entropy_with_logits.\n\nNOTE: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.\nIf using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.\n\nIn all,\n\n tf.nn.softmax_cross_entropy_with_logits\uff1a label must be valid probability distribution;\n tf.nn.sparse_softmax_cross_entropy_with_logits: label must be class id.\n\nHence, if necessary, we should check that label must be nonnegative and their sum equals one.\nimport numpy as np\nimport tensorflow as tf\n\nlogits = np.array([[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]])\nlabels = np.array([[0.0, 1.0, 1.0, 0.0, 1.0, 0.0]])    # invalid\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\nwith tf.Session() as sess:\n    print(sess.run(loss))\n    # [ 5.13077533]", "body": "Oh, I checked the API of [` tf.nn.softmax_cross_entropy_with_logits `](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits). \r\n> NOTE: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.\r\n>\r\n> If using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.\r\n\r\nIn all,\r\n+ ` tf.nn.softmax_cross_entropy_with_logits`\uff1a `label` must be valid probability distribution;\r\n+ ` tf.nn.sparse_softmax_cross_entropy_with_logits`: `label` must be class id.\r\n\r\nHence, if necessary, we should check that label must be nonnegative and their sum equals one.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nlogits = np.array([[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]])\r\nlabels = np.array([[0.0, 1.0, 1.0, 0.0, 1.0, 0.0]])    # invalid\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\nwith tf.Session() as sess:\r\n    print(sess.run(loss))\r\n    # [ 5.13077533]\r\n```"}