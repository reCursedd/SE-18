{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/406974830", "html_url": "https://github.com/tensorflow/tensorflow/issues/10408#issuecomment-406974830", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10408", "id": 406974830, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjk3NDgzMA==", "user": {"login": "carusyte", "id": 1680881, "node_id": "MDQ6VXNlcjE2ODA4ODE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1680881?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carusyte", "html_url": "https://github.com/carusyte", "followers_url": "https://api.github.com/users/carusyte/followers", "following_url": "https://api.github.com/users/carusyte/following{/other_user}", "gists_url": "https://api.github.com/users/carusyte/gists{/gist_id}", "starred_url": "https://api.github.com/users/carusyte/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carusyte/subscriptions", "organizations_url": "https://api.github.com/users/carusyte/orgs", "repos_url": "https://api.github.com/users/carusyte/repos", "events_url": "https://api.github.com/users/carusyte/events{/privacy}", "received_events_url": "https://api.github.com/users/carusyte/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-23T08:06:43Z", "updated_at": "2018-07-23T08:06:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I recently resolved my model's \"memory leak\" issue, it turns out to be a counter-paradigm of how to construct and run the model(i.e various TensorOps)...<br>\nI mistakenly added <a href=\"https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay_restarts\" rel=\"nofollow\">cosine_decay_restarts</a> ops in every iteration of the training, something like:</p>\n<pre><code>while ... :\n    ....\n    dlr = tf.train.cosine_decay_restarts(\n            learning_rate=LEARNING_RATE,\n            global_step=cur_step,\n            first_decay_steps=LR_DECAY_STEPS,\n            t_mul=1.0,\n            m_mul=1.0,\n            alpha=LEARNING_RATE_ALPHA\n        )\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\n    ...\n    #train the model using the decayed learning rate\n</code></pre>\n<p>And the training python process would get killed by the OOM killer at some point.<br>\nMy instincts told me I shouldn't re-construct the <code>train.cosine_decay_restarts</code> in every loop, so after this simple remedy, the \"memory leak\" issue was gone...</p>\n<pre><code>dlr = tf.train.cosine_decay_restarts(\n    learning_rate=LEARNING_RATE,\n    global_step=cur_step,\n    first_decay_steps=LR_DECAY_STEPS,\n    t_mul=1.0,\n    m_mul=1.0,\n    alpha=LEARNING_RATE_ALPHA\n)\n...\nwhile ... :\n    ...\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\n    ...\n    #train the model using the decayed learning rate\n</code></pre>\n<p>So maybe everybody could make a coarse check of whether you are re-constructing the model in every loop...</p>", "body_text": "I recently resolved my model's \"memory leak\" issue, it turns out to be a counter-paradigm of how to construct and run the model(i.e various TensorOps)...\nI mistakenly added cosine_decay_restarts ops in every iteration of the training, something like:\nwhile ... :\n    ....\n    dlr = tf.train.cosine_decay_restarts(\n            learning_rate=LEARNING_RATE,\n            global_step=cur_step,\n            first_decay_steps=LR_DECAY_STEPS,\n            t_mul=1.0,\n            m_mul=1.0,\n            alpha=LEARNING_RATE_ALPHA\n        )\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\n    ...\n    #train the model using the decayed learning rate\n\nAnd the training python process would get killed by the OOM killer at some point.\nMy instincts told me I shouldn't re-construct the train.cosine_decay_restarts in every loop, so after this simple remedy, the \"memory leak\" issue was gone...\ndlr = tf.train.cosine_decay_restarts(\n    learning_rate=LEARNING_RATE,\n    global_step=cur_step,\n    first_decay_steps=LR_DECAY_STEPS,\n    t_mul=1.0,\n    m_mul=1.0,\n    alpha=LEARNING_RATE_ALPHA\n)\n...\nwhile ... :\n    ...\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\n    ...\n    #train the model using the decayed learning rate\n\nSo maybe everybody could make a coarse check of whether you are re-constructing the model in every loop...", "body": "I recently resolved my model's \"memory leak\" issue, it turns out to be a counter-paradigm of how to construct and run the model(i.e various TensorOps)...\r\nI mistakenly added [cosine_decay_restarts](https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay_restarts) ops in every iteration of the training, something like:\r\n```\r\nwhile ... :\r\n    ....\r\n    dlr = tf.train.cosine_decay_restarts(\r\n            learning_rate=LEARNING_RATE,\r\n            global_step=cur_step,\r\n            first_decay_steps=LR_DECAY_STEPS,\r\n            t_mul=1.0,\r\n            m_mul=1.0,\r\n            alpha=LEARNING_RATE_ALPHA\r\n        )\r\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\r\n    ...\r\n    #train the model using the decayed learning rate\r\n```\r\nAnd the training python process would get killed by the OOM killer at some point.\r\nMy instincts told me I shouldn't re-construct the `train.cosine_decay_restarts` in every loop, so after this simple remedy, the \"memory leak\" issue was gone...\r\n```\r\ndlr = tf.train.cosine_decay_restarts(\r\n    learning_rate=LEARNING_RATE,\r\n    global_step=cur_step,\r\n    first_decay_steps=LR_DECAY_STEPS,\r\n    t_mul=1.0,\r\n    m_mul=1.0,\r\n    alpha=LEARNING_RATE_ALPHA\r\n)\r\n...\r\nwhile ... :\r\n    ...\r\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\r\n    ...\r\n    #train the model using the decayed learning rate\r\n```\r\nSo maybe everybody could make a coarse check of whether you are re-constructing the model in every loop..."}