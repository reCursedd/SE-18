{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/425647147", "html_url": "https://github.com/tensorflow/tensorflow/issues/136#issuecomment-425647147", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/136", "id": 425647147, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTY0NzE0Nw==", "user": {"login": "ykp1992", "id": 20219194, "node_id": "MDQ6VXNlcjIwMjE5MTk0", "avatar_url": "https://avatars2.githubusercontent.com/u/20219194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ykp1992", "html_url": "https://github.com/ykp1992", "followers_url": "https://api.github.com/users/ykp1992/followers", "following_url": "https://api.github.com/users/ykp1992/following{/other_user}", "gists_url": "https://api.github.com/users/ykp1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/ykp1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ykp1992/subscriptions", "organizations_url": "https://api.github.com/users/ykp1992/orgs", "repos_url": "https://api.github.com/users/ykp1992/repos", "events_url": "https://api.github.com/users/ykp1992/events{/privacy}", "received_events_url": "https://api.github.com/users/ykp1992/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-29T13:57:49Z", "updated_at": "2018-09-29T13:57:49Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>I was trying the Deep MNIST from the tutorial on the site. Using the entire test set throws the ran out of memory trying to allocate 78.1KiB (So close!)<br>\nChanging the batches of the test images works, But pushing it to batches of 5000, just so I could see where exactly the program breaks, gave me a warning that it ran out of memory. The strange thing is that, while the previous attempt, without batching, completely broke the program, this didn't. I still got the result, but with a lot of warning that says:</p>\n<blockquote>\n<p>W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:217] Ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.</p>\n</blockquote>\n<p>I am using a 2GB 960M and I can confirm that the BFC option is enabled, since there were related errors with chunks when it completely crashed the first time.</p>\n</blockquote>\n<p>Do you know what happens when this message occurs now? I am confused and have no idea why we can get a result when we run out of memory.</p>", "body_text": "I was trying the Deep MNIST from the tutorial on the site. Using the entire test set throws the ran out of memory trying to allocate 78.1KiB (So close!)\nChanging the batches of the test images works, But pushing it to batches of 5000, just so I could see where exactly the program breaks, gave me a warning that it ran out of memory. The strange thing is that, while the previous attempt, without batching, completely broke the program, this didn't. I still got the result, but with a lot of warning that says:\n\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:217] Ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n\nI am using a 2GB 960M and I can confirm that the BFC option is enabled, since there were related errors with chunks when it completely crashed the first time.\n\nDo you know what happens when this message occurs now? I am confused and have no idea why we can get a result when we run out of memory.", "body": "> I was trying the Deep MNIST from the tutorial on the site. Using the entire test set throws the ran out of memory trying to allocate 78.1KiB (So close!)\r\n> Changing the batches of the test images works, But pushing it to batches of 5000, just so I could see where exactly the program breaks, gave me a warning that it ran out of memory. The strange thing is that, while the previous attempt, without batching, completely broke the program, this didn't. I still got the result, but with a lot of warning that says:\r\n> \r\n> > W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:217] Ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n> \r\n> I am using a 2GB 960M and I can confirm that the BFC option is enabled, since there were related errors with chunks when it completely crashed the first time.\r\n\r\nDo you know what happens when this message occurs now? I am confused and have no idea why we can get a result when we run out of memory."}