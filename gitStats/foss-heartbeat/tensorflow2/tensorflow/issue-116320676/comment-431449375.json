{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/431449375", "html_url": "https://github.com/tensorflow/tensorflow/issues/136#issuecomment-431449375", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/136", "id": 431449375, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTQ0OTM3NQ==", "user": {"login": "phoenix1992", "id": 16773331, "node_id": "MDQ6VXNlcjE2NzczMzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/16773331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phoenix1992", "html_url": "https://github.com/phoenix1992", "followers_url": "https://api.github.com/users/phoenix1992/followers", "following_url": "https://api.github.com/users/phoenix1992/following{/other_user}", "gists_url": "https://api.github.com/users/phoenix1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/phoenix1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phoenix1992/subscriptions", "organizations_url": "https://api.github.com/users/phoenix1992/orgs", "repos_url": "https://api.github.com/users/phoenix1992/repos", "events_url": "https://api.github.com/users/phoenix1992/events{/privacy}", "received_events_url": "https://api.github.com/users/phoenix1992/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-19T18:06:08Z", "updated_at": "2018-10-19T18:06:08Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,</p>\n<ol>\n<li>Character level Embedding Vector -&gt; Embedding lookup -&gt; LSTM1</li>\n<li>Word level Embedding Vector-&gt;Embedding lookup -&gt; LSTM2</li>\n<li>[LSTM1+LSTM2] -&gt; single layer MLP-&gt; softmax layer</li>\n<li>[LSTM1+LSTM2] -&gt; Single layer MLP-&gt; WGAN discriminator</li>\n</ol>\n<p>while I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work.</p>\n<blockquote>\n<p>ResourceExhaustedError (see above for traceback): OOM when allocating<br>\ntensor with shape[24760,100] \t [[Node:<br>\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =<br>\nSplit[T=DT_FLOAT, num_split=4,<br>\n_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,<br>\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:<br>\nbi-lstm/bidirectional_rnn/bw/bw/stack/_167 =<br>\n_Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",<br>\nsend_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",<br>\nsend_device_incarnation=1,<br>\ntensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",<br>\ntensor_type=DT_INT32,<br>\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]</p>\n</blockquote>\n<p>tensor with <em><strong>shape[24760,100]</strong></em> means 2476000_32/1024_1024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?</p>\n<p>Extra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.<br>\nThe size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.</p>\n<p><em><strong>Note</strong></em> : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.</p>\n</blockquote>\n<p>Hello, have you solved it? I have also encountered the same problem as you.</p>", "body_text": "I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,\n\nCharacter level Embedding Vector -> Embedding lookup -> LSTM1\nWord level Embedding Vector->Embedding lookup -> LSTM2\n[LSTM1+LSTM2] -> single layer MLP-> softmax layer\n[LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\n\nwhile I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work.\n\nResourceExhaustedError (see above for traceback): OOM when allocating\ntensor with shape[24760,100] \t [[Node:\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =\nSplit[T=DT_FLOAT, num_split=4,\n_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:\nbi-lstm/bidirectional_rnn/bw/bw/stack/_167 =\n_Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",\nsend_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",\nsend_device_incarnation=1,\ntensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",\ntensor_type=DT_INT32,\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\n\ntensor with shape[24760,100] means 2476000_32/1024_1024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?\nExtra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.\nThe size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.\nNote : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.\n\nHello, have you solved it? I have also encountered the same problem as you.", "body": "> I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,\r\n> \r\n> 1. Character level Embedding Vector -> Embedding lookup -> LSTM1\r\n> 2. Word level Embedding Vector->Embedding lookup -> LSTM2\r\n> 3. [LSTM1+LSTM2] -> single layer MLP-> softmax layer\r\n> 4. [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\r\n> \r\n> while I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work.\r\n> \r\n> > ResourceExhaustedError (see above for traceback): OOM when allocating\r\n> > tensor with shape[24760,100] \t [[Node:\r\n> > chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =\r\n> > Split[T=DT_FLOAT, num_split=4,\r\n> > _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,\r\n> > chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:\r\n> > bi-lstm/bidirectional_rnn/bw/bw/stack/_167 =\r\n> > _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",\r\n> > send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",\r\n> > send_device_incarnation=1,\r\n> > tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",\r\n> > tensor_type=DT_INT32,\r\n> > _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\r\n> \r\n> tensor with _**shape[24760,100]**_ means 2476000_32/1024_1024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?\r\n> \r\n> Extra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.\r\n> The size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.\r\n> \r\n> _**Note**_ : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.\r\n\r\nHello, have you solved it? I have also encountered the same problem as you.  "}