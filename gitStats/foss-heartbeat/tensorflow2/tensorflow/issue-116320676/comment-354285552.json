{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354285552", "html_url": "https://github.com/tensorflow/tensorflow/issues/136#issuecomment-354285552", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/136", "id": 354285552, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDI4NTU1Mg==", "user": {"login": "sbmaruf", "id": 32699797, "node_id": "MDQ6VXNlcjMyNjk5Nzk3", "avatar_url": "https://avatars0.githubusercontent.com/u/32699797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbmaruf", "html_url": "https://github.com/sbmaruf", "followers_url": "https://api.github.com/users/sbmaruf/followers", "following_url": "https://api.github.com/users/sbmaruf/following{/other_user}", "gists_url": "https://api.github.com/users/sbmaruf/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbmaruf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbmaruf/subscriptions", "organizations_url": "https://api.github.com/users/sbmaruf/orgs", "repos_url": "https://api.github.com/users/sbmaruf/repos", "events_url": "https://api.github.com/users/sbmaruf/events{/privacy}", "received_events_url": "https://api.github.com/users/sbmaruf/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-28T13:05:54Z", "updated_at": "2017-12-28T13:29:11Z", "author_association": "NONE", "body_html": "<p>I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,</p>\n<ol>\n<li>\n<p>Character level Embedding Vector -&gt; Embedding lookup -&gt; LSTM1</p>\n</li>\n<li>\n<p>Word level Embedding Vector-&gt;Embedding lookup -&gt; LSTM2</p>\n</li>\n<li>\n<p>[LSTM1+LSTM2] -&gt; single layer MLP-&gt; softmax layer</p>\n</li>\n<li>\n<p>[LSTM1+LSTM2] -&gt; Single layer MLP-&gt; WGAN discriminator</p>\n</li>\n</ol>\n<p>while I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work.</p>\n<blockquote>\n<p>ResourceExhaustedError (see above for traceback): OOM when allocating<br>\ntensor with shape[24760,100] \t [[Node:<br>\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =<br>\nSplit[T=DT_FLOAT, num_split=4,<br>\n_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,<br>\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:<br>\nbi-lstm/bidirectional_rnn/bw/bw/stack/_167 =<br>\n_Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",<br>\nsend_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",<br>\nsend_device_incarnation=1,<br>\ntensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",<br>\ntensor_type=DT_INT32,<br>\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]</p>\n</blockquote>\n<p>tensor with <em><strong>shape[24760,100]</strong></em> means 2476000<em>32/1024</em>1024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?</p>\n<p>Extra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.<br>\nThe size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.</p>\n<p><em><strong>Note</strong></em> : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.</p>", "body_text": "I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,\n\n\nCharacter level Embedding Vector -> Embedding lookup -> LSTM1\n\n\nWord level Embedding Vector->Embedding lookup -> LSTM2\n\n\n[LSTM1+LSTM2] -> single layer MLP-> softmax layer\n\n\n[LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\n\n\nwhile I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work.\n\nResourceExhaustedError (see above for traceback): OOM when allocating\ntensor with shape[24760,100] \t [[Node:\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =\nSplit[T=DT_FLOAT, num_split=4,\n_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,\nchars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:\nbi-lstm/bidirectional_rnn/bw/bw/stack/_167 =\n_Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",\nsend_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",\nsend_device_incarnation=1,\ntensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",\ntensor_type=DT_INT32,\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\n\ntensor with shape[24760,100] means 247600032/10241024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?\nExtra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.\nThe size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.\nNote : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.", "body": "I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,\r\n\r\n1. Character level Embedding Vector -> Embedding lookup -> LSTM1\r\n\r\n2. Word level Embedding Vector->Embedding lookup -> LSTM2 \r\n\r\n3. [LSTM1+LSTM2] -> single layer MLP-> softmax layer\r\n\r\n4. [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\r\n\r\nwhile I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work. \r\n\r\n> ResourceExhaustedError (see above for traceback): OOM when allocating\r\n> tensor with shape[24760,100] \t [[Node:\r\n> chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =\r\n> Split[T=DT_FLOAT, num_split=4,\r\n> _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,\r\n> chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:\r\n> bi-lstm/bidirectional_rnn/bw/bw/stack/_167 =\r\n> _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",\r\n> send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",\r\n> send_device_incarnation=1,\r\n> tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",\r\n> tensor_type=DT_INT32,\r\n> _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\ntensor with ***shape[24760,100]*** means 2476000*32/1024*1024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?\r\n\r\nExtra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.\r\nThe size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.\r\n\r\n***Note*** : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch."}