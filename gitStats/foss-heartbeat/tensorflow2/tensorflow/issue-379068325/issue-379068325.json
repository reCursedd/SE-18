{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23621", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23621/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23621/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23621/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23621", "id": 379068325, "node_id": "MDU6SXNzdWUzNzkwNjgzMjU=", "number": 23621, "title": "tf.assign does not support gradient?", "user": {"login": "ahatamiz", "id": 26806394, "node_id": "MDQ6VXNlcjI2ODA2Mzk0", "avatar_url": "https://avatars2.githubusercontent.com/u/26806394?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahatamiz", "html_url": "https://github.com/ahatamiz", "followers_url": "https://api.github.com/users/ahatamiz/followers", "following_url": "https://api.github.com/users/ahatamiz/following{/other_user}", "gists_url": "https://api.github.com/users/ahatamiz/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahatamiz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahatamiz/subscriptions", "organizations_url": "https://api.github.com/users/ahatamiz/orgs", "repos_url": "https://api.github.com/users/ahatamiz/repos", "events_url": "https://api.github.com/users/ahatamiz/events{/privacy}", "received_events_url": "https://api.github.com/users/ahatamiz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-09T08:34:38Z", "updated_at": "2018-11-13T14:37:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a bug. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1 LTS</li>\n<li>TensorFlow version (use command below):1.11</li>\n<li>Python version:3.6</li>\n<li>GCC/Compiler version (if compiling from source):</li>\n<li>CUDA/cuDNN version:CUDA release 9.0, V9.0.176</li>\n<li>GPU model and memory: TITAN Xp / 12Gb</li>\n</ul>\n<p>You can collect some of this information using our environment capture <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">script</a><br>\nYou can also obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<p><strong>Describe the current behavior</strong><br>\nIt seems like tf.assign does not support gradients. In the forward pass, everything seems to work ok. But when I try to do backpropogate the gradients,it does not work. The error specifies that there are no variables to optimize which is not certainly the case.<br>\n<strong>Describe the expected behavior</strong><br>\ntf.assign should simply allow for gradient to flow, just like its python counterpart Python assign operator.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef func_foo(x):\n\n    phi = tf.Variable(tf.zeros([1,10,10,1], dtype='float32'), \n                    dtype='float32',trainable=False)\n\n    phi=tf.assign(phi,x)\n\n    c3=tf.nn.sigmoid(phi)\n    c4=tf.reduce_mean(c3)\n\nreturn 1-c4\n\na = np.random.randint(2, size=(10,10))\nk = np.array([[1,1,1],[1,1,1],[1,1,1]],dtype=np.float32)\nflip = [slice(None, None, -1), slice(None, None, -1)]\nk = k[flip]\n\na=a.astype(np.float32)\na_tensor = tf.reshape(a, [1, 10, 10, 1])\nk_weight = tf.reshape(np.array(k), [3,3,1,1])\n\nc2=tf.layers.conv2d(a_tensor,filters=1, kernel_size=3, strides=1, padding=\"same\",activation=tf.nn.relu) \ntotal_loss=func_foo(c2)    \ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)\ninit = tf.initialize_all_variables()\nsess=tf.Session()\nwith tf.Session() as sess:\n    init = tf.initialize_all_variables()\n    sess.run(init)\n    _,=sess.run([train_op])``\n</code></pre>\n<p><strong>Other info / logs</strong><br>\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.</p>\n<p>Traceback (most recent call last):<br>\nFile \"test_tf2.py\", line 31, in <br>\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)<br>\nFile \"/home/ali/anaconda3/envs/tf19/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 406, in minimize<br>\n([str(v) for _, v in grads_and_vars], loss))<br>\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"&lt;tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 1) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'conv2d/bias:0' shape=(1,) dtype=float32_ref&gt;\"] and loss Tensor(\"sub:0\", shape=(), dtype=float32).</p>", "body_text": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1 LTS\nTensorFlow version (use command below):1.11\nPython version:3.6\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:CUDA release 9.0, V9.0.176\nGPU model and memory: TITAN Xp / 12Gb\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the current behavior\nIt seems like tf.assign does not support gradients. In the forward pass, everything seems to work ok. But when I try to do backpropogate the gradients,it does not work. The error specifies that there are no variables to optimize which is not certainly the case.\nDescribe the expected behavior\ntf.assign should simply allow for gradient to flow, just like its python counterpart Python assign operator.\nCode to reproduce the issue\nimport tensorflow as tf\nimport numpy as np\n\ndef func_foo(x):\n\n    phi = tf.Variable(tf.zeros([1,10,10,1], dtype='float32'), \n                    dtype='float32',trainable=False)\n\n    phi=tf.assign(phi,x)\n\n    c3=tf.nn.sigmoid(phi)\n    c4=tf.reduce_mean(c3)\n\nreturn 1-c4\n\na = np.random.randint(2, size=(10,10))\nk = np.array([[1,1,1],[1,1,1],[1,1,1]],dtype=np.float32)\nflip = [slice(None, None, -1), slice(None, None, -1)]\nk = k[flip]\n\na=a.astype(np.float32)\na_tensor = tf.reshape(a, [1, 10, 10, 1])\nk_weight = tf.reshape(np.array(k), [3,3,1,1])\n\nc2=tf.layers.conv2d(a_tensor,filters=1, kernel_size=3, strides=1, padding=\"same\",activation=tf.nn.relu) \ntotal_loss=func_foo(c2)    \ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)\ninit = tf.initialize_all_variables()\nsess=tf.Session()\nwith tf.Session() as sess:\n    init = tf.initialize_all_variables()\n    sess.run(init)\n    _,=sess.run([train_op])``\n\nOther info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\nTraceback (most recent call last):\nFile \"test_tf2.py\", line 31, in \ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)\nFile \"/home/ali/anaconda3/envs/tf19/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 406, in minimize\n([str(v) for _, v in grads_and_vars], loss))\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 1) dtype=float32_ref>\", \"<tf.Variable 'conv2d/bias:0' shape=(1,) dtype=float32_ref>\"] and loss Tensor(\"sub:0\", shape=(), dtype=float32).", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1 LTS\r\n- TensorFlow version (use command below):1.11\r\n- Python version:3.6\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:CUDA release 9.0, V9.0.176\r\n- GPU model and memory: TITAN Xp / 12Gb\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nIt seems like tf.assign does not support gradients. In the forward pass, everything seems to work ok. But when I try to do backpropogate the gradients,it does not work. The error specifies that there are no variables to optimize which is not certainly the case. \r\n**Describe the expected behavior**\r\ntf.assign should simply allow for gradient to flow, just like its python counterpart Python assign operator.\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    def func_foo(x):\r\n \r\n        phi = tf.Variable(tf.zeros([1,10,10,1], dtype='float32'), \r\n                        dtype='float32',trainable=False)\r\n\r\n        phi=tf.assign(phi,x)\r\n\r\n        c3=tf.nn.sigmoid(phi)\r\n        c4=tf.reduce_mean(c3)\r\n\r\n    return 1-c4\r\n\r\n    a = np.random.randint(2, size=(10,10))\r\n    k = np.array([[1,1,1],[1,1,1],[1,1,1]],dtype=np.float32)\r\n    flip = [slice(None, None, -1), slice(None, None, -1)]\r\n    k = k[flip]\r\n\r\n    a=a.astype(np.float32)\r\n    a_tensor = tf.reshape(a, [1, 10, 10, 1])\r\n    k_weight = tf.reshape(np.array(k), [3,3,1,1])\r\n\r\n    c2=tf.layers.conv2d(a_tensor,filters=1, kernel_size=3, strides=1, padding=\"same\",activation=tf.nn.relu) \r\n    total_loss=func_foo(c2)    \r\n    train_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)\r\n    init = tf.initialize_all_variables()\r\n    sess=tf.Session()\r\n    with tf.Session() as sess:\r\n        init = tf.initialize_all_variables()\r\n        sess.run(init)\r\n        _,=sess.run([train_op])``\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"test_tf2.py\", line 31, in <module>\r\n    train_op = tf.train.AdamOptimizer(1e-3).minimize(total_loss,colocate_gradients_with_ops=True)\r\n  File \"/home/ali/anaconda3/envs/tf19/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 406, in minimize\r\n    ([str(v) for _, v in grads_and_vars], loss))\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 1) dtype=float32_ref>\", \"<tf.Variable 'conv2d/bias:0' shape=(1,) dtype=float32_ref>\"] and loss Tensor(\"sub:0\", shape=(), dtype=float32).\r\n\r\n"}