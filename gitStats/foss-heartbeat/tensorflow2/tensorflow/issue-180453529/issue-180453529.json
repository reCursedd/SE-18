{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4703", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4703/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4703/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4703/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4703", "id": 180453529, "node_id": "MDU6SXNzdWUxODA0NTM1Mjk=", "number": 4703, "title": "Conditional print doesn't work appropriately as the control flow describe", "user": {"login": "Cvikli", "id": 11393096, "node_id": "MDQ6VXNlcjExMzkzMDk2", "avatar_url": "https://avatars0.githubusercontent.com/u/11393096?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cvikli", "html_url": "https://github.com/Cvikli", "followers_url": "https://api.github.com/users/Cvikli/followers", "following_url": "https://api.github.com/users/Cvikli/following{/other_user}", "gists_url": "https://api.github.com/users/Cvikli/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cvikli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cvikli/subscriptions", "organizations_url": "https://api.github.com/users/Cvikli/orgs", "repos_url": "https://api.github.com/users/Cvikli/repos", "events_url": "https://api.github.com/users/Cvikli/events{/privacy}", "received_events_url": "https://api.github.com/users/Cvikli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-10-01T12:12:26Z", "updated_at": "2016-10-03T05:24:11Z", "closed_at": "2016-10-03T05:24:11Z", "author_association": "NONE", "body_html": "<p>I stucked with this problem. I want to check if gradients or variables are NaN before applying them. Print conditionally the name of the variable and a value for both gradient and variable.</p>\n<h3>Environment info</h3>\n<p>Operating System: Linux<br>\nInstalled version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n-rw-r--r-- 1 root root 189170 Jul 11 23:19 /opt/cuda/lib/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root     16 Jul 11 23:19 /opt/cuda/lib/libcudart.so -&gt; libcudart.so.7.5<br>\nlrwxrwxrwx 1 root root     19 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18<br>\n-rwxr-xr-x 1 root root 311596 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5.18<br>\n-rw-r--r-- 1 root root 558020 Jul 11 23:19 /opt/cuda/lib/libcudart_static.a</p>\n<h3>Tensorflow version:</h3>\n<p>I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally<br>\nBackend Qt5Agg is interactive backend. Turning interactive mode on.<br>\n0.9.0</p>\n<h3>Bazel</h3>\n<p>If installed from source, provide</p>\n<p>Extracting Bazel installation...<br>\nBuild label: 0.3.1- (@non-git)<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Wed Aug 31 19:34:37 2016 (1472672077)<br>\nBuild timestamp: 1472672077<br>\nBuild timestamp as int: 1472672077</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>I reproduced with MNIST. I tried many different way in my code. Checked documentation, and try to explicitly restrict control flow, and still no success. The tf.print runs anyway, I didn't find anything that this is on purpose, or how to manage it.</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# Import data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef main(_):\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n    W = tf.Variable(tf.zeros([784, 10]))\n    b = tf.Variable(tf.zeros([10]))\n    y = tf.matmul(x, W) + b\n\n    # Define loss and optimizer\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n    train_step = tf.train.GradientDescentOptimizer(0.5)\n    gvs = train_step.compute_gradients(cross_entropy)\n\n    gvs = [(tf.select(tf.is_nan(grad), grad, tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)])), var) for grad, var in gvs]\n\n    minimizer = train_step.apply_gradients(gvs)\n\n    sess = tf.InteractiveSession()\n    # Train\n    tf.initialize_all_variables().run()\n    for _ in range(1000):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        sess.run(minimizer, feed_dict={x: batch_xs, y_: batch_ys})\n\n    # Test trained model\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                        y_: mnist.test.labels}))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='/tmp/data',\n                        help='Directory for storing data')\n    FLAGS = parser.parse_args()\n    tf.app.run()\n</code></pre>\n<h3>What other attempted solutions have you tried?</h3>\n<h2>1 with control flow restriction + replacing the condition with tf.greater or tf.is_finite, etc.</h2>\n<pre><code>    new_gvs = list()\n    for grad, var in gvs :\n        # c = tf.greater(grad, 1)\n        c = tf.is_finite(var)\n        with tf.control_dependencies([c]):\n            newGrad = tf.select(c, tf.Print(grad, [grad, var, tf.is_nan(grad), c]), grad)\n            with tf.control_dependencies([newGrad]):\n                new_gvs.append((newGrad, var))\n    gvs = new_gvs\n</code></pre>\n<h2>2 with is_nan 'var'</h2>\n<pre><code>gvs = [(tf.select(tf.is_nan(var), tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)]), grad), var) for grad, var in gvs]\n</code></pre>\n<h3>Logs or other output that would be helpful</h3>\n<p>It just logs everything, as if there were no condition.</p>", "body_text": "I stucked with this problem. I want to check if gradients or variables are NaN before applying them. Print conditionally the name of the variable and a value for both gradient and variable.\nEnvironment info\nOperating System: Linux\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root 189170 Jul 11 23:19 /opt/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jul 11 23:19 /opt/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jul 11 23:19 /opt/cuda/lib/libcudart_static.a\nTensorflow version:\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nBackend Qt5Agg is interactive backend. Turning interactive mode on.\n0.9.0\nBazel\nIf installed from source, provide\nExtracting Bazel installation...\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Aug 31 19:34:37 2016 (1472672077)\nBuild timestamp: 1472672077\nBuild timestamp as int: 1472672077\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nI reproduced with MNIST. I tried many different way in my code. Checked documentation, and try to explicitly restrict control flow, and still no success. The tf.print runs anyway, I didn't find anything that this is on purpose, or how to manage it.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# Import data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef main(_):\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n    W = tf.Variable(tf.zeros([784, 10]))\n    b = tf.Variable(tf.zeros([10]))\n    y = tf.matmul(x, W) + b\n\n    # Define loss and optimizer\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n    train_step = tf.train.GradientDescentOptimizer(0.5)\n    gvs = train_step.compute_gradients(cross_entropy)\n\n    gvs = [(tf.select(tf.is_nan(grad), grad, tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)])), var) for grad, var in gvs]\n\n    minimizer = train_step.apply_gradients(gvs)\n\n    sess = tf.InteractiveSession()\n    # Train\n    tf.initialize_all_variables().run()\n    for _ in range(1000):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        sess.run(minimizer, feed_dict={x: batch_xs, y_: batch_ys})\n\n    # Test trained model\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                        y_: mnist.test.labels}))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='/tmp/data',\n                        help='Directory for storing data')\n    FLAGS = parser.parse_args()\n    tf.app.run()\n\nWhat other attempted solutions have you tried?\n1 with control flow restriction + replacing the condition with tf.greater or tf.is_finite, etc.\n    new_gvs = list()\n    for grad, var in gvs :\n        # c = tf.greater(grad, 1)\n        c = tf.is_finite(var)\n        with tf.control_dependencies([c]):\n            newGrad = tf.select(c, tf.Print(grad, [grad, var, tf.is_nan(grad), c]), grad)\n            with tf.control_dependencies([newGrad]):\n                new_gvs.append((newGrad, var))\n    gvs = new_gvs\n\n2 with is_nan 'var'\ngvs = [(tf.select(tf.is_nan(var), tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)]), grad), var) for grad, var in gvs]\n\nLogs or other output that would be helpful\nIt just logs everything, as if there were no condition.", "body": "I stucked with this problem. I want to check if gradients or variables are NaN before applying them. Print conditionally the name of the variable and a value for both gradient and variable. \n### Environment info\n\nOperating System: Linux\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root 189170 Jul 11 23:19 /opt/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jul 11 23:19 /opt/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jul 11 23:19 /opt/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jul 11 23:19 /opt/cuda/lib/libcudart_static.a\n### Tensorflow version:\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nBackend Qt5Agg is interactive backend. Turning interactive mode on.\n0.9.0\n### Bazel\n\nIf installed from source, provide \n\nExtracting Bazel installation...\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Aug 31 19:34:37 2016 (1472672077)\nBuild timestamp: 1472672077\nBuild timestamp as int: 1472672077\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI reproduced with MNIST. I tried many different way in my code. Checked documentation, and try to explicitly restrict control flow, and still no success. The tf.print runs anyway, I didn't find anything that this is on purpose, or how to manage it. \n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# Import data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef main(_):\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n    W = tf.Variable(tf.zeros([784, 10]))\n    b = tf.Variable(tf.zeros([10]))\n    y = tf.matmul(x, W) + b\n\n    # Define loss and optimizer\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n    train_step = tf.train.GradientDescentOptimizer(0.5)\n    gvs = train_step.compute_gradients(cross_entropy)\n\n    gvs = [(tf.select(tf.is_nan(grad), grad, tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)])), var) for grad, var in gvs]\n\n    minimizer = train_step.apply_gradients(gvs)\n\n    sess = tf.InteractiveSession()\n    # Train\n    tf.initialize_all_variables().run()\n    for _ in range(1000):\n        batch_xs, batch_ys = mnist.train.next_batch(100)\n        sess.run(minimizer, feed_dict={x: batch_xs, y_: batch_ys})\n\n    # Test trained model\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                        y_: mnist.test.labels}))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str, default='/tmp/data',\n                        help='Directory for storing data')\n    FLAGS = parser.parse_args()\n    tf.app.run()\n```\n### What other attempted solutions have you tried?\n## 1 with control flow restriction + replacing the condition with tf.greater or tf.is_finite, etc.\n\n```\n    new_gvs = list()\n    for grad, var in gvs :\n        # c = tf.greater(grad, 1)\n        c = tf.is_finite(var)\n        with tf.control_dependencies([c]):\n            newGrad = tf.select(c, tf.Print(grad, [grad, var, tf.is_nan(grad), c]), grad)\n            with tf.control_dependencies([newGrad]):\n                new_gvs.append((newGrad, var))\n    gvs = new_gvs\n```\n## 2 with is_nan 'var'\n\n```\ngvs = [(tf.select(tf.is_nan(var), tf.Print(grad, [grad, var, tf.is_nan(grad), tf.is_nan(var)]), grad), var) for grad, var in gvs]\n```\n### Logs or other output that would be helpful\n\nIt just logs everything, as if there were no condition.\n"}