{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280421351", "html_url": "https://github.com/tensorflow/tensorflow/issues/7551#issuecomment-280421351", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7551", "id": 280421351, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDQyMTM1MQ==", "user": {"login": "taion", "id": 3112159, "node_id": "MDQ6VXNlcjMxMTIxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3112159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taion", "html_url": "https://github.com/taion", "followers_url": "https://api.github.com/users/taion/followers", "following_url": "https://api.github.com/users/taion/following{/other_user}", "gists_url": "https://api.github.com/users/taion/gists{/gist_id}", "starred_url": "https://api.github.com/users/taion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taion/subscriptions", "organizations_url": "https://api.github.com/users/taion/orgs", "repos_url": "https://api.github.com/users/taion/repos", "events_url": "https://api.github.com/users/taion/events{/privacy}", "received_events_url": "https://api.github.com/users/taion/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-16T18:46:50Z", "updated_at": "2017-02-16T18:46:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Clearer description of the issue. My per-epoch timings using the 1.0.0 gpu-devel Docker image on an AWS p2.xlarge, using WRN-16-4 without dropout:</p>\n<table>\n<thead>\n<tr>\n<th>Data format</th>\n<th>Fused batch norm</th>\n<th>Batch time (ms)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NHWC</td>\n<td>No</td>\n<td>340</td>\n</tr>\n<tr>\n<td>NHWC</td>\n<td>Yes</td>\n<td>320</td>\n</tr>\n<tr>\n<td>NCHW</td>\n<td>No</td>\n<td><strong>1740</strong></td>\n</tr>\n<tr>\n<td>NCHW</td>\n<td>Yes</td>\n<td>270</td>\n</tr>\n</tbody>\n</table>\n<p>I believe this is happening because the broadcasting on the unfused batch norm hits an extreme slow path.</p>\n<p>This is problematic because if I only follow the \"use NCHW\" recommendation from the performance guide, my models actually run hugely slower. This tripped me up quite a bit.</p>", "body_text": "Clearer description of the issue. My per-epoch timings using the 1.0.0 gpu-devel Docker image on an AWS p2.xlarge, using WRN-16-4 without dropout:\n\n\n\nData format\nFused batch norm\nBatch time (ms)\n\n\n\n\nNHWC\nNo\n340\n\n\nNHWC\nYes\n320\n\n\nNCHW\nNo\n1740\n\n\nNCHW\nYes\n270\n\n\n\nI believe this is happening because the broadcasting on the unfused batch norm hits an extreme slow path.\nThis is problematic because if I only follow the \"use NCHW\" recommendation from the performance guide, my models actually run hugely slower. This tripped me up quite a bit.", "body": "Clearer description of the issue. My per-epoch timings using the 1.0.0 gpu-devel Docker image on an AWS p2.xlarge, using WRN-16-4 without dropout:\r\n\r\nData format | Fused batch norm | Batch time (ms)\r\n-|-|-\r\nNHWC | No | 340\r\nNHWC | Yes | 320\r\nNCHW | No | **1740**\r\nNCHW | Yes | 270\r\n\r\nI believe this is happening because the broadcasting on the unfused batch norm hits an extreme slow path.\r\n\r\nThis is problematic because if I only follow the \"use NCHW\" recommendation from the performance guide, my models actually run hugely slower. This tripped me up quite a bit."}