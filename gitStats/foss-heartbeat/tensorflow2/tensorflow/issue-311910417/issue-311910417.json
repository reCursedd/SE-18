{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18290", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18290/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18290/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18290/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18290", "id": 311910417, "node_id": "MDU6SXNzdWUzMTE5MTA0MTc=", "number": 18290, "title": "Tensorflow reimplementation performs significantly worse than original POC", "user": {"login": "tastyminerals", "id": 7676160, "node_id": "MDQ6VXNlcjc2NzYxNjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7676160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tastyminerals", "html_url": "https://github.com/tastyminerals", "followers_url": "https://api.github.com/users/tastyminerals/followers", "following_url": "https://api.github.com/users/tastyminerals/following{/other_user}", "gists_url": "https://api.github.com/users/tastyminerals/gists{/gist_id}", "starred_url": "https://api.github.com/users/tastyminerals/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tastyminerals/subscriptions", "organizations_url": "https://api.github.com/users/tastyminerals/orgs", "repos_url": "https://api.github.com/users/tastyminerals/repos", "events_url": "https://api.github.com/users/tastyminerals/events{/privacy}", "received_events_url": "https://api.github.com/users/tastyminerals/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-04-06T09:37:01Z", "updated_at": "2018-04-13T08:23:42Z", "closed_at": "2018-04-13T08:23:42Z", "author_association": "NONE", "body_html": "<h3>Intro</h3>\n<p>I have asked this question several times on stackoverflow using different wording and level of details yet didn't get a single answer. Given the nature of the problem it is understandable, you cannot debug the network without training data and code. Still, maybe there is something obvious that I am missing here. We are trying to rewrite successful Torch POC binary classifier using Tensorflow to put it into production.</p>\n<h3>Torch POC</h3>\n<p>Torch model is a sequential binary classifier that works on word sequences and attempts to predict if the current word belongs to class 1 or 0. The model is quite simple, we have embedding layer and a special feature layer which we sum together before feeding the result vector into LSTM / GRU cell. At the output we do linear transform with sigmoid, compute binary cross entropy loss and update our parameters. Depending on the vocabulary size the model consists of 700k - 1000k params.</p>\n<h3>Tensorflow reimplementation</h3>\n<p>We have been using standard Tensorflow language model trainable on Penn Treebank dataset as our code base. We have adapted it to the point where it looks identical to our Torch POC (same hyperparams and equal number of parameters) and started training.</p>\n<h3>Problem</h3>\n<p>It quickly became clear that Tensorflow reimplemetation does not learn anything even though the loss drops and test dataset shows error decrease: <code>Test loss reduced: 97690.06433105469 --&gt; 9929.968887329102</code>. Loading the trained model and quering it with words showed that the model predictions are garbage. Besides the loss values are different for Torch and Tensorflow.</p>\n<h3>Tensorflow implementation (main part):</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>################### PLACEHOLDERS ####################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> We use placeholders for the word, feature inputs and corresponding targets</span>\n<span class=\"pl-c1\">self</span>.words_input <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [config.batch_size, config.seq_length])\n<span class=\"pl-c1\">self</span>.feats_input <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [config.batch_size, config.seq_length, config.nfeats])\n<span class=\"pl-c1\">self</span>.targets_input <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [config.batch_size, config.seq_length])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>################### VARIABLES ####################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> We use variables for trainable network params like word embeddings, weights and biases</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> select params initialization</span>\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.init_method <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>xavier<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-c1\">self</span>.initer <span class=\"pl-k\">=</span> tf.contrib.layers.xavier_initializer(<span class=\"pl-v\">uniform</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-k\">elif</span> <span class=\"pl-c1\">self</span>.init_method <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>uniform<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-c1\">self</span>.initer <span class=\"pl-k\">=</span> tf.random_uniform_initializer(<span class=\"pl-k\">-</span>config.init_scale, config.init_scale)\n\n<span class=\"pl-k\">elif</span> <span class=\"pl-c1\">self</span>.init_method <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>normal<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-c1\">self</span>.initer <span class=\"pl-k\">=</span> tf.random_normal_initializer()\n\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-c1\">self</span>.initer <span class=\"pl-k\">=</span> tf.zeros_initializer()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> word embeddings</span>\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input_layer<span class=\"pl-pds\">\"</span></span>):\n    <span class=\"pl-c1\">self</span>.embedding <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">self</span>.vocab_size, <span class=\"pl-c1\">self</span>.input_size],\n                                     <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.initer, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> feature weights for linear transform</span>\n    <span class=\"pl-c1\">self</span>.feature_weight <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feature_weigths<span class=\"pl-pds\">\"</span></span>, [config.nfeats, config.input_size],\n                                          <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.initer, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> feature biases for linear transform</span>\n    <span class=\"pl-c1\">self</span>.feature_bias <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feature_bias<span class=\"pl-pds\">\"</span></span>, [config.input_size],\n                                        <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.initer, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> weights and biases of output layer (follows hidden layer(s))</span>\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_layer<span class=\"pl-pds\">\"</span></span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this is where we define out linear + sigmoid sizes [hidden_size x 1 or several classes]</span>\n    <span class=\"pl-c1\">self</span>.output_w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_w<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">self</span>.last_layer_size, <span class=\"pl-c1\">self</span>.pay_type],\n                                    <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.initer, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    <span class=\"pl-c1\">self</span>.output_b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_b<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">self</span>.pay_type], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.initer,\n                                    <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>################### GRAPH ####################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create embedding lookup table with embedding variable and word inputs placeholder</span>\nword_embeddings <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(<span class=\"pl-c1\">self</span>.embedding, <span class=\"pl-c1\">self</span>.words_input)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [32 x 25 x 100]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create feature linear transform layer with feature inputs placeholder</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> first we need to swap tensor dims from [0, 1, 2] --&gt; [1, 0, 2] to make seq_length first</span>\n_feats_trans <span class=\"pl-k\">=</span> tf.transpose(<span class=\"pl-c1\">self</span>.feats_input, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [25 x 32 x 4]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> apply linear transform without activation in order</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to expand feature vectors [batch_size x nfeats] -&gt; [batch_size x input_size]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> this is needed to sum them with word embeddings before recurrent layer</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>_feats_exp = tf.map_fn(self._linear, _feats_trans, dtype=tf.float32, back_prop=True)  # [25 x 32 x 100]</span>\n_feats_exp <span class=\"pl-k\">=</span> [tf.nn.xw_plus_b(s, <span class=\"pl-c1\">self</span>.feature_weight, <span class=\"pl-c1\">self</span>.feature_bias)\n              <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> tf.unstack(_feats_trans, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> now stack the list of tensors and transpose back to [batch_size x seq_length x input_size]</span>\nfeats_exp <span class=\"pl-k\">=</span> tf.transpose(tf.stack(_feats_exp, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>), [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [32 x 25 x 100]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> sum the outputs of the embedding and linear ops</span>\ninputs_sum <span class=\"pl-k\">=</span> tf.add(word_embeddings, feats_exp)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [32 x 25 x 100]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> apply dropout here if needed</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>if self.training and self.input_keep_prob &lt; 1:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>    inputs_sum = tf.nn.dropout(inputs_sum, self.input_keep_prob)</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> split the input matrices vertically into separate tensors</span>\n_inputs_spl <span class=\"pl-k\">=</span> tf.split(inputs_sum, config.seq_length, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> remove single tensor dimensions</span>\ninputs <span class=\"pl-k\">=</span> [tf.squeeze(split, [<span class=\"pl-c1\">1</span>]) <span class=\"pl-k\">for</span> split <span class=\"pl-k\">in</span> _inputs_spl]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [25 * [32 x 100]]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> build recurrent cells</span>\n<span class=\"pl-c1\">self</span>.cell <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._create_recurrent_cell(config)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> initialize the hidden (recurrent) state to zero</span>\n<span class=\"pl-c1\">self</span>.initial_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.cell.zero_state(config.batch_size, tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create recurrent network using rnn cell and return outputs and final state</span>\n_outputs, <span class=\"pl-c1\">self</span>.final_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._apply_rec_cell(inputs, <span class=\"pl-c1\">self</span>.initial_state, <span class=\"pl-c1\">self</span>.cell, config)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [800 x 200]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> apply dropout here if needed</span>\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.training <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.input_keep_prob <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1</span>:\n    cell_output <span class=\"pl-k\">=</span> tf.nn.dropout(_outputs, <span class=\"pl-c1\">self</span>.input_keep_prob)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the hidden layer output is fed into matmul(x, weights) + biases function</span>\nlogits <span class=\"pl-k\">=</span> tf.nn.xw_plus_b(cell_output, <span class=\"pl-c1\">self</span>.output_w, <span class=\"pl-c1\">self</span>.output_b)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [800 x 1]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> transform logits into [seq_length x batch_size x 1]</span>\n_logits <span class=\"pl-k\">=</span> tf.reshape(logits, [config.seq_length, config.batch_size, config.pay_type])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [25 x 32 x 1]</span>\n\n<span class=\"pl-c1\">self</span>.logits <span class=\"pl-k\">=</span> _logits\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> add single dim to target tensor [32 x 25] --&gt; [32 x 25 x 1]</span>\n_targets <span class=\"pl-k\">=</span> tf.expand_dims(<span class=\"pl-c1\">self</span>.targets_input, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> transform targets tensor [32 x 25 x 1] --&gt; [25 x 32 x 1] for loss computation</span>\n_targets <span class=\"pl-k\">=</span> tf.transpose(_targets, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>################### LOSS CALCULATION ####################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> unstack logits and targets along seq_length dimension</span>\n_logits_uns <span class=\"pl-k\">=</span> tf.unstack(_logits, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n_targets_uns <span class=\"pl-k\">=</span> tf.unstack(_targets, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> apply loss function to each sequence and compute individual losses</span>\nloss_fn <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>loss_fn = tf.nn.sigmoid_cross_entropy_with_logits</span>\nindividual_losses <span class=\"pl-k\">=</span> []\n\n<span class=\"pl-k\">for</span> t, o <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(_targets_uns, _logits_uns):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [32 x 1], [32 x 1]</span>\n    _loss <span class=\"pl-k\">=</span> loss_fn(t, o, <span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">label_smoothing</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sigmoid_cross_entropy<span class=\"pl-pds\">\"</span></span>,\n                    <span class=\"pl-v\">loss_collection</span><span class=\"pl-k\">=</span>tf.GraphKeys.<span class=\"pl-c1\">LOSSES</span>, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span>tf.losses.Reduction.<span class=\"pl-c1\">NONE</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>_loss = loss_fn(labels=t, logits=o, name=\"sigmoid_cross_entropy\")</span>\n    individual_losses.append(_loss)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate the loss sum of individual losses</span>\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>):\n    <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_sum(individual_losses)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>################### TRAINING ####################</span>\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.training:\n    <span class=\"pl-c1\">self</span>.lr <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    tvars <span class=\"pl-k\">=</span> tf.trainable_variables()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> print model shapes and total params</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MODEL params:<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> tvars:\n        <span class=\"pl-c1\">print</span>(t.shape)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TOTAL params:<span class=\"pl-pds\">\"</span></span>, np.sum([np.prod(t.shape) <span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> tvars]))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> clip the gradient by norm</span>\n    <span class=\"pl-k\">if</span> config.grad_clip <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n        grads, _ <span class=\"pl-k\">=</span> tf.clip_by_global_norm(tf.gradients(<span class=\"pl-c1\">self</span>.loss, tvars), config.grad_clip)\n    <span class=\"pl-k\">else</span>:\n        grads <span class=\"pl-k\">=</span> tf.gradients(<span class=\"pl-c1\">self</span>.loss, tvars)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> update variables (weights, biases, embeddings...)</span>\n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>optimizer<span class=\"pl-pds\">\"</span></span>):\n        optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">self</span>.lr)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>optimizer = tf.train.GradientDescentOptimizer(self.lr)</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute grads/vars for tensorboard</span>\n        <span class=\"pl-c1\">self</span>.grads_and_vars <span class=\"pl-k\">=</span> optimizer.compute_gradients(<span class=\"pl-c1\">self</span>.loss)\n\n        <span class=\"pl-c1\">self</span>.train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(grads, tvars),          <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_or_create_global_step())</pre></div>\n<h3>Recurrent cell creation</h3>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_create_recurrent_cell</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">config</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Define and return recurrent cell.</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n            cell <span class=\"pl-k\">=</span> rnn.GRUCell(<span class=\"pl-c1\">self</span>.hidden_size,\n                                           <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.initer,\n                                           <span class=\"pl-v\">bias_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.initer,\n                                           <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.tanh)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> apply dropout if required</span>\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.training <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.output_keep_prob <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1.0</span>:\n                cell <span class=\"pl-k\">=</span> rnn.DropoutWrapper(cell, <span class=\"pl-v\">output_keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.output_keep_prob)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> we might use several rnn cells in future</span>\n        <span class=\"pl-k\">return</span> rnn.MultiRNNCell(cell, <span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<h3>Recurrent cell application</h3>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_apply_rec_cell</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">initial_state</span>, <span class=\"pl-smi\">cell</span>, <span class=\"pl-smi\">config</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Apply recurrence cell to each input sequence.</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n\n        <span class=\"pl-k\">with</span> variable_scope.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>recurrent_cell<span class=\"pl-pds\">\"</span></span>):\n            state <span class=\"pl-k\">=</span> initial_state\n            _outputs <span class=\"pl-k\">=</span> []\n            <span class=\"pl-k\">for</span> i, inp <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(inputs):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 25 * [32 x 100]</span>\n                <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n                    variable_scope.get_variable_scope().reuse_variables()\n                output, state <span class=\"pl-k\">=</span> cell(inp, state)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [32 x 200]</span>\n                _outputs.append(output)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> concat the outputs and reshape them into 2D tensor (for xw_plus_b)</span>\n        outputs <span class=\"pl-k\">=</span> tf.reshape(tf.concat(_outputs, <span class=\"pl-c1\">1</span>), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.recurrent_state_size]) \n        <span class=\"pl-k\">return</span> outputs, state</pre></div>\n<h3>Torch POC training analysis</h3>\n<p>We first take a look at our Torch model and see how it trains during first 10 epochs. We use batch_size=32, seq_length=25, word embedding and feature vectors of size 100 and GRU cell size = 200.</p>\n<pre><code>nn.Sequential {\n  [input -&gt; (1) -&gt; (2) -&gt; (3) -&gt; (4) -&gt; output]\n  (1): nn.ParallelTable {\n    input\n      |`-&gt; (1): nn.Sequential {\n      |      [input -&gt; (1) -&gt; (2) -&gt; output]\n      |      (1): nn.LookupTable\n      |      (2): nn.SplitTable\n      |    }\n       `-&gt; (2): nn.Sequential {\n             [input -&gt; (1) -&gt; (2) -&gt; output]\n             (1): nn.SplitTable\n             (2): nn.MapTable {\n               nn.Linear(4 -&gt; 100)\n             }\n           }\n       ... -&gt; output\n  }\n  (2): nn.ZipTable\n  (3): nn.MapTable {\n    nn.CAddTable\n  }\n  (4): nn.Sequencer @ nn.Recursor @ nn.Sequential {\n    [input -&gt; (1) -&gt; (2) -&gt; (3) -&gt; (4) -&gt; output]\n    (1): nn.RecGRU(100 -&gt; 200)\n    (2): nn.Dropout(0.5, busy)\n    (3): nn.Linear(200 -&gt; 1)\n    (4): nn.Sigmoid\n  }\n}\n\nEpoch #1\t\ntraining...\t\n8.5397211080362e-08\tembeddings grads\t\n0.020771607756615\tfeature weights grads\t\n0.00044380914187059\tfeature bias grads\t\n8.6396757978946e-06\tgru grads\t\n5.7720841141418e-05\tgru bias grads\t\n0.022520124912262\toutput_w grads\t\n0.32349386811256\toutput_b grads\t\nlearning rate:\t0.0497500005\t\nElapsed time: 3.190759\t\nSpeed: 0.000019 sec/batch\t\nTraining ERR: 959.54436812177\t\nvalidating...\t\nValidation ERR: 121.4889880009\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 1, ERR: 121.488988\t\n\t\nEpoch #2\t\ntraining...\t\n5.9534809082606e-08\tembeddings grads\t\n0.010755381546915\tfeature weights grads\t\n0.00030940235592425\tfeature bias grads\t\n2.632729774632e-05\tgru grads\t\n8.1771839177236e-05\tgru bias grads\t\n0.044476393610239\toutput_w grads\t\n0.37297031283379\toutput_b grads\t\nlearning rate:\t0.049500001\t\nElapsed time: 2.988541\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 862.21877676807\t\nvalidating...\t\nValidation ERR: 115.8837774843\t\ncheck early-stopping...\t\nFound new minima. Saving\nLast best epoch: 2, ERR: 115.883777\t\n\t\nEpoch #3\t\ntraining...\t\n1.9829073494293e-08\tembeddings grads\t\n0.0085931243374944\tfeature weights grads\t\n0.00010305171599612\tfeature bias grads\t\n-2.4194558136514e-05\tgru grads\t\n-4.8788820095069e-06\tgru bias grads\t\n0.0073388875462115\toutput_w grads\t\n0.40312686562538\toutput_b grads\t\nlearning rate:\t0.0492500015\t\nElapsed time: 2.969960\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 820.93807517737\t\nvalidating...\t\nValidation ERR: 110.50921051018\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 3, ERR: 110.509211\t\n\t\nEpoch #4\t\ntraining...\t\n-7.4974275676709e-09\tembeddings grads\t\n0.0028696460649371\tfeature weights grads\t\n-3.8964077248238e-05\tfeature bias grads\t\n-3.4580276405904e-05\tgru grads\t\n-3.2265303161694e-05\tgru bias grads\t\n0.049573861062527\toutput_w grads\t\n0.4097863137722\toutput_b grads\t\nlearning rate:\t0.049000002\t\nElapsed time: 3.005452\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 783.27910768799\t\nvalidating...\t\nValidation ERR: 107.62939401716\t\ncheck early-stopping...\t\nFound new minima. Saving\nLast best epoch: 4, ERR: 107.629394\t\n\t\nEpoch #5\t\ntraining...\t\n-3.623286559673e-08\tembeddings grads\t\n-0.01051034592092\tfeature weights grads\t\n-0.00018830213230103\tfeature bias grads\t\n4.4970302042202e-06\tgru grads\t\n3.8041966035962e-05\tgru bias grads\t\n0.0042808093130589\toutput_w grads\t\n0.13134820759296\toutput_b grads\t\nlearning rate:\t0.0487500025\t\nElapsed time: 2.966464\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 753.93339692801\t\nvalidating...\t\nValidation ERR: 108.99283232633\t\ncheck early-stopping...\t\nLast best epoch: 4, ERR: 107.629394\t\n\t\nEpoch #6\t\ntraining...\t\n-1.7328080303969e-08\tembeddings grads\t\n-0.0070463065057993\tfeature weights grads\t\n-9.0054127213079e-05\tfeature bias grads\t\n2.1397584077931e-06\tgru grads\t\n6.5339445427526e-05\tgru bias grads\t\n0.0060789352282882\toutput_w grads\t\n0.13912197947502\toutput_b grads\t\nlearning rate:\t0.048500003\t\nElapsed time: 2.984332\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 727.82390461024\t\nvalidating...\t\nValidation ERR: 101.84884516429\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 6, ERR: 101.848845\t\n\t\nEpoch #7\t\ntraining...\t\n-2.1446611597753e-08\tembeddings grads\t\n-0.0099087124690413\tfeature weights grads\t\n-0.000111457935418\tfeature bias grads\t\n4.289226126275e-06\tgru grads\t\n9.2848667918588e-06\tgru bias grads\t\n0.033868614584208\toutput_w grads\t\n0.36384600400925\toutput_b grads\t\nlearning rate:\t0.0482500035\t\nElapsed time: 2.971069\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 693.15547975153\t\nvalidating...\t\nValidation ERR: 89.342911911197\t\ncheck early-stopping...\t\nFound new minima. Saving\nLast best epoch: 7, ERR: 89.342912\t\n\t\nEpoch #8\t\ntraining...\t\n-2.4536879195125e-08\tembeddings grads\t\n-0.019674839451909\tfeature weights grads\t\n-0.00012751790927723\tfeature bias grads\t\n2.5825884222286e-06\tgru grads\t\n2.7131845854456e-06\tgru bias grads\t\n0.021293396130204\toutput_w grads\t\n0.52193140983582\toutput_b grads\t\nlearning rate:\t0.048000004\t\nElapsed time: 2.977743\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 662.76471467828\t\nvalidating...\t\nValidation ERR: 85.326015817933\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 8, ERR: 85.326016\t\n\t\nEpoch #9\t\ntraining...\t\n-4.5795339076449e-08\tembeddings grads\t\n-0.019793825224042\tfeature weights grads\t\n-0.00023799829068594\tfeature bias grads\t\n-6.2910985434428e-06\tgru grads\t\n-2.7645726731862e-05\tgru bias grads\t\n-0.0087647764012218\toutput_w grads\t\n0.11675848066807\toutput_b grads\t\nlearning rate:\t0.0477500045\t\nElapsed time: 3.058577\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 630.08797416603\t\nvalidating...\t\nValidation ERR: 81.433456174098\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 9, ERR: 81.433456\t\n\t\nEpoch #10\t\ntraining...\t\n-1.2227498302764e-07\tembeddings grads\t\n-0.079100400209427\tfeature weights grads\t\n-0.00063546327874064\tfeature bias grads\t\n2.7701785256795e-06\tgru grads\t\n-3.344654396642e-05\tgru bias grads\t\n-0.0030312589369714\toutput_w grads\t\n0.41179794073105\toutput_b grads\t\nlearning rate:\t0.047500005\t\nElapsed time: 3.000976\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 603.50259356992\t\nvalidating...\t\nValidation ERR: 79.982634914108\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 10, ERR: 79.982635\t\n</code></pre>\n<p>You can well see that during first 10 epochs Torch POC loss dropped significantly. Now let's take a look at idential Tensorflow implementation that uses the same dataset, same hyperparameters and has the same number of parameters as well as optimizer.</p>\n<h3>Tensorflow version training analysis</h3>\n<pre><code>MODEL params:\n(5197, 100)\n(4, 100)\n(100,)\n(200, 1)\n(1,)\n(300, 400)\n(400,)\n(300, 200)\n(200,)\nTOTAL params: 701001\n2018-04-06 11:26:30.534418: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-04-06 11:26:30.592130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-04-06 11:26:30.592438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.61GiB\n2018-04-06 11:26:30.592470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\n\nERROR: unable to remove saved_model dir!\nsaved_model does not exist or locked, remove manually\n\n&gt;&gt;&gt; Start test loss: 143462.57397460938\n\nEpoch: 1\n&gt; lr update: 0.0497500005\n#################### DEBUGGING ####################\n-8.42829e-09 \t Model/input_layer/embedding:0_grads\n-1.0331846e-05 \t Model/input_layer/feature_weigths:0_grads\n-6.7426217e-06 \t Model/input_layer/feature_bias:0_grads\n0.32880843 \t Model/output_layer/output_w:0_grads\n-18.802212 \t Model/output_layer/output_b:0_grads\n-2.583741e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n2.0594268e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n1.7847007e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n-1.2758308e-05 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 41251.31823730469\n&gt; Valid loss: 4779.414421081543\n&gt; Best valid loss so far: 143462.57397460938\nStopping in (35) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 2\n&gt; lr update: 0.049500001\n#################### DEBUGGING ####################\n4.380058e-12 \t Model/input_layer/embedding:0_grads\n-1.5687052e-09 \t Model/input_layer/feature_weigths:0_grads\n3.5040453e-09 \t Model/input_layer/feature_bias:0_grads\n1.0858363 \t Model/output_layer/output_w:0_grads\n-24.059809 \t Model/output_layer/output_b:0_grads\n9.927889e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n-1.6885447e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n7.744753e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n-1.343922e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 36458.438621520996\n&gt; Valid loss: 4771.496253967285\n&gt; Best valid loss so far: 4779.414421081543\nStopping in (35) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 3\n&gt; lr update: 0.0492500015\n#################### DEBUGGING ####################\n-9.34557e-13 \t Model/input_layer/embedding:0_grads\n-2.8814911e-18 \t Model/input_layer/feature_weigths:0_grads\n-7.4764217e-10 \t Model/input_layer/feature_bias:0_grads\n1.0264161 \t Model/output_layer/output_w:0_grads\n-28.068089 \t Model/output_layer/output_b:0_grads\n-3.470961e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n3.2109366e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-3.8721065e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n3.5263255e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 36571.039489746094\n&gt; Valid loss: 5612.627830505371\n&gt; Best valid loss so far: 4771.496253967285\nStopping in (35) epochs if no new minima!\n\nEpoch: 4\n&gt; lr update: 0.049000002\n#################### DEBUGGING ####################\n-1.5510707e-08 \t Model/input_layer/embedding:0_grads\n-1.9394596e-05 \t Model/input_layer/feature_weigths:0_grads\n-1.2408569e-05 \t Model/input_layer/feature_bias:0_grads\n0.4255897 \t Model/output_layer/output_w:0_grads\n-15.242744 \t Model/output_layer/output_b:0_grads\n-2.0200261e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n1.8209105e-08 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n2.867294e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n-2.1903145e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 37006.26306152344\n&gt; Valid loss: 4739.623916625977\n&gt; Best valid loss so far: 4771.496253967285\nStopping in (34) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 5\n&gt; lr update: 0.0487500025\n#################### DEBUGGING ####################\n1.2948664e-11 \t Model/input_layer/embedding:0_grads\n4.020792e-08 \t Model/input_layer/feature_weigths:0_grads\n1.035893e-08 \t Model/input_layer/feature_bias:0_grads\n-0.23937465 \t Model/output_layer/output_w:0_grads\n-4.5712795 \t Model/output_layer/output_b:0_grads\n-1.2125412e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n1.2779661e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-5.819682e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n3.9421697e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 35687.37390899658\n&gt; Valid loss: 4758.33650970459\n&gt; Best valid loss so far: 4739.623916625977\nStopping in (35) epochs if no new minima!\n\nEpoch: 6\n&gt; lr update: 0.048500003\n#################### DEBUGGING ####################\n8.107671e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n6.486137e-11 \t Model/input_layer/feature_bias:0_grads\n0.86348265 \t Model/output_layer/output_w:0_grads\n-19.398884 \t Model/output_layer/output_b:0_grads\n2.4077628e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n-2.4667464e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-1.07752e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n9.429133e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 34959.51641845703\n&gt; Valid loss: 4720.412452697754\n&gt; Best valid loss so far: 4739.623916625977\nStopping in (34) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 7\n&gt; lr update: 0.0482500035\n#################### DEBUGGING ####################\n1.1791363e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n9.433085e-12 \t Model/input_layer/feature_bias:0_grads\n0.46038043 \t Model/output_layer/output_w:0_grads\n-18.42015 \t Model/output_layer/output_b:0_grads\n-5.90758e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n6.253203e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-1.6275301e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n1.5087512e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 35122.075828552246\n&gt; Valid loss: 4872.291694641113\n&gt; Best valid loss so far: 4720.412452697754\nStopping in (35) epochs if no new minima!\n\nEpoch: 8\n&gt; lr update: 0.048000004\n#################### DEBUGGING ####################\n1.5793947e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n1.26351585e-11 \t Model/input_layer/feature_bias:0_grads\n-0.23440647 \t Model/output_layer/output_w:0_grads\n-17.745054 \t Model/output_layer/output_b:0_grads\n6.400091e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n-6.770721e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-7.499852e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n6.953364e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 35376.37155151367\n&gt; Valid loss: 4934.555885314941\n&gt; Best valid loss so far: 4720.412452697754\nStopping in (34) epochs if no new minima!\n\nEpoch: 9\n&gt; lr update: 0.0477500045\n#################### DEBUGGING ####################\n-1.2424676e-13 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n-9.939738e-11 \t Model/input_layer/feature_bias:0_grads\n0.5383458 \t Model/output_layer/output_w:0_grads\n-21.300901 \t Model/output_layer/output_b:0_grads\n-7.6299974e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n8.020704e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-7.250712e-16 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n6.68652e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 35422.02657318115\n&gt; Valid loss: 4915.4898681640625\n&gt; Best valid loss so far: 4720.412452697754\nStopping in (33) epochs if no new minima!\n\nEpoch: 10\n&gt; lr update: 0.047500005\n#################### DEBUGGING ####################\n4.3053272e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n3.4442636e-11 \t Model/input_layer/feature_bias:0_grads\n-0.4148861 \t Model/output_layer/output_w:0_grads\n-16.681393 \t Model/output_layer/output_b:0_grads\n-3.3981418e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n3.5677305e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-1.0744528e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n9.898426e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n&gt; Train loss: 35475.495765686035\n&gt; Valid loss: 4763.377861022949\n&gt; Best valid loss so far: 4720.412452697754\nStopping in (32) epochs if no new minima!\n</code></pre>\n<p>You can well see the gradients are very low in comparison to Torch POC up to the point that  some of them become zero or very close. Additionally, binary cross entropy loss is much higher and basically does not drop. Using standard <code>tf.train.GradientDescentOptimizer(self.lr)</code> does not make the gradients that small and is more stable in general but produces the same garbage predictions after training. We have been trying to figure out the reason for several weeks now and are simply out of clue what could cause such a drastic difference.  We checked our data preprocessing up to the point where we printed out the input matrix values and compared them side by side with Torch. They are identical so it is highly unlikely to be the data feeding mechanism. There is something not right with backpropagation it seems but our lack of experience with Tensorflow does not allow us to proceed further. Maybe somebody here could give us an advice on what could be the reason?</p>\n<p>Addtionally attaching Tensorboard output as well.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7676160/38414743-a9d43972-398f-11e8-87d1-3a934fc9af3e.png\"><img src=\"https://user-images.githubusercontent.com/7676160/38414743-a9d43972-398f-11e8-87d1-3a934fc9af3e.png\" alt=\"2018-04-06-114156_793x370_scrot\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7676160/38414748-acc7cd60-398f-11e8-8947-645e4bc63b43.png\"><img src=\"https://user-images.githubusercontent.com/7676160/38414748-acc7cd60-398f-11e8-8947-645e4bc63b43.png\" alt=\"2018-04-06-114228_1534x946_scrot\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7676160/38414750-af4aab34-398f-11e8-8d9b-d20c5bdc1f7c.png\"><img src=\"https://user-images.githubusercontent.com/7676160/38414750-af4aab34-398f-11e8-8d9b-d20c5bdc1f7c.png\" alt=\"2018-04-06-114236_1544x719_scrot\" style=\"max-width:100%;\"></a></p>", "body_text": "Intro\nI have asked this question several times on stackoverflow using different wording and level of details yet didn't get a single answer. Given the nature of the problem it is understandable, you cannot debug the network without training data and code. Still, maybe there is something obvious that I am missing here. We are trying to rewrite successful Torch POC binary classifier using Tensorflow to put it into production.\nTorch POC\nTorch model is a sequential binary classifier that works on word sequences and attempts to predict if the current word belongs to class 1 or 0. The model is quite simple, we have embedding layer and a special feature layer which we sum together before feeding the result vector into LSTM / GRU cell. At the output we do linear transform with sigmoid, compute binary cross entropy loss and update our parameters. Depending on the vocabulary size the model consists of 700k - 1000k params.\nTensorflow reimplementation\nWe have been using standard Tensorflow language model trainable on Penn Treebank dataset as our code base. We have adapted it to the point where it looks identical to our Torch POC (same hyperparams and equal number of parameters) and started training.\nProblem\nIt quickly became clear that Tensorflow reimplemetation does not learn anything even though the loss drops and test dataset shows error decrease: Test loss reduced: 97690.06433105469 --> 9929.968887329102. Loading the trained model and quering it with words showed that the model predictions are garbage. Besides the loss values are different for Torch and Tensorflow.\nTensorflow implementation (main part):\n#################### PLACEHOLDERS ####################\n# We use placeholders for the word, feature inputs and corresponding targets\nself.words_input = tf.placeholder(tf.int32, [config.batch_size, config.seq_length])\nself.feats_input = tf.placeholder(tf.float32, [config.batch_size, config.seq_length, config.nfeats])\nself.targets_input = tf.placeholder(tf.float32, [config.batch_size, config.seq_length])\n\n#################### VARIABLES ####################\n# We use variables for trainable network params like word embeddings, weights and biases\n# select params initialization\nif self.init_method == \"xavier\":\n    self.initer = tf.contrib.layers.xavier_initializer(uniform=False)\n\nelif self.init_method == \"uniform\":\n    self.initer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n\nelif self.init_method == \"normal\":\n    self.initer = tf.random_normal_initializer()\n\nelse:\n    self.initer = tf.zeros_initializer()\n\n# word embeddings\nwith tf.variable_scope(\"input_layer\"):\n    self.embedding = tf.get_variable(\"embedding\", [self.vocab_size, self.input_size],\n                                     initializer=self.initer, trainable=True)\n    # feature weights for linear transform\n    self.feature_weight = tf.get_variable(\"feature_weigths\", [config.nfeats, config.input_size],\n                                          initializer=self.initer, trainable=True)\n    # feature biases for linear transform\n    self.feature_bias = tf.get_variable(\"feature_bias\", [config.input_size],\n                                        initializer=self.initer, trainable=True)\n\n# weights and biases of output layer (follows hidden layer(s))\nwith tf.variable_scope(\"output_layer\"):\n    # this is where we define out linear + sigmoid sizes [hidden_size x 1 or several classes]\n    self.output_w = tf.get_variable(\"output_w\", [self.last_layer_size, self.pay_type],\n                                    initializer=self.initer, trainable=True)\n\n    self.output_b = tf.get_variable(\"output_b\", [self.pay_type], initializer=self.initer,\n                                    trainable=True)\n\n\n#################### GRAPH ####################\n# create embedding lookup table with embedding variable and word inputs placeholder\nword_embeddings = tf.nn.embedding_lookup(self.embedding, self.words_input)  # [32 x 25 x 100]\n\n# create feature linear transform layer with feature inputs placeholder\n# first we need to swap tensor dims from [0, 1, 2] --> [1, 0, 2] to make seq_length first\n_feats_trans = tf.transpose(self.feats_input, [1, 0, 2])  # [25 x 32 x 4]\n\n# apply linear transform without activation in order\n# to expand feature vectors [batch_size x nfeats] -> [batch_size x input_size]\n# this is needed to sum them with word embeddings before recurrent layer\n#_feats_exp = tf.map_fn(self._linear, _feats_trans, dtype=tf.float32, back_prop=True)  # [25 x 32 x 100]\n_feats_exp = [tf.nn.xw_plus_b(s, self.feature_weight, self.feature_bias)\n              for s in tf.unstack(_feats_trans, axis=0)]\n\n# now stack the list of tensors and transpose back to [batch_size x seq_length x input_size]\nfeats_exp = tf.transpose(tf.stack(_feats_exp, axis=0), [1, 0, 2])  # [32 x 25 x 100]\n\n# sum the outputs of the embedding and linear ops\ninputs_sum = tf.add(word_embeddings, feats_exp)  # [32 x 25 x 100]\n\n# apply dropout here if needed\n#if self.training and self.input_keep_prob < 1:\n#    inputs_sum = tf.nn.dropout(inputs_sum, self.input_keep_prob)\n\n# split the input matrices vertically into separate tensors\n_inputs_spl = tf.split(inputs_sum, config.seq_length, axis=1)\n# remove single tensor dimensions\ninputs = [tf.squeeze(split, [1]) for split in _inputs_spl]  # [25 * [32 x 100]]\n\n# build recurrent cells\nself.cell = self._create_recurrent_cell(config)\n\n# initialize the hidden (recurrent) state to zero\nself.initial_state = self.cell.zero_state(config.batch_size, tf.float32)\n\n# create recurrent network using rnn cell and return outputs and final state\n_outputs, self.final_state = self._apply_rec_cell(inputs, self.initial_state, self.cell, config)  # [800 x 200]\n\n# apply dropout here if needed\nif self.training and self.input_keep_prob < 1:\n    cell_output = tf.nn.dropout(_outputs, self.input_keep_prob)\n\n# the hidden layer output is fed into matmul(x, weights) + biases function\nlogits = tf.nn.xw_plus_b(cell_output, self.output_w, self.output_b)  # [800 x 1]\n\n# transform logits into [seq_length x batch_size x 1]\n_logits = tf.reshape(logits, [config.seq_length, config.batch_size, config.pay_type])  # [25 x 32 x 1]\n\nself.logits = _logits\n\n# add single dim to target tensor [32 x 25] --> [32 x 25 x 1]\n_targets = tf.expand_dims(self.targets_input, axis=2)\n# transform targets tensor [32 x 25 x 1] --> [25 x 32 x 1] for loss computation\n_targets = tf.transpose(_targets, [1, 0, 2])\n\n#################### LOSS CALCULATION ####################\n# unstack logits and targets along seq_length dimension\n_logits_uns = tf.unstack(_logits, axis=0)\n_targets_uns = tf.unstack(_targets, axis=0)\n\n# apply loss function to each sequence and compute individual losses\nloss_fn = tf.losses.sigmoid_cross_entropy\n#loss_fn = tf.nn.sigmoid_cross_entropy_with_logits\nindividual_losses = []\n\nfor t, o in zip(_targets_uns, _logits_uns):  # [32 x 1], [32 x 1]\n    _loss = loss_fn(t, o, weights=1.0, label_smoothing=0, scope=\"sigmoid_cross_entropy\",\n                    loss_collection=tf.GraphKeys.LOSSES, reduction=tf.losses.Reduction.NONE)\n    #_loss = loss_fn(labels=t, logits=o, name=\"sigmoid_cross_entropy\")\n    individual_losses.append(_loss)\n\n# calculate the loss sum of individual losses\nwith tf.name_scope('loss'):\n    self.loss = tf.reduce_sum(individual_losses)\n\n#################### TRAINING ####################\nif self.training:\n    self.lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n\n    # print model shapes and total params\n    print(\"MODEL params:\")\n    for t in tvars:\n        print(t.shape)\n    print(\"TOTAL params:\", np.sum([np.prod(t.shape) for t in tvars]))\n\n    # clip the gradient by norm\n    if config.grad_clip > 0:\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), config.grad_clip)\n    else:\n        grads = tf.gradients(self.loss, tvars)\n\n    # update variables (weights, biases, embeddings...)\n    with tf.name_scope(\"optimizer\"):\n        optimizer = tf.train.AdamOptimizer(self.lr)\n        #optimizer = tf.train.GradientDescentOptimizer(self.lr)\n        # compute grads/vars for tensorboard\n        self.grads_and_vars = optimizer.compute_gradients(self.loss)\n\n        self.train_op = optimizer.apply_gradients(zip(grads, tvars),          global_step=tf.train.get_or_create_global_step())\nRecurrent cell creation\n    def _create_recurrent_cell(self, config):\n        \"\"\"\n        Define and return recurrent cell.\n        \"\"\"\n            cell = rnn.GRUCell(self.hidden_size,\n                                           kernel_initializer=self.initer,\n                                           bias_initializer=self.initer,\n                                           activation=tf.nn.tanh)\n\n            # apply dropout if required\n            if self.training and self.output_keep_prob < 1.0:\n                cell = rnn.DropoutWrapper(cell, output_keep_prob=self.output_keep_prob)\n\n        # we might use several rnn cells in future\n        return rnn.MultiRNNCell(cell, state_is_tuple=True)\nRecurrent cell application\n    def _apply_rec_cell(self, inputs, initial_state, cell, config):\n        \"\"\"\n        Apply recurrence cell to each input sequence.\n        \"\"\"\n\n        with variable_scope.variable_scope(\"recurrent_cell\"):\n            state = initial_state\n            _outputs = []\n            for i, inp in enumerate(inputs):  # 25 * [32 x 100]\n                if i > 0:\n                    variable_scope.get_variable_scope().reuse_variables()\n                output, state = cell(inp, state)  # [32 x 200]\n                _outputs.append(output)\n\n        # concat the outputs and reshape them into 2D tensor (for xw_plus_b)\n        outputs = tf.reshape(tf.concat(_outputs, 1), [-1, self.recurrent_state_size]) \n        return outputs, state\nTorch POC training analysis\nWe first take a look at our Torch model and see how it trains during first 10 epochs. We use batch_size=32, seq_length=25, word embedding and feature vectors of size 100 and GRU cell size = 200.\nnn.Sequential {\n  [input -> (1) -> (2) -> (3) -> (4) -> output]\n  (1): nn.ParallelTable {\n    input\n      |`-> (1): nn.Sequential {\n      |      [input -> (1) -> (2) -> output]\n      |      (1): nn.LookupTable\n      |      (2): nn.SplitTable\n      |    }\n       `-> (2): nn.Sequential {\n             [input -> (1) -> (2) -> output]\n             (1): nn.SplitTable\n             (2): nn.MapTable {\n               nn.Linear(4 -> 100)\n             }\n           }\n       ... -> output\n  }\n  (2): nn.ZipTable\n  (3): nn.MapTable {\n    nn.CAddTable\n  }\n  (4): nn.Sequencer @ nn.Recursor @ nn.Sequential {\n    [input -> (1) -> (2) -> (3) -> (4) -> output]\n    (1): nn.RecGRU(100 -> 200)\n    (2): nn.Dropout(0.5, busy)\n    (3): nn.Linear(200 -> 1)\n    (4): nn.Sigmoid\n  }\n}\n\nEpoch #1\t\ntraining...\t\n8.5397211080362e-08\tembeddings grads\t\n0.020771607756615\tfeature weights grads\t\n0.00044380914187059\tfeature bias grads\t\n8.6396757978946e-06\tgru grads\t\n5.7720841141418e-05\tgru bias grads\t\n0.022520124912262\toutput_w grads\t\n0.32349386811256\toutput_b grads\t\nlearning rate:\t0.0497500005\t\nElapsed time: 3.190759\t\nSpeed: 0.000019 sec/batch\t\nTraining ERR: 959.54436812177\t\nvalidating...\t\nValidation ERR: 121.4889880009\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 1, ERR: 121.488988\t\n\t\nEpoch #2\t\ntraining...\t\n5.9534809082606e-08\tembeddings grads\t\n0.010755381546915\tfeature weights grads\t\n0.00030940235592425\tfeature bias grads\t\n2.632729774632e-05\tgru grads\t\n8.1771839177236e-05\tgru bias grads\t\n0.044476393610239\toutput_w grads\t\n0.37297031283379\toutput_b grads\t\nlearning rate:\t0.049500001\t\nElapsed time: 2.988541\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 862.21877676807\t\nvalidating...\t\nValidation ERR: 115.8837774843\t\ncheck early-stopping...\t\nFound new minima. Saving\nLast best epoch: 2, ERR: 115.883777\t\n\t\nEpoch #3\t\ntraining...\t\n1.9829073494293e-08\tembeddings grads\t\n0.0085931243374944\tfeature weights grads\t\n0.00010305171599612\tfeature bias grads\t\n-2.4194558136514e-05\tgru grads\t\n-4.8788820095069e-06\tgru bias grads\t\n0.0073388875462115\toutput_w grads\t\n0.40312686562538\toutput_b grads\t\nlearning rate:\t0.0492500015\t\nElapsed time: 2.969960\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 820.93807517737\t\nvalidating...\t\nValidation ERR: 110.50921051018\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 3, ERR: 110.509211\t\n\t\nEpoch #4\t\ntraining...\t\n-7.4974275676709e-09\tembeddings grads\t\n0.0028696460649371\tfeature weights grads\t\n-3.8964077248238e-05\tfeature bias grads\t\n-3.4580276405904e-05\tgru grads\t\n-3.2265303161694e-05\tgru bias grads\t\n0.049573861062527\toutput_w grads\t\n0.4097863137722\toutput_b grads\t\nlearning rate:\t0.049000002\t\nElapsed time: 3.005452\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 783.27910768799\t\nvalidating...\t\nValidation ERR: 107.62939401716\t\ncheck early-stopping...\t\nFound new minima. Saving\nLast best epoch: 4, ERR: 107.629394\t\n\t\nEpoch #5\t\ntraining...\t\n-3.623286559673e-08\tembeddings grads\t\n-0.01051034592092\tfeature weights grads\t\n-0.00018830213230103\tfeature bias grads\t\n4.4970302042202e-06\tgru grads\t\n3.8041966035962e-05\tgru bias grads\t\n0.0042808093130589\toutput_w grads\t\n0.13134820759296\toutput_b grads\t\nlearning rate:\t0.0487500025\t\nElapsed time: 2.966464\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 753.93339692801\t\nvalidating...\t\nValidation ERR: 108.99283232633\t\ncheck early-stopping...\t\nLast best epoch: 4, ERR: 107.629394\t\n\t\nEpoch #6\t\ntraining...\t\n-1.7328080303969e-08\tembeddings grads\t\n-0.0070463065057993\tfeature weights grads\t\n-9.0054127213079e-05\tfeature bias grads\t\n2.1397584077931e-06\tgru grads\t\n6.5339445427526e-05\tgru bias grads\t\n0.0060789352282882\toutput_w grads\t\n0.13912197947502\toutput_b grads\t\nlearning rate:\t0.048500003\t\nElapsed time: 2.984332\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 727.82390461024\t\nvalidating...\t\nValidation ERR: 101.84884516429\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 6, ERR: 101.848845\t\n\t\nEpoch #7\t\ntraining...\t\n-2.1446611597753e-08\tembeddings grads\t\n-0.0099087124690413\tfeature weights grads\t\n-0.000111457935418\tfeature bias grads\t\n4.289226126275e-06\tgru grads\t\n9.2848667918588e-06\tgru bias grads\t\n0.033868614584208\toutput_w grads\t\n0.36384600400925\toutput_b grads\t\nlearning rate:\t0.0482500035\t\nElapsed time: 2.971069\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 693.15547975153\t\nvalidating...\t\nValidation ERR: 89.342911911197\t\ncheck early-stopping...\t\nFound new minima. Saving\nLast best epoch: 7, ERR: 89.342912\t\n\t\nEpoch #8\t\ntraining...\t\n-2.4536879195125e-08\tembeddings grads\t\n-0.019674839451909\tfeature weights grads\t\n-0.00012751790927723\tfeature bias grads\t\n2.5825884222286e-06\tgru grads\t\n2.7131845854456e-06\tgru bias grads\t\n0.021293396130204\toutput_w grads\t\n0.52193140983582\toutput_b grads\t\nlearning rate:\t0.048000004\t\nElapsed time: 2.977743\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 662.76471467828\t\nvalidating...\t\nValidation ERR: 85.326015817933\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 8, ERR: 85.326016\t\n\t\nEpoch #9\t\ntraining...\t\n-4.5795339076449e-08\tembeddings grads\t\n-0.019793825224042\tfeature weights grads\t\n-0.00023799829068594\tfeature bias grads\t\n-6.2910985434428e-06\tgru grads\t\n-2.7645726731862e-05\tgru bias grads\t\n-0.0087647764012218\toutput_w grads\t\n0.11675848066807\toutput_b grads\t\nlearning rate:\t0.0477500045\t\nElapsed time: 3.058577\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 630.08797416603\t\nvalidating...\t\nValidation ERR: 81.433456174098\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 9, ERR: 81.433456\t\n\t\nEpoch #10\t\ntraining...\t\n-1.2227498302764e-07\tembeddings grads\t\n-0.079100400209427\tfeature weights grads\t\n-0.00063546327874064\tfeature bias grads\t\n2.7701785256795e-06\tgru grads\t\n-3.344654396642e-05\tgru bias grads\t\n-0.0030312589369714\toutput_w grads\t\n0.41179794073105\toutput_b grads\t\nlearning rate:\t0.047500005\t\nElapsed time: 3.000976\t\nSpeed: 0.000018 sec/batch\t\nTraining ERR: 603.50259356992\t\nvalidating...\t\nValidation ERR: 79.982634914108\t\ncheck early-stopping...\t\nFound new minima. Saving \nLast best epoch: 10, ERR: 79.982635\t\n\nYou can well see that during first 10 epochs Torch POC loss dropped significantly. Now let's take a look at idential Tensorflow implementation that uses the same dataset, same hyperparameters and has the same number of parameters as well as optimizer.\nTensorflow version training analysis\nMODEL params:\n(5197, 100)\n(4, 100)\n(100,)\n(200, 1)\n(1,)\n(300, 400)\n(400,)\n(300, 200)\n(200,)\nTOTAL params: 701001\n2018-04-06 11:26:30.534418: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-04-06 11:26:30.592130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-04-06 11:26:30.592438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.61GiB\n2018-04-06 11:26:30.592470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\n\nERROR: unable to remove saved_model dir!\nsaved_model does not exist or locked, remove manually\n\n>>> Start test loss: 143462.57397460938\n\nEpoch: 1\n> lr update: 0.0497500005\n#################### DEBUGGING ####################\n-8.42829e-09 \t Model/input_layer/embedding:0_grads\n-1.0331846e-05 \t Model/input_layer/feature_weigths:0_grads\n-6.7426217e-06 \t Model/input_layer/feature_bias:0_grads\n0.32880843 \t Model/output_layer/output_w:0_grads\n-18.802212 \t Model/output_layer/output_b:0_grads\n-2.583741e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n2.0594268e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n1.7847007e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n-1.2758308e-05 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 41251.31823730469\n> Valid loss: 4779.414421081543\n> Best valid loss so far: 143462.57397460938\nStopping in (35) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 2\n> lr update: 0.049500001\n#################### DEBUGGING ####################\n4.380058e-12 \t Model/input_layer/embedding:0_grads\n-1.5687052e-09 \t Model/input_layer/feature_weigths:0_grads\n3.5040453e-09 \t Model/input_layer/feature_bias:0_grads\n1.0858363 \t Model/output_layer/output_w:0_grads\n-24.059809 \t Model/output_layer/output_b:0_grads\n9.927889e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n-1.6885447e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n7.744753e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n-1.343922e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 36458.438621520996\n> Valid loss: 4771.496253967285\n> Best valid loss so far: 4779.414421081543\nStopping in (35) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 3\n> lr update: 0.0492500015\n#################### DEBUGGING ####################\n-9.34557e-13 \t Model/input_layer/embedding:0_grads\n-2.8814911e-18 \t Model/input_layer/feature_weigths:0_grads\n-7.4764217e-10 \t Model/input_layer/feature_bias:0_grads\n1.0264161 \t Model/output_layer/output_w:0_grads\n-28.068089 \t Model/output_layer/output_b:0_grads\n-3.470961e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n3.2109366e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-3.8721065e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n3.5263255e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 36571.039489746094\n> Valid loss: 5612.627830505371\n> Best valid loss so far: 4771.496253967285\nStopping in (35) epochs if no new minima!\n\nEpoch: 4\n> lr update: 0.049000002\n#################### DEBUGGING ####################\n-1.5510707e-08 \t Model/input_layer/embedding:0_grads\n-1.9394596e-05 \t Model/input_layer/feature_weigths:0_grads\n-1.2408569e-05 \t Model/input_layer/feature_bias:0_grads\n0.4255897 \t Model/output_layer/output_w:0_grads\n-15.242744 \t Model/output_layer/output_b:0_grads\n-2.0200261e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n1.8209105e-08 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n2.867294e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n-2.1903145e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 37006.26306152344\n> Valid loss: 4739.623916625977\n> Best valid loss so far: 4771.496253967285\nStopping in (34) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 5\n> lr update: 0.0487500025\n#################### DEBUGGING ####################\n1.2948664e-11 \t Model/input_layer/embedding:0_grads\n4.020792e-08 \t Model/input_layer/feature_weigths:0_grads\n1.035893e-08 \t Model/input_layer/feature_bias:0_grads\n-0.23937465 \t Model/output_layer/output_w:0_grads\n-4.5712795 \t Model/output_layer/output_b:0_grads\n-1.2125412e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n1.2779661e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-5.819682e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n3.9421697e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 35687.37390899658\n> Valid loss: 4758.33650970459\n> Best valid loss so far: 4739.623916625977\nStopping in (35) epochs if no new minima!\n\nEpoch: 6\n> lr update: 0.048500003\n#################### DEBUGGING ####################\n8.107671e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n6.486137e-11 \t Model/input_layer/feature_bias:0_grads\n0.86348265 \t Model/output_layer/output_w:0_grads\n-19.398884 \t Model/output_layer/output_b:0_grads\n2.4077628e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n-2.4667464e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-1.07752e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n9.429133e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 34959.51641845703\n> Valid loss: 4720.412452697754\n> Best valid loss so far: 4739.623916625977\nStopping in (34) epochs if no new minima!\n!!! NEW local minima found, saving the model...\n\nEpoch: 7\n> lr update: 0.0482500035\n#################### DEBUGGING ####################\n1.1791363e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n9.433085e-12 \t Model/input_layer/feature_bias:0_grads\n0.46038043 \t Model/output_layer/output_w:0_grads\n-18.42015 \t Model/output_layer/output_b:0_grads\n-5.90758e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n6.253203e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-1.6275301e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n1.5087512e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 35122.075828552246\n> Valid loss: 4872.291694641113\n> Best valid loss so far: 4720.412452697754\nStopping in (35) epochs if no new minima!\n\nEpoch: 8\n> lr update: 0.048000004\n#################### DEBUGGING ####################\n1.5793947e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n1.26351585e-11 \t Model/input_layer/feature_bias:0_grads\n-0.23440647 \t Model/output_layer/output_w:0_grads\n-17.745054 \t Model/output_layer/output_b:0_grads\n6.400091e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n-6.770721e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-7.499852e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n6.953364e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 35376.37155151367\n> Valid loss: 4934.555885314941\n> Best valid loss so far: 4720.412452697754\nStopping in (34) epochs if no new minima!\n\nEpoch: 9\n> lr update: 0.0477500045\n#################### DEBUGGING ####################\n-1.2424676e-13 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n-9.939738e-11 \t Model/input_layer/feature_bias:0_grads\n0.5383458 \t Model/output_layer/output_w:0_grads\n-21.300901 \t Model/output_layer/output_b:0_grads\n-7.6299974e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n8.020704e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-7.250712e-16 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n6.68652e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 35422.02657318115\n> Valid loss: 4915.4898681640625\n> Best valid loss so far: 4720.412452697754\nStopping in (33) epochs if no new minima!\n\nEpoch: 10\n> lr update: 0.047500005\n#################### DEBUGGING ####################\n4.3053272e-14 \t Model/input_layer/embedding:0_grads\n0.0 \t Model/input_layer/feature_weigths:0_grads\n3.4442636e-11 \t Model/input_layer/feature_bias:0_grads\n-0.4148861 \t Model/output_layer/output_w:0_grads\n-16.681393 \t Model/output_layer/output_b:0_grads\n-3.3981418e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\n3.5677305e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\n-1.0744528e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\n9.898426e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\n\n==================== debug_var ====================\n> Train loss: 35475.495765686035\n> Valid loss: 4763.377861022949\n> Best valid loss so far: 4720.412452697754\nStopping in (32) epochs if no new minima!\n\nYou can well see the gradients are very low in comparison to Torch POC up to the point that  some of them become zero or very close. Additionally, binary cross entropy loss is much higher and basically does not drop. Using standard tf.train.GradientDescentOptimizer(self.lr) does not make the gradients that small and is more stable in general but produces the same garbage predictions after training. We have been trying to figure out the reason for several weeks now and are simply out of clue what could cause such a drastic difference.  We checked our data preprocessing up to the point where we printed out the input matrix values and compared them side by side with Torch. They are identical so it is highly unlikely to be the data feeding mechanism. There is something not right with backpropagation it seems but our lack of experience with Tensorflow does not allow us to proceed further. Maybe somebody here could give us an advice on what could be the reason?\nAddtionally attaching Tensorboard output as well.", "body": "### Intro\r\nI have asked this question several times on stackoverflow using different wording and level of details yet didn't get a single answer. Given the nature of the problem it is understandable, you cannot debug the network without training data and code. Still, maybe there is something obvious that I am missing here. We are trying to rewrite successful Torch POC binary classifier using Tensorflow to put it into production.\r\n\r\n### Torch POC\r\nTorch model is a sequential binary classifier that works on word sequences and attempts to predict if the current word belongs to class 1 or 0. The model is quite simple, we have embedding layer and a special feature layer which we sum together before feeding the result vector into LSTM / GRU cell. At the output we do linear transform with sigmoid, compute binary cross entropy loss and update our parameters. Depending on the vocabulary size the model consists of 700k - 1000k params. \r\n\r\n### Tensorflow reimplementation\r\nWe have been using standard Tensorflow language model trainable on Penn Treebank dataset as our code base. We have adapted it to the point where it looks identical to our Torch POC (same hyperparams and equal number of parameters) and started training. \r\n\r\n### Problem\r\nIt quickly became clear that Tensorflow reimplemetation does not learn anything even though the loss drops and test dataset shows error decrease: `Test loss reduced: 97690.06433105469 --> 9929.968887329102`. Loading the trained model and quering it with words showed that the model predictions are garbage. Besides the loss values are different for Torch and Tensorflow. \r\n\r\n### Tensorflow implementation (main part):\r\n```python\r\n#################### PLACEHOLDERS ####################\r\n# We use placeholders for the word, feature inputs and corresponding targets\r\nself.words_input = tf.placeholder(tf.int32, [config.batch_size, config.seq_length])\r\nself.feats_input = tf.placeholder(tf.float32, [config.batch_size, config.seq_length, config.nfeats])\r\nself.targets_input = tf.placeholder(tf.float32, [config.batch_size, config.seq_length])\r\n\r\n#################### VARIABLES ####################\r\n# We use variables for trainable network params like word embeddings, weights and biases\r\n# select params initialization\r\nif self.init_method == \"xavier\":\r\n    self.initer = tf.contrib.layers.xavier_initializer(uniform=False)\r\n\r\nelif self.init_method == \"uniform\":\r\n    self.initer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\r\n\r\nelif self.init_method == \"normal\":\r\n    self.initer = tf.random_normal_initializer()\r\n\r\nelse:\r\n    self.initer = tf.zeros_initializer()\r\n\r\n# word embeddings\r\nwith tf.variable_scope(\"input_layer\"):\r\n    self.embedding = tf.get_variable(\"embedding\", [self.vocab_size, self.input_size],\r\n                                     initializer=self.initer, trainable=True)\r\n    # feature weights for linear transform\r\n    self.feature_weight = tf.get_variable(\"feature_weigths\", [config.nfeats, config.input_size],\r\n                                          initializer=self.initer, trainable=True)\r\n    # feature biases for linear transform\r\n    self.feature_bias = tf.get_variable(\"feature_bias\", [config.input_size],\r\n                                        initializer=self.initer, trainable=True)\r\n\r\n# weights and biases of output layer (follows hidden layer(s))\r\nwith tf.variable_scope(\"output_layer\"):\r\n    # this is where we define out linear + sigmoid sizes [hidden_size x 1 or several classes]\r\n    self.output_w = tf.get_variable(\"output_w\", [self.last_layer_size, self.pay_type],\r\n                                    initializer=self.initer, trainable=True)\r\n\r\n    self.output_b = tf.get_variable(\"output_b\", [self.pay_type], initializer=self.initer,\r\n                                    trainable=True)\r\n\r\n\r\n#################### GRAPH ####################\r\n# create embedding lookup table with embedding variable and word inputs placeholder\r\nword_embeddings = tf.nn.embedding_lookup(self.embedding, self.words_input)  # [32 x 25 x 100]\r\n\r\n# create feature linear transform layer with feature inputs placeholder\r\n# first we need to swap tensor dims from [0, 1, 2] --> [1, 0, 2] to make seq_length first\r\n_feats_trans = tf.transpose(self.feats_input, [1, 0, 2])  # [25 x 32 x 4]\r\n\r\n# apply linear transform without activation in order\r\n# to expand feature vectors [batch_size x nfeats] -> [batch_size x input_size]\r\n# this is needed to sum them with word embeddings before recurrent layer\r\n#_feats_exp = tf.map_fn(self._linear, _feats_trans, dtype=tf.float32, back_prop=True)  # [25 x 32 x 100]\r\n_feats_exp = [tf.nn.xw_plus_b(s, self.feature_weight, self.feature_bias)\r\n              for s in tf.unstack(_feats_trans, axis=0)]\r\n\r\n# now stack the list of tensors and transpose back to [batch_size x seq_length x input_size]\r\nfeats_exp = tf.transpose(tf.stack(_feats_exp, axis=0), [1, 0, 2])  # [32 x 25 x 100]\r\n\r\n# sum the outputs of the embedding and linear ops\r\ninputs_sum = tf.add(word_embeddings, feats_exp)  # [32 x 25 x 100]\r\n\r\n# apply dropout here if needed\r\n#if self.training and self.input_keep_prob < 1:\r\n#    inputs_sum = tf.nn.dropout(inputs_sum, self.input_keep_prob)\r\n\r\n# split the input matrices vertically into separate tensors\r\n_inputs_spl = tf.split(inputs_sum, config.seq_length, axis=1)\r\n# remove single tensor dimensions\r\ninputs = [tf.squeeze(split, [1]) for split in _inputs_spl]  # [25 * [32 x 100]]\r\n\r\n# build recurrent cells\r\nself.cell = self._create_recurrent_cell(config)\r\n\r\n# initialize the hidden (recurrent) state to zero\r\nself.initial_state = self.cell.zero_state(config.batch_size, tf.float32)\r\n\r\n# create recurrent network using rnn cell and return outputs and final state\r\n_outputs, self.final_state = self._apply_rec_cell(inputs, self.initial_state, self.cell, config)  # [800 x 200]\r\n\r\n# apply dropout here if needed\r\nif self.training and self.input_keep_prob < 1:\r\n    cell_output = tf.nn.dropout(_outputs, self.input_keep_prob)\r\n\r\n# the hidden layer output is fed into matmul(x, weights) + biases function\r\nlogits = tf.nn.xw_plus_b(cell_output, self.output_w, self.output_b)  # [800 x 1]\r\n\r\n# transform logits into [seq_length x batch_size x 1]\r\n_logits = tf.reshape(logits, [config.seq_length, config.batch_size, config.pay_type])  # [25 x 32 x 1]\r\n\r\nself.logits = _logits\r\n\r\n# add single dim to target tensor [32 x 25] --> [32 x 25 x 1]\r\n_targets = tf.expand_dims(self.targets_input, axis=2)\r\n# transform targets tensor [32 x 25 x 1] --> [25 x 32 x 1] for loss computation\r\n_targets = tf.transpose(_targets, [1, 0, 2])\r\n\r\n#################### LOSS CALCULATION ####################\r\n# unstack logits and targets along seq_length dimension\r\n_logits_uns = tf.unstack(_logits, axis=0)\r\n_targets_uns = tf.unstack(_targets, axis=0)\r\n\r\n# apply loss function to each sequence and compute individual losses\r\nloss_fn = tf.losses.sigmoid_cross_entropy\r\n#loss_fn = tf.nn.sigmoid_cross_entropy_with_logits\r\nindividual_losses = []\r\n\r\nfor t, o in zip(_targets_uns, _logits_uns):  # [32 x 1], [32 x 1]\r\n    _loss = loss_fn(t, o, weights=1.0, label_smoothing=0, scope=\"sigmoid_cross_entropy\",\r\n                    loss_collection=tf.GraphKeys.LOSSES, reduction=tf.losses.Reduction.NONE)\r\n    #_loss = loss_fn(labels=t, logits=o, name=\"sigmoid_cross_entropy\")\r\n    individual_losses.append(_loss)\r\n\r\n# calculate the loss sum of individual losses\r\nwith tf.name_scope('loss'):\r\n    self.loss = tf.reduce_sum(individual_losses)\r\n\r\n#################### TRAINING ####################\r\nif self.training:\r\n    self.lr = tf.Variable(0.0, trainable=False)\r\n    tvars = tf.trainable_variables()\r\n\r\n    # print model shapes and total params\r\n    print(\"MODEL params:\")\r\n    for t in tvars:\r\n        print(t.shape)\r\n    print(\"TOTAL params:\", np.sum([np.prod(t.shape) for t in tvars]))\r\n\r\n    # clip the gradient by norm\r\n    if config.grad_clip > 0:\r\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), config.grad_clip)\r\n    else:\r\n        grads = tf.gradients(self.loss, tvars)\r\n\r\n    # update variables (weights, biases, embeddings...)\r\n    with tf.name_scope(\"optimizer\"):\r\n        optimizer = tf.train.AdamOptimizer(self.lr)\r\n        #optimizer = tf.train.GradientDescentOptimizer(self.lr)\r\n        # compute grads/vars for tensorboard\r\n        self.grads_and_vars = optimizer.compute_gradients(self.loss)\r\n\r\n        self.train_op = optimizer.apply_gradients(zip(grads, tvars),          global_step=tf.train.get_or_create_global_step())\r\n```\r\n### Recurrent cell creation\r\n```python\r\n    def _create_recurrent_cell(self, config):\r\n        \"\"\"\r\n        Define and return recurrent cell.\r\n        \"\"\"\r\n            cell = rnn.GRUCell(self.hidden_size,\r\n                                           kernel_initializer=self.initer,\r\n                                           bias_initializer=self.initer,\r\n                                           activation=tf.nn.tanh)\r\n\r\n            # apply dropout if required\r\n            if self.training and self.output_keep_prob < 1.0:\r\n                cell = rnn.DropoutWrapper(cell, output_keep_prob=self.output_keep_prob)\r\n\r\n        # we might use several rnn cells in future\r\n        return rnn.MultiRNNCell(cell, state_is_tuple=True)\r\n```\r\n\r\n### Recurrent cell application\r\n```python\r\n    def _apply_rec_cell(self, inputs, initial_state, cell, config):\r\n        \"\"\"\r\n        Apply recurrence cell to each input sequence.\r\n        \"\"\"\r\n\r\n        with variable_scope.variable_scope(\"recurrent_cell\"):\r\n            state = initial_state\r\n            _outputs = []\r\n            for i, inp in enumerate(inputs):  # 25 * [32 x 100]\r\n                if i > 0:\r\n                    variable_scope.get_variable_scope().reuse_variables()\r\n                output, state = cell(inp, state)  # [32 x 200]\r\n                _outputs.append(output)\r\n\r\n        # concat the outputs and reshape them into 2D tensor (for xw_plus_b)\r\n        outputs = tf.reshape(tf.concat(_outputs, 1), [-1, self.recurrent_state_size]) \r\n        return outputs, state\r\n```\r\n\r\n### Torch POC training analysis\r\nWe first take a look at our Torch model and see how it trains during first 10 epochs. We use batch_size=32, seq_length=25, word embedding and feature vectors of size 100 and GRU cell size = 200.\r\n\r\n```\r\nnn.Sequential {\r\n  [input -> (1) -> (2) -> (3) -> (4) -> output]\r\n  (1): nn.ParallelTable {\r\n    input\r\n      |`-> (1): nn.Sequential {\r\n      |      [input -> (1) -> (2) -> output]\r\n      |      (1): nn.LookupTable\r\n      |      (2): nn.SplitTable\r\n      |    }\r\n       `-> (2): nn.Sequential {\r\n             [input -> (1) -> (2) -> output]\r\n             (1): nn.SplitTable\r\n             (2): nn.MapTable {\r\n               nn.Linear(4 -> 100)\r\n             }\r\n           }\r\n       ... -> output\r\n  }\r\n  (2): nn.ZipTable\r\n  (3): nn.MapTable {\r\n    nn.CAddTable\r\n  }\r\n  (4): nn.Sequencer @ nn.Recursor @ nn.Sequential {\r\n    [input -> (1) -> (2) -> (3) -> (4) -> output]\r\n    (1): nn.RecGRU(100 -> 200)\r\n    (2): nn.Dropout(0.5, busy)\r\n    (3): nn.Linear(200 -> 1)\r\n    (4): nn.Sigmoid\r\n  }\r\n}\r\n\r\nEpoch #1\t\r\ntraining...\t\r\n8.5397211080362e-08\tembeddings grads\t\r\n0.020771607756615\tfeature weights grads\t\r\n0.00044380914187059\tfeature bias grads\t\r\n8.6396757978946e-06\tgru grads\t\r\n5.7720841141418e-05\tgru bias grads\t\r\n0.022520124912262\toutput_w grads\t\r\n0.32349386811256\toutput_b grads\t\r\nlearning rate:\t0.0497500005\t\r\nElapsed time: 3.190759\t\r\nSpeed: 0.000019 sec/batch\t\r\nTraining ERR: 959.54436812177\t\r\nvalidating...\t\r\nValidation ERR: 121.4889880009\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 1, ERR: 121.488988\t\r\n\t\r\nEpoch #2\t\r\ntraining...\t\r\n5.9534809082606e-08\tembeddings grads\t\r\n0.010755381546915\tfeature weights grads\t\r\n0.00030940235592425\tfeature bias grads\t\r\n2.632729774632e-05\tgru grads\t\r\n8.1771839177236e-05\tgru bias grads\t\r\n0.044476393610239\toutput_w grads\t\r\n0.37297031283379\toutput_b grads\t\r\nlearning rate:\t0.049500001\t\r\nElapsed time: 2.988541\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 862.21877676807\t\r\nvalidating...\t\r\nValidation ERR: 115.8837774843\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving\r\nLast best epoch: 2, ERR: 115.883777\t\r\n\t\r\nEpoch #3\t\r\ntraining...\t\r\n1.9829073494293e-08\tembeddings grads\t\r\n0.0085931243374944\tfeature weights grads\t\r\n0.00010305171599612\tfeature bias grads\t\r\n-2.4194558136514e-05\tgru grads\t\r\n-4.8788820095069e-06\tgru bias grads\t\r\n0.0073388875462115\toutput_w grads\t\r\n0.40312686562538\toutput_b grads\t\r\nlearning rate:\t0.0492500015\t\r\nElapsed time: 2.969960\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 820.93807517737\t\r\nvalidating...\t\r\nValidation ERR: 110.50921051018\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 3, ERR: 110.509211\t\r\n\t\r\nEpoch #4\t\r\ntraining...\t\r\n-7.4974275676709e-09\tembeddings grads\t\r\n0.0028696460649371\tfeature weights grads\t\r\n-3.8964077248238e-05\tfeature bias grads\t\r\n-3.4580276405904e-05\tgru grads\t\r\n-3.2265303161694e-05\tgru bias grads\t\r\n0.049573861062527\toutput_w grads\t\r\n0.4097863137722\toutput_b grads\t\r\nlearning rate:\t0.049000002\t\r\nElapsed time: 3.005452\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 783.27910768799\t\r\nvalidating...\t\r\nValidation ERR: 107.62939401716\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving\r\nLast best epoch: 4, ERR: 107.629394\t\r\n\t\r\nEpoch #5\t\r\ntraining...\t\r\n-3.623286559673e-08\tembeddings grads\t\r\n-0.01051034592092\tfeature weights grads\t\r\n-0.00018830213230103\tfeature bias grads\t\r\n4.4970302042202e-06\tgru grads\t\r\n3.8041966035962e-05\tgru bias grads\t\r\n0.0042808093130589\toutput_w grads\t\r\n0.13134820759296\toutput_b grads\t\r\nlearning rate:\t0.0487500025\t\r\nElapsed time: 2.966464\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 753.93339692801\t\r\nvalidating...\t\r\nValidation ERR: 108.99283232633\t\r\ncheck early-stopping...\t\r\nLast best epoch: 4, ERR: 107.629394\t\r\n\t\r\nEpoch #6\t\r\ntraining...\t\r\n-1.7328080303969e-08\tembeddings grads\t\r\n-0.0070463065057993\tfeature weights grads\t\r\n-9.0054127213079e-05\tfeature bias grads\t\r\n2.1397584077931e-06\tgru grads\t\r\n6.5339445427526e-05\tgru bias grads\t\r\n0.0060789352282882\toutput_w grads\t\r\n0.13912197947502\toutput_b grads\t\r\nlearning rate:\t0.048500003\t\r\nElapsed time: 2.984332\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 727.82390461024\t\r\nvalidating...\t\r\nValidation ERR: 101.84884516429\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 6, ERR: 101.848845\t\r\n\t\r\nEpoch #7\t\r\ntraining...\t\r\n-2.1446611597753e-08\tembeddings grads\t\r\n-0.0099087124690413\tfeature weights grads\t\r\n-0.000111457935418\tfeature bias grads\t\r\n4.289226126275e-06\tgru grads\t\r\n9.2848667918588e-06\tgru bias grads\t\r\n0.033868614584208\toutput_w grads\t\r\n0.36384600400925\toutput_b grads\t\r\nlearning rate:\t0.0482500035\t\r\nElapsed time: 2.971069\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 693.15547975153\t\r\nvalidating...\t\r\nValidation ERR: 89.342911911197\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving\r\nLast best epoch: 7, ERR: 89.342912\t\r\n\t\r\nEpoch #8\t\r\ntraining...\t\r\n-2.4536879195125e-08\tembeddings grads\t\r\n-0.019674839451909\tfeature weights grads\t\r\n-0.00012751790927723\tfeature bias grads\t\r\n2.5825884222286e-06\tgru grads\t\r\n2.7131845854456e-06\tgru bias grads\t\r\n0.021293396130204\toutput_w grads\t\r\n0.52193140983582\toutput_b grads\t\r\nlearning rate:\t0.048000004\t\r\nElapsed time: 2.977743\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 662.76471467828\t\r\nvalidating...\t\r\nValidation ERR: 85.326015817933\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 8, ERR: 85.326016\t\r\n\t\r\nEpoch #9\t\r\ntraining...\t\r\n-4.5795339076449e-08\tembeddings grads\t\r\n-0.019793825224042\tfeature weights grads\t\r\n-0.00023799829068594\tfeature bias grads\t\r\n-6.2910985434428e-06\tgru grads\t\r\n-2.7645726731862e-05\tgru bias grads\t\r\n-0.0087647764012218\toutput_w grads\t\r\n0.11675848066807\toutput_b grads\t\r\nlearning rate:\t0.0477500045\t\r\nElapsed time: 3.058577\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 630.08797416603\t\r\nvalidating...\t\r\nValidation ERR: 81.433456174098\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 9, ERR: 81.433456\t\r\n\t\r\nEpoch #10\t\r\ntraining...\t\r\n-1.2227498302764e-07\tembeddings grads\t\r\n-0.079100400209427\tfeature weights grads\t\r\n-0.00063546327874064\tfeature bias grads\t\r\n2.7701785256795e-06\tgru grads\t\r\n-3.344654396642e-05\tgru bias grads\t\r\n-0.0030312589369714\toutput_w grads\t\r\n0.41179794073105\toutput_b grads\t\r\nlearning rate:\t0.047500005\t\r\nElapsed time: 3.000976\t\r\nSpeed: 0.000018 sec/batch\t\r\nTraining ERR: 603.50259356992\t\r\nvalidating...\t\r\nValidation ERR: 79.982634914108\t\r\ncheck early-stopping...\t\r\nFound new minima. Saving \r\nLast best epoch: 10, ERR: 79.982635\t\r\n```\r\n\r\nYou can well see that during first 10 epochs Torch POC loss dropped significantly. Now let's take a look at idential Tensorflow implementation that uses the same dataset, same hyperparameters and has the same number of parameters as well as optimizer.\r\n\r\n### Tensorflow version training analysis\r\n```\r\nMODEL params:\r\n(5197, 100)\r\n(4, 100)\r\n(100,)\r\n(200, 1)\r\n(1,)\r\n(300, 400)\r\n(400,)\r\n(300, 200)\r\n(200,)\r\nTOTAL params: 701001\r\n2018-04-06 11:26:30.534418: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-04-06 11:26:30.592130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-04-06 11:26:30.592438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.61GiB\r\n2018-04-06 11:26:30.592470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\r\nERROR: unable to remove saved_model dir!\r\nsaved_model does not exist or locked, remove manually\r\n\r\n>>> Start test loss: 143462.57397460938\r\n\r\nEpoch: 1\r\n> lr update: 0.0497500005\r\n#################### DEBUGGING ####################\r\n-8.42829e-09 \t Model/input_layer/embedding:0_grads\r\n-1.0331846e-05 \t Model/input_layer/feature_weigths:0_grads\r\n-6.7426217e-06 \t Model/input_layer/feature_bias:0_grads\r\n0.32880843 \t Model/output_layer/output_w:0_grads\r\n-18.802212 \t Model/output_layer/output_b:0_grads\r\n-2.583741e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n2.0594268e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n1.7847007e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n-1.2758308e-05 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 41251.31823730469\r\n> Valid loss: 4779.414421081543\r\n> Best valid loss so far: 143462.57397460938\r\nStopping in (35) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 2\r\n> lr update: 0.049500001\r\n#################### DEBUGGING ####################\r\n4.380058e-12 \t Model/input_layer/embedding:0_grads\r\n-1.5687052e-09 \t Model/input_layer/feature_weigths:0_grads\r\n3.5040453e-09 \t Model/input_layer/feature_bias:0_grads\r\n1.0858363 \t Model/output_layer/output_w:0_grads\r\n-24.059809 \t Model/output_layer/output_b:0_grads\r\n9.927889e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n-1.6885447e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n7.744753e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n-1.343922e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 36458.438621520996\r\n> Valid loss: 4771.496253967285\r\n> Best valid loss so far: 4779.414421081543\r\nStopping in (35) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 3\r\n> lr update: 0.0492500015\r\n#################### DEBUGGING ####################\r\n-9.34557e-13 \t Model/input_layer/embedding:0_grads\r\n-2.8814911e-18 \t Model/input_layer/feature_weigths:0_grads\r\n-7.4764217e-10 \t Model/input_layer/feature_bias:0_grads\r\n1.0264161 \t Model/output_layer/output_w:0_grads\r\n-28.068089 \t Model/output_layer/output_b:0_grads\r\n-3.470961e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n3.2109366e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-3.8721065e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n3.5263255e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 36571.039489746094\r\n> Valid loss: 5612.627830505371\r\n> Best valid loss so far: 4771.496253967285\r\nStopping in (35) epochs if no new minima!\r\n\r\nEpoch: 4\r\n> lr update: 0.049000002\r\n#################### DEBUGGING ####################\r\n-1.5510707e-08 \t Model/input_layer/embedding:0_grads\r\n-1.9394596e-05 \t Model/input_layer/feature_weigths:0_grads\r\n-1.2408569e-05 \t Model/input_layer/feature_bias:0_grads\r\n0.4255897 \t Model/output_layer/output_w:0_grads\r\n-15.242744 \t Model/output_layer/output_b:0_grads\r\n-2.0200261e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n1.8209105e-08 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n2.867294e-07 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n-2.1903145e-06 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 37006.26306152344\r\n> Valid loss: 4739.623916625977\r\n> Best valid loss so far: 4771.496253967285\r\nStopping in (34) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 5\r\n> lr update: 0.0487500025\r\n#################### DEBUGGING ####################\r\n1.2948664e-11 \t Model/input_layer/embedding:0_grads\r\n4.020792e-08 \t Model/input_layer/feature_weigths:0_grads\r\n1.035893e-08 \t Model/input_layer/feature_bias:0_grads\r\n-0.23937465 \t Model/output_layer/output_w:0_grads\r\n-4.5712795 \t Model/output_layer/output_b:0_grads\r\n-1.2125412e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n1.2779661e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-5.819682e-10 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n3.9421697e-09 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35687.37390899658\r\n> Valid loss: 4758.33650970459\r\n> Best valid loss so far: 4739.623916625977\r\nStopping in (35) epochs if no new minima!\r\n\r\nEpoch: 6\r\n> lr update: 0.048500003\r\n#################### DEBUGGING ####################\r\n8.107671e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n6.486137e-11 \t Model/input_layer/feature_bias:0_grads\r\n0.86348265 \t Model/output_layer/output_w:0_grads\r\n-19.398884 \t Model/output_layer/output_b:0_grads\r\n2.4077628e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n-2.4667464e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-1.07752e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n9.429133e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 34959.51641845703\r\n> Valid loss: 4720.412452697754\r\n> Best valid loss so far: 4739.623916625977\r\nStopping in (34) epochs if no new minima!\r\n!!! NEW local minima found, saving the model...\r\n\r\nEpoch: 7\r\n> lr update: 0.0482500035\r\n#################### DEBUGGING ####################\r\n1.1791363e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n9.433085e-12 \t Model/input_layer/feature_bias:0_grads\r\n0.46038043 \t Model/output_layer/output_w:0_grads\r\n-18.42015 \t Model/output_layer/output_b:0_grads\r\n-5.90758e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n6.253203e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-1.6275301e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n1.5087512e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35122.075828552246\r\n> Valid loss: 4872.291694641113\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (35) epochs if no new minima!\r\n\r\nEpoch: 8\r\n> lr update: 0.048000004\r\n#################### DEBUGGING ####################\r\n1.5793947e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n1.26351585e-11 \t Model/input_layer/feature_bias:0_grads\r\n-0.23440647 \t Model/output_layer/output_w:0_grads\r\n-17.745054 \t Model/output_layer/output_b:0_grads\r\n6.400091e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n-6.770721e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-7.499852e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n6.953364e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35376.37155151367\r\n> Valid loss: 4934.555885314941\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (34) epochs if no new minima!\r\n\r\nEpoch: 9\r\n> lr update: 0.0477500045\r\n#################### DEBUGGING ####################\r\n-1.2424676e-13 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n-9.939738e-11 \t Model/input_layer/feature_bias:0_grads\r\n0.5383458 \t Model/output_layer/output_w:0_grads\r\n-21.300901 \t Model/output_layer/output_b:0_grads\r\n-7.6299974e-13 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n8.020704e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-7.250712e-16 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n6.68652e-15 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35422.02657318115\r\n> Valid loss: 4915.4898681640625\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (33) epochs if no new minima!\r\n\r\nEpoch: 10\r\n> lr update: 0.047500005\r\n#################### DEBUGGING ####################\r\n4.3053272e-14 \t Model/input_layer/embedding:0_grads\r\n0.0 \t Model/input_layer/feature_weigths:0_grads\r\n3.4442636e-11 \t Model/input_layer/feature_bias:0_grads\r\n-0.4148861 \t Model/output_layer/output_w:0_grads\r\n-16.681393 \t Model/output_layer/output_b:0_grads\r\n-3.3981418e-12 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0_grads\r\n3.5677305e-11 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/gates/bias:0_grads\r\n-1.0744528e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0_grads\r\n9.898426e-14 \t Model/recurrent_cell/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0_grads\r\n\r\n==================== debug_var ====================\r\n> Train loss: 35475.495765686035\r\n> Valid loss: 4763.377861022949\r\n> Best valid loss so far: 4720.412452697754\r\nStopping in (32) epochs if no new minima!\r\n```\r\n\r\nYou can well see the gradients are very low in comparison to Torch POC up to the point that  some of them become zero or very close. Additionally, binary cross entropy loss is much higher and basically does not drop. Using standard `tf.train.GradientDescentOptimizer(self.lr)` does not make the gradients that small and is more stable in general but produces the same garbage predictions after training. We have been trying to figure out the reason for several weeks now and are simply out of clue what could cause such a drastic difference.  We checked our data preprocessing up to the point where we printed out the input matrix values and compared them side by side with Torch. They are identical so it is highly unlikely to be the data feeding mechanism. There is something not right with backpropagation it seems but our lack of experience with Tensorflow does not allow us to proceed further. Maybe somebody here could give us an advice on what could be the reason?\r\n\r\nAddtionally attaching Tensorboard output as well. \r\n\r\n![2018-04-06-114156_793x370_scrot](https://user-images.githubusercontent.com/7676160/38414743-a9d43972-398f-11e8-87d1-3a934fc9af3e.png)\r\n\r\n![2018-04-06-114228_1534x946_scrot](https://user-images.githubusercontent.com/7676160/38414748-acc7cd60-398f-11e8-8947-645e4bc63b43.png)\r\n\r\n![2018-04-06-114236_1544x719_scrot](https://user-images.githubusercontent.com/7676160/38414750-af4aab34-398f-11e8-8d9b-d20c5bdc1f7c.png)\r\n"}