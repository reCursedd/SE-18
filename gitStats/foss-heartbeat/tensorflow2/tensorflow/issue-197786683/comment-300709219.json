{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300709219", "html_url": "https://github.com/tensorflow/tensorflow/issues/6531#issuecomment-300709219", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6531", "id": 300709219, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDcwOTIxOQ==", "user": {"login": "PatWie", "id": 6756603, "node_id": "MDQ6VXNlcjY3NTY2MDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6756603?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PatWie", "html_url": "https://github.com/PatWie", "followers_url": "https://api.github.com/users/PatWie/followers", "following_url": "https://api.github.com/users/PatWie/following{/other_user}", "gists_url": "https://api.github.com/users/PatWie/gists{/gist_id}", "starred_url": "https://api.github.com/users/PatWie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PatWie/subscriptions", "organizations_url": "https://api.github.com/users/PatWie/orgs", "repos_url": "https://api.github.com/users/PatWie/repos", "events_url": "https://api.github.com/users/PatWie/events{/privacy}", "received_events_url": "https://api.github.com/users/PatWie/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-11T07:42:51Z", "updated_at": "2017-05-11T07:44:35Z", "author_association": "NONE", "body_html": "<p>What's about</p>\n<p><code>tf.losses.sparse_softmax_cross_entropy(labels,logits, ...)</code><br>\nvs<br>\n<code>tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)</code><br>\n?</p>\n<p>Is there any reason to provide both functions? I understand that <code>tf.losses</code> also maintains <code>tf.GraphKeys.LOSSES</code>. But wouldn't it be more consistent (kwargs vs. args, '_with_logits' vs. '') to only support a mixture by</p>\n<div class=\"highlight highlight-source-python\"><pre>tf.metrics.sparse_softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels,<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, \n                                                    <span class=\"pl-c1\">...</span>, <span class=\"pl-v\">use_collection</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)</pre></div>\n<p>(I really mean <code>metrics</code> and not <code>losses</code> to have one consistent modul which can also include accuracy.)</p>\n<p>This way, <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> can be just wrap<code>tf.metrics.sparse_softmax_cross_entropy_with_logits</code>.</p>\n<p>It would be much easier, if it would be possible to just write</p>\n<div class=\"highlight highlight-source-python\"><pre>prob <span class=\"pl-k\">=</span> tf.nn.softmax(logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>prob<span class=\"pl-pds\">'</span></span>)\npred <span class=\"pl-k\">=</span> tf.arg_max(prob, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>pred<span class=\"pl-pds\">'</span></span>)\n\ncost <span class=\"pl-k\">=</span> tf.metrics.sparse_softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels,<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\naccuracy <span class=\"pl-k\">=</span> tf.metrics.accuracy(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels,<span class=\"pl-v\">pred</span><span class=\"pl-k\">=</span>pred) <span class=\"pl-c\"><span class=\"pl-c\">#</span> or even support logits</span>\nprecision <span class=\"pl-k\">=</span> tf.metrics.accuracy(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels,<span class=\"pl-v\">pred</span><span class=\"pl-k\">=</span>pred)</pre></div>", "body_text": "What's about\ntf.losses.sparse_softmax_cross_entropy(labels,logits, ...)\nvs\ntf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n?\nIs there any reason to provide both functions? I understand that tf.losses also maintains tf.GraphKeys.LOSSES. But wouldn't it be more consistent (kwargs vs. args, '_with_logits' vs. '') to only support a mixture by\ntf.metrics.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits, \n                                                    ..., use_collection=False)\n(I really mean metrics and not losses to have one consistent modul which can also include accuracy.)\nThis way, tf.nn.sparse_softmax_cross_entropy_with_logits can be just wraptf.metrics.sparse_softmax_cross_entropy_with_logits.\nIt would be much easier, if it would be possible to just write\nprob = tf.nn.softmax(logits, name='prob')\npred = tf.arg_max(prob, 1, name='pred')\n\ncost = tf.metrics.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)\naccuracy = tf.metrics.accuracy(labels=labels,pred=pred) # or even support logits\nprecision = tf.metrics.accuracy(labels=labels,pred=pred)", "body": "What's about \r\n\r\n```tf.losses.sparse_softmax_cross_entropy(labels,logits, ...)```\r\nvs\r\n```tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)```\r\n?\r\n\r\nIs there any reason to provide both functions? I understand that `tf.losses` also maintains `tf.GraphKeys.LOSSES`. But wouldn't it be more consistent (kwargs vs. args, '_with_logits' vs. '') to only support a mixture by\r\n\r\n```python\r\ntf.metrics.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits, \r\n                                                    ..., use_collection=False)\r\n```\r\n(I really mean `metrics` and not `losses` to have one consistent modul which can also include accuracy.)\r\n\r\nThis way, `tf.nn.sparse_softmax_cross_entropy_with_logits` can be just wrap`tf.metrics.sparse_softmax_cross_entropy_with_logits`.\r\n\r\nIt would be much easier, if it would be possible to just write\r\n```python\r\nprob = tf.nn.softmax(logits, name='prob')\r\npred = tf.arg_max(prob, 1, name='pred')\r\n\r\ncost = tf.metrics.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)\r\naccuracy = tf.metrics.accuracy(labels=labels,pred=pred) # or even support logits\r\nprecision = tf.metrics.accuracy(labels=labels,pred=pred)\r\n```\r\n"}