{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310744042", "html_url": "https://github.com/tensorflow/tensorflow/issues/10835#issuecomment-310744042", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10835", "id": 310744042, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDc0NDA0Mg==", "user": {"login": "AnishShah", "id": 3175743, "node_id": "MDQ6VXNlcjMxNzU3NDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/3175743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnishShah", "html_url": "https://github.com/AnishShah", "followers_url": "https://api.github.com/users/AnishShah/followers", "following_url": "https://api.github.com/users/AnishShah/following{/other_user}", "gists_url": "https://api.github.com/users/AnishShah/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnishShah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnishShah/subscriptions", "organizations_url": "https://api.github.com/users/AnishShah/orgs", "repos_url": "https://api.github.com/users/AnishShah/repos", "events_url": "https://api.github.com/users/AnishShah/events{/privacy}", "received_events_url": "https://api.github.com/users/AnishShah/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-23T18:45:31Z", "updated_at": "2017-06-23T18:45:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This output is from tensorflow <code>master</code> branch. Except <code>reduce_prod</code>, all <code>reduce_*</code> &amp; its gradient op work perfectly for negative axis.</p>\n<pre><code>In [1]: import tensorflow as tf\n\nIn [2]: vars = tf.Variable([[1., 2.], [3., 4.]])\n\nIn [3]: tf.InteractiveSession()\n2017-06-24 00:12:38.861801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861832: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861854: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861864: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nOut[3]: &lt;tensorflow.python.client.session.InteractiveSession at 0x7f4145d54510&gt;\n\nIn [4]: tf.global_variables_initializer().run()\n\nIn [5]: print(tf.gradients(tf.reduce_max(vars, -1), vars)[0].eval())\n[[ 0.  1.]\n [ 0.  1.]]\n\nIn [6]: print(tf.gradients(tf.reduce_min(vars, -1), vars)[0].eval())\n[[ 1.  0.]\n [ 1.  0.]]\n\nIn [7]: print(tf.gradients(tf.reduce_mean(vars, -1), vars)[0].eval())\n[[ 0.5  0.5]\n [ 0.5  0.5]]\n\nIn [8]: print(tf.gradients(tf.reduce_sum(vars, -1), vars)[0].eval())\n[[ 1.  1.]\n [ 1.  1.]]\n\nIn [9]: print(tf.gradients(tf.reduce_logsumexp(vars, -1), vars)[0].eval())\n[[ 0.26894143  0.7310586 ]\n [ 0.26894143  0.7310586 ]]\n\n</code></pre>", "body_text": "This output is from tensorflow master branch. Except reduce_prod, all reduce_* & its gradient op work perfectly for negative axis.\nIn [1]: import tensorflow as tf\n\nIn [2]: vars = tf.Variable([[1., 2.], [3., 4.]])\n\nIn [3]: tf.InteractiveSession()\n2017-06-24 00:12:38.861801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861832: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861854: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-24 00:12:38.861864: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nOut[3]: <tensorflow.python.client.session.InteractiveSession at 0x7f4145d54510>\n\nIn [4]: tf.global_variables_initializer().run()\n\nIn [5]: print(tf.gradients(tf.reduce_max(vars, -1), vars)[0].eval())\n[[ 0.  1.]\n [ 0.  1.]]\n\nIn [6]: print(tf.gradients(tf.reduce_min(vars, -1), vars)[0].eval())\n[[ 1.  0.]\n [ 1.  0.]]\n\nIn [7]: print(tf.gradients(tf.reduce_mean(vars, -1), vars)[0].eval())\n[[ 0.5  0.5]\n [ 0.5  0.5]]\n\nIn [8]: print(tf.gradients(tf.reduce_sum(vars, -1), vars)[0].eval())\n[[ 1.  1.]\n [ 1.  1.]]\n\nIn [9]: print(tf.gradients(tf.reduce_logsumexp(vars, -1), vars)[0].eval())\n[[ 0.26894143  0.7310586 ]\n [ 0.26894143  0.7310586 ]]", "body": "This output is from tensorflow `master` branch. Except `reduce_prod`, all `reduce_*` & its gradient op work perfectly for negative axis.\r\n\r\n```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: vars = tf.Variable([[1., 2.], [3., 4.]])\r\n\r\nIn [3]: tf.InteractiveSession()\r\n2017-06-24 00:12:38.861801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 00:12:38.861832: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 00:12:38.861845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 00:12:38.861854: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-24 00:12:38.861864: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nOut[3]: <tensorflow.python.client.session.InteractiveSession at 0x7f4145d54510>\r\n\r\nIn [4]: tf.global_variables_initializer().run()\r\n\r\nIn [5]: print(tf.gradients(tf.reduce_max(vars, -1), vars)[0].eval())\r\n[[ 0.  1.]\r\n [ 0.  1.]]\r\n\r\nIn [6]: print(tf.gradients(tf.reduce_min(vars, -1), vars)[0].eval())\r\n[[ 1.  0.]\r\n [ 1.  0.]]\r\n\r\nIn [7]: print(tf.gradients(tf.reduce_mean(vars, -1), vars)[0].eval())\r\n[[ 0.5  0.5]\r\n [ 0.5  0.5]]\r\n\r\nIn [8]: print(tf.gradients(tf.reduce_sum(vars, -1), vars)[0].eval())\r\n[[ 1.  1.]\r\n [ 1.  1.]]\r\n\r\nIn [9]: print(tf.gradients(tf.reduce_logsumexp(vars, -1), vars)[0].eval())\r\n[[ 0.26894143  0.7310586 ]\r\n [ 0.26894143  0.7310586 ]]\r\n\r\n```"}