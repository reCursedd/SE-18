{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7764", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7764/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7764/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7764/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/7764", "id": 209378615, "node_id": "MDExOlB1bGxSZXF1ZXN0MTA3MzUzMTc2", "number": 7764, "title": "native Cuda kernel for tf.dynamic_stitch op", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136613, "node_id": "MDU6TGFiZWwzMDAxMzY2MTM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20no", "name": "cla: no", "color": "eb6420", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-02-22T07:59:58Z", "updated_at": "2017-03-10T07:08:28Z", "closed_at": "2017-03-10T02:59:46Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7764", "html_url": "https://github.com/tensorflow/tensorflow/pull/7764", "diff_url": "https://github.com/tensorflow/tensorflow/pull/7764.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/7764.patch"}, "body_html": "<p>Correspond to issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"205278217\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7251\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7251/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7251\">#7251</a><br>\nHere is the comparison between<br>\nmy implementation:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6672514/23201105/ed219d82-f912-11e6-9ed1-c86c80d63921.png\"><img src=\"https://cloud.githubusercontent.com/assets/6672514/23201105/ed219d82-f912-11e6-9ed1-c86c80d63921.png\" alt=\"dynamic_stitch_gpu\" style=\"max-width:100%;\"></a><br>\nand current implementation:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/6672514/23201120/faf28278-f912-11e6-9b4f-f8793bd9faa6.png\"><img src=\"https://cloud.githubusercontent.com/assets/6672514/23201120/faf28278-f912-11e6-9b4f-f8793bd9faa6.png\" alt=\"dynamic_stitch_cpu\" style=\"max-width:100%;\"></a><br>\ngenerated by Nvidia visual profiler with <a href=\"https://gist.github.com/MycChiu/c353fd3c98c47dceae866ed30f40a5a8\">this code</a>. <em>Since the kernel uses <code>CudaDeviceArrayStruct</code>, which depends on code from <code>//tensorflow/core:lib</code>, I can't generate custom_op library and use the custom_op for performance comparison. I ended up manually switch the tensorflow directory python uses between runs</em><br>\nCurrently, I still have two problems:</p>\n<ol>\n<li>For some reason, tensorflow won't use the GPU kernel automatically, I had to force it with <code>tf.device</code>. What's even worse, tensorflow seems to refuse loading the GPU kernel in <code>test_session()</code> even if I tried to force it with <code>tf.device</code>, so this code is currently not tested with <code>dynamic_stitch_test.py</code>.<br>\n<del>2. As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> suggested, Cuda kernels need to be real-valued, so I ended up calling <code>TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_DYNAMIC_STITCH_GPU);</code> to register the kernel. However, I feel like this will break some people's current code if their input data is from the unsupported data type.</del> <em>fixed in the latest commit.</em></li>\n</ol>\n<p>Finally, this implementation is really under-optimized for cases where the size of each input data slice is small, because the indices are still processed by the CPU (plus CUDA reads data in warps of 32 threads). However, I still don't think it will be worse than processing everything on CPU then copy them back to GPU though.</p>", "body_text": "Correspond to issue #7251\nHere is the comparison between\nmy implementation:\n\nand current implementation:\n\ngenerated by Nvidia visual profiler with this code. Since the kernel uses CudaDeviceArrayStruct, which depends on code from //tensorflow/core:lib, I can't generate custom_op library and use the custom_op for performance comparison. I ended up manually switch the tensorflow directory python uses between runs\nCurrently, I still have two problems:\n\nFor some reason, tensorflow won't use the GPU kernel automatically, I had to force it with tf.device. What's even worse, tensorflow seems to refuse loading the GPU kernel in test_session() even if I tried to force it with tf.device, so this code is currently not tested with dynamic_stitch_test.py.\n2. As @yaroslavvb suggested, Cuda kernels need to be real-valued, so I ended up calling TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_DYNAMIC_STITCH_GPU); to register the kernel. However, I feel like this will break some people's current code if their input data is from the unsupported data type. fixed in the latest commit.\n\nFinally, this implementation is really under-optimized for cases where the size of each input data slice is small, because the indices are still processed by the CPU (plus CUDA reads data in warps of 32 threads). However, I still don't think it will be worse than processing everything on CPU then copy them back to GPU though.", "body": "Correspond to issue #7251 \r\nHere is the comparison between \r\nmy implementation:\r\n![dynamic_stitch_gpu](https://cloud.githubusercontent.com/assets/6672514/23201105/ed219d82-f912-11e6-9ed1-c86c80d63921.png)\r\nand current implementation:\r\n![dynamic_stitch_cpu](https://cloud.githubusercontent.com/assets/6672514/23201120/faf28278-f912-11e6-9b4f-f8793bd9faa6.png)\r\ngenerated by Nvidia visual profiler with [this code](https://gist.github.com/MycChiu/c353fd3c98c47dceae866ed30f40a5a8). *Since the kernel uses `CudaDeviceArrayStruct`, which depends on code from `//tensorflow/core:lib`, I can't generate custom_op library and use the custom_op for performance comparison. I ended up manually switch the tensorflow directory python uses between runs*\r\nCurrently, I still have two problems:\r\n1. For some reason, tensorflow won't use the GPU kernel automatically, I had to force it with `tf.device`. What's even worse, tensorflow seems to refuse loading the GPU kernel in `test_session()` even if I tried to force it with `tf.device`, so this code is currently not tested with `dynamic_stitch_test.py`.\r\n~~2. As @yaroslavvb suggested, Cuda kernels need to be real-valued, so I ended up calling `TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_DYNAMIC_STITCH_GPU);` to register the kernel. However, I feel like this will break some people's current code if their input data is from the unsupported data type.~~ *fixed in the latest commit.*\r\n\r\nFinally, this implementation is really under-optimized for cases where the size of each input data slice is small, because the indices are still processed by the CPU (plus CUDA reads data in warps of 32 threads). However, I still don't think it will be worse than processing everything on CPU then copy them back to GPU though."}