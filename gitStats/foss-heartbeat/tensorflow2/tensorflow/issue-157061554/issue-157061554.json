{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2521", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2521/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2521/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2521/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2521", "id": 157061554, "node_id": "MDU6SXNzdWUxNTcwNjE1NTQ=", "number": 2521, "title": "Retrain.py with multiple GPUs", "user": {"login": "danleh93", "id": 19287183, "node_id": "MDQ6VXNlcjE5Mjg3MTgz", "avatar_url": "https://avatars3.githubusercontent.com/u/19287183?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danleh93", "html_url": "https://github.com/danleh93", "followers_url": "https://api.github.com/users/danleh93/followers", "following_url": "https://api.github.com/users/danleh93/following{/other_user}", "gists_url": "https://api.github.com/users/danleh93/gists{/gist_id}", "starred_url": "https://api.github.com/users/danleh93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danleh93/subscriptions", "organizations_url": "https://api.github.com/users/danleh93/orgs", "repos_url": "https://api.github.com/users/danleh93/repos", "events_url": "https://api.github.com/users/danleh93/events{/privacy}", "received_events_url": "https://api.github.com/users/danleh93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-05-26T19:36:08Z", "updated_at": "2016-05-26T21:10:01Z", "closed_at": "2016-05-26T21:10:01Z", "author_association": "NONE", "body_html": "<p>I am trying to run <strong>retrain.py</strong> on an AWS GPU instance with 4 K520's. When running <strong>retrain.py</strong> and watching the GPU activity using <code>nvidia-smi</code>, it seems like all of the calculations are happening on one GPU. The output looks like:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      2833    C   python                                        3801MiB |\n|    1      2833    C   python                                          37MiB |\n|    2      2833    C   python                                          37MiB |\n|    3      2833    C   python                                          37MiB |\n+-----------------------------------------------------------------------------+\n\n</code></pre>\n<p>However, when I run <strong>cifar10_multi_gpu_train.py</strong> and set <code>--num_gpus</code> to 4, the computations are distributed evenly as follows:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     14226    C   python                                        3841MiB |\n|    1     14226    C   python                                        3841MiB |\n|    2     14226    C   python                                        3841MiB |\n|    3     14226    C   python                                        3841MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>Is there a way to run <strong>retrain.py</strong> on multiple GPUs like on the cifar10_multi_gpu_train.py? If not, is there an easy way to use my own custom images for the <strong>cifar10_multi_gpu_train.py</strong>?</p>", "body_text": "I am trying to run retrain.py on an AWS GPU instance with 4 K520's. When running retrain.py and watching the GPU activity using nvidia-smi, it seems like all of the calculations are happening on one GPU. The output looks like:\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      2833    C   python                                        3801MiB |\n|    1      2833    C   python                                          37MiB |\n|    2      2833    C   python                                          37MiB |\n|    3      2833    C   python                                          37MiB |\n+-----------------------------------------------------------------------------+\n\n\nHowever, when I run cifar10_multi_gpu_train.py and set --num_gpus to 4, the computations are distributed evenly as follows:\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     14226    C   python                                        3841MiB |\n|    1     14226    C   python                                        3841MiB |\n|    2     14226    C   python                                        3841MiB |\n|    3     14226    C   python                                        3841MiB |\n+-----------------------------------------------------------------------------+\n\nIs there a way to run retrain.py on multiple GPUs like on the cifar10_multi_gpu_train.py? If not, is there an easy way to use my own custom images for the cifar10_multi_gpu_train.py?", "body": "I am trying to run **retrain.py** on an AWS GPU instance with 4 K520's. When running **retrain.py** and watching the GPU activity using `nvidia-smi`, it seems like all of the calculations are happening on one GPU. The output looks like:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      2833    C   python                                        3801MiB |\n|    1      2833    C   python                                          37MiB |\n|    2      2833    C   python                                          37MiB |\n|    3      2833    C   python                                          37MiB |\n+-----------------------------------------------------------------------------+\n\n```\n\nHowever, when I run **cifar10_multi_gpu_train.py** and set `--num_gpus` to 4, the computations are distributed evenly as follows:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     14226    C   python                                        3841MiB |\n|    1     14226    C   python                                        3841MiB |\n|    2     14226    C   python                                        3841MiB |\n|    3     14226    C   python                                        3841MiB |\n+-----------------------------------------------------------------------------+\n```\n\nIs there a way to run **retrain.py** on multiple GPUs like on the cifar10_multi_gpu_train.py? If not, is there an easy way to use my own custom images for the **cifar10_multi_gpu_train.py**?\n"}