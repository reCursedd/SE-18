{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234755558", "html_url": "https://github.com/tensorflow/tensorflow/issues/2987#issuecomment-234755558", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2987", "id": 234755558, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDc1NTU1OA==", "user": {"login": "rohitgirdhar", "id": 1893429, "node_id": "MDQ6VXNlcjE4OTM0Mjk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1893429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohitgirdhar", "html_url": "https://github.com/rohitgirdhar", "followers_url": "https://api.github.com/users/rohitgirdhar/followers", "following_url": "https://api.github.com/users/rohitgirdhar/following{/other_user}", "gists_url": "https://api.github.com/users/rohitgirdhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohitgirdhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohitgirdhar/subscriptions", "organizations_url": "https://api.github.com/users/rohitgirdhar/orgs", "repos_url": "https://api.github.com/users/rohitgirdhar/repos", "events_url": "https://api.github.com/users/rohitgirdhar/events{/privacy}", "received_events_url": "https://api.github.com/users/rohitgirdhar/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-24T03:45:00Z", "updated_at": "2016-07-24T03:46:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm facing the same problem. I'm running the inception model fine-tuning on my data. Here's a part of the log being printed. I'm using all TensorFlow data read operations (so no copy from numpy to TF, as I'm not using <code>DequeueMany</code> manually, but through <code>shuffle_batch_join</code>). I'll try to provide the timeline when I have a chance to re-run the training. The machine resources otherwise seem fine (CPU/memory is not choked, the prefetch queues logs show &gt;70% full)</p>\n<pre lang=\"txt\"><code>2016-07-23 14:34:15.784645: step 2720, loss = 2.63 (68.5 examples/sec; 3.740 sec/batch)                                                                                                                                                                                                                                                                         [224/1046]\n2016-07-23 14:34:54.449992: step 2730, loss = 2.23 (65.7 examples/sec; 3.897 sec/batch)\n2016-07-23 14:35:30.115364: step 2740, loss = 2.11 (66.2 examples/sec; 3.867 sec/batch)\n2016-07-23 14:36:06.048820: step 2750, loss = 2.14 (64.8 examples/sec; 3.953 sec/batch)\n2016-07-23 14:36:43.302534: step 2760, loss = 2.18 (71.4 examples/sec; 3.585 sec/batch)\n2016-07-23 14:37:21.960443: step 2770, loss = 2.26 (74.2 examples/sec; 3.451 sec/batch)\n2016-07-23 14:37:59.039025: step 2780, loss = 2.04 (67.2 examples/sec; 3.808 sec/batch)\n2016-07-23 14:38:39.144252: step 2790, loss = 2.45 (59.2 examples/sec; 4.321 sec/batch)\n2016-07-23 14:39:17.613073: step 2800, loss = 2.18 (81.0 examples/sec; 3.162 sec/batch)\n2016-07-23 14:39:56.705607: step 2810, loss = 2.55 (66.5 examples/sec; 3.847 sec/batch)\n2016-07-23 14:40:32.612656: step 2820, loss = 2.38 (78.8 examples/sec; 3.247 sec/batch)\n2016-07-23 14:41:09.194193: step 2830, loss = 2.47 (67.1 examples/sec; 3.814 sec/batch)\n2016-07-23 14:41:44.523209: step 2840, loss = 2.30 (77.5 examples/sec; 3.303 sec/batch)\n2016-07-23 14:42:21.437528: step 2850, loss = 2.27 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:42:55.936083: step 2860, loss = 2.22 (80.3 examples/sec; 3.189 sec/batch)\n2016-07-23 14:43:32.151591: step 2870, loss = 2.35 (60.1 examples/sec; 4.259 sec/batch)\n2016-07-23 14:44:07.513355: step 2880, loss = 2.38 (65.1 examples/sec; 3.931 sec/batch)\n2016-07-23 14:44:42.496976: step 2890, loss = 1.99 (67.7 examples/sec; 3.781 sec/batch)\n2016-07-23 14:45:20.375483: step 2900, loss = 2.67 (50.2 examples/sec; 5.100 sec/batch)\n2016-07-23 14:46:00.563096: step 2910, loss = 2.23 (62.5 examples/sec; 4.093 sec/batch)\n2016-07-23 14:46:39.610138: step 2920, loss = 2.19 (80.1 examples/sec; 3.197 sec/batch)\n2016-07-23 14:47:19.147337: step 2930, loss = 2.31 (51.6 examples/sec; 4.957 sec/batch)\n2016-07-23 14:48:03.449487: step 2940, loss = 2.27 (50.8 examples/sec; 5.041 sec/batch)\n2016-07-23 14:48:47.893646: step 2950, loss = 2.29 (52.0 examples/sec; 4.919 sec/batch)\n2016-07-23 14:49:27.023626: step 2960, loss = 2.27 (73.7 examples/sec; 3.475 sec/batch)\n2016-07-23 14:50:11.125284: step 2970, loss = 2.08 (65.4 examples/sec; 3.914 sec/batch)\n2016-07-23 14:50:51.295322: step 2980, loss = 2.19 (50.7 examples/sec; 5.053 sec/batch)\n2016-07-23 14:51:33.655334: step 2990, loss = 2.30 (66.0 examples/sec; 3.879 sec/batch)\n2016-07-23 14:52:10.323608: step 3000, loss = 2.25 (68.8 examples/sec; 3.723 sec/batch)\n2016-07-23 14:52:52.609913: step 3010, loss = 2.04 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:53:31.631134: step 3020, loss = 2.31 (54.5 examples/sec; 4.696 sec/batch)\n2016-07-23 14:54:13.547641: step 3030, loss = 2.33 (55.1 examples/sec; 4.643 sec/batch)\n2016-07-23 14:55:00.528403: step 3040, loss = 2.29 (49.1 examples/sec; 5.217 sec/batch)\n2016-07-23 14:55:45.258737: step 3050, loss = 2.17 (51.4 examples/sec; 4.979 sec/batch)\n2016-07-23 14:56:31.159313: step 3060, loss = 2.29 (51.5 examples/sec; 4.968 sec/batch)\n2016-07-23 14:57:15.148195: step 3070, loss = 2.27 (51.8 examples/sec; 4.946 sec/batch)\n2016-07-23 14:57:54.451044: step 3080, loss = 2.40 (68.8 examples/sec; 3.724 sec/batch)\n2016-07-23 14:58:32.325357: step 3090, loss = 1.93 (58.4 examples/sec; 4.380 sec/batch)\n2016-07-23 14:59:06.746820: step 3100, loss = 2.02 (88.0 examples/sec; 2.910 sec/batch)\n2016-07-23 14:59:39.746600: step 3110, loss = 2.20 (86.2 examples/sec; 2.970 sec/batch)\n2016-07-23 15:00:10.409635: step 3120, loss = 2.19 (79.7 examples/sec; 3.213 sec/batch)\n2016-07-23 15:00:41.756720: step 3130, loss = 2.28 (83.6 examples/sec; 3.062 sec/batch)\n2016-07-23 15:01:12.757209: step 3140, loss = 2.26 (85.6 examples/sec; 2.989 sec/batch)\n2016-07-23 15:01:43.993791: step 3150, loss = 2.23 (79.6 examples/sec; 3.214 sec/batch)\n2016-07-23 15:02:15.348264: step 3160, loss = 2.04 (82.8 examples/sec; 3.090 sec/batch)\n2016-07-23 15:02:46.752311: step 3170, loss = 2.26 (83.7 examples/sec; 3.058 sec/batch)\n2016-07-23 15:03:18.581683: step 3180, loss = 1.92 (78.9 examples/sec; 3.243 sec/batch)\n2016-07-23 15:03:50.599771: step 3190, loss = 2.38 (80.9 examples/sec; 3.166 sec/batch)\n2016-07-23 15:04:22.723497: step 3200, loss = 2.00 (78.4 examples/sec; 3.266 sec/batch)\n2016-07-23 15:05:16.565673: step 3210, loss = 1.99 (25.1 examples/sec; 10.188 sec/batch)\n2016-07-23 15:06:37.336032: step 3220, loss = 2.34 (28.4 examples/sec; 9.013 sec/batch)\n2016-07-23 15:08:03.531096: step 3230, loss = 2.10 (26.1 examples/sec; 9.817 sec/batch)\n2016-07-23 15:09:26.177609: step 3240, loss = 2.34 (29.7 examples/sec; 8.613 sec/batch)\n2016-07-23 15:10:48.697512: step 3250, loss = 2.06 (25.0 examples/sec; 10.229 sec/batch)\n2016-07-23 15:12:14.720998: step 3260, loss = 2.16 (26.0 examples/sec; 9.861 sec/batch)\n2016-07-23 15:13:42.700315: step 3270, loss = 2.06 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:15:07.717132: step 3280, loss = 2.00 (26.9 examples/sec; 9.524 sec/batch)\n2016-07-23 15:16:29.206475: step 3290, loss = 2.15 (71.1 examples/sec; 3.600 sec/batch)\n2016-07-23 15:17:56.483761: step 3300, loss = 2.17 (25.3 examples/sec; 10.102 sec/batch)\n2016-07-23 15:19:32.355597: step 3310, loss = 2.26 (24.7 examples/sec; 10.352 sec/batch)\n2016-07-23 15:20:58.064109: step 3320, loss = 2.18 (25.8 examples/sec; 9.907 sec/batch)\n2016-07-23 15:22:18.608871: step 3330, loss = 2.02 (27.9 examples/sec; 9.183 sec/batch)\n2016-07-23 15:23:39.227969: step 3340, loss = 2.17 (26.8 examples/sec; 9.567 sec/batch)\n2016-07-23 15:25:03.643470: step 3350, loss = 2.01 (25.7 examples/sec; 9.976 sec/batch)\n2016-07-23 15:26:25.196536: step 3360, loss = 2.08 (33.1 examples/sec; 7.728 sec/batch)\n2016-07-23 15:27:50.458838: step 3370, loss = 2.01 (26.9 examples/sec; 9.533 sec/batch)\n2016-07-23 15:29:12.861687: step 3380, loss = 2.07 (66.9 examples/sec; 3.827 sec/batch)\n2016-07-23 15:30:31.386601: step 3390, loss = 1.95 (71.9 examples/sec; 3.559 sec/batch)\n2016-07-23 15:31:58.734378: step 3400, loss = 2.37 (25.6 examples/sec; 10.011 sec/batch)\n2016-07-23 15:33:33.894047: step 3410, loss = 2.06 (26.7 examples/sec; 9.581 sec/batch)\n2016-07-23 15:34:54.087397: step 3420, loss = 1.99 (27.5 examples/sec; 9.311 sec/batch)\n2016-07-23 15:36:11.707716: step 3430, loss = 2.15 (69.0 examples/sec; 3.710 sec/batch)\n2016-07-23 15:37:31.881528: step 3440, loss = 2.20 (46.0 examples/sec; 5.570 sec/batch)\n2016-07-23 15:38:55.509458: step 3450, loss = 2.08 (25.5 examples/sec; 10.041 sec/batch)\n2016-07-23 15:40:16.870401: step 3460, loss = 2.35 (26.5 examples/sec; 9.647 sec/batch)\n2016-07-23 15:41:40.889863: step 3470, loss = 2.23 (27.3 examples/sec; 9.389 sec/batch)\n2016-07-23 15:43:06.156092: step 3480, loss = 2.08 (27.3 examples/sec; 9.385 sec/batch)\n2016-07-23 15:44:25.849125: step 3490, loss = 2.04 (65.0 examples/sec; 3.939 sec/batch)\n2016-07-23 15:45:46.423841: step 3500, loss = 2.04 (55.5 examples/sec; 4.616 sec/batch)\n2016-07-23 15:47:24.597796: step 3510, loss = 2.13 (25.8 examples/sec; 9.938 sec/batch)\n2016-07-23 15:48:50.303574: step 3520, loss = 1.99 (25.7 examples/sec; 9.968 sec/batch)\n2016-07-23 15:50:13.826881: step 3530, loss = 2.03 (31.7 examples/sec; 8.069 sec/batch)\n2016-07-23 15:51:27.094435: step 3540, loss = 1.93 (63.6 examples/sec; 4.026 sec/batch)\n2016-07-23 15:52:46.522184: step 3550, loss = 2.13 (70.3 examples/sec; 3.639 sec/batch)\n2016-07-23 15:54:10.492599: step 3560, loss = 2.17 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:55:32.567000: step 3570, loss = 1.93 (27.0 examples/sec; 9.465 sec/batch)\n2016-07-23 15:56:58.073701: step 3580, loss = 2.21 (25.5 examples/sec; 10.058 sec/batch)\n2016-07-23 15:58:21.894001: step 3590, loss = 2.14 (31.5 examples/sec; 8.129 sec/batch)\n</code></pre>", "body_text": "I'm facing the same problem. I'm running the inception model fine-tuning on my data. Here's a part of the log being printed. I'm using all TensorFlow data read operations (so no copy from numpy to TF, as I'm not using DequeueMany manually, but through shuffle_batch_join). I'll try to provide the timeline when I have a chance to re-run the training. The machine resources otherwise seem fine (CPU/memory is not choked, the prefetch queues logs show >70% full)\n2016-07-23 14:34:15.784645: step 2720, loss = 2.63 (68.5 examples/sec; 3.740 sec/batch)                                                                                                                                                                                                                                                                         [224/1046]\n2016-07-23 14:34:54.449992: step 2730, loss = 2.23 (65.7 examples/sec; 3.897 sec/batch)\n2016-07-23 14:35:30.115364: step 2740, loss = 2.11 (66.2 examples/sec; 3.867 sec/batch)\n2016-07-23 14:36:06.048820: step 2750, loss = 2.14 (64.8 examples/sec; 3.953 sec/batch)\n2016-07-23 14:36:43.302534: step 2760, loss = 2.18 (71.4 examples/sec; 3.585 sec/batch)\n2016-07-23 14:37:21.960443: step 2770, loss = 2.26 (74.2 examples/sec; 3.451 sec/batch)\n2016-07-23 14:37:59.039025: step 2780, loss = 2.04 (67.2 examples/sec; 3.808 sec/batch)\n2016-07-23 14:38:39.144252: step 2790, loss = 2.45 (59.2 examples/sec; 4.321 sec/batch)\n2016-07-23 14:39:17.613073: step 2800, loss = 2.18 (81.0 examples/sec; 3.162 sec/batch)\n2016-07-23 14:39:56.705607: step 2810, loss = 2.55 (66.5 examples/sec; 3.847 sec/batch)\n2016-07-23 14:40:32.612656: step 2820, loss = 2.38 (78.8 examples/sec; 3.247 sec/batch)\n2016-07-23 14:41:09.194193: step 2830, loss = 2.47 (67.1 examples/sec; 3.814 sec/batch)\n2016-07-23 14:41:44.523209: step 2840, loss = 2.30 (77.5 examples/sec; 3.303 sec/batch)\n2016-07-23 14:42:21.437528: step 2850, loss = 2.27 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:42:55.936083: step 2860, loss = 2.22 (80.3 examples/sec; 3.189 sec/batch)\n2016-07-23 14:43:32.151591: step 2870, loss = 2.35 (60.1 examples/sec; 4.259 sec/batch)\n2016-07-23 14:44:07.513355: step 2880, loss = 2.38 (65.1 examples/sec; 3.931 sec/batch)\n2016-07-23 14:44:42.496976: step 2890, loss = 1.99 (67.7 examples/sec; 3.781 sec/batch)\n2016-07-23 14:45:20.375483: step 2900, loss = 2.67 (50.2 examples/sec; 5.100 sec/batch)\n2016-07-23 14:46:00.563096: step 2910, loss = 2.23 (62.5 examples/sec; 4.093 sec/batch)\n2016-07-23 14:46:39.610138: step 2920, loss = 2.19 (80.1 examples/sec; 3.197 sec/batch)\n2016-07-23 14:47:19.147337: step 2930, loss = 2.31 (51.6 examples/sec; 4.957 sec/batch)\n2016-07-23 14:48:03.449487: step 2940, loss = 2.27 (50.8 examples/sec; 5.041 sec/batch)\n2016-07-23 14:48:47.893646: step 2950, loss = 2.29 (52.0 examples/sec; 4.919 sec/batch)\n2016-07-23 14:49:27.023626: step 2960, loss = 2.27 (73.7 examples/sec; 3.475 sec/batch)\n2016-07-23 14:50:11.125284: step 2970, loss = 2.08 (65.4 examples/sec; 3.914 sec/batch)\n2016-07-23 14:50:51.295322: step 2980, loss = 2.19 (50.7 examples/sec; 5.053 sec/batch)\n2016-07-23 14:51:33.655334: step 2990, loss = 2.30 (66.0 examples/sec; 3.879 sec/batch)\n2016-07-23 14:52:10.323608: step 3000, loss = 2.25 (68.8 examples/sec; 3.723 sec/batch)\n2016-07-23 14:52:52.609913: step 3010, loss = 2.04 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:53:31.631134: step 3020, loss = 2.31 (54.5 examples/sec; 4.696 sec/batch)\n2016-07-23 14:54:13.547641: step 3030, loss = 2.33 (55.1 examples/sec; 4.643 sec/batch)\n2016-07-23 14:55:00.528403: step 3040, loss = 2.29 (49.1 examples/sec; 5.217 sec/batch)\n2016-07-23 14:55:45.258737: step 3050, loss = 2.17 (51.4 examples/sec; 4.979 sec/batch)\n2016-07-23 14:56:31.159313: step 3060, loss = 2.29 (51.5 examples/sec; 4.968 sec/batch)\n2016-07-23 14:57:15.148195: step 3070, loss = 2.27 (51.8 examples/sec; 4.946 sec/batch)\n2016-07-23 14:57:54.451044: step 3080, loss = 2.40 (68.8 examples/sec; 3.724 sec/batch)\n2016-07-23 14:58:32.325357: step 3090, loss = 1.93 (58.4 examples/sec; 4.380 sec/batch)\n2016-07-23 14:59:06.746820: step 3100, loss = 2.02 (88.0 examples/sec; 2.910 sec/batch)\n2016-07-23 14:59:39.746600: step 3110, loss = 2.20 (86.2 examples/sec; 2.970 sec/batch)\n2016-07-23 15:00:10.409635: step 3120, loss = 2.19 (79.7 examples/sec; 3.213 sec/batch)\n2016-07-23 15:00:41.756720: step 3130, loss = 2.28 (83.6 examples/sec; 3.062 sec/batch)\n2016-07-23 15:01:12.757209: step 3140, loss = 2.26 (85.6 examples/sec; 2.989 sec/batch)\n2016-07-23 15:01:43.993791: step 3150, loss = 2.23 (79.6 examples/sec; 3.214 sec/batch)\n2016-07-23 15:02:15.348264: step 3160, loss = 2.04 (82.8 examples/sec; 3.090 sec/batch)\n2016-07-23 15:02:46.752311: step 3170, loss = 2.26 (83.7 examples/sec; 3.058 sec/batch)\n2016-07-23 15:03:18.581683: step 3180, loss = 1.92 (78.9 examples/sec; 3.243 sec/batch)\n2016-07-23 15:03:50.599771: step 3190, loss = 2.38 (80.9 examples/sec; 3.166 sec/batch)\n2016-07-23 15:04:22.723497: step 3200, loss = 2.00 (78.4 examples/sec; 3.266 sec/batch)\n2016-07-23 15:05:16.565673: step 3210, loss = 1.99 (25.1 examples/sec; 10.188 sec/batch)\n2016-07-23 15:06:37.336032: step 3220, loss = 2.34 (28.4 examples/sec; 9.013 sec/batch)\n2016-07-23 15:08:03.531096: step 3230, loss = 2.10 (26.1 examples/sec; 9.817 sec/batch)\n2016-07-23 15:09:26.177609: step 3240, loss = 2.34 (29.7 examples/sec; 8.613 sec/batch)\n2016-07-23 15:10:48.697512: step 3250, loss = 2.06 (25.0 examples/sec; 10.229 sec/batch)\n2016-07-23 15:12:14.720998: step 3260, loss = 2.16 (26.0 examples/sec; 9.861 sec/batch)\n2016-07-23 15:13:42.700315: step 3270, loss = 2.06 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:15:07.717132: step 3280, loss = 2.00 (26.9 examples/sec; 9.524 sec/batch)\n2016-07-23 15:16:29.206475: step 3290, loss = 2.15 (71.1 examples/sec; 3.600 sec/batch)\n2016-07-23 15:17:56.483761: step 3300, loss = 2.17 (25.3 examples/sec; 10.102 sec/batch)\n2016-07-23 15:19:32.355597: step 3310, loss = 2.26 (24.7 examples/sec; 10.352 sec/batch)\n2016-07-23 15:20:58.064109: step 3320, loss = 2.18 (25.8 examples/sec; 9.907 sec/batch)\n2016-07-23 15:22:18.608871: step 3330, loss = 2.02 (27.9 examples/sec; 9.183 sec/batch)\n2016-07-23 15:23:39.227969: step 3340, loss = 2.17 (26.8 examples/sec; 9.567 sec/batch)\n2016-07-23 15:25:03.643470: step 3350, loss = 2.01 (25.7 examples/sec; 9.976 sec/batch)\n2016-07-23 15:26:25.196536: step 3360, loss = 2.08 (33.1 examples/sec; 7.728 sec/batch)\n2016-07-23 15:27:50.458838: step 3370, loss = 2.01 (26.9 examples/sec; 9.533 sec/batch)\n2016-07-23 15:29:12.861687: step 3380, loss = 2.07 (66.9 examples/sec; 3.827 sec/batch)\n2016-07-23 15:30:31.386601: step 3390, loss = 1.95 (71.9 examples/sec; 3.559 sec/batch)\n2016-07-23 15:31:58.734378: step 3400, loss = 2.37 (25.6 examples/sec; 10.011 sec/batch)\n2016-07-23 15:33:33.894047: step 3410, loss = 2.06 (26.7 examples/sec; 9.581 sec/batch)\n2016-07-23 15:34:54.087397: step 3420, loss = 1.99 (27.5 examples/sec; 9.311 sec/batch)\n2016-07-23 15:36:11.707716: step 3430, loss = 2.15 (69.0 examples/sec; 3.710 sec/batch)\n2016-07-23 15:37:31.881528: step 3440, loss = 2.20 (46.0 examples/sec; 5.570 sec/batch)\n2016-07-23 15:38:55.509458: step 3450, loss = 2.08 (25.5 examples/sec; 10.041 sec/batch)\n2016-07-23 15:40:16.870401: step 3460, loss = 2.35 (26.5 examples/sec; 9.647 sec/batch)\n2016-07-23 15:41:40.889863: step 3470, loss = 2.23 (27.3 examples/sec; 9.389 sec/batch)\n2016-07-23 15:43:06.156092: step 3480, loss = 2.08 (27.3 examples/sec; 9.385 sec/batch)\n2016-07-23 15:44:25.849125: step 3490, loss = 2.04 (65.0 examples/sec; 3.939 sec/batch)\n2016-07-23 15:45:46.423841: step 3500, loss = 2.04 (55.5 examples/sec; 4.616 sec/batch)\n2016-07-23 15:47:24.597796: step 3510, loss = 2.13 (25.8 examples/sec; 9.938 sec/batch)\n2016-07-23 15:48:50.303574: step 3520, loss = 1.99 (25.7 examples/sec; 9.968 sec/batch)\n2016-07-23 15:50:13.826881: step 3530, loss = 2.03 (31.7 examples/sec; 8.069 sec/batch)\n2016-07-23 15:51:27.094435: step 3540, loss = 1.93 (63.6 examples/sec; 4.026 sec/batch)\n2016-07-23 15:52:46.522184: step 3550, loss = 2.13 (70.3 examples/sec; 3.639 sec/batch)\n2016-07-23 15:54:10.492599: step 3560, loss = 2.17 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:55:32.567000: step 3570, loss = 1.93 (27.0 examples/sec; 9.465 sec/batch)\n2016-07-23 15:56:58.073701: step 3580, loss = 2.21 (25.5 examples/sec; 10.058 sec/batch)\n2016-07-23 15:58:21.894001: step 3590, loss = 2.14 (31.5 examples/sec; 8.129 sec/batch)", "body": "I'm facing the same problem. I'm running the inception model fine-tuning on my data. Here's a part of the log being printed. I'm using all TensorFlow data read operations (so no copy from numpy to TF, as I'm not using `DequeueMany` manually, but through `shuffle_batch_join`). I'll try to provide the timeline when I have a chance to re-run the training. The machine resources otherwise seem fine (CPU/memory is not choked, the prefetch queues logs show >70% full) \n\n``` txt\n2016-07-23 14:34:15.784645: step 2720, loss = 2.63 (68.5 examples/sec; 3.740 sec/batch)                                                                                                                                                                                                                                                                         [224/1046]\n2016-07-23 14:34:54.449992: step 2730, loss = 2.23 (65.7 examples/sec; 3.897 sec/batch)\n2016-07-23 14:35:30.115364: step 2740, loss = 2.11 (66.2 examples/sec; 3.867 sec/batch)\n2016-07-23 14:36:06.048820: step 2750, loss = 2.14 (64.8 examples/sec; 3.953 sec/batch)\n2016-07-23 14:36:43.302534: step 2760, loss = 2.18 (71.4 examples/sec; 3.585 sec/batch)\n2016-07-23 14:37:21.960443: step 2770, loss = 2.26 (74.2 examples/sec; 3.451 sec/batch)\n2016-07-23 14:37:59.039025: step 2780, loss = 2.04 (67.2 examples/sec; 3.808 sec/batch)\n2016-07-23 14:38:39.144252: step 2790, loss = 2.45 (59.2 examples/sec; 4.321 sec/batch)\n2016-07-23 14:39:17.613073: step 2800, loss = 2.18 (81.0 examples/sec; 3.162 sec/batch)\n2016-07-23 14:39:56.705607: step 2810, loss = 2.55 (66.5 examples/sec; 3.847 sec/batch)\n2016-07-23 14:40:32.612656: step 2820, loss = 2.38 (78.8 examples/sec; 3.247 sec/batch)\n2016-07-23 14:41:09.194193: step 2830, loss = 2.47 (67.1 examples/sec; 3.814 sec/batch)\n2016-07-23 14:41:44.523209: step 2840, loss = 2.30 (77.5 examples/sec; 3.303 sec/batch)\n2016-07-23 14:42:21.437528: step 2850, loss = 2.27 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:42:55.936083: step 2860, loss = 2.22 (80.3 examples/sec; 3.189 sec/batch)\n2016-07-23 14:43:32.151591: step 2870, loss = 2.35 (60.1 examples/sec; 4.259 sec/batch)\n2016-07-23 14:44:07.513355: step 2880, loss = 2.38 (65.1 examples/sec; 3.931 sec/batch)\n2016-07-23 14:44:42.496976: step 2890, loss = 1.99 (67.7 examples/sec; 3.781 sec/batch)\n2016-07-23 14:45:20.375483: step 2900, loss = 2.67 (50.2 examples/sec; 5.100 sec/batch)\n2016-07-23 14:46:00.563096: step 2910, loss = 2.23 (62.5 examples/sec; 4.093 sec/batch)\n2016-07-23 14:46:39.610138: step 2920, loss = 2.19 (80.1 examples/sec; 3.197 sec/batch)\n2016-07-23 14:47:19.147337: step 2930, loss = 2.31 (51.6 examples/sec; 4.957 sec/batch)\n2016-07-23 14:48:03.449487: step 2940, loss = 2.27 (50.8 examples/sec; 5.041 sec/batch)\n2016-07-23 14:48:47.893646: step 2950, loss = 2.29 (52.0 examples/sec; 4.919 sec/batch)\n2016-07-23 14:49:27.023626: step 2960, loss = 2.27 (73.7 examples/sec; 3.475 sec/batch)\n2016-07-23 14:50:11.125284: step 2970, loss = 2.08 (65.4 examples/sec; 3.914 sec/batch)\n2016-07-23 14:50:51.295322: step 2980, loss = 2.19 (50.7 examples/sec; 5.053 sec/batch)\n2016-07-23 14:51:33.655334: step 2990, loss = 2.30 (66.0 examples/sec; 3.879 sec/batch)\n2016-07-23 14:52:10.323608: step 3000, loss = 2.25 (68.8 examples/sec; 3.723 sec/batch)\n2016-07-23 14:52:52.609913: step 3010, loss = 2.04 (64.8 examples/sec; 3.949 sec/batch)\n2016-07-23 14:53:31.631134: step 3020, loss = 2.31 (54.5 examples/sec; 4.696 sec/batch)\n2016-07-23 14:54:13.547641: step 3030, loss = 2.33 (55.1 examples/sec; 4.643 sec/batch)\n2016-07-23 14:55:00.528403: step 3040, loss = 2.29 (49.1 examples/sec; 5.217 sec/batch)\n2016-07-23 14:55:45.258737: step 3050, loss = 2.17 (51.4 examples/sec; 4.979 sec/batch)\n2016-07-23 14:56:31.159313: step 3060, loss = 2.29 (51.5 examples/sec; 4.968 sec/batch)\n2016-07-23 14:57:15.148195: step 3070, loss = 2.27 (51.8 examples/sec; 4.946 sec/batch)\n2016-07-23 14:57:54.451044: step 3080, loss = 2.40 (68.8 examples/sec; 3.724 sec/batch)\n2016-07-23 14:58:32.325357: step 3090, loss = 1.93 (58.4 examples/sec; 4.380 sec/batch)\n2016-07-23 14:59:06.746820: step 3100, loss = 2.02 (88.0 examples/sec; 2.910 sec/batch)\n2016-07-23 14:59:39.746600: step 3110, loss = 2.20 (86.2 examples/sec; 2.970 sec/batch)\n2016-07-23 15:00:10.409635: step 3120, loss = 2.19 (79.7 examples/sec; 3.213 sec/batch)\n2016-07-23 15:00:41.756720: step 3130, loss = 2.28 (83.6 examples/sec; 3.062 sec/batch)\n2016-07-23 15:01:12.757209: step 3140, loss = 2.26 (85.6 examples/sec; 2.989 sec/batch)\n2016-07-23 15:01:43.993791: step 3150, loss = 2.23 (79.6 examples/sec; 3.214 sec/batch)\n2016-07-23 15:02:15.348264: step 3160, loss = 2.04 (82.8 examples/sec; 3.090 sec/batch)\n2016-07-23 15:02:46.752311: step 3170, loss = 2.26 (83.7 examples/sec; 3.058 sec/batch)\n2016-07-23 15:03:18.581683: step 3180, loss = 1.92 (78.9 examples/sec; 3.243 sec/batch)\n2016-07-23 15:03:50.599771: step 3190, loss = 2.38 (80.9 examples/sec; 3.166 sec/batch)\n2016-07-23 15:04:22.723497: step 3200, loss = 2.00 (78.4 examples/sec; 3.266 sec/batch)\n2016-07-23 15:05:16.565673: step 3210, loss = 1.99 (25.1 examples/sec; 10.188 sec/batch)\n2016-07-23 15:06:37.336032: step 3220, loss = 2.34 (28.4 examples/sec; 9.013 sec/batch)\n2016-07-23 15:08:03.531096: step 3230, loss = 2.10 (26.1 examples/sec; 9.817 sec/batch)\n2016-07-23 15:09:26.177609: step 3240, loss = 2.34 (29.7 examples/sec; 8.613 sec/batch)\n2016-07-23 15:10:48.697512: step 3250, loss = 2.06 (25.0 examples/sec; 10.229 sec/batch)\n2016-07-23 15:12:14.720998: step 3260, loss = 2.16 (26.0 examples/sec; 9.861 sec/batch)\n2016-07-23 15:13:42.700315: step 3270, loss = 2.06 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:15:07.717132: step 3280, loss = 2.00 (26.9 examples/sec; 9.524 sec/batch)\n2016-07-23 15:16:29.206475: step 3290, loss = 2.15 (71.1 examples/sec; 3.600 sec/batch)\n2016-07-23 15:17:56.483761: step 3300, loss = 2.17 (25.3 examples/sec; 10.102 sec/batch)\n2016-07-23 15:19:32.355597: step 3310, loss = 2.26 (24.7 examples/sec; 10.352 sec/batch)\n2016-07-23 15:20:58.064109: step 3320, loss = 2.18 (25.8 examples/sec; 9.907 sec/batch)\n2016-07-23 15:22:18.608871: step 3330, loss = 2.02 (27.9 examples/sec; 9.183 sec/batch)\n2016-07-23 15:23:39.227969: step 3340, loss = 2.17 (26.8 examples/sec; 9.567 sec/batch)\n2016-07-23 15:25:03.643470: step 3350, loss = 2.01 (25.7 examples/sec; 9.976 sec/batch)\n2016-07-23 15:26:25.196536: step 3360, loss = 2.08 (33.1 examples/sec; 7.728 sec/batch)\n2016-07-23 15:27:50.458838: step 3370, loss = 2.01 (26.9 examples/sec; 9.533 sec/batch)\n2016-07-23 15:29:12.861687: step 3380, loss = 2.07 (66.9 examples/sec; 3.827 sec/batch)\n2016-07-23 15:30:31.386601: step 3390, loss = 1.95 (71.9 examples/sec; 3.559 sec/batch)\n2016-07-23 15:31:58.734378: step 3400, loss = 2.37 (25.6 examples/sec; 10.011 sec/batch)\n2016-07-23 15:33:33.894047: step 3410, loss = 2.06 (26.7 examples/sec; 9.581 sec/batch)\n2016-07-23 15:34:54.087397: step 3420, loss = 1.99 (27.5 examples/sec; 9.311 sec/batch)\n2016-07-23 15:36:11.707716: step 3430, loss = 2.15 (69.0 examples/sec; 3.710 sec/batch)\n2016-07-23 15:37:31.881528: step 3440, loss = 2.20 (46.0 examples/sec; 5.570 sec/batch)\n2016-07-23 15:38:55.509458: step 3450, loss = 2.08 (25.5 examples/sec; 10.041 sec/batch)\n2016-07-23 15:40:16.870401: step 3460, loss = 2.35 (26.5 examples/sec; 9.647 sec/batch)\n2016-07-23 15:41:40.889863: step 3470, loss = 2.23 (27.3 examples/sec; 9.389 sec/batch)\n2016-07-23 15:43:06.156092: step 3480, loss = 2.08 (27.3 examples/sec; 9.385 sec/batch)\n2016-07-23 15:44:25.849125: step 3490, loss = 2.04 (65.0 examples/sec; 3.939 sec/batch)\n2016-07-23 15:45:46.423841: step 3500, loss = 2.04 (55.5 examples/sec; 4.616 sec/batch)\n2016-07-23 15:47:24.597796: step 3510, loss = 2.13 (25.8 examples/sec; 9.938 sec/batch)\n2016-07-23 15:48:50.303574: step 3520, loss = 1.99 (25.7 examples/sec; 9.968 sec/batch)\n2016-07-23 15:50:13.826881: step 3530, loss = 2.03 (31.7 examples/sec; 8.069 sec/batch)\n2016-07-23 15:51:27.094435: step 3540, loss = 1.93 (63.6 examples/sec; 4.026 sec/batch)\n2016-07-23 15:52:46.522184: step 3550, loss = 2.13 (70.3 examples/sec; 3.639 sec/batch)\n2016-07-23 15:54:10.492599: step 3560, loss = 2.17 (25.6 examples/sec; 10.000 sec/batch)\n2016-07-23 15:55:32.567000: step 3570, loss = 1.93 (27.0 examples/sec; 9.465 sec/batch)\n2016-07-23 15:56:58.073701: step 3580, loss = 2.21 (25.5 examples/sec; 10.058 sec/batch)\n2016-07-23 15:58:21.894001: step 3590, loss = 2.14 (31.5 examples/sec; 8.129 sec/batch)\n```\n"}