{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2987", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2987/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2987/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2987/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2987", "id": 161581993, "node_id": "MDU6SXNzdWUxNjE1ODE5OTM=", "number": 2987, "title": "Random slow downs during training.", "user": {"login": "chasep255", "id": 15787797, "node_id": "MDQ6VXNlcjE1Nzg3Nzk3", "avatar_url": "https://avatars3.githubusercontent.com/u/15787797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chasep255", "html_url": "https://github.com/chasep255", "followers_url": "https://api.github.com/users/chasep255/followers", "following_url": "https://api.github.com/users/chasep255/following{/other_user}", "gists_url": "https://api.github.com/users/chasep255/gists{/gist_id}", "starred_url": "https://api.github.com/users/chasep255/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chasep255/subscriptions", "organizations_url": "https://api.github.com/users/chasep255/orgs", "repos_url": "https://api.github.com/users/chasep255/repos", "events_url": "https://api.github.com/users/chasep255/events{/privacy}", "received_events_url": "https://api.github.com/users/chasep255/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2016-06-22T02:39:52Z", "updated_at": "2018-01-18T17:35:45Z", "closed_at": "2016-06-28T16:57:02Z", "author_association": "NONE", "body_html": "<p>Usually the speed of each training batch is somewhat consistent for me when using tensorflow.  However, I am currently training an RNN.  For the first 3 epochs each batch of 128 samples took about 1 second.  Now on the third epoch my times have become inconsistent.  The batches will randomly become very slow.  I can hear the fan on my GTX Titian spin down indicating that during these slowdowns it is not using the compute resources on the GPU.</p>\n<p>I am timing it like this so I don't think anything else in my code could be affecting it.</p>\n<pre><code>            begin_time = time.time()\n            loss, ts = sess.run([cost, train_step], feed_dict = {input_tensor: x_train, expected_output: y_train, keep_prob: 0.8})\n            end_time = time.time()\n</code></pre>\n<p>Here is the output with timing information.</p>\n<pre><code>Epoch 3        Batch 858      Loss 172.072438250   Last Loss 206.985626221   Time 1.432 \nEpoch 3        Batch 859      Loss 172.067967827   Last Loss 168.227874756   Time 1.419 \nEpoch 3        Batch 860      Loss 172.057925642   Last Loss 163.421646118   Time 1.447 \nEpoch 3        Batch 861      Loss 172.056937587   Last Loss 171.206222534   Time 1.339 \nEpoch 3        Batch 862      Loss 172.051488565   Last Loss 167.354431152   Time 1.285 \nEpoch 3        Batch 863      Loss 172.016926642   Last Loss 142.189987183   Time 1.310 \nEpoch 3        Batch 864      Loss 172.011091517   Last Loss 166.969543457   Time 1.291 \nEpoch 3        Batch 865      Loss 172.011779468   Last Loss 172.606857300   Time 1.317 \nEpoch 3        Batch 866      Loss 172.004100742   Last Loss 165.354324341   Time 1.307 \nEpoch 3        Batch 867      Loss 172.008801860   Last Loss 176.084671021   Time 1.372 \nEpoch 3        Batch 868      Loss 172.032961320   Last Loss 193.003372192   Time 1.298 \nEpoch 3        Batch 869      Loss 172.036121868   Last Loss 174.782638550   Time 1.310 \nEpoch 3        Batch 870      Loss 172.044807513   Last Loss 179.601318359   Time 1.429 \nEpoch 3        Batch 871      Loss 172.066208551   Last Loss 190.706512451   Time 1.311 \nEpoch 3        Batch 872      Loss 172.052568940   Last Loss 160.158828735   Time 2.614 \nEpoch 3        Batch 873      Loss 172.032694941   Last Loss 154.682693481   Time 2.208 \nEpoch 3        Batch 874      Loss 172.040674805   Last Loss 179.015075684   Time 3.138 \nEpoch 3        Batch 875      Loss 172.034015673   Last Loss 166.207275391   Time 1.750 \nEpoch 3        Batch 876      Loss 172.029174909   Last Loss 167.788665771   Time 1.955 \nEpoch 3        Batch 877      Loss 172.042762183   Last Loss 183.958801270   Time 2.576 \nEpoch 3        Batch 878      Loss 172.028727179   Last Loss 159.705993652   Time 3.130 \nEpoch 3        Batch 879      Loss 172.035255710   Last Loss 177.773834229   Time 2.622 \nEpoch 3        Batch 880      Loss 172.043415273   Last Loss 179.223831177   Time 1.581 \nEpoch 3        Batch 881      Loss 172.040243793   Last Loss 169.246170044   Time 3.093 \nEpoch 3        Batch 882      Loss 172.017778805   Last Loss 152.203659058   Time 2.470 \nEpoch 3        Batch 883      Loss 172.018503957   Last Loss 172.658813477   Time 2.540 \nEpoch 3        Batch 884      Loss 172.054275064   Last Loss 203.675933838   Time 2.597 \nEpoch 3        Batch 885      Loss 172.029123448   Last Loss 149.769943237   Time 2.915 \nEpoch 3        Batch 886      Loss 172.013957027   Last Loss 158.576507568   Time 3.095 \nEpoch 3        Batch 887      Loss 171.994215424   Last Loss 154.483413696   Time 2.250 \nEpoch 3        Batch 888      Loss 171.998252946   Last Loss 175.583572388   Time 2.997 \nEpoch 3        Batch 889      Loss 171.974352959   Last Loss 150.727264404   Time 3.417 \nEpoch 3        Batch 890      Loss 171.968166587   Last Loss 166.462295532   Time 2.290 \nEpoch 3        Batch 891      Loss 172.011782095   Last Loss 210.873199463   Time 1.358 \nEpoch 3        Batch 892      Loss 172.013166695   Last Loss 173.248229980   Time 0.910 \nEpoch 3        Batch 893      Loss 172.016952293   Last Loss 175.397491455   Time 0.893 \nEpoch 3        Batch 894      Loss 172.015184030   Last Loss 170.434356689   Time 0.986 \nEpoch 3        Batch 895      Loss 172.001527531   Last Loss 159.778961182   Time 1.000 \nEpoch 3        Batch 896      Loss 172.008338426   Last Loss 178.110900879   Time 0.999 \nEpoch 3        Batch 897      Loss 172.015577083   Last Loss 178.508651733   Time 1.699 \nEpoch 3        Batch 898      Loss 172.076713689   Last Loss 226.977386475   Time 1.570 \nEpoch 3        Batch 899      Loss 172.043461711   Last Loss 142.149932861   Time 1.699 \nEpoch 3        Batch 900      Loss 172.069320933   Last Loss 195.342620850   Time 1.678 \nEpoch 3        Batch 901      Loss 172.063626562   Last Loss 166.932998657   Time 1.685 \nEpoch 3        Batch 902      Loss 172.060249506   Last Loss 169.014144897   Time 1.467 \nEpoch 3        Batch 903      Loss 172.068654811   Last Loss 179.658645630   Time 3.737 \nEpoch 3        Batch 904      Loss 172.061790508   Last Loss 165.856460571   Time 3.705 \nEpoch 3        Batch 905      Loss 172.078748707   Last Loss 187.425918579   Time 3.376 \nEpoch 3        Batch 906      Loss 172.065538518   Last Loss 160.097106934   Time 4.606 \nEpoch 3        Batch 907      Loss 172.040349112   Last Loss 149.193557739   Time 4.513 \nEpoch 3        Batch 908      Loss 172.078197416   Last Loss 206.444458008   Time 2.491 \nEpoch 3        Batch 909      Loss 172.081030156   Last Loss 174.655990601   Time 4.244 \n</code></pre>\n<p>Here is what the output normally looks like most of the time when this issue is not occurring.</p>\n<pre><code>Epoch 1        Batch 213      Loss 262.820208541   Last Loss 189.761398315   Time 0.978 \nEpoch 1        Batch 214      Loss 262.570083973   Last Loss 209.043426514   Time 0.984 \nEpoch 1        Batch 215      Loss 262.265294534   Last Loss 196.735565186   Time 0.985 \nEpoch 1        Batch 216      Loss 261.973181588   Last Loss 198.876785278   Time 0.988 \nEpoch 1        Batch 217      Loss 261.685996729   Last Loss 199.366882324   Time 0.989 \nEpoch 1        Batch 218      Loss 261.385472058   Last Loss 195.871093750   Time 0.981 \nEpoch 1        Batch 219      Loss 261.200773135   Last Loss 220.751708984   Time 0.983 \nEpoch 1        Batch 220      Loss 260.912834202   Last Loss 197.566268921   Time 0.982 \nEpoch 1        Batch 221      Loss 260.757691254   Last Loss 226.471099854   Time 0.981 \nEpoch 1        Batch 222      Loss 260.588221186   Last Loss 222.965866089   Time 0.984 \nEpoch 1        Batch 223      Loss 260.272108146   Last Loss 189.778900146   Time 0.992 \nEpoch 1        Batch 224      Loss 259.962260607   Last Loss 190.556411743   Time 0.981 \nEpoch 1        Batch 225      Loss 259.509933337   Last Loss 157.736297607   Time 0.985 \nEpoch 1        Batch 226      Loss 259.273305347   Last Loss 205.795379639   Time 0.982 \nEpoch 1        Batch 227      Loss 259.062388972   Last Loss 211.184371948   Time 0.991 \nEpoch 1        Batch 228      Loss 258.901376783   Last Loss 222.190597534   Time 0.984 \nEpoch 1        Batch 229      Loss 258.643836975   Last Loss 199.667221069   Time 0.986 \nEpoch 1        Batch 230      Loss 258.344224079   Last Loss 189.433258057   Time 0.979 \n\n</code></pre>\n<p>After a while it will make it through the slow phase and return to being 1s per batch.  I am using Ubuntu 16.04 with CUDNN 5 and Cuda 8.  This problem is intermittent and I have no real way to reproduce it.</p>", "body_text": "Usually the speed of each training batch is somewhat consistent for me when using tensorflow.  However, I am currently training an RNN.  For the first 3 epochs each batch of 128 samples took about 1 second.  Now on the third epoch my times have become inconsistent.  The batches will randomly become very slow.  I can hear the fan on my GTX Titian spin down indicating that during these slowdowns it is not using the compute resources on the GPU.\nI am timing it like this so I don't think anything else in my code could be affecting it.\n            begin_time = time.time()\n            loss, ts = sess.run([cost, train_step], feed_dict = {input_tensor: x_train, expected_output: y_train, keep_prob: 0.8})\n            end_time = time.time()\n\nHere is the output with timing information.\nEpoch 3        Batch 858      Loss 172.072438250   Last Loss 206.985626221   Time 1.432 \nEpoch 3        Batch 859      Loss 172.067967827   Last Loss 168.227874756   Time 1.419 \nEpoch 3        Batch 860      Loss 172.057925642   Last Loss 163.421646118   Time 1.447 \nEpoch 3        Batch 861      Loss 172.056937587   Last Loss 171.206222534   Time 1.339 \nEpoch 3        Batch 862      Loss 172.051488565   Last Loss 167.354431152   Time 1.285 \nEpoch 3        Batch 863      Loss 172.016926642   Last Loss 142.189987183   Time 1.310 \nEpoch 3        Batch 864      Loss 172.011091517   Last Loss 166.969543457   Time 1.291 \nEpoch 3        Batch 865      Loss 172.011779468   Last Loss 172.606857300   Time 1.317 \nEpoch 3        Batch 866      Loss 172.004100742   Last Loss 165.354324341   Time 1.307 \nEpoch 3        Batch 867      Loss 172.008801860   Last Loss 176.084671021   Time 1.372 \nEpoch 3        Batch 868      Loss 172.032961320   Last Loss 193.003372192   Time 1.298 \nEpoch 3        Batch 869      Loss 172.036121868   Last Loss 174.782638550   Time 1.310 \nEpoch 3        Batch 870      Loss 172.044807513   Last Loss 179.601318359   Time 1.429 \nEpoch 3        Batch 871      Loss 172.066208551   Last Loss 190.706512451   Time 1.311 \nEpoch 3        Batch 872      Loss 172.052568940   Last Loss 160.158828735   Time 2.614 \nEpoch 3        Batch 873      Loss 172.032694941   Last Loss 154.682693481   Time 2.208 \nEpoch 3        Batch 874      Loss 172.040674805   Last Loss 179.015075684   Time 3.138 \nEpoch 3        Batch 875      Loss 172.034015673   Last Loss 166.207275391   Time 1.750 \nEpoch 3        Batch 876      Loss 172.029174909   Last Loss 167.788665771   Time 1.955 \nEpoch 3        Batch 877      Loss 172.042762183   Last Loss 183.958801270   Time 2.576 \nEpoch 3        Batch 878      Loss 172.028727179   Last Loss 159.705993652   Time 3.130 \nEpoch 3        Batch 879      Loss 172.035255710   Last Loss 177.773834229   Time 2.622 \nEpoch 3        Batch 880      Loss 172.043415273   Last Loss 179.223831177   Time 1.581 \nEpoch 3        Batch 881      Loss 172.040243793   Last Loss 169.246170044   Time 3.093 \nEpoch 3        Batch 882      Loss 172.017778805   Last Loss 152.203659058   Time 2.470 \nEpoch 3        Batch 883      Loss 172.018503957   Last Loss 172.658813477   Time 2.540 \nEpoch 3        Batch 884      Loss 172.054275064   Last Loss 203.675933838   Time 2.597 \nEpoch 3        Batch 885      Loss 172.029123448   Last Loss 149.769943237   Time 2.915 \nEpoch 3        Batch 886      Loss 172.013957027   Last Loss 158.576507568   Time 3.095 \nEpoch 3        Batch 887      Loss 171.994215424   Last Loss 154.483413696   Time 2.250 \nEpoch 3        Batch 888      Loss 171.998252946   Last Loss 175.583572388   Time 2.997 \nEpoch 3        Batch 889      Loss 171.974352959   Last Loss 150.727264404   Time 3.417 \nEpoch 3        Batch 890      Loss 171.968166587   Last Loss 166.462295532   Time 2.290 \nEpoch 3        Batch 891      Loss 172.011782095   Last Loss 210.873199463   Time 1.358 \nEpoch 3        Batch 892      Loss 172.013166695   Last Loss 173.248229980   Time 0.910 \nEpoch 3        Batch 893      Loss 172.016952293   Last Loss 175.397491455   Time 0.893 \nEpoch 3        Batch 894      Loss 172.015184030   Last Loss 170.434356689   Time 0.986 \nEpoch 3        Batch 895      Loss 172.001527531   Last Loss 159.778961182   Time 1.000 \nEpoch 3        Batch 896      Loss 172.008338426   Last Loss 178.110900879   Time 0.999 \nEpoch 3        Batch 897      Loss 172.015577083   Last Loss 178.508651733   Time 1.699 \nEpoch 3        Batch 898      Loss 172.076713689   Last Loss 226.977386475   Time 1.570 \nEpoch 3        Batch 899      Loss 172.043461711   Last Loss 142.149932861   Time 1.699 \nEpoch 3        Batch 900      Loss 172.069320933   Last Loss 195.342620850   Time 1.678 \nEpoch 3        Batch 901      Loss 172.063626562   Last Loss 166.932998657   Time 1.685 \nEpoch 3        Batch 902      Loss 172.060249506   Last Loss 169.014144897   Time 1.467 \nEpoch 3        Batch 903      Loss 172.068654811   Last Loss 179.658645630   Time 3.737 \nEpoch 3        Batch 904      Loss 172.061790508   Last Loss 165.856460571   Time 3.705 \nEpoch 3        Batch 905      Loss 172.078748707   Last Loss 187.425918579   Time 3.376 \nEpoch 3        Batch 906      Loss 172.065538518   Last Loss 160.097106934   Time 4.606 \nEpoch 3        Batch 907      Loss 172.040349112   Last Loss 149.193557739   Time 4.513 \nEpoch 3        Batch 908      Loss 172.078197416   Last Loss 206.444458008   Time 2.491 \nEpoch 3        Batch 909      Loss 172.081030156   Last Loss 174.655990601   Time 4.244 \n\nHere is what the output normally looks like most of the time when this issue is not occurring.\nEpoch 1        Batch 213      Loss 262.820208541   Last Loss 189.761398315   Time 0.978 \nEpoch 1        Batch 214      Loss 262.570083973   Last Loss 209.043426514   Time 0.984 \nEpoch 1        Batch 215      Loss 262.265294534   Last Loss 196.735565186   Time 0.985 \nEpoch 1        Batch 216      Loss 261.973181588   Last Loss 198.876785278   Time 0.988 \nEpoch 1        Batch 217      Loss 261.685996729   Last Loss 199.366882324   Time 0.989 \nEpoch 1        Batch 218      Loss 261.385472058   Last Loss 195.871093750   Time 0.981 \nEpoch 1        Batch 219      Loss 261.200773135   Last Loss 220.751708984   Time 0.983 \nEpoch 1        Batch 220      Loss 260.912834202   Last Loss 197.566268921   Time 0.982 \nEpoch 1        Batch 221      Loss 260.757691254   Last Loss 226.471099854   Time 0.981 \nEpoch 1        Batch 222      Loss 260.588221186   Last Loss 222.965866089   Time 0.984 \nEpoch 1        Batch 223      Loss 260.272108146   Last Loss 189.778900146   Time 0.992 \nEpoch 1        Batch 224      Loss 259.962260607   Last Loss 190.556411743   Time 0.981 \nEpoch 1        Batch 225      Loss 259.509933337   Last Loss 157.736297607   Time 0.985 \nEpoch 1        Batch 226      Loss 259.273305347   Last Loss 205.795379639   Time 0.982 \nEpoch 1        Batch 227      Loss 259.062388972   Last Loss 211.184371948   Time 0.991 \nEpoch 1        Batch 228      Loss 258.901376783   Last Loss 222.190597534   Time 0.984 \nEpoch 1        Batch 229      Loss 258.643836975   Last Loss 199.667221069   Time 0.986 \nEpoch 1        Batch 230      Loss 258.344224079   Last Loss 189.433258057   Time 0.979 \n\n\nAfter a while it will make it through the slow phase and return to being 1s per batch.  I am using Ubuntu 16.04 with CUDNN 5 and Cuda 8.  This problem is intermittent and I have no real way to reproduce it.", "body": "Usually the speed of each training batch is somewhat consistent for me when using tensorflow.  However, I am currently training an RNN.  For the first 3 epochs each batch of 128 samples took about 1 second.  Now on the third epoch my times have become inconsistent.  The batches will randomly become very slow.  I can hear the fan on my GTX Titian spin down indicating that during these slowdowns it is not using the compute resources on the GPU.  \n\nI am timing it like this so I don't think anything else in my code could be affecting it.\n\n```\n            begin_time = time.time()\n            loss, ts = sess.run([cost, train_step], feed_dict = {input_tensor: x_train, expected_output: y_train, keep_prob: 0.8})\n            end_time = time.time()\n```\n\nHere is the output with timing information.\n\n```\nEpoch 3        Batch 858      Loss 172.072438250   Last Loss 206.985626221   Time 1.432 \nEpoch 3        Batch 859      Loss 172.067967827   Last Loss 168.227874756   Time 1.419 \nEpoch 3        Batch 860      Loss 172.057925642   Last Loss 163.421646118   Time 1.447 \nEpoch 3        Batch 861      Loss 172.056937587   Last Loss 171.206222534   Time 1.339 \nEpoch 3        Batch 862      Loss 172.051488565   Last Loss 167.354431152   Time 1.285 \nEpoch 3        Batch 863      Loss 172.016926642   Last Loss 142.189987183   Time 1.310 \nEpoch 3        Batch 864      Loss 172.011091517   Last Loss 166.969543457   Time 1.291 \nEpoch 3        Batch 865      Loss 172.011779468   Last Loss 172.606857300   Time 1.317 \nEpoch 3        Batch 866      Loss 172.004100742   Last Loss 165.354324341   Time 1.307 \nEpoch 3        Batch 867      Loss 172.008801860   Last Loss 176.084671021   Time 1.372 \nEpoch 3        Batch 868      Loss 172.032961320   Last Loss 193.003372192   Time 1.298 \nEpoch 3        Batch 869      Loss 172.036121868   Last Loss 174.782638550   Time 1.310 \nEpoch 3        Batch 870      Loss 172.044807513   Last Loss 179.601318359   Time 1.429 \nEpoch 3        Batch 871      Loss 172.066208551   Last Loss 190.706512451   Time 1.311 \nEpoch 3        Batch 872      Loss 172.052568940   Last Loss 160.158828735   Time 2.614 \nEpoch 3        Batch 873      Loss 172.032694941   Last Loss 154.682693481   Time 2.208 \nEpoch 3        Batch 874      Loss 172.040674805   Last Loss 179.015075684   Time 3.138 \nEpoch 3        Batch 875      Loss 172.034015673   Last Loss 166.207275391   Time 1.750 \nEpoch 3        Batch 876      Loss 172.029174909   Last Loss 167.788665771   Time 1.955 \nEpoch 3        Batch 877      Loss 172.042762183   Last Loss 183.958801270   Time 2.576 \nEpoch 3        Batch 878      Loss 172.028727179   Last Loss 159.705993652   Time 3.130 \nEpoch 3        Batch 879      Loss 172.035255710   Last Loss 177.773834229   Time 2.622 \nEpoch 3        Batch 880      Loss 172.043415273   Last Loss 179.223831177   Time 1.581 \nEpoch 3        Batch 881      Loss 172.040243793   Last Loss 169.246170044   Time 3.093 \nEpoch 3        Batch 882      Loss 172.017778805   Last Loss 152.203659058   Time 2.470 \nEpoch 3        Batch 883      Loss 172.018503957   Last Loss 172.658813477   Time 2.540 \nEpoch 3        Batch 884      Loss 172.054275064   Last Loss 203.675933838   Time 2.597 \nEpoch 3        Batch 885      Loss 172.029123448   Last Loss 149.769943237   Time 2.915 \nEpoch 3        Batch 886      Loss 172.013957027   Last Loss 158.576507568   Time 3.095 \nEpoch 3        Batch 887      Loss 171.994215424   Last Loss 154.483413696   Time 2.250 \nEpoch 3        Batch 888      Loss 171.998252946   Last Loss 175.583572388   Time 2.997 \nEpoch 3        Batch 889      Loss 171.974352959   Last Loss 150.727264404   Time 3.417 \nEpoch 3        Batch 890      Loss 171.968166587   Last Loss 166.462295532   Time 2.290 \nEpoch 3        Batch 891      Loss 172.011782095   Last Loss 210.873199463   Time 1.358 \nEpoch 3        Batch 892      Loss 172.013166695   Last Loss 173.248229980   Time 0.910 \nEpoch 3        Batch 893      Loss 172.016952293   Last Loss 175.397491455   Time 0.893 \nEpoch 3        Batch 894      Loss 172.015184030   Last Loss 170.434356689   Time 0.986 \nEpoch 3        Batch 895      Loss 172.001527531   Last Loss 159.778961182   Time 1.000 \nEpoch 3        Batch 896      Loss 172.008338426   Last Loss 178.110900879   Time 0.999 \nEpoch 3        Batch 897      Loss 172.015577083   Last Loss 178.508651733   Time 1.699 \nEpoch 3        Batch 898      Loss 172.076713689   Last Loss 226.977386475   Time 1.570 \nEpoch 3        Batch 899      Loss 172.043461711   Last Loss 142.149932861   Time 1.699 \nEpoch 3        Batch 900      Loss 172.069320933   Last Loss 195.342620850   Time 1.678 \nEpoch 3        Batch 901      Loss 172.063626562   Last Loss 166.932998657   Time 1.685 \nEpoch 3        Batch 902      Loss 172.060249506   Last Loss 169.014144897   Time 1.467 \nEpoch 3        Batch 903      Loss 172.068654811   Last Loss 179.658645630   Time 3.737 \nEpoch 3        Batch 904      Loss 172.061790508   Last Loss 165.856460571   Time 3.705 \nEpoch 3        Batch 905      Loss 172.078748707   Last Loss 187.425918579   Time 3.376 \nEpoch 3        Batch 906      Loss 172.065538518   Last Loss 160.097106934   Time 4.606 \nEpoch 3        Batch 907      Loss 172.040349112   Last Loss 149.193557739   Time 4.513 \nEpoch 3        Batch 908      Loss 172.078197416   Last Loss 206.444458008   Time 2.491 \nEpoch 3        Batch 909      Loss 172.081030156   Last Loss 174.655990601   Time 4.244 \n```\n\nHere is what the output normally looks like most of the time when this issue is not occurring.\n\n```\nEpoch 1        Batch 213      Loss 262.820208541   Last Loss 189.761398315   Time 0.978 \nEpoch 1        Batch 214      Loss 262.570083973   Last Loss 209.043426514   Time 0.984 \nEpoch 1        Batch 215      Loss 262.265294534   Last Loss 196.735565186   Time 0.985 \nEpoch 1        Batch 216      Loss 261.973181588   Last Loss 198.876785278   Time 0.988 \nEpoch 1        Batch 217      Loss 261.685996729   Last Loss 199.366882324   Time 0.989 \nEpoch 1        Batch 218      Loss 261.385472058   Last Loss 195.871093750   Time 0.981 \nEpoch 1        Batch 219      Loss 261.200773135   Last Loss 220.751708984   Time 0.983 \nEpoch 1        Batch 220      Loss 260.912834202   Last Loss 197.566268921   Time 0.982 \nEpoch 1        Batch 221      Loss 260.757691254   Last Loss 226.471099854   Time 0.981 \nEpoch 1        Batch 222      Loss 260.588221186   Last Loss 222.965866089   Time 0.984 \nEpoch 1        Batch 223      Loss 260.272108146   Last Loss 189.778900146   Time 0.992 \nEpoch 1        Batch 224      Loss 259.962260607   Last Loss 190.556411743   Time 0.981 \nEpoch 1        Batch 225      Loss 259.509933337   Last Loss 157.736297607   Time 0.985 \nEpoch 1        Batch 226      Loss 259.273305347   Last Loss 205.795379639   Time 0.982 \nEpoch 1        Batch 227      Loss 259.062388972   Last Loss 211.184371948   Time 0.991 \nEpoch 1        Batch 228      Loss 258.901376783   Last Loss 222.190597534   Time 0.984 \nEpoch 1        Batch 229      Loss 258.643836975   Last Loss 199.667221069   Time 0.986 \nEpoch 1        Batch 230      Loss 258.344224079   Last Loss 189.433258057   Time 0.979 \n\n```\n\nAfter a while it will make it through the slow phase and return to being 1s per batch.  I am using Ubuntu 16.04 with CUDNN 5 and Cuda 8.  This problem is intermittent and I have no real way to reproduce it.\n"}