{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421784247", "html_url": "https://github.com/tensorflow/tensorflow/issues/22088#issuecomment-421784247", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22088", "id": 421784247, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTc4NDI0Nw==", "user": {"login": "jrabary", "id": 1025387, "node_id": "MDQ6VXNlcjEwMjUzODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1025387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrabary", "html_url": "https://github.com/jrabary", "followers_url": "https://api.github.com/users/jrabary/followers", "following_url": "https://api.github.com/users/jrabary/following{/other_user}", "gists_url": "https://api.github.com/users/jrabary/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrabary/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrabary/subscriptions", "organizations_url": "https://api.github.com/users/jrabary/orgs", "repos_url": "https://api.github.com/users/jrabary/repos", "events_url": "https://api.github.com/users/jrabary/events{/privacy}", "received_events_url": "https://api.github.com/users/jrabary/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-16T15:38:36Z", "updated_at": "2018-09-16T15:38:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=14104855\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/guptapriya\">@guptapriya</a> here is an example of code to reproduce this. Tested on Nvidia Titan X. The model is big enough to completely fill the memory of a single GPU but it still works in one device strategy.<br>\nYou get the problem when you change the number of GPU to 2 for example.</p>\n<pre><code>import tensorflow as tf\nlayers = tf.keras.layers\n\nclass MyModel(tf.keras.Model):\n    \"\"\"\n    Simple CNN model.\n    \"\"\"\n\n    def __init__(self, name=''):\n        super(MyModel, self).__init__(name=name)\n\n        self.flatten = layers.Flatten()\n\n        kernel_initializer = tf.variance_scaling_initializer(scale=1.0 / 3.0, distribution='uniform')\n\n        self.conv1 = layers.Conv2D(32, (3, 3), name='conv1', activation=tf.nn.relu6,\n                                   kernel_initializer=kernel_initializer)\n        self.conv2 = layers.Conv2D(64, (3, 3), name='conv2', activation=tf.nn.relu6,\n                                   kernel_initializer=kernel_initializer)\n        self.conv3 = layers.Conv2D(64, (3, 3), name='conv3', activation=tf.nn.relu6,\n                                   kernel_initializer=kernel_initializer)\n        self.fc1 = layers.Dense(512, name='fc1', activation=tf.nn.relu6)\n        self.steer_predictor = layers.Dense(1, name='steer_predictor')\n\n    def call(self, inputs, training=True):\n        y = self.conv1(inputs)\n\n        y = self.conv2(y)\n\n        y = self.conv3(y)\n        y = self.flatten(y)\n\n        y = self.fc1(y)\n        y = self.steer_predictor(y)\n\n        return y\n\n\ndef get_distribution_strategy(num_gpus, all_reduce_alg=None):\n    \"\"\"Return a DistributionStrategy for running the model.\n    Args:\n      num_gpus: Number of GPUs to run this model.\n      all_reduce_alg: Specify which algorithm to use when performing all-reduce.\n        See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.\n        If None, DistributionStrategy will choose based on device topology.\n    Returns:\n      tf.contrib.distribute.DistibutionStrategy object.\n    \"\"\"\n    if num_gpus == 0:\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:CPU:0\")\n    elif num_gpus == 1:\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:GPU:0\")\n    else:\n        if all_reduce_alg:\n            return tf.contrib.distribute.MirroredStrategy(\n                num_gpus=num_gpus,\n                cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(\n                    all_reduce_alg, num_packs=num_gpus))\n        else:\n            return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\n\n\ndef model_fn(features, labels, mode, params):\n    \"\"\" Model function to be used by the estimator.\n\n    Returns:\n      An EstimatorSpec object\n    \"\"\"\n\n    is_training = mode == tf.estimator.ModeKeys.TRAIN\n\n    model = MyModel()\n\n    predictions = model(features, training=is_training)\n\n    loss = tf.losses.mean_squared_error(labels, predictions)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n\n        global_step = tf.train.get_or_create_global_step()\n\n        learning_rate = tf.train.linear_cosine_decay(0.0001,\n                                                     global_step,\n                                                     10000,\n                                                     beta=0.01)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n\n        train_op = tf.contrib.training.create_train_op(loss,\n                                                       optimizer,\n                                                       global_step,\n                                                       summarize_gradients=False)\n        # summaries\n        tf.summary.image('inputs', features, max_outputs=6)\n        tf.summary.scalar('training/learning_rate', learning_rate)\n\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          predictions=None,\n                                          loss=loss,\n                                          train_op=train_op,\n                                          training_hooks=None)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        eval_metric_ops = {\n            'mae': tf.metrics.mean_absolute_error(labels, predictions),\n        }\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            loss=loss,\n            eval_metric_ops=eval_metric_ops\n        )\n\n\ndef create_input_fn():\n\n    def input_fn():\n        features = tf.random_uniform([100, 88, 200, 3])\n        labels = tf.random_uniform([100, 1])\n        data = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(128)\n        return data\n\n    return input_fn\n\n\n\ndef main(_):\n\n    num_gpus = 2\n\n    # run configuration\n    distribution = get_distribution_strategy(num_gpus)\n\n    # tf session config\n    session_config = tf.ConfigProto(inter_op_parallelism_threads=64,\n                                    intra_op_parallelism_threads=64,\n                                    allow_soft_placement=True)\n\n    run_config = tf.estimator.RunConfig(save_summary_steps=100,\n                                        train_distribute=distribution,\n                                        session_config=session_config)\n\n    # Create estimator that trains and evaluates the model\n    ml_estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir='/tmp/model',\n        config=run_config,\n        params={}\n    )\n\n    ml_estimator.train(input_fn=create_input_fn(), steps=100)\n\n\nif __name__ == '__main__':\n    # Set tensorflow verbosity\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Run the experiment\n    tf.app.run()\n</code></pre>", "body_text": "@guptapriya here is an example of code to reproduce this. Tested on Nvidia Titan X. The model is big enough to completely fill the memory of a single GPU but it still works in one device strategy.\nYou get the problem when you change the number of GPU to 2 for example.\nimport tensorflow as tf\nlayers = tf.keras.layers\n\nclass MyModel(tf.keras.Model):\n    \"\"\"\n    Simple CNN model.\n    \"\"\"\n\n    def __init__(self, name=''):\n        super(MyModel, self).__init__(name=name)\n\n        self.flatten = layers.Flatten()\n\n        kernel_initializer = tf.variance_scaling_initializer(scale=1.0 / 3.0, distribution='uniform')\n\n        self.conv1 = layers.Conv2D(32, (3, 3), name='conv1', activation=tf.nn.relu6,\n                                   kernel_initializer=kernel_initializer)\n        self.conv2 = layers.Conv2D(64, (3, 3), name='conv2', activation=tf.nn.relu6,\n                                   kernel_initializer=kernel_initializer)\n        self.conv3 = layers.Conv2D(64, (3, 3), name='conv3', activation=tf.nn.relu6,\n                                   kernel_initializer=kernel_initializer)\n        self.fc1 = layers.Dense(512, name='fc1', activation=tf.nn.relu6)\n        self.steer_predictor = layers.Dense(1, name='steer_predictor')\n\n    def call(self, inputs, training=True):\n        y = self.conv1(inputs)\n\n        y = self.conv2(y)\n\n        y = self.conv3(y)\n        y = self.flatten(y)\n\n        y = self.fc1(y)\n        y = self.steer_predictor(y)\n\n        return y\n\n\ndef get_distribution_strategy(num_gpus, all_reduce_alg=None):\n    \"\"\"Return a DistributionStrategy for running the model.\n    Args:\n      num_gpus: Number of GPUs to run this model.\n      all_reduce_alg: Specify which algorithm to use when performing all-reduce.\n        See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.\n        If None, DistributionStrategy will choose based on device topology.\n    Returns:\n      tf.contrib.distribute.DistibutionStrategy object.\n    \"\"\"\n    if num_gpus == 0:\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:CPU:0\")\n    elif num_gpus == 1:\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:GPU:0\")\n    else:\n        if all_reduce_alg:\n            return tf.contrib.distribute.MirroredStrategy(\n                num_gpus=num_gpus,\n                cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(\n                    all_reduce_alg, num_packs=num_gpus))\n        else:\n            return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\n\n\ndef model_fn(features, labels, mode, params):\n    \"\"\" Model function to be used by the estimator.\n\n    Returns:\n      An EstimatorSpec object\n    \"\"\"\n\n    is_training = mode == tf.estimator.ModeKeys.TRAIN\n\n    model = MyModel()\n\n    predictions = model(features, training=is_training)\n\n    loss = tf.losses.mean_squared_error(labels, predictions)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n\n        global_step = tf.train.get_or_create_global_step()\n\n        learning_rate = tf.train.linear_cosine_decay(0.0001,\n                                                     global_step,\n                                                     10000,\n                                                     beta=0.01)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n\n        train_op = tf.contrib.training.create_train_op(loss,\n                                                       optimizer,\n                                                       global_step,\n                                                       summarize_gradients=False)\n        # summaries\n        tf.summary.image('inputs', features, max_outputs=6)\n        tf.summary.scalar('training/learning_rate', learning_rate)\n\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          predictions=None,\n                                          loss=loss,\n                                          train_op=train_op,\n                                          training_hooks=None)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        eval_metric_ops = {\n            'mae': tf.metrics.mean_absolute_error(labels, predictions),\n        }\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            loss=loss,\n            eval_metric_ops=eval_metric_ops\n        )\n\n\ndef create_input_fn():\n\n    def input_fn():\n        features = tf.random_uniform([100, 88, 200, 3])\n        labels = tf.random_uniform([100, 1])\n        data = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(128)\n        return data\n\n    return input_fn\n\n\n\ndef main(_):\n\n    num_gpus = 2\n\n    # run configuration\n    distribution = get_distribution_strategy(num_gpus)\n\n    # tf session config\n    session_config = tf.ConfigProto(inter_op_parallelism_threads=64,\n                                    intra_op_parallelism_threads=64,\n                                    allow_soft_placement=True)\n\n    run_config = tf.estimator.RunConfig(save_summary_steps=100,\n                                        train_distribute=distribution,\n                                        session_config=session_config)\n\n    # Create estimator that trains and evaluates the model\n    ml_estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir='/tmp/model',\n        config=run_config,\n        params={}\n    )\n\n    ml_estimator.train(input_fn=create_input_fn(), steps=100)\n\n\nif __name__ == '__main__':\n    # Set tensorflow verbosity\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Run the experiment\n    tf.app.run()", "body": "@guptapriya here is an example of code to reproduce this. Tested on Nvidia Titan X. The model is big enough to completely fill the memory of a single GPU but it still works in one device strategy. \r\nYou get the problem when you change the number of GPU to 2 for example.\r\n\r\n```\r\nimport tensorflow as tf\r\nlayers = tf.keras.layers\r\n\r\nclass MyModel(tf.keras.Model):\r\n    \"\"\"\r\n    Simple CNN model.\r\n    \"\"\"\r\n\r\n    def __init__(self, name=''):\r\n        super(MyModel, self).__init__(name=name)\r\n\r\n        self.flatten = layers.Flatten()\r\n\r\n        kernel_initializer = tf.variance_scaling_initializer(scale=1.0 / 3.0, distribution='uniform')\r\n\r\n        self.conv1 = layers.Conv2D(32, (3, 3), name='conv1', activation=tf.nn.relu6,\r\n                                   kernel_initializer=kernel_initializer)\r\n        self.conv2 = layers.Conv2D(64, (3, 3), name='conv2', activation=tf.nn.relu6,\r\n                                   kernel_initializer=kernel_initializer)\r\n        self.conv3 = layers.Conv2D(64, (3, 3), name='conv3', activation=tf.nn.relu6,\r\n                                   kernel_initializer=kernel_initializer)\r\n        self.fc1 = layers.Dense(512, name='fc1', activation=tf.nn.relu6)\r\n        self.steer_predictor = layers.Dense(1, name='steer_predictor')\r\n\r\n    def call(self, inputs, training=True):\r\n        y = self.conv1(inputs)\r\n\r\n        y = self.conv2(y)\r\n\r\n        y = self.conv3(y)\r\n        y = self.flatten(y)\r\n\r\n        y = self.fc1(y)\r\n        y = self.steer_predictor(y)\r\n\r\n        return y\r\n\r\n\r\ndef get_distribution_strategy(num_gpus, all_reduce_alg=None):\r\n    \"\"\"Return a DistributionStrategy for running the model.\r\n    Args:\r\n      num_gpus: Number of GPUs to run this model.\r\n      all_reduce_alg: Specify which algorithm to use when performing all-reduce.\r\n        See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.\r\n        If None, DistributionStrategy will choose based on device topology.\r\n    Returns:\r\n      tf.contrib.distribute.DistibutionStrategy object.\r\n    \"\"\"\r\n    if num_gpus == 0:\r\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:CPU:0\")\r\n    elif num_gpus == 1:\r\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:GPU:0\")\r\n    else:\r\n        if all_reduce_alg:\r\n            return tf.contrib.distribute.MirroredStrategy(\r\n                num_gpus=num_gpus,\r\n                cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(\r\n                    all_reduce_alg, num_packs=num_gpus))\r\n        else:\r\n            return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    \"\"\" Model function to be used by the estimator.\r\n\r\n    Returns:\r\n      An EstimatorSpec object\r\n    \"\"\"\r\n\r\n    is_training = mode == tf.estimator.ModeKeys.TRAIN\r\n\r\n    model = MyModel()\r\n\r\n    predictions = model(features, training=is_training)\r\n\r\n    loss = tf.losses.mean_squared_error(labels, predictions)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n\r\n        global_step = tf.train.get_or_create_global_step()\r\n\r\n        learning_rate = tf.train.linear_cosine_decay(0.0001,\r\n                                                     global_step,\r\n                                                     10000,\r\n                                                     beta=0.01)\r\n\r\n        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\r\n\r\n        train_op = tf.contrib.training.create_train_op(loss,\r\n                                                       optimizer,\r\n                                                       global_step,\r\n                                                       summarize_gradients=False)\r\n        # summaries\r\n        tf.summary.image('inputs', features, max_outputs=6)\r\n        tf.summary.scalar('training/learning_rate', learning_rate)\r\n\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=None,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=None)\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        eval_metric_ops = {\r\n            'mae': tf.metrics.mean_absolute_error(labels, predictions),\r\n        }\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            loss=loss,\r\n            eval_metric_ops=eval_metric_ops\r\n        )\r\n\r\n\r\ndef create_input_fn():\r\n\r\n    def input_fn():\r\n        features = tf.random_uniform([100, 88, 200, 3])\r\n        labels = tf.random_uniform([100, 1])\r\n        data = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(128)\r\n        return data\r\n\r\n    return input_fn\r\n\r\n\r\n\r\ndef main(_):\r\n\r\n    num_gpus = 2\r\n\r\n    # run configuration\r\n    distribution = get_distribution_strategy(num_gpus)\r\n\r\n    # tf session config\r\n    session_config = tf.ConfigProto(inter_op_parallelism_threads=64,\r\n                                    intra_op_parallelism_threads=64,\r\n                                    allow_soft_placement=True)\r\n\r\n    run_config = tf.estimator.RunConfig(save_summary_steps=100,\r\n                                        train_distribute=distribution,\r\n                                        session_config=session_config)\r\n\r\n    # Create estimator that trains and evaluates the model\r\n    ml_estimator = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        model_dir='/tmp/model',\r\n        config=run_config,\r\n        params={}\r\n    )\r\n\r\n    ml_estimator.train(input_fn=create_input_fn(), steps=100)\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Set tensorflow verbosity\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n    # Run the experiment\r\n    tf.app.run()\r\n```"}