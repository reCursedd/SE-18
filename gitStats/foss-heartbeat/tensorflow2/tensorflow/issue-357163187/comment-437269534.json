{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/437269534", "html_url": "https://github.com/tensorflow/tensorflow/issues/22088#issuecomment-437269534", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22088", "id": 437269534, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzI2OTUzNA==", "user": {"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-09T07:04:02Z", "updated_at": "2018-11-09T07:04:02Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1025387\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jrabary\">@jrabary</a> thanks for trying it out. num_packs=0 means that we will reduce each gradient separately, instead of trying to combine all of them into a small number of tensors first. Performance impact will depend on the use case. In the cases where the gradient tensors are large though (like it seems in your use case), it is not feasible to combine them given the limited memory.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1647833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuefengz\">@yuefengz</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2314265\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dubey\">@dubey</a> it seems like we should not try to combine gradients if there isn't enough memory. Can we do this in MirroredStrategy? Does CollectiveAllReduceStrategy check this?</p>", "body_text": "@jrabary thanks for trying it out. num_packs=0 means that we will reduce each gradient separately, instead of trying to combine all of them into a small number of tensors first. Performance impact will depend on the use case. In the cases where the gradient tensors are large though (like it seems in your use case), it is not feasible to combine them given the limited memory.\n@yuefengz @dubey it seems like we should not try to combine gradients if there isn't enough memory. Can we do this in MirroredStrategy? Does CollectiveAllReduceStrategy check this?", "body": "@jrabary thanks for trying it out. num_packs=0 means that we will reduce each gradient separately, instead of trying to combine all of them into a small number of tensors first. Performance impact will depend on the use case. In the cases where the gradient tensors are large though (like it seems in your use case), it is not feasible to combine them given the limited memory. \r\n\r\n@yuefengz @dubey it seems like we should not try to combine gradients if there isn't enough memory. Can we do this in MirroredStrategy? Does CollectiveAllReduceStrategy check this?\r\n"}