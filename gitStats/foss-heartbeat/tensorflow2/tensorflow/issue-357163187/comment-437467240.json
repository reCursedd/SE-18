{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/437467240", "html_url": "https://github.com/tensorflow/tensorflow/issues/22088#issuecomment-437467240", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22088", "id": 437467240, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzQ2NzI0MA==", "user": {"login": "yuefengz", "id": 1647833, "node_id": "MDQ6VXNlcjE2NDc4MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1647833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuefengz", "html_url": "https://github.com/yuefengz", "followers_url": "https://api.github.com/users/yuefengz/followers", "following_url": "https://api.github.com/users/yuefengz/following{/other_user}", "gists_url": "https://api.github.com/users/yuefengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuefengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuefengz/subscriptions", "organizations_url": "https://api.github.com/users/yuefengz/orgs", "repos_url": "https://api.github.com/users/yuefengz/repos", "events_url": "https://api.github.com/users/yuefengz/events{/privacy}", "received_events_url": "https://api.github.com/users/yuefengz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-09T19:22:21Z", "updated_at": "2018-11-09T19:22:44Z", "author_association": "MEMBER", "body_html": "<p>The <code>concat</code> op has to create a memory block to store the concatenated result. In our our nccl packing algorithm, we concat all gradients into one large tensor. We can switch to a different tensor aggregation method (specifying non-zero <code>agg_small_grads_max_bytes</code> and <code>agg_small_grads_max_group</code> and set <code>num_packs</code> to 0) or even disable tensor aggregation. <code>CollectiveAllReduceStrategy</code> is another option which I think avoids creating large concatenated tensors as well.</p>", "body_text": "The concat op has to create a memory block to store the concatenated result. In our our nccl packing algorithm, we concat all gradients into one large tensor. We can switch to a different tensor aggregation method (specifying non-zero agg_small_grads_max_bytes and agg_small_grads_max_group and set num_packs to 0) or even disable tensor aggregation. CollectiveAllReduceStrategy is another option which I think avoids creating large concatenated tensors as well.", "body": "The `concat` op has to create a memory block to store the concatenated result. In our our nccl packing algorithm, we concat all gradients into one large tensor. We can switch to a different tensor aggregation method (specifying non-zero `agg_small_grads_max_bytes` and `agg_small_grads_max_group` and set `num_packs` to 0) or even disable tensor aggregation. `CollectiveAllReduceStrategy` is another option which I think avoids creating large concatenated tensors as well. "}