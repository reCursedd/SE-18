{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/437417503", "html_url": "https://github.com/tensorflow/tensorflow/issues/22088#issuecomment-437417503", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22088", "id": 437417503, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzQxNzUwMw==", "user": {"login": "dubey", "id": 2314265, "node_id": "MDQ6VXNlcjIzMTQyNjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/2314265?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dubey", "html_url": "https://github.com/dubey", "followers_url": "https://api.github.com/users/dubey/followers", "following_url": "https://api.github.com/users/dubey/following{/other_user}", "gists_url": "https://api.github.com/users/dubey/gists{/gist_id}", "starred_url": "https://api.github.com/users/dubey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dubey/subscriptions", "organizations_url": "https://api.github.com/users/dubey/orgs", "repos_url": "https://api.github.com/users/dubey/repos", "events_url": "https://api.github.com/users/dubey/events{/privacy}", "received_events_url": "https://api.github.com/users/dubey/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-09T16:38:18Z", "updated_at": "2018-11-09T16:38:18Z", "author_association": "MEMBER", "body_html": "<p>From what I can understand, packing and splitting shouldn't affect overall memory usage.  This logic concats many (small) tensors into <code>num_packs</code> (larger) tensors.  But overall memory usage should remain the same.</p>\n<p>I'm not aware of a size limitation when using <code>nccl</code>.</p>\n<p>Looking at the logs above, the <code>op_kernel.cc</code> failure reports OOM at <code>GPU:1</code>, but the <code>ResourceExhaustedError</code> reports OOM at <code>GPU:0</code>.  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1647833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuefengz\">@yuefengz</a> could there be an issue with placing the concat and split ops on correct devices?  I noticed that we use <code>ops.colocate_with</code> which was recently deprecated.</p>\n<p>Collectives has this logic built into the C++ backend via the <code>ScopedAllocator</code>.  Conceptually it does a similar thing.  We haven't seen any OOMs due to <code>ScopedAllocator</code>.</p>", "body_text": "From what I can understand, packing and splitting shouldn't affect overall memory usage.  This logic concats many (small) tensors into num_packs (larger) tensors.  But overall memory usage should remain the same.\nI'm not aware of a size limitation when using nccl.\nLooking at the logs above, the op_kernel.cc failure reports OOM at GPU:1, but the ResourceExhaustedError reports OOM at GPU:0.  @yuefengz could there be an issue with placing the concat and split ops on correct devices?  I noticed that we use ops.colocate_with which was recently deprecated.\nCollectives has this logic built into the C++ backend via the ScopedAllocator.  Conceptually it does a similar thing.  We haven't seen any OOMs due to ScopedAllocator.", "body": "From what I can understand, packing and splitting shouldn't affect overall memory usage.  This logic concats many (small) tensors into `num_packs` (larger) tensors.  But overall memory usage should remain the same.\r\n\r\nI'm not aware of a size limitation when using `nccl`.\r\n\r\nLooking at the logs above, the `op_kernel.cc` failure reports OOM at `GPU:1`, but the `ResourceExhaustedError` reports OOM at `GPU:0`.  @yuefengz could there be an issue with placing the concat and split ops on correct devices?  I noticed that we use `ops.colocate_with` which was recently deprecated.\r\n\r\nCollectives has this logic built into the C++ backend via the `ScopedAllocator`.  Conceptually it does a similar thing.  We haven't seen any OOMs due to `ScopedAllocator`."}