{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21229", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21229/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21229/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21229/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21229", "id": 345587046, "node_id": "MDU6SXNzdWUzNDU1ODcwNDY=", "number": 21229, "title": "Default behavior of tf.layers.batch_normalization", "user": {"login": "hsgkim", "id": 34839382, "node_id": "MDQ6VXNlcjM0ODM5Mzgy", "avatar_url": "https://avatars0.githubusercontent.com/u/34839382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hsgkim", "html_url": "https://github.com/hsgkim", "followers_url": "https://api.github.com/users/hsgkim/followers", "following_url": "https://api.github.com/users/hsgkim/following{/other_user}", "gists_url": "https://api.github.com/users/hsgkim/gists{/gist_id}", "starred_url": "https://api.github.com/users/hsgkim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hsgkim/subscriptions", "organizations_url": "https://api.github.com/users/hsgkim/orgs", "repos_url": "https://api.github.com/users/hsgkim/repos", "events_url": "https://api.github.com/users/hsgkim/events{/privacy}", "received_events_url": "https://api.github.com/users/hsgkim/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": {"login": "nealwu", "id": 726075, "node_id": "MDQ6VXNlcjcyNjA3NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/726075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nealwu", "html_url": "https://github.com/nealwu", "followers_url": "https://api.github.com/users/nealwu/followers", "following_url": "https://api.github.com/users/nealwu/following{/other_user}", "gists_url": "https://api.github.com/users/nealwu/gists{/gist_id}", "starred_url": "https://api.github.com/users/nealwu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nealwu/subscriptions", "organizations_url": "https://api.github.com/users/nealwu/orgs", "repos_url": "https://api.github.com/users/nealwu/repos", "events_url": "https://api.github.com/users/nealwu/events{/privacy}", "received_events_url": "https://api.github.com/users/nealwu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "nealwu", "id": 726075, "node_id": "MDQ6VXNlcjcyNjA3NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/726075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nealwu", "html_url": "https://github.com/nealwu", "followers_url": "https://api.github.com/users/nealwu/followers", "following_url": "https://api.github.com/users/nealwu/following{/other_user}", "gists_url": "https://api.github.com/users/nealwu/gists{/gist_id}", "starred_url": "https://api.github.com/users/nealwu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nealwu/subscriptions", "organizations_url": "https://api.github.com/users/nealwu/orgs", "repos_url": "https://api.github.com/users/nealwu/repos", "events_url": "https://api.github.com/users/nealwu/events{/privacy}", "received_events_url": "https://api.github.com/users/nealwu/received_events", "type": "User", "site_admin": false}, {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-30T02:05:52Z", "updated_at": "2018-11-15T19:03:04Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04.5 LTS (Xenial Xerus)</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip (binary)</li>\n<li><strong>TensorFlow version (use command below)</strong>: (tf.GIT_VERSION, tf.VERSION) == ('v1.9.0-0-g25c197e023', '1.9.0')</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/6.1</li>\n<li><strong>GPU model and memory</strong>: NVIDIA GeForce GTX 1080Ti 11177MiB</li>\n<li><strong>Exact command to reproduce</strong>: tf.layers.batch_normalization</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I was trying to train my network using multiple batch normalization layers(<code>tf.layers.batch_normalization</code>), only to fail the training process even though I was basically copying a published paper. I had skimmed through the documentation as it looked fairly straightforward, especially after already having had added multiple layers using <code>tf.layers</code>. Only after a couple days (as training takes quite a long time) did I realize I had missed the note part of the documentation. This is of course my fault for not reading the documentation thoroughly, but it really strikes me odd that the default behavior of this layer is not to update the ops right away. I guess it might be due to performance reasons, but if that is the case why not just have <code>defer_updates</code> argument which defaults to False? I think most beginners would expect the layer to update everything right away, and maybe not even expect everything to be fast anyway. Only power users would really want to fine-tune and optimize their network, in which case then they can set <code>defer_updates</code> to True.</p>\n<p>I think I just rambled a lot here; my point is that I think it would be better if <code>tf.layers.batch_normalization</code> would update ops without the need of <code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</code> and <code>with tf.control_dependencies(update_ops)</code>, and for people who do not want this, throw <code>defer_updates=True</code>.</p>\n<p>Edit: formatting</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS (Xenial Xerus)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): pip (binary)\nTensorFlow version (use command below): (tf.GIT_VERSION, tf.VERSION) == ('v1.9.0-0-g25c197e023', '1.9.0')\nPython version: 3.5.2\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 9.0/6.1\nGPU model and memory: NVIDIA GeForce GTX 1080Ti 11177MiB\nExact command to reproduce: tf.layers.batch_normalization\n\nDescribe the problem\nI was trying to train my network using multiple batch normalization layers(tf.layers.batch_normalization), only to fail the training process even though I was basically copying a published paper. I had skimmed through the documentation as it looked fairly straightforward, especially after already having had added multiple layers using tf.layers. Only after a couple days (as training takes quite a long time) did I realize I had missed the note part of the documentation. This is of course my fault for not reading the documentation thoroughly, but it really strikes me odd that the default behavior of this layer is not to update the ops right away. I guess it might be due to performance reasons, but if that is the case why not just have defer_updates argument which defaults to False? I think most beginners would expect the layer to update everything right away, and maybe not even expect everything to be fast anyway. Only power users would really want to fine-tune and optimize their network, in which case then they can set defer_updates to True.\nI think I just rambled a lot here; my point is that I think it would be better if tf.layers.batch_normalization would update ops without the need of update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) and with tf.control_dependencies(update_ops), and for people who do not want this, throw defer_updates=True.\nEdit: formatting", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS (Xenial Xerus)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: pip (binary)\r\n- **TensorFlow version (use command below)**: (tf.GIT_VERSION, tf.VERSION) == ('v1.9.0-0-g25c197e023', '1.9.0')\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/6.1\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1080Ti 11177MiB\r\n- **Exact command to reproduce**: tf.layers.batch_normalization\r\n### Describe the problem\r\nI was trying to train my network using multiple batch normalization layers(`tf.layers.batch_normalization`), only to fail the training process even though I was basically copying a published paper. I had skimmed through the documentation as it looked fairly straightforward, especially after already having had added multiple layers using `tf.layers`. Only after a couple days (as training takes quite a long time) did I realize I had missed the note part of the documentation. This is of course my fault for not reading the documentation thoroughly, but it really strikes me odd that the default behavior of this layer is not to update the ops right away. I guess it might be due to performance reasons, but if that is the case why not just have `defer_updates` argument which defaults to False? I think most beginners would expect the layer to update everything right away, and maybe not even expect everything to be fast anyway. Only power users would really want to fine-tune and optimize their network, in which case then they can set `defer_updates` to True.\r\n\r\nI think I just rambled a lot here; my point is that I think it would be better if `tf.layers.batch_normalization` would update ops without the need of `update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)` and `with tf.control_dependencies(update_ops)`, and for people who do not want this, throw `defer_updates=True`.\r\n\r\nEdit: formatting\r\n\r\n\r\n\r\n"}