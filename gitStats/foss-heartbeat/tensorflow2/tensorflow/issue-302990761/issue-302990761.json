{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17500", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17500/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17500/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17500/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17500", "id": 302990761, "node_id": "MDU6SXNzdWUzMDI5OTA3NjE=", "number": 17500, "title": "'colocate_gradients_with_ops' colocate with unused ops", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "agarwal-ashish", "id": 19335798, "node_id": "MDQ6VXNlcjE5MzM1Nzk4", "avatar_url": "https://avatars3.githubusercontent.com/u/19335798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agarwal-ashish", "html_url": "https://github.com/agarwal-ashish", "followers_url": "https://api.github.com/users/agarwal-ashish/followers", "following_url": "https://api.github.com/users/agarwal-ashish/following{/other_user}", "gists_url": "https://api.github.com/users/agarwal-ashish/gists{/gist_id}", "starred_url": "https://api.github.com/users/agarwal-ashish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agarwal-ashish/subscriptions", "organizations_url": "https://api.github.com/users/agarwal-ashish/orgs", "repos_url": "https://api.github.com/users/agarwal-ashish/repos", "events_url": "https://api.github.com/users/agarwal-ashish/events{/privacy}", "received_events_url": "https://api.github.com/users/agarwal-ashish/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "agarwal-ashish", "id": 19335798, "node_id": "MDQ6VXNlcjE5MzM1Nzk4", "avatar_url": "https://avatars3.githubusercontent.com/u/19335798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agarwal-ashish", "html_url": "https://github.com/agarwal-ashish", "followers_url": "https://api.github.com/users/agarwal-ashish/followers", "following_url": "https://api.github.com/users/agarwal-ashish/following{/other_user}", "gists_url": "https://api.github.com/users/agarwal-ashish/gists{/gist_id}", "starred_url": "https://api.github.com/users/agarwal-ashish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agarwal-ashish/subscriptions", "organizations_url": "https://api.github.com/users/agarwal-ashish/orgs", "repos_url": "https://api.github.com/users/agarwal-ashish/repos", "events_url": "https://api.github.com/users/agarwal-ashish/events{/privacy}", "received_events_url": "https://api.github.com/users/agarwal-ashish/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-03-07T07:25:15Z", "updated_at": "2018-03-11T04:41:22Z", "closed_at": "2018-03-11T04:41:22Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:v1.6.0-0-gd2e24b6039 1.6.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7</li>\n<li><strong>GPU model and memory</strong>: P100</li>\n<li><strong>Exact command to reproduce</strong>: here</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> time\n\nG <span class=\"pl-k\">=</span> tf.get_default_graph()\n\n<span class=\"pl-en\">@tf.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SummaryGrad<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">summarize_grad_fn</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">x</span>):\n    my_name_is_not_x <span class=\"pl-k\">=</span> tf.Print(x, [x])\n    <span class=\"pl-k\">return</span> x\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">layer</span>(<span class=\"pl-smi\">x</span>):\n    l <span class=\"pl-k\">=</span> tf.layers.conv2d(x, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>)\n    l <span class=\"pl-k\">=</span> tf.nn.relu(l)\n    <span class=\"pl-k\">with</span> G.gradient_override_map({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Identity<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SummaryGrad<span class=\"pl-pds\">\"</span></span>}):\n        <span class=\"pl-k\">return</span> tf.identity(l)\n\nimg <span class=\"pl-k\">=</span> tf.random_normal((<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>,<span class=\"pl-c1\">224</span>))\nl <span class=\"pl-k\">=</span> layer(img)\nl <span class=\"pl-k\">=</span> layer(l)\nl <span class=\"pl-k\">=</span> layer(l)\n\nloss <span class=\"pl-k\">=</span> tf.reduce_mean(l)\n\nopt <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.1</span>)\ngrads <span class=\"pl-k\">=</span> opt.compute_gradients(loss, <span class=\"pl-v\">colocate_gradients_with_ops</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nop <span class=\"pl-k\">=</span> opt.apply_gradients(grads)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n\n    <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">20</span>):\n        start <span class=\"pl-k\">=</span> time.time()\n        sess.run(op)\n        <span class=\"pl-k\">if</span> k <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">2</span>:\n            <span class=\"pl-c1\">print</span>(time.time() <span class=\"pl-k\">-</span> start)</pre></div>\n<p>If I use <code>colocate_gradients_with_ops=False</code>, or remove the <code>tf.Print</code> line, the above code runs with expected speed.<br>\nOtherwise, it places the gradients on CPUs, resulting in 2x slow down, and a lot of H2D/D2H copy shown in profiling.<br>\nPerhaps this colocate option can be made smarter.</p>\n<p>In this case, Print op depends on some gradients, but no other gradients depend on the Print op. i.e., the Print op <strong>does not appear on the subgraph which <code>grads</code> depends on</strong>. Therefore the colocation seems totally unnecessary here.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):v1.6.0-0-gd2e24b6039 1.6.0\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0/7\nGPU model and memory: P100\nExact command to reproduce: here\n\nimport tensorflow as tf\nimport time\n\nG = tf.get_default_graph()\n\n@tf.RegisterGradient(\"SummaryGrad\")\ndef summarize_grad_fn(op, x):\n    my_name_is_not_x = tf.Print(x, [x])\n    return x\n\ndef layer(x):\n    l = tf.layers.conv2d(x, 64, 3, data_format='channels_first')\n    l = tf.nn.relu(l)\n    with G.gradient_override_map({\"Identity\": \"SummaryGrad\"}):\n        return tf.identity(l)\n\nimg = tf.random_normal((64,3, 224,224))\nl = layer(img)\nl = layer(l)\nl = layer(l)\n\nloss = tf.reduce_mean(l)\n\nopt = tf.train.GradientDescentOptimizer(0.1)\ngrads = opt.compute_gradients(loss, colocate_gradients_with_ops=True)\nop = opt.apply_gradients(grads)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for k in range(20):\n        start = time.time()\n        sess.run(op)\n        if k > 2:\n            print(time.time() - start)\nIf I use colocate_gradients_with_ops=False, or remove the tf.Print line, the above code runs with expected speed.\nOtherwise, it places the gradients on CPUs, resulting in 2x slow down, and a lot of H2D/D2H copy shown in profiling.\nPerhaps this colocate option can be made smarter.\nIn this case, Print op depends on some gradients, but no other gradients depend on the Print op. i.e., the Print op does not appear on the subgraph which grads depends on. Therefore the colocation seems totally unnecessary here.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:v1.6.0-0-gd2e24b6039 1.6.0 \r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: P100\r\n- **Exact command to reproduce**: here\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\nG = tf.get_default_graph()\r\n\r\n@tf.RegisterGradient(\"SummaryGrad\")\r\ndef summarize_grad_fn(op, x):\r\n    my_name_is_not_x = tf.Print(x, [x])\r\n    return x\r\n\r\ndef layer(x):\r\n    l = tf.layers.conv2d(x, 64, 3, data_format='channels_first')\r\n    l = tf.nn.relu(l)\r\n    with G.gradient_override_map({\"Identity\": \"SummaryGrad\"}):\r\n        return tf.identity(l)\r\n\r\nimg = tf.random_normal((64,3, 224,224))\r\nl = layer(img)\r\nl = layer(l)\r\nl = layer(l)\r\n\r\nloss = tf.reduce_mean(l)\r\n\r\nopt = tf.train.GradientDescentOptimizer(0.1)\r\ngrads = opt.compute_gradients(loss, colocate_gradients_with_ops=True)\r\nop = opt.apply_gradients(grads)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    for k in range(20):\r\n        start = time.time()\r\n        sess.run(op)\r\n        if k > 2:\r\n            print(time.time() - start)\r\n```\r\n\r\nIf I use `colocate_gradients_with_ops=False`, or remove the `tf.Print` line, the above code runs with expected speed.\r\nOtherwise, it places the gradients on CPUs, resulting in 2x slow down, and a lot of H2D/D2H copy shown in profiling.\r\nPerhaps this colocate option can be made smarter.\r\n\r\nIn this case, Print op depends on some gradients, but no other gradients depend on the Print op. i.e., the Print op __does not appear on the subgraph which `grads` depends on__. Therefore the colocation seems totally unnecessary here."}