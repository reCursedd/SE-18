{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21838", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21838/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21838/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21838/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21838", "id": 353628194, "node_id": "MDU6SXNzdWUzNTM2MjgxOTQ=", "number": 21838, "title": "Keras Layer add_variable not taking Dimension objects", "user": {"login": "ericcao119", "id": 9809018, "node_id": "MDQ6VXNlcjk4MDkwMTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/9809018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericcao119", "html_url": "https://github.com/ericcao119", "followers_url": "https://api.github.com/users/ericcao119/followers", "following_url": "https://api.github.com/users/ericcao119/following{/other_user}", "gists_url": "https://api.github.com/users/ericcao119/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericcao119/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericcao119/subscriptions", "organizations_url": "https://api.github.com/users/ericcao119/orgs", "repos_url": "https://api.github.com/users/ericcao119/repos", "events_url": "https://api.github.com/users/ericcao119/events{/privacy}", "received_events_url": "https://api.github.com/users/ericcao119/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-24T03:13:17Z", "updated_at": "2018-11-22T18:58:04Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Heavily based on example custom layer on tensorflow's keras page.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Google Colab (Linux Ubuntu 17.10)</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Unsure</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.0</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: Unsure. nvcc --version did not work.</li>\n<li><strong>GPU model and memory</strong>: K80 GPU (Memory unsure)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>My <a href=\"https://stackoverflow.com/questions/51939618/what-could-cause-a-type-error-during-the-build-process-of-keras-layer\" rel=\"nofollow\">question</a> was answered on StackOverflow and a user recommended me to create a bug report here. I was creating a keras layer based on the NALU design, but I found that while on my local version of tensorflow (1.8.0) the code would run as expected, the same code would not run on Google Colab's version of tensorflow(1.10.0). The issue ended up being that tensorflow's Dimension type was not accepted in the Layer.add_variable method. It requires the dimensions to be converted to ints, which does not happen implicitly. I think that it would make sense for the method to accept Dimension objects as they are used to specify the shape of Tensors. As well, it provides backwards compatibility to previous versions of tensorflow, which do support pass Dimension objects to Layers.add_variable.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p>Source:</p>\n<pre><code>import tensorflow as tf\n\nclass NALU(tf.keras.layers.Layer):\n    def __init__(self, num_outputs, **kwargs):\n        self.num_outputs = num_outputs\n        super(NALU, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        shape = tf.TensorShape((input_shape[1], self.num_outputs)).as_list()\n        get = tf.keras.initializers.get\n        self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\n        self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\n        self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add &amp; multiply\n        self.GM = self.add_variable(\"GM\", shape=shape, initializer=get('glorot_uniform')) # Gate multiply\n        \n        super(NALU, self).build(input_shape)\n    \n    def call(self, x):\n        gam = tf.sigmoid(tf.matmul(x, self.GAM)) # The gate\n        gm = tf.sigmoid(tf.matmul(x, self.GM)) # The gate\n        \n        W = tf.tanh(self.W_) * tf.sigmoid(self.M_)\n        add = tf.matmul(x, W)\n        \n        # represents positive multiplication\n        m = tf.exp(\n            tf.matmul(tf.log(tf.abs(x) + 1e-14), W)\n        )\n        \n        mul = (-1 * m) * (1.0 - gm) + gm * m \n        \n        y = gam * add + (1.0 - gam) * mul\n        return y\n    \n    def compute_output_shape(self, input_shape):\n        # --------------IMPORTANT LINE---------------------\n        shape = tf.TensorShape(input_shape)\n        #  shape = tf.TensorShape(input_shape).as_list() is required to make the function work\n        # -------------------------------------------------\n        shape[-1] = self.num_outputs\n        return tf.TensorShape(shape)\n    \n    def get_config(self):\n        base_config = super(NALU, self).get_config()\n        base_config['num_outputs'] = self.num_outputs\n        \n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\ndef nalu_model():\n    inp = tf.keras.layers.Input(shape=(2,))\n    out = NALU(1)(inp)\n    \n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    return model\n</code></pre>\n<p>Test Case:</p>\n<pre><code>import numpy as np\n\ndef create_train():\n#     x_train = np.random.randint(-100, 100, size=(10000000, 2), dtype=np.int32)\n    x_train = np.random.uniform(-100, 100, size=(10000000, 2))\n\n    first_neg = np.copy(x_train)\n    first_neg[:, 0] *= -1\n\n    second_neg = np.copy(x_train)\n    second_neg[:, 1] *= -1\n\n    all_neg = x_train * -1\n\n    x_train = np.append(x_train, first_neg)\n    x_train = np.append(x_train, second_neg)\n    x_train = np.append(x_train, all_neg)\n    x_train = x_train.reshape((40000000, 2))\n\n    com = x_train[:,::-1]\n    x_train = np.append(x_train, com).reshape((80000000, 2))\n    y_train = x_train[:, 0] * x_train[:, 1]\n    \n    return x_train, y_train\n\nx_train, y_train = create_train()\nx_test = np.random.randint(-1000, 1000, size=(10000, 2))\ny_test = x_test[:, 0] * x_test[:, 1]\n\nmodel = nalu_model()\n\nmodel.compile(optimizer='RMSProp',\n              loss='MSE',\n              metrics=['accuracy', 'MAE'])\n\ncb = [tf.keras.callbacks.TensorBoard(log_dir='./add_logs', histogram_freq=1, batch_size=32, write_graph=True, write_grads=True),\n      tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00000001, patience=5, verbose=2, mode='auto')]\n\nmodel.fit(x_train, y_train, epochs=500, batch_size=256, shuffle=True, callbacks=cb, validation_split=0.4)\nmodel.evaluate(x_test, y_test)\n</code></pre>\n<p>Error:</p>\n<pre><code>TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-7-55fb94a8f3b1&gt; in &lt;module&gt;()\n     82 y_test = x_test[:, 0] * x_test[:, 1]\n     83 \n---&gt; 84 model = nalu_model()\n     85 \n     86 model.compile(optimizer='RMSProp',\n\n&lt;ipython-input-7-55fb94a8f3b1&gt; in nalu_model()\n     48 def nalu_model():\n     49     inp = tf.keras.layers.Input(shape=(2,))\n---&gt; 50     out = NALU(1)(inp)\n     51 \n     52     model = tf.keras.models.Model(inputs=inp, outputs=out)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    726         if all(hasattr(x, 'shape') for x in input_list):\n    727           input_shapes = nest.map_structure(lambda x: x.shape, inputs)\n--&gt; 728         self.build(input_shapes)\n    729         self.built = True\n    730 \n\n&lt;ipython-input-7-55fb94a8f3b1&gt; in build(self, input_shape)\n      9         shape = tf.TensorShape((input_shape[1], self.num_outputs))\n     10         get = tf.keras.initializers.get\n---&gt; 11         self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\n     12         self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\n     13         self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add &amp; multiply\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_variable(self, *args, **kwargs)\n    459   def add_variable(self, *args, **kwargs):\n    460     \"\"\"Alias for `add_weight`.\"\"\"\n--&gt; 461     return self.add_weight(*args, **kwargs)\n    462 \n    463   def add_weight(self,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, getter)\n    563         use_resource=use_resource,\n    564         synchronization=synchronization,\n--&gt; 565         aggregation=aggregation)\n    566 \n    567     if regularizer is not None:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\n    533     new_variable = getter(\n    534         name=name, shape=shape, dtype=dtype, initializer=initializer,\n--&gt; 535         **kwargs_for_getter)\n    536 \n    537     # If we set an initializer and the variable processed it, tracking will not\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in make_variable(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, synchronization, aggregation, partitioner)\n   1916       use_resource=use_resource,\n   1917       synchronization=synchronization,\n-&gt; 1918       aggregation=aggregation)\n   1919   return v\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource, synchronization, aggregation)\n   2441       use_resource=use_resource,\n   2442       synchronization=synchronization,\n-&gt; 2443       aggregation=aggregation)\n   2444 \n   2445 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in &lt;lambda&gt;(**kwargs)\n   2423              synchronization=VariableSynchronization.AUTO,\n   2424              aggregation=VariableAggregation.NONE):\n-&gt; 2425   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n   2426   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\n   2427     previous_getter = _make_getter(getter, previous_getter)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\n   2393         collections=collections, validate_shape=validate_shape,\n   2394         caching_device=caching_device, name=name, dtype=dtype,\n-&gt; 2395         constraint=constraint)\n   2396   elif not use_resource and context.executing_eagerly():\n   2397     raise RuntimeError(\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)\n    310           name=name,\n    311           dtype=dtype,\n--&gt; 312           constraint=constraint)\n    313 \n    314   # pylint: disable=unused-argument\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)\n    415               with ops.name_scope(\"Initializer\"), ops.device(None):\n    416                 initial_value = ops.convert_to_tensor(\n--&gt; 417                     initial_value(), name=\"initial_value\", dtype=dtype)\n    418               self._handle = _eager_safe_variable_handle(\n    419                   shape=initial_value.get_shape(),\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in &lt;lambda&gt;()\n   1901         initializer = initializer(dtype=dtype)\n   1902       init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n-&gt; 1903           shape, dtype=dtype, partition_info=partition_info)\n   1904       variable_dtype = dtype.base_dtype\n   1905   if use_resource is None:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)\n    474       scale /= max(1., fan_out)\n    475     else:\n--&gt; 476       scale /= max(1., (fan_in + fan_out) / 2.)\n    477     if self.distribution == \"normal\" or self.distribution == \"truncated_normal\":\n    478       # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'float'\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Heavily based on example custom layer on tensorflow's keras page.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Linux Ubuntu 17.10)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): Unsure\nTensorFlow version (use command below): 1.10.0\nPython version: 3.6.3\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: Unsure. nvcc --version did not work.\nGPU model and memory: K80 GPU (Memory unsure)\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nMy question was answered on StackOverflow and a user recommended me to create a bug report here. I was creating a keras layer based on the NALU design, but I found that while on my local version of tensorflow (1.8.0) the code would run as expected, the same code would not run on Google Colab's version of tensorflow(1.10.0). The issue ended up being that tensorflow's Dimension type was not accepted in the Layer.add_variable method. It requires the dimensions to be converted to ints, which does not happen implicitly. I think that it would make sense for the method to accept Dimension objects as they are used to specify the shape of Tensors. As well, it provides backwards compatibility to previous versions of tensorflow, which do support pass Dimension objects to Layers.add_variable.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nSource:\nimport tensorflow as tf\n\nclass NALU(tf.keras.layers.Layer):\n    def __init__(self, num_outputs, **kwargs):\n        self.num_outputs = num_outputs\n        super(NALU, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        shape = tf.TensorShape((input_shape[1], self.num_outputs)).as_list()\n        get = tf.keras.initializers.get\n        self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\n        self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\n        self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply\n        self.GM = self.add_variable(\"GM\", shape=shape, initializer=get('glorot_uniform')) # Gate multiply\n        \n        super(NALU, self).build(input_shape)\n    \n    def call(self, x):\n        gam = tf.sigmoid(tf.matmul(x, self.GAM)) # The gate\n        gm = tf.sigmoid(tf.matmul(x, self.GM)) # The gate\n        \n        W = tf.tanh(self.W_) * tf.sigmoid(self.M_)\n        add = tf.matmul(x, W)\n        \n        # represents positive multiplication\n        m = tf.exp(\n            tf.matmul(tf.log(tf.abs(x) + 1e-14), W)\n        )\n        \n        mul = (-1 * m) * (1.0 - gm) + gm * m \n        \n        y = gam * add + (1.0 - gam) * mul\n        return y\n    \n    def compute_output_shape(self, input_shape):\n        # --------------IMPORTANT LINE---------------------\n        shape = tf.TensorShape(input_shape)\n        #  shape = tf.TensorShape(input_shape).as_list() is required to make the function work\n        # -------------------------------------------------\n        shape[-1] = self.num_outputs\n        return tf.TensorShape(shape)\n    \n    def get_config(self):\n        base_config = super(NALU, self).get_config()\n        base_config['num_outputs'] = self.num_outputs\n        \n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\ndef nalu_model():\n    inp = tf.keras.layers.Input(shape=(2,))\n    out = NALU(1)(inp)\n    \n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    return model\n\nTest Case:\nimport numpy as np\n\ndef create_train():\n#     x_train = np.random.randint(-100, 100, size=(10000000, 2), dtype=np.int32)\n    x_train = np.random.uniform(-100, 100, size=(10000000, 2))\n\n    first_neg = np.copy(x_train)\n    first_neg[:, 0] *= -1\n\n    second_neg = np.copy(x_train)\n    second_neg[:, 1] *= -1\n\n    all_neg = x_train * -1\n\n    x_train = np.append(x_train, first_neg)\n    x_train = np.append(x_train, second_neg)\n    x_train = np.append(x_train, all_neg)\n    x_train = x_train.reshape((40000000, 2))\n\n    com = x_train[:,::-1]\n    x_train = np.append(x_train, com).reshape((80000000, 2))\n    y_train = x_train[:, 0] * x_train[:, 1]\n    \n    return x_train, y_train\n\nx_train, y_train = create_train()\nx_test = np.random.randint(-1000, 1000, size=(10000, 2))\ny_test = x_test[:, 0] * x_test[:, 1]\n\nmodel = nalu_model()\n\nmodel.compile(optimizer='RMSProp',\n              loss='MSE',\n              metrics=['accuracy', 'MAE'])\n\ncb = [tf.keras.callbacks.TensorBoard(log_dir='./add_logs', histogram_freq=1, batch_size=32, write_graph=True, write_grads=True),\n      tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00000001, patience=5, verbose=2, mode='auto')]\n\nmodel.fit(x_train, y_train, epochs=500, batch_size=256, shuffle=True, callbacks=cb, validation_split=0.4)\nmodel.evaluate(x_test, y_test)\n\nError:\nTypeError                                 Traceback (most recent call last)\n<ipython-input-7-55fb94a8f3b1> in <module>()\n     82 y_test = x_test[:, 0] * x_test[:, 1]\n     83 \n---> 84 model = nalu_model()\n     85 \n     86 model.compile(optimizer='RMSProp',\n\n<ipython-input-7-55fb94a8f3b1> in nalu_model()\n     48 def nalu_model():\n     49     inp = tf.keras.layers.Input(shape=(2,))\n---> 50     out = NALU(1)(inp)\n     51 \n     52     model = tf.keras.models.Model(inputs=inp, outputs=out)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    726         if all(hasattr(x, 'shape') for x in input_list):\n    727           input_shapes = nest.map_structure(lambda x: x.shape, inputs)\n--> 728         self.build(input_shapes)\n    729         self.built = True\n    730 \n\n<ipython-input-7-55fb94a8f3b1> in build(self, input_shape)\n      9         shape = tf.TensorShape((input_shape[1], self.num_outputs))\n     10         get = tf.keras.initializers.get\n---> 11         self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\n     12         self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\n     13         self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_variable(self, *args, **kwargs)\n    459   def add_variable(self, *args, **kwargs):\n    460     \"\"\"Alias for `add_weight`.\"\"\"\n--> 461     return self.add_weight(*args, **kwargs)\n    462 \n    463   def add_weight(self,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, getter)\n    563         use_resource=use_resource,\n    564         synchronization=synchronization,\n--> 565         aggregation=aggregation)\n    566 \n    567     if regularizer is not None:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\n    533     new_variable = getter(\n    534         name=name, shape=shape, dtype=dtype, initializer=initializer,\n--> 535         **kwargs_for_getter)\n    536 \n    537     # If we set an initializer and the variable processed it, tracking will not\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in make_variable(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, synchronization, aggregation, partitioner)\n   1916       use_resource=use_resource,\n   1917       synchronization=synchronization,\n-> 1918       aggregation=aggregation)\n   1919   return v\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource, synchronization, aggregation)\n   2441       use_resource=use_resource,\n   2442       synchronization=synchronization,\n-> 2443       aggregation=aggregation)\n   2444 \n   2445 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>(**kwargs)\n   2423              synchronization=VariableSynchronization.AUTO,\n   2424              aggregation=VariableAggregation.NONE):\n-> 2425   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n   2426   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\n   2427     previous_getter = _make_getter(getter, previous_getter)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\n   2393         collections=collections, validate_shape=validate_shape,\n   2394         caching_device=caching_device, name=name, dtype=dtype,\n-> 2395         constraint=constraint)\n   2396   elif not use_resource and context.executing_eagerly():\n   2397     raise RuntimeError(\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)\n    310           name=name,\n    311           dtype=dtype,\n--> 312           constraint=constraint)\n    313 \n    314   # pylint: disable=unused-argument\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)\n    415               with ops.name_scope(\"Initializer\"), ops.device(None):\n    416                 initial_value = ops.convert_to_tensor(\n--> 417                     initial_value(), name=\"initial_value\", dtype=dtype)\n    418               self._handle = _eager_safe_variable_handle(\n    419                   shape=initial_value.get_shape(),\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in <lambda>()\n   1901         initializer = initializer(dtype=dtype)\n   1902       init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n-> 1903           shape, dtype=dtype, partition_info=partition_info)\n   1904       variable_dtype = dtype.base_dtype\n   1905   if use_resource is None:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)\n    474       scale /= max(1., fan_out)\n    475     else:\n--> 476       scale /= max(1., (fan_in + fan_out) / 2.)\n    477     if self.distribution == \"normal\" or self.distribution == \"truncated_normal\":\n    478       # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'float'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Heavily based on example custom layer on tensorflow's keras page.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab (Linux Ubuntu 17.10)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Unsure \r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Unsure. nvcc --version did not work.\r\n- **GPU model and memory**: K80 GPU (Memory unsure)\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nMy [question](https://stackoverflow.com/questions/51939618/what-could-cause-a-type-error-during-the-build-process-of-keras-layer) was answered on StackOverflow and a user recommended me to create a bug report here. I was creating a keras layer based on the NALU design, but I found that while on my local version of tensorflow (1.8.0) the code would run as expected, the same code would not run on Google Colab's version of tensorflow(1.10.0). The issue ended up being that tensorflow's Dimension type was not accepted in the Layer.add_variable method. It requires the dimensions to be converted to ints, which does not happen implicitly. I think that it would make sense for the method to accept Dimension objects as they are used to specify the shape of Tensors. As well, it provides backwards compatibility to previous versions of tensorflow, which do support pass Dimension objects to Layers.add_variable.\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nSource:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass NALU(tf.keras.layers.Layer):\r\n    def __init__(self, num_outputs, **kwargs):\r\n        self.num_outputs = num_outputs\r\n        super(NALU, self).__init__(**kwargs)\r\n        \r\n    def build(self, input_shape):\r\n        shape = tf.TensorShape((input_shape[1], self.num_outputs)).as_list()\r\n        get = tf.keras.initializers.get\r\n        self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\r\n        self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\r\n        self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply\r\n        self.GM = self.add_variable(\"GM\", shape=shape, initializer=get('glorot_uniform')) # Gate multiply\r\n        \r\n        super(NALU, self).build(input_shape)\r\n    \r\n    def call(self, x):\r\n        gam = tf.sigmoid(tf.matmul(x, self.GAM)) # The gate\r\n        gm = tf.sigmoid(tf.matmul(x, self.GM)) # The gate\r\n        \r\n        W = tf.tanh(self.W_) * tf.sigmoid(self.M_)\r\n        add = tf.matmul(x, W)\r\n        \r\n        # represents positive multiplication\r\n        m = tf.exp(\r\n            tf.matmul(tf.log(tf.abs(x) + 1e-14), W)\r\n        )\r\n        \r\n        mul = (-1 * m) * (1.0 - gm) + gm * m \r\n        \r\n        y = gam * add + (1.0 - gam) * mul\r\n        return y\r\n    \r\n    def compute_output_shape(self, input_shape):\r\n        # --------------IMPORTANT LINE---------------------\r\n        shape = tf.TensorShape(input_shape)\r\n        #  shape = tf.TensorShape(input_shape).as_list() is required to make the function work\r\n        # -------------------------------------------------\r\n        shape[-1] = self.num_outputs\r\n        return tf.TensorShape(shape)\r\n    \r\n    def get_config(self):\r\n        base_config = super(NALU, self).get_config()\r\n        base_config['num_outputs'] = self.num_outputs\r\n        \r\n    @classmethod\r\n    def from_config(cls, config):\r\n        return cls(**config)\r\n\r\ndef nalu_model():\r\n    inp = tf.keras.layers.Input(shape=(2,))\r\n    out = NALU(1)(inp)\r\n    \r\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\r\n    return model\r\n```\r\nTest Case:\r\n```\r\nimport numpy as np\r\n\r\ndef create_train():\r\n#     x_train = np.random.randint(-100, 100, size=(10000000, 2), dtype=np.int32)\r\n    x_train = np.random.uniform(-100, 100, size=(10000000, 2))\r\n\r\n    first_neg = np.copy(x_train)\r\n    first_neg[:, 0] *= -1\r\n\r\n    second_neg = np.copy(x_train)\r\n    second_neg[:, 1] *= -1\r\n\r\n    all_neg = x_train * -1\r\n\r\n    x_train = np.append(x_train, first_neg)\r\n    x_train = np.append(x_train, second_neg)\r\n    x_train = np.append(x_train, all_neg)\r\n    x_train = x_train.reshape((40000000, 2))\r\n\r\n    com = x_train[:,::-1]\r\n    x_train = np.append(x_train, com).reshape((80000000, 2))\r\n    y_train = x_train[:, 0] * x_train[:, 1]\r\n    \r\n    return x_train, y_train\r\n\r\nx_train, y_train = create_train()\r\nx_test = np.random.randint(-1000, 1000, size=(10000, 2))\r\ny_test = x_test[:, 0] * x_test[:, 1]\r\n\r\nmodel = nalu_model()\r\n\r\nmodel.compile(optimizer='RMSProp',\r\n              loss='MSE',\r\n              metrics=['accuracy', 'MAE'])\r\n\r\ncb = [tf.keras.callbacks.TensorBoard(log_dir='./add_logs', histogram_freq=1, batch_size=32, write_graph=True, write_grads=True),\r\n      tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00000001, patience=5, verbose=2, mode='auto')]\r\n\r\nmodel.fit(x_train, y_train, epochs=500, batch_size=256, shuffle=True, callbacks=cb, validation_split=0.4)\r\nmodel.evaluate(x_test, y_test)\r\n```\r\n\r\n\r\nError:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-55fb94a8f3b1> in <module>()\r\n     82 y_test = x_test[:, 0] * x_test[:, 1]\r\n     83 \r\n---> 84 model = nalu_model()\r\n     85 \r\n     86 model.compile(optimizer='RMSProp',\r\n\r\n<ipython-input-7-55fb94a8f3b1> in nalu_model()\r\n     48 def nalu_model():\r\n     49     inp = tf.keras.layers.Input(shape=(2,))\r\n---> 50     out = NALU(1)(inp)\r\n     51 \r\n     52     model = tf.keras.models.Model(inputs=inp, outputs=out)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    726         if all(hasattr(x, 'shape') for x in input_list):\r\n    727           input_shapes = nest.map_structure(lambda x: x.shape, inputs)\r\n--> 728         self.build(input_shapes)\r\n    729         self.built = True\r\n    730 \r\n\r\n<ipython-input-7-55fb94a8f3b1> in build(self, input_shape)\r\n      9         shape = tf.TensorShape((input_shape[1], self.num_outputs))\r\n     10         get = tf.keras.initializers.get\r\n---> 11         self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\r\n     12         self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\r\n     13         self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_variable(self, *args, **kwargs)\r\n    459   def add_variable(self, *args, **kwargs):\r\n    460     \"\"\"Alias for `add_weight`.\"\"\"\r\n--> 461     return self.add_weight(*args, **kwargs)\r\n    462 \r\n    463   def add_weight(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, getter)\r\n    563         use_resource=use_resource,\r\n    564         synchronization=synchronization,\r\n--> 565         aggregation=aggregation)\r\n    566 \r\n    567     if regularizer is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    533     new_variable = getter(\r\n    534         name=name, shape=shape, dtype=dtype, initializer=initializer,\r\n--> 535         **kwargs_for_getter)\r\n    536 \r\n    537     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in make_variable(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, synchronization, aggregation, partitioner)\r\n   1916       use_resource=use_resource,\r\n   1917       synchronization=synchronization,\r\n-> 1918       aggregation=aggregation)\r\n   1919   return v\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource, synchronization, aggregation)\r\n   2441       use_resource=use_resource,\r\n   2442       synchronization=synchronization,\r\n-> 2443       aggregation=aggregation)\r\n   2444 \r\n   2445 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>(**kwargs)\r\n   2423              synchronization=VariableSynchronization.AUTO,\r\n   2424              aggregation=VariableAggregation.NONE):\r\n-> 2425   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n   2426   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n   2427     previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\r\n   2393         collections=collections, validate_shape=validate_shape,\r\n   2394         caching_device=caching_device, name=name, dtype=dtype,\r\n-> 2395         constraint=constraint)\r\n   2396   elif not use_resource and context.executing_eagerly():\r\n   2397     raise RuntimeError(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)\r\n    310           name=name,\r\n    311           dtype=dtype,\r\n--> 312           constraint=constraint)\r\n    313 \r\n    314   # pylint: disable=unused-argument\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)\r\n    415               with ops.name_scope(\"Initializer\"), ops.device(None):\r\n    416                 initial_value = ops.convert_to_tensor(\r\n--> 417                     initial_value(), name=\"initial_value\", dtype=dtype)\r\n    418               self._handle = _eager_safe_variable_handle(\r\n    419                   shape=initial_value.get_shape(),\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in <lambda>()\r\n   1901         initializer = initializer(dtype=dtype)\r\n   1902       init_val = lambda: initializer(  # pylint: disable=g-long-lambda\r\n-> 1903           shape, dtype=dtype, partition_info=partition_info)\r\n   1904       variable_dtype = dtype.base_dtype\r\n   1905   if use_resource is None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)\r\n    474       scale /= max(1., fan_out)\r\n    475     else:\r\n--> 476       scale /= max(1., (fan_in + fan_out) / 2.)\r\n    477     if self.distribution == \"normal\" or self.distribution == \"truncated_normal\":\r\n    478       # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\r\n\r\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'float'\r\n```"}