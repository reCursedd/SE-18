{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1578", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1578/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1578/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1578/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1578", "id": 142647609, "node_id": "MDU6SXNzdWUxNDI2NDc2MDk=", "number": 1578, "title": "Release GPU memory after computation", "user": {"login": "shawnLeeZX", "id": 2428233, "node_id": "MDQ6VXNlcjI0MjgyMzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2428233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnLeeZX", "html_url": "https://github.com/shawnLeeZX", "followers_url": "https://api.github.com/users/shawnLeeZX/followers", "following_url": "https://api.github.com/users/shawnLeeZX/following{/other_user}", "gists_url": "https://api.github.com/users/shawnLeeZX/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnLeeZX/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnLeeZX/subscriptions", "organizations_url": "https://api.github.com/users/shawnLeeZX/orgs", "repos_url": "https://api.github.com/users/shawnLeeZX/repos", "events_url": "https://api.github.com/users/shawnLeeZX/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnLeeZX/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2016-03-22T13:19:43Z", "updated_at": "2018-06-24T06:22:45Z", "closed_at": "2016-03-22T14:56:15Z", "author_association": "NONE", "body_html": "<p>Is it possible to release all resources after computation?</p>\n<p>For example,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">10000000</span>):\n  t0 <span class=\"pl-k\">=</span> time.clock()\n\n  <span class=\"pl-k\">with</span> tf.Graph().as_default():\n    sess <span class=\"pl-k\">=</span> tf.Session()\n\n    a <span class=\"pl-k\">=</span> tf.placeholder(tf.int16, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>a<span class=\"pl-pds\">'</span></span>)\n    y <span class=\"pl-k\">=</span> tf.identity(a, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>)\n\n    sess.run(y, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{a: <span class=\"pl-c1\">3</span>})\n    sess.close()\n\n  time.sleep(<span class=\"pl-c1\">20</span>)\n\n<span class=\"pl-c1\">print</span> time.clock() <span class=\"pl-k\">-</span> t0</pre></div>\n<p>When the program is sleeping, I type <code>nvidia-smi</code>, and the memory is always occupied.</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">|</span> NVIDIA-SMI 352.79     Driver Version: 352.79         <span class=\"pl-k\">|</span>                       \n<span class=\"pl-k\">|</span>-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span> GPU  Name        Persistence-M<span class=\"pl-k\">|</span> Bus-Id        Disp.A <span class=\"pl-k\">|</span> Volatile Uncorr. ECC <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> Fan  Temp  Perf  Pwr:Usage/Cap<span class=\"pl-k\">|</span>         Memory-Usage <span class=\"pl-k\">|</span> GPU-Util  Compute M. <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>===============================+======================+======================<span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>   0  GeForce GTX TIT...  Off  <span class=\"pl-k\">|</span> 0000:03:00.0      On <span class=\"pl-k\">|</span>                  N/A <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> 29%   48C    P2    81W / 250W <span class=\"pl-k\">|</span>   5843MiB /  6143MiB <span class=\"pl-k\">|</span>      1%      Default <span class=\"pl-k\">|</span></pre></div>\n<p>The behavior I have observed is that only after the program exit, the memory is released. It makes using multiprocessing hard. Suppose one process is waited on a lock for another progress to finish, and all two processes need to join the main process. Then when process one release the lock, process two cannot get GPU memory, so it would fail.</p>\n<p>Is there any way to release memory, so when the above program(not the two process example) is sleeping, it will release memory?</p>\n<p>For bugs or installation issues, please provide the following information.<br>\nThe more information you provide, the more easily we will be able to offer<br>\nhelp and advice.</p>\n<h3>Environment info</h3>\n<p>Operating System:</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl</pre></div>\n<ol>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\".</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1</pre></div>", "body_text": "Is it possible to release all resources after computation?\nFor example,\nimport time\nimport tensorflow as tf\n\nfor i in range(0,10000000):\n  t0 = time.clock()\n\n  with tf.Graph().as_default():\n    sess = tf.Session()\n\n    a = tf.placeholder(tf.int16, name='a')\n    y = tf.identity(a, name='y')\n\n    sess.run(y, feed_dict={a: 3})\n    sess.close()\n\n  time.sleep(20)\n\nprint time.clock() - t0\nWhen the program is sleeping, I type nvidia-smi, and the memory is always occupied.\n| NVIDIA-SMI 352.79     Driver Version: 352.79         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:03:00.0      On |                  N/A |\n| 29%   48C    P2    81W / 250W |   5843MiB /  6143MiB |      1%      Default |\nThe behavior I have observed is that only after the program exit, the memory is released. It makes using multiprocessing hard. Suppose one process is waited on a lock for another progress to finish, and all two processes need to join the main process. Then when process one release the lock, process two cannot get GPU memory, so it would fail.\nIs there any way to release memory, so when the above program(not the two process example) is sleeping, it will release memory?\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\nEnvironment info\nOperating System:\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\n\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1", "body": "Is it possible to release all resources after computation?\n\nFor example,\n\n``` python\nimport time\nimport tensorflow as tf\n\nfor i in range(0,10000000):\n  t0 = time.clock()\n\n  with tf.Graph().as_default():\n    sess = tf.Session()\n\n    a = tf.placeholder(tf.int16, name='a')\n    y = tf.identity(a, name='y')\n\n    sess.run(y, feed_dict={a: 3})\n    sess.close()\n\n  time.sleep(20)\n\nprint time.clock() - t0\n```\n\nWhen the program is sleeping, I type `nvidia-smi`, and the memory is always occupied.\n\n``` bash\n| NVIDIA-SMI 352.79     Driver Version: 352.79         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:03:00.0      On |                  N/A |\n| 29%   48C    P2    81W / 250W |   5843MiB /  6143MiB |      1%      Default |\n```\n\nThe behavior I have observed is that only after the program exit, the memory is released. It makes using multiprocessing hard. Suppose one process is waited on a lock for another progress to finish, and all two processes need to join the main process. Then when process one release the lock, process two cannot get GPU memory, so it would fail.\n\nIs there any way to release memory, so when the above program(not the two process example) is sleeping, it will release memory? \n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n``` bash\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n```\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n``` bash\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1\n```\n"}