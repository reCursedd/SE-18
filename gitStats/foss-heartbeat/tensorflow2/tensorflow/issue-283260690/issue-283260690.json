{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15483", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15483/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15483/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15483/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15483", "id": 283260690, "node_id": "MDU6SXNzdWUyODMyNjA2OTA=", "number": 15483, "title": "What is the best practice for running training and evaluation on the same machine?", "user": {"login": "scotthuang1989", "id": 5325686, "node_id": "MDQ6VXNlcjUzMjU2ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/5325686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scotthuang1989", "html_url": "https://github.com/scotthuang1989", "followers_url": "https://api.github.com/users/scotthuang1989/followers", "following_url": "https://api.github.com/users/scotthuang1989/following{/other_user}", "gists_url": "https://api.github.com/users/scotthuang1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/scotthuang1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scotthuang1989/subscriptions", "organizations_url": "https://api.github.com/users/scotthuang1989/orgs", "repos_url": "https://api.github.com/users/scotthuang1989/repos", "events_url": "https://api.github.com/users/scotthuang1989/events{/privacy}", "received_events_url": "https://api.github.com/users/scotthuang1989/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-19T15:00:54Z", "updated_at": "2017-12-19T17:40:18Z", "closed_at": "2017-12-19T17:40:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I ask the question at <a href=\"https://stackoverflow.com/questions/47883570/what-is-the-best-practice-for-running-training-and-evaluation-on-the-same-machin\" rel=\"nofollow\">stackoverflow</a>. No answer so far, maybe I can find people have similar needs.</p>\n<p>Here is the question:</p>\n<p><strong>What I want to do?</strong></p>\n<ol>\n<li>\n<p>I only have 1 machine.</p>\n</li>\n<li>\n<p>I want to evaluate the mode periodically.</p>\n</li>\n</ol>\n<p><strong>What I have now?</strong></p>\n<ol>\n<li>\n<p>use a placeholder. Say I run 1000 step of training by feeding the training data. then I feed in validation dataset for evaluation. put it in a loop.</p>\n<p>But as google suggested, placeholder is not a good way for long run training.</p>\n</li>\n<li>\n<p>So, I use slim dataset to feed in data. Now, the model is bonded with training dataset like this:</p>\n<blockquote>\n<pre><code> net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\n                            scope='conv1')\n</code></pre>\n</blockquote>\n</li>\n</ol>\n<p>I have to construct another model(in another graph) which is bonded with validation dataset.</p>\n<p><strong>Is there a better way of doing that?</strong></p>\n<p>I know that google is focusing on distribution training on large scale, but I think as tensorflow  is a low-level and flexible framwork. There must be a way can do what I want.</p>", "body_text": "I ask the question at stackoverflow. No answer so far, maybe I can find people have similar needs.\nHere is the question:\nWhat I want to do?\n\n\nI only have 1 machine.\n\n\nI want to evaluate the mode periodically.\n\n\nWhat I have now?\n\n\nuse a placeholder. Say I run 1000 step of training by feeding the training data. then I feed in validation dataset for evaluation. put it in a loop.\nBut as google suggested, placeholder is not a good way for long run training.\n\n\nSo, I use slim dataset to feed in data. Now, the model is bonded with training dataset like this:\n\n net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\n                            scope='conv1')\n\n\n\n\nI have to construct another model(in another graph) which is bonded with validation dataset.\nIs there a better way of doing that?\nI know that google is focusing on distribution training on large scale, but I think as tensorflow  is a low-level and flexible framwork. There must be a way can do what I want.", "body": "I ask the question at [stackoverflow](https://stackoverflow.com/questions/47883570/what-is-the-best-practice-for-running-training-and-evaluation-on-the-same-machin). No answer so far, maybe I can find people have similar needs.\r\n\r\nHere is the question:\r\n\r\n**What I want to do?**\r\n\r\n1. I only have 1 machine. \r\n\r\n2. I want to evaluate the mode periodically. \r\n\r\n**What I have now?** \r\n\r\n\r\n1.  use a placeholder. Say I run 1000 step of training by feeding the training data. then I feed in validation dataset for evaluation. put it in a loop.\r\n\r\n    But as google suggested, placeholder is not a good way for long run training.\r\n\r\n\r\n2. So, I use slim dataset to feed in data. Now, the model is bonded with training dataset like this:\r\n    >      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\r\n    >                                 scope='conv1')\r\n\r\n I have to construct another model(in another graph) which is bonded with validation dataset. \r\n\r\n**Is there a better way of doing that?**\r\n\r\nI know that google is focusing on distribution training on large scale, but I think as tensorflow  is a low-level and flexible framwork. There must be a way can do what I want. "}