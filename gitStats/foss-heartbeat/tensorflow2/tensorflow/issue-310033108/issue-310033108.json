{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18117", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18117/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18117/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18117/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18117", "id": 310033108, "node_id": "MDU6SXNzdWUzMTAwMzMxMDg=", "number": 18117, "title": "unexpected 10 sec hang, related to tf.TensorArray", "user": {"login": "albertz", "id": 59132, "node_id": "MDQ6VXNlcjU5MTMy", "avatar_url": "https://avatars0.githubusercontent.com/u/59132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertz", "html_url": "https://github.com/albertz", "followers_url": "https://api.github.com/users/albertz/followers", "following_url": "https://api.github.com/users/albertz/following{/other_user}", "gists_url": "https://api.github.com/users/albertz/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertz/subscriptions", "organizations_url": "https://api.github.com/users/albertz/orgs", "repos_url": "https://api.github.com/users/albertz/repos", "events_url": "https://api.github.com/users/albertz/events{/privacy}", "received_events_url": "https://api.github.com/users/albertz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-03-30T10:26:12Z", "updated_at": "2018-04-11T00:02:12Z", "closed_at": "2018-04-11T00:02:12Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes, code is below</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Debian 9.4, Linux wc4 4.9.0-6-amd64  SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.5.0-0-g37aa430d84 1.5.0</li>\n<li><strong>Python version</strong>: 3.5.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: -</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: -</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080, 11GB</li>\n<li><strong>Exact command to reproduce</strong>: see code below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The test case below causes an unexpected hang for about 10 seconds.<br>\nI think it is related to <code>tf.TensorArray</code> but also the other operations in the graph are important.<br>\nI have already tried to reduce it as much as possible. If I remove something now,<br>\nthe hang will disappear.</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nimport contextlib\nimport numpy\n\n@contextlib.contextmanager\ndef make_scope():\n  \"\"\"\n  :rtype: tf.Session\n  \"\"\"\n  with tf.Graph().as_default() as graph:\n    with tf.Session(graph=graph) as session:\n      yield session\n\n\ndef test_slow_TensorArray():\n  import time\n  random = numpy.random.RandomState(seed=1)\n  num_inputs = 4\n  num_outputs = 3\n  seq_len = 10\n  limit = 1.0\n\n  def linear(x, output_dim):\n    input_dim = x.get_shape().dims[-1].value\n    assert input_dim is not None\n    with tf.variable_scope(\"linear\", reuse=tf.AUTO_REUSE):\n      weights = tf.get_variable(\"W\", shape=(input_dim, output_dim))\n      bias = tf.get_variable(\"b\", shape=(output_dim,))\n    assert x.get_shape().ndims == 2  # (batch,input_dim)\n    return tf.matmul(x, weights) + bias\n\n  with make_scope() as session:\n    print(\"create graph\")\n    src_placeholder = tf.placeholder(tf.float32, (None, seq_len, num_inputs), name=\"src_placeholder\")\n    tgt_placeholder = tf.placeholder(tf.float32, (None, seq_len, num_outputs), name=\"tgt_placeholder\")\n    batch_size = tf.shape(src_placeholder)[0]\n\n    def make_feed_dict():\n      return {\n        src_placeholder: random.uniform(-limit, limit, (1, seq_len, num_inputs)),\n        tgt_placeholder: random.uniform(-limit, limit, (1, seq_len, num_outputs)),\n      }\n\n    state = tf.zeros((batch_size, num_outputs))\n    loss_ta = tf.TensorArray(tf.float32, size=seq_len, element_shape=(None,))\n    # Unroll the loop here.\n    for f in range(seq_len):\n      inputs = src_placeholder[:, f]\n      x = tf.concat([inputs, state], axis=-1)\n      with tf.variable_scope('h'):\n        h = tf.tanh(linear(x, num_outputs))\n      with tf.variable_scope('t'):\n        t = tf.sigmoid(linear(x, num_outputs))\n      state += t * (h - state)\n      frame_loss = tf.reduce_mean(tf.squared_difference(tgt_placeholder[:, f], state), axis=1)\n      assert frame_loss.get_shape().ndims == 1  # (batch,)\n      loss_ta = loss_ta.write(f, frame_loss)\n    loss = tf.reduce_sum(loss_ta.stack())\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.1, epsilon=1e-16, use_locking=False)\n    minimize_op = optimizer.minimize(loss)\n\n    print('variables:')\n    train_vars = (\n      tf.trainable_variables() +\n      tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n    print(train_vars)\n    print('init vars')\n    session.run(tf.global_variables_initializer())\n    print('graph size:', session.graph_def.ByteSize())\n    print('train')\n    for s in range(10):\n      start_time = time.time()\n      loss_val, _ = session.run([loss, minimize_op], feed_dict=make_feed_dict())\n      print(\"step %i, loss: %f, time: %f\" % (s, loss_val, time.time() - start_time))\n\ntest_slow_TensorArray()\n</code></pre>\n<p>My output:</p>\n<pre><code>create graph\nvariables:\n[&lt;tf.Variable 'h/linear/W:0' shape=(7, 3) dtype=float32_ref&gt;, &lt;tf.Variable 'h/linear/b:0' shape=(3,) dtype=float32_ref&gt;, &lt;tf.Variable 't/linear/W:0' shape=(7, 3) dtype=float32_ref&gt;, &lt;tf.Variable 't/linear/b:0' shape=(3,) dtype=float32_ref&gt;]\ninit vars\ngraph size: 222234\ntrain\nstep 0, loss: 5.506713, time: 10.675434\nstep 1, loss: 7.865020, time: 0.003913\nstep 2, loss: 5.450877, time: 0.003354\nstep 3, loss: 3.361173, time: 0.003227\nstep 4, loss: 4.493120, time: 0.003563\nstep 5, loss: 5.137649, time: 0.003203\nstep 6, loss: 3.610677, time: 0.003376\nstep 7, loss: 3.657249, time: 0.003544\nstep 8, loss: 4.405594, time: 0.003454\nstep 9, loss: 4.380188, time: 0.003491\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, code is below\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.4, Linux wc4 4.9.0-6-amd64  SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\nTensorFlow installed from (source or binary): binary pip\nTensorFlow version (use command below): v1.5.0-0-g37aa430d84 1.5.0\nPython version: 3.5.3\nBazel version (if compiling from source): -\nGCC/Compiler version (if compiling from source): -\nCUDA/cuDNN version: 9.0\nGPU model and memory: GeForce GTX 1080, 11GB\nExact command to reproduce: see code below\n\nDescribe the problem\nThe test case below causes an unexpected hang for about 10 seconds.\nI think it is related to tf.TensorArray but also the other operations in the graph are important.\nI have already tried to reduce it as much as possible. If I remove something now,\nthe hang will disappear.\nSource code / logs\nimport tensorflow as tf\nimport contextlib\nimport numpy\n\n@contextlib.contextmanager\ndef make_scope():\n  \"\"\"\n  :rtype: tf.Session\n  \"\"\"\n  with tf.Graph().as_default() as graph:\n    with tf.Session(graph=graph) as session:\n      yield session\n\n\ndef test_slow_TensorArray():\n  import time\n  random = numpy.random.RandomState(seed=1)\n  num_inputs = 4\n  num_outputs = 3\n  seq_len = 10\n  limit = 1.0\n\n  def linear(x, output_dim):\n    input_dim = x.get_shape().dims[-1].value\n    assert input_dim is not None\n    with tf.variable_scope(\"linear\", reuse=tf.AUTO_REUSE):\n      weights = tf.get_variable(\"W\", shape=(input_dim, output_dim))\n      bias = tf.get_variable(\"b\", shape=(output_dim,))\n    assert x.get_shape().ndims == 2  # (batch,input_dim)\n    return tf.matmul(x, weights) + bias\n\n  with make_scope() as session:\n    print(\"create graph\")\n    src_placeholder = tf.placeholder(tf.float32, (None, seq_len, num_inputs), name=\"src_placeholder\")\n    tgt_placeholder = tf.placeholder(tf.float32, (None, seq_len, num_outputs), name=\"tgt_placeholder\")\n    batch_size = tf.shape(src_placeholder)[0]\n\n    def make_feed_dict():\n      return {\n        src_placeholder: random.uniform(-limit, limit, (1, seq_len, num_inputs)),\n        tgt_placeholder: random.uniform(-limit, limit, (1, seq_len, num_outputs)),\n      }\n\n    state = tf.zeros((batch_size, num_outputs))\n    loss_ta = tf.TensorArray(tf.float32, size=seq_len, element_shape=(None,))\n    # Unroll the loop here.\n    for f in range(seq_len):\n      inputs = src_placeholder[:, f]\n      x = tf.concat([inputs, state], axis=-1)\n      with tf.variable_scope('h'):\n        h = tf.tanh(linear(x, num_outputs))\n      with tf.variable_scope('t'):\n        t = tf.sigmoid(linear(x, num_outputs))\n      state += t * (h - state)\n      frame_loss = tf.reduce_mean(tf.squared_difference(tgt_placeholder[:, f], state), axis=1)\n      assert frame_loss.get_shape().ndims == 1  # (batch,)\n      loss_ta = loss_ta.write(f, frame_loss)\n    loss = tf.reduce_sum(loss_ta.stack())\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.1, epsilon=1e-16, use_locking=False)\n    minimize_op = optimizer.minimize(loss)\n\n    print('variables:')\n    train_vars = (\n      tf.trainable_variables() +\n      tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n    print(train_vars)\n    print('init vars')\n    session.run(tf.global_variables_initializer())\n    print('graph size:', session.graph_def.ByteSize())\n    print('train')\n    for s in range(10):\n      start_time = time.time()\n      loss_val, _ = session.run([loss, minimize_op], feed_dict=make_feed_dict())\n      print(\"step %i, loss: %f, time: %f\" % (s, loss_val, time.time() - start_time))\n\ntest_slow_TensorArray()\n\nMy output:\ncreate graph\nvariables:\n[<tf.Variable 'h/linear/W:0' shape=(7, 3) dtype=float32_ref>, <tf.Variable 'h/linear/b:0' shape=(3,) dtype=float32_ref>, <tf.Variable 't/linear/W:0' shape=(7, 3) dtype=float32_ref>, <tf.Variable 't/linear/b:0' shape=(3,) dtype=float32_ref>]\ninit vars\ngraph size: 222234\ntrain\nstep 0, loss: 5.506713, time: 10.675434\nstep 1, loss: 7.865020, time: 0.003913\nstep 2, loss: 5.450877, time: 0.003354\nstep 3, loss: 3.361173, time: 0.003227\nstep 4, loss: 4.493120, time: 0.003563\nstep 5, loss: 5.137649, time: 0.003203\nstep 6, loss: 3.610677, time: 0.003376\nstep 7, loss: 3.657249, time: 0.003544\nstep 8, loss: 4.405594, time: 0.003454\nstep 9, loss: 4.380188, time: 0.003491", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, code is below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9.4, Linux wc4 4.9.0-6-amd64  SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\r\n- **TensorFlow installed from (source or binary)**: binary pip\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GeForce GTX 1080, 11GB\r\n- **Exact command to reproduce**: see code below\r\n\r\n### Describe the problem\r\nThe test case below causes an unexpected hang for about 10 seconds.\r\nI think it is related to `tf.TensorArray` but also the other operations in the graph are important.\r\nI have already tried to reduce it as much as possible. If I remove something now,\r\nthe hang will disappear.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport contextlib\r\nimport numpy\r\n\r\n@contextlib.contextmanager\r\ndef make_scope():\r\n  \"\"\"\r\n  :rtype: tf.Session\r\n  \"\"\"\r\n  with tf.Graph().as_default() as graph:\r\n    with tf.Session(graph=graph) as session:\r\n      yield session\r\n\r\n\r\ndef test_slow_TensorArray():\r\n  import time\r\n  random = numpy.random.RandomState(seed=1)\r\n  num_inputs = 4\r\n  num_outputs = 3\r\n  seq_len = 10\r\n  limit = 1.0\r\n\r\n  def linear(x, output_dim):\r\n    input_dim = x.get_shape().dims[-1].value\r\n    assert input_dim is not None\r\n    with tf.variable_scope(\"linear\", reuse=tf.AUTO_REUSE):\r\n      weights = tf.get_variable(\"W\", shape=(input_dim, output_dim))\r\n      bias = tf.get_variable(\"b\", shape=(output_dim,))\r\n    assert x.get_shape().ndims == 2  # (batch,input_dim)\r\n    return tf.matmul(x, weights) + bias\r\n\r\n  with make_scope() as session:\r\n    print(\"create graph\")\r\n    src_placeholder = tf.placeholder(tf.float32, (None, seq_len, num_inputs), name=\"src_placeholder\")\r\n    tgt_placeholder = tf.placeholder(tf.float32, (None, seq_len, num_outputs), name=\"tgt_placeholder\")\r\n    batch_size = tf.shape(src_placeholder)[0]\r\n\r\n    def make_feed_dict():\r\n      return {\r\n        src_placeholder: random.uniform(-limit, limit, (1, seq_len, num_inputs)),\r\n        tgt_placeholder: random.uniform(-limit, limit, (1, seq_len, num_outputs)),\r\n      }\r\n\r\n    state = tf.zeros((batch_size, num_outputs))\r\n    loss_ta = tf.TensorArray(tf.float32, size=seq_len, element_shape=(None,))\r\n    # Unroll the loop here.\r\n    for f in range(seq_len):\r\n      inputs = src_placeholder[:, f]\r\n      x = tf.concat([inputs, state], axis=-1)\r\n      with tf.variable_scope('h'):\r\n        h = tf.tanh(linear(x, num_outputs))\r\n      with tf.variable_scope('t'):\r\n        t = tf.sigmoid(linear(x, num_outputs))\r\n      state += t * (h - state)\r\n      frame_loss = tf.reduce_mean(tf.squared_difference(tgt_placeholder[:, f], state), axis=1)\r\n      assert frame_loss.get_shape().ndims == 1  # (batch,)\r\n      loss_ta = loss_ta.write(f, frame_loss)\r\n    loss = tf.reduce_sum(loss_ta.stack())\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.1, epsilon=1e-16, use_locking=False)\r\n    minimize_op = optimizer.minimize(loss)\r\n\r\n    print('variables:')\r\n    train_vars = (\r\n      tf.trainable_variables() +\r\n      tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\r\n    print(train_vars)\r\n    print('init vars')\r\n    session.run(tf.global_variables_initializer())\r\n    print('graph size:', session.graph_def.ByteSize())\r\n    print('train')\r\n    for s in range(10):\r\n      start_time = time.time()\r\n      loss_val, _ = session.run([loss, minimize_op], feed_dict=make_feed_dict())\r\n      print(\"step %i, loss: %f, time: %f\" % (s, loss_val, time.time() - start_time))\r\n\r\ntest_slow_TensorArray()\r\n```\r\n\r\nMy output:\r\n\r\n```\r\ncreate graph\r\nvariables:\r\n[<tf.Variable 'h/linear/W:0' shape=(7, 3) dtype=float32_ref>, <tf.Variable 'h/linear/b:0' shape=(3,) dtype=float32_ref>, <tf.Variable 't/linear/W:0' shape=(7, 3) dtype=float32_ref>, <tf.Variable 't/linear/b:0' shape=(3,) dtype=float32_ref>]\r\ninit vars\r\ngraph size: 222234\r\ntrain\r\nstep 0, loss: 5.506713, time: 10.675434\r\nstep 1, loss: 7.865020, time: 0.003913\r\nstep 2, loss: 5.450877, time: 0.003354\r\nstep 3, loss: 3.361173, time: 0.003227\r\nstep 4, loss: 4.493120, time: 0.003563\r\nstep 5, loss: 5.137649, time: 0.003203\r\nstep 6, loss: 3.610677, time: 0.003376\r\nstep 7, loss: 3.657249, time: 0.003544\r\nstep 8, loss: 4.405594, time: 0.003454\r\nstep 9, loss: 4.380188, time: 0.003491\r\n```\r\n"}