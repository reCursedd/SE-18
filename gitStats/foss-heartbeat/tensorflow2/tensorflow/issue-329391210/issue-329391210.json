{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19771", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19771/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19771/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19771/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19771", "id": 329391210, "node_id": "MDU6SXNzdWUzMjkzOTEyMTA=", "number": 19771, "title": "tensordot/conj interplay", "user": {"login": "mrader1248", "id": 32457308, "node_id": "MDQ6VXNlcjMyNDU3MzA4", "avatar_url": "https://avatars1.githubusercontent.com/u/32457308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrader1248", "html_url": "https://github.com/mrader1248", "followers_url": "https://api.github.com/users/mrader1248/followers", "following_url": "https://api.github.com/users/mrader1248/following{/other_user}", "gists_url": "https://api.github.com/users/mrader1248/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrader1248/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrader1248/subscriptions", "organizations_url": "https://api.github.com/users/mrader1248/orgs", "repos_url": "https://api.github.com/users/mrader1248/repos", "events_url": "https://api.github.com/users/mrader1248/events{/privacy}", "received_events_url": "https://api.github.com/users/mrader1248/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-06-05T10:16:22Z", "updated_at": "2018-11-20T07:53:36Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: CentOS Linux 7, Kernel 3.10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.8.0-0-g93bc2e2072 1.8.0</li>\n<li><strong>Python version</strong>: Python 3.6.2 :: Continuum Analytics, Inc.</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0.176 (but not in use)</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080 (but not in use)</li>\n<li><strong>Exact command to reproduce</strong>: execute python script below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am using the code below to contract four tensors <code>A[l,m,n] B[q,l,i] C[j,p,m,q] conj(D[p,n,k])</code> using a series of tensordot operations. To verify the result, I compare tensorflow with pure numpy. It seems like the <code>tf.conj</code> node is ignored (or an additional conjugation is executed) when executing tensorflow's <code>tensordot</code>s, as the first <code>print</code> gives me <code>False</code> and the second one <code>True</code>, although it should be the other way round. Just as a remark: The third part of my code uses <code>einsum</code> instead and works as intended (output of the third <code>print</code> should be <code>True</code> and is indeed <code>True</code>). Thank you for your help!</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nm <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\ndtype <span class=\"pl-k\">=</span> np.complex128\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## generate random tensors ###</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">rand</span>(<span class=\"pl-k\">*</span><span class=\"pl-smi\">shape</span>):\n    <span class=\"pl-k\">return</span> np.asarray(\n        np.random.random(shape) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1<span class=\"pl-k\">j</span></span><span class=\"pl-k\">*</span>np.random.random(shape),\n        dtype\n    )\n\nA <span class=\"pl-k\">=</span> rand(n, m, n)\nB <span class=\"pl-k\">=</span> rand(m, n, n)\nC <span class=\"pl-k\">=</span> rand(m, m, m, m)\nD <span class=\"pl-k\">=</span> rand(m, n, n)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## perform numpy computation ###</span>\n\nX_np <span class=\"pl-k\">=</span> np.tensordot(A, B, (<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>))\nY_np <span class=\"pl-k\">=</span> np.tensordot(X_np, C, ((<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">2</span>),(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>)))\nZ_np <span class=\"pl-k\">=</span> np.tensordot(Y_np, D.conj(), ((<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">0</span>),(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>)))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## build tensorflow computation graph ###</span>\n\nA_node <span class=\"pl-k\">=</span> tf.placeholder(dtype, (n, m, n))\nB_node <span class=\"pl-k\">=</span> tf.placeholder(dtype, (m, n, n))\nC_node <span class=\"pl-k\">=</span> tf.placeholder(dtype, (m, m, m, m))\nD_node <span class=\"pl-k\">=</span> tf.placeholder(dtype, (m, n, n))\n\nX_node <span class=\"pl-k\">=</span> tf.tensordot(A_node, B_node, (<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>))\nY_node <span class=\"pl-k\">=</span> tf.tensordot(X_node, C_node, ((<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">2</span>),(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>)))\nZ_node <span class=\"pl-k\">=</span> tf.tensordot(Y_node, tf.conj(D_node), ((<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">0</span>),(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>)))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## run tensorflow computation ###</span>\n\nsession <span class=\"pl-k\">=</span> tf.Session()\nZ_tf <span class=\"pl-k\">=</span> session.run(\n    Z_node, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{A_node: A, B_node: B, C_node: C, D_node: D}\n)\nsession.close()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## compare results ###</span>\n\n<span class=\"pl-c1\">print</span>(np.allclose(Z_np, Z_tf))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## run tensorflow computation again with numpy conj ###</span>\n\nsession <span class=\"pl-k\">=</span> tf.Session()\nZ_tf <span class=\"pl-k\">=</span> session.run(\n    Z_node, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{A_node: A, B_node: B, C_node: C, D_node: D.conj()}\n)\nsession.close()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## compare results ###</span>\n\n<span class=\"pl-c1\">print</span>(np.allclose(Z_np, Z_tf))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## build tensorflow computation graph using einsum ###</span>\n\nW_node <span class=\"pl-k\">=</span> tf.einsum(\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lmn,qli,jpmq,pnk-&gt;ijk<span class=\"pl-pds\">\"</span></span>, A_node, B_node, C_node, tf.conj(D_node)\n)\n\nsession <span class=\"pl-k\">=</span> tf.Session()\nW_tf <span class=\"pl-k\">=</span> session.run(\n    W_node, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{A_node: A, B_node: B, C_node: C, D_node: D}\n)\nsession.close()\n\n<span class=\"pl-c1\">print</span>(np.allclose(Z_np, W_tf))</pre></div>\n<h3>Output</h3>\n<pre><code>False\nTrue\nTrue\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7, Kernel 3.10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0\nPython version: Python 3.6.2 :: Continuum Analytics, Inc.\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: CUDA 9.0.176 (but not in use)\nGPU model and memory: GeForce GTX 1080 (but not in use)\nExact command to reproduce: execute python script below\n\nDescribe the problem\nI am using the code below to contract four tensors A[l,m,n] B[q,l,i] C[j,p,m,q] conj(D[p,n,k]) using a series of tensordot operations. To verify the result, I compare tensorflow with pure numpy. It seems like the tf.conj node is ignored (or an additional conjugation is executed) when executing tensorflow's tensordots, as the first print gives me False and the second one True, although it should be the other way round. Just as a remark: The third part of my code uses einsum instead and works as intended (output of the third print should be True and is indeed True). Thank you for your help!\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\n\nn = 100\nm = 4\ndtype = np.complex128\n\n### generate random tensors ###\n\ndef rand(*shape):\n    return np.asarray(\n        np.random.random(shape) + 1j*np.random.random(shape),\n        dtype\n    )\n\nA = rand(n, m, n)\nB = rand(m, n, n)\nC = rand(m, m, m, m)\nD = rand(m, n, n)\n\n### perform numpy computation ###\n\nX_np = np.tensordot(A, B, (0,1))\nY_np = np.tensordot(X_np, C, ((0,2),(2,3)))\nZ_np = np.tensordot(Y_np, D.conj(), ((3,0),(0,1)))\n\n### build tensorflow computation graph ###\n\nA_node = tf.placeholder(dtype, (n, m, n))\nB_node = tf.placeholder(dtype, (m, n, n))\nC_node = tf.placeholder(dtype, (m, m, m, m))\nD_node = tf.placeholder(dtype, (m, n, n))\n\nX_node = tf.tensordot(A_node, B_node, (0,1))\nY_node = tf.tensordot(X_node, C_node, ((0,2),(2,3)))\nZ_node = tf.tensordot(Y_node, tf.conj(D_node), ((3,0),(0,1)))\n\n### run tensorflow computation ###\n\nsession = tf.Session()\nZ_tf = session.run(\n    Z_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D}\n)\nsession.close()\n\n### compare results ###\n\nprint(np.allclose(Z_np, Z_tf))\n\n### run tensorflow computation again with numpy conj ###\n\nsession = tf.Session()\nZ_tf = session.run(\n    Z_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D.conj()}\n)\nsession.close()\n\n### compare results ###\n\nprint(np.allclose(Z_np, Z_tf))\n\n### build tensorflow computation graph using einsum ###\n\nW_node = tf.einsum(\n    \"lmn,qli,jpmq,pnk->ijk\", A_node, B_node, C_node, tf.conj(D_node)\n)\n\nsession = tf.Session()\nW_tf = session.run(\n    W_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D}\n)\nsession.close()\n\nprint(np.allclose(Z_np, W_tf))\nOutput\nFalse\nTrue\nTrue", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7, Kernel 3.10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.2 :: Continuum Analytics, Inc.\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0.176 (but not in use)\r\n- **GPU model and memory**: GeForce GTX 1080 (but not in use)\r\n- **Exact command to reproduce**: execute python script below\r\n\r\n### Describe the problem\r\nI am using the code below to contract four tensors `A[l,m,n] B[q,l,i] C[j,p,m,q] conj(D[p,n,k])` using a series of tensordot operations. To verify the result, I compare tensorflow with pure numpy. It seems like the `tf.conj` node is ignored (or an additional conjugation is executed) when executing tensorflow's `tensordot`s, as the first `print` gives me `False` and the second one `True`, although it should be the other way round. Just as a remark: The third part of my code uses `einsum` instead and works as intended (output of the third `print` should be `True` and is indeed `True`). Thank you for your help!\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nn = 100\r\nm = 4\r\ndtype = np.complex128\r\n\r\n### generate random tensors ###\r\n\r\ndef rand(*shape):\r\n    return np.asarray(\r\n        np.random.random(shape) + 1j*np.random.random(shape),\r\n        dtype\r\n    )\r\n\r\nA = rand(n, m, n)\r\nB = rand(m, n, n)\r\nC = rand(m, m, m, m)\r\nD = rand(m, n, n)\r\n\r\n### perform numpy computation ###\r\n\r\nX_np = np.tensordot(A, B, (0,1))\r\nY_np = np.tensordot(X_np, C, ((0,2),(2,3)))\r\nZ_np = np.tensordot(Y_np, D.conj(), ((3,0),(0,1)))\r\n\r\n### build tensorflow computation graph ###\r\n\r\nA_node = tf.placeholder(dtype, (n, m, n))\r\nB_node = tf.placeholder(dtype, (m, n, n))\r\nC_node = tf.placeholder(dtype, (m, m, m, m))\r\nD_node = tf.placeholder(dtype, (m, n, n))\r\n\r\nX_node = tf.tensordot(A_node, B_node, (0,1))\r\nY_node = tf.tensordot(X_node, C_node, ((0,2),(2,3)))\r\nZ_node = tf.tensordot(Y_node, tf.conj(D_node), ((3,0),(0,1)))\r\n\r\n### run tensorflow computation ###\r\n\r\nsession = tf.Session()\r\nZ_tf = session.run(\r\n    Z_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D}\r\n)\r\nsession.close()\r\n\r\n### compare results ###\r\n\r\nprint(np.allclose(Z_np, Z_tf))\r\n\r\n### run tensorflow computation again with numpy conj ###\r\n\r\nsession = tf.Session()\r\nZ_tf = session.run(\r\n    Z_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D.conj()}\r\n)\r\nsession.close()\r\n\r\n### compare results ###\r\n\r\nprint(np.allclose(Z_np, Z_tf))\r\n\r\n### build tensorflow computation graph using einsum ###\r\n\r\nW_node = tf.einsum(\r\n    \"lmn,qli,jpmq,pnk->ijk\", A_node, B_node, C_node, tf.conj(D_node)\r\n)\r\n\r\nsession = tf.Session()\r\nW_tf = session.run(\r\n    W_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D}\r\n)\r\nsession.close()\r\n\r\nprint(np.allclose(Z_np, W_tf))\r\n```\r\n### Output\r\n```\r\nFalse\r\nTrue\r\nTrue\r\n```\r\n"}