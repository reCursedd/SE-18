{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12712", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12712/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12712/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12712/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12712", "id": 254131369, "node_id": "MDU6SXNzdWUyNTQxMzEzNjk=", "number": 12712, "title": "sharing variables but matrices are transposed even though src & dst tensors appear to have same shape", "user": {"login": "memo", "id": 144230, "node_id": "MDQ6VXNlcjE0NDIzMA==", "avatar_url": "https://avatars0.githubusercontent.com/u/144230?v=4", "gravatar_id": "", "url": "https://api.github.com/users/memo", "html_url": "https://github.com/memo", "followers_url": "https://api.github.com/users/memo/followers", "following_url": "https://api.github.com/users/memo/following{/other_user}", "gists_url": "https://api.github.com/users/memo/gists{/gist_id}", "starred_url": "https://api.github.com/users/memo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/memo/subscriptions", "organizations_url": "https://api.github.com/users/memo/orgs", "repos_url": "https://api.github.com/users/memo/repos", "events_url": "https://api.github.com/users/memo/events{/privacy}", "received_events_url": "https://api.github.com/users/memo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-30T20:50:55Z", "updated_at": "2017-08-31T08:48:23Z", "closed_at": "2017-08-31T08:46:51Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary (pip install tensorflow-gpu==1.2.1)</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\nv1.2.0-5-g435cdfc, 1.2.1</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\nPython 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)<br>\nType \"copyright\", \"credits\" or \"license\" for more information.<br>\nIPython 5.3.0 -- An enhanced Interactive Python.</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nCuda V8.0.61, CuDNN 5.1</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nGeForce GTX 1080, 8GB</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>At first I thought I was doing something wrong (initially posted on <a href=\"https://stackoverflow.com/questions/45969089/trying-to-share-tensorflow-variables-between-autoencoder-and-decoder-but-weight\" rel=\"nofollow\">stackoverflow</a>), but now I think this is a bug. Full text from SO pasted below.</p>\n<p>I want to share variables between an autoencoder and a decoder. But the weights matrix from z to the first fully connected flat layer is transposed, even though at every step of the graph construction I dump the previous tensor scopename and shape to the console, and in both cases ('autoencoder' and 'decoder') the scopenames and shapes are identical.</p>\n<p>To be specific the error is</p>\n<blockquote>\n<p>Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).</p>\n</blockquote>\n<p>But in both cases I'm creating a dense layer from (?, 128) to units=65536 with the same code:</p>\n<pre><code>print('dense from {} to {}'.format(t, post_z_flat_dim))\nt = tf.layers.dense(inputs=t, units=post_z_flat_dim, ...)\n</code></pre>\n<p>which gives the output</p>\n<blockquote>\n<p>autoencoder: dense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536</p>\n</blockquote>\n<blockquote>\n<p>decoder: dense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536</p>\n</blockquote>\n<p>The only difference is, for the decoder t is a placeholder whereas for autoencoder it's the result of a sequence of operations (but still of shape [None, 128])</p>\n<p>Relevant code is below followed by the output.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,\n                 <span class=\"pl-smi\">usage</span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span> Usage enum</span>\n                 <span class=\"pl-smi\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span> instance of another model to reuse sess, graph and variables from</span>\n                 ):\n\n        <span class=\"pl-k\">if</span> reuse <span class=\"pl-k\">==</span> <span class=\"pl-c1\">None</span>:\n            config <span class=\"pl-k\">=</span> tf.ConfigProto()\n            config.gpu_options.allow_growth<span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>    \n            <span class=\"pl-c1\">self</span>.graph <span class=\"pl-k\">=</span> tf.Graph()\n            <span class=\"pl-c1\">self</span>.sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.graph, <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\n            root_scope <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.name\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">self</span>.graph <span class=\"pl-k\">=</span> reuse.graph\n            <span class=\"pl-c1\">self</span>.sess <span class=\"pl-k\">=</span> reuse.sess\n            root_scope <span class=\"pl-k\">=</span> reuse.name\n\n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">self</span>.graph.as_default():\n            <span class=\"pl-k\">with</span> tf.variable_scope(root_scope, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span>(reuse <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">None</span>)):\n                \n                <span class=\"pl-k\">if</span> usage <span class=\"pl-k\">&gt;=</span> Usage.autoencoder:\n                    <span class=\"pl-c1\">self</span>.x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, (<span class=\"pl-c1\">None</span>,) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">tuple</span>(img_shape), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>)\n                    t <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.x        \n                    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&gt; x<span class=\"pl-pds\">'</span></span>, t.shape, t.name)\n\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ENCODER</span>\n                    <span class=\"pl-k\">for</span> i, filter_depth <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(filters):\n                        name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>encoder<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(i)\n                        <span class=\"pl-k\">with</span> tf.variable_scope(name):         \n                            t <span class=\"pl-k\">=</span> tf.layers.conv2d(<span class=\"pl-c1\">...</span>)                \n                            <span class=\"pl-k\">if</span> usage <span class=\"pl-k\">&gt;=</span> Usage.trainer: t <span class=\"pl-k\">=</span> tf.layers.dropout(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dropout_conv_amt_T, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&gt; conv<span class=\"pl-pds\">'</span></span>, t.shape, t.name)\n                            \n                 \n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Z    </span>\n                <span class=\"pl-k\">if</span> usage <span class=\"pl-k\">&gt;=</span> Usage.autoencoder:\n                    t <span class=\"pl-k\">=</span> tf.contrib.layers.flatten(t)\n                    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&gt; pre z flat<span class=\"pl-pds\">'</span></span>, t.shape, t.name)\n\n                    <span class=\"pl-k\">if</span> usage <span class=\"pl-k\">&gt;=</span> Usage.trainer:\n                        t <span class=\"pl-k\">=</span> tf.layers.dropout(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dropout_fc_amt_T, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                    \n                    <span class=\"pl-k\">def</span> <span class=\"pl-en\">vae_z</span>(<span class=\"pl-smi\">t</span>):\n                        <span class=\"pl-c1\">self</span>.mu <span class=\"pl-k\">=</span> tf.layers.dense(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>z_dim, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n                        <span class=\"pl-c1\">self</span>.log_sigma <span class=\"pl-k\">=</span> tf.layers.dense(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>z_dim, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)                        \n                        epsilon <span class=\"pl-k\">=</span> tf.random_normal(tf.shape(<span class=\"pl-c1\">self</span>.log_sigma), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>epsilon<span class=\"pl-pds\">'</span></span>)\n                        t <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.mu <span class=\"pl-k\">+</span> epsilon <span class=\"pl-k\">*</span> tf.exp(<span class=\"pl-c1\">self</span>.log_sigma)\n                        <span class=\"pl-k\">return</span> t\n                    \n                    <span class=\"pl-k\">def</span> <span class=\"pl-en\">nonvae_z</span>(<span class=\"pl-smi\">t</span>):\n                        <span class=\"pl-k\">return</span> tf.layers.dense(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>z_dim, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n                    \n                    t <span class=\"pl-k\">=</span> tf.cond(<span class=\"pl-c1\">self</span>.use_vae_T, <span class=\"pl-k\">lambda</span>: vae_z(t), <span class=\"pl-k\">lambda</span>: nonvae_z(t), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>z<span class=\"pl-pds\">'</span></span>)\n                    <span class=\"pl-c1\">self</span>.z <span class=\"pl-k\">=</span> t\n                <span class=\"pl-k\">else</span>:\n                    t <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, z_dim], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>z<span class=\"pl-pds\">'</span></span>)\n                    <span class=\"pl-c1\">self</span>.z <span class=\"pl-k\">=</span> t\n\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&gt; z<span class=\"pl-pds\">'</span></span>, t.shape, t.name)\n                \n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span> is there a better way to calculate desired flat shape post z?</span>\n                post_z_img_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.img_shape[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">//</span> (<span class=\"pl-c1\">2</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">len</span>(filters))\n                post_z_img_shape <span class=\"pl-k\">=</span> [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, post_z_img_dim, post_z_img_dim, filters[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]]\n                post_z_flat_dim <span class=\"pl-k\">=</span> filters[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> post_z_img_dim <span class=\"pl-k\">*</span> post_z_img_dim\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dense from <span class=\"pl-c1\">{}</span> to <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(t, post_z_flat_dim))\n                t <span class=\"pl-k\">=</span> tf.layers.dense(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>t, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>post_z_flat_dim, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>activation_fc)\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&gt; post z flat<span class=\"pl-pds\">'</span></span>, t.shape, t.name)\n                \n                t <span class=\"pl-k\">=</span> tf.reshape(t, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>tf.constant(post_z_img_shape))\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&gt; post z img<span class=\"pl-pds\">'</span></span>, t.shape, t.name)\n\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER   </span>\n</pre></div>\n<p>Output when I use the above. Note the scopenames and shapes in decoder1 (which is created without variable sharing), and it's identical to autoencoder. But decoder2 (which tries to share variables with autoencoder) fails.</p>\n<pre><code>autoencoder = Model(usage=Usage.autoencoder, reuse=None)\n--------------------------------------------------------------------------------\n__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\n&gt; x (?, 256, 256, 3) x:0\n&gt; conv (?, 128, 128, 64) encoder0/conv/Relu:0\n&gt; conv (?, 64, 64, 128) encoder1/conv/Relu:0\n&gt; conv (?, 32, 32, 128) encoder2/conv/Relu:0\n&gt; conv (?, 16, 16, 256) encoder3/conv/Relu:0\n&gt; pre z flat (?, 65536) Flatten/Reshape:0\n&gt; z (?, 128) z/Merge:0\ndense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536\n&gt; post z flat (?, 65536) dense/Relu:0\n&gt; post z img (?, 16, 16, 256) Reshape:0\n&gt; deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\n&gt; deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\n&gt; deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\n&gt; deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\n&gt; y (?, 256, 256, 3) final/conv/Sigmoid:0\n\n\n\ndecoder1 = Model(usage=Usage.decoder, reuse=None)\n--------------------------------------------------------------------------------\n__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\n&gt; z (?, 128) z:0\ndense from Tensor(\"z:0\", shape=(?, 128), dtype=float32) to 65536\n&gt; post z flat (?, 65536) dense/Relu:0\n&gt; post z img (?, 16, 16, 256) Reshape:0\n&gt; deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\n&gt; deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\n&gt; deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\n&gt; deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\n&gt; y (?, 256, 256, 3) final/conv/Sigmoid:0\n\n\n\ndecoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\n--------------------------------------------------------------------------------\n__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\n&gt; z (?, 128) z_1:0\ndense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536\nTraceback (most recent call last):\n\n  File \"&lt;ipython-input-4-6f2915008bb4&gt;\", line 1, in &lt;module&gt;\n    decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\n\n  File \"/home/memo/Dropbox/research/py/apps/webcam-pix2pix-tensorflow/models/cnnvae.py\", line 152, in __init__\n    t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 218, in dense\n    return layer.apply(inputs)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 286, in __call__\n    self.build(input_shapes[0])\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 123, in build\n    trainable=True)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1049, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 948, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 349, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 275, in variable_getter\n    variable_getter=functools.partial(getter, **kwargs))\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 228, in _add_variable\n    trainable=trainable and self.trainable)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 341, in _true_getter\n    use_resource=use_resource)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 658, in _get_single_variable\n    found_var.get_shape()))\n\nValueError: Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).\n</code></pre>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\n\n\nTensorFlow installed from (source or binary):\nbinary (pip install tensorflow-gpu==1.2.1)\n\n\nTensorFlow version (use command below):\nv1.2.0-5-g435cdfc, 1.2.1\n\n\nPython version:\nPython 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)\nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 5.3.0 -- An enhanced Interactive Python.\n\n\nBazel version (if compiling from source):\n\n\nCUDA/cuDNN version:\nCuda V8.0.61, CuDNN 5.1\n\n\nGPU model and memory:\nGeForce GTX 1080, 8GB\n\n\nDescribe the problem\nAt first I thought I was doing something wrong (initially posted on stackoverflow), but now I think this is a bug. Full text from SO pasted below.\nI want to share variables between an autoencoder and a decoder. But the weights matrix from z to the first fully connected flat layer is transposed, even though at every step of the graph construction I dump the previous tensor scopename and shape to the console, and in both cases ('autoencoder' and 'decoder') the scopenames and shapes are identical.\nTo be specific the error is\n\nTrying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).\n\nBut in both cases I'm creating a dense layer from (?, 128) to units=65536 with the same code:\nprint('dense from {} to {}'.format(t, post_z_flat_dim))\nt = tf.layers.dense(inputs=t, units=post_z_flat_dim, ...)\n\nwhich gives the output\n\nautoencoder: dense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536\n\n\ndecoder: dense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536\n\nThe only difference is, for the decoder t is a placeholder whereas for autoencoder it's the result of a sequence of operations (but still of shape [None, 128])\nRelevant code is below followed by the output.\nclass Model:\n    def __init__(self,\n                 usage, # Usage enum\n                 reuse=None, # instance of another model to reuse sess, graph and variables from\n                 ):\n\n        if reuse == None:\n            config = tf.ConfigProto()\n            config.gpu_options.allow_growth=True    \n            self.graph = tf.Graph()\n            self.sess = tf.Session(graph=self.graph, config=config)\n            root_scope = self.name\n        else:\n            self.graph = reuse.graph\n            self.sess = reuse.sess\n            root_scope = reuse.name\n\n        with self.graph.as_default():\n            with tf.variable_scope(root_scope, reuse=(reuse != None)):\n                \n                if usage >= Usage.autoencoder:\n                    self.x = tf.placeholder(tf.float32, (None,) + tuple(img_shape), name='x')\n                    t = self.x        \n                    print('> x', t.shape, t.name)\n\n                    # ENCODER\n                    for i, filter_depth in enumerate(filters):\n                        name = 'encoder{}'.format(i)\n                        with tf.variable_scope(name):         \n                            t = tf.layers.conv2d(...)                \n                            if usage >= Usage.trainer: t = tf.layers.dropout(inputs=t, rate=self.dropout_conv_amt_T, training=True)\n                            print('> conv', t.shape, t.name)\n                            \n                 \n                # Z    \n                if usage >= Usage.autoencoder:\n                    t = tf.contrib.layers.flatten(t)\n                    print('> pre z flat', t.shape, t.name)\n\n                    if usage >= Usage.trainer:\n                        t = tf.layers.dropout(inputs=t, rate=self.dropout_fc_amt_T, training=True)\n                    \n                    def vae_z(t):\n                        self.mu = tf.layers.dense(inputs=t, units=z_dim, activation=None)\n                        self.log_sigma = tf.layers.dense(inputs=t, units=z_dim, activation=None)                        \n                        epsilon = tf.random_normal(tf.shape(self.log_sigma), name='epsilon')\n                        t = self.mu + epsilon * tf.exp(self.log_sigma)\n                        return t\n                    \n                    def nonvae_z(t):\n                        return tf.layers.dense(inputs=t, units=z_dim, activation=None)\n                    \n                    t = tf.cond(self.use_vae_T, lambda: vae_z(t), lambda: nonvae_z(t), name='z')\n                    self.z = t\n                else:\n                    t = tf.placeholder(tf.float32, [None, z_dim], name='z')\n                    self.z = t\n\n                print('> z', t.shape, t.name)\n                \n                # TODO is there a better way to calculate desired flat shape post z?\n                post_z_img_dim = self.img_shape[0] // (2**len(filters))\n                post_z_img_shape = [-1, post_z_img_dim, post_z_img_dim, filters[-1]]\n                post_z_flat_dim = filters[-1] * post_z_img_dim * post_z_img_dim\n                print('dense from {} to {}'.format(t, post_z_flat_dim))\n                t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)\n                print('> post z flat', t.shape, t.name)\n                \n                t = tf.reshape(t, shape=tf.constant(post_z_img_shape))\n                print('> post z img', t.shape, t.name)\n\n\n                # DECODER   \n\nOutput when I use the above. Note the scopenames and shapes in decoder1 (which is created without variable sharing), and it's identical to autoencoder. But decoder2 (which tries to share variables with autoencoder) fails.\nautoencoder = Model(usage=Usage.autoencoder, reuse=None)\n--------------------------------------------------------------------------------\n__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\n> x (?, 256, 256, 3) x:0\n> conv (?, 128, 128, 64) encoder0/conv/Relu:0\n> conv (?, 64, 64, 128) encoder1/conv/Relu:0\n> conv (?, 32, 32, 128) encoder2/conv/Relu:0\n> conv (?, 16, 16, 256) encoder3/conv/Relu:0\n> pre z flat (?, 65536) Flatten/Reshape:0\n> z (?, 128) z/Merge:0\ndense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536\n> post z flat (?, 65536) dense/Relu:0\n> post z img (?, 16, 16, 256) Reshape:0\n> deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\n> deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\n> deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\n> deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\n> y (?, 256, 256, 3) final/conv/Sigmoid:0\n\n\n\ndecoder1 = Model(usage=Usage.decoder, reuse=None)\n--------------------------------------------------------------------------------\n__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\n> z (?, 128) z:0\ndense from Tensor(\"z:0\", shape=(?, 128), dtype=float32) to 65536\n> post z flat (?, 65536) dense/Relu:0\n> post z img (?, 16, 16, 256) Reshape:0\n> deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\n> deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\n> deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\n> deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\n> y (?, 256, 256, 3) final/conv/Sigmoid:0\n\n\n\ndecoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\n--------------------------------------------------------------------------------\n__main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\n> z (?, 128) z_1:0\ndense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536\nTraceback (most recent call last):\n\n  File \"<ipython-input-4-6f2915008bb4>\", line 1, in <module>\n    decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\n\n  File \"/home/memo/Dropbox/research/py/apps/webcam-pix2pix-tensorflow/models/cnnvae.py\", line 152, in __init__\n    t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 218, in dense\n    return layer.apply(inputs)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 286, in __call__\n    self.build(input_shapes[0])\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 123, in build\n    trainable=True)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1049, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 948, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 349, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 275, in variable_getter\n    variable_getter=functools.partial(getter, **kwargs))\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 228, in _add_variable\n    trainable=trainable and self.trainable)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 341, in _true_getter\n    use_resource=use_resource)\n\n  File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 658, in _get_single_variable\n    found_var.get_shape()))\n\nValueError: Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip install tensorflow-gpu==1.2.1)\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.2.0-5-g435cdfc, 1.2.1\r\n\r\n- **Python version**: \r\nPython 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)\r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\nIPython 5.3.0 -- An enhanced Interactive Python.\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCuda V8.0.61, CuDNN 5.1\r\n\r\n- **GPU model and memory**:\r\nGeForce GTX 1080, 8GB\r\n\r\n### Describe the problem\r\nAt first I thought I was doing something wrong (initially posted on [stackoverflow](https://stackoverflow.com/questions/45969089/trying-to-share-tensorflow-variables-between-autoencoder-and-decoder-but-weight)), but now I think this is a bug. Full text from SO pasted below.\r\n\r\nI want to share variables between an autoencoder and a decoder. But the weights matrix from z to the first fully connected flat layer is transposed, even though at every step of the graph construction I dump the previous tensor scopename and shape to the console, and in both cases ('autoencoder' and 'decoder') the scopenames and shapes are identical.\r\n\r\nTo be specific the error is \r\n\r\n> Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).\r\n\r\nBut in both cases I'm creating a dense layer from (?, 128) to units=65536 with the same code:\r\n\r\n    print('dense from {} to {}'.format(t, post_z_flat_dim))\r\n    t = tf.layers.dense(inputs=t, units=post_z_flat_dim, ...)\r\n\r\nwhich gives the output\r\n> autoencoder: dense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536\r\n\r\n> decoder: dense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536\r\n\r\nThe only difference is, for the decoder t is a placeholder whereas for autoencoder it's the result of a sequence of operations (but still of shape [None, 128])\r\n\r\nRelevant code is below followed by the output.\r\n\r\n```python\r\nclass Model:\r\n    def __init__(self,\r\n                 usage, # Usage enum\r\n                 reuse=None, # instance of another model to reuse sess, graph and variables from\r\n                 ):\r\n\r\n        if reuse == None:\r\n            config = tf.ConfigProto()\r\n            config.gpu_options.allow_growth=True    \r\n            self.graph = tf.Graph()\r\n            self.sess = tf.Session(graph=self.graph, config=config)\r\n            root_scope = self.name\r\n        else:\r\n            self.graph = reuse.graph\r\n            self.sess = reuse.sess\r\n            root_scope = reuse.name\r\n\r\n        with self.graph.as_default():\r\n            with tf.variable_scope(root_scope, reuse=(reuse != None)):\r\n                \r\n                if usage >= Usage.autoencoder:\r\n                    self.x = tf.placeholder(tf.float32, (None,) + tuple(img_shape), name='x')\r\n                    t = self.x        \r\n                    print('> x', t.shape, t.name)\r\n\r\n                    # ENCODER\r\n                    for i, filter_depth in enumerate(filters):\r\n                        name = 'encoder{}'.format(i)\r\n                        with tf.variable_scope(name):         \r\n                            t = tf.layers.conv2d(...)                \r\n                            if usage >= Usage.trainer: t = tf.layers.dropout(inputs=t, rate=self.dropout_conv_amt_T, training=True)\r\n                            print('> conv', t.shape, t.name)\r\n                            \r\n                 \r\n                # Z    \r\n                if usage >= Usage.autoencoder:\r\n                    t = tf.contrib.layers.flatten(t)\r\n                    print('> pre z flat', t.shape, t.name)\r\n\r\n                    if usage >= Usage.trainer:\r\n                        t = tf.layers.dropout(inputs=t, rate=self.dropout_fc_amt_T, training=True)\r\n                    \r\n                    def vae_z(t):\r\n                        self.mu = tf.layers.dense(inputs=t, units=z_dim, activation=None)\r\n                        self.log_sigma = tf.layers.dense(inputs=t, units=z_dim, activation=None)                        \r\n                        epsilon = tf.random_normal(tf.shape(self.log_sigma), name='epsilon')\r\n                        t = self.mu + epsilon * tf.exp(self.log_sigma)\r\n                        return t\r\n                    \r\n                    def nonvae_z(t):\r\n                        return tf.layers.dense(inputs=t, units=z_dim, activation=None)\r\n                    \r\n                    t = tf.cond(self.use_vae_T, lambda: vae_z(t), lambda: nonvae_z(t), name='z')\r\n                    self.z = t\r\n                else:\r\n                    t = tf.placeholder(tf.float32, [None, z_dim], name='z')\r\n                    self.z = t\r\n\r\n                print('> z', t.shape, t.name)\r\n                \r\n                # TODO is there a better way to calculate desired flat shape post z?\r\n                post_z_img_dim = self.img_shape[0] // (2**len(filters))\r\n                post_z_img_shape = [-1, post_z_img_dim, post_z_img_dim, filters[-1]]\r\n                post_z_flat_dim = filters[-1] * post_z_img_dim * post_z_img_dim\r\n                print('dense from {} to {}'.format(t, post_z_flat_dim))\r\n                t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)\r\n                print('> post z flat', t.shape, t.name)\r\n                \r\n                t = tf.reshape(t, shape=tf.constant(post_z_img_shape))\r\n                print('> post z img', t.shape, t.name)\r\n\r\n\r\n                # DECODER   \r\n\r\n```\r\n\r\nOutput when I use the above. Note the scopenames and shapes in decoder1 (which is created without variable sharing), and it's identical to autoencoder. But decoder2 (which tries to share variables with autoencoder) fails.\r\n\r\n    autoencoder = Model(usage=Usage.autoencoder, reuse=None)\r\n    --------------------------------------------------------------------------------\r\n    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\r\n    > x (?, 256, 256, 3) x:0\r\n    > conv (?, 128, 128, 64) encoder0/conv/Relu:0\r\n    > conv (?, 64, 64, 128) encoder1/conv/Relu:0\r\n    > conv (?, 32, 32, 128) encoder2/conv/Relu:0\r\n    > conv (?, 16, 16, 256) encoder3/conv/Relu:0\r\n    > pre z flat (?, 65536) Flatten/Reshape:0\r\n    > z (?, 128) z/Merge:0\r\n    dense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536\r\n    > post z flat (?, 65536) dense/Relu:0\r\n    > post z img (?, 16, 16, 256) Reshape:0\r\n    > deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\r\n    > deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\r\n    > deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\r\n    > deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\r\n    > y (?, 256, 256, 3) final/conv/Sigmoid:0\r\n\r\n\r\n    \r\n    decoder1 = Model(usage=Usage.decoder, reuse=None)\r\n    --------------------------------------------------------------------------------\r\n    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\r\n    > z (?, 128) z:0\r\n    dense from Tensor(\"z:0\", shape=(?, 128), dtype=float32) to 65536\r\n    > post z flat (?, 65536) dense/Relu:0\r\n    > post z img (?, 16, 16, 256) Reshape:0\r\n    > deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\r\n    > deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\r\n    > deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\r\n    > deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\r\n    > y (?, 256, 256, 3) final/conv/Sigmoid:0\r\n    \r\n\r\n\r\n    decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\r\n    --------------------------------------------------------------------------------\r\n    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\r\n    > z (?, 128) z_1:0\r\n    dense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536\r\n    Traceback (most recent call last):\r\n    \r\n      File \"<ipython-input-4-6f2915008bb4>\", line 1, in <module>\r\n        decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\r\n    \r\n      File \"/home/memo/Dropbox/research/py/apps/webcam-pix2pix-tensorflow/models/cnnvae.py\", line 152, in __init__\r\n        t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 218, in dense\r\n        return layer.apply(inputs)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\r\n        return self.__call__(inputs, **kwargs)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 286, in __call__\r\n        self.build(input_shapes[0])\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 123, in build\r\n        trainable=True)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1049, in get_variable\r\n        use_resource=use_resource, custom_getter=custom_getter)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 948, in get_variable\r\n        use_resource=use_resource, custom_getter=custom_getter)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 349, in get_variable\r\n        validate_shape=validate_shape, use_resource=use_resource)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 275, in variable_getter\r\n        variable_getter=functools.partial(getter, **kwargs))\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 228, in _add_variable\r\n        trainable=trainable and self.trainable)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 341, in _true_getter\r\n        use_resource=use_resource)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 658, in _get_single_variable\r\n        found_var.get_shape()))\r\n    \r\n    ValueError: Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).\r\n\r\n\r\n\r\n\r\n"}