{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/343813227", "html_url": "https://github.com/tensorflow/tensorflow/issues/1511#issuecomment-343813227", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1511", "id": 343813227, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzgxMzIyNw==", "user": {"login": "monajalal", "id": 1892917, "node_id": "MDQ6VXNlcjE4OTI5MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1892917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/monajalal", "html_url": "https://github.com/monajalal", "followers_url": "https://api.github.com/users/monajalal/followers", "following_url": "https://api.github.com/users/monajalal/following{/other_user}", "gists_url": "https://api.github.com/users/monajalal/gists{/gist_id}", "starred_url": "https://api.github.com/users/monajalal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/monajalal/subscriptions", "organizations_url": "https://api.github.com/users/monajalal/orgs", "repos_url": "https://api.github.com/users/monajalal/repos", "events_url": "https://api.github.com/users/monajalal/events{/privacy}", "received_events_url": "https://api.github.com/users/monajalal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-13T04:58:11Z", "updated_at": "2017-11-13T04:58:11Z", "author_association": "NONE", "body_html": "<p>for</p>\n<pre><code>with tf.name_scope('loss'):\n    #cross_entropy = None\n    val = tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y_conv )\n    cross_entropy = tf.reduce_mean(val)\n\nwith tf.name_scope('adam_optimizer'):\n    #train_step = None\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n</code></pre>\n<p>I get this error:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-10-65094b0104ba&gt; in &lt;module&gt;()\n      6 with tf.name_scope('adam_optimizer'):\n      7     #train_step = None\n----&gt; 8     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    320           \"No gradients provided for any variable, check your graph for ops\"\n    321           \" that do not support gradients, between variables %s and loss %s.\" %\n--&gt; 322           ([str(v) for _, v in grads_and_vars], loss))\n    323 \n    324     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"&lt;tf.Variable 'Variable:0' shape=(32,) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_1:0' shape=(64,) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_2:0' shape=(1024,) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_4:0' shape=(5, 5, 1, 32) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_5:0' shape=(5, 5, 32, 64) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_6:0' shape=(3136, 1024) dtype=float32_ref&gt;\", \"&lt;tf.Variable 'Variable_7:0' shape=(1024, 10) dtype=float32_ref&gt;\"] and loss Tensor(\"loss_4/Mean:0\", shape=(), dtype=float32).\n\n</code></pre>\n<p>how should I fix?</p>", "body_text": "for\nwith tf.name_scope('loss'):\n    #cross_entropy = None\n    val = tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y_conv )\n    cross_entropy = tf.reduce_mean(val)\n\nwith tf.name_scope('adam_optimizer'):\n    #train_step = None\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\nI get this error:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-10-65094b0104ba> in <module>()\n      6 with tf.name_scope('adam_optimizer'):\n      7     #train_step = None\n----> 8     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    320           \"No gradients provided for any variable, check your graph for ops\"\n    321           \" that do not support gradients, between variables %s and loss %s.\" %\n--> 322           ([str(v) for _, v in grads_and_vars], loss))\n    323 \n    324     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(1024,) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(5, 5, 1, 32) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(5, 5, 32, 64) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(3136, 1024) dtype=float32_ref>\", \"<tf.Variable 'Variable_7:0' shape=(1024, 10) dtype=float32_ref>\"] and loss Tensor(\"loss_4/Mean:0\", shape=(), dtype=float32).\n\n\nhow should I fix?", "body": "for \r\n```\r\nwith tf.name_scope('loss'):\r\n    #cross_entropy = None\r\n    val = tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y_conv )\r\n    cross_entropy = tf.reduce_mean(val)\r\n\r\nwith tf.name_scope('adam_optimizer'):\r\n    #train_step = None\r\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n```\r\n\r\nI get this error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-65094b0104ba> in <module>()\r\n      6 with tf.name_scope('adam_optimizer'):\r\n      7     #train_step = None\r\n----> 8     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    320           \"No gradients provided for any variable, check your graph for ops\"\r\n    321           \" that do not support gradients, between variables %s and loss %s.\" %\r\n--> 322           ([str(v) for _, v in grads_and_vars], loss))\r\n    323 \r\n    324     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'Variable_1:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'Variable_2:0' shape=(1024,) dtype=float32_ref>\", \"<tf.Variable 'Variable_3:0' shape=(10,) dtype=float32_ref>\", \"<tf.Variable 'Variable_4:0' shape=(5, 5, 1, 32) dtype=float32_ref>\", \"<tf.Variable 'Variable_5:0' shape=(5, 5, 32, 64) dtype=float32_ref>\", \"<tf.Variable 'Variable_6:0' shape=(3136, 1024) dtype=float32_ref>\", \"<tf.Variable 'Variable_7:0' shape=(1024, 10) dtype=float32_ref>\"] and loss Tensor(\"loss_4/Mean:0\", shape=(), dtype=float32).\r\n\r\n```\r\n\r\nhow should I fix?"}