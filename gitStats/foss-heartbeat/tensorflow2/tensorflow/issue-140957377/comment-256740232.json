{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/256740232", "html_url": "https://github.com/tensorflow/tensorflow/issues/1511#issuecomment-256740232", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1511", "id": 256740232, "node_id": "MDEyOklzc3VlQ29tbWVudDI1Njc0MDIzMg==", "user": {"login": "rosecatherinek", "id": 11052056, "node_id": "MDQ6VXNlcjExMDUyMDU2", "avatar_url": "https://avatars2.githubusercontent.com/u/11052056?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rosecatherinek", "html_url": "https://github.com/rosecatherinek", "followers_url": "https://api.github.com/users/rosecatherinek/followers", "following_url": "https://api.github.com/users/rosecatherinek/following{/other_user}", "gists_url": "https://api.github.com/users/rosecatherinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/rosecatherinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rosecatherinek/subscriptions", "organizations_url": "https://api.github.com/users/rosecatherinek/orgs", "repos_url": "https://api.github.com/users/rosecatherinek/repos", "events_url": "https://api.github.com/users/rosecatherinek/events{/privacy}", "received_events_url": "https://api.github.com/users/rosecatherinek/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-27T19:10:33Z", "updated_at": "2016-10-27T19:10:33Z", "author_association": "NONE", "body_html": "<p>Hi Team,</p>\n<p>I am running into the same problem when I cast my tensor to float32. But without casting, I get the error that the expected type is float and not int. So, either way, I can't find a way to proceed...</p>\n<p>In my setting, I am trying to minimize the squared error of the difference of two tensors.</p>\n<pre><code>softmax_w = tf.Variable(tf.zeros([SIZE_LSTM_UNITS, NUM_CLASSES], dtype=tf.float32))\nsoftmax_b = tf.Variable(tf.zeros([NUM_CLASSES], dtype=tf.float32))\nlogits = tf.matmul(out, softmax_w) + softmax_b\n</code></pre>\n<p>If I compute the loss with casting as below:</p>\n<pre><code>predDiff = tf.cast(tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1)), tf.float32)\nl2loss = tf.nn.l2_loss(predDiff)\ntrainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n</code></pre>\n<p>(where, logits and train_labels are 1-hot vectors), then I get the following error:<br>\n<code>trainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize name=name) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 309, in apply_gradients (converted_grads_and_vars,)) ValueError: No gradients provided for any variable: ((None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f2c7363bf90&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f2ce284e9d0&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f2ce284e510&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf050&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf450&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f2ce2c9d510&gt;), (None, &lt;tensorflow.python.ops.variables.Variable object at 0x7f2ce287ae90&gt;))</code></p>\n<p>Instead, if I compute the loss without casting as below:</p>\n<pre><code>predDiff = tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1))\n</code></pre>\n<p>then, I get the following error:<br>\n<code>trainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize grad_loss=grad_loss) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 238, in compute_gradients self._assert_valid_dtypes([loss]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 379, in _assert_valid_dtypes dtype, t.name, [v for v in valid_dtypes])) ValueError: Invalid type tf.int64 for L2Loss:0, expected: [tf.float32, tf.float64, tf.float16].</code></p>\n<p>Not sure if I am missing something obvious. Would it be possible for you to give any pointers?</p>\n<p>Thanks a lot!</p>", "body_text": "Hi Team,\nI am running into the same problem when I cast my tensor to float32. But without casting, I get the error that the expected type is float and not int. So, either way, I can't find a way to proceed...\nIn my setting, I am trying to minimize the squared error of the difference of two tensors.\nsoftmax_w = tf.Variable(tf.zeros([SIZE_LSTM_UNITS, NUM_CLASSES], dtype=tf.float32))\nsoftmax_b = tf.Variable(tf.zeros([NUM_CLASSES], dtype=tf.float32))\nlogits = tf.matmul(out, softmax_w) + softmax_b\n\nIf I compute the loss with casting as below:\npredDiff = tf.cast(tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1)), tf.float32)\nl2loss = tf.nn.l2_loss(predDiff)\ntrainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n\n(where, logits and train_labels are 1-hot vectors), then I get the following error:\ntrainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize name=name) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 309, in apply_gradients (converted_grads_and_vars,)) ValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f2c7363bf90>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce284e9d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce284e510>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf050>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf450>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce2c9d510>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce287ae90>))\nInstead, if I compute the loss without casting as below:\npredDiff = tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1))\n\nthen, I get the following error:\ntrainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize grad_loss=grad_loss) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 238, in compute_gradients self._assert_valid_dtypes([loss]) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 379, in _assert_valid_dtypes dtype, t.name, [v for v in valid_dtypes])) ValueError: Invalid type tf.int64 for L2Loss:0, expected: [tf.float32, tf.float64, tf.float16].\nNot sure if I am missing something obvious. Would it be possible for you to give any pointers?\nThanks a lot!", "body": "Hi Team,\n\nI am running into the same problem when I cast my tensor to float32. But without casting, I get the error that the expected type is float and not int. So, either way, I can't find a way to proceed...\n\nIn my setting, I am trying to minimize the squared error of the difference of two tensors.\n\n```\nsoftmax_w = tf.Variable(tf.zeros([SIZE_LSTM_UNITS, NUM_CLASSES], dtype=tf.float32))\nsoftmax_b = tf.Variable(tf.zeros([NUM_CLASSES], dtype=tf.float32))\nlogits = tf.matmul(out, softmax_w) + softmax_b\n```\n\nIf I compute the loss with casting as below: \n\n```\npredDiff = tf.cast(tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1)), tf.float32)\nl2loss = tf.nn.l2_loss(predDiff)\ntrainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n```\n\n(where, logits and train_labels are 1-hot vectors), then I get the following error: \n`trainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 309, in apply_gradients\n    (converted_grads_and_vars,))\nValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f2c7363bf90>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce284e9d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce284e510>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf050>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce26cf450>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce2c9d510>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f2ce287ae90>))`\n\nInstead, if I compute the loss without casting as below:\n\n```\npredDiff = tf.sub(tf.arg_max(logits, 1), tf.arg_max(train_labels, 1))\n```\n\nthen, I get the following error: \n`trainStep = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(l2loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 238, in compute_gradients\n    self._assert_valid_dtypes([loss])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 379, in _assert_valid_dtypes\n    dtype, t.name, [v for v in valid_dtypes]))\nValueError: Invalid type tf.int64 for L2Loss:0, expected: [tf.float32, tf.float64, tf.float16].`\n\nNot sure if I am missing something obvious. Would it be possible for you to give any pointers? \n\nThanks a lot!\n"}