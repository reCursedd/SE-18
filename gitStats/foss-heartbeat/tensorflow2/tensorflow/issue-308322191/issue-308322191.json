{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17980", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17980/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17980/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17980/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17980", "id": 308322191, "node_id": "MDU6SXNzdWUzMDgzMjIxOTE=", "number": 17980, "title": "tf.einsum doesn't perform common subgraph elimination", "user": {"login": "matt-chan", "id": 1048117, "node_id": "MDQ6VXNlcjEwNDgxMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1048117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matt-chan", "html_url": "https://github.com/matt-chan", "followers_url": "https://api.github.com/users/matt-chan/followers", "following_url": "https://api.github.com/users/matt-chan/following{/other_user}", "gists_url": "https://api.github.com/users/matt-chan/gists{/gist_id}", "starred_url": "https://api.github.com/users/matt-chan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matt-chan/subscriptions", "organizations_url": "https://api.github.com/users/matt-chan/orgs", "repos_url": "https://api.github.com/users/matt-chan/repos", "events_url": "https://api.github.com/users/matt-chan/events{/privacy}", "received_events_url": "https://api.github.com/users/matt-chan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-03-25T04:48:53Z", "updated_at": "2018-10-15T21:36:33Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: RHEL 7.4</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary (pip)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 'unknown' 1.6.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When I inspect the computation graph in tensorboard, there are two separate calls to einsum and no subgraphs (ie calls to matmul) are shared. I would have expected the \"jb,abcd\" tensors to be contracted once, then contracted with (ia + ai) afterwards.</p>\n<h3>Source code / logs</h3>\n<pre><code>tf_eri = tf.constant(np.random.random([10,10,10,10])\ntf_ao = tf.constant(np.random.random([10,10])\n\ntf_foo = tf.einsum(\"ia,jb,abcd\", tf_ao, tf_ao, tf_eri)\ntf_bar = tf.einsum(\"ai,jb,abcd\", tf_ao, tf_ao, tf_eri)\ntf_res = tf_foo + tf_bar\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1048117/37871855-3a6564d0-2fc6-11e8-961b-a34d14175e50.png\"><img src=\"https://user-images.githubusercontent.com/1048117/37871855-3a6564d0-2fc6-11e8-961b-a34d14175e50.png\" alt=\"graph\" style=\"max-width:100%;\"></a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.4\nTensorFlow installed from (source or binary): Binary (pip)\nTensorFlow version (use command below): 'unknown' 1.6.0\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nWhen I inspect the computation graph in tensorboard, there are two separate calls to einsum and no subgraphs (ie calls to matmul) are shared. I would have expected the \"jb,abcd\" tensors to be contracted once, then contracted with (ia + ai) afterwards.\nSource code / logs\ntf_eri = tf.constant(np.random.random([10,10,10,10])\ntf_ao = tf.constant(np.random.random([10,10])\n\ntf_foo = tf.einsum(\"ia,jb,abcd\", tf_ao, tf_ao, tf_eri)\ntf_bar = tf.einsum(\"ai,jb,abcd\", tf_ao, tf_ao, tf_eri)\ntf_res = tf_foo + tf_bar", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7.4\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: 'unknown' 1.6.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\nWhen I inspect the computation graph in tensorboard, there are two separate calls to einsum and no subgraphs (ie calls to matmul) are shared. I would have expected the \"jb,abcd\" tensors to be contracted once, then contracted with (ia + ai) afterwards. \r\n\r\n### Source code / logs\r\n```\r\ntf_eri = tf.constant(np.random.random([10,10,10,10])\r\ntf_ao = tf.constant(np.random.random([10,10])\r\n\r\ntf_foo = tf.einsum(\"ia,jb,abcd\", tf_ao, tf_ao, tf_eri)\r\ntf_bar = tf.einsum(\"ai,jb,abcd\", tf_ao, tf_ao, tf_eri)\r\ntf_res = tf_foo + tf_bar\r\n```\r\n\r\n![graph](https://user-images.githubusercontent.com/1048117/37871855-3a6564d0-2fc6-11e8-961b-a34d14175e50.png)\r\n"}