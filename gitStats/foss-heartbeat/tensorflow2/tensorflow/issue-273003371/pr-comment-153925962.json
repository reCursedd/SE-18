{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/153925962", "pull_request_review_id": 80010258, "id": 153925962, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzkyNTk2Mg==", "diff_hunk": "@@ -675,6 +681,627 @@ class MklFusedBatchNormGradOp : public OpKernel {\n     }\n   } MklFusedBatchNormGradOpContext;\n };\n+#endif\n+\n+#ifdef INTEL_MKL_DNN\n+\n+template <typename Device, typename T>\n+class MklFusedBatchNormOp : public OpKernel {\n+ public:\n+  explicit MklFusedBatchNormOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    float epsilon;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon));\n+    epsilon_ = T(epsilon);\n+    string tensor_format;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &tensor_format));\n+    OP_REQUIRES(context, FormatFromString(tensor_format, &tensor_format_),\n+                errors::InvalidArgument(\"Invalid data format\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    try {\n+      auto cpu_engine = engine(engine::cpu, 0);\n+      const size_t src_index = 0;    // index of src input tensor\n+      const size_t scale_index = 1;  // index of scale tensor\n+      const size_t shift_index = 2;  // index of shift tensor\n+      const size_t mean_index = 3;   // index of est_mean tensor\n+      const size_t var_index = 4;    // index of est_variance tensor\n+\n+      const Tensor& src_tensor          = MklGetInput(context, src_index);\n+      const Tensor& scale_tensor        = MklGetInput(context, scale_index);\n+      const Tensor& shift_tensor        = MklGetInput(context, shift_index);\n+      const Tensor& est_mean_tensor     = MklGetInput(context, mean_index);\n+      const Tensor& est_variance_tensor = MklGetInput(context, var_index);\n+\n+      MklDnnShape dnn_shape_src;\n+      GetMklShape(context, src_index, &dnn_shape_src);\n+\n+      if (dnn_shape_src.IsMklTensor()) {\n+        OP_REQUIRES(context, dnn_shape_src.GetDimension() == 4,\n+                    errors::InvalidArgument(\n+                        \"input must be 4-dimensional\",\n+                        src_tensor.shape().DebugString()));\n+      } else {\n+        OP_REQUIRES(context, src_tensor.dims() == 4,\n+                    errors::InvalidArgument(\n+                        \"input must be 4-dimensional\",\n+                        src_tensor.shape().DebugString()));\n+      }\n+      OP_REQUIRES(context, scale_tensor.dims() == 1,\n+                  errors::InvalidArgument(\n+                      \"scale must be 1-dimensional\",\n+                      scale_tensor.shape().DebugString()));\n+      OP_REQUIRES(context, shift_tensor.dims() == 1,\n+                  errors::InvalidArgument(\"offset must be 1-dimensional\",\n+                                        shift_tensor.shape().DebugString()));\n+      OP_REQUIRES(context, est_mean_tensor.dims() == 1,\n+                  errors::InvalidArgument(\n+                      \"estimated_mean must be 1-dimensional\",\n+                      est_mean_tensor.shape().DebugString()));\n+      OP_REQUIRES(context, est_variance_tensor.dims() == 1,\n+                  errors::InvalidArgument(\n+                      \"estimated_variance must be 1-dimensional\",\n+                      est_variance_tensor.shape().DebugString()));\n+\n+      if (is_training_) {\n+        OP_REQUIRES(context, est_mean_tensor.dim_size(0) == 0,\n+                    errors::InvalidArgument(\n+                        \"estimated_mean must be empty for training\",\n+                        est_mean_tensor.shape().DebugString()));\n+        OP_REQUIRES(context, est_variance_tensor.dim_size(0) == 0,\n+                    errors::InvalidArgument(\n+                        \"estimated_variance must be empty for training\",\n+                        est_variance_tensor.shape().DebugString()));\n+      }\n+\n+      if (dnn_shape_src.IsMklTensor())\n+        depth_ = dnn_shape_src.DimSize(MklDnnDims::Dim_C);\n+      else\n+        ExtractParams(context);\n+\n+      // Indices of output tensors\n+      const size_t dst_index = 0;\n+      const size_t batch_mean_index = 1;\n+      const size_t batch_variance_index = 2;\n+      const size_t saved_mean_index = 3;\n+      const size_t saved_variance_index = 4;\n+\n+      // allocate batch mean output tensor\n+      Tensor* batch_mean_tensor = nullptr;\n+      MklDnnShape mkl_shape_batch_mean;\n+      mkl_shape_batch_mean.SetMklTensor(false);\n+      AllocateOutputSetMklShape(context,\n+                                batch_mean_index,\n+                                &batch_mean_tensor,\n+                                scale_tensor.shape(),\n+                                mkl_shape_batch_mean);\n+      CHECK_NOTNULL(batch_mean_tensor);\n+\n+      // Batch variance\n+      Tensor* batch_variance_tensor = nullptr;\n+      MklDnnShape mkl_shape_batch_variance;\n+      mkl_shape_batch_variance.SetMklTensor(false);\n+      AllocateOutputSetMklShape(context,\n+                                batch_variance_index,\n+                                &batch_variance_tensor,\n+                                scale_tensor.shape(),\n+                                mkl_shape_batch_variance);\n+      CHECK_NOTNULL(batch_variance_tensor);\n+\n+      if (is_training_)\n+        SetMeanVariance(*batch_mean_tensor, *batch_variance_tensor);\n+      else\n+        SetMeanVariance(est_mean_tensor, est_variance_tensor);\n+\n+      MklDnnData<T> src(&cpu_engine);\n+      MklDnnData<T> dst(&cpu_engine);\n+\n+      memory::format format_m;\n+      if (dnn_shape_src.IsMklTensor()) {\n+        if (dnn_shape_src.IsTensorInNCHWFormat()) {\n+          format_m = memory::format::nchw;\n+        } else {\n+          format_m = memory::format::nhwc;\n+        }\n+      } else {\n+        format_m = TFDataFormatToMklDnnDataFormat(tensor_format_);\n+      }\n+\n+      // set src primitive\n+      memory::dims src_dims;\n+      if (dnn_shape_src.IsMklTensor()) {\n+        src_dims = TFShapeToMklDnnDimsInNCHW(dnn_shape_src.GetTfShape(),\n+                                             tensor_format_);\n+      } else {\n+        src_dims = TFShapeToMklDnnDimsInNCHW(src_tensor.shape(),\n+                                             tensor_format_);\n+      }\n+\n+      auto src_md = dnn_shape_src.IsMklTensor()\n+                    ? dnn_shape_src.GetMklLayout()\n+                    : memory::desc(src_dims, MklDnnType<T>(), format_m);\n+      src.SetUsrMem(src_md, &src_tensor);\n+\n+      // set weights primitive\n+      // MKL-DNN packs scale & shift as \"weights\":\n+      // <scale>...<scale><shift>...<shift>\n+      auto weights_desc = memory::desc({2, depth_},\n+                                       MklDnnType<T>(),\n+                                       memory::format::nc);\n+      auto weights_pd = memory::primitive_desc(weights_desc, cpu_engine);\n+      auto weights_m = memory(weights_pd);\n+      T* weights_data = reinterpret_cast<T*>(\n+                        weights_m.get_data_handle());\n+      T* scale_tf = reinterpret_cast<T*>(\n+                    const_cast<T*>(scale_tensor.flat<T>().data()));\n+      T* shift_tf = reinterpret_cast<T*>(\n+                    const_cast<T*>(shift_tensor.flat<T>().data()));\n+\n+      for (int k=0; k < depth_; k++) {\n+        weights_data[k] = scale_tf[k];\n+        weights_data[k + depth_] = shift_tf[k];\n+      }\n+\n+      // Mean and variance (without Bessel's correction) saved for backward\n+      // computation to serve as pre-computed mean and variance.\n+      Tensor* saved_mean_tensor = nullptr;\n+      MklDnnShape mkl_shape_saved_mean;\n+      mkl_shape_saved_mean.SetMklTensor(false);\n+      AllocateOutputSetMklShape(context, saved_mean_index,\n+                                &saved_mean_tensor,\n+                                scale_tensor.shape(),\n+                                mkl_shape_saved_mean);\n+      CHECK_NOTNULL(saved_mean_tensor);\n+\n+      Tensor* saved_variance_tensor = nullptr;\n+      MklDnnShape mkl_shape_saved_variance;\n+      mkl_shape_saved_variance.SetMklTensor(false);\n+      AllocateOutputSetMklShape(context, saved_variance_index,\n+                                &saved_variance_tensor,\n+                                scale_tensor.shape(),\n+                                mkl_shape_saved_variance);\n+      CHECK_NOTNULL(saved_variance_tensor);\n+\n+      // set mean primitive\n+      auto mean_desc = memory::desc({1, depth_},\n+                                    MklDnnType<T>(),\n+                                    memory::format::nc);\n+      auto mean_pd = memory::primitive_desc(mean_desc, cpu_engine);\n+      char* saved_mean_data_tf = reinterpret_cast<char*>\n+                                 (saved_mean_tensor->flat<T>().data());\n+      std::memcpy(saved_mean_data_tf,\n+                  reinterpret_cast<char*>(mean_values_),\n+                  depth_*sizeof(T));\n+      auto mean_m = memory(mean_pd,\n+                           reinterpret_cast<void*>(saved_mean_data_tf));\n+\n+      // set variance primitive\n+      auto variance_desc = memory::desc({1, depth_},\n+                                    MklDnnType<T>(),\n+                                    memory::format::nc);\n+      auto variance_pd = memory::primitive_desc(variance_desc, cpu_engine);\n+      char* saved_variance_data_tf = reinterpret_cast<char*>\n+                  (saved_variance_tensor->flat<T>().data());\n+      std::memcpy(saved_variance_data_tf,\n+                  reinterpret_cast<char*>(variance_values_),\n+                  depth_*sizeof(T));\n+      auto variance_m = memory(variance_pd, saved_variance_data_tf);\n+\n+      prop_kind pk = (is_training_) ?\n+                     prop_kind::forward_training :\n+                     prop_kind::forward_scoring;\n+      auto bnrm_fwd_desc = batch_normalization_forward::desc(\n+                               pk, src.GetUsrMemDesc(), epsilon_,\n+                               is_training_ ? use_scale_shift :\n+                               (use_scale_shift | use_global_stats));\n+      auto bnrm_fwd_pd = batch_normalization_forward::primitive_desc(\n+                             bnrm_fwd_desc, cpu_engine);\n+\n+      // allocate dst tensor\n+      MklDnnShape dnn_shape_dst;\n+      TensorShape tf_shape_dst;\n+      Tensor* dst_tensor = nullptr;\n+      if (dnn_shape_src.IsMklTensor()) {\n+        dnn_shape_dst.SetMklTensor(true);\n+        auto dst_pd = bnrm_fwd_pd.dst_primitive_desc();\n+        dnn_shape_dst.SetMklLayout(&dst_pd);\n+        dnn_shape_dst.SetElemType(MklDnnType<T>());\n+        dnn_shape_dst.SetTfLayout(dnn_shape_src.GetDimension(),\n+                                  src_dims, format_m);\n+        tf_shape_dst.AddDim(dst_pd.get_size()/sizeof(T));\n+      } else {\n+        dnn_shape_dst.SetMklTensor(false);\n+        tf_shape_dst = src_tensor.shape();\n+      }\n+      AllocateOutputSetMklShape(context, dst_index, &dst_tensor,\n+                                tf_shape_dst, dnn_shape_dst);\n+\n+      // Output of batchnorm has same shape as input.\n+      dst.SetUsrMem(src_md, dst_tensor);\n+\n+      primitive bnrm_fwd_op;\n+      if (is_training_)", "path": "tensorflow/core/kernels/mkl_fused_batch_norm_op.cc", "position": null, "original_position": 332, "commit_id": "4a5c2d7753510a18f59b107edb4caf616e96a3f1", "original_commit_id": "3c80ab2158f05c090b1f6252baf78ab7743545df", "user": {"login": "andydavis1", "id": 15696327, "node_id": "MDQ6VXNlcjE1Njk2MzI3", "avatar_url": "https://avatars0.githubusercontent.com/u/15696327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andydavis1", "html_url": "https://github.com/andydavis1", "followers_url": "https://api.github.com/users/andydavis1/followers", "following_url": "https://api.github.com/users/andydavis1/following{/other_user}", "gists_url": "https://api.github.com/users/andydavis1/gists{/gist_id}", "starred_url": "https://api.github.com/users/andydavis1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andydavis1/subscriptions", "organizations_url": "https://api.github.com/users/andydavis1/orgs", "repos_url": "https://api.github.com/users/andydavis1/repos", "events_url": "https://api.github.com/users/andydavis1/events{/privacy}", "received_events_url": "https://api.github.com/users/andydavis1/received_events", "type": "User", "site_admin": false}, "body": "Add braces around if/else here.", "created_at": "2017-11-29T21:52:40Z", "updated_at": "2017-12-01T20:49:45Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14459#discussion_r153925962", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14459", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/153925962"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14459#discussion_r153925962"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14459"}}, "body_html": "<p>Add braces around if/else here.</p>", "body_text": "Add braces around if/else here."}