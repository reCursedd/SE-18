{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16354", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16354/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16354/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16354/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16354", "id": 291098517, "node_id": "MDU6SXNzdWUyOTEwOTg1MTc=", "number": 16354, "title": "Dataset map func shape inference ", "user": {"login": "automateljw", "id": 5774705, "node_id": "MDQ6VXNlcjU3NzQ3MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/5774705?v=4", "gravatar_id": "", "url": "https://api.github.com/users/automateljw", "html_url": "https://github.com/automateljw", "followers_url": "https://api.github.com/users/automateljw/followers", "following_url": "https://api.github.com/users/automateljw/following{/other_user}", "gists_url": "https://api.github.com/users/automateljw/gists{/gist_id}", "starred_url": "https://api.github.com/users/automateljw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/automateljw/subscriptions", "organizations_url": "https://api.github.com/users/automateljw/orgs", "repos_url": "https://api.github.com/users/automateljw/repos", "events_url": "https://api.github.com/users/automateljw/events{/privacy}", "received_events_url": "https://api.github.com/users/automateljw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-24T06:31:42Z", "updated_at": "2018-01-25T08:19:55Z", "closed_at": "2018-01-25T08:19:55Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<pre><code>tf.VERSION = 1.4.1\ntf.GIT_VERSION = v1.4.0-19-ga52c8d9\ntf.COMPILER_VERSION = v1.4.0-19-ga52c8d9\nSanity check: array([1], dtype=int32)\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I use dataset api to build  input_fn  in train and evaluate process\uff0c the input_fn is as follows</p>\n<pre><code>def input_fn(data_dir, num_epochs, shuffle, batch_size):\n  \"\"\"Generate an input function for the Estimator.\"\"\"\n  data_files = tf.gfile.Glob(data_dir)\n  assert len(data_files), ('%s has no files!' % data_files) \n  \n  def parse_line(value):\n    columns = module_decode_file.decode_file(value, \n            record_defaults=_COLUMN_DEFAULTS,\n            output_size=_COLUMN_SIZESt,\n            field_outer_delim=',',\n            field_inner_delim='%')\n    features = dict(zip(_COLUMN_NAMES, columns))\n    labels = features.pop('label')\n    return features, labels\n\n  # Extract lines from input files using the Dataset API.\n  dataset = tf.data.TextLineDataset(data_files)\n  if shuffle:\n    dataset = dataset.shuffle(buffer_size=batch_size*3)\n  dataset = dataset.map(parse_line, num_parallel_calls=1)\n  dataset = dataset.repeat(num_epochs)\n  dataset = dataset.batch(batch_size)\n\n  iterator = dataset.make_one_shot_iterator()\n  features, labels = iterator.get_next(\"iterator\")\n  return features, labels\n</code></pre>\n<p>I use custom op decode_file, which is like tf.decode_csv.</p>\n<p>the decode_file's SetShapeFn is as follows:</p>\n<pre><code>    .SetShapeFn([](InferenceContext* c) {\n      std::vector&lt;int&gt; output_size;\n      c-&gt;GetAttr(\"output_size\", &amp;output_size);\n\n      for (int i = 0; i &lt; c-&gt;num_outputs(); ++i) {\n        ShapeHandle s = c-&gt;MakeShape({DimensionOrConstant(output_size[i])});\n        c-&gt;set_output(i, s); \n      }   \n      return Status::OK();\n    })  \n</code></pre>\n<p>this custom op  can work well  in train and evaluate ,  but  when it is used for export_savedmodel api, i need modify  SetShapeFn as follows ( to support batch predict)</p>\n<pre><code> .SetShapeFn([](InferenceContext* c) {\n      std::vector&lt;int&gt; output_size;\n      c-&gt;GetAttr(\"output_size\", &amp;output_size);\n      for (int i = 0; i &lt; c-&gt;num_outputs(); ++i) {\n          c-&gt;set_output(i, c-&gt;Matrix(c-&gt;Dim(c-&gt;input(0), 0), output_size[i]));\n      }\n      return Status::OK();\n}\n\n# my serving_input_fn\ndef string_decode_serving_input_receiver_fn():\n  string_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.string)\n\n  columns = module_decode_file_serve.decode_file_serve(string_placeholder,\n          record_defaults=_COLUMN_DEFAULTS,\n          output_size=_COLUMN_SIZESt, \n          field_outer_delim=',',\n          field_inner_delim='%')\n  features = dict(zip(_COLUMN_NAMES, columns))\n  features.pop('label')\n  return tf.estimator.export.ServingInputReceiver(features, string_placeholder)\n</code></pre>\n<p>Why does dataset map api (dataset = dataset.map(parse_line)) only handle single record?  and pase_line's shape inference does not include batch dimmension<br>\nAt present I need use different SetShapeFn in train and export_savedmodel<br>\nCan dataset map function add new feature to  handle batch dimmension ?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>  thanks! (sorry for bad description ...\uff09</p>", "body_text": "System information\ntf.VERSION = 1.4.1\ntf.GIT_VERSION = v1.4.0-19-ga52c8d9\ntf.COMPILER_VERSION = v1.4.0-19-ga52c8d9\nSanity check: array([1], dtype=int32)\n\nDescribe the problem\nI use dataset api to build  input_fn  in train and evaluate process\uff0c the input_fn is as follows\ndef input_fn(data_dir, num_epochs, shuffle, batch_size):\n  \"\"\"Generate an input function for the Estimator.\"\"\"\n  data_files = tf.gfile.Glob(data_dir)\n  assert len(data_files), ('%s has no files!' % data_files) \n  \n  def parse_line(value):\n    columns = module_decode_file.decode_file(value, \n            record_defaults=_COLUMN_DEFAULTS,\n            output_size=_COLUMN_SIZESt,\n            field_outer_delim=',',\n            field_inner_delim='%')\n    features = dict(zip(_COLUMN_NAMES, columns))\n    labels = features.pop('label')\n    return features, labels\n\n  # Extract lines from input files using the Dataset API.\n  dataset = tf.data.TextLineDataset(data_files)\n  if shuffle:\n    dataset = dataset.shuffle(buffer_size=batch_size*3)\n  dataset = dataset.map(parse_line, num_parallel_calls=1)\n  dataset = dataset.repeat(num_epochs)\n  dataset = dataset.batch(batch_size)\n\n  iterator = dataset.make_one_shot_iterator()\n  features, labels = iterator.get_next(\"iterator\")\n  return features, labels\n\nI use custom op decode_file, which is like tf.decode_csv.\nthe decode_file's SetShapeFn is as follows:\n    .SetShapeFn([](InferenceContext* c) {\n      std::vector<int> output_size;\n      c->GetAttr(\"output_size\", &output_size);\n\n      for (int i = 0; i < c->num_outputs(); ++i) {\n        ShapeHandle s = c->MakeShape({DimensionOrConstant(output_size[i])});\n        c->set_output(i, s); \n      }   \n      return Status::OK();\n    })  \n\nthis custom op  can work well  in train and evaluate ,  but  when it is used for export_savedmodel api, i need modify  SetShapeFn as follows ( to support batch predict)\n .SetShapeFn([](InferenceContext* c) {\n      std::vector<int> output_size;\n      c->GetAttr(\"output_size\", &output_size);\n      for (int i = 0; i < c->num_outputs(); ++i) {\n          c->set_output(i, c->Matrix(c->Dim(c->input(0), 0), output_size[i]));\n      }\n      return Status::OK();\n}\n\n# my serving_input_fn\ndef string_decode_serving_input_receiver_fn():\n  string_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.string)\n\n  columns = module_decode_file_serve.decode_file_serve(string_placeholder,\n          record_defaults=_COLUMN_DEFAULTS,\n          output_size=_COLUMN_SIZESt, \n          field_outer_delim=',',\n          field_inner_delim='%')\n  features = dict(zip(_COLUMN_NAMES, columns))\n  features.pop('label')\n  return tf.estimator.export.ServingInputReceiver(features, string_placeholder)\n\nWhy does dataset map api (dataset = dataset.map(parse_line)) only handle single record?  and pase_line's shape inference does not include batch dimmension\nAt present I need use different SetShapeFn in train and export_savedmodel\nCan dataset map function add new feature to  handle batch dimmension ?\n@mrry  thanks! (sorry for bad description ...\uff09", "body": "\r\n------------------------\r\n\r\n### System information\r\n```\r\ntf.VERSION = 1.4.1\r\ntf.GIT_VERSION = v1.4.0-19-ga52c8d9\r\ntf.COMPILER_VERSION = v1.4.0-19-ga52c8d9\r\nSanity check: array([1], dtype=int32)\r\n```\r\n\r\n### Describe the problem\r\n\r\nI use dataset api to build  input_fn  in train and evaluate process\uff0c the input_fn is as follows\r\n```\r\ndef input_fn(data_dir, num_epochs, shuffle, batch_size):\r\n  \"\"\"Generate an input function for the Estimator.\"\"\"\r\n  data_files = tf.gfile.Glob(data_dir)\r\n  assert len(data_files), ('%s has no files!' % data_files) \r\n  \r\n  def parse_line(value):\r\n    columns = module_decode_file.decode_file(value, \r\n            record_defaults=_COLUMN_DEFAULTS,\r\n            output_size=_COLUMN_SIZESt,\r\n            field_outer_delim=',',\r\n            field_inner_delim='%')\r\n    features = dict(zip(_COLUMN_NAMES, columns))\r\n    labels = features.pop('label')\r\n    return features, labels\r\n\r\n  # Extract lines from input files using the Dataset API.\r\n  dataset = tf.data.TextLineDataset(data_files)\r\n  if shuffle:\r\n    dataset = dataset.shuffle(buffer_size=batch_size*3)\r\n  dataset = dataset.map(parse_line, num_parallel_calls=1)\r\n  dataset = dataset.repeat(num_epochs)\r\n  dataset = dataset.batch(batch_size)\r\n\r\n  iterator = dataset.make_one_shot_iterator()\r\n  features, labels = iterator.get_next(\"iterator\")\r\n  return features, labels\r\n```\r\nI use custom op decode_file, which is like tf.decode_csv.\r\n\r\nthe decode_file's SetShapeFn is as follows: \r\n```\r\n    .SetShapeFn([](InferenceContext* c) {\r\n      std::vector<int> output_size;\r\n      c->GetAttr(\"output_size\", &output_size);\r\n\r\n      for (int i = 0; i < c->num_outputs(); ++i) {\r\n        ShapeHandle s = c->MakeShape({DimensionOrConstant(output_size[i])});\r\n        c->set_output(i, s); \r\n      }   \r\n      return Status::OK();\r\n    })  \r\n```\r\nthis custom op  can work well  in train and evaluate ,  but  when it is used for export_savedmodel api, i need modify  SetShapeFn as follows ( to support batch predict)\r\n``` \r\n .SetShapeFn([](InferenceContext* c) {\r\n      std::vector<int> output_size;\r\n      c->GetAttr(\"output_size\", &output_size);\r\n      for (int i = 0; i < c->num_outputs(); ++i) {\r\n          c->set_output(i, c->Matrix(c->Dim(c->input(0), 0), output_size[i]));\r\n      }\r\n      return Status::OK();\r\n}\r\n\r\n# my serving_input_fn\r\ndef string_decode_serving_input_receiver_fn():\r\n  string_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.string)\r\n\r\n  columns = module_decode_file_serve.decode_file_serve(string_placeholder,\r\n          record_defaults=_COLUMN_DEFAULTS,\r\n          output_size=_COLUMN_SIZESt, \r\n          field_outer_delim=',',\r\n          field_inner_delim='%')\r\n  features = dict(zip(_COLUMN_NAMES, columns))\r\n  features.pop('label')\r\n  return tf.estimator.export.ServingInputReceiver(features, string_placeholder)\r\n```\r\n\r\nWhy does dataset map api (dataset = dataset.map(parse_line)) only handle single record?  and pase_line's shape inference does not include batch dimmension\r\nAt present I need use different SetShapeFn in train and export_savedmodel\r\nCan dataset map function add new feature to  handle batch dimmension ?\r\n@mrry  thanks! (sorry for bad description ...\uff09\r\n"}