{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/260547383", "html_url": "https://github.com/tensorflow/tensorflow/issues/5608#issuecomment-260547383", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5608", "id": 260547383, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MDU0NzM4Mw==", "user": {"login": "DeNeutoy", "id": 16001974, "node_id": "MDQ6VXNlcjE2MDAxOTc0", "avatar_url": "https://avatars1.githubusercontent.com/u/16001974?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DeNeutoy", "html_url": "https://github.com/DeNeutoy", "followers_url": "https://api.github.com/users/DeNeutoy/followers", "following_url": "https://api.github.com/users/DeNeutoy/following{/other_user}", "gists_url": "https://api.github.com/users/DeNeutoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/DeNeutoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DeNeutoy/subscriptions", "organizations_url": "https://api.github.com/users/DeNeutoy/orgs", "repos_url": "https://api.github.com/users/DeNeutoy/repos", "events_url": "https://api.github.com/users/DeNeutoy/events{/privacy}", "received_events_url": "https://api.github.com/users/DeNeutoy/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-15T04:56:17Z", "updated_at": "2016-11-15T05:23:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5994634\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ethancaballero\">@ethancaballero</a> As far as I am aware it is not possible to do this, as the <code>tf.while_loop</code> is not actually creating a dynamic graph - it is merely an instruction to repeat part of the graph until a condition is satisfied.</p>\n<p>However, presumably there is some upper bound on the number of layers your want your model to be able to adaptively select (if not I would strongly advise this - you would be surprised how many steps these kind of models can take). What you should then do, is create these variables beforehand in a python for loop:</p>\n<pre><code>Wt = []\nfor i in range(num_untied_layers):\n    with tf.variable_scope(\"W_{}\".format(i))\n        Wt[i] = tf.get_variable(\"Wt\")\n</code></pre>\n<p>Then, you can evaluate these within your loop by using tf.case - this is a bit ugly and there is probably a better way to do it:</p>\n<pre><code>counter = tf.constant(0)\ndef generate_predicate_function_pair(counter, i):\n    return (tf.less(counter, i), function_using_ith_weight(inputs, weights, ...))\n\npredicate_list = [generate_predicate_function_pair(i) for i in range(max_attention_hops)] \n\n# Now start a while loop which contains a tensor constant called counter which just\n# increments the loop ...\n\nresult = tf.case(predicate_list, exclusive=True)\n</code></pre>\n<p>Note that you will also need to factor this max number of steps into your halting function, as otherwise it will crash if you go over it.</p>", "body_text": "@ethancaballero As far as I am aware it is not possible to do this, as the tf.while_loop is not actually creating a dynamic graph - it is merely an instruction to repeat part of the graph until a condition is satisfied.\nHowever, presumably there is some upper bound on the number of layers your want your model to be able to adaptively select (if not I would strongly advise this - you would be surprised how many steps these kind of models can take). What you should then do, is create these variables beforehand in a python for loop:\nWt = []\nfor i in range(num_untied_layers):\n    with tf.variable_scope(\"W_{}\".format(i))\n        Wt[i] = tf.get_variable(\"Wt\")\n\nThen, you can evaluate these within your loop by using tf.case - this is a bit ugly and there is probably a better way to do it:\ncounter = tf.constant(0)\ndef generate_predicate_function_pair(counter, i):\n    return (tf.less(counter, i), function_using_ith_weight(inputs, weights, ...))\n\npredicate_list = [generate_predicate_function_pair(i) for i in range(max_attention_hops)] \n\n# Now start a while loop which contains a tensor constant called counter which just\n# increments the loop ...\n\nresult = tf.case(predicate_list, exclusive=True)\n\nNote that you will also need to factor this max number of steps into your halting function, as otherwise it will crash if you go over it.", "body": "@ethancaballero As far as I am aware it is not possible to do this, as the `tf.while_loop` is not actually creating a dynamic graph - it is merely an instruction to repeat part of the graph until a condition is satisfied.\n\nHowever, presumably there is some upper bound on the number of layers your want your model to be able to adaptively select (if not I would strongly advise this - you would be surprised how many steps these kind of models can take). What you should then do, is create these variables beforehand in a python for loop:\n\n```\nWt = []\nfor i in range(num_untied_layers):\n    with tf.variable_scope(\"W_{}\".format(i))\n        Wt[i] = tf.get_variable(\"Wt\")\n```\n\nThen, you can evaluate these within your loop by using tf.case - this is a bit ugly and there is probably a better way to do it:\n\n```\ncounter = tf.constant(0)\ndef generate_predicate_function_pair(counter, i):\n    return (tf.less(counter, i), function_using_ith_weight(inputs, weights, ...))\n\npredicate_list = [generate_predicate_function_pair(i) for i in range(max_attention_hops)] \n\n# Now start a while loop which contains a tensor constant called counter which just\n# increments the loop ...\n\nresult = tf.case(predicate_list, exclusive=True)\n```\n\nNote that you will also need to factor this max number of steps into your halting function, as otherwise it will crash if you go over it.\n"}