{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6104", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6104/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6104/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6104/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6104", "id": 193645275, "node_id": "MDU6SXNzdWUxOTM2NDUyNzU=", "number": 6104, "title": "Generating NaN when computing gradient", "user": {"login": "zodiacR", "id": 2304163, "node_id": "MDQ6VXNlcjIzMDQxNjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2304163?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zodiacR", "html_url": "https://github.com/zodiacR", "followers_url": "https://api.github.com/users/zodiacR/followers", "following_url": "https://api.github.com/users/zodiacR/following{/other_user}", "gists_url": "https://api.github.com/users/zodiacR/gists{/gist_id}", "starred_url": "https://api.github.com/users/zodiacR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zodiacR/subscriptions", "organizations_url": "https://api.github.com/users/zodiacR/orgs", "repos_url": "https://api.github.com/users/zodiacR/repos", "events_url": "https://api.github.com/users/zodiacR/events{/privacy}", "received_events_url": "https://api.github.com/users/zodiacR/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-12-05T23:56:48Z", "updated_at": "2016-12-06T16:56:44Z", "closed_at": "2016-12-06T16:56:44Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p><strong>Operating System:</strong><br>\nRed Hat Enterprise Linux Server release 6.6<br>\n<strong>Tensorflow version:</strong><br>\n0.10.0rc0</p>\n<p><strong>Installed version of CUDA and cuDNN:</strong><br>\n/usr/local/cuda/lib64/libcudart.so.7.5.23</p>\n<p>I'm running a model with temporal attention strategy (<a href=\"https://arxiv.org/abs/1608.02927\" rel=\"nofollow\">https://arxiv.org/abs/1608.02927</a>). I use <code>tf.nn.seq2seq.sequence_loss_by_example()</code> to compute the loss, and use adam gradient (lr:0.001) to minimize loss. The loss is not NaN, but gradients of all weigths became NaN values. If I use plain attention strategy, it won't have this NaN problem.</p>\n<p>I even print out all hyperparameters, their values land in a sensible range until their gradients become NaN.</p>\n<p>Hope someone can help me fix this issue. Thanks in advance.</p>", "body_text": "Environment info\nOperating System:\nRed Hat Enterprise Linux Server release 6.6\nTensorflow version:\n0.10.0rc0\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda/lib64/libcudart.so.7.5.23\nI'm running a model with temporal attention strategy (https://arxiv.org/abs/1608.02927). I use tf.nn.seq2seq.sequence_loss_by_example() to compute the loss, and use adam gradient (lr:0.001) to minimize loss. The loss is not NaN, but gradients of all weigths became NaN values. If I use plain attention strategy, it won't have this NaN problem.\nI even print out all hyperparameters, their values land in a sensible range until their gradients become NaN.\nHope someone can help me fix this issue. Thanks in advance.", "body": "### Environment info\r\n**Operating System:**\r\nRed Hat Enterprise Linux Server release 6.6\r\n**Tensorflow version:** \r\n0.10.0rc0\r\n\r\n**Installed version of CUDA and cuDNN:** \r\n/usr/local/cuda/lib64/libcudart.so.7.5.23\r\n\r\nI'm running a model with temporal attention strategy (https://arxiv.org/abs/1608.02927). I use `tf.nn.seq2seq.sequence_loss_by_example()` to compute the loss, and use adam gradient (lr:0.001) to minimize loss. The loss is not NaN, but gradients of all weigths became NaN values. If I use plain attention strategy, it won't have this NaN problem.\r\n\r\nI even print out all hyperparameters, their values land in a sensible range until their gradients become NaN.\r\n\r\nHope someone can help me fix this issue. Thanks in advance.\r\n"}