{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20176", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20176/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20176/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20176/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20176", "id": 334406629, "node_id": "MDU6SXNzdWUzMzQ0MDY2Mjk=", "number": 20176, "title": "Mobilenet V2 SSDLite inference test with Android", "user": {"login": "achraf-boussaada", "id": 31047155, "node_id": "MDQ6VXNlcjMxMDQ3MTU1", "avatar_url": "https://avatars2.githubusercontent.com/u/31047155?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achraf-boussaada", "html_url": "https://github.com/achraf-boussaada", "followers_url": "https://api.github.com/users/achraf-boussaada/followers", "following_url": "https://api.github.com/users/achraf-boussaada/following{/other_user}", "gists_url": "https://api.github.com/users/achraf-boussaada/gists{/gist_id}", "starred_url": "https://api.github.com/users/achraf-boussaada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achraf-boussaada/subscriptions", "organizations_url": "https://api.github.com/users/achraf-boussaada/orgs", "repos_url": "https://api.github.com/users/achraf-boussaada/repos", "events_url": "https://api.github.com/users/achraf-boussaada/events{/privacy}", "received_events_url": "https://api.github.com/users/achraf-boussaada/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "achowdhery", "id": 4723042, "node_id": "MDQ6VXNlcjQ3MjMwNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4723042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achowdhery", "html_url": "https://github.com/achowdhery", "followers_url": "https://api.github.com/users/achowdhery/followers", "following_url": "https://api.github.com/users/achowdhery/following{/other_user}", "gists_url": "https://api.github.com/users/achowdhery/gists{/gist_id}", "starred_url": "https://api.github.com/users/achowdhery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achowdhery/subscriptions", "organizations_url": "https://api.github.com/users/achowdhery/orgs", "repos_url": "https://api.github.com/users/achowdhery/repos", "events_url": "https://api.github.com/users/achowdhery/events{/privacy}", "received_events_url": "https://api.github.com/users/achowdhery/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "achowdhery", "id": 4723042, "node_id": "MDQ6VXNlcjQ3MjMwNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4723042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achowdhery", "html_url": "https://github.com/achowdhery", "followers_url": "https://api.github.com/users/achowdhery/followers", "following_url": "https://api.github.com/users/achowdhery/following{/other_user}", "gists_url": "https://api.github.com/users/achowdhery/gists{/gist_id}", "starred_url": "https://api.github.com/users/achowdhery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achowdhery/subscriptions", "organizations_url": "https://api.github.com/users/achowdhery/orgs", "repos_url": "https://api.github.com/users/achowdhery/repos", "events_url": "https://api.github.com/users/achowdhery/events{/privacy}", "received_events_url": "https://api.github.com/users/achowdhery/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-06-21T09:17:14Z", "updated_at": "2018-07-16T23:28:26Z", "closed_at": "2018-07-16T23:28:26Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nno</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nmacOS Sierra version 10.12.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nfrom source</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.8.0 to train and 1.9.0 to build the inference library</li>\n<li><strong>Python version</strong>:<br>\n3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.13.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nGCC 4.2.1</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nusing CPU only</li>\n<li><strong>GPU model and memory</strong>:<br>\nusing CPU only</li>\n<li><strong>Exact command to reproduce</strong>:<br>\ncommand to create the appropriate Ops for the Mobilenet V2 SSDLlite (since the default Ops are not enough to make it work on Android) :</li>\n</ul>\n<pre><code> bazel build tensorflow/python/tools:print_selective_registration_header &amp;&amp; \\\n bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\n  --graphs=path/to/graph.pb &gt; ops_to_register.h \n</code></pre>\n<p>command to build <code>tensorflow_inference.so</code> (I build it for all possible CPU types):</p>\n<pre><code>      bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\n    --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\n    //tensorflow/contrib/android:libtensorflow_inference.so \\\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n    --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a \n</code></pre>\n<p>command to optimize the graph for inference :</p>\n<pre><code>bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=$DETECT_PB \\\n--out_graph=$STRIPPED_PB \\\n--inputs='image_tensor' \\\n--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\\n--transforms='\n  strip_unused_nodes(type=float, shape=\"1,300,300,3\")\n  remove_nodes(op=Identity, op=CheckNumerics)\n  fold_constants(ignore_errors=true)\n  fold_batch_norms\n  fold_old_batch_norms'\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I'm working with the Object Detection Demo for android and I'm using my custom trained model (used Mobilenet V1 SSD for the training part) and everything works as expected. As the Tensorflow team released the Mobilenet V2 SSDLite (supposed to be 35% faster than the V1 SSD version) I wanted to give it a try and retrained my model. I optimized the graph for inference and tested it but I was disappointed with the results.</p>\n<p><strong>Test device Pixel 2 :</strong></p>\n<p>Mobilenet V2 SSDLite</p>\n<ul>\n<li>nodes observed : 1185</li>\n<li>inference time : around 1200ms</li>\n</ul>\n<p>Mobilenet V1 SSD</p>\n<ul>\n<li>nodes observed : 893</li>\n<li>inference time : around 300ms</li>\n</ul>\n<p>Instead of adding the Tensorflow dependency in my Gradle I'm using the latest stable Nightly build for both test use cases but when testing with the V2 model I replace the <code>libtensorflow_inference.so</code> with the manually built one.</p>\n<p>any idea why I'm getting these results ?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nno\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nmacOS Sierra version 10.12.6\nTensorFlow installed from (source or binary):\nfrom source\nTensorFlow version (use command below):\n1.8.0 to train and 1.9.0 to build the inference library\nPython version:\n3.6.5\nBazel version (if compiling from source):\n0.13.0\nGCC/Compiler version (if compiling from source):\nGCC 4.2.1\nCUDA/cuDNN version:\nusing CPU only\nGPU model and memory:\nusing CPU only\nExact command to reproduce:\ncommand to create the appropriate Ops for the Mobilenet V2 SSDLlite (since the default Ops are not enough to make it work on Android) :\n\n bazel build tensorflow/python/tools:print_selective_registration_header && \\\n bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\n  --graphs=path/to/graph.pb > ops_to_register.h \n\ncommand to build tensorflow_inference.so (I build it for all possible CPU types):\n      bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\n    --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\n    //tensorflow/contrib/android:libtensorflow_inference.so \\\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n    --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a \n\ncommand to optimize the graph for inference :\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=$DETECT_PB \\\n--out_graph=$STRIPPED_PB \\\n--inputs='image_tensor' \\\n--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\\n--transforms='\n  strip_unused_nodes(type=float, shape=\"1,300,300,3\")\n  remove_nodes(op=Identity, op=CheckNumerics)\n  fold_constants(ignore_errors=true)\n  fold_batch_norms\n  fold_old_batch_norms'\n\nDescribe the problem\nI'm working with the Object Detection Demo for android and I'm using my custom trained model (used Mobilenet V1 SSD for the training part) and everything works as expected. As the Tensorflow team released the Mobilenet V2 SSDLite (supposed to be 35% faster than the V1 SSD version) I wanted to give it a try and retrained my model. I optimized the graph for inference and tested it but I was disappointed with the results.\nTest device Pixel 2 :\nMobilenet V2 SSDLite\n\nnodes observed : 1185\ninference time : around 1200ms\n\nMobilenet V1 SSD\n\nnodes observed : 893\ninference time : around 300ms\n\nInstead of adding the Tensorflow dependency in my Gradle I'm using the latest stable Nightly build for both test use cases but when testing with the V2 model I replace the libtensorflow_inference.so with the manually built one.\nany idea why I'm getting these results ?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS Sierra version 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom source\r\n- **TensorFlow version (use command below)**:\r\n1.8.0 to train and 1.9.0 to build the inference library\r\n- **Python version**: \r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC 4.2.1\r\n- **CUDA/cuDNN version**:\r\nusing CPU only\r\n- **GPU model and memory**:\r\nusing CPU only\r\n- **Exact command to reproduce**:\r\ncommand to create the appropriate Ops for the Mobilenet V2 SSDLlite (since the default Ops are not enough to make it work on Android) :\r\n```\r\n bazel build tensorflow/python/tools:print_selective_registration_header && \\\r\n bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n  --graphs=path/to/graph.pb > ops_to_register.h \r\n```\r\n\r\ncommand to build `tensorflow_inference.so` (I build it for all possible CPU types): \r\n```\r\n      bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\r\n    --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n    //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n    --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a \r\n```\r\ncommand to optimize the graph for inference : \r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=$DETECT_PB \\\r\n--out_graph=$STRIPPED_PB \\\r\n--inputs='image_tensor' \\\r\n--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,300,300,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms'\r\n```\r\n### Describe the problem\r\nI'm working with the Object Detection Demo for android and I'm using my custom trained model (used Mobilenet V1 SSD for the training part) and everything works as expected. As the Tensorflow team released the Mobilenet V2 SSDLite (supposed to be 35% faster than the V1 SSD version) I wanted to give it a try and retrained my model. I optimized the graph for inference and tested it but I was disappointed with the results. \r\n\r\n**Test device Pixel 2 :** \r\n\r\nMobilenet V2 SSDLite\r\n- nodes observed : 1185\r\n- inference time : around 1200ms  \r\n\r\nMobilenet V1 SSD \r\n- nodes observed : 893\r\n- inference time : around 300ms\r\n\r\nInstead of adding the Tensorflow dependency in my Gradle I'm using the latest stable Nightly build for both test use cases but when testing with the V2 model I replace the `libtensorflow_inference.so` with the manually built one.\r\n\r\nany idea why I'm getting these results ?\r\n"}