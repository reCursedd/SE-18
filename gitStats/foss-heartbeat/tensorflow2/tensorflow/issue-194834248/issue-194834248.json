{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6253", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6253/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6253/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6253/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6253", "id": 194834248, "node_id": "MDU6SXNzdWUxOTQ4MzQyNDg=", "number": 6253, "title": "reduce_mean operation gives inconsistent results on GPU", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-12-11T14:18:18Z", "updated_at": "2016-12-11T19:02:59Z", "closed_at": "2016-12-11T19:02:59Z", "author_association": "NONE", "body_html": "<p>After hyperparameter optimization, the program I wrote does some checks to see that the results are consistent. However even with the same seeds everywhere, I found that TF doesn't always give the same results. Furthermore: the results of numpy mean and tensorflow reduce_mean also differ.</p>\n<p>After quite some digging, I found it's because the reduce_mean operation on the GPU gives inconsistent results. I think it's because the last two bits of the mantissa of the float returned are sometimes 01 and sometimes 10. The differences are minimally, however after a lot of reduce_mean operations, the differences can become quite significant.</p>\n<p>Below I've included a short code to reproduce the error. When using the GPU the results of the mean operation are sometimes different. When using the CPU the results are consistent However with both CPU &amp; GPU the results of Numpy and TensorFlow are still sometimes different.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None</p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nDebian 8.6<br>\nKernel: Linux 3.16.0-4-amd64</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n/usr/local/cuda/lib64/libcudadevrt.a /usr/local/cuda/lib64/libcudart.so.8.0.44 /usr/local/cuda/lib64/libcudnn.so.5<br>\n/usr/local/cuda/lib64/libcudart.so /usr/local/cuda/lib64/libcudart_static.a /usr/local/cuda/lib64/libcudnn.so.5.1.5<br>\n/usr/local/cuda/lib64/libcudart.so.8.0 /usr/local/cuda/lib64/libcudnn.so /usr/local/cuda/lib64/libcudnn_static.a</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>\n<p>A link to the pip package you installed:<br>\npip install tensorflow-gpu</p>\n</li>\n<li>\n<p>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.<br>\n0.12.rc1</p>\n</li>\n</ol>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)</li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>import os\n# comment line below to use CPU instead of GPU\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\n\nimport tensorflow as tf\nimport numpy as np\n\nSIZE = 1000\n\ntf_x = tf.placeholder(tf.float32, (None))\ntf_var2 = tf.reduce_mean(tf_x)\n\nx = np.random.rand(SIZE).astype(np.float32)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\ntf_mean= np.empty(SIZE, dtype=np.float32)\nnp_mean=np.empty(SIZE, dtype=np.float32)\n\nfor j in range(SIZE):\n    x_evaled, mean_ = sess.run([tf_x, tf_var2], feed_dict={tf_x: x})\n    tf_mean[j] = mean_\n    np_mean[j] = x.mean()\n\nsame_ = (tf_mean == np_mean).astype(np.float32).mean()\nconsistency_ = (tf_mean == tf_mean[0]).astype(np.float32).mean()\n\n# print results\nprint('\\nMin, Max TF mean: {}, {} \\nMin, Max NP mean: {}, {}'.format(tf_mean.min(), tf_mean.max(), np_mean.min(), np_mean.max()))\nprint('TF &amp; NP was the same: {:.2%}'.format(same_))\nprint('TF consistency: {:.2%}'.format(consistency_))\n</code></pre>\n<h3>What other attempted solutions have you tried?</h3>\n<p>Changing the float to 16, 32, or 64 bit made no difference</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>Using CPU:<br>\n'''<br>\nMin, Max TF mean: 0.49667325615882874, 0.49667325615882874<br>\nMin, Max NP mean: 0.4966733157634735, 0.4966733157634735<br>\nTF &amp; NP were the same: 0.00%<br>\nTF consistency: 100.00%<br>\n'''<br>\nUsing GPU:<br>\n'''<br>\nMin, Max TF mean: 0.49240124225616455, 0.4924013018608093<br>\nMin, Max NP mean: 0.49240124225616455, 0.49240124225616455<br>\nTF &amp; NP were the same: 29.80%<br>\nTF consistency: 47.90%<br>\n'''</p>", "body_text": "After hyperparameter optimization, the program I wrote does some checks to see that the results are consistent. However even with the same seeds everywhere, I found that TF doesn't always give the same results. Furthermore: the results of numpy mean and tensorflow reduce_mean also differ.\nAfter quite some digging, I found it's because the reduce_mean operation on the GPU gives inconsistent results. I think it's because the last two bits of the mantissa of the float returned are sometimes 01 and sometimes 10. The differences are minimally, however after a lot of reduce_mean operations, the differences can become quite significant.\nBelow I've included a short code to reproduce the error. When using the GPU the results of the mean operation are sometimes different. When using the CPU the results are consistent However with both CPU & GPU the results of Numpy and TensorFlow are still sometimes different.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone\nEnvironment info\nOperating System:\nDebian 8.6\nKernel: Linux 3.16.0-4-amd64\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n/usr/local/cuda/lib64/libcudadevrt.a /usr/local/cuda/lib64/libcudart.so.8.0.44 /usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudart.so /usr/local/cuda/lib64/libcudart_static.a /usr/local/cuda/lib64/libcudnn.so.5.1.5\n/usr/local/cuda/lib64/libcudart.so.8.0 /usr/local/cuda/lib64/libcudnn.so /usr/local/cuda/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\n\nA link to the pip package you installed:\npip install tensorflow-gpu\n\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n0.12.rc1\n\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport os\n# comment line below to use CPU instead of GPU\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\n\nimport tensorflow as tf\nimport numpy as np\n\nSIZE = 1000\n\ntf_x = tf.placeholder(tf.float32, (None))\ntf_var2 = tf.reduce_mean(tf_x)\n\nx = np.random.rand(SIZE).astype(np.float32)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\ntf_mean= np.empty(SIZE, dtype=np.float32)\nnp_mean=np.empty(SIZE, dtype=np.float32)\n\nfor j in range(SIZE):\n    x_evaled, mean_ = sess.run([tf_x, tf_var2], feed_dict={tf_x: x})\n    tf_mean[j] = mean_\n    np_mean[j] = x.mean()\n\nsame_ = (tf_mean == np_mean).astype(np.float32).mean()\nconsistency_ = (tf_mean == tf_mean[0]).astype(np.float32).mean()\n\n# print results\nprint('\\nMin, Max TF mean: {}, {} \\nMin, Max NP mean: {}, {}'.format(tf_mean.min(), tf_mean.max(), np_mean.min(), np_mean.max()))\nprint('TF & NP was the same: {:.2%}'.format(same_))\nprint('TF consistency: {:.2%}'.format(consistency_))\n\nWhat other attempted solutions have you tried?\nChanging the float to 16, 32, or 64 bit made no difference\nLogs or other output that would be helpful\nUsing CPU:\n'''\nMin, Max TF mean: 0.49667325615882874, 0.49667325615882874\nMin, Max NP mean: 0.4966733157634735, 0.4966733157634735\nTF & NP were the same: 0.00%\nTF consistency: 100.00%\n'''\nUsing GPU:\n'''\nMin, Max TF mean: 0.49240124225616455, 0.4924013018608093\nMin, Max NP mean: 0.49240124225616455, 0.49240124225616455\nTF & NP were the same: 29.80%\nTF consistency: 47.90%\n'''", "body": "After hyperparameter optimization, the program I wrote does some checks to see that the results are consistent. However even with the same seeds everywhere, I found that TF doesn't always give the same results. Furthermore: the results of numpy mean and tensorflow reduce_mean also differ.\r\n\r\nAfter quite some digging, I found it's because the reduce_mean operation on the GPU gives inconsistent results. I think it's because the last two bits of the mantissa of the float returned are sometimes 01 and sometimes 10. The differences are minimally, however after a lot of reduce_mean operations, the differences can become quite significant.\r\n\r\nBelow I've included a short code to reproduce the error. When using the GPU the results of the mean operation are sometimes different. When using the CPU the results are consistent However with both CPU & GPU the results of Numpy and TensorFlow are still sometimes different.\r\n\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System:\r\nDebian 8.6\r\nKernel: Linux 3.16.0-4-amd64\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n/usr/local/cuda/lib64/libcudadevrt.a /usr/local/cuda/lib64/libcudart.so.8.0.44 /usr/local/cuda/lib64/libcudnn.so.5\r\n/usr/local/cuda/lib64/libcudart.so /usr/local/cuda/lib64/libcudart_static.a /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda/lib64/libcudart.so.8.0 /usr/local/cuda/lib64/libcudnn.so /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\npip install tensorflow-gpu\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.rc1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nimport os\r\n# comment line below to use CPU instead of GPU\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nSIZE = 1000\r\n\r\ntf_x = tf.placeholder(tf.float32, (None))\r\ntf_var2 = tf.reduce_mean(tf_x)\r\n\r\nx = np.random.rand(SIZE).astype(np.float32)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\ntf_mean= np.empty(SIZE, dtype=np.float32)\r\nnp_mean=np.empty(SIZE, dtype=np.float32)\r\n\r\nfor j in range(SIZE):\r\n    x_evaled, mean_ = sess.run([tf_x, tf_var2], feed_dict={tf_x: x})\r\n    tf_mean[j] = mean_\r\n    np_mean[j] = x.mean()\r\n\r\nsame_ = (tf_mean == np_mean).astype(np.float32).mean()\r\nconsistency_ = (tf_mean == tf_mean[0]).astype(np.float32).mean()\r\n\r\n# print results\r\nprint('\\nMin, Max TF mean: {}, {} \\nMin, Max NP mean: {}, {}'.format(tf_mean.min(), tf_mean.max(), np_mean.min(), np_mean.max()))\r\nprint('TF & NP was the same: {:.2%}'.format(same_))\r\nprint('TF consistency: {:.2%}'.format(consistency_))\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nChanging the float to 16, 32, or 64 bit made no difference\r\n\r\n### Logs or other output that would be helpful\r\nUsing CPU:\r\n'''\r\nMin, Max TF mean: 0.49667325615882874, 0.49667325615882874 \r\nMin, Max NP mean: 0.4966733157634735, 0.4966733157634735\r\nTF & NP were the same: 0.00%\r\nTF consistency: 100.00%\r\n'''\r\nUsing GPU:\r\n'''\r\nMin, Max TF mean: 0.49240124225616455, 0.4924013018608093 \r\nMin, Max NP mean: 0.49240124225616455, 0.49240124225616455\r\nTF & NP were the same: 29.80%\r\nTF consistency: 47.90%\r\n'''\r\n"}