{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/79580860", "pull_request_review_id": 718826, "id": 79580860, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc5NTgwODYw", "diff_hunk": "@@ -262,14 +262,15 @@ class Im2ColConvFunctor {\n                 errors::InvalidArgument(\"Im2Col patch too large for buffer\"));\n     const size_t patches_per_chunk =\n         max_chunk_size / (filter_value_count * sizeof(T1));\n+    const size_t chunk_value_count = max_chunk_size / sizeof(T1);", "path": "tensorflow/core/kernels/conv_ops_using_gemm.cc", "position": null, "original_position": 4, "commit_id": "d6f387ac14119a959fab5e87680f79aaa2807978", "original_commit_id": "3908e2d3dc799daac0f756fe6fb0f54a3087c9df", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "body": "This is a bug fix because I was actually using max_chunk_size (which is in bytes) as the number of _elements_ to allocate below. This led to a 64MB buffer being allocated for 16m floats, rather than the 16MB buffer with 4m floats that I was expecting.\n\nGood point on the size calculation! I've updated it to effective do a ceil on the integer calculation, to make sure we allocate enough space even for odd-sized types.\n", "created_at": "2016-09-20T10:33:57Z", "updated_at": "2016-09-20T10:34:11Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4457#discussion_r79580860", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4457", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/79580860"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4457#discussion_r79580860"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4457"}}, "body_html": "<p>This is a bug fix because I was actually using max_chunk_size (which is in bytes) as the number of <em>elements</em> to allocate below. This led to a 64MB buffer being allocated for 16m floats, rather than the 16MB buffer with 4m floats that I was expecting.</p>\n<p>Good point on the size calculation! I've updated it to effective do a ceil on the integer calculation, to make sure we allocate enough space even for odd-sized types.</p>", "body_text": "This is a bug fix because I was actually using max_chunk_size (which is in bytes) as the number of elements to allocate below. This led to a 64MB buffer being allocated for 16m floats, rather than the 16MB buffer with 4m floats that I was expecting.\nGood point on the size calculation! I've updated it to effective do a ceil on the integer calculation, to make sure we allocate enough space even for odd-sized types.", "in_reply_to_id": 79404708}