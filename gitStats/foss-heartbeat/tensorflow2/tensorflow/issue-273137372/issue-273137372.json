{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14481", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14481/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14481/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14481/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14481", "id": 273137372, "node_id": "MDU6SXNzdWUyNzMxMzczNzI=", "number": 14481, "title": "Tensorflow stops training on random epoch", "user": {"login": "amin07", "id": 15855504, "node_id": "MDQ6VXNlcjE1ODU1NTA0", "avatar_url": "https://avatars1.githubusercontent.com/u/15855504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amin07", "html_url": "https://github.com/amin07", "followers_url": "https://api.github.com/users/amin07/followers", "following_url": "https://api.github.com/users/amin07/following{/other_user}", "gists_url": "https://api.github.com/users/amin07/gists{/gist_id}", "starred_url": "https://api.github.com/users/amin07/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amin07/subscriptions", "organizations_url": "https://api.github.com/users/amin07/orgs", "repos_url": "https://api.github.com/users/amin07/repos", "events_url": "https://api.github.com/users/amin07/events{/privacy}", "received_events_url": "https://api.github.com/users/amin07/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-11-11T10:56:32Z", "updated_at": "2017-12-20T02:28:06Z", "closed_at": "2017-12-20T02:27:48Z", "author_association": "NONE", "body_html": "<p>When I run the following code on GPU, it trains well for some epoches and then just hangs.<br>\nWhile hanged processes are still alive but the GPU usages become 0%.<br>\nIn the below code I am using Dataset API from tf.contrib.data.Dataset. But I also tried using placeholder and feed dictionary approach which hangs as well on random epoch during training.<br>\nI am struggling with the problem for last 2-3 weeks and cannot find a way out.<br>\nI am running the code on a remote GPU cluster. Here are some information about cluster node,<br>\nUsing tensorflow gpu version 1.4</p>\n<pre><code>   NodeName=node050 Arch=x86_64 CoresPerSocket=1\n   CPUAlloc=0 CPUErr=0 CPUTot=24 CPULoad=12.03 Features=Proc24,GPU4\n   Gres=gpu:4\n   NodeAddr=node050 NodeHostName=node050 Version=15.08\n   OS=Linux RealMemory=129088 AllocMem=0 FreeMem=125664 Sockets=24 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A\n   BootTime=2017-11-07T08:20:00 SlurmdStartTime=2017-11-07T08:24:06\n   CapWatts=n/a\n   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n</code></pre>\n<p>CODE</p>\n<pre><code>dat_split = np.load('data/dat_split2.npy')\nX_train = dat_split[0].astype(np.float32)\nX_test = dat_split[1].astype(np.float32)\ny_train = dat_split[2].astype(np.int32)\ny_test = dat_split[3].astype(np.int32)\n\nnum_epochs = 100\n\n\ntrain_data_len = X_train.shape[0]\ntest_data_len = X_test.shape[0]\nnum_joints = len(considered_joints)\nnum_classes = len(classes)\n\n\n############ taking batch_size even data##########\neven_train_len = (train_data_len//batch_size)*batch_size\neven_test_len = (test_data_len//batch_size)*batch_size\n\nX_train = X_train[:even_train_len]\nX_test = X_test[:even_test_len]\ny_train = y_train[:even_train_len]\ny_test = y_test[:even_test_len]\n\n\ntrain_dat = Dataset.from_tensor_slices((X_train, y_train))\ntrain_dat = train_dat.batch(batch_size)\n\ntest_dat  = Dataset.from_tensor_slices((X_test, y_test))\ntest_dat = test_dat.batch(batch_size)\n    \niterator = Iterator.from_structure(train_dat.output_types, train_dat.output_shapes)\n\ntrainig_iterator_init = iterator.make_initializer(train_dat)\ntest_iterator_init = iterator.make_initializer(test_dat)\n   \nif __name__ == '__main__':\n   \n    global_cell = GlobalLSTM(num_units=num_units_each_cell, num_joints=num_joints)   #GlobalLSTM is a subtype of RNNCell\n    next_element = iterator.get_next()\n    X_loaded2, Y_loaded = next_element\n    X_loaded = tf.where(tf.is_nan(X_loaded2), tf.zeros_like(X_loaded2), X_loaded2)\n    \n    init_state = global_cell.zero_state((batch_size), tf.float32)\n    rnn_ops, rnn_state = tf.nn.dynamic_rnn(global_cell, X_loaded, dtype=tf.float32)\n    \n    with tf.variable_scope('softmax__'):\n        W = tf.get_variable('W', [(num_joints)*num_units_each_cell, num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\n        b = tf.get_variable('b', [num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\n\n  \n      \n    final_logits = tf.matmul(rnn_state[1], W) + b       # taking h state of rnn \n    with tf.name_scope(\"loss_comp\"):\n        total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_logits, labels=tf.one_hot(Y_loaded, num_classes)))\n    with tf.name_scope(\"train_step\"):\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n\n    with tf.name_scope(\"pred_accu\"):\n        predictions = tf.nn.softmax(final_logits)\n        pred2 = tf.reshape(tf.argmax(predictions, 1), [-1, 1])\n        correct_pred = tf.equal(pred2, tf.cast(Y_loaded, tf.int64))\n        accuracy_ = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    \n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n  \n        tic = time.clock()    \n        for step in range(num_epochs):\n            sess.run(trainig_iterator_init)\n            batch_cnt = train_data_len//batch_size\n            epch_loss = 0.0\n            epch_acc = 0.0\n            for bt in range(batch_cnt):\n                _, loss_, acc = sess.run([train_step, total_loss, accuracy_])\n                epch_loss += loss_\n                epch_acc += acc\n            print ('loss after epoch, ', step,': ', epch_loss/batch_cnt, ' ## accuracy : ', epch_acc/batch_cnt)\n            \n        print (\"optimization finished, time required: \", time.clock()-tic)\n\t\t\n\t\t\n\t\t#############test accuracy##############\n        batch_cnt = test_data_len//batch_size\n        sess.run(test_iterator_init)\n        print ('testing accuracy on test data : batch number', batch_cnt)\n        epch_acc = 0.0\n        for bt in range(batch_cnt):\n            acc = sess.run(accuracy_)\n            epch_acc += acc\n        print ('testing accuracy : ', epch_acc/batch_cnt)  \n</code></pre>\n<p>Here are some screen shot of different hangs,<br>\n<strong>Hanged on an epoch</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/15855504/32688711-1be4b622-c6a4-11e7-93eb-92f29ad8ee37.JPG\"><img src=\"https://user-images.githubusercontent.com/15855504/32688711-1be4b622-c6a4-11e7-93eb-92f29ad8ee37.JPG\" alt=\"hanged_epc\" style=\"max-width:100%;\"></a><br>\n<strong>GPU usage that time</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/15855504/32688718-3763037c-c6a4-11e7-9ed0-d89efd1663c6.JPG\"><img src=\"https://user-images.githubusercontent.com/15855504/32688718-3763037c-c6a4-11e7-9ed0-d89efd1663c6.JPG\" alt=\"hanged\" style=\"max-width:100%;\"></a><br>\n<strong>GPU usage while running (not hanged)</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/15855504/32688721-4b8d42cc-c6a4-11e7-9ebe-61d48ed2bcac.JPG\"><img src=\"https://user-images.githubusercontent.com/15855504/32688721-4b8d42cc-c6a4-11e7-9ebe-61d48ed2bcac.JPG\" alt=\"running_gpuusage\" style=\"max-width:100%;\"></a><br>\n<strong>Hanged on another eopch</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/15855504/32688742-b35474de-c6a4-11e7-8492-493d9ce128cb.JPG\"><img src=\"https://user-images.githubusercontent.com/15855504/32688742-b35474de-c6a4-11e7-8492-493d9ce128cb.JPG\" alt=\"hanged2\" style=\"max-width:100%;\"></a></p>\n<p>This type of random hanging behavior keeps repeating on each run.<br>\nEach time it hangs on a random epoch. That's why I cannot figure out what is going wrong.<br>\nBy looking at code or other set up can anybody please give me any idea about what is going wrong or how can I debug this out? Thanks</p>", "body_text": "When I run the following code on GPU, it trains well for some epoches and then just hangs.\nWhile hanged processes are still alive but the GPU usages become 0%.\nIn the below code I am using Dataset API from tf.contrib.data.Dataset. But I also tried using placeholder and feed dictionary approach which hangs as well on random epoch during training.\nI am struggling with the problem for last 2-3 weeks and cannot find a way out.\nI am running the code on a remote GPU cluster. Here are some information about cluster node,\nUsing tensorflow gpu version 1.4\n   NodeName=node050 Arch=x86_64 CoresPerSocket=1\n   CPUAlloc=0 CPUErr=0 CPUTot=24 CPULoad=12.03 Features=Proc24,GPU4\n   Gres=gpu:4\n   NodeAddr=node050 NodeHostName=node050 Version=15.08\n   OS=Linux RealMemory=129088 AllocMem=0 FreeMem=125664 Sockets=24 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A\n   BootTime=2017-11-07T08:20:00 SlurmdStartTime=2017-11-07T08:24:06\n   CapWatts=n/a\n   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n\nCODE\ndat_split = np.load('data/dat_split2.npy')\nX_train = dat_split[0].astype(np.float32)\nX_test = dat_split[1].astype(np.float32)\ny_train = dat_split[2].astype(np.int32)\ny_test = dat_split[3].astype(np.int32)\n\nnum_epochs = 100\n\n\ntrain_data_len = X_train.shape[0]\ntest_data_len = X_test.shape[0]\nnum_joints = len(considered_joints)\nnum_classes = len(classes)\n\n\n############ taking batch_size even data##########\neven_train_len = (train_data_len//batch_size)*batch_size\neven_test_len = (test_data_len//batch_size)*batch_size\n\nX_train = X_train[:even_train_len]\nX_test = X_test[:even_test_len]\ny_train = y_train[:even_train_len]\ny_test = y_test[:even_test_len]\n\n\ntrain_dat = Dataset.from_tensor_slices((X_train, y_train))\ntrain_dat = train_dat.batch(batch_size)\n\ntest_dat  = Dataset.from_tensor_slices((X_test, y_test))\ntest_dat = test_dat.batch(batch_size)\n    \niterator = Iterator.from_structure(train_dat.output_types, train_dat.output_shapes)\n\ntrainig_iterator_init = iterator.make_initializer(train_dat)\ntest_iterator_init = iterator.make_initializer(test_dat)\n   \nif __name__ == '__main__':\n   \n    global_cell = GlobalLSTM(num_units=num_units_each_cell, num_joints=num_joints)   #GlobalLSTM is a subtype of RNNCell\n    next_element = iterator.get_next()\n    X_loaded2, Y_loaded = next_element\n    X_loaded = tf.where(tf.is_nan(X_loaded2), tf.zeros_like(X_loaded2), X_loaded2)\n    \n    init_state = global_cell.zero_state((batch_size), tf.float32)\n    rnn_ops, rnn_state = tf.nn.dynamic_rnn(global_cell, X_loaded, dtype=tf.float32)\n    \n    with tf.variable_scope('softmax__'):\n        W = tf.get_variable('W', [(num_joints)*num_units_each_cell, num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\n        b = tf.get_variable('b', [num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\n\n  \n      \n    final_logits = tf.matmul(rnn_state[1], W) + b       # taking h state of rnn \n    with tf.name_scope(\"loss_comp\"):\n        total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_logits, labels=tf.one_hot(Y_loaded, num_classes)))\n    with tf.name_scope(\"train_step\"):\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n\n    with tf.name_scope(\"pred_accu\"):\n        predictions = tf.nn.softmax(final_logits)\n        pred2 = tf.reshape(tf.argmax(predictions, 1), [-1, 1])\n        correct_pred = tf.equal(pred2, tf.cast(Y_loaded, tf.int64))\n        accuracy_ = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    \n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n  \n        tic = time.clock()    \n        for step in range(num_epochs):\n            sess.run(trainig_iterator_init)\n            batch_cnt = train_data_len//batch_size\n            epch_loss = 0.0\n            epch_acc = 0.0\n            for bt in range(batch_cnt):\n                _, loss_, acc = sess.run([train_step, total_loss, accuracy_])\n                epch_loss += loss_\n                epch_acc += acc\n            print ('loss after epoch, ', step,': ', epch_loss/batch_cnt, ' ## accuracy : ', epch_acc/batch_cnt)\n            \n        print (\"optimization finished, time required: \", time.clock()-tic)\n\t\t\n\t\t\n\t\t#############test accuracy##############\n        batch_cnt = test_data_len//batch_size\n        sess.run(test_iterator_init)\n        print ('testing accuracy on test data : batch number', batch_cnt)\n        epch_acc = 0.0\n        for bt in range(batch_cnt):\n            acc = sess.run(accuracy_)\n            epch_acc += acc\n        print ('testing accuracy : ', epch_acc/batch_cnt)  \n\nHere are some screen shot of different hangs,\nHanged on an epoch\n\nGPU usage that time\n\nGPU usage while running (not hanged)\n\nHanged on another eopch\n\nThis type of random hanging behavior keeps repeating on each run.\nEach time it hangs on a random epoch. That's why I cannot figure out what is going wrong.\nBy looking at code or other set up can anybody please give me any idea about what is going wrong or how can I debug this out? Thanks", "body": "When I run the following code on GPU, it trains well for some epoches and then just hangs.\r\nWhile hanged processes are still alive but the GPU usages become 0%.\r\nIn the below code I am using Dataset API from tf.contrib.data.Dataset. But I also tried using placeholder and feed dictionary approach which hangs as well on random epoch during training. \r\nI am struggling with the problem for last 2-3 weeks and cannot find a way out.\r\nI am running the code on a remote GPU cluster. Here are some information about cluster node,\r\nUsing tensorflow gpu version 1.4\r\n```\r\n   NodeName=node050 Arch=x86_64 CoresPerSocket=1\r\n   CPUAlloc=0 CPUErr=0 CPUTot=24 CPULoad=12.03 Features=Proc24,GPU4\r\n   Gres=gpu:4\r\n   NodeAddr=node050 NodeHostName=node050 Version=15.08\r\n   OS=Linux RealMemory=129088 AllocMem=0 FreeMem=125664 Sockets=24 Boards=1\r\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A\r\n   BootTime=2017-11-07T08:20:00 SlurmdStartTime=2017-11-07T08:24:06\r\n   CapWatts=n/a\r\n   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0\r\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\r\n```\r\nCODE\r\n\r\n```\r\ndat_split = np.load('data/dat_split2.npy')\r\nX_train = dat_split[0].astype(np.float32)\r\nX_test = dat_split[1].astype(np.float32)\r\ny_train = dat_split[2].astype(np.int32)\r\ny_test = dat_split[3].astype(np.int32)\r\n\r\nnum_epochs = 100\r\n\r\n\r\ntrain_data_len = X_train.shape[0]\r\ntest_data_len = X_test.shape[0]\r\nnum_joints = len(considered_joints)\r\nnum_classes = len(classes)\r\n\r\n\r\n############ taking batch_size even data##########\r\neven_train_len = (train_data_len//batch_size)*batch_size\r\neven_test_len = (test_data_len//batch_size)*batch_size\r\n\r\nX_train = X_train[:even_train_len]\r\nX_test = X_test[:even_test_len]\r\ny_train = y_train[:even_train_len]\r\ny_test = y_test[:even_test_len]\r\n\r\n\r\ntrain_dat = Dataset.from_tensor_slices((X_train, y_train))\r\ntrain_dat = train_dat.batch(batch_size)\r\n\r\ntest_dat  = Dataset.from_tensor_slices((X_test, y_test))\r\ntest_dat = test_dat.batch(batch_size)\r\n    \r\niterator = Iterator.from_structure(train_dat.output_types, train_dat.output_shapes)\r\n\r\ntrainig_iterator_init = iterator.make_initializer(train_dat)\r\ntest_iterator_init = iterator.make_initializer(test_dat)\r\n   \r\nif __name__ == '__main__':\r\n   \r\n    global_cell = GlobalLSTM(num_units=num_units_each_cell, num_joints=num_joints)   #GlobalLSTM is a subtype of RNNCell\r\n    next_element = iterator.get_next()\r\n    X_loaded2, Y_loaded = next_element\r\n    X_loaded = tf.where(tf.is_nan(X_loaded2), tf.zeros_like(X_loaded2), X_loaded2)\r\n    \r\n    init_state = global_cell.zero_state((batch_size), tf.float32)\r\n    rnn_ops, rnn_state = tf.nn.dynamic_rnn(global_cell, X_loaded, dtype=tf.float32)\r\n    \r\n    with tf.variable_scope('softmax__'):\r\n        W = tf.get_variable('W', [(num_joints)*num_units_each_cell, num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\r\n        b = tf.get_variable('b', [num_classes], initializer=tf.truncated_normal_initializer(0.0, 1.0))\r\n\r\n  \r\n      \r\n    final_logits = tf.matmul(rnn_state[1], W) + b       # taking h state of rnn \r\n    with tf.name_scope(\"loss_comp\"):\r\n        total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_logits, labels=tf.one_hot(Y_loaded, num_classes)))\r\n    with tf.name_scope(\"train_step\"):\r\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\r\n\r\n    with tf.name_scope(\"pred_accu\"):\r\n        predictions = tf.nn.softmax(final_logits)\r\n        pred2 = tf.reshape(tf.argmax(predictions, 1), [-1, 1])\r\n        correct_pred = tf.equal(pred2, tf.cast(Y_loaded, tf.int64))\r\n        accuracy_ = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n    \r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n  \r\n        tic = time.clock()    \r\n        for step in range(num_epochs):\r\n            sess.run(trainig_iterator_init)\r\n            batch_cnt = train_data_len//batch_size\r\n            epch_loss = 0.0\r\n            epch_acc = 0.0\r\n            for bt in range(batch_cnt):\r\n                _, loss_, acc = sess.run([train_step, total_loss, accuracy_])\r\n                epch_loss += loss_\r\n                epch_acc += acc\r\n            print ('loss after epoch, ', step,': ', epch_loss/batch_cnt, ' ## accuracy : ', epch_acc/batch_cnt)\r\n            \r\n        print (\"optimization finished, time required: \", time.clock()-tic)\r\n\t\t\r\n\t\t\r\n\t\t#############test accuracy##############\r\n        batch_cnt = test_data_len//batch_size\r\n        sess.run(test_iterator_init)\r\n        print ('testing accuracy on test data : batch number', batch_cnt)\r\n        epch_acc = 0.0\r\n        for bt in range(batch_cnt):\r\n            acc = sess.run(accuracy_)\r\n            epch_acc += acc\r\n        print ('testing accuracy : ', epch_acc/batch_cnt)  \r\n```\r\nHere are some screen shot of different hangs,\r\n**Hanged on an epoch**\r\n![hanged_epc](https://user-images.githubusercontent.com/15855504/32688711-1be4b622-c6a4-11e7-93eb-92f29ad8ee37.JPG)\r\n**GPU usage that time**\r\n![hanged](https://user-images.githubusercontent.com/15855504/32688718-3763037c-c6a4-11e7-9ed0-d89efd1663c6.JPG)\r\n**GPU usage while running (not hanged)**\r\n![running_gpuusage](https://user-images.githubusercontent.com/15855504/32688721-4b8d42cc-c6a4-11e7-9ebe-61d48ed2bcac.JPG)\r\n**Hanged on another eopch**\r\n![hanged2](https://user-images.githubusercontent.com/15855504/32688742-b35474de-c6a4-11e7-8492-493d9ce128cb.JPG)\r\n\r\n\r\nThis type of random hanging behavior keeps repeating on each run. \r\nEach time it hangs on a random epoch. That's why I cannot figure out what is going wrong.\r\nBy looking at code or other set up can anybody please give me any idea about what is going wrong or how can I debug this out? Thanks \r\n\r\n\r\n\r\n"}