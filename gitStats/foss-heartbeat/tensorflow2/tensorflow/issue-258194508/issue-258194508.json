{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13076", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13076/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13076/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13076/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13076", "id": 258194508, "node_id": "MDU6SXNzdWUyNTgxOTQ1MDg=", "number": 13076, "title": "unit test tools print all failure logs if failed", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-09-16T00:29:27Z", "updated_at": "2017-09-21T22:11:16Z", "closed_at": "2017-09-21T22:11:16Z", "author_association": "MEMBER", "body_html": "<p>When a PR is tested failed, Jenkins just reports failure and tells where to check log. However, at least for me, I don't know whether I have the permission, and where to find the logs. So I have to reproduce the failed test on my own machine, but maintaining all kinds of environment is an unnecessary burden for each developer (cpu, gpu, py2, py3, window...)</p>\n<div class=\"highlight highlight-source-shell\"><pre>//tensorflow/tools/test:check_futures_test                               PASSED <span class=\"pl-k\">in</span> 1.3s\n//tensorflow/user_ops:ackermann_test                                     PASSED <span class=\"pl-k\">in</span> 2.3s\n//tensorflow/user_ops:duplicate_op_test                                  PASSED <span class=\"pl-k\">in</span> 2.2s\n//tensorflow/user_ops:invalid_op_test                                    PASSED <span class=\"pl-k\">in</span> 2.1s\n//tensorflow/python/kernel_tests:matrix_solve_ls_op_test                TIMEOUT <span class=\"pl-k\">in</span> 65.0s\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/matrix_solve_ls_op_test/test.log\n\nExecuted 1205 out of 1205 tests: 1204 tests pass and 1 fails locally.</pre></div>\n<p>It will be more convenient to attach total failure content at the end for developers to debug, like reports of <code>py.test</code>:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ pytest\n======= <span class=\"pl-c1\">test</span> session starts ========\nplatform linux -- Python 3.x.y, pytest-3.x.y, py-1.x.y, pluggy-0.x.y\nrootdir: <span class=\"pl-smi\">$REGENDOC_TMPDIR</span>, inifile:\ncollected 1 item\n\ntest_sample.py F\n\n======= FAILURES ========\n_______ test_answer ________\n\n    def <span class=\"pl-en\">test_answer</span>():\n<span class=\"pl-k\">&gt;</span>       assert inc(3) == 5\nE       assert 4 == 5\nE        +  where 4 = inc(3)\n\ntest_sample.py:5: AssertionError\n======= 1 failed <span class=\"pl-k\">in</span> 0.12 seconds ========</pre></div>", "body_text": "When a PR is tested failed, Jenkins just reports failure and tells where to check log. However, at least for me, I don't know whether I have the permission, and where to find the logs. So I have to reproduce the failed test on my own machine, but maintaining all kinds of environment is an unnecessary burden for each developer (cpu, gpu, py2, py3, window...)\n//tensorflow/tools/test:check_futures_test                               PASSED in 1.3s\n//tensorflow/user_ops:ackermann_test                                     PASSED in 2.3s\n//tensorflow/user_ops:duplicate_op_test                                  PASSED in 2.2s\n//tensorflow/user_ops:invalid_op_test                                    PASSED in 2.1s\n//tensorflow/python/kernel_tests:matrix_solve_ls_op_test                TIMEOUT in 65.0s\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/matrix_solve_ls_op_test/test.log\n\nExecuted 1205 out of 1205 tests: 1204 tests pass and 1 fails locally.\nIt will be more convenient to attach total failure content at the end for developers to debug, like reports of py.test:\n$ pytest\n======= test session starts ========\nplatform linux -- Python 3.x.y, pytest-3.x.y, py-1.x.y, pluggy-0.x.y\nrootdir: $REGENDOC_TMPDIR, inifile:\ncollected 1 item\n\ntest_sample.py F\n\n======= FAILURES ========\n_______ test_answer ________\n\n    def test_answer():\n>       assert inc(3) == 5\nE       assert 4 == 5\nE        +  where 4 = inc(3)\n\ntest_sample.py:5: AssertionError\n======= 1 failed in 0.12 seconds ========", "body": "When a PR is tested failed, Jenkins just reports failure and tells where to check log. However, at least for me, I don't know whether I have the permission, and where to find the logs. So I have to reproduce the failed test on my own machine, but maintaining all kinds of environment is an unnecessary burden for each developer (cpu, gpu, py2, py3, window...)\r\n\r\n```bash\r\n//tensorflow/tools/test:check_futures_test                               PASSED in 1.3s\r\n//tensorflow/user_ops:ackermann_test                                     PASSED in 2.3s\r\n//tensorflow/user_ops:duplicate_op_test                                  PASSED in 2.2s\r\n//tensorflow/user_ops:invalid_op_test                                    PASSED in 2.1s\r\n//tensorflow/python/kernel_tests:matrix_solve_ls_op_test                TIMEOUT in 65.0s\r\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/matrix_solve_ls_op_test/test.log\r\n\r\nExecuted 1205 out of 1205 tests: 1204 tests pass and 1 fails locally.\r\n```\r\n\r\nIt will be more convenient to attach total failure content at the end for developers to debug, like reports of `py.test`:\r\n\r\n```bash\r\n$ pytest\r\n======= test session starts ========\r\nplatform linux -- Python 3.x.y, pytest-3.x.y, py-1.x.y, pluggy-0.x.y\r\nrootdir: $REGENDOC_TMPDIR, inifile:\r\ncollected 1 item\r\n\r\ntest_sample.py F\r\n\r\n======= FAILURES ========\r\n_______ test_answer ________\r\n\r\n    def test_answer():\r\n>       assert inc(3) == 5\r\nE       assert 4 == 5\r\nE        +  where 4 = inc(3)\r\n\r\ntest_sample.py:5: AssertionError\r\n======= 1 failed in 0.12 seconds ========\r\n```\r\n\r\n"}