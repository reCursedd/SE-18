{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/327217445", "html_url": "https://github.com/tensorflow/tensorflow/issues/11232#issuecomment-327217445", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11232", "id": 327217445, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzIxNzQ0NQ==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-05T15:45:17Z", "updated_at": "2017-09-05T15:45:17Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17828795\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/harelfar2\">@harelfar2</a></p>\n<p>6x slower comparing CPU to CPU is not something I saw in my testing even when almost purposely picking bad configs for MKL.  The Maven build should only be SSE 3.1 (or something like that) and in the simple Resnet/Inception/AlexNet conv tests MKL was faster even with the less than optimal data_format and env variables.  I have likely pasted this too many places but here is a big part of the post I am trying to get through the editing process.</p>\n<h3>TensorFlow with Intel\u00ae MKL DNN</h3>\n<p>Intel\u00ae has added optimizations to TensorFlow for Intel\u00ae Xeon\u00ae and Intel\u00ae Xeon<br>\nPhi\u2122 though the use of Intel\u00ae Math Kernel Library for Deep Neural Networks<br>\n(Intel\u00ae MKL-DNN) optimized primitives. The optimizations also provide speedups<br>\nfor the consumer line of processors, e.g. i5 and i7 Intel processors. The Intel<br>\npublished paper<br>\n<a href=\"https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture\" rel=\"nofollow\">TensorFlow* Optimizations on Modern Intel\u00ae Architecture</a><br>\ncontains additional details on the implementation.</p>\n<blockquote>\n<p>Note: MKL was added as of TensorFlow 1.2 and currently only works on Linux. It<br>\nalso does not work when also using <code>--config=cuda</code>.</p>\n</blockquote>\n<p>In addition to providing significant performance improvements for training CNN<br>\nbased models, compiling with the MKL creates a binary that is optimized for AVX<br>\nand AVX2. The result is a single binary that is optimized and compatible with<br>\nmost modern (post-2011) processors.</p>\n<p>TensorFlow can be compiled with the MKL optimizations using the following<br>\ncommands that depending on the version of the TensorFlow source used.</p>\n<p>For TensorFlow source versions after 1.3.0:</p>\n<div class=\"highlight highlight-source-shell\"><pre>./configure\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Pick the desired options</span>\nbazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package\n</pre></div>\n<p>For TensorFlow versions 1.2.0 through 1.3.0:</p>\n<div class=\"highlight highlight-source-shell\"><pre>./configure\nDo you wish to build TensorFlow with MKL support<span class=\"pl-k\">?</span> [y/N] Y\nDo you wish to download MKL LIB from the web<span class=\"pl-k\">?</span> [Y/n] Y\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Select the defaults for the rest of the options.</span>\n\nbazel build --config=mkl --copt=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-DEIGEN_USE_VML<span class=\"pl-pds\">\"</span></span> -c opt //tensorflow/tools/pip_package:build_pip_package\n</pre></div>\n<h4>Tuning MKL for the best performance</h4>\n<p>This section details the different configurations and environment variables that<br>\ncan be used to tune the MKL to get optimal performance. Before tweaking various<br>\nenvironment variables make sure the model is using the <code>NCHW</code> (<code>channels_first</code>)<br>\n<a href=\"#data-formats\">data format</a>. The MKL is optimized for <code>NCHW</code> and Intel is<br>\nworking to get near performance parity when using <code>NHWC</code>.</p>\n<p>MKL uses the following environment variables to tune performance:</p>\n<ul>\n<li>KMP_BLOCKTIME - Sets the time, in milliseconds, that a thread should wait,<br>\nafter completing the execution of a parallel region, before sleeping.</li>\n<li>KMP_AFFINITY - Enables the run-time library to bind threads to physical<br>\nprocessing units.</li>\n<li>KMP_SETTINGS - Enables (true) or disables (false) the printing of OpenMP*<br>\nrun-time library environment variables during program execution.</li>\n<li>OMP_NUM_THREADS - Specifies the number of threads to use.</li>\n</ul>\n<p>More details on the KMP variables are on<br>\n<a href=\"https://software.intel.com/en-us/node/522775\" rel=\"nofollow\">Intel's</a> site and the OMP<br>\nvariables on<br>\n<a href=\"https://gcc.gnu.org/onlinedocs/libgomp/Environment-Variables.html\" rel=\"nofollow\">gnu.org</a></p>\n<p>While there can be substantial gains from adjusting the environment variables,<br>\nwhich is discussed below, the simplified advice is to set the<br>\n<code>inter_op_parallelism_threads</code> equal to the number of physical CPUs and to set<br>\nthe following environment variables:</p>\n<ul>\n<li>KMP_BLOCKTIME=0</li>\n<li>KMP_AFFINITY=granularity=fine,verbose,compact,1,0</li>\n</ul>\n<p>Example setting MKL variables with command-line arguments:</p>\n<div class=\"highlight highlight-source-shell\"><pre>KMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \\\nKMP_SETTINGS=1 python your_python_script.py</pre></div>\n<p>Example setting MKL variables with python <code>os.environ</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>KMP_BLOCKTIME<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">FLAGS</span>.kmp_blocktime)\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>KMP_SETTINGS<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">FLAGS</span>.kmp_settings)\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>KMP_AFFINITY<span class=\"pl-pds\">\"</span></span>]<span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.kmp_affinity\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.num_intra_threads <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n  os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>OMP_NUM_THREADS<span class=\"pl-pds\">\"</span></span>]<span class=\"pl-k\">=</span> <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">FLAGS</span>.num_intra_threads)\n</pre></div>\n<p>There are models and hardware platforms that benefit from different settings.<br>\nEach variable that impacts performance is discussed below.</p>\n<ul>\n<li>\n<p><strong>KMP_BLOCKTIME</strong>: The MKL default is 200ms, which was not optimal in our<br>\ntesting. 0 (0ms) was a good default for CNN based models that were tested.<br>\nThe best performance for AlexNex was achieved at 30ms and both GoogleNet and<br>\nVGG11 performed best set at 1ms.</p>\n</li>\n<li>\n<p><strong>KMP_AFFINITY</strong>: The recommended setting is<br>\n<code>granularity=fine,verbose,compact,1,0</code>.</p>\n</li>\n<li>\n<p><strong>OMP_NUM_THREADS</strong>: This defaults to the number of physical cores.<br>\nAdjusting this parameter beyond matching the number of cores can have an<br>\nimpact when using Intel\u00ae Xeon Phi\u2122 (Knights Landing) for some models. See<br>\n<a href=\"https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture\" rel=\"nofollow\">TensorFlow* Optimizations on Modern Intel\u00ae Architecture</a><br>\nfor optimal settings.</p>\n</li>\n<li>\n<p><strong>intra_op_parallelism_threads</strong>: Setting this equal to the number of<br>\nphysical cores is recommended. Setting the value to 0, which is the default<br>\nand will result in the value being set to the number of logical cores, is an<br>\noption to try for some architectures.  This value and <code>OMP_NUM_THREADS</code><br>\nshould be equal.</p>\n</li>\n<li>\n<p><strong>inter_op_parallelism_threads</strong>: Setting this equal to the number of<br>\nsockets is recommended. Setting the value to 0, which is the default,<br>\nresults in the value being set to the number of logical cores.</p>\n</li>\n</ul>\n<h3>Comparing compiler optimizations</h3>\n<p>Collected below are performance results running training and inference on<br>\ndifferent types of CPUs on different platforms with various compiler<br>\noptimizations.  The models used were ResNet-50<br>\n(<a href=\"https://arxiv.org/abs/1512.03385\" rel=\"nofollow\">arXiv:1512.03385</a>) and<br>\nInceptionV3 (<a href=\"https://arxiv.org/abs/1512.00567\" rel=\"nofollow\">arXiv:1512.00567</a>).</p>\n<p>For each test, when the MKL optimization was used the environment variable<br>\nKMP_BLOCKTIME was set to 0 (0ms) and KMP_AFFINITY to<br>\n<code>granularity=fine,verbose,compact,1,0</code>.</p>\n<h4>Inference InceptionV3</h4>\n<p><strong>Environment</strong></p>\n<ul>\n<li>Instance Type: AWS EC2 m4.xlarge</li>\n<li>CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)</li>\n<li>Dataset: ImageNet</li>\n<li>TensorFlow Version: 1.2.0 RC2</li>\n<li>Test Script: <a href=\"https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\">tf_cnn_benchmarks.py</a></li>\n</ul>\n<p><strong>Batch Size: 1</strong></p>\n<p>Command executed for the MKL test:</p>\n<div class=\"highlight highlight-source-shell\"><pre>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<span class=\"pl-k\">&lt;</span>path to ImageNet TFRecords<span class=\"pl-k\">&gt;</span></pre></div>\n<p>| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |<br>\n:              :             : (step time)  :               :               :<br>\n| ------------ | ----------- | ------------ | ------------- | ------------- |<br>\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |<br>\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |<br>\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |<br>\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |<br>\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |</p>\n<p><strong>Batch Size: 32</strong></p>\n<p>Command executed for the MKL test:</p>\n<div class=\"highlight highlight-source-shell\"><pre>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<span class=\"pl-k\">&lt;</span>path to ImageNet TFRecords<span class=\"pl-k\">&gt;</span></pre></div>\n<p>| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |<br>\n:              :             : (step time)   :               :               :<br>\n| ------------ | ----------- | ------------- | ------------- | ------------- |<br>\n| MKL          | NCHW        | 10.24         | 4             | 1             |<br>\n:              :             : (3125ms)      :               :               :<br>\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |<br>\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |<br>\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |<br>\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |</p>\n<h4>Inference ResNet-50</h4>\n<p><strong>Environment</strong></p>\n<ul>\n<li>Instance Type: AWS EC2 m4.xlarge</li>\n<li>CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)</li>\n<li>Dataset: ImageNet</li>\n<li>TensorFlow Version: 1.2.0 RC2</li>\n<li>Test Script: <a href=\"https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\">tf_cnn_benchmarks.py</a></li>\n</ul>\n<p><strong>Batch Size: 1</strong></p>\n<p>Command executed for the MKL test:</p>\n<div class=\"highlight highlight-source-shell\"><pre>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<span class=\"pl-k\">&lt;</span>path to ImageNet TFRecords<span class=\"pl-k\">&gt;</span></pre></div>\n<p>| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |<br>\n:              :             : (step time)  :               :               :<br>\n| ------------ | ----------- | ------------ | ------------- | ------------- |<br>\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |<br>\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |<br>\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |<br>\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |<br>\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |</p>\n<p><strong>Batch Size: 32</strong></p>\n<p>Command executed for the MKL test:</p>\n<div class=\"highlight highlight-source-shell\"><pre>python tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<span class=\"pl-k\">&lt;</span>path to ImageNet TFRecords<span class=\"pl-k\">&gt;</span></pre></div>\n<p>| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |<br>\n:              :             : (step time)   :               :               :<br>\n| ------------ | ----------- | ------------- | ------------- | ------------- |<br>\n| MKL          | NCHW        | 10.24         | 4             | 1             |<br>\n:              :             : (3125ms)      :               :               :<br>\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |<br>\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |<br>\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |<br>\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |</p>\n<h4>Training InceptionV3</h4>\n<p><strong>Environment</strong></p>\n<ul>\n<li>Instance Type: Dedicated AWS EC2 r4.16xlarge (Broadwell)</li>\n<li>CPU: Intel Xeon E5-2686 v4 (Broadwell) Processors</li>\n<li>Dataset: ImageNet</li>\n<li>TensorFlow Version: 1.2.0 RC2</li>\n<li>Test Script: <a href=\"https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\">tf_cnn_benchmarks.py</a></li>\n</ul>\n<p>Command executed for MKL test:</p>\n<div class=\"highlight highlight-source-shell\"><pre>python tf_cnn_benchmarks.py --device=cpu --mkl=True --kmp_blocktime=0 \\\n--nodistortions --model=resnet50 --data_format=NCHW --batch_size=32 \\\n--num_inter_threads=2 --num_intra_threads=36 \\\n--data_dir=<span class=\"pl-k\">&lt;</span>path to ImageNet TFRecords<span class=\"pl-k\">&gt;</span></pre></div>\n<table>\n<thead>\n<tr>\n<th>Optimization</th>\n<th>Data Format</th>\n<th>Images/Sec</th>\n<th>Intra threads</th>\n<th>Inter Threads</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MKL</td>\n<td>NCHW</td>\n<td>20.8</td>\n<td>36</td>\n<td>2</td>\n</tr>\n<tr>\n<td>AVX2</td>\n<td>NHWC</td>\n<td>6.2</td>\n<td>36</td>\n<td>0</td>\n</tr>\n<tr>\n<td>AVX</td>\n<td>NHWC</td>\n<td>5.7</td>\n<td>36</td>\n<td>0</td>\n</tr>\n<tr>\n<td>SSE3</td>\n<td>NHWC</td>\n<td>4.3</td>\n<td>36</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>ResNet and <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"nofollow\">AlexNet</a><br>\nwere also run on this configuration but in an ad hoc manner. There were not<br>\nenough runs executed to publish a coherent table of results. The incomplete<br>\nresults strongly indicated the final result would be similar to the table above<br>\nwith MKL providing significant 3x+ gains over AVX2.</p>", "body_text": "@harelfar2\n6x slower comparing CPU to CPU is not something I saw in my testing even when almost purposely picking bad configs for MKL.  The Maven build should only be SSE 3.1 (or something like that) and in the simple Resnet/Inception/AlexNet conv tests MKL was faster even with the less than optimal data_format and env variables.  I have likely pasted this too many places but here is a big part of the post I am trying to get through the editing process.\nTensorFlow with Intel\u00ae MKL DNN\nIntel\u00ae has added optimizations to TensorFlow for Intel\u00ae Xeon\u00ae and Intel\u00ae Xeon\nPhi\u2122 though the use of Intel\u00ae Math Kernel Library for Deep Neural Networks\n(Intel\u00ae MKL-DNN) optimized primitives. The optimizations also provide speedups\nfor the consumer line of processors, e.g. i5 and i7 Intel processors. The Intel\npublished paper\nTensorFlow* Optimizations on Modern Intel\u00ae Architecture\ncontains additional details on the implementation.\n\nNote: MKL was added as of TensorFlow 1.2 and currently only works on Linux. It\nalso does not work when also using --config=cuda.\n\nIn addition to providing significant performance improvements for training CNN\nbased models, compiling with the MKL creates a binary that is optimized for AVX\nand AVX2. The result is a single binary that is optimized and compatible with\nmost modern (post-2011) processors.\nTensorFlow can be compiled with the MKL optimizations using the following\ncommands that depending on the version of the TensorFlow source used.\nFor TensorFlow source versions after 1.3.0:\n./configure\n# Pick the desired options\nbazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package\n\nFor TensorFlow versions 1.2.0 through 1.3.0:\n./configure\nDo you wish to build TensorFlow with MKL support? [y/N] Y\nDo you wish to download MKL LIB from the web? [Y/n] Y\n# Select the defaults for the rest of the options.\n\nbazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package\n\nTuning MKL for the best performance\nThis section details the different configurations and environment variables that\ncan be used to tune the MKL to get optimal performance. Before tweaking various\nenvironment variables make sure the model is using the NCHW (channels_first)\ndata format. The MKL is optimized for NCHW and Intel is\nworking to get near performance parity when using NHWC.\nMKL uses the following environment variables to tune performance:\n\nKMP_BLOCKTIME - Sets the time, in milliseconds, that a thread should wait,\nafter completing the execution of a parallel region, before sleeping.\nKMP_AFFINITY - Enables the run-time library to bind threads to physical\nprocessing units.\nKMP_SETTINGS - Enables (true) or disables (false) the printing of OpenMP*\nrun-time library environment variables during program execution.\nOMP_NUM_THREADS - Specifies the number of threads to use.\n\nMore details on the KMP variables are on\nIntel's site and the OMP\nvariables on\ngnu.org\nWhile there can be substantial gains from adjusting the environment variables,\nwhich is discussed below, the simplified advice is to set the\ninter_op_parallelism_threads equal to the number of physical CPUs and to set\nthe following environment variables:\n\nKMP_BLOCKTIME=0\nKMP_AFFINITY=granularity=fine,verbose,compact,1,0\n\nExample setting MKL variables with command-line arguments:\nKMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \\\nKMP_SETTINGS=1 python your_python_script.py\nExample setting MKL variables with python os.environ:\nos.environ[\"KMP_BLOCKTIME\"] = str(FLAGS.kmp_blocktime)\nos.environ[\"KMP_SETTINGS\"] = str(FLAGS.kmp_settings)\nos.environ[\"KMP_AFFINITY\"]= FLAGS.kmp_affinity\nif FLAGS.num_intra_threads > 0:\n  os.environ[\"OMP_NUM_THREADS\"]= str(FLAGS.num_intra_threads)\n\nThere are models and hardware platforms that benefit from different settings.\nEach variable that impacts performance is discussed below.\n\n\nKMP_BLOCKTIME: The MKL default is 200ms, which was not optimal in our\ntesting. 0 (0ms) was a good default for CNN based models that were tested.\nThe best performance for AlexNex was achieved at 30ms and both GoogleNet and\nVGG11 performed best set at 1ms.\n\n\nKMP_AFFINITY: The recommended setting is\ngranularity=fine,verbose,compact,1,0.\n\n\nOMP_NUM_THREADS: This defaults to the number of physical cores.\nAdjusting this parameter beyond matching the number of cores can have an\nimpact when using Intel\u00ae Xeon Phi\u2122 (Knights Landing) for some models. See\nTensorFlow* Optimizations on Modern Intel\u00ae Architecture\nfor optimal settings.\n\n\nintra_op_parallelism_threads: Setting this equal to the number of\nphysical cores is recommended. Setting the value to 0, which is the default\nand will result in the value being set to the number of logical cores, is an\noption to try for some architectures.  This value and OMP_NUM_THREADS\nshould be equal.\n\n\ninter_op_parallelism_threads: Setting this equal to the number of\nsockets is recommended. Setting the value to 0, which is the default,\nresults in the value being set to the number of logical cores.\n\n\nComparing compiler optimizations\nCollected below are performance results running training and inference on\ndifferent types of CPUs on different platforms with various compiler\noptimizations.  The models used were ResNet-50\n(arXiv:1512.03385) and\nInceptionV3 (arXiv:1512.00567).\nFor each test, when the MKL optimization was used the environment variable\nKMP_BLOCKTIME was set to 0 (0ms) and KMP_AFFINITY to\ngranularity=fine,verbose,compact,1,0.\nInference InceptionV3\nEnvironment\n\nInstance Type: AWS EC2 m4.xlarge\nCPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\nDataset: ImageNet\nTensorFlow Version: 1.2.0 RC2\nTest Script: tf_cnn_benchmarks.py\n\nBatch Size: 1\nCommand executed for the MKL test:\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<path to ImageNet TFRecords>\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\n:              :             : (step time)  :               :               :\n| ------------ | ----------- | ------------ | ------------- | ------------- |\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |\nBatch Size: 32\nCommand executed for the MKL test:\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<path to ImageNet TFRecords>\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\n:              :             : (step time)   :               :               :\n| ------------ | ----------- | ------------- | ------------- | ------------- |\n| MKL          | NCHW        | 10.24         | 4             | 1             |\n:              :             : (3125ms)      :               :               :\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |\nInference ResNet-50\nEnvironment\n\nInstance Type: AWS EC2 m4.xlarge\nCPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\nDataset: ImageNet\nTensorFlow Version: 1.2.0 RC2\nTest Script: tf_cnn_benchmarks.py\n\nBatch Size: 1\nCommand executed for the MKL test:\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<path to ImageNet TFRecords>\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\n:              :             : (step time)  :               :               :\n| ------------ | ----------- | ------------ | ------------- | ------------- |\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |\nBatch Size: 32\nCommand executed for the MKL test:\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\n--data_dir=<path to ImageNet TFRecords>\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\n:              :             : (step time)   :               :               :\n| ------------ | ----------- | ------------- | ------------- | ------------- |\n| MKL          | NCHW        | 10.24         | 4             | 1             |\n:              :             : (3125ms)      :               :               :\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |\nTraining InceptionV3\nEnvironment\n\nInstance Type: Dedicated AWS EC2 r4.16xlarge (Broadwell)\nCPU: Intel Xeon E5-2686 v4 (Broadwell) Processors\nDataset: ImageNet\nTensorFlow Version: 1.2.0 RC2\nTest Script: tf_cnn_benchmarks.py\n\nCommand executed for MKL test:\npython tf_cnn_benchmarks.py --device=cpu --mkl=True --kmp_blocktime=0 \\\n--nodistortions --model=resnet50 --data_format=NCHW --batch_size=32 \\\n--num_inter_threads=2 --num_intra_threads=36 \\\n--data_dir=<path to ImageNet TFRecords>\n\n\n\nOptimization\nData Format\nImages/Sec\nIntra threads\nInter Threads\n\n\n\n\nMKL\nNCHW\n20.8\n36\n2\n\n\nAVX2\nNHWC\n6.2\n36\n0\n\n\nAVX\nNHWC\n5.7\n36\n0\n\n\nSSE3\nNHWC\n4.3\n36\n0\n\n\n\nResNet and AlexNet\nwere also run on this configuration but in an ad hoc manner. There were not\nenough runs executed to publish a coherent table of results. The incomplete\nresults strongly indicated the final result would be similar to the table above\nwith MKL providing significant 3x+ gains over AVX2.", "body": "@harelfar2 \r\n\r\n6x slower comparing CPU to CPU is not something I saw in my testing even when almost purposely picking bad configs for MKL.  The Maven build should only be SSE 3.1 (or something like that) and in the simple Resnet/Inception/AlexNet conv tests MKL was faster even with the less than optimal data_format and env variables.  I have likely pasted this too many places but here is a big part of the post I am trying to get through the editing process.\r\n\r\n### TensorFlow with Intel\u00ae MKL DNN\r\n\r\nIntel\u00ae has added optimizations to TensorFlow for Intel\u00ae Xeon\u00ae and Intel\u00ae Xeon\r\nPhi\u2122 though the use of Intel\u00ae Math Kernel Library for Deep Neural Networks\r\n(Intel\u00ae MKL-DNN) optimized primitives. The optimizations also provide speedups\r\nfor the consumer line of processors, e.g. i5 and i7 Intel processors. The Intel\r\npublished paper\r\n[TensorFlow* Optimizations on Modern Intel\u00ae Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture)\r\ncontains additional details on the implementation.\r\n\r\n> Note: MKL was added as of TensorFlow 1.2 and currently only works on Linux. It\r\n> also does not work when also using `--config=cuda`.\r\n\r\nIn addition to providing significant performance improvements for training CNN\r\nbased models, compiling with the MKL creates a binary that is optimized for AVX\r\nand AVX2. The result is a single binary that is optimized and compatible with\r\nmost modern (post-2011) processors.\r\n\r\nTensorFlow can be compiled with the MKL optimizations using the following\r\ncommands that depending on the version of the TensorFlow source used.\r\n\r\nFor TensorFlow source versions after 1.3.0:\r\n\r\n```bash\r\n./configure\r\n# Pick the desired options\r\nbazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\n\r\nFor TensorFlow versions 1.2.0 through 1.3.0:\r\n\r\n```bash\r\n./configure\r\nDo you wish to build TensorFlow with MKL support? [y/N] Y\r\nDo you wish to download MKL LIB from the web? [Y/n] Y\r\n# Select the defaults for the rest of the options.\r\n\r\nbazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\n\r\n#### Tuning MKL for the best performance\r\n\r\nThis section details the different configurations and environment variables that\r\ncan be used to tune the MKL to get optimal performance. Before tweaking various\r\nenvironment variables make sure the model is using the `NCHW` (`channels_first`)\r\n[data format](#data-formats). The MKL is optimized for `NCHW` and Intel is\r\nworking to get near performance parity when using `NHWC`.\r\n\r\nMKL uses the following environment variables to tune performance:\r\n\r\n*   KMP_BLOCKTIME - Sets the time, in milliseconds, that a thread should wait,\r\n    after completing the execution of a parallel region, before sleeping.\r\n*   KMP_AFFINITY - Enables the run-time library to bind threads to physical\r\n    processing units.\r\n*   KMP_SETTINGS - Enables (true) or disables (false) the printing of OpenMP*\r\n    run-time library environment variables during program execution.\r\n*   OMP_NUM_THREADS - Specifies the number of threads to use.\r\n\r\nMore details on the KMP variables are on\r\n[Intel's](https://software.intel.com/en-us/node/522775) site and the OMP\r\nvariables on\r\n[gnu.org](https://gcc.gnu.org/onlinedocs/libgomp/Environment-Variables.html)\r\n\r\nWhile there can be substantial gains from adjusting the environment variables,\r\nwhich is discussed below, the simplified advice is to set the\r\n`inter_op_parallelism_threads` equal to the number of physical CPUs and to set\r\nthe following environment variables:\r\n\r\n*   KMP_BLOCKTIME=0\r\n*   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n\r\nExample setting MKL variables with command-line arguments:\r\n\r\n```bash\r\nKMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \\\r\nKMP_SETTINGS=1 python your_python_script.py\r\n```\r\n\r\nExample setting MKL variables with python `os.environ`:\r\n\r\n```python\r\nos.environ[\"KMP_BLOCKTIME\"] = str(FLAGS.kmp_blocktime)\r\nos.environ[\"KMP_SETTINGS\"] = str(FLAGS.kmp_settings)\r\nos.environ[\"KMP_AFFINITY\"]= FLAGS.kmp_affinity\r\nif FLAGS.num_intra_threads > 0:\r\n  os.environ[\"OMP_NUM_THREADS\"]= str(FLAGS.num_intra_threads)\r\n\r\n```\r\n\r\nThere are models and hardware platforms that benefit from different settings.\r\nEach variable that impacts performance is discussed below.\r\n\r\n*   **KMP_BLOCKTIME**: The MKL default is 200ms, which was not optimal in our\r\n    testing. 0 (0ms) was a good default for CNN based models that were tested.\r\n    The best performance for AlexNex was achieved at 30ms and both GoogleNet and\r\n    VGG11 performed best set at 1ms.\r\n\r\n*   **KMP_AFFINITY**: The recommended setting is\r\n    `granularity=fine,verbose,compact,1,0`.\r\n\r\n*   **OMP_NUM_THREADS**: This defaults to the number of physical cores.\r\n    Adjusting this parameter beyond matching the number of cores can have an\r\n    impact when using Intel\u00ae Xeon Phi\u2122 (Knights Landing) for some models. See\r\n    [TensorFlow* Optimizations on Modern Intel\u00ae Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture)\r\n    for optimal settings.\r\n\r\n*   **intra_op_parallelism_threads**: Setting this equal to the number of\r\n    physical cores is recommended. Setting the value to 0, which is the default\r\n    and will result in the value being set to the number of logical cores, is an\r\n    option to try for some architectures.  This value and `OMP_NUM_THREADS`\r\n    should be equal.\r\n\r\n*   **inter_op_parallelism_threads**: Setting this equal to the number of\r\n    sockets is recommended. Setting the value to 0, which is the default,\r\n    results in the value being set to the number of logical cores.\r\n\r\n### Comparing compiler optimizations\r\n\r\nCollected below are performance results running training and inference on\r\ndifferent types of CPUs on different platforms with various compiler\r\noptimizations.  The models used were ResNet-50\r\n([arXiv:1512.03385](https://arxiv.org/abs/1512.03385)) and\r\nInceptionV3 ([arXiv:1512.00567](https://arxiv.org/abs/1512.00567)).\r\n\r\nFor each test, when the MKL optimization was used the environment variable\r\nKMP_BLOCKTIME was set to 0 (0ms) and KMP_AFFINITY to\r\n`granularity=fine,verbose,compact,1,0`.\r\n\r\n#### Inference InceptionV3\r\n\r\n**Environment**\r\n\r\n*   Instance Type: AWS EC2 m4.xlarge\r\n*   CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\n**Batch Size: 1**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\r\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\r\n:              :             : (step time)  :               :               :\r\n| ------------ | ----------- | ------------ | ------------- | ------------- |\r\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |\r\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |\r\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |\r\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |\r\n\r\n**Batch Size: 32**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\r\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\r\n:              :             : (step time)   :               :               :\r\n| ------------ | ----------- | ------------- | ------------- | ------------- |\r\n| MKL          | NCHW        | 10.24         | 4             | 1             |\r\n:              :             : (3125ms)      :               :               :\r\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |\r\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |\r\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |\r\n\r\n#### Inference ResNet-50\r\n\r\n**Environment**\r\n\r\n*   Instance Type: AWS EC2 m4.xlarge\r\n*   CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\n**Batch Size: 1**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\r\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\r\n:              :             : (step time)  :               :               :\r\n| ------------ | ----------- | ------------ | ------------- | ------------- |\r\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |\r\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |\r\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |\r\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |\r\n\r\n**Batch Size: 32**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\r\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\r\n:              :             : (step time)   :               :               :\r\n| ------------ | ----------- | ------------- | ------------- | ------------- |\r\n| MKL          | NCHW        | 10.24         | 4             | 1             |\r\n:              :             : (3125ms)      :               :               :\r\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |\r\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |\r\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |\r\n\r\n#### Training InceptionV3\r\n\r\n**Environment**\r\n\r\n*   Instance Type: Dedicated AWS EC2 r4.16xlarge (Broadwell)\r\n*   CPU: Intel Xeon E5-2686 v4 (Broadwell) Processors\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\nCommand executed for MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --device=cpu --mkl=True --kmp_blocktime=0 \\\r\n--nodistortions --model=resnet50 --data_format=NCHW --batch_size=32 \\\r\n--num_inter_threads=2 --num_intra_threads=36 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\nOptimization | Data Format | Images/Sec | Intra threads | Inter Threads\r\n------------ | ----------- | ---------- | ------------- | -------------\r\nMKL          | NCHW        | 20.8       | 36            | 2\r\nAVX2         | NHWC        | 6.2        | 36            | 0\r\nAVX          | NHWC        | 5.7        | 36            | 0\r\nSSE3         | NHWC        | 4.3        | 36            | 0\r\n\r\nResNet and [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\r\nwere also run on this configuration but in an ad hoc manner. There were not\r\nenough runs executed to publish a coherent table of results. The incomplete\r\nresults strongly indicated the final result would be similar to the table above\r\nwith MKL providing significant 3x+ gains over AVX2.\r\n\r\n"}