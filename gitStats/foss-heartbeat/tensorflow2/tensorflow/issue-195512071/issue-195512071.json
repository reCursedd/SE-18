{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6310", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6310/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6310/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6310/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6310", "id": 195512071, "node_id": "MDU6SXNzdWUxOTU1MTIwNzE=", "number": 6310, "title": "Multiple GPUs: Out of Memory", "user": {"login": "KlaymenGC", "id": 6898967, "node_id": "MDQ6VXNlcjY4OTg5Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6898967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KlaymenGC", "html_url": "https://github.com/KlaymenGC", "followers_url": "https://api.github.com/users/KlaymenGC/followers", "following_url": "https://api.github.com/users/KlaymenGC/following{/other_user}", "gists_url": "https://api.github.com/users/KlaymenGC/gists{/gist_id}", "starred_url": "https://api.github.com/users/KlaymenGC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KlaymenGC/subscriptions", "organizations_url": "https://api.github.com/users/KlaymenGC/orgs", "repos_url": "https://api.github.com/users/KlaymenGC/repos", "events_url": "https://api.github.com/users/KlaymenGC/events{/privacy}", "received_events_url": "https://api.github.com/users/KlaymenGC/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-12-14T11:55:36Z", "updated_at": "2017-11-22T12:54:58Z", "closed_at": "2016-12-15T19:33:48Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I used to train my model on a single GPU with a batch size of <code>20</code> (images) without a problem. Now I want to take advantage of multiple GPU training. Currently I'm using 4 GPUs and want to do <strong>data-parallel replicated training</strong> with a batch size of <code>20*4</code>.</p>\n<p>If I understood it correctly, each batch will be splitted into 4 equal parts (in this case for every model on each GPU, the batch size will be <code>20</code>) and be feeded to each GPU.  But I'm getting out of memory error:</p>\n<pre><code>...\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ********************************************************************************************\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 1.32GiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[80,921600,6]\n...\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[80,921600,6]\n</code></pre>\n<p>any suggestions will be greatly appreciated!</p>", "body_text": "Hi,\nI used to train my model on a single GPU with a batch size of 20 (images) without a problem. Now I want to take advantage of multiple GPU training. Currently I'm using 4 GPUs and want to do data-parallel replicated training with a batch size of 20*4.\nIf I understood it correctly, each batch will be splitted into 4 equal parts (in this case for every model on each GPU, the batch size will be 20) and be feeded to each GPU.  But I'm getting out of memory error:\n...\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ********************************************************************************************\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 1.32GiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[80,921600,6]\n...\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[80,921600,6]\n\nany suggestions will be greatly appreciated!", "body": "Hi,\r\n\r\nI used to train my model on a single GPU with a batch size of ```20``` (images) without a problem. Now I want to take advantage of multiple GPU training. Currently I'm using 4 GPUs and want to do **data-parallel replicated training** with a batch size of ```20*4```.\r\n\r\nIf I understood it correctly, each batch will be splitted into 4 equal parts (in this case for every model on each GPU, the batch size will be ```20```) and be feeded to each GPU.  But I'm getting out of memory error:\r\n\r\n```\r\n...\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ********************************************************************************************\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 1.32GiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[80,921600,6]\r\n...\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[80,921600,6]\r\n```\r\nany suggestions will be greatly appreciated!"}