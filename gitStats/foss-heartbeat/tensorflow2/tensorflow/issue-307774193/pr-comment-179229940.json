{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/179229940", "pull_request_review_id": 109432024, "id": 179229940, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTIyOTk0MA==", "diff_hunk": "@@ -49,8 +51,22 @@ thread::ThreadPool* ComputePool(const SessionOptions& options) {\n int32 NumInterOpThreadsFromSessionOptions(const SessionOptions& options) {\n   const int32 t = options.config.inter_op_parallelism_threads();\n   if (t != 0) return t;\n+#ifdef INTEL_MKL\n+  // MKL library executes ops in parallel using OMP threads\n+  // Set inter_op conservatively to avoid thread oversubscription that could \n+  // lead to severe perf degradations and OMP resource exhaustion\n+  const int mkl_intra_op = omp_get_max_threads();\n+  CHECK_GE(mkl_intra_op, 1);\n+  const int32 mkl_inter_op = std::ceil(", "path": "tensorflow/core/common_runtime/process_util.cc", "position": null, "original_position": 19, "commit_id": "2ef8e85e7d572d6cbbec2eea66df8462914a2908", "original_commit_id": "b3e956b5aa424b2fdcb91d1a673f62788567f97c", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "I completely agree that we want to avoid thread oversubscription, which can be very severe with the current (silly) default settings, but this seems a bit too pessimistic/conservative. Do you assume that omp_get_max_threads() is significantly smaller than the number of cores on the machine? If not (i.e. if mkl_inter_op ends up being 1), you may hurt performance for models that have a mixture of large expensive and many cheaper ops. If the model is completely dominated by a few expensive, heavily threaded ops, it's fine, of course. There is no simple way of solving this in general without a global scheduling strategy, i.e. the inter and intra threadpools being combined or being aware of each other.\r\n\r\nWould you agree that setting inter op equal to\r\n\r\n```max(ceil(num_cores / intra), 2)```\r\n\r\nwould be a reasonable heuristic, or do you have performance data supporting your choice?\r\n", "created_at": "2018-04-04T17:55:54Z", "updated_at": "2018-04-04T22:10:03Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17931#discussion_r179229940", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17931", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/179229940"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17931#discussion_r179229940"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17931"}}, "body_html": "<p>I completely agree that we want to avoid thread oversubscription, which can be very severe with the current (silly) default settings, but this seems a bit too pessimistic/conservative. Do you assume that omp_get_max_threads() is significantly smaller than the number of cores on the machine? If not (i.e. if mkl_inter_op ends up being 1), you may hurt performance for models that have a mixture of large expensive and many cheaper ops. If the model is completely dominated by a few expensive, heavily threaded ops, it's fine, of course. There is no simple way of solving this in general without a global scheduling strategy, i.e. the inter and intra threadpools being combined or being aware of each other.</p>\n<p>Would you agree that setting inter op equal to</p>\n<p><code>max(ceil(num_cores / intra), 2)</code></p>\n<p>would be a reasonable heuristic, or do you have performance data supporting your choice?</p>", "body_text": "I completely agree that we want to avoid thread oversubscription, which can be very severe with the current (silly) default settings, but this seems a bit too pessimistic/conservative. Do you assume that omp_get_max_threads() is significantly smaller than the number of cores on the machine? If not (i.e. if mkl_inter_op ends up being 1), you may hurt performance for models that have a mixture of large expensive and many cheaper ops. If the model is completely dominated by a few expensive, heavily threaded ops, it's fine, of course. There is no simple way of solving this in general without a global scheduling strategy, i.e. the inter and intra threadpools being combined or being aware of each other.\nWould you agree that setting inter op equal to\nmax(ceil(num_cores / intra), 2)\nwould be a reasonable heuristic, or do you have performance data supporting your choice?"}