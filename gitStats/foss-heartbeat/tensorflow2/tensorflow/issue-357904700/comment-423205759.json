{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423205759", "html_url": "https://github.com/tensorflow/tensorflow/issues/22138#issuecomment-423205759", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22138", "id": 423205759, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzIwNTc1OQ==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T14:31:43Z", "updated_at": "2018-09-20T14:31:43Z", "author_association": "MEMBER", "body_html": "<p>The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the Mul and Sub at the start of your network need quantization information for TOCO. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after mul and sub, see how the rewriter does it: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475</a></p>\n<p>That being said this can be complicated, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a <em>floating point</em> version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case.</p>", "body_text": "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the Mul and Sub at the start of your network need quantization information for TOCO. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after mul and sub, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\nThat being said this can be complicated, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case.", "body": "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the Mul and Sub at the start of your network need quantization information for TOCO. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after mul and sub, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\r\n\r\nThat being said this can be complicated, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a *floating point* version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case."}