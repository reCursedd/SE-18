{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20644", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20644/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20644/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20644/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20644", "id": 339403858, "node_id": "MDU6SXNzdWUzMzk0MDM4NTg=", "number": 20644, "title": "feature request: Robbins-Monro type learning rate decay", "user": {"login": "jeffpollock9", "id": 5081168, "node_id": "MDQ6VXNlcjUwODExNjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/5081168?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffpollock9", "html_url": "https://github.com/jeffpollock9", "followers_url": "https://api.github.com/users/jeffpollock9/followers", "following_url": "https://api.github.com/users/jeffpollock9/following{/other_user}", "gists_url": "https://api.github.com/users/jeffpollock9/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffpollock9/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffpollock9/subscriptions", "organizations_url": "https://api.github.com/users/jeffpollock9/orgs", "repos_url": "https://api.github.com/users/jeffpollock9/repos", "events_url": "https://api.github.com/users/jeffpollock9/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffpollock9/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-07-09T11:04:47Z", "updated_at": "2018-11-11T18:40:17Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: N/A</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<p>I was wondering if there is any appetite for a <a href=\"https://en.wikipedia.org/wiki/Stochastic_approximation#Robbins%E2%80%93Monro_algorithm\" rel=\"nofollow\">Robbins-Monro</a> type learning rate decay in tensorflow? The decay would be roughly (a more general solution is implemented at the bottom):</p>\n<div class=\"highlight highlight-source-python\"><pre>decayed_learning_rate <span class=\"pl-k\">=</span> learning_rate <span class=\"pl-k\">*</span> (global_step <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">^</span> (<span class=\"pl-k\">-</span>decay_rate) </pre></div>\n<p>As far as I can tell it is not already implemented in tensorflow, which surprised me since I think this is the learning rate decay rate required for theoretical convergence using the Adam optimizer in Section 4 of <a href=\"https://arxiv.org/pdf/1412.6980.pdf\" rel=\"nofollow\">the paper</a>, which has:</p>\n<div class=\"highlight highlight-source-python\"><pre>alpha_t <span class=\"pl-k\">=</span> alpha <span class=\"pl-k\">/</span> sqrt(t)</pre></div>\n<p>which is the same as the first equation with <code>decay_rate = 0.5</code>, and I assume they start at <code>t = 1</code> while tensorflow starts with <code>global_step = 0</code>.</p>\n<p>I have an implementation I have been using (mostly copied from the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/learning_rate_decay.py\">already implemented ones</a>):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">robbins_monro_decay</span>(<span class=\"pl-smi\">learning_rate</span>,\n                        <span class=\"pl-smi\">global_step</span>,\n                        <span class=\"pl-smi\">decay_steps</span>,\n                        <span class=\"pl-smi\">decay_rate</span>,\n                        <span class=\"pl-smi\">staircase</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                        <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>A Robbins-Monro type decay<span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">if</span> global_step <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>global_step is required for robbins_monro_decay.<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-k\">with</span> tf.name_scope(\n            name,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RobbinsMonroDecay<span class=\"pl-pds\">\"</span></span>,\n            [learning_rate, global_step, decay_steps, decay_rate]) <span class=\"pl-k\">as</span> name:\n\n        learning_rate <span class=\"pl-k\">=</span> tf.convert_to_tensor(\n            learning_rate, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>learning_rate<span class=\"pl-pds\">\"</span></span>)\n        dtype <span class=\"pl-k\">=</span> learning_rate.dtype\n        decay_steps <span class=\"pl-k\">=</span> tf.cast(decay_steps, dtype)\n        decay_rate <span class=\"pl-k\">=</span> tf.cast(decay_rate, dtype)\n        global_step <span class=\"pl-k\">=</span> tf.cast(global_step, dtype)\n        p <span class=\"pl-k\">=</span> global_step <span class=\"pl-k\">/</span> decay_steps\n\n        <span class=\"pl-k\">if</span> staircase:\n            p <span class=\"pl-k\">=</span> tf.floor(p)\n\n        <span class=\"pl-k\">return</span> tf.multiply(\n            learning_rate, tf.pow(p <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span>decay_rate), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name)</pre></div>\n<p>I can make a full pull request if that would be useful? I would be more than happy to add some additional documentation/code or change any of the code/naming/whatever.</p>\n<p>Thanks!</p>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\nTensorFlow installed from (source or binary): N/A\nTensorFlow version (use command below): 1.8\nPython version: N/A\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nI was wondering if there is any appetite for a Robbins-Monro type learning rate decay in tensorflow? The decay would be roughly (a more general solution is implemented at the bottom):\ndecayed_learning_rate = learning_rate * (global_step + 1) ^ (-decay_rate) \nAs far as I can tell it is not already implemented in tensorflow, which surprised me since I think this is the learning rate decay rate required for theoretical convergence using the Adam optimizer in Section 4 of the paper, which has:\nalpha_t = alpha / sqrt(t)\nwhich is the same as the first equation with decay_rate = 0.5, and I assume they start at t = 1 while tensorflow starts with global_step = 0.\nI have an implementation I have been using (mostly copied from the already implemented ones):\ndef robbins_monro_decay(learning_rate,\n                        global_step,\n                        decay_steps,\n                        decay_rate,\n                        staircase=False,\n                        name=None):\n    \"\"\"A Robbins-Monro type decay\"\"\"\n\n    if global_step is None:\n        raise ValueError(\"global_step is required for robbins_monro_decay.\")\n\n    with tf.name_scope(\n            name,\n            \"RobbinsMonroDecay\",\n            [learning_rate, global_step, decay_steps, decay_rate]) as name:\n\n        learning_rate = tf.convert_to_tensor(\n            learning_rate, name=\"learning_rate\")\n        dtype = learning_rate.dtype\n        decay_steps = tf.cast(decay_steps, dtype)\n        decay_rate = tf.cast(decay_rate, dtype)\n        global_step = tf.cast(global_step, dtype)\n        p = global_step / decay_steps\n\n        if staircase:\n            p = tf.floor(p)\n\n        return tf.multiply(\n            learning_rate, tf.pow(p + 1, -decay_rate), name=name)\nI can make a full pull request if that would be useful? I would be more than happy to add some additional documentation/code or change any of the code/naming/whatever.\nThanks!", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nI was wondering if there is any appetite for a [Robbins-Monro](https://en.wikipedia.org/wiki/Stochastic_approximation#Robbins%E2%80%93Monro_algorithm) type learning rate decay in tensorflow? The decay would be roughly (a more general solution is implemented at the bottom):\r\n\r\n```python\r\ndecayed_learning_rate = learning_rate * (global_step + 1) ^ (-decay_rate) \r\n```\r\n\r\nAs far as I can tell it is not already implemented in tensorflow, which surprised me since I think this is the learning rate decay rate required for theoretical convergence using the Adam optimizer in Section 4 of [the paper](https://arxiv.org/pdf/1412.6980.pdf), which has:\r\n\r\n```python\r\nalpha_t = alpha / sqrt(t)\r\n```\r\n\r\nwhich is the same as the first equation with `decay_rate = 0.5`, and I assume they start at `t = 1` while tensorflow starts with `global_step = 0`.\r\n\r\nI have an implementation I have been using (mostly copied from the [already implemented ones](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/learning_rate_decay.py)):\r\n\r\n```python\r\ndef robbins_monro_decay(learning_rate,\r\n                        global_step,\r\n                        decay_steps,\r\n                        decay_rate,\r\n                        staircase=False,\r\n                        name=None):\r\n    \"\"\"A Robbins-Monro type decay\"\"\"\r\n\r\n    if global_step is None:\r\n        raise ValueError(\"global_step is required for robbins_monro_decay.\")\r\n\r\n    with tf.name_scope(\r\n            name,\r\n            \"RobbinsMonroDecay\",\r\n            [learning_rate, global_step, decay_steps, decay_rate]) as name:\r\n\r\n        learning_rate = tf.convert_to_tensor(\r\n            learning_rate, name=\"learning_rate\")\r\n        dtype = learning_rate.dtype\r\n        decay_steps = tf.cast(decay_steps, dtype)\r\n        decay_rate = tf.cast(decay_rate, dtype)\r\n        global_step = tf.cast(global_step, dtype)\r\n        p = global_step / decay_steps\r\n\r\n        if staircase:\r\n            p = tf.floor(p)\r\n\r\n        return tf.multiply(\r\n            learning_rate, tf.pow(p + 1, -decay_rate), name=name)\r\n```\r\n\r\nI can make a full pull request if that would be useful? I would be more than happy to add some additional documentation/code or change any of the code/naming/whatever.\r\n\r\nThanks!"}