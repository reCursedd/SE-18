{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16362", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16362/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16362/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16362/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16362", "id": 291199399, "node_id": "MDU6SXNzdWUyOTExOTkzOTk=", "number": 16362, "title": "`tf.foldl` should have more robust input handling (like `tf.scan`)", "user": {"login": "mholzel", "id": 7227349, "node_id": "MDQ6VXNlcjcyMjczNDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/7227349?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mholzel", "html_url": "https://github.com/mholzel", "followers_url": "https://api.github.com/users/mholzel/followers", "following_url": "https://api.github.com/users/mholzel/following{/other_user}", "gists_url": "https://api.github.com/users/mholzel/gists{/gist_id}", "starred_url": "https://api.github.com/users/mholzel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mholzel/subscriptions", "organizations_url": "https://api.github.com/users/mholzel/orgs", "repos_url": "https://api.github.com/users/mholzel/repos", "events_url": "https://api.github.com/users/mholzel/events{/privacy}", "received_events_url": "https://api.github.com/users/mholzel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-24T12:58:43Z", "updated_at": "2018-01-25T00:37:32Z", "closed_at": "2018-01-25T00:37:32Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li>Windows 10 x64</li>\n<li>Installed from binary</li>\n<li>TensorFlow 1.4.0 (Cpu version)</li>\n<li>Python 3.6.1</li>\n</ul>\n<h3>Bug Description</h3>\n<p><code>tf.foldl</code> (and <code>tf.foldr</code>) are conceptually very very similar to <code>tf.scan</code>. Therefore the implementations are also very similar. However, <code>tf.scan</code> accepts initializer lists or tuples with varying type arguments, while <code>tf.foldl</code> does not. I think this is a simple oversight, and it seems that cutting and pasting some code from <code>tf.scan</code> to <code>tf.foldl</code> fixes this problem. Specifically. the master <code>tf.foldl </code>code is (after removing the docstring):</p>\n<pre><code>def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\n          swap_memory=False, name=None):\n  if not callable(fn):\n    raise TypeError(\"fn must be callable.\")\n\n  with ops.name_scope(name, \"foldl\", [elems]):\n    # Any get_variable calls in fn will cache the first call locally\n    # and not issue repeated network I/O requests for each iteration.\n    varscope = vs.get_variable_scope()\n    varscope_caching_device_was_none = False\n    if varscope.caching_device is None:\n      # TODO(ebrevdo): Change to using colocate_with here and in other methods.\n      varscope.set_caching_device(lambda op: op.device)\n      varscope_caching_device_was_none = True\n\n    # Convert elems to tensor array.\n    elems = ops.convert_to_tensor(elems, name=\"elems\")\n    n = array_ops.shape(elems)[0]\n    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\n                                            dynamic_size=False,\n                                            infer_shape=True)\n    elems_ta = elems_ta.unstack(elems)\n\n    if initializer is None:\n      a = elems_ta.read(0)\n      i = constant_op.constant(1)\n    else:\n      a = ops.convert_to_tensor(initializer)\n      i = constant_op.constant(0)\n\n    def compute(i, a):\n      a = fn(a, elems_ta.read(i))\n      return [i + 1, a]\n    _, r_a = control_flow_ops.while_loop(\n        lambda i, a: i &lt; n, compute, [i, a],\n        parallel_iterations=parallel_iterations,\n        back_prop=back_prop,\n        swap_memory=swap_memory)\n\n    if varscope_caching_device_was_none:\n      varscope.set_caching_device(None)\n    return r_a\n\n</code></pre>\n<p>Modifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the \"useModifications\" flag:</p>\n<pre><code>def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\n          swap_memory=False, name=None):\n    if not callable(fn):\n        raise TypeError(\"fn must be callable.\")\n\n    with ops.name_scope(name, \"foldl\", [elems]):\n        # Any get_variable calls in fn will cache the first call locally\n        # and not issue repeated network I/O requests for each iteration.\n        varscope = vs.get_variable_scope()\n        varscope_caching_device_was_none = False\n        if varscope.caching_device is None:\n            # TODO(ebrevdo): Change to using colocate_with here and in other methods.\n            varscope.set_caching_device(lambda op: op.device)\n            varscope_caching_device_was_none = True\n\n        # Convert elems to tensor array.\n        elems = ops.convert_to_tensor(elems, name=\"elems\")\n        n = array_ops.shape(elems)[0]\n        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\n                                                dynamic_size=False,\n                                                infer_shape=True)\n        elems_ta = elems_ta.unstack(elems)\n\n        if initializer is None:\n            a = elems_ta.read(0)\n            i = constant_op.constant(1)\n        else:\n            useModifications = True\n            if useModifications:\n                output_is_sequence = nest.is_sequence(initializer)\n                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]\n                initializer_flat = output_flatten(initializer)\n                a = [ops.convert_to_tensor(init) for init in initializer_flat]\n            else:\n                a = ops.convert_to_tensor(initializer)\n\n            i = constant_op.constant(0)\n\n        def compute(i, a):\n            a = fn(a, elems_ta.read(i))\n            return [i + 1, a]\n\n        _, r_a = control_flow_ops.while_loop(\n            lambda i, a: i &lt; n, compute, (i, a),\n            parallel_iterations=parallel_iterations,\n            back_prop=back_prop,\n            swap_memory=swap_memory)\n\n        if varscope_caching_device_was_none:\n            varscope.set_caching_device(None)\n        return r_a\n</code></pre>\n<p>Here is a MWE:</p>\n<pre><code>import tensorflow as tf\n\na = tf.constant( 1, dtype = tf.float32 )\nb = tf.constant( 2, dtype = tf.int64   )\n\nuseTuple = False\n\ndef body( ab, i ):\n    a = ab[0]\n    b = ab[1]\n    if useTuple:\n        return (a,b)\n    else:\n        return [a,b]\n\nN = 3\nwith tf.Session() as sess:\n    if useTuple:\n        ab = (a,b)\n    else:\n        ab = [a,b]\n    print( \"new foldl :\", sess.run(   foldl(  body, tf.range(N), ab ) ) )  \n    print( \"tf.scan   :\", sess.run( tf.scan(  body, tf.range(N), ab ) ) )\n    print( \"tf.foldl  :\", sess.run( tf.foldl( body, tf.range(N), ab ) ) )\n</code></pre>\n<p>with useTuple = False, this returns</p>\n<pre><code>new foldl : [1.0, 2]\ntf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]\n# Crash for tf.foldl with error: \nTypeError: Cannot convert a list containing a tensor of dtype &lt;dtype: 'int64'&gt; to &lt;dtype: 'float32'&gt; (Tensor is: &lt;tf.Tensor 'Const_5:0' shape=() dtype=int64&gt;)\n\n</code></pre>", "body_text": "System information\n\nWindows 10 x64\nInstalled from binary\nTensorFlow 1.4.0 (Cpu version)\nPython 3.6.1\n\nBug Description\ntf.foldl (and tf.foldr) are conceptually very very similar to tf.scan. Therefore the implementations are also very similar. However, tf.scan accepts initializer lists or tuples with varying type arguments, while tf.foldl does not. I think this is a simple oversight, and it seems that cutting and pasting some code from tf.scan to tf.foldl fixes this problem. Specifically. the master tf.foldl code is (after removing the docstring):\ndef foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\n          swap_memory=False, name=None):\n  if not callable(fn):\n    raise TypeError(\"fn must be callable.\")\n\n  with ops.name_scope(name, \"foldl\", [elems]):\n    # Any get_variable calls in fn will cache the first call locally\n    # and not issue repeated network I/O requests for each iteration.\n    varscope = vs.get_variable_scope()\n    varscope_caching_device_was_none = False\n    if varscope.caching_device is None:\n      # TODO(ebrevdo): Change to using colocate_with here and in other methods.\n      varscope.set_caching_device(lambda op: op.device)\n      varscope_caching_device_was_none = True\n\n    # Convert elems to tensor array.\n    elems = ops.convert_to_tensor(elems, name=\"elems\")\n    n = array_ops.shape(elems)[0]\n    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\n                                            dynamic_size=False,\n                                            infer_shape=True)\n    elems_ta = elems_ta.unstack(elems)\n\n    if initializer is None:\n      a = elems_ta.read(0)\n      i = constant_op.constant(1)\n    else:\n      a = ops.convert_to_tensor(initializer)\n      i = constant_op.constant(0)\n\n    def compute(i, a):\n      a = fn(a, elems_ta.read(i))\n      return [i + 1, a]\n    _, r_a = control_flow_ops.while_loop(\n        lambda i, a: i < n, compute, [i, a],\n        parallel_iterations=parallel_iterations,\n        back_prop=back_prop,\n        swap_memory=swap_memory)\n\n    if varscope_caching_device_was_none:\n      varscope.set_caching_device(None)\n    return r_a\n\n\nModifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the \"useModifications\" flag:\ndef foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\n          swap_memory=False, name=None):\n    if not callable(fn):\n        raise TypeError(\"fn must be callable.\")\n\n    with ops.name_scope(name, \"foldl\", [elems]):\n        # Any get_variable calls in fn will cache the first call locally\n        # and not issue repeated network I/O requests for each iteration.\n        varscope = vs.get_variable_scope()\n        varscope_caching_device_was_none = False\n        if varscope.caching_device is None:\n            # TODO(ebrevdo): Change to using colocate_with here and in other methods.\n            varscope.set_caching_device(lambda op: op.device)\n            varscope_caching_device_was_none = True\n\n        # Convert elems to tensor array.\n        elems = ops.convert_to_tensor(elems, name=\"elems\")\n        n = array_ops.shape(elems)[0]\n        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\n                                                dynamic_size=False,\n                                                infer_shape=True)\n        elems_ta = elems_ta.unstack(elems)\n\n        if initializer is None:\n            a = elems_ta.read(0)\n            i = constant_op.constant(1)\n        else:\n            useModifications = True\n            if useModifications:\n                output_is_sequence = nest.is_sequence(initializer)\n                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]\n                initializer_flat = output_flatten(initializer)\n                a = [ops.convert_to_tensor(init) for init in initializer_flat]\n            else:\n                a = ops.convert_to_tensor(initializer)\n\n            i = constant_op.constant(0)\n\n        def compute(i, a):\n            a = fn(a, elems_ta.read(i))\n            return [i + 1, a]\n\n        _, r_a = control_flow_ops.while_loop(\n            lambda i, a: i < n, compute, (i, a),\n            parallel_iterations=parallel_iterations,\n            back_prop=back_prop,\n            swap_memory=swap_memory)\n\n        if varscope_caching_device_was_none:\n            varscope.set_caching_device(None)\n        return r_a\n\nHere is a MWE:\nimport tensorflow as tf\n\na = tf.constant( 1, dtype = tf.float32 )\nb = tf.constant( 2, dtype = tf.int64   )\n\nuseTuple = False\n\ndef body( ab, i ):\n    a = ab[0]\n    b = ab[1]\n    if useTuple:\n        return (a,b)\n    else:\n        return [a,b]\n\nN = 3\nwith tf.Session() as sess:\n    if useTuple:\n        ab = (a,b)\n    else:\n        ab = [a,b]\n    print( \"new foldl :\", sess.run(   foldl(  body, tf.range(N), ab ) ) )  \n    print( \"tf.scan   :\", sess.run( tf.scan(  body, tf.range(N), ab ) ) )\n    print( \"tf.foldl  :\", sess.run( tf.foldl( body, tf.range(N), ab ) ) )\n\nwith useTuple = False, this returns\nnew foldl : [1.0, 2]\ntf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]\n# Crash for tf.foldl with error: \nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)", "body": "### System information\r\n- Windows 10 x64\r\n- Installed from binary\r\n- TensorFlow 1.4.0 (Cpu version)\r\n- Python 3.6.1\r\n\r\n### Bug Description\r\n`tf.foldl` (and `tf.foldr`) are conceptually very very similar to `tf.scan`. Therefore the implementations are also very similar. However, `tf.scan` accepts initializer lists or tuples with varying type arguments, while `tf.foldl` does not. I think this is a simple oversight, and it seems that cutting and pasting some code from `tf.scan` to `tf.foldl` fixes this problem. Specifically. the master `tf.foldl `code is (after removing the docstring):\r\n\r\n```\r\ndef foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\r\n          swap_memory=False, name=None):\r\n  if not callable(fn):\r\n    raise TypeError(\"fn must be callable.\")\r\n\r\n  with ops.name_scope(name, \"foldl\", [elems]):\r\n    # Any get_variable calls in fn will cache the first call locally\r\n    # and not issue repeated network I/O requests for each iteration.\r\n    varscope = vs.get_variable_scope()\r\n    varscope_caching_device_was_none = False\r\n    if varscope.caching_device is None:\r\n      # TODO(ebrevdo): Change to using colocate_with here and in other methods.\r\n      varscope.set_caching_device(lambda op: op.device)\r\n      varscope_caching_device_was_none = True\r\n\r\n    # Convert elems to tensor array.\r\n    elems = ops.convert_to_tensor(elems, name=\"elems\")\r\n    n = array_ops.shape(elems)[0]\r\n    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\r\n                                            dynamic_size=False,\r\n                                            infer_shape=True)\r\n    elems_ta = elems_ta.unstack(elems)\r\n\r\n    if initializer is None:\r\n      a = elems_ta.read(0)\r\n      i = constant_op.constant(1)\r\n    else:\r\n      a = ops.convert_to_tensor(initializer)\r\n      i = constant_op.constant(0)\r\n\r\n    def compute(i, a):\r\n      a = fn(a, elems_ta.read(i))\r\n      return [i + 1, a]\r\n    _, r_a = control_flow_ops.while_loop(\r\n        lambda i, a: i < n, compute, [i, a],\r\n        parallel_iterations=parallel_iterations,\r\n        back_prop=back_prop,\r\n        swap_memory=swap_memory)\r\n\r\n    if varscope_caching_device_was_none:\r\n      varscope.set_caching_device(None)\r\n    return r_a\r\n\r\n```\r\n\r\nModifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the \"useModifications\" flag:\r\n\r\n```\r\ndef foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\r\n          swap_memory=False, name=None):\r\n    if not callable(fn):\r\n        raise TypeError(\"fn must be callable.\")\r\n\r\n    with ops.name_scope(name, \"foldl\", [elems]):\r\n        # Any get_variable calls in fn will cache the first call locally\r\n        # and not issue repeated network I/O requests for each iteration.\r\n        varscope = vs.get_variable_scope()\r\n        varscope_caching_device_was_none = False\r\n        if varscope.caching_device is None:\r\n            # TODO(ebrevdo): Change to using colocate_with here and in other methods.\r\n            varscope.set_caching_device(lambda op: op.device)\r\n            varscope_caching_device_was_none = True\r\n\r\n        # Convert elems to tensor array.\r\n        elems = ops.convert_to_tensor(elems, name=\"elems\")\r\n        n = array_ops.shape(elems)[0]\r\n        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\r\n                                                dynamic_size=False,\r\n                                                infer_shape=True)\r\n        elems_ta = elems_ta.unstack(elems)\r\n\r\n        if initializer is None:\r\n            a = elems_ta.read(0)\r\n            i = constant_op.constant(1)\r\n        else:\r\n            useModifications = True\r\n            if useModifications:\r\n                output_is_sequence = nest.is_sequence(initializer)\r\n                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]\r\n                initializer_flat = output_flatten(initializer)\r\n                a = [ops.convert_to_tensor(init) for init in initializer_flat]\r\n            else:\r\n                a = ops.convert_to_tensor(initializer)\r\n\r\n            i = constant_op.constant(0)\r\n\r\n        def compute(i, a):\r\n            a = fn(a, elems_ta.read(i))\r\n            return [i + 1, a]\r\n\r\n        _, r_a = control_flow_ops.while_loop(\r\n            lambda i, a: i < n, compute, (i, a),\r\n            parallel_iterations=parallel_iterations,\r\n            back_prop=back_prop,\r\n            swap_memory=swap_memory)\r\n\r\n        if varscope_caching_device_was_none:\r\n            varscope.set_caching_device(None)\r\n        return r_a\r\n```\r\n\r\nHere is a MWE:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.constant( 1, dtype = tf.float32 )\r\nb = tf.constant( 2, dtype = tf.int64   )\r\n\r\nuseTuple = False\r\n\r\ndef body( ab, i ):\r\n    a = ab[0]\r\n    b = ab[1]\r\n    if useTuple:\r\n        return (a,b)\r\n    else:\r\n        return [a,b]\r\n\r\nN = 3\r\nwith tf.Session() as sess:\r\n    if useTuple:\r\n        ab = (a,b)\r\n    else:\r\n        ab = [a,b]\r\n    print( \"new foldl :\", sess.run(   foldl(  body, tf.range(N), ab ) ) )  \r\n    print( \"tf.scan   :\", sess.run( tf.scan(  body, tf.range(N), ab ) ) )\r\n    print( \"tf.foldl  :\", sess.run( tf.foldl( body, tf.range(N), ab ) ) )\r\n```\r\n\r\nwith useTuple = False, this returns \r\n\r\n```\r\nnew foldl : [1.0, 2]\r\ntf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]\r\n# Crash for tf.foldl with error: \r\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)\r\n\r\n```"}