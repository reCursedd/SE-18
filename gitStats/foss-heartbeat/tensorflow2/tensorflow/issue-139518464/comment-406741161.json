{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/406741161", "html_url": "https://github.com/tensorflow/tensorflow/issues/1439#issuecomment-406741161", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1439", "id": 406741161, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjc0MTE2MQ==", "user": {"login": "mehdirezaie", "id": 15114808, "node_id": "MDQ6VXNlcjE1MTE0ODA4", "avatar_url": "https://avatars0.githubusercontent.com/u/15114808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mehdirezaie", "html_url": "https://github.com/mehdirezaie", "followers_url": "https://api.github.com/users/mehdirezaie/followers", "following_url": "https://api.github.com/users/mehdirezaie/following{/other_user}", "gists_url": "https://api.github.com/users/mehdirezaie/gists{/gist_id}", "starred_url": "https://api.github.com/users/mehdirezaie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mehdirezaie/subscriptions", "organizations_url": "https://api.github.com/users/mehdirezaie/orgs", "repos_url": "https://api.github.com/users/mehdirezaie/repos", "events_url": "https://api.github.com/users/mehdirezaie/events{/privacy}", "received_events_url": "https://api.github.com/users/mehdirezaie/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-20T22:20:55Z", "updated_at": "2018-07-22T15:40:02Z", "author_association": "NONE", "body_html": "<p>Hi Tensorflowers!</p>\n<p>I am experiencing the same slow down issue by looping over sessions. In each loop I attempt to set a different seed for kernel initializer in <code>tf.layers.dense</code> , and I believe that \"the program is constantly adding more nodes to the graph\" causes this problem. In the following snipped I do my best to show how my program is structured. I set a global seed, and then generate a set of seeds in order to initialize my kernels differently in each run (or chain). The program is a feed forward neural net with two hidden layers with relu on the hidden layer units, and mean squared error as the cost function.</p>\n<p>The reason I am doing this is because I want to have independent trainings, and reproducible results. I was wondering if there is any way around to set the seed for the kernel initilizers without creating multiple nodes? At this point, I really appreciate any form of comments or help.</p>\n<p><strong>UPDATE</strong>: I figured I could reset the graph by adding <code>tf.reset_default_graph()</code> after sess.close()</p>\n<pre><code>global_seed = 1234\nnp.random.seed(global_seed)\nseeds = np.random.randint(0, 4294967295, size=nchain)\nfor ii in range(nchain): # loop over different runs/chains\n            tf.set_random_seed(seeds[ii]) # set the seed\n            # set up the model x [input] -&gt; y [output] with two hidden layers\n            x   = tf.placeholder(tf.float32, [None, nfeature])\n            # nfeature, Units, scale are number of features, number of units on each hidden layer \n            # and regularization scale, respectively\n            kernel_init0 = tf.random_normal_initializer(stddev=np.sqrt(1./(nfeature+1)), seed=seeds[ii])\n            kernel_init1 = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[0]+1)), seed=seeds[ii]) \n            kernel_init  = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[1]+1)), seed=seeds[ii]) \n            y0 = tf.layers.dense(x,  units=Units[0], activation=tf.nn.relu, kernel_initializer=kernel_init0,\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\n            y1 = tf.layers.dense(y0, units=Units[1], activation=tf.nn.relu, kernel_initializer=kernel_init1,\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\n            y  = tf.layers.dense(y1, units=1,   activation=None,  kernel_initializer=kernel_init,\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\n            # placeholders for the input errorbar and label\n            y_  = tf.placeholder(tf.float32, [None, 1])\n            w   = tf.placeholder(tf.float32, [None, 1])\n\n            #\n            # objective function plus regularization term\n            mse = tf.losses.mean_squared_error(y_, y, weights=w)\n            l2_loss = tf.losses.get_regularization_loss()\n            mse += l2_loss\n            #\n            global_step = tf.Variable(0, name='global_step', trainable=False)\n            optimizer   = tf.train.AdamOptimizer(learning_rate)\n            train_step  = optimizer.minimize(mse, global_step=global_step)            \n            # \n            # initialize the NN\n            sess = tf.InteractiveSession()\n            tf.global_variables_initializer().run()            \n            for epoch in range(number_of_epoch): # loop on training epochs\n                # do training ...\n            # evaluate something on test data    \n            sess.close()\n            # UPDATE July 22, 2018\n            tf.reset_default_graph()\n</code></pre>", "body_text": "Hi Tensorflowers!\nI am experiencing the same slow down issue by looping over sessions. In each loop I attempt to set a different seed for kernel initializer in tf.layers.dense , and I believe that \"the program is constantly adding more nodes to the graph\" causes this problem. In the following snipped I do my best to show how my program is structured. I set a global seed, and then generate a set of seeds in order to initialize my kernels differently in each run (or chain). The program is a feed forward neural net with two hidden layers with relu on the hidden layer units, and mean squared error as the cost function.\nThe reason I am doing this is because I want to have independent trainings, and reproducible results. I was wondering if there is any way around to set the seed for the kernel initilizers without creating multiple nodes? At this point, I really appreciate any form of comments or help.\nUPDATE: I figured I could reset the graph by adding tf.reset_default_graph() after sess.close()\nglobal_seed = 1234\nnp.random.seed(global_seed)\nseeds = np.random.randint(0, 4294967295, size=nchain)\nfor ii in range(nchain): # loop over different runs/chains\n            tf.set_random_seed(seeds[ii]) # set the seed\n            # set up the model x [input] -> y [output] with two hidden layers\n            x   = tf.placeholder(tf.float32, [None, nfeature])\n            # nfeature, Units, scale are number of features, number of units on each hidden layer \n            # and regularization scale, respectively\n            kernel_init0 = tf.random_normal_initializer(stddev=np.sqrt(1./(nfeature+1)), seed=seeds[ii])\n            kernel_init1 = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[0]+1)), seed=seeds[ii]) \n            kernel_init  = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[1]+1)), seed=seeds[ii]) \n            y0 = tf.layers.dense(x,  units=Units[0], activation=tf.nn.relu, kernel_initializer=kernel_init0,\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\n            y1 = tf.layers.dense(y0, units=Units[1], activation=tf.nn.relu, kernel_initializer=kernel_init1,\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\n            y  = tf.layers.dense(y1, units=1,   activation=None,  kernel_initializer=kernel_init,\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\n            # placeholders for the input errorbar and label\n            y_  = tf.placeholder(tf.float32, [None, 1])\n            w   = tf.placeholder(tf.float32, [None, 1])\n\n            #\n            # objective function plus regularization term\n            mse = tf.losses.mean_squared_error(y_, y, weights=w)\n            l2_loss = tf.losses.get_regularization_loss()\n            mse += l2_loss\n            #\n            global_step = tf.Variable(0, name='global_step', trainable=False)\n            optimizer   = tf.train.AdamOptimizer(learning_rate)\n            train_step  = optimizer.minimize(mse, global_step=global_step)            \n            # \n            # initialize the NN\n            sess = tf.InteractiveSession()\n            tf.global_variables_initializer().run()            \n            for epoch in range(number_of_epoch): # loop on training epochs\n                # do training ...\n            # evaluate something on test data    \n            sess.close()\n            # UPDATE July 22, 2018\n            tf.reset_default_graph()", "body": "Hi Tensorflowers!\r\n\r\nI am experiencing the same slow down issue by looping over sessions. In each loop I attempt to set a different seed for kernel initializer in `tf.layers.dense` , and I believe that \"the program is constantly adding more nodes to the graph\" causes this problem. In the following snipped I do my best to show how my program is structured. I set a global seed, and then generate a set of seeds in order to initialize my kernels differently in each run (or chain). The program is a feed forward neural net with two hidden layers with relu on the hidden layer units, and mean squared error as the cost function. \r\n\r\nThe reason I am doing this is because I want to have independent trainings, and reproducible results. I was wondering if there is any way around to set the seed for the kernel initilizers without creating multiple nodes? At this point, I really appreciate any form of comments or help.\r\n\r\n\r\n**UPDATE**: I figured I could reset the graph by adding `tf.reset_default_graph()` after sess.close()\r\n```\r\nglobal_seed = 1234\r\nnp.random.seed(global_seed)\r\nseeds = np.random.randint(0, 4294967295, size=nchain)\r\nfor ii in range(nchain): # loop over different runs/chains\r\n            tf.set_random_seed(seeds[ii]) # set the seed\r\n            # set up the model x [input] -> y [output] with two hidden layers\r\n            x   = tf.placeholder(tf.float32, [None, nfeature])\r\n            # nfeature, Units, scale are number of features, number of units on each hidden layer \r\n            # and regularization scale, respectively\r\n            kernel_init0 = tf.random_normal_initializer(stddev=np.sqrt(1./(nfeature+1)), seed=seeds[ii])\r\n            kernel_init1 = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[0]+1)), seed=seeds[ii]) \r\n            kernel_init  = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[1]+1)), seed=seeds[ii]) \r\n            y0 = tf.layers.dense(x,  units=Units[0], activation=tf.nn.relu, kernel_initializer=kernel_init0,\r\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\r\n            y1 = tf.layers.dense(y0, units=Units[1], activation=tf.nn.relu, kernel_initializer=kernel_init1,\r\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\r\n            y  = tf.layers.dense(y1, units=1,   activation=None,  kernel_initializer=kernel_init,\r\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\r\n            # placeholders for the input errorbar and label\r\n            y_  = tf.placeholder(tf.float32, [None, 1])\r\n            w   = tf.placeholder(tf.float32, [None, 1])\r\n\r\n            #\r\n            # objective function plus regularization term\r\n            mse = tf.losses.mean_squared_error(y_, y, weights=w)\r\n            l2_loss = tf.losses.get_regularization_loss()\r\n            mse += l2_loss\r\n            #\r\n            global_step = tf.Variable(0, name='global_step', trainable=False)\r\n            optimizer   = tf.train.AdamOptimizer(learning_rate)\r\n            train_step  = optimizer.minimize(mse, global_step=global_step)            \r\n            # \r\n            # initialize the NN\r\n            sess = tf.InteractiveSession()\r\n            tf.global_variables_initializer().run()            \r\n            for epoch in range(number_of_epoch): # loop on training epochs\r\n                # do training ...\r\n            # evaluate something on test data    \r\n            sess.close()\r\n            # UPDATE July 22, 2018\r\n            tf.reset_default_graph()\r\n```"}