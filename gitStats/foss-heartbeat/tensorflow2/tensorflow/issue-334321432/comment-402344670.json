{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/402344670", "html_url": "https://github.com/tensorflow/tensorflow/issues/20169#issuecomment-402344670", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20169", "id": 402344670, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjM0NDY3MA==", "user": {"login": "Anukkavishka", "id": 32035622, "node_id": "MDQ6VXNlcjMyMDM1NjIy", "avatar_url": "https://avatars1.githubusercontent.com/u/32035622?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Anukkavishka", "html_url": "https://github.com/Anukkavishka", "followers_url": "https://api.github.com/users/Anukkavishka/followers", "following_url": "https://api.github.com/users/Anukkavishka/following{/other_user}", "gists_url": "https://api.github.com/users/Anukkavishka/gists{/gist_id}", "starred_url": "https://api.github.com/users/Anukkavishka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Anukkavishka/subscriptions", "organizations_url": "https://api.github.com/users/Anukkavishka/orgs", "repos_url": "https://api.github.com/users/Anukkavishka/repos", "events_url": "https://api.github.com/users/Anukkavishka/events{/privacy}", "received_events_url": "https://api.github.com/users/Anukkavishka/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-04T02:27:50Z", "updated_at": "2018-07-04T02:27:50Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">On Wed, Jul 4, 2018 at 7:55 AM Anuk Kavishka &lt;anukkavishka94@gmail.com&gt;\nwrote:</div>\n<div class=\"email-quoted-reply\"> Ubuntu 18.0.4 64bit\n i have installed a binary version of tensorflow\n\n *tensorflow-1.8.0*\n\n Thanks for looking into this\n\n On Wed, Jul 4, 2018 at 4:53 AM shivaniag ***@***.***&gt; wrote:\n\n&gt; Please provide details about what platform you are using (operating\n&gt; system, architecture). Also include your TensorFlow version. Also, did you\n&gt; compile from source or install a binary?\n&gt; Could you also reference the code for the error.\n&gt;\n&gt; \u2014\n&gt; You are receiving this because you authored the thread.\n&gt; Reply to this email directly, view it on GitHub\n&gt; &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"334321432\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20169\" href=\"https://github.com/tensorflow/tensorflow/issues/20169#issuecomment-402318715\">#20169 (comment)</a>&gt;,\n&gt; or mute the thread\n&gt; &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AejTJiTkrNIdGNZYPKDmp9Sb_KkKQ0R1ks5uC_z-gaJpZM4UxVCR\">https://github.com/notifications/unsubscribe-auth/AejTJiTkrNIdGNZYPKDmp9Sb_KkKQ0R1ks5uC_z-gaJpZM4UxVCR</a>&gt;\n&gt; .\n&gt;\n</div>\n<div class=\"email-fragment\">import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n#code above here let's you import the tf sites repository for datasets for tutorials\nsess=tf.InteractiveSession()#in this Deep nueral network we use InteractiveSession()\n#we use the TF helper function to pull down the data from the MNIST site\nmnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n\n#we create a placeholder for 28*28 image data to be passed to our neural network\nx=tf.placeholder(tf.float32,shape=[None,784])\ny_=tf.placeholder(tf.float32,shape=[None,10])\n\n#changing the MNIST input data from a list of values to a 28*28*1 grayscale valued cube\n#which the Convolution NN can use:\n\nx_image=tf.reshape(x,[-1,28,28,1],name=\"x_image\")\n\n#we are going to use RELU as our activation function and if the value of x&lt;0 y=0;x&gt;0 y=x\n\n#define helper functions to created the weights and baises variables and convolution and pooling layers\n\n#these must be initialized to a small positive number\n#and with some noise you don't end up going to zero when comparing diffs\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape,stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1,shape=shape)\n    return tf.Variable(initial)\n\n#convolution and the pooling we do Convolution,and then pooling to control overfitting\n\ndef conv2d(x,W):\n    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x , ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\n\n\n#define layers in the NN\n\n#1st CNN\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115947948\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/32\" href=\"https://github.com/tensorflow/tensorflow/issues/32\">#32</a> features for each 5x5 patch of the image\n\nW_conv1=weight_variable([5,5,1,32]) <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> para represents the input channels since it's gray scale\n                                    #we have only 1,if we were using RGB we will have 3\nb_conv1=bias_variable([32])\n#Do convolution on images,add bias and push through RELU activation\nh_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n#take the results and run through max_pool\nh_pool1=max_pool_2x2(h_conv1)\n\n#2nd convolutional layer\n#process the 32 features from the first convolutional layer in 5x5 patch.\n#Return 64 features weights and biases.\n\nW_conv2=weight_variable([5,5,32,64])\nb_conv2=bias_variable([64])\n\n#Do convolution of the output of the 1st convolution latyer pool results\nh_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n#take the results and run through max_pool\nh_pool2=max_pool_2x2(h_conv2)\n\n#defining fully connected layer\n\nW_fulconnc1=weight_variable([7 * 7 * 64,1024])\nb_fulconnc1=bias_variable([1024])\n\n#connect output of pooling layer 2 as input for the fully connected layer\nh_pool2_flat=tf.reshape(h_pool2, [-1 , 7 * 7 * 64] )\nh_fulconnc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fulconnc1) + b_fulconnc1)\n\n#since we are using backProp algorithm alos the model might overfit the testing data\n#and the real world data will be predicted incorrectly,so we drop out few nuerons from the\n#fully connected layer\n#to solve this we use the below code\nkeep_prob=tf.placeholder(tf.float32) # get the drop out probability as a training input\nh_fulconnc1_drop=tf.nn.dropout(h_fulconnc1,keep_prob)\n\n#Readout layer\nW_fulconnc2=weight_variable([1024,10])\nb_fulconnc2=bias_variable([10])\n\n#define model\ny_conv=tf.matmul(h_fulconnc1_drop,W_fulconnc2) +b_fulconnc2\n\n\n\n#now we have to create a function to measure the loss just like in previous models\ncross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv,labels=y_))\n\n#y_ is the result given value and the y_conv is the predicted value\n\n#now let's optimize\ntrain_step=tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n#what is the correct prediction\n\ncorrect_prediction=tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))\n\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n#initialize all of the variables\nsess.run(tf.global_variables_initializer())\n\n#train the model\nimport time\n\n#define num of steps and how often we display the progress\n\nnum_steps =3000\ndisplay_every=100\n\n#starting the timer\n\nstart_time=time.time()\nend_time=time.time()\n\nfor i in range(num_steps):\n    batch=mnist.train.next_batch(50)\n    train_step.run(feed_dict={x:batch[0] , y_:batch[1] , keep_prob:0.5})\n\n    #periodic status display\n    if i%display_every == 0 :\n        train_accuracy=accuracy.eval(feed_dict={\n             x:batch[0] , y_:batch[1] , keep_prob :1.0})\n        end_time=time.time()\n        print(\"step {0},elapsed time {1:.2f} seconds,training accuracy{2:.3f}% \".format(i,end_time-start_time,train_accuracy*100))\n\n\n#display the summary\n\nend_time=time.time()\n\nprint(\"Total training time for {0} batches : {1:.2f} seconds\".format(i+1,end_time-start_time))\n\n#test the accuracy on the test data\n\nprint(\"Test Accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\nx:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})*100.0))</div>", "body_text": "On Wed, Jul 4, 2018 at 7:55 AM Anuk Kavishka <anukkavishka94@gmail.com>\nwrote:\n Ubuntu 18.0.4 64bit\n i have installed a binary version of tensorflow\n\n *tensorflow-1.8.0*\n\n Thanks for looking into this\n\n On Wed, Jul 4, 2018 at 4:53 AM shivaniag ***@***.***> wrote:\n\n> Please provide details about what platform you are using (operating\n> system, architecture). Also include your TensorFlow version. Also, did you\n> compile from source or install a binary?\n> Could you also reference the code for the error.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <#20169 (comment)>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AejTJiTkrNIdGNZYPKDmp9Sb_KkKQ0R1ks5uC_z-gaJpZM4UxVCR>\n> .\n>\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n#code above here let's you import the tf sites repository for datasets for tutorials\nsess=tf.InteractiveSession()#in this Deep nueral network we use InteractiveSession()\n#we use the TF helper function to pull down the data from the MNIST site\nmnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n\n#we create a placeholder for 28*28 image data to be passed to our neural network\nx=tf.placeholder(tf.float32,shape=[None,784])\ny_=tf.placeholder(tf.float32,shape=[None,10])\n\n#changing the MNIST input data from a list of values to a 28*28*1 grayscale valued cube\n#which the Convolution NN can use:\n\nx_image=tf.reshape(x,[-1,28,28,1],name=\"x_image\")\n\n#we are going to use RELU as our activation function and if the value of x<0 y=0;x>0 y=x\n\n#define helper functions to created the weights and baises variables and convolution and pooling layers\n\n#these must be initialized to a small positive number\n#and with some noise you don't end up going to zero when comparing diffs\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape,stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1,shape=shape)\n    return tf.Variable(initial)\n\n#convolution and the pooling we do Convolution,and then pooling to control overfitting\n\ndef conv2d(x,W):\n    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x , ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\n\n\n#define layers in the NN\n\n#1st CNN\n#32 features for each 5x5 patch of the image\n\nW_conv1=weight_variable([5,5,1,32]) #1 para represents the input channels since it's gray scale\n                                    #we have only 1,if we were using RGB we will have 3\nb_conv1=bias_variable([32])\n#Do convolution on images,add bias and push through RELU activation\nh_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n#take the results and run through max_pool\nh_pool1=max_pool_2x2(h_conv1)\n\n#2nd convolutional layer\n#process the 32 features from the first convolutional layer in 5x5 patch.\n#Return 64 features weights and biases.\n\nW_conv2=weight_variable([5,5,32,64])\nb_conv2=bias_variable([64])\n\n#Do convolution of the output of the 1st convolution latyer pool results\nh_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n#take the results and run through max_pool\nh_pool2=max_pool_2x2(h_conv2)\n\n#defining fully connected layer\n\nW_fulconnc1=weight_variable([7 * 7 * 64,1024])\nb_fulconnc1=bias_variable([1024])\n\n#connect output of pooling layer 2 as input for the fully connected layer\nh_pool2_flat=tf.reshape(h_pool2, [-1 , 7 * 7 * 64] )\nh_fulconnc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fulconnc1) + b_fulconnc1)\n\n#since we are using backProp algorithm alos the model might overfit the testing data\n#and the real world data will be predicted incorrectly,so we drop out few nuerons from the\n#fully connected layer\n#to solve this we use the below code\nkeep_prob=tf.placeholder(tf.float32) # get the drop out probability as a training input\nh_fulconnc1_drop=tf.nn.dropout(h_fulconnc1,keep_prob)\n\n#Readout layer\nW_fulconnc2=weight_variable([1024,10])\nb_fulconnc2=bias_variable([10])\n\n#define model\ny_conv=tf.matmul(h_fulconnc1_drop,W_fulconnc2) +b_fulconnc2\n\n\n\n#now we have to create a function to measure the loss just like in previous models\ncross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv,labels=y_))\n\n#y_ is the result given value and the y_conv is the predicted value\n\n#now let's optimize\ntrain_step=tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n#what is the correct prediction\n\ncorrect_prediction=tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))\n\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n#initialize all of the variables\nsess.run(tf.global_variables_initializer())\n\n#train the model\nimport time\n\n#define num of steps and how often we display the progress\n\nnum_steps =3000\ndisplay_every=100\n\n#starting the timer\n\nstart_time=time.time()\nend_time=time.time()\n\nfor i in range(num_steps):\n    batch=mnist.train.next_batch(50)\n    train_step.run(feed_dict={x:batch[0] , y_:batch[1] , keep_prob:0.5})\n\n    #periodic status display\n    if i%display_every == 0 :\n        train_accuracy=accuracy.eval(feed_dict={\n             x:batch[0] , y_:batch[1] , keep_prob :1.0})\n        end_time=time.time()\n        print(\"step {0},elapsed time {1:.2f} seconds,training accuracy{2:.3f}% \".format(i,end_time-start_time,train_accuracy*100))\n\n\n#display the summary\n\nend_time=time.time()\n\nprint(\"Total training time for {0} batches : {1:.2f} seconds\".format(i+1,end_time-start_time))\n\n#test the accuracy on the test data\n\nprint(\"Test Accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\nx:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})*100.0))", "body": "On Wed, Jul 4, 2018 at 7:55 AM Anuk Kavishka <anukkavishka94@gmail.com>\nwrote:\n\n> Ubuntu 18.0.4 64bit\n> i have installed a binary version of tensorflow\n>\n> *tensorflow-1.8.0*\n>\n> Thanks for looking into this\n>\n> On Wed, Jul 4, 2018 at 4:53 AM shivaniag <notifications@github.com> wrote:\n>\n>> Please provide details about what platform you are using (operating\n>> system, architecture). Also include your TensorFlow version. Also, did you\n>> compile from source or install a binary?\n>> Could you also reference the code for the error.\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/20169#issuecomment-402318715>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AejTJiTkrNIdGNZYPKDmp9Sb_KkKQ0R1ks5uC_z-gaJpZM4UxVCR>\n>> .\n>>\n>\n\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n#code above here let's you import the tf sites repository for datasets for tutorials\r\nsess=tf.InteractiveSession()#in this Deep nueral network we use InteractiveSession()\r\n#we use the TF helper function to pull down the data from the MNIST site\r\nmnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\r\n\r\n#we create a placeholder for 28*28 image data to be passed to our neural network\r\nx=tf.placeholder(tf.float32,shape=[None,784]) \r\ny_=tf.placeholder(tf.float32,shape=[None,10])\r\n\r\n#changing the MNIST input data from a list of values to a 28*28*1 grayscale valued cube\r\n#which the Convolution NN can use:\r\n\r\nx_image=tf.reshape(x,[-1,28,28,1],name=\"x_image\")\r\n\r\n#we are going to use RELU as our activation function and if the value of x<0 y=0;x>0 y=x\r\n\r\n#define helper functions to created the weights and baises variables and convolution and pooling layers\r\n\r\n#these must be initialized to a small positive number\r\n#and with some noise you don't end up going to zero when comparing diffs\r\n\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape,stddev=0.1)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1,shape=shape)\r\n    return tf.Variable(initial)\r\n    \r\n#convolution and the pooling we do Convolution,and then pooling to control overfitting\r\n\r\ndef conv2d(x,W):\r\n    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n    return tf.nn.max_pool(x , ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\r\n\r\n\r\n#define layers in the NN\r\n\r\n#1st CNN\r\n#32 features for each 5x5 patch of the image\r\n\r\nW_conv1=weight_variable([5,5,1,32]) #1 para represents the input channels since it's gray scale \r\n                                    #we have only 1,if we were using RGB we will have 3\r\nb_conv1=bias_variable([32])\r\n#Do convolution on images,add bias and push through RELU activation\r\nh_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\r\n#take the results and run through max_pool\r\nh_pool1=max_pool_2x2(h_conv1)\r\n\r\n#2nd convolutional layer \r\n#process the 32 features from the first convolutional layer in 5x5 patch.\r\n#Return 64 features weights and biases.\r\n\r\nW_conv2=weight_variable([5,5,32,64]) \r\nb_conv2=bias_variable([64])\r\n\r\n#Do convolution of the output of the 1st convolution latyer pool results\r\nh_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\r\n#take the results and run through max_pool\r\nh_pool2=max_pool_2x2(h_conv2)\r\n\r\n#defining fully connected layer\r\n\r\nW_fulconnc1=weight_variable([7 * 7 * 64,1024]) \r\nb_fulconnc1=bias_variable([1024])\r\n\r\n#connect output of pooling layer 2 as input for the fully connected layer\r\nh_pool2_flat=tf.reshape(h_pool2, [-1 , 7 * 7 * 64] )\r\nh_fulconnc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fulconnc1) + b_fulconnc1)\r\n\r\n#since we are using backProp algorithm alos the model might overfit the testing data\r\n#and the real world data will be predicted incorrectly,so we drop out few nuerons from the\r\n#fully connected layer\r\n#to solve this we use the below code\r\nkeep_prob=tf.placeholder(tf.float32) # get the drop out probability as a training input\r\nh_fulconnc1_drop=tf.nn.dropout(h_fulconnc1,keep_prob)\r\n\r\n#Readout layer\r\nW_fulconnc2=weight_variable([1024,10]) \r\nb_fulconnc2=bias_variable([10])\r\n\r\n#define model\r\ny_conv=tf.matmul(h_fulconnc1_drop,W_fulconnc2) +b_fulconnc2\r\n\r\n\r\n\r\n#now we have to create a function to measure the loss just like in previous models\r\ncross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv,labels=y_))\r\n\r\n#y_ is the result given value and the y_conv is the predicted value\r\n\r\n#now let's optimize \r\ntrain_step=tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\n#what is the correct prediction\r\n\r\ncorrect_prediction=tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))\r\n\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n#initialize all of the variables\r\nsess.run(tf.global_variables_initializer())\r\n\r\n#train the model\r\nimport time\r\n\r\n#define num of steps and how often we display the progress\r\n\r\nnum_steps =3000\r\ndisplay_every=100\r\n\r\n#starting the timer\r\n\r\nstart_time=time.time()\r\nend_time=time.time()\r\n\r\nfor i in range(num_steps):\r\n    batch=mnist.train.next_batch(50)\r\n    train_step.run(feed_dict={x:batch[0] , y_:batch[1] , keep_prob:0.5})\r\n\r\n    #periodic status display\r\n    if i%display_every == 0 :\r\n        train_accuracy=accuracy.eval(feed_dict={\r\n             x:batch[0] , y_:batch[1] , keep_prob :1.0})\r\n        end_time=time.time()\r\n        print(\"step {0},elapsed time {1:.2f} seconds,training accuracy{2:.3f}% \".format(i,end_time-start_time,train_accuracy*100))\r\n\r\n\r\n#display the summary\r\n\r\nend_time=time.time()\r\n\r\nprint(\"Total training time for {0} batches : {1:.2f} seconds\".format(i+1,end_time-start_time))\r\n\r\n#test the accuracy on the test data\r\n\r\nprint(\"Test Accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\r\nx:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})*100.0))\r\n\r\n\r\n"}