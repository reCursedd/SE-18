{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203911478", "pull_request_review_id": 138896924, "id": 203911478, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzkxMTQ3OA==", "diff_hunk": "@@ -54,6 +54,65 @@ def _MatrixDeterminantGrad(op, grad):\n                                                    0))\n   return multipliers * a_adj_inv\n \n+@ops.RegisterGradient(\"MatrixSquareRoot\")\n+def _MatrixSquareRootGrad(op, grad):\n+  \"\"\"Gradient for MatrixSquareRoot.\"\"\"\n+\n+  # Let A be an m x m square matrix (or batch of matrices)\n+  # Let R = sqrtm(A)\n+  # By definition, A = RR\n+  # Take the differential: dA = d(RR) = RdR + dRR\n+  # Solve the resulting Sylvester equation for dR\n+\n+  # Used to find Kronecker products within the Sylvester equation\n+  def _KroneckerProduct(mat1, mat2):\n+    \"\"\"Computes the Kronecker product of two matrices.\"\"\"\n+    m1, n1 = mat1.get_shape().as_list()\n+    mat1_rsh = array_ops.reshape(mat1, [m1, 1, n1, 1])\n+    m2, n2 = mat2.get_shape().as_list()\n+    mat2_rsh = array_ops.reshape(mat2, [1, m2, 1, n2])\n+    return array_ops.reshape(mat1_rsh * mat2_rsh, [m1 * m2, n1 * n2])\n+\n+  sqrt_batch = op.outputs[0] # R\n+  shape_batch = sqrt_batch.get_shape()\n+  order = shape_batch.as_list()[-1] # m\n+  identity = _linalg.eye(order, dtype=sqrt_batch.dtype) # m x m identity matrix\n+  shape_matrix = [order, order] # Tensor may be a batch of matrices of shape m x m\n+\n+  # Flatten batches containing R and dA\n+  flat_sqrt = array_ops.reshape(sqrt_batch, [-1])\n+  flat_grad = array_ops.reshape(grad, [-1])\n+\n+  # Split flattened batches into m x m matrices\n+  num_elements = flat_sqrt.get_shape().as_list()[-1]\n+  num_splits = int(num_elements / (order * order))\n+  split_sqrt = array_ops.split(flat_sqrt, num_splits)\n+  split_grad = array_ops.split(flat_grad, num_splits)\n+\n+  matrix_gradients = [] # Raw gradients of all m x m matrices\n+  for flat_sqrt_matrix, flat_grad_matrix in zip(split_sqrt, split_grad):\n+    sqrt_matrix = array_ops.reshape(flat_sqrt_matrix, shape_matrix) # m x m matrix R\n+    grad_matrix = array_ops.reshape(flat_grad_matrix, shape_matrix) # m x m matrix dA\n+\n+    # The transpose of R is taken in the k1 term instead of k2 in\n+    # order to prevent redundant transposition of R (i.e. (R')' = R)\n+    sqrt_matrix_transpose = array_ops.matrix_transpose(sqrt_matrix)\n+    k1 = _KroneckerProduct(identity, sqrt_matrix_transpose)\n+    k2 = _KroneckerProduct(sqrt_matrix, identity)\n+\n+    # Solve for vec(dR) by vectorizing dA\n+    inv_ksum = _linalg.inv(math_ops.add(k1, k2))", "path": "tensorflow/python/ops/linalg_grad.py", "position": null, "original_position": 51, "commit_id": "d0f92dfff7fa4e549d9737625d40fb638a51e06c", "original_commit_id": "cc7b422897ed523927d1dc32aea00cf3d3cc1540", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "Instead of forming the inverse and multiplying by it, can you write this in terms of linalg.matrix_solve? \r\n\r\n", "created_at": "2018-07-20T00:32:12Z", "updated_at": "2018-10-22T05:20:23Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20611#discussion_r203911478", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20611", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203911478"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20611#discussion_r203911478"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20611"}}, "body_html": "<p>Instead of forming the inverse and multiplying by it, can you write this in terms of linalg.matrix_solve?</p>", "body_text": "Instead of forming the inverse and multiplying by it, can you write this in terms of linalg.matrix_solve?"}