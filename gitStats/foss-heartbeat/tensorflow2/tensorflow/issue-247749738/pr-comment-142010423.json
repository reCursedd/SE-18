{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/142010423", "pull_request_review_id": 66317621, "id": 142010423, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MjAxMDQyMw==", "diff_hunk": "@@ -335,6 +337,7 @@ def __init__(self,\n                scale=False,\n                probability_fn=None,\n                score_mask_value=float(\"-inf\"),\n+               dtype=dtypes.float32,", "path": "tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", "position": null, "original_position": 22, "commit_id": "675287e27b25adde1c8bd43c6d1d7c99ece5e032", "original_commit_id": "03d09c1374a52ba802511ee43099d2f1b1ab36ff", "user": {"login": "ceteke", "id": 8996007, "node_id": "MDQ6VXNlcjg5OTYwMDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8996007?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ceteke", "html_url": "https://github.com/ceteke", "followers_url": "https://api.github.com/users/ceteke/followers", "following_url": "https://api.github.com/users/ceteke/following{/other_user}", "gists_url": "https://api.github.com/users/ceteke/gists{/gist_id}", "starred_url": "https://api.github.com/users/ceteke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ceteke/subscriptions", "organizations_url": "https://api.github.com/users/ceteke/orgs", "repos_url": "https://api.github.com/users/ceteke/repos", "events_url": "https://api.github.com/users/ceteke/events{/privacy}", "received_events_url": "https://api.github.com/users/ceteke/received_events", "type": "User", "site_admin": false}, "body": "Actually, I removed dtype from BaseAttention since dtype is given in Bahdanau and Luong attentions and this dtype is used in memory and query layers. The dype is passed to the layers in the attention classes.\r\n\r\nDefault score_mask_value is None. If no score_mask_value is given, memory.dtype.as_numpy_dtype(-np.inf) is used.\r\n\r\nIf dtype given in attetions is None dtype is float32.", "created_at": "2017-09-30T17:05:01Z", "updated_at": "2017-11-09T10:26:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12007#discussion_r142010423", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12007", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/142010423"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12007#discussion_r142010423"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12007"}}, "body_html": "<p>Actually, I removed dtype from BaseAttention since dtype is given in Bahdanau and Luong attentions and this dtype is used in memory and query layers. The dype is passed to the layers in the attention classes.</p>\n<p>Default score_mask_value is None. If no score_mask_value is given, memory.dtype.as_numpy_dtype(-np.inf) is used.</p>\n<p>If dtype given in attetions is None dtype is float32.</p>", "body_text": "Actually, I removed dtype from BaseAttention since dtype is given in Bahdanau and Luong attentions and this dtype is used in memory and query layers. The dype is passed to the layers in the attention classes.\nDefault score_mask_value is None. If no score_mask_value is given, memory.dtype.as_numpy_dtype(-np.inf) is used.\nIf dtype given in attetions is None dtype is float32.", "in_reply_to_id": 139300862}