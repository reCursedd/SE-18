{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266941947", "html_url": "https://github.com/tensorflow/tensorflow/issues/6224#issuecomment-266941947", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6224", "id": 266941947, "node_id": "MDEyOklzc3VlQ29tbWVudDI2Njk0MTk0Nw==", "user": {"login": "ZhengBitFusion", "id": 13840805, "node_id": "MDQ6VXNlcjEzODQwODA1", "avatar_url": "https://avatars1.githubusercontent.com/u/13840805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhengBitFusion", "html_url": "https://github.com/ZhengBitFusion", "followers_url": "https://api.github.com/users/ZhengBitFusion/followers", "following_url": "https://api.github.com/users/ZhengBitFusion/following{/other_user}", "gists_url": "https://api.github.com/users/ZhengBitFusion/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhengBitFusion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhengBitFusion/subscriptions", "organizations_url": "https://api.github.com/users/ZhengBitFusion/orgs", "repos_url": "https://api.github.com/users/ZhengBitFusion/repos", "events_url": "https://api.github.com/users/ZhengBitFusion/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhengBitFusion/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-14T04:49:50Z", "updated_at": "2016-12-14T04:50:23Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a> I tested with 1/1000 query frequency using this application <a href=\"https://github.com/openai/pixel-cnn\">https://github.com/openai/pixel-cnn</a>. It's actually 1% faster on native CUDA driver, and 10% faster on a remote CUDA service, meaning that the CUDA APIs are forwarded to a remote system.</p>\n<p>Since it is to protect against out-of-resource, why to use so many resources in the first place? Could the events be reused? Or, can the events be self-destroyed? A naive approach would be that once the event is recorded, insert a callback immediately after to destroy it, if you only want to record the event for the last time. Or, can the query frequency be adjustable at runtime?</p>", "body_text": "@poxvoculi I tested with 1/1000 query frequency using this application https://github.com/openai/pixel-cnn. It's actually 1% faster on native CUDA driver, and 10% faster on a remote CUDA service, meaning that the CUDA APIs are forwarded to a remote system.\nSince it is to protect against out-of-resource, why to use so many resources in the first place? Could the events be reused? Or, can the events be self-destroyed? A naive approach would be that once the event is recorded, insert a callback immediately after to destroy it, if you only want to record the event for the last time. Or, can the query frequency be adjustable at runtime?", "body": "@poxvoculi I tested with 1/1000 query frequency using this application https://github.com/openai/pixel-cnn. It's actually 1% faster on native CUDA driver, and 10% faster on a remote CUDA service, meaning that the CUDA APIs are forwarded to a remote system.\r\n\r\nSince it is to protect against out-of-resource, why to use so many resources in the first place? Could the events be reused? Or, can the events be self-destroyed? A naive approach would be that once the event is recorded, insert a callback immediately after to destroy it, if you only want to record the event for the last time. Or, can the query frequency be adjustable at runtime?"}