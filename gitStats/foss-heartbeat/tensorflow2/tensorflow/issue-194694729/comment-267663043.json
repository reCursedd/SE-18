{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267663043", "html_url": "https://github.com/tensorflow/tensorflow/issues/6224#issuecomment-267663043", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6224", "id": 267663043, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzY2MzA0Mw==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-16T18:29:56Z", "updated_at": "2016-12-16T18:29:56Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13840805\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ZhengBitFusion\">@ZhengBitFusion</a> It's interesting that you're seeing a 10% speedup with a remote CUDA service.  That's not a configuration that we tried or considered during development.  This part of TensorFlow was developed and tuned some time ago, before we fully settled some other details, and not reconsidered since.  And of course the GPU environment has continued to evolve.  It would be nice to drive a reexamination of the EventMgr design with a benchmark suite that could be tested on multiple system configurations.  Unfortunately I don't know of anything especially suited to this purpose.  My recollection is that we tuned using a data-parallel version of inception.</p>\n<p>In the short term I think it would be good to replace the timing constants kPollingDelayUsecs and kPollingSuspendMsecs with values that can be user-set in GPUOptions<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L14\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L14</a><br>\nThis will allow you to immediately improve your remote CUDA case.</p>\n<p>In the longer term, we should gather or develop an approprate benchmark suite and try using the cuda callback mechanism.</p>", "body_text": "@ZhengBitFusion It's interesting that you're seeing a 10% speedup with a remote CUDA service.  That's not a configuration that we tried or considered during development.  This part of TensorFlow was developed and tuned some time ago, before we fully settled some other details, and not reconsidered since.  And of course the GPU environment has continued to evolve.  It would be nice to drive a reexamination of the EventMgr design with a benchmark suite that could be tested on multiple system configurations.  Unfortunately I don't know of anything especially suited to this purpose.  My recollection is that we tuned using a data-parallel version of inception.\nIn the short term I think it would be good to replace the timing constants kPollingDelayUsecs and kPollingSuspendMsecs with values that can be user-set in GPUOptions\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L14\nThis will allow you to immediately improve your remote CUDA case.\nIn the longer term, we should gather or develop an approprate benchmark suite and try using the cuda callback mechanism.", "body": "@ZhengBitFusion It's interesting that you're seeing a 10% speedup with a remote CUDA service.  That's not a configuration that we tried or considered during development.  This part of TensorFlow was developed and tuned some time ago, before we fully settled some other details, and not reconsidered since.  And of course the GPU environment has continued to evolve.  It would be nice to drive a reexamination of the EventMgr design with a benchmark suite that could be tested on multiple system configurations.  Unfortunately I don't know of anything especially suited to this purpose.  My recollection is that we tuned using a data-parallel version of inception.\r\n\r\nIn the short term I think it would be good to replace the timing constants kPollingDelayUsecs and kPollingSuspendMsecs with values that can be user-set in GPUOptions \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L14\r\nThis will allow you to immediately improve your remote CUDA case.\r\n\r\nIn the longer term, we should gather or develop an approprate benchmark suite and try using the cuda callback mechanism."}