{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235109641", "html_url": "https://github.com/tensorflow/tensorflow/issues/2937#issuecomment-235109641", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2937", "id": 235109641, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTEwOTY0MQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-25T22:47:05Z", "updated_at": "2016-07-25T22:47:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1710528\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bhack\">@bhack</a>, I don't think anyone sets a clear ratio. And it varies case by case for each op. For new ops, we firstly prefer a fast solution with source within reasonable amount of effort. If it takes a lot of effort to match the performance of Cudnn, or any other library, we would not hesitate to use that library. Overall, we go with the fastest solution. If the performance is about the same between our custom code and Cudnn, we prefer the one with the source.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=112599\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Mistobaan\">@Mistobaan</a>, we are working on adding a Cudnn RNN layer to tf.contrib. We will announce in the release note when it is done.</p>", "body_text": "@bhack, I don't think anyone sets a clear ratio. And it varies case by case for each op. For new ops, we firstly prefer a fast solution with source within reasonable amount of effort. If it takes a lot of effort to match the performance of Cudnn, or any other library, we would not hesitate to use that library. Overall, we go with the fastest solution. If the performance is about the same between our custom code and Cudnn, we prefer the one with the source.\n@Mistobaan, we are working on adding a Cudnn RNN layer to tf.contrib. We will announce in the release note when it is done.", "body": "@bhack, I don't think anyone sets a clear ratio. And it varies case by case for each op. For new ops, we firstly prefer a fast solution with source within reasonable amount of effort. If it takes a lot of effort to match the performance of Cudnn, or any other library, we would not hesitate to use that library. Overall, we go with the fastest solution. If the performance is about the same between our custom code and Cudnn, we prefer the one with the source.\n\n@Mistobaan, we are working on adding a Cudnn RNN layer to tf.contrib. We will announce in the release note when it is done. \n"}