{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13439", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13439/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13439/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13439/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13439", "id": 261998092, "node_id": "MDU6SXNzdWUyNjE5OTgwOTI=", "number": 13439, "title": "Keras has much better gradients calculated than native TF", "user": {"login": "OscarDPan", "id": 10855426, "node_id": "MDQ6VXNlcjEwODU1NDI2", "avatar_url": "https://avatars0.githubusercontent.com/u/10855426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OscarDPan", "html_url": "https://github.com/OscarDPan", "followers_url": "https://api.github.com/users/OscarDPan/followers", "following_url": "https://api.github.com/users/OscarDPan/following{/other_user}", "gists_url": "https://api.github.com/users/OscarDPan/gists{/gist_id}", "starred_url": "https://api.github.com/users/OscarDPan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OscarDPan/subscriptions", "organizations_url": "https://api.github.com/users/OscarDPan/orgs", "repos_url": "https://api.github.com/users/OscarDPan/repos", "events_url": "https://api.github.com/users/OscarDPan/events{/privacy}", "received_events_url": "https://api.github.com/users/OscarDPan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2017-10-02T07:09:27Z", "updated_at": "2018-02-06T20:00:40Z", "closed_at": "2018-02-06T20:00:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi,<br>\nI am not sure if this is a bug in some TF function or Keras has just some clever ways to pull things off.<br>\nI was prototyping a simple logistic regression model with Keras and trying to write the exact same model with TF to reproduce the result. However, there's something unexplainable to me that Keras always has much better gradients calculated than TF does when I use mini-batch SGD.</p>\n<p>tensorflow==1.2.1<br>\nKeras==2.0.8<br>\nGPU: Tesla P40</p>\n<p>Keras version:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">custom_objective</span>(<span class=\"pl-smi\">y_true</span>, <span class=\"pl-smi\">y_pred</span>):\n    loss <span class=\"pl-k\">=</span> tf.reduce_mean(<span class=\"pl-k\">-</span>(y_true<span class=\"pl-k\">*</span>tf.log(y_pred)<span class=\"pl-k\">+</span>((<span class=\"pl-c1\">1.0</span><span class=\"pl-k\">-</span>y_true)<span class=\"pl-k\">*</span>tf.log(<span class=\"pl-c1\">1.0</span><span class=\"pl-k\">-</span>y_pred))))\n    <span class=\"pl-k\">return</span> loss\nmodel <span class=\"pl-k\">=</span> Sequential()\nmodel.add(Dense(<span class=\"pl-c1\">1</span>,<span class=\"pl-v\">input_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2440000</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">bias_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>zeros<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>zeros<span class=\"pl-pds\">'</span></span>))\nsgd <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.5</span>)\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>custom_objective, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>sgd)\nmodel.fit_generator(generator, <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">callbacks</span><span class=\"pl-k\">=</span>[ival], <span class=\"pl-v\">max_queue_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">use_multiprocessing</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">initial_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)</pre></div>\n<p>TF version:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">linear</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">n_input</span>, <span class=\"pl-smi\">n_output</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">with</span> tf.variable_scope(name <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc<span class=\"pl-pds\">'</span></span>):\n        W <span class=\"pl-k\">=</span> tf.get_variable(\n            <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> shape = [n_input, n_output],</span>\n            <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> initializer=tf.contrib.layers.xavier_initializer())</span>\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.zeros(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[n_input,n_output]))\n        b <span class=\"pl-k\">=</span> tf.get_variable(\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>,\n            <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[n_output],\n            <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>))\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(x, tf.SparseTensor):\n            h <span class=\"pl-k\">=</span> tf.nn.bias_add(\n                tf.matmul(x, W),\n                b,\n                <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>h<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">else</span>:\n            h <span class=\"pl-k\">=</span> tf.nn.bias_add(\n                tf.sparse_tensor_dense_matmul(x, W),\n                b,\n                <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>h<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> h, W, b\n\ntf.reset_default_graph()\nX_shape <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X_shape<span class=\"pl-pds\">\"</span></span>)\nX_indices <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X_indices<span class=\"pl-pds\">\"</span></span>)\nX_values <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X_values<span class=\"pl-pds\">\"</span></span>)\ny <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>)\nH <span class=\"pl-k\">=</span> tf.SparseTensor(<span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>X_indices, <span class=\"pl-v\">values</span><span class=\"pl-k\">=</span>X_values, <span class=\"pl-v\">dense_shape</span><span class=\"pl-k\">=</span>X_shape)\nlogit, w, b <span class=\"pl-k\">=</span> linear(H, <span class=\"pl-c1\">2440000</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_layer<span class=\"pl-pds\">\"</span></span>)\ny_pred <span class=\"pl-k\">=</span> tf.nn.sigmoid(logit)\ntrain_error <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>(y<span class=\"pl-k\">*</span>tf.log(y_pred) <span class=\"pl-k\">+</span> ((<span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">-</span> y) <span class=\"pl-k\">*</span> tf.log(<span class=\"pl-c1\">1.0</span><span class=\"pl-k\">-</span>y_pred)))\nloss <span class=\"pl-k\">=</span> tf.reduce_mean(train_error)\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\ngvs <span class=\"pl-k\">=</span> optimizer.compute_gradients(loss,[w,b])\ntrain_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(gvs)\nsess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), <span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>tf.get_default_graph())\nsess.run(tf.global_variables_initializer())</pre></div>\n<p>TL;DR<br>\nKeras has better gradients calculated/updates than TF.</p>\n<p>Both version implements a vanilla logistic regression, with <strong>same native TF optimizer</strong>, <strong>same user defined cross entropy</strong> and <strong>same data generator</strong>(except for Keras accepts a dense matrix and TF accepts sparse matrix.tocoo()), <strong>same learning rate</strong>, <strong>same zero initializer for both w and b</strong>.<br>\nSimple calculus can show that if the first batch contains all NEGATIVE examples, the gradient for b in the first update must be exactly 0.5.</p>\n<p>If a batch has very few examples (e.g 1-9), both version produce an exact gradient of 0.5 for b.<br>\nWhen sample size goes above 9, Keras starts to have a way better gradients calculated for both b and w. For example, with sample size 10, Keras calculates 0.50000006 for b and TF gives 0.49999988. With sample size 12, Keras gives 0.49999994 but TF gives 0.50000012. Though both give wrong gradient, Keras is always better, not to mentions the weights gradients. Also trying casting the loss to float16, 32 or 64 won't make the gradient as good as Keras'.</p>\n<p>The accumulated differences after 100 batches of training makes TF's model worse than Keras' in terms of AUC.</p>\n<p>At this stage I am not sure where I should look for so I resort to the community to help me with this \"unexplainable\" phenomena. Any suggestion will be much appreciated.</p>\n<p>Oscar</p>", "body_text": "Hi,\nI am not sure if this is a bug in some TF function or Keras has just some clever ways to pull things off.\nI was prototyping a simple logistic regression model with Keras and trying to write the exact same model with TF to reproduce the result. However, there's something unexplainable to me that Keras always has much better gradients calculated than TF does when I use mini-batch SGD.\ntensorflow==1.2.1\nKeras==2.0.8\nGPU: Tesla P40\nKeras version:\ndef custom_objective(y_true, y_pred):\n    loss = tf.reduce_mean(-(y_true*tf.log(y_pred)+((1.0-y_true)*tf.log(1.0-y_pred))))\n    return loss\nmodel = Sequential()\nmodel.add(Dense(1,input_dim=2440000, activation='sigmoid', bias_initializer='zeros', kernel_initializer='zeros'))\nsgd = tf.train.GradientDescentOptimizer(0.5)\nmodel.compile(loss=custom_objective, optimizer=sgd)\nmodel.fit_generator(generator, steps_per_epoch=1, epochs=1, callbacks=[ival], max_queue_size=10, workers=1, use_multiprocessing=False, initial_epoch=0)\nTF version:\ndef linear(x, n_input, n_output, name=None):\n    with tf.variable_scope(name or 'fc'):\n        W = tf.get_variable(\n            name = \"W\",\n            # shape = [n_input, n_output],\n            dtype=tf.float32,\n            # initializer=tf.contrib.layers.xavier_initializer())\n            initializer=tf.zeros(shape=[n_input,n_output]))\n        b = tf.get_variable(\n            name='bias',\n            shape=[n_output],\n            dtype=tf.float32,\n            initializer=tf.constant_initializer(0.0))\n        if not isinstance(x, tf.SparseTensor):\n            h = tf.nn.bias_add(\n                tf.matmul(x, W),\n                b,\n                name='h')\n        else:\n            h = tf.nn.bias_add(\n                tf.sparse_tensor_dense_matmul(x, W),\n                b,\n                name='h')\n    return h, W, b\n\ntf.reset_default_graph()\nX_shape = tf.placeholder(tf.int64, shape=[2], name=\"X_shape\")\nX_indices = tf.placeholder(tf.int64, name=\"X_indices\")\nX_values = tf.placeholder(tf.float32, shape=[None], name=\"X_values\")\ny = tf.placeholder(dtype=tf.float32, name=\"y\")\nH = tf.SparseTensor(indices=X_indices, values=X_values, dense_shape=X_shape)\nlogit, w, b = linear(H, 2440000, 1, name=\"output_layer\")\ny_pred = tf.nn.sigmoid(logit)\ntrain_error = -(y*tf.log(y_pred) + ((1.0 - y) * tf.log(1.0-y_pred)))\nloss = tf.reduce_mean(train_error)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\ngvs = optimizer.compute_gradients(loss,[w,b])\ntrain_op = optimizer.apply_gradients(gvs)\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True), graph=tf.get_default_graph())\nsess.run(tf.global_variables_initializer())\nTL;DR\nKeras has better gradients calculated/updates than TF.\nBoth version implements a vanilla logistic regression, with same native TF optimizer, same user defined cross entropy and same data generator(except for Keras accepts a dense matrix and TF accepts sparse matrix.tocoo()), same learning rate, same zero initializer for both w and b.\nSimple calculus can show that if the first batch contains all NEGATIVE examples, the gradient for b in the first update must be exactly 0.5.\nIf a batch has very few examples (e.g 1-9), both version produce an exact gradient of 0.5 for b.\nWhen sample size goes above 9, Keras starts to have a way better gradients calculated for both b and w. For example, with sample size 10, Keras calculates 0.50000006 for b and TF gives 0.49999988. With sample size 12, Keras gives 0.49999994 but TF gives 0.50000012. Though both give wrong gradient, Keras is always better, not to mentions the weights gradients. Also trying casting the loss to float16, 32 or 64 won't make the gradient as good as Keras'.\nThe accumulated differences after 100 batches of training makes TF's model worse than Keras' in terms of AUC.\nAt this stage I am not sure where I should look for so I resort to the community to help me with this \"unexplainable\" phenomena. Any suggestion will be much appreciated.\nOscar", "body": "Hi,\r\nI am not sure if this is a bug in some TF function or Keras has just some clever ways to pull things off.\r\nI was prototyping a simple logistic regression model with Keras and trying to write the exact same model with TF to reproduce the result. However, there's something unexplainable to me that Keras always has much better gradients calculated than TF does when I use mini-batch SGD.\r\n\r\ntensorflow==1.2.1\r\nKeras==2.0.8\r\nGPU: Tesla P40\r\n\r\nKeras version:\r\n```python\r\ndef custom_objective(y_true, y_pred):\r\n    loss = tf.reduce_mean(-(y_true*tf.log(y_pred)+((1.0-y_true)*tf.log(1.0-y_pred))))\r\n    return loss\r\nmodel = Sequential()\r\nmodel.add(Dense(1,input_dim=2440000, activation='sigmoid', bias_initializer='zeros', kernel_initializer='zeros'))\r\nsgd = tf.train.GradientDescentOptimizer(0.5)\r\nmodel.compile(loss=custom_objective, optimizer=sgd)\r\nmodel.fit_generator(generator, steps_per_epoch=1, epochs=1, callbacks=[ival], max_queue_size=10, workers=1, use_multiprocessing=False, initial_epoch=0)\r\n```\r\nTF version:\r\n```python\r\ndef linear(x, n_input, n_output, name=None):\r\n    with tf.variable_scope(name or 'fc'):\r\n        W = tf.get_variable(\r\n            name = \"W\",\r\n            # shape = [n_input, n_output],\r\n            dtype=tf.float32,\r\n            # initializer=tf.contrib.layers.xavier_initializer())\r\n            initializer=tf.zeros(shape=[n_input,n_output]))\r\n        b = tf.get_variable(\r\n            name='bias',\r\n            shape=[n_output],\r\n            dtype=tf.float32,\r\n            initializer=tf.constant_initializer(0.0))\r\n        if not isinstance(x, tf.SparseTensor):\r\n            h = tf.nn.bias_add(\r\n                tf.matmul(x, W),\r\n                b,\r\n                name='h')\r\n        else:\r\n            h = tf.nn.bias_add(\r\n                tf.sparse_tensor_dense_matmul(x, W),\r\n                b,\r\n                name='h')\r\n    return h, W, b\r\n\r\ntf.reset_default_graph()\r\nX_shape = tf.placeholder(tf.int64, shape=[2], name=\"X_shape\")\r\nX_indices = tf.placeholder(tf.int64, name=\"X_indices\")\r\nX_values = tf.placeholder(tf.float32, shape=[None], name=\"X_values\")\r\ny = tf.placeholder(dtype=tf.float32, name=\"y\")\r\nH = tf.SparseTensor(indices=X_indices, values=X_values, dense_shape=X_shape)\r\nlogit, w, b = linear(H, 2440000, 1, name=\"output_layer\")\r\ny_pred = tf.nn.sigmoid(logit)\r\ntrain_error = -(y*tf.log(y_pred) + ((1.0 - y) * tf.log(1.0-y_pred)))\r\nloss = tf.reduce_mean(train_error)\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\r\ngvs = optimizer.compute_gradients(loss,[w,b])\r\ntrain_op = optimizer.apply_gradients(gvs)\r\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True), graph=tf.get_default_graph())\r\nsess.run(tf.global_variables_initializer())\r\n```\r\n\r\nTL;DR\r\nKeras has better gradients calculated/updates than TF.\r\n\r\nBoth version implements a vanilla logistic regression, with **same native TF optimizer**, **same user defined cross entropy** and **same data generator**(except for Keras accepts a dense matrix and TF accepts sparse matrix.tocoo()), **same learning rate**, **same zero initializer for both w and b**.\r\nSimple calculus can show that if the first batch contains all NEGATIVE examples, the gradient for b in the first update must be exactly 0.5.\r\n\r\nIf a batch has very few examples (e.g 1-9), both version produce an exact gradient of 0.5 for b.\r\nWhen sample size goes above 9, Keras starts to have a way better gradients calculated for both b and w. For example, with sample size 10, Keras calculates 0.50000006 for b and TF gives 0.49999988. With sample size 12, Keras gives 0.49999994 but TF gives 0.50000012. Though both give wrong gradient, Keras is always better, not to mentions the weights gradients. Also trying casting the loss to float16, 32 or 64 won't make the gradient as good as Keras'.\r\n\r\nThe accumulated differences after 100 batches of training makes TF's model worse than Keras' in terms of AUC.\r\n\r\nAt this stage I am not sure where I should look for so I resort to the community to help me with this \"unexplainable\" phenomena. Any suggestion will be much appreciated.\r\n\r\nOscar\r\n"}