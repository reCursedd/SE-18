{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281898378", "html_url": "https://github.com/tensorflow/tensorflow/issues/7643#issuecomment-281898378", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7643", "id": 281898378, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTg5ODM3OA==", "user": {"login": "dillonalaird", "id": 3057947, "node_id": "MDQ6VXNlcjMwNTc5NDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3057947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dillonalaird", "html_url": "https://github.com/dillonalaird", "followers_url": "https://api.github.com/users/dillonalaird/followers", "following_url": "https://api.github.com/users/dillonalaird/following{/other_user}", "gists_url": "https://api.github.com/users/dillonalaird/gists{/gist_id}", "starred_url": "https://api.github.com/users/dillonalaird/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dillonalaird/subscriptions", "organizations_url": "https://api.github.com/users/dillonalaird/orgs", "repos_url": "https://api.github.com/users/dillonalaird/repos", "events_url": "https://api.github.com/users/dillonalaird/events{/privacy}", "received_events_url": "https://api.github.com/users/dillonalaird/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-23T05:26:23Z", "updated_at": "2017-02-23T05:26:23Z", "author_association": "NONE", "body_html": "<p>I think this works</p>\n<pre><code>max_seq_len = 10\nx = tf.placeholder(tf.float32, [None, 5])\nW = tf.get_variable(\"W\", [5, 3])\nh = tf.matmul(x, W)\n\ndef body(old_g, t):\n    g = tf.gradients([h[t]], [W])[0]\n    new_g = tuple(tf.select(tf.equal(ti, t), g, old_g[ti]) for ti in range(len(old_g)))\n    return new_g, t + 1\n\ndef cond(_, t):\n    return tf.less(t, tf.shape(h)[0])\n\ngrads = tf.while_loop(cond, body, [(tf.zeros_like(W),)*max_seq_len, tf.constant(0)])\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    grads_out = sess.run(grads, feed_dict={x: np.random.randn(2*5).reshape(2, 5)})\n</code></pre>\n<p>You basically just build a giant empty tuple and fill it in as you go. dynmaic_rnn does something similar to this.</p>", "body_text": "I think this works\nmax_seq_len = 10\nx = tf.placeholder(tf.float32, [None, 5])\nW = tf.get_variable(\"W\", [5, 3])\nh = tf.matmul(x, W)\n\ndef body(old_g, t):\n    g = tf.gradients([h[t]], [W])[0]\n    new_g = tuple(tf.select(tf.equal(ti, t), g, old_g[ti]) for ti in range(len(old_g)))\n    return new_g, t + 1\n\ndef cond(_, t):\n    return tf.less(t, tf.shape(h)[0])\n\ngrads = tf.while_loop(cond, body, [(tf.zeros_like(W),)*max_seq_len, tf.constant(0)])\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    grads_out = sess.run(grads, feed_dict={x: np.random.randn(2*5).reshape(2, 5)})\n\nYou basically just build a giant empty tuple and fill it in as you go. dynmaic_rnn does something similar to this.", "body": "I think this works\r\n\r\n```\r\nmax_seq_len = 10\r\nx = tf.placeholder(tf.float32, [None, 5])\r\nW = tf.get_variable(\"W\", [5, 3])\r\nh = tf.matmul(x, W)\r\n\r\ndef body(old_g, t):\r\n    g = tf.gradients([h[t]], [W])[0]\r\n    new_g = tuple(tf.select(tf.equal(ti, t), g, old_g[ti]) for ti in range(len(old_g)))\r\n    return new_g, t + 1\r\n\r\ndef cond(_, t):\r\n    return tf.less(t, tf.shape(h)[0])\r\n\r\ngrads = tf.while_loop(cond, body, [(tf.zeros_like(W),)*max_seq_len, tf.constant(0)])\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    grads_out = sess.run(grads, feed_dict={x: np.random.randn(2*5).reshape(2, 5)})\r\n```\r\n\r\nYou basically just build a giant empty tuple and fill it in as you go. dynmaic_rnn does something similar to this. "}