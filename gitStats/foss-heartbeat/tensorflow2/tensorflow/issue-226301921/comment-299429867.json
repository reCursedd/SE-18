{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299429867", "html_url": "https://github.com/tensorflow/tensorflow/issues/9664#issuecomment-299429867", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9664", "id": 299429867, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTQyOTg2Nw==", "user": {"login": "hicham-eyeem", "id": 24877550, "node_id": "MDQ6VXNlcjI0ODc3NTUw", "avatar_url": "https://avatars1.githubusercontent.com/u/24877550?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hicham-eyeem", "html_url": "https://github.com/hicham-eyeem", "followers_url": "https://api.github.com/users/hicham-eyeem/followers", "following_url": "https://api.github.com/users/hicham-eyeem/following{/other_user}", "gists_url": "https://api.github.com/users/hicham-eyeem/gists{/gist_id}", "starred_url": "https://api.github.com/users/hicham-eyeem/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hicham-eyeem/subscriptions", "organizations_url": "https://api.github.com/users/hicham-eyeem/orgs", "repos_url": "https://api.github.com/users/hicham-eyeem/repos", "events_url": "https://api.github.com/users/hicham-eyeem/events{/privacy}", "received_events_url": "https://api.github.com/users/hicham-eyeem/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-05T10:14:58Z", "updated_at": "2017-05-05T10:14:58Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a> thank you for the feedback. I am familiar with the 8bit quantization tool. Unfortunately, it results in a major drop in performance in my case. Probably because the network uses a custom activation (not ReLU, but something similar to ELU) while the quantization tool is (I believe) optimized for the ReLU activation. Also, I would like to use a higher precision than 8-bit for my problem, hence my interest in the half-precision support.<br>\nBut since Eigen supports half-precision, I believe half-precision support would be relatively easier to integrate than 8bit quantization since it does not require any fancy op mapping.</p>", "body_text": "@andrewharp thank you for the feedback. I am familiar with the 8bit quantization tool. Unfortunately, it results in a major drop in performance in my case. Probably because the network uses a custom activation (not ReLU, but something similar to ELU) while the quantization tool is (I believe) optimized for the ReLU activation. Also, I would like to use a higher precision than 8-bit for my problem, hence my interest in the half-precision support.\nBut since Eigen supports half-precision, I believe half-precision support would be relatively easier to integrate than 8bit quantization since it does not require any fancy op mapping.", "body": "@andrewharp thank you for the feedback. I am familiar with the 8bit quantization tool. Unfortunately, it results in a major drop in performance in my case. Probably because the network uses a custom activation (not ReLU, but something similar to ELU) while the quantization tool is (I believe) optimized for the ReLU activation. Also, I would like to use a higher precision than 8-bit for my problem, hence my interest in the half-precision support.\r\nBut since Eigen supports half-precision, I believe half-precision support would be relatively easier to integrate than 8bit quantization since it does not require any fancy op mapping. "}