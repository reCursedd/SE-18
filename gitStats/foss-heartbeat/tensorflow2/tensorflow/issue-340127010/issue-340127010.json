{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20687", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20687/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20687/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20687/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20687", "id": 340127010, "node_id": "MDU6SXNzdWUzNDAxMjcwMTA=", "number": 20687, "title": "Speeding up TF custom ops on GPU", "user": {"login": "badreddine1758", "id": 32060704, "node_id": "MDQ6VXNlcjMyMDYwNzA0", "avatar_url": "https://avatars1.githubusercontent.com/u/32060704?v=4", "gravatar_id": "", "url": "https://api.github.com/users/badreddine1758", "html_url": "https://github.com/badreddine1758", "followers_url": "https://api.github.com/users/badreddine1758/followers", "following_url": "https://api.github.com/users/badreddine1758/following{/other_user}", "gists_url": "https://api.github.com/users/badreddine1758/gists{/gist_id}", "starred_url": "https://api.github.com/users/badreddine1758/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/badreddine1758/subscriptions", "organizations_url": "https://api.github.com/users/badreddine1758/orgs", "repos_url": "https://api.github.com/users/badreddine1758/repos", "events_url": "https://api.github.com/users/badreddine1758/events{/privacy}", "received_events_url": "https://api.github.com/users/badreddine1758/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-11T07:41:46Z", "updated_at": "2018-11-15T19:02:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I have created some TF custom Ops [which you can think of as matmul or conv operations], as per TF's <a href=\"https://www.tensorflow.org/extend/adding_an_op\" rel=\"nofollow\">tutorial</a>, and would like to speed them up. I have used AWS' P3 instances (NVIDIA V100 GPU) to run my Ops. Depending on the size of the model, my GPU ops are between 1.5x and 5x slower than TF functions.</p>\n<p>I have seen that TF's implementation of various operations (such as Con2D, MatMul, Pooling etc) use DeviceMemory objects to encapsulate tensor data to be passed to the target device (GPU), then call different wrappers that end up launching Kernels using \"<a href=\"https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/stream_executor/stream_executor_pimpl.h#L729\">Stream::ThenLaunch</a>\".</p>\n<p>A different type of implementation <a href=\"https://github.com/openai/blocksparse\">(OpenAI's block sparse conv2d)</a>, choses to launch the Kernel using cuLaunchKernel after they allocate CUdeviceptr to the different Kernel arguments.</p>\n<p>In my implementation, when I register my TF Ops, the Input/Output tensors are presumably on the device, as I do not set them to be located on the Host(is this correct?). Is TF copying tensors back and forth, between host and GPU after each OP?</p>\n<p>Would it make any difference, in terms of speed, if the Input/Output tensors are set on the Host then copied to the device (cudaMalloc/cudaMemCpy) used by GPU Kernels then copy the results to the Host?</p>\n<p>Would my Kernels run any faster if I were to use TF's approach, to use the DeviceMemory class for my arguments and the Stream::ThenLaunch method to launch the Kernel, or OpenAI's CUdeviceptr and cuLaunchKernel approach?</p>\n<p>Thanks</p>", "body_text": "I have created some TF custom Ops [which you can think of as matmul or conv operations], as per TF's tutorial, and would like to speed them up. I have used AWS' P3 instances (NVIDIA V100 GPU) to run my Ops. Depending on the size of the model, my GPU ops are between 1.5x and 5x slower than TF functions.\nI have seen that TF's implementation of various operations (such as Con2D, MatMul, Pooling etc) use DeviceMemory objects to encapsulate tensor data to be passed to the target device (GPU), then call different wrappers that end up launching Kernels using \"Stream::ThenLaunch\".\nA different type of implementation (OpenAI's block sparse conv2d), choses to launch the Kernel using cuLaunchKernel after they allocate CUdeviceptr to the different Kernel arguments.\nIn my implementation, when I register my TF Ops, the Input/Output tensors are presumably on the device, as I do not set them to be located on the Host(is this correct?). Is TF copying tensors back and forth, between host and GPU after each OP?\nWould it make any difference, in terms of speed, if the Input/Output tensors are set on the Host then copied to the device (cudaMalloc/cudaMemCpy) used by GPU Kernels then copy the results to the Host?\nWould my Kernels run any faster if I were to use TF's approach, to use the DeviceMemory class for my arguments and the Stream::ThenLaunch method to launch the Kernel, or OpenAI's CUdeviceptr and cuLaunchKernel approach?\nThanks", "body": "I have created some TF custom Ops [which you can think of as matmul or conv operations], as per TF's [tutorial](https://www.tensorflow.org/extend/adding_an_op), and would like to speed them up. I have used AWS' P3 instances (NVIDIA V100 GPU) to run my Ops. Depending on the size of the model, my GPU ops are between 1.5x and 5x slower than TF functions.\r\n\r\nI have seen that TF's implementation of various operations (such as Con2D, MatMul, Pooling etc) use DeviceMemory objects to encapsulate tensor data to be passed to the target device (GPU), then call different wrappers that end up launching Kernels using \"[Stream::ThenLaunch](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/stream_executor/stream_executor_pimpl.h#L729)\".\r\n\r\nA different type of implementation [(OpenAI's block sparse conv2d)](https://github.com/openai/blocksparse), choses to launch the Kernel using cuLaunchKernel after they allocate CUdeviceptr to the different Kernel arguments.\r\n\r\nIn my implementation, when I register my TF Ops, the Input/Output tensors are presumably on the device, as I do not set them to be located on the Host(is this correct?). Is TF copying tensors back and forth, between host and GPU after each OP?\r\n\r\nWould it make any difference, in terms of speed, if the Input/Output tensors are set on the Host then copied to the device (cudaMalloc/cudaMemCpy) used by GPU Kernels then copy the results to the Host?\r\n\r\nWould my Kernels run any faster if I were to use TF's approach, to use the DeviceMemory class for my arguments and the Stream::ThenLaunch method to launch the Kernel, or OpenAI's CUdeviceptr and cuLaunchKernel approach?\r\n\r\nThanks"}