{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404429801", "html_url": "https://github.com/tensorflow/tensorflow/issues/20687#issuecomment-404429801", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20687", "id": 404429801, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDQyOTgwMQ==", "user": {"login": "xysmlx", "id": 7117752, "node_id": "MDQ6VXNlcjcxMTc3NTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/7117752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xysmlx", "html_url": "https://github.com/xysmlx", "followers_url": "https://api.github.com/users/xysmlx/followers", "following_url": "https://api.github.com/users/xysmlx/following{/other_user}", "gists_url": "https://api.github.com/users/xysmlx/gists{/gist_id}", "starred_url": "https://api.github.com/users/xysmlx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xysmlx/subscriptions", "organizations_url": "https://api.github.com/users/xysmlx/orgs", "repos_url": "https://api.github.com/users/xysmlx/repos", "events_url": "https://api.github.com/users/xysmlx/events{/privacy}", "received_events_url": "https://api.github.com/users/xysmlx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T08:17:50Z", "updated_at": "2018-07-12T08:17:50Z", "author_association": "NONE", "body_html": "<p>In TensorFlow's abstraction, when the input tensor is not in the operator's device, there will be a data transfer operation between two devices. These data transfer operations are automatically added when the TensorFlow generates the dataflow graph from your source code. The data transfer operations are executed in another stream to overlap computation and I/O for GPUs.</p>\n<p>The Stream::ThenLaunch function is a high level abstraction of CUDA launch APIs. There is no difference between these two approaches.</p>\n<p>I think the way to speedup your custom operator is to optimize your GPU kernels. You can use nvprof to profile your implementation to find the bottleneck.</p>\n<p>The GEMM and CONV kernels in TensorFlow are from cuBLAS and cuDNN. You can also compare them with your kernels.</p>", "body_text": "In TensorFlow's abstraction, when the input tensor is not in the operator's device, there will be a data transfer operation between two devices. These data transfer operations are automatically added when the TensorFlow generates the dataflow graph from your source code. The data transfer operations are executed in another stream to overlap computation and I/O for GPUs.\nThe Stream::ThenLaunch function is a high level abstraction of CUDA launch APIs. There is no difference between these two approaches.\nI think the way to speedup your custom operator is to optimize your GPU kernels. You can use nvprof to profile your implementation to find the bottleneck.\nThe GEMM and CONV kernels in TensorFlow are from cuBLAS and cuDNN. You can also compare them with your kernels.", "body": "In TensorFlow's abstraction, when the input tensor is not in the operator's device, there will be a data transfer operation between two devices. These data transfer operations are automatically added when the TensorFlow generates the dataflow graph from your source code. The data transfer operations are executed in another stream to overlap computation and I/O for GPUs.\r\n\r\nThe Stream::ThenLaunch function is a high level abstraction of CUDA launch APIs. There is no difference between these two approaches.\r\n\r\nI think the way to speedup your custom operator is to optimize your GPU kernels. You can use nvprof to profile your implementation to find the bottleneck.\r\n\r\nThe GEMM and CONV kernels in TensorFlow are from cuBLAS and cuDNN. You can also compare them with your kernels."}