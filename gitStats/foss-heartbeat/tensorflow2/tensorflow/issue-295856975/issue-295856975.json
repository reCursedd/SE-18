{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16889", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16889/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16889/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16889/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16889", "id": 295856975, "node_id": "MDU6SXNzdWUyOTU4NTY5NzU=", "number": 16889, "title": "Problems Getting TensorFlow to behave Deterministically", "user": {"login": "rundembear", "id": 7693798, "node_id": "MDQ6VXNlcjc2OTM3OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7693798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rundembear", "html_url": "https://github.com/rundembear", "followers_url": "https://api.github.com/users/rundembear/followers", "following_url": "https://api.github.com/users/rundembear/following{/other_user}", "gists_url": "https://api.github.com/users/rundembear/gists{/gist_id}", "starred_url": "https://api.github.com/users/rundembear/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rundembear/subscriptions", "organizations_url": "https://api.github.com/users/rundembear/orgs", "repos_url": "https://api.github.com/users/rundembear/repos", "events_url": "https://api.github.com/users/rundembear/events{/privacy}", "received_events_url": "https://api.github.com/users/rundembear/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-02-09T12:36:18Z", "updated_at": "2018-11-22T18:54:04Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n</ul>\n<p>Yes, the code I am working with is company proprietary. If you need an example I will need to try to extract something that I can share.</p>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n</ul>\n<p>Mac OS 10.13.2</p>\n<ul>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n</ul>\n<p>binary</p>\n<ul>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n</ul>\n<p>v1.3.0-rc2-20-g0787eee 1.3.0</p>\n<ul>\n<li><strong>Python version</strong>:</li>\n</ul>\n<p>3.6.4</p>\n<ul>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n</ul>\n<p>Running on CPU.</p>\n<ul>\n<li>\n<p><strong>GPU model and memory</strong>:</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:</p>\n</li>\n</ul>\n<p>Please see description of problem. I don't currently have an example I can give you but I am hoping that the information I am providing is enough to get at least some idea about what is going on.</p>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<p>I am having a major struggle trying to get TensorFlow to behave in a deterministic manner. I started out trying to compare two different versions of my source code, but the numeric differences from one execution to the next were large enough to make comparisons problematic.</p>\n<p>So I went to trying to run exactly the same code in two different shells. And this is where things got weird.</p>\n<p>I went through the code, and everywhere where TensorFlow is imported, everywhere I did operations using the form \"with graph as default:\", and prior to each call to run I added in a call to     tf.set_random_seed(42) [and yes, I used 42 in all of the calls].</p>\n<p>I also placed breakpoints before anything in my code which was a tf call with 'random' in the name (I even searched the docs to see if there were random calls that didn't have random in the name, I found a couple but I am not using them). In all of the random calls that I am using I added in a seed=42 argument. So as far as I can tell I have covered all of the bases.</p>\n<p>So what happens? I run the code in one shell, get a set of numbers. Run code in a second shell, get identical numbers. Good. Run the code a few more times, keep getting the same numbers. Better.<br>\nRun the code again, get completely different numbers!!! If I keep running the code in the different shells sometimes I get the same numbers from execution to execution. Sometimes I get different numbers. Sometimes it goes on to completely new numbers. Sometimes it goes back to an older set of numbers.</p>\n<p>So I am getting something that is consistent enough that it feels like I haven't missed any places where I need to set the seed, but not consistent enough to say that it is deterministic.</p>\n<p>I really want to be able to run the code over and over again and get exactly the same numeric result. Is this an unreasonable expectation?</p>\n<p>Can anyone offer an explanation as to why the code would generate exactly the same result (and I'm looking down to 8 decimal places so this is fairly precise) five times in a row, and then switch to a completely different result and then repeat that result multiple times in a row?</p>\n<p>I can certainly try to reduce this into an example I can share (the model I am using is a toy model we use for unit testing). I wanted to see if anyone had any insight into the general problem before I put the time into that.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p>I will include two examples of the numeric output I am getting. This data is part of the returned result from Session.run().</p>\n<p>Example A:</p>\n<p>out_node_weights [[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184<br>\n9.10322666  9.09527016  9.10811996  9.09029293]<br>\n[ 9.34178257  9.34915924  9.34216785  9.34740925  9.32617569  9.34794617<br>\n9.34459496  9.35203457  9.34202194  9.34494209]<br>\n[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184<br>\n9.10322666  9.09527016  9.10811996  9.09029293]<br>\n[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184<br>\n9.10322666  9.09527016  9.10811996  9.09029293]<br>\n[ 9.59115124  9.59841442  9.59205914  9.59892082  9.58935356  9.59801197<br>\n9.59655094  9.5970974   9.5989027   9.59136868]<br>\n[ 9.39457417  9.39968395  9.39309788  9.4028101   9.40468502  9.39697838<br>\n9.39671326  9.39805126  9.39696026  9.39565849]]</p>\n<p>Example B:</p>\n<p>out_node_weights [[ 9.04592514  9.05451775  9.05065632  9.05263042  9.06681919  9.05208111<br>\n9.05834484  9.05059719  9.06315136  9.04525948]<br>\n[ 9.25311852  9.26151276  9.25516319  9.25806046  9.23816681  9.26067734<br>\n9.25860214  9.26416397  9.25700569  9.25596619]<br>\n[ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269<br>\n9.27913475  9.27859116  9.28033924  9.27534485]<br>\n[ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269<br>\n9.27913475  9.27859116  9.28033924  9.27534485]<br>\n[ 9.50198936  9.51004601  9.50419044  9.50922203  9.50067139  9.50993729<br>\n9.50946426  9.5085659   9.51255226  9.50196838]<br>\n[ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269<br>\n9.27913475  9.27859116  9.28033924  9.27534485]]</p>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nYes, the code I am working with is company proprietary. If you need an example I will need to try to extract something that I can share.\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\nMac OS 10.13.2\n\nTensorFlow installed from (source or binary):\n\nbinary\n\nTensorFlow version (use command below):\n\nv1.3.0-rc2-20-g0787eee 1.3.0\n\nPython version:\n\n3.6.4\n\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\n\nRunning on CPU.\n\n\nGPU model and memory:\n\n\nExact command to reproduce:\n\n\nPlease see description of problem. I don't currently have an example I can give you but I am hoping that the information I am providing is enough to get at least some idea about what is going on.\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nI am having a major struggle trying to get TensorFlow to behave in a deterministic manner. I started out trying to compare two different versions of my source code, but the numeric differences from one execution to the next were large enough to make comparisons problematic.\nSo I went to trying to run exactly the same code in two different shells. And this is where things got weird.\nI went through the code, and everywhere where TensorFlow is imported, everywhere I did operations using the form \"with graph as default:\", and prior to each call to run I added in a call to     tf.set_random_seed(42) [and yes, I used 42 in all of the calls].\nI also placed breakpoints before anything in my code which was a tf call with 'random' in the name (I even searched the docs to see if there were random calls that didn't have random in the name, I found a couple but I am not using them). In all of the random calls that I am using I added in a seed=42 argument. So as far as I can tell I have covered all of the bases.\nSo what happens? I run the code in one shell, get a set of numbers. Run code in a second shell, get identical numbers. Good. Run the code a few more times, keep getting the same numbers. Better.\nRun the code again, get completely different numbers!!! If I keep running the code in the different shells sometimes I get the same numbers from execution to execution. Sometimes I get different numbers. Sometimes it goes on to completely new numbers. Sometimes it goes back to an older set of numbers.\nSo I am getting something that is consistent enough that it feels like I haven't missed any places where I need to set the seed, but not consistent enough to say that it is deterministic.\nI really want to be able to run the code over and over again and get exactly the same numeric result. Is this an unreasonable expectation?\nCan anyone offer an explanation as to why the code would generate exactly the same result (and I'm looking down to 8 decimal places so this is fairly precise) five times in a row, and then switch to a completely different result and then repeat that result multiple times in a row?\nI can certainly try to reduce this into an example I can share (the model I am using is a toy model we use for unit testing). I wanted to see if anyone had any insight into the general problem before I put the time into that.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nI will include two examples of the numeric output I am getting. This data is part of the returned result from Session.run().\nExample A:\nout_node_weights [[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\n9.10322666  9.09527016  9.10811996  9.09029293]\n[ 9.34178257  9.34915924  9.34216785  9.34740925  9.32617569  9.34794617\n9.34459496  9.35203457  9.34202194  9.34494209]\n[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\n9.10322666  9.09527016  9.10811996  9.09029293]\n[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\n9.10322666  9.09527016  9.10811996  9.09029293]\n[ 9.59115124  9.59841442  9.59205914  9.59892082  9.58935356  9.59801197\n9.59655094  9.5970974   9.5989027   9.59136868]\n[ 9.39457417  9.39968395  9.39309788  9.4028101   9.40468502  9.39697838\n9.39671326  9.39805126  9.39696026  9.39565849]]\nExample B:\nout_node_weights [[ 9.04592514  9.05451775  9.05065632  9.05263042  9.06681919  9.05208111\n9.05834484  9.05059719  9.06315136  9.04525948]\n[ 9.25311852  9.26151276  9.25516319  9.25806046  9.23816681  9.26067734\n9.25860214  9.26416397  9.25700569  9.25596619]\n[ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\n9.27913475  9.27859116  9.28033924  9.27534485]\n[ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\n9.27913475  9.27859116  9.28033924  9.27534485]\n[ 9.50198936  9.51004601  9.50419044  9.50922203  9.50067139  9.50993729\n9.50946426  9.5085659   9.51255226  9.50196838]\n[ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\n9.27913475  9.27859116  9.28033924  9.27534485]]", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes, the code I am working with is company proprietary. If you need an example I will need to try to extract something that I can share. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nMac OS 10.13.2\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\n\r\n- **Python version**: \r\n\r\n3.6.4\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n\r\nRunning on CPU.\r\n\r\n- **GPU model and memory**:\r\n\r\n- **Exact command to reproduce**:\r\n\r\nPlease see description of problem. I don't currently have an example I can give you but I am hoping that the information I am providing is enough to get at least some idea about what is going on.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI am having a major struggle trying to get TensorFlow to behave in a deterministic manner. I started out trying to compare two different versions of my source code, but the numeric differences from one execution to the next were large enough to make comparisons problematic.\r\n\r\nSo I went to trying to run exactly the same code in two different shells. And this is where things got weird.\r\n\r\nI went through the code, and everywhere where TensorFlow is imported, everywhere I did operations using the form \"with graph as default:\", and prior to each call to run I added in a call to     tf.set_random_seed(42) [and yes, I used 42 in all of the calls].\r\n\r\nI also placed breakpoints before anything in my code which was a tf call with 'random' in the name (I even searched the docs to see if there were random calls that didn't have random in the name, I found a couple but I am not using them). In all of the random calls that I am using I added in a seed=42 argument. So as far as I can tell I have covered all of the bases.\r\n\r\nSo what happens? I run the code in one shell, get a set of numbers. Run code in a second shell, get identical numbers. Good. Run the code a few more times, keep getting the same numbers. Better.\r\nRun the code again, get completely different numbers!!! If I keep running the code in the different shells sometimes I get the same numbers from execution to execution. Sometimes I get different numbers. Sometimes it goes on to completely new numbers. Sometimes it goes back to an older set of numbers.\r\n\r\nSo I am getting something that is consistent enough that it feels like I haven't missed any places where I need to set the seed, but not consistent enough to say that it is deterministic.\r\n\r\nI really want to be able to run the code over and over again and get exactly the same numeric result. Is this an unreasonable expectation? \r\n\r\nCan anyone offer an explanation as to why the code would generate exactly the same result (and I'm looking down to 8 decimal places so this is fairly precise) five times in a row, and then switch to a completely different result and then repeat that result multiple times in a row?\r\n\r\nI can certainly try to reduce this into an example I can share (the model I am using is a toy model we use for unit testing). I wanted to see if anyone had any insight into the general problem before I put the time into that.\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI will include two examples of the numeric output I am getting. This data is part of the returned result from Session.run().\r\n\r\nExample A:\r\n\r\nout_node_weights [[ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\r\n   9.10322666  9.09527016  9.10811996  9.09029293]\r\n [ 9.34178257  9.34915924  9.34216785  9.34740925  9.32617569  9.34794617\r\n   9.34459496  9.35203457  9.34202194  9.34494209]\r\n [ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\r\n   9.10322666  9.09527016  9.10811996  9.09029293]\r\n [ 9.09113979  9.09934711  9.09535408  9.09768963  9.11262989  9.09756184\r\n   9.10322666  9.09527016  9.10811996  9.09029293]\r\n [ 9.59115124  9.59841442  9.59205914  9.59892082  9.58935356  9.59801197\r\n   9.59655094  9.5970974   9.5989027   9.59136868]\r\n [ 9.39457417  9.39968395  9.39309788  9.4028101   9.40468502  9.39697838\r\n   9.39671326  9.39805126  9.39696026  9.39565849]]\r\n\r\nExample B:\r\n\r\nout_node_weights [[ 9.04592514  9.05451775  9.05065632  9.05263042  9.06681919  9.05208111\r\n   9.05834484  9.05059719  9.06315136  9.04525948]\r\n [ 9.25311852  9.26151276  9.25516319  9.25806046  9.23816681  9.26067734\r\n   9.25860214  9.26416397  9.25700569  9.25596619]\r\n [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\r\n   9.27913475  9.27859116  9.28033924  9.27534485]\r\n [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\r\n   9.27913475  9.27859116  9.28033924  9.27534485]\r\n [ 9.50198936  9.51004601  9.50419044  9.50922203  9.50067139  9.50993729\r\n   9.50946426  9.5085659   9.51255226  9.50196838]\r\n [ 9.27453995  9.28063679  9.27472687  9.28252983  9.28667641  9.27761269\r\n   9.27913475  9.27859116  9.28033924  9.27534485]]"}