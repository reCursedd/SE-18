{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/332693487", "html_url": "https://github.com/tensorflow/tensorflow/pull/11922#issuecomment-332693487", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11922", "id": 332693487, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjY5MzQ4Nw==", "user": {"login": "liuzjmike", "id": 15717157, "node_id": "MDQ6VXNlcjE1NzE3MTU3", "avatar_url": "https://avatars1.githubusercontent.com/u/15717157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liuzjmike", "html_url": "https://github.com/liuzjmike", "followers_url": "https://api.github.com/users/liuzjmike/followers", "following_url": "https://api.github.com/users/liuzjmike/following{/other_user}", "gists_url": "https://api.github.com/users/liuzjmike/gists{/gist_id}", "starred_url": "https://api.github.com/users/liuzjmike/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liuzjmike/subscriptions", "organizations_url": "https://api.github.com/users/liuzjmike/orgs", "repos_url": "https://api.github.com/users/liuzjmike/repos", "events_url": "https://api.github.com/users/liuzjmike/events{/privacy}", "received_events_url": "https://api.github.com/users/liuzjmike/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-28T00:33:09Z", "updated_at": "2017-09-28T00:33:09Z", "author_association": "NONE", "body_html": "<p>Thank you for the tip. I tried running with</p>\n<pre><code>bazel run //tensorflow/python:batch_norm_benchmark -- --benchmarks=BatchNormBenchmark.benchmark_batch_norm\n</code></pre>\n<p>and the core dump did not happen. It might be due to some mechanism of bazel.</p>\n<p>Yes it looks like the proposed impl is sadly slower than the original one, maybe due to the original impl optimizing on the calculation dimension. Maybe we can give users an option in tf.contrib.layer.batch_norm to use the proposed impl if they want to do the per-example gradient operation. If you don't want to make that change, I'm ok with closing this PR.</p>", "body_text": "Thank you for the tip. I tried running with\nbazel run //tensorflow/python:batch_norm_benchmark -- --benchmarks=BatchNormBenchmark.benchmark_batch_norm\n\nand the core dump did not happen. It might be due to some mechanism of bazel.\nYes it looks like the proposed impl is sadly slower than the original one, maybe due to the original impl optimizing on the calculation dimension. Maybe we can give users an option in tf.contrib.layer.batch_norm to use the proposed impl if they want to do the per-example gradient operation. If you don't want to make that change, I'm ok with closing this PR.", "body": "Thank you for the tip. I tried running with\r\n```\r\nbazel run //tensorflow/python:batch_norm_benchmark -- --benchmarks=BatchNormBenchmark.benchmark_batch_norm\r\n```\r\nand the core dump did not happen. It might be due to some mechanism of bazel.\r\n\r\nYes it looks like the proposed impl is sadly slower than the original one, maybe due to the original impl optimizing on the calculation dimension. Maybe we can give users an option in tf.contrib.layer.batch_norm to use the proposed impl if they want to do the per-example gradient operation. If you don't want to make that change, I'm ok with closing this PR."}