{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8656", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8656/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8656/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8656/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8656", "id": 216412936, "node_id": "MDU6SXNzdWUyMTY0MTI5MzY=", "number": 8656, "title": "Inconsistent behaviour between CPU and GPU gradient step operation", "user": {"login": "matthiasreisser", "id": 8858505, "node_id": "MDQ6VXNlcjg4NTg1MDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8858505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthiasreisser", "html_url": "https://github.com/matthiasreisser", "followers_url": "https://api.github.com/users/matthiasreisser/followers", "following_url": "https://api.github.com/users/matthiasreisser/following{/other_user}", "gists_url": "https://api.github.com/users/matthiasreisser/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthiasreisser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthiasreisser/subscriptions", "organizations_url": "https://api.github.com/users/matthiasreisser/orgs", "repos_url": "https://api.github.com/users/matthiasreisser/repos", "events_url": "https://api.github.com/users/matthiasreisser/events{/privacy}", "received_events_url": "https://api.github.com/users/matthiasreisser/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-23T12:34:40Z", "updated_at": "2017-03-26T11:29:19Z", "closed_at": "2017-03-25T16:39:36Z", "author_association": "NONE", "body_html": "<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None</p>\n<h3>Environment info</h3>\n<p>Operating System: Linux Mint 17.2 Rafaela</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\nls -l /usr/local/cuda/lib64/libcud*<br>\n/usr/local/cuda/lib64/libcudadevrt.a<br>\n/usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0<br>\n/usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.61<br>\n/usr/local/cuda/lib64/libcudart.so.8.0.61<br>\n/usr/local/cuda/lib64/libcudart_static.a<br>\n/usr/local/cuda/lib64/libcudnn.so<br>\n/usr/local/cuda/lib64/libcudnn.so.5<br>\n/usr/local/cuda/lib64/libcudnn.so.5.1.10<br>\n/usr/local/cuda/lib64/libcudnn_static.a</p>\n<ol start=\"2\">\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.<br>\n1.1.0-rc0<br>\nThis bug appeared also on the current TF 1.0 Release when installed via pip install tensorflow-gpu</li>\n</ol>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\ngit rev-parse HEAD: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/49380d638bdc2983722c9a2831ca74770dc6ba43/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/49380d638bdc2983722c9a2831ca74770dc6ba43\"><tt>49380d6</tt></a></li>\n<li>The output of <code>bazel version</code><br>\nBuild label: 0.4.5</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport tensorflow as tf\na = tf.Variable(1.0)\nloss = (a-2.0)**2\noptimizer = tf.train.GradientDescentOptimizer(1.0)\ntrain_op = optimizer.minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nprint(sess.run([train_op,a]))\nprint(sess.run(a))\n</code></pre>\n<p>The two print statements evaluate to</p>\n<pre><code>[None, 3.0]\n3.0\n</code></pre>\n<p>When allowing GPU computation to happen by commenting out the second line above, the two print statements evaluate to:</p>\n<pre><code>[None, 1.0]\n3.0\n</code></pre>\n<p>So apparently when using the GPU, the Variable <code>a</code> is evaluated before the gradient op is executed - and the other way around on CPU. I am not entirely sure what the desired behaviour is supposed to be, but I'm pretty sure they should not be inconsistent.</p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>A few things I have observed:</p>\n<pre><code>import os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport tensorflow as tf\n\nwith tf.device(\"/cpu:0\"):\n    a = tf.Variable(1.0)\nloss = (a-2.0)**2\noptimizer = tf.train.GradientDescentOptimizer(1.0)\ntrain_op = optimizer.minimize(loss)\ninit_op = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init_op)\nprint(sess.run([train_op,a]))\nprint(sess.run(a))\n</code></pre>\n<p>evaluates to</p>\n<pre><code>[None, 3.0]\n3.0\n</code></pre>\n<p>The following code</p>\n<pre><code>import os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport tensorflow as tf\n\na = tf.Variable(1.0)\nloss = (a-2.0)**2\noptimizer = tf.train.GradientDescentOptimizer(1.0)\ntrain_op = optimizer.minimize(loss)\ninit_op = tf.global_variables_initializer()\nwith tf.control_dependencies([train_op]):\n    a = tf.identity(a)\nsess = tf.Session()\nsess.run(init_op)\nprint(sess.run(a))\n</code></pre>\n<p>evaluates to<br>\n<code>3.0</code>. This seems to \"enforce\" the behaviour of CPU-only computation when using the GPU.</p>\n<p>Also the initial example has been run on three different machines, all with the same TitanX GPU Model.</p>\n<p>What am I missing? Any help would be greatly appreciated.</p>\n<p>Matthias</p>", "body_text": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone\nEnvironment info\nOperating System: Linux Mint 17.2 Rafaela\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nls -l /usr/local/cuda/lib64/libcud*\n/usr/local/cuda/lib64/libcudadevrt.a\n/usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\n/usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\n/usr/local/cuda/lib64/libcudart.so.8.0.61\n/usr/local/cuda/lib64/libcudart_static.a\n/usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudnn.so.5.1.10\n/usr/local/cuda/lib64/libcudnn_static.a\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n1.1.0-rc0\nThis bug appeared also on the current TF 1.0 Release when installed via pip install tensorflow-gpu\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\ngit rev-parse HEAD: 49380d6\nThe output of bazel version\nBuild label: 0.4.5\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport tensorflow as tf\na = tf.Variable(1.0)\nloss = (a-2.0)**2\noptimizer = tf.train.GradientDescentOptimizer(1.0)\ntrain_op = optimizer.minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nprint(sess.run([train_op,a]))\nprint(sess.run(a))\n\nThe two print statements evaluate to\n[None, 3.0]\n3.0\n\nWhen allowing GPU computation to happen by commenting out the second line above, the two print statements evaluate to:\n[None, 1.0]\n3.0\n\nSo apparently when using the GPU, the Variable a is evaluated before the gradient op is executed - and the other way around on CPU. I am not entirely sure what the desired behaviour is supposed to be, but I'm pretty sure they should not be inconsistent.\nWhat other attempted solutions have you tried?\nA few things I have observed:\nimport os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport tensorflow as tf\n\nwith tf.device(\"/cpu:0\"):\n    a = tf.Variable(1.0)\nloss = (a-2.0)**2\noptimizer = tf.train.GradientDescentOptimizer(1.0)\ntrain_op = optimizer.minimize(loss)\ninit_op = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init_op)\nprint(sess.run([train_op,a]))\nprint(sess.run(a))\n\nevaluates to\n[None, 3.0]\n3.0\n\nThe following code\nimport os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport tensorflow as tf\n\na = tf.Variable(1.0)\nloss = (a-2.0)**2\noptimizer = tf.train.GradientDescentOptimizer(1.0)\ntrain_op = optimizer.minimize(loss)\ninit_op = tf.global_variables_initializer()\nwith tf.control_dependencies([train_op]):\n    a = tf.identity(a)\nsess = tf.Session()\nsess.run(init_op)\nprint(sess.run(a))\n\nevaluates to\n3.0. This seems to \"enforce\" the behaviour of CPU-only computation when using the GPU.\nAlso the initial example has been run on three different machines, all with the same TitanX GPU Model.\nWhat am I missing? Any help would be greatly appreciated.\nMatthias", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: Linux Mint 17.2 Rafaela\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nls -l /usr/local/cuda/lib64/libcud*\r\n/usr/local/cuda/lib64/libcudadevrt.a\r\n/usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\n/usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda/lib64/libcudart_static.a\r\n/usr/local/cuda/lib64/libcudnn.so\r\n/usr/local/cuda/lib64/libcudnn.so.5\r\n/usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.1.0-rc0\r\nThis bug appeared also on the current TF 1.0 Release when installed via pip install tensorflow-gpu\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\ngit rev-parse HEAD: 49380d638bdc2983722c9a2831ca74770dc6ba43\r\n2. The output of `bazel version`\r\nBuild label: 0.4.5\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport tensorflow as tf\r\na = tf.Variable(1.0)\r\nloss = (a-2.0)**2\r\noptimizer = tf.train.GradientDescentOptimizer(1.0)\r\ntrain_op = optimizer.minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run([train_op,a]))\r\nprint(sess.run(a))\r\n```\r\nThe two print statements evaluate to \r\n```\r\n[None, 3.0]\r\n3.0\r\n```\r\n\r\nWhen allowing GPU computation to happen by commenting out the second line above, the two print statements evaluate to:\r\n\r\n```\r\n[None, 1.0]\r\n3.0\r\n```\r\nSo apparently when using the GPU, the Variable `a` is evaluated before the gradient op is executed - and the other way around on CPU. I am not entirely sure what the desired behaviour is supposed to be, but I'm pretty sure they should not be inconsistent. \r\n\r\n### What other attempted solutions have you tried?\r\nA few things I have observed:\r\n```\r\nimport os\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    a = tf.Variable(1.0)\r\nloss = (a-2.0)**2\r\noptimizer = tf.train.GradientDescentOptimizer(1.0)\r\ntrain_op = optimizer.minimize(loss)\r\ninit_op = tf.global_variables_initializer()\r\n\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nprint(sess.run([train_op,a]))\r\nprint(sess.run(a))\r\n```\r\nevaluates to \r\n```\r\n[None, 3.0]\r\n3.0\r\n```\r\n\r\nThe following code\r\n```\r\nimport os\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(1.0)\r\nloss = (a-2.0)**2\r\noptimizer = tf.train.GradientDescentOptimizer(1.0)\r\ntrain_op = optimizer.minimize(loss)\r\ninit_op = tf.global_variables_initializer()\r\nwith tf.control_dependencies([train_op]):\r\n    a = tf.identity(a)\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nprint(sess.run(a))\r\n```\r\nevaluates to \r\n`3.0`. This seems to \"enforce\" the behaviour of CPU-only computation when using the GPU.\r\n\r\n\r\nAlso the initial example has been run on three different machines, all with the same TitanX GPU Model.\r\n\r\nWhat am I missing? Any help would be greatly appreciated.\r\n\r\nMatthias\r\n"}