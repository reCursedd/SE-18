{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422182708", "html_url": "https://github.com/tensorflow/tensorflow/issues/3103#issuecomment-422182708", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3103", "id": 422182708, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjE4MjcwOA==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-17T21:46:45Z", "updated_at": "2018-09-17T21:46:45Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a><br>\nI just re-ran my initial examples on TF 1.5.0 and found that that <code>reduce_mean</code> call produced consistent results across 10K trials with the mean kernel running on a K80 GPU. This observation agrees with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8398334\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ahmedhosny\">@ahmedhosny</a> 's observation.</p>\n<p>I dug through the source a little bit and I think the reduction logic happens in <a href=\"https://github.com/eigenteam/eigen-git-mirror/blob/master/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h\">https://github.com/eigenteam/eigen-git-mirror/blob/master/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h</a> . This (and the <code>TensorReductionGpu.h</code> file) are new since I opened this issue in 2016 and contain reduction tree logic. However, they also still contain atomic floating point adds. I think these are to reduce the values between the different GPU blocks.</p>\n<p>This file has been recently modified by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16907534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rmlarsen\">@rmlarsen</a> . <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16907534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rmlarsen\">@rmlarsen</a> , do you have any insight on how deterministic these reductions are? When is atomic floating point add used?</p>", "body_text": "@yaroslavvb\nI just re-ran my initial examples on TF 1.5.0 and found that that reduce_mean call produced consistent results across 10K trials with the mean kernel running on a K80 GPU. This observation agrees with @ahmedhosny 's observation.\nI dug through the source a little bit and I think the reduction logic happens in https://github.com/eigenteam/eigen-git-mirror/blob/master/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h . This (and the TensorReductionGpu.h file) are new since I opened this issue in 2016 and contain reduction tree logic. However, they also still contain atomic floating point adds. I think these are to reduce the values between the different GPU blocks.\nThis file has been recently modified by @rmlarsen . @rmlarsen , do you have any insight on how deterministic these reductions are? When is atomic floating point add used?", "body": "@yaroslavvb \r\nI just re-ran my initial examples on TF 1.5.0 and found that that `reduce_mean` call produced consistent results across 10K trials with the mean kernel running on a K80 GPU. This observation agrees with @ahmedhosny 's observation.\r\n\r\nI dug through the source a little bit and I think the reduction logic happens in https://github.com/eigenteam/eigen-git-mirror/blob/master/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h . This (and the `TensorReductionGpu.h` file) are new since I opened this issue in 2016 and contain reduction tree logic. However, they also still contain atomic floating point adds. I think these are to reduce the values between the different GPU blocks.\r\n\r\nThis file has been recently modified by @rmlarsen . @rmlarsen , do you have any insight on how deterministic these reductions are? When is atomic floating point add used?"}