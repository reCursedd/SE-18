{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3103", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3103/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3103/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3103/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3103", "id": 162952191, "node_id": "MDU6SXNzdWUxNjI5NTIxOTE=", "number": 3103, "title": "Non-deterministic mean and sum reduction", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-06-29T15:15:06Z", "updated_at": "2018-09-17T21:46:45Z", "closed_at": "2016-07-13T17:33:36Z", "author_association": "NONE", "body_html": "<p>I'm running Tensorflow 0.9.0 installed from wheel on Python 2.7 on a K40 with CUDA 7.0.</p>\n<p>The following test case attempts to minimize the mean of a vector through gradient descent. The script finds that the vectors are equal at all steps, but the means are not. I believe the vectors being equal at all steps is pure numerical luck, since non-deterministic loss likely means non-deterministic gradient which means non-deterministic/reproducible iterative optimization. I've observed cases where training results in different final losses where the only source of non-determinism is from reduce_mean.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nn_dims <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\nn_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\n\nnp.random.seed(<span class=\"pl-c1\">2016</span>)\n\nvec <span class=\"pl-k\">=</span> tf.Variable(np.random.randn(n_dims).astype(np.float32))\nmean <span class=\"pl-k\">=</span> tf.reduce_mean(vec)\n\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.01</span>)\ntrain_step <span class=\"pl-k\">=</span> optimizer.minimize(mean)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">generate</span>():\n    data <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.initialize_all_variables())\n\n        <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(n_steps):\n            _vec, _mean, _ <span class=\"pl-k\">=</span> sess.run([vec, mean, train_step])\n            data.append((_vec, _mean))\n\n    <span class=\"pl-k\">return</span> [np.array([f[i] <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> data]) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">2</span>)]\n\nfirst_vec, first_mean <span class=\"pl-k\">=</span> generate()\nsecond_vec, second_mean <span class=\"pl-k\">=</span> generate()\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>vecs equal:<span class=\"pl-pds\">'</span></span>, np.all(first_vec <span class=\"pl-k\">==</span> second_vec)\n\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>mean equal:<span class=\"pl-pds\">'</span></span>, np.all(first_mean <span class=\"pl-k\">==</span> second_mean)\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>means not equal at idxs:<span class=\"pl-pds\">'</span></span>, np.nonzero(first_mean <span class=\"pl-k\">!=</span> second_mean)[<span class=\"pl-c1\">0</span>]</pre></div>\n<p>Example output:</p>\n<pre><code>vecs equal: True\nmean equal: False\nmeans not equal at idxs: [ 4  5 11 18 34 38 44 49]\n</code></pre>\n<p>From looking through the code, it appears the GPU mean reduction is implemented with GPU sum reduction: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L43\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L43</a><br>\nI've confirmed my test case still triggers when I replace \"reduce_mean\" with \"reduce_sum\".</p>\n<p>The GPU sum reduction appears to be implemented using CUDA's atomicAdd: <a href=\"https://bitbucket.org/eigen/eigen/src/241472d2a52142e23b0b2ba5c301c6c146298fa9/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h?at=default&amp;fileviewer=file-view-default#TensorReductionCuda.h-97\" rel=\"nofollow\">https://bitbucket.org/eigen/eigen/src/241472d2a52142e23b0b2ba5c301c6c146298fa9/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h?at=default&amp;fileviewer=file-view-default#TensorReductionCuda.h-97</a></p>\n<p>Atomic floating point adds on GPU are the problem. Having floating point adds to the same address in an undefined order is inherently non-deterministic due to non-associativity of floating point arithmetic.</p>\n<p>This issue could be solved (and reduction performance improved) by using some sort of reduction tree to reduce within blocks, and then launching a second kernel (or doing some manual block synchronization tricks) to reduce across blocks.</p>", "body_text": "I'm running Tensorflow 0.9.0 installed from wheel on Python 2.7 on a K40 with CUDA 7.0.\nThe following test case attempts to minimize the mean of a vector through gradient descent. The script finds that the vectors are equal at all steps, but the means are not. I believe the vectors being equal at all steps is pure numerical luck, since non-deterministic loss likely means non-deterministic gradient which means non-deterministic/reproducible iterative optimization. I've observed cases where training results in different final losses where the only source of non-determinism is from reduce_mean.\nimport numpy as np\nimport tensorflow as tf\n\nn_dims = 1000\nn_steps = 50\n\nnp.random.seed(2016)\n\nvec = tf.Variable(np.random.randn(n_dims).astype(np.float32))\nmean = tf.reduce_mean(vec)\n\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = optimizer.minimize(mean)\n\ndef generate():\n    data = []\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n\n        for _ in xrange(n_steps):\n            _vec, _mean, _ = sess.run([vec, mean, train_step])\n            data.append((_vec, _mean))\n\n    return [np.array([f[i] for f in data]) for i in xrange(2)]\n\nfirst_vec, first_mean = generate()\nsecond_vec, second_mean = generate()\nprint 'vecs equal:', np.all(first_vec == second_vec)\n\nprint 'mean equal:', np.all(first_mean == second_mean)\nprint 'means not equal at idxs:', np.nonzero(first_mean != second_mean)[0]\nExample output:\nvecs equal: True\nmean equal: False\nmeans not equal at idxs: [ 4  5 11 18 34 38 44 49]\n\nFrom looking through the code, it appears the GPU mean reduction is implemented with GPU sum reduction: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L43\nI've confirmed my test case still triggers when I replace \"reduce_mean\" with \"reduce_sum\".\nThe GPU sum reduction appears to be implemented using CUDA's atomicAdd: https://bitbucket.org/eigen/eigen/src/241472d2a52142e23b0b2ba5c301c6c146298fa9/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h?at=default&fileviewer=file-view-default#TensorReductionCuda.h-97\nAtomic floating point adds on GPU are the problem. Having floating point adds to the same address in an undefined order is inherently non-deterministic due to non-associativity of floating point arithmetic.\nThis issue could be solved (and reduction performance improved) by using some sort of reduction tree to reduce within blocks, and then launching a second kernel (or doing some manual block synchronization tricks) to reduce across blocks.", "body": "I'm running Tensorflow 0.9.0 installed from wheel on Python 2.7 on a K40 with CUDA 7.0.\n\nThe following test case attempts to minimize the mean of a vector through gradient descent. The script finds that the vectors are equal at all steps, but the means are not. I believe the vectors being equal at all steps is pure numerical luck, since non-deterministic loss likely means non-deterministic gradient which means non-deterministic/reproducible iterative optimization. I've observed cases where training results in different final losses where the only source of non-determinism is from reduce_mean.\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nn_dims = 1000\nn_steps = 50\n\nnp.random.seed(2016)\n\nvec = tf.Variable(np.random.randn(n_dims).astype(np.float32))\nmean = tf.reduce_mean(vec)\n\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = optimizer.minimize(mean)\n\ndef generate():\n    data = []\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n\n        for _ in xrange(n_steps):\n            _vec, _mean, _ = sess.run([vec, mean, train_step])\n            data.append((_vec, _mean))\n\n    return [np.array([f[i] for f in data]) for i in xrange(2)]\n\nfirst_vec, first_mean = generate()\nsecond_vec, second_mean = generate()\nprint 'vecs equal:', np.all(first_vec == second_vec)\n\nprint 'mean equal:', np.all(first_mean == second_mean)\nprint 'means not equal at idxs:', np.nonzero(first_mean != second_mean)[0]\n```\n\nExample output:\n\n```\nvecs equal: True\nmean equal: False\nmeans not equal at idxs: [ 4  5 11 18 34 38 44 49]\n```\n\nFrom looking through the code, it appears the GPU mean reduction is implemented with GPU sum reduction: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L43\nI've confirmed my test case still triggers when I replace \"reduce_mean\" with \"reduce_sum\".\n\nThe GPU sum reduction appears to be implemented using CUDA's atomicAdd: https://bitbucket.org/eigen/eigen/src/241472d2a52142e23b0b2ba5c301c6c146298fa9/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h?at=default&fileviewer=file-view-default#TensorReductionCuda.h-97\n\nAtomic floating point adds on GPU are the problem. Having floating point adds to the same address in an undefined order is inherently non-deterministic due to non-associativity of floating point arithmetic.\n\nThis issue could be solved (and reduction performance improved) by using some sort of reduction tree to reduce within blocks, and then launching a second kernel (or doing some manual block synchronization tricks) to reduce across blocks.\n"}