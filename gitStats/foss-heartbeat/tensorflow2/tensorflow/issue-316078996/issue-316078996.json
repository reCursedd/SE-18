{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18711", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18711/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18711/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18711/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18711", "id": 316078996, "node_id": "MDU6SXNzdWUzMTYwNzg5OTY=", "number": 18711, "title": "Suggestion for efficient upsample+conv2d and conv2d+pool", "user": {"login": "alexlee-gk", "id": 839426, "node_id": "MDQ6VXNlcjgzOTQyNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/839426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexlee-gk", "html_url": "https://github.com/alexlee-gk", "followers_url": "https://api.github.com/users/alexlee-gk/followers", "following_url": "https://api.github.com/users/alexlee-gk/following{/other_user}", "gists_url": "https://api.github.com/users/alexlee-gk/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexlee-gk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexlee-gk/subscriptions", "organizations_url": "https://api.github.com/users/alexlee-gk/orgs", "repos_url": "https://api.github.com/users/alexlee-gk/repos", "events_url": "https://api.github.com/users/alexlee-gk/events{/privacy}", "received_events_url": "https://api.github.com/users/alexlee-gk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-04-19T23:50:49Z", "updated_at": "2018-11-20T07:52:40Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.7.0-3-g024aecf414 1.7.0</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: /A</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0/cuDNN 7.0</li>\n<li><strong>GPU model and memory</strong>: Titan Xp 12GB</li>\n<li><strong>Exact command to reproduce</strong>: CUDA_VISIBLE_DEVICES=0 python test_conv_upsample_pool_ops.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>TLDR: upsampling+conv2d (which is very expensive) could be implemented such that it has the same time and memory complexity as strided conv2d_transposed. Similarly for conv2d+average_pooling and strided conv2d. An efficient implementation could be S^2 faster, where S is the stride (typically S=2).</p>\n<p>Strided conv2d_transposed have been traditionally used in decoders for dense prediction problems (e.g. image generation, video prediction, semantic segmentation). However, this op often produces outputs with checkerboard artifacts, e.g. see [1] and papers citing it. An alternative is to use bilinear upsampling followed by a standard convolution (with no strides). This is widely used in several recent works and it mitigates the checkerboard artifacts but at a cost: it's computationally and memory expensive (e.g. see [2]). upsampling+conv2d does S^2 more computation than the strided counterpart, where S is the stride factor. Furthermore, the intermediate tensor after upsampling could be very large if the input has a large number of channels.</p>\n<p>Under certain conditions (which happens to be the most common use case), upsampling+conv2d could be rewritten as convolving the upsampling kernel with the given kernel, followed by conv2d_transposed of the given input and the combined kernels. This follows from commutative and associative properties of linearity in convolutions (taking proper care of flipping the filters so that the ops are actual convolutions).</p>\n<p>A similar reasoning applies for an efficient implementation of conv2d+average_pooling.</p>\n<p>Implementations of the mentioned ops are here (see <code>upsample_conv2d</code> and <code>conv_pool2d</code>): <a href=\"https://github.com/alexlee-gk/video_prediction/blob/master/video_prediction/ops.py\">https://github.com/alexlee-gk/video_prediction/blob/master/video_prediction/ops.py</a></p>\n<p>A script that tests for equivalence and timings is here: <a href=\"https://gist.github.com/alexlee-gk/1ae88125ec38efc48b542a4c0356078f\">https://gist.github.com/alexlee-gk/1ae88125ec38efc48b542a4c0356078f</a></p>\n<p>I report some timings below of hundreds of evaluations. The naive op should be about 4x slower than the strided counterpart since S^2 = 4. In theory, the optimized op should be as fast as the strided counterpart, but my implementation is about 2x slower (but still much faster than the naive version), and I think there is room for improvement.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"right\">upsample + conv2d (optimized)</th>\n<th align=\"right\">upsample + conv2d (naive)</th>\n<th align=\"right\">strided conv2d_transpose</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">forward pass</td>\n<td align=\"right\">11.0s</td>\n<td align=\"right\">16.3s</td>\n<td align=\"right\">4.7s</td>\n</tr>\n<tr>\n<td align=\"left\">forward and backward pass</td>\n<td align=\"right\">6.5s</td>\n<td align=\"right\">17.5s</td>\n<td align=\"right\">2.6s</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"right\">conv2d + pool (optimized)</th>\n<th align=\"right\">conv2d + pool (naive)</th>\n<th align=\"right\">strided conv2d</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">forward pass</td>\n<td align=\"right\">4.1s</td>\n<td align=\"right\">8.2s</td>\n<td align=\"right\">2.6s</td>\n</tr>\n<tr>\n<td align=\"left\">forward and backward pass</td>\n<td align=\"right\">4.1s</td>\n<td align=\"right\">8.4s</td>\n<td align=\"right\">2.6s</td>\n</tr>\n</tbody>\n</table>\n<p>[1] <a href=\"https://distill.pub/2016/deconv-checkerboard\" rel=\"nofollow\">https://distill.pub/2016/deconv-checkerboard</a><br>\n[2] <a href=\"https://arxiv.org/abs/1707.05847\" rel=\"nofollow\">https://arxiv.org/abs/1707.05847</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.7.0-3-g024aecf414 1.7.0\nPython version: 3.5.2\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): /A\nCUDA/cuDNN version: CUDA 9.0/cuDNN 7.0\nGPU model and memory: Titan Xp 12GB\nExact command to reproduce: CUDA_VISIBLE_DEVICES=0 python test_conv_upsample_pool_ops.py\n\nDescribe the problem\nTLDR: upsampling+conv2d (which is very expensive) could be implemented such that it has the same time and memory complexity as strided conv2d_transposed. Similarly for conv2d+average_pooling and strided conv2d. An efficient implementation could be S^2 faster, where S is the stride (typically S=2).\nStrided conv2d_transposed have been traditionally used in decoders for dense prediction problems (e.g. image generation, video prediction, semantic segmentation). However, this op often produces outputs with checkerboard artifacts, e.g. see [1] and papers citing it. An alternative is to use bilinear upsampling followed by a standard convolution (with no strides). This is widely used in several recent works and it mitigates the checkerboard artifacts but at a cost: it's computationally and memory expensive (e.g. see [2]). upsampling+conv2d does S^2 more computation than the strided counterpart, where S is the stride factor. Furthermore, the intermediate tensor after upsampling could be very large if the input has a large number of channels.\nUnder certain conditions (which happens to be the most common use case), upsampling+conv2d could be rewritten as convolving the upsampling kernel with the given kernel, followed by conv2d_transposed of the given input and the combined kernels. This follows from commutative and associative properties of linearity in convolutions (taking proper care of flipping the filters so that the ops are actual convolutions).\nA similar reasoning applies for an efficient implementation of conv2d+average_pooling.\nImplementations of the mentioned ops are here (see upsample_conv2d and conv_pool2d): https://github.com/alexlee-gk/video_prediction/blob/master/video_prediction/ops.py\nA script that tests for equivalence and timings is here: https://gist.github.com/alexlee-gk/1ae88125ec38efc48b542a4c0356078f\nI report some timings below of hundreds of evaluations. The naive op should be about 4x slower than the strided counterpart since S^2 = 4. In theory, the optimized op should be as fast as the strided counterpart, but my implementation is about 2x slower (but still much faster than the naive version), and I think there is room for improvement.\n\n\n\n\nupsample + conv2d (optimized)\nupsample + conv2d (naive)\nstrided conv2d_transpose\n\n\n\n\nforward pass\n11.0s\n16.3s\n4.7s\n\n\nforward and backward pass\n6.5s\n17.5s\n2.6s\n\n\n\n\n\n\n\nconv2d + pool (optimized)\nconv2d + pool (naive)\nstrided conv2d\n\n\n\n\nforward pass\n4.1s\n8.2s\n2.6s\n\n\nforward and backward pass\n4.1s\n8.4s\n2.6s\n\n\n\n[1] https://distill.pub/2016/deconv-checkerboard\n[2] https://arxiv.org/abs/1707.05847", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: /A\r\n- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7.0\r\n- **GPU model and memory**: Titan Xp 12GB\r\n- **Exact command to reproduce**: CUDA_VISIBLE_DEVICES=0 python test_conv_upsample_pool_ops.py\r\n\r\n### Describe the problem\r\nTLDR: upsampling+conv2d (which is very expensive) could be implemented such that it has the same time and memory complexity as strided conv2d_transposed. Similarly for conv2d+average_pooling and strided conv2d. An efficient implementation could be S^2 faster, where S is the stride (typically S=2).\r\n\r\nStrided conv2d_transposed have been traditionally used in decoders for dense prediction problems (e.g. image generation, video prediction, semantic segmentation). However, this op often produces outputs with checkerboard artifacts, e.g. see [1] and papers citing it. An alternative is to use bilinear upsampling followed by a standard convolution (with no strides). This is widely used in several recent works and it mitigates the checkerboard artifacts but at a cost: it's computationally and memory expensive (e.g. see [2]). upsampling+conv2d does S^2 more computation than the strided counterpart, where S is the stride factor. Furthermore, the intermediate tensor after upsampling could be very large if the input has a large number of channels.\r\n\r\nUnder certain conditions (which happens to be the most common use case), upsampling+conv2d could be rewritten as convolving the upsampling kernel with the given kernel, followed by conv2d_transposed of the given input and the combined kernels. This follows from commutative and associative properties of linearity in convolutions (taking proper care of flipping the filters so that the ops are actual convolutions).\r\n\r\nA similar reasoning applies for an efficient implementation of conv2d+average_pooling.\r\n\r\nImplementations of the mentioned ops are here (see `upsample_conv2d` and `conv_pool2d`): https://github.com/alexlee-gk/video_prediction/blob/master/video_prediction/ops.py\r\n\r\nA script that tests for equivalence and timings is here: https://gist.github.com/alexlee-gk/1ae88125ec38efc48b542a4c0356078f\r\n\r\nI report some timings below of hundreds of evaluations. The naive op should be about 4x slower than the strided counterpart since S^2 = 4. In theory, the optimized op should be as fast as the strided counterpart, but my implementation is about 2x slower (but still much faster than the naive version), and I think there is room for improvement.\r\n\r\n|                           | upsample + conv2d (optimized) | upsample + conv2d (naive) | strided conv2d_transpose |\r\n|:------------------------- | -----------------------------:| -------------------------:| --------------:|\r\n| forward pass              | 11.0s                         | 16.3s                     | 4.7s           |\r\n| forward and backward pass | 6.5s                          | 17.5s                     | 2.6s           |\r\n\r\n|                           | conv2d + pool (optimized) | conv2d + pool (naive) | strided conv2d |\r\n|:------------------------- | -------------------------:| ---------------------:| --------------:|\r\n| forward pass              | 4.1s                      | 8.2s                  | 2.6s           |\r\n| forward and backward pass | 4.1s                      | 8.4s                  | 2.6s           |\r\n\r\n[1] https://distill.pub/2016/deconv-checkerboard\r\n[2] https://arxiv.org/abs/1707.05847"}