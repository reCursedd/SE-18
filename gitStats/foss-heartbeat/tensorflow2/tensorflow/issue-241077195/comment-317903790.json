{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317903790", "html_url": "https://github.com/tensorflow/tensorflow/issues/11334#issuecomment-317903790", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11334", "id": 317903790, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzkwMzc5MA==", "user": {"login": "Mr-Grieves", "id": 19175336, "node_id": "MDQ6VXNlcjE5MTc1MzM2", "avatar_url": "https://avatars2.githubusercontent.com/u/19175336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mr-Grieves", "html_url": "https://github.com/Mr-Grieves", "followers_url": "https://api.github.com/users/Mr-Grieves/followers", "following_url": "https://api.github.com/users/Mr-Grieves/following{/other_user}", "gists_url": "https://api.github.com/users/Mr-Grieves/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mr-Grieves/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mr-Grieves/subscriptions", "organizations_url": "https://api.github.com/users/Mr-Grieves/orgs", "repos_url": "https://api.github.com/users/Mr-Grieves/repos", "events_url": "https://api.github.com/users/Mr-Grieves/events{/privacy}", "received_events_url": "https://api.github.com/users/Mr-Grieves/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-25T23:37:26Z", "updated_at": "2017-07-25T23:40:30Z", "author_association": "NONE", "body_html": "<p>Experiencing the same problem as above: lots of memory leakage due to the feeding operation.</p>\n<p>While I'm sure I'm missing a lot of intricacies here, the source seems obvious to me. During every call to <code>feed</code>, we are allocating new memory for the input tensor (float tensor in my case):<br>\n<strong><code>addFeed(inputName, Tensor.create(dims, FloatBuffer.wrap(src))); </code></strong></p>\n<p>Is there no way to overwrite the data in the existing FloatBuffer after the initial feed call? Instead of allocating space for a new Tensor each time (with the call to <code>create</code>)? Would that not solve the memory leak we are experiencing?</p>", "body_text": "Experiencing the same problem as above: lots of memory leakage due to the feeding operation.\nWhile I'm sure I'm missing a lot of intricacies here, the source seems obvious to me. During every call to feed, we are allocating new memory for the input tensor (float tensor in my case):\naddFeed(inputName, Tensor.create(dims, FloatBuffer.wrap(src))); \nIs there no way to overwrite the data in the existing FloatBuffer after the initial feed call? Instead of allocating space for a new Tensor each time (with the call to create)? Would that not solve the memory leak we are experiencing?", "body": "Experiencing the same problem as above: lots of memory leakage due to the feeding operation.\r\n\r\nWhile I'm sure I'm missing a lot of intricacies here, the source seems obvious to me. During every call to `feed`, we are allocating new memory for the input tensor (float tensor in my case):\r\n**`addFeed(inputName, Tensor.create(dims, FloatBuffer.wrap(src))); `**\r\n\r\nIs there no way to overwrite the data in the existing FloatBuffer after the initial feed call? Instead of allocating space for a new Tensor each time (with the call to `create`)? Would that not solve the memory leak we are experiencing?"}