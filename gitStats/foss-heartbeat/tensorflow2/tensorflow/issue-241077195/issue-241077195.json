{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11334", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11334/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11334/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11334/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11334", "id": 241077195, "node_id": "MDU6SXNzdWUyNDEwNzcxOTU=", "number": 11334, "title": "Memory Overhead/Leak in Android lib", "user": {"login": "faifai21", "id": 4616968, "node_id": "MDQ6VXNlcjQ2MTY5Njg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4616968?v=4", "gravatar_id": "", "url": "https://api.github.com/users/faifai21", "html_url": "https://github.com/faifai21", "followers_url": "https://api.github.com/users/faifai21/followers", "following_url": "https://api.github.com/users/faifai21/following{/other_user}", "gists_url": "https://api.github.com/users/faifai21/gists{/gist_id}", "starred_url": "https://api.github.com/users/faifai21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/faifai21/subscriptions", "organizations_url": "https://api.github.com/users/faifai21/orgs", "repos_url": "https://api.github.com/users/faifai21/repos", "events_url": "https://api.github.com/users/faifai21/events{/privacy}", "received_events_url": "https://api.github.com/users/faifai21/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 30, "created_at": "2017-07-06T20:31:54Z", "updated_at": "2018-03-28T07:11:00Z", "closed_at": "2018-03-20T18:49:13Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Nexus 6p, Android v7.1.2</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>:  1.2.0-rc2</li>\n<li><strong>Python version</strong>: 2.7.10</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5-homebrew</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:<br>\n-- Selective Headers: <code>bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a</code><br>\n-- Added <code>tensorflow/core/kernels/random_shuffle_queue_op.cc</code> and <code>tensorflow/core/kernels/random_shuffle_op.cc</code> to <code>tf_op_files.txt</code> file<br>\n-- Removed unused nodes: <code>bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\ --in_graph=model.pb \\ --out_graph=optimized_model.pb \\ --inputs='input' \\ --outputs='output' \\ --transforms=' strip_unused_nodes(type=float, shape=\"1,299,299,3\")'</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The Tensorflow Android library is using a lot more memory than I expected. It almost seems like it's maintaining a reference to all input arrays, as memory usage balloons the longer the model is used.<br>\nHere is an example of the memory usage with feed/run/fetch commented out (source code below):</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/4616968/27930636-3342a548-624c-11e7-91ed-3f4f56b7cf5f.png\"><img src=\"https://user-images.githubusercontent.com/4616968/27930636-3342a548-624c-11e7-91ed-3f4f56b7cf5f.png\" alt=\"no_tensorflow\" style=\"max-width:100%;\"></a></p>\n<p>Here is the same timeframe, with the only difference being that feed/run/fetch is enabled:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/4616968/27930738-954e125e-624c-11e7-82f2-cc21e67bc5d3.png\"><img src=\"https://user-images.githubusercontent.com/4616968/27930738-954e125e-624c-11e7-82f2-cc21e67bc5d3.png\" alt=\"tensorflow\" style=\"max-width:100%;\"></a></p>\n<p>Memory usage is over three times worse. The longer I leave the model running, the more memory usage increases (it eventually gets to 110 mb).</p>\n<p>The below method is being called at a rate of 4.419011933 per sec (i.e. it's processing 4.412 input arrays per second), where each input array is of size 96*96*3 (27648).</p>\n<p>This is being run on a Nexus 6p, running stock 7.1.2. The model is a conv net with inception, batch norm and dropout, trained using tensorflow slim.</p>\n<h3>Source code / logs</h3>\n<p>Commented out:</p>\n<div class=\"highlight highlight-source-java\"><pre><span class=\"pl-k\">public</span> <span class=\"pl-k\">float</span>[] runInference(<span class=\"pl-k\">float</span>[] pixels) {\n        assertRightSize(pixels);\n        <span class=\"pl-k\">final</span> <span class=\"pl-k\">float</span>[] outputArray <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">float</span>[<span class=\"pl-c1\">128</span>];\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> Simulate some sort of output</span>\n        <span class=\"pl-smi\">Arrays</span><span class=\"pl-k\">.</span>fill(outputArray, <span class=\"pl-k\">new</span> <span class=\"pl-smi\">Random</span>()<span class=\"pl-k\">.</span>nextInt(<span class=\"pl-c1\">1000</span>)<span class=\"pl-k\">/</span><span class=\"pl-k\">new</span> <span class=\"pl-smi\">Random</span>()<span class=\"pl-k\">.</span>nextFloat());\n<span class=\"pl-c\"><span class=\"pl-c\">//</span>     inferenceInterface.feed(\"phase_train\", new bool[]{false});</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span>     inferenceInterface.feed(\"input\", pixels, 1, 96, 96, 3);</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span>     inferenceInterface.run(new String[]{\"output\"});</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> Copy the output Tensor back into the output array.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span>     inferenceInterface.fetch(\"output\", outputArray);</span>\n\n        <span class=\"pl-k\">return</span> outputArray;\n    }</pre></div>\n<p>Enabled:</p>\n<div class=\"highlight highlight-source-java\"><pre><span class=\"pl-k\">public</span> <span class=\"pl-k\">float</span>[] runInference(<span class=\"pl-k\">float</span>[] pixels) {\n        assertRightSize(pixels);\n        <span class=\"pl-k\">final</span> <span class=\"pl-k\">float</span>[] outputArray <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">float</span>[<span class=\"pl-c1\">128</span>];\n        inferenceInterface<span class=\"pl-k\">.</span>feed(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>phase_train<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-k\">new</span> <span class=\"pl-smi\">bool</span>[]{<span class=\"pl-c1\">false</span>});\n        inferenceInterface<span class=\"pl-k\">.</span>feed(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>, pixels, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">3</span>);\n        inferenceInterface<span class=\"pl-k\">.</span>run(<span class=\"pl-k\">new</span> <span class=\"pl-smi\">String</span>[]{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>});\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> Copy the output Tensor back into the output array.</span>\n        inferenceInterface<span class=\"pl-k\">.</span>fetch(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>, outputArray);\n\n        <span class=\"pl-k\">return</span> outputArray;\n    }</pre></div>\n<p>where <code>float[] pixels</code> is a float array of size <code>27648</code>, denoting the pixels in an image of size 96x96.</p>\n<p>The custom code is an update to the InferenceInterface to accept boolean types during feeding:</p>\n<div class=\"highlight highlight-source-java\"><pre><span class=\"pl-k\">public</span> <span class=\"pl-k\">void</span> feed(<span class=\"pl-smi\">String</span> inputName, <span class=\"pl-k\">boolean</span>[] src, <span class=\"pl-k\">long</span><span class=\"pl-k\">.</span><span class=\"pl-c1\">..</span> dims) {\n        <span class=\"pl-k\">byte</span>[] b <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">byte</span>[src<span class=\"pl-k\">.</span>length];\n        <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>; i <span class=\"pl-k\">&lt;</span> src<span class=\"pl-k\">.</span>length; <span class=\"pl-k\">++</span>i) {\n            b[i] <span class=\"pl-k\">=</span> (<span class=\"pl-k\">byte</span>) (src[i] <span class=\"pl-k\">?</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">:</span> <span class=\"pl-c1\">0</span>);\n        }\n        addFeed(inputName, <span class=\"pl-smi\">Tensor</span><span class=\"pl-k\">.</span>create(<span class=\"pl-smi\">DataType</span><span class=\"pl-c1\"><span class=\"pl-k\">.</span>BOOL</span>, dims, <span class=\"pl-smi\">ByteBuffer</span><span class=\"pl-k\">.</span>wrap(b)));\n    }</pre></div>\n<p>Please let me know if there's any other information I can provide.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Nexus 6p, Android v7.1.2\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below):  1.2.0-rc2\nPython version: 2.7.10\nBazel version (if compiling from source): 0.4.5-homebrew\nCUDA/cuDNN version: N/A\nGPU model and memory:\nExact command to reproduce:\n-- Selective Headers: bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\n-- Added tensorflow/core/kernels/random_shuffle_queue_op.cc and tensorflow/core/kernels/random_shuffle_op.cc to tf_op_files.txt file\n-- Removed unused nodes: bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\ --in_graph=model.pb \\ --out_graph=optimized_model.pb \\ --inputs='input' \\ --outputs='output' \\ --transforms=' strip_unused_nodes(type=float, shape=\"1,299,299,3\")'\n\nDescribe the problem\nThe Tensorflow Android library is using a lot more memory than I expected. It almost seems like it's maintaining a reference to all input arrays, as memory usage balloons the longer the model is used.\nHere is an example of the memory usage with feed/run/fetch commented out (source code below):\n\nHere is the same timeframe, with the only difference being that feed/run/fetch is enabled:\n\nMemory usage is over three times worse. The longer I leave the model running, the more memory usage increases (it eventually gets to 110 mb).\nThe below method is being called at a rate of 4.419011933 per sec (i.e. it's processing 4.412 input arrays per second), where each input array is of size 96*96*3 (27648).\nThis is being run on a Nexus 6p, running stock 7.1.2. The model is a conv net with inception, batch norm and dropout, trained using tensorflow slim.\nSource code / logs\nCommented out:\npublic float[] runInference(float[] pixels) {\n        assertRightSize(pixels);\n        final float[] outputArray = new float[128];\n        // Simulate some sort of output\n        Arrays.fill(outputArray, new Random().nextInt(1000)/new Random().nextFloat());\n//     inferenceInterface.feed(\"phase_train\", new bool[]{false});\n//     inferenceInterface.feed(\"input\", pixels, 1, 96, 96, 3);\n//     inferenceInterface.run(new String[]{\"output\"});\n        // Copy the output Tensor back into the output array.\n//     inferenceInterface.fetch(\"output\", outputArray);\n\n        return outputArray;\n    }\nEnabled:\npublic float[] runInference(float[] pixels) {\n        assertRightSize(pixels);\n        final float[] outputArray = new float[128];\n        inferenceInterface.feed(\"phase_train\", new bool[]{false});\n        inferenceInterface.feed(\"input\", pixels, 1, 96, 96, 3);\n        inferenceInterface.run(new String[]{\"output\"});\n        // Copy the output Tensor back into the output array.\n        inferenceInterface.fetch(\"output\", outputArray);\n\n        return outputArray;\n    }\nwhere float[] pixels is a float array of size 27648, denoting the pixels in an image of size 96x96.\nThe custom code is an update to the InferenceInterface to accept boolean types during feeding:\npublic void feed(String inputName, boolean[] src, long... dims) {\n        byte[] b = new byte[src.length];\n        for (int i = 0; i < src.length; ++i) {\n            b[i] = (byte) (src[i] ? 1 : 0);\n        }\n        addFeed(inputName, Tensor.create(DataType.BOOL, dims, ByteBuffer.wrap(b)));\n    }\nPlease let me know if there's any other information I can provide.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Nexus 6p, Android v7.1.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.2.0-rc2\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: 0.4.5-homebrew\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\n-- Selective Headers: ` bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a `\r\n-- Added `tensorflow/core/kernels/random_shuffle_queue_op.cc` and `tensorflow/core/kernels/random_shuffle_op.cc` to `tf_op_files.txt` file\r\n-- Removed unused nodes: `bazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=model.pb \\\r\n--out_graph=optimized_model.pb \\\r\n--inputs='input' \\\r\n--outputs='output' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")'`\r\n\r\n### Describe the problem\r\nThe Tensorflow Android library is using a lot more memory than I expected. It almost seems like it's maintaining a reference to all input arrays, as memory usage balloons the longer the model is used. \r\nHere is an example of the memory usage with feed/run/fetch commented out (source code below):\r\n\r\n![no_tensorflow](https://user-images.githubusercontent.com/4616968/27930636-3342a548-624c-11e7-91ed-3f4f56b7cf5f.png)\r\n\r\nHere is the same timeframe, with the only difference being that feed/run/fetch is enabled:\r\n\r\n![tensorflow](https://user-images.githubusercontent.com/4616968/27930738-954e125e-624c-11e7-82f2-cc21e67bc5d3.png)\r\n\r\nMemory usage is over three times worse. The longer I leave the model running, the more memory usage increases (it eventually gets to 110 mb).\r\n\r\nThe below method is being called at a rate of 4.419011933 per sec (i.e. it's processing 4.412 input arrays per second), where each input array is of size 96\\*96\\*3 (27648).\r\n\r\nThis is being run on a Nexus 6p, running stock 7.1.2. The model is a conv net with inception, batch norm and dropout, trained using tensorflow slim. \r\n\r\n### Source code / logs\r\nCommented out:\r\n```java\r\npublic float[] runInference(float[] pixels) {\r\n        assertRightSize(pixels);\r\n        final float[] outputArray = new float[128];\r\n        // Simulate some sort of output\r\n        Arrays.fill(outputArray, new Random().nextInt(1000)/new Random().nextFloat());\r\n//     inferenceInterface.feed(\"phase_train\", new bool[]{false});\r\n//     inferenceInterface.feed(\"input\", pixels, 1, 96, 96, 3);\r\n//     inferenceInterface.run(new String[]{\"output\"});\r\n        // Copy the output Tensor back into the output array.\r\n//     inferenceInterface.fetch(\"output\", outputArray);\r\n\r\n        return outputArray;\r\n    }\r\n```\r\nEnabled:\r\n\r\n```java\r\npublic float[] runInference(float[] pixels) {\r\n        assertRightSize(pixels);\r\n        final float[] outputArray = new float[128];\r\n        inferenceInterface.feed(\"phase_train\", new bool[]{false});\r\n        inferenceInterface.feed(\"input\", pixels, 1, 96, 96, 3);\r\n        inferenceInterface.run(new String[]{\"output\"});\r\n        // Copy the output Tensor back into the output array.\r\n        inferenceInterface.fetch(\"output\", outputArray);\r\n\r\n        return outputArray;\r\n    }\r\n```\r\n\r\nwhere `float[] pixels` is a float array of size `27648`, denoting the pixels in an image of size 96x96.\r\n\r\nThe custom code is an update to the InferenceInterface to accept boolean types during feeding: \r\n\r\n```java\r\npublic void feed(String inputName, boolean[] src, long... dims) {\r\n        byte[] b = new byte[src.length];\r\n        for (int i = 0; i < src.length; ++i) {\r\n            b[i] = (byte) (src[i] ? 1 : 0);\r\n        }\r\n        addFeed(inputName, Tensor.create(DataType.BOOL, dims, ByteBuffer.wrap(b)));\r\n    }\r\n```\r\n\r\nPlease let me know if there's any other information I can provide."}