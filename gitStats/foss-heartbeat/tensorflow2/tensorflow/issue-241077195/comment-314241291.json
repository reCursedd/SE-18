{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/314241291", "html_url": "https://github.com/tensorflow/tensorflow/issues/11334#issuecomment-314241291", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11334", "id": 314241291, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDI0MTI5MQ==", "user": {"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-10T21:02:38Z", "updated_at": "2017-07-10T21:02:38Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4616968\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/faifai21\">@faifai21</a> Thanks for the detailed report! Are you able to determine exactly how much the memory use increases per inference pass? If you change the size of your input array, does the rate of increase vary proportionally? I'm just curious if something else in the run() call could be allocating memory that never gets cleaned up.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> Any idea why this might be happening, given that closeFetches() and closeFeeds() are both guaranteed to be called every invocation of TensorFlowInferenceInterface.run()?</p>", "body_text": "@faifai21 Thanks for the detailed report! Are you able to determine exactly how much the memory use increases per inference pass? If you change the size of your input array, does the rate of increase vary proportionally? I'm just curious if something else in the run() call could be allocating memory that never gets cleaned up.\n@asimshankar Any idea why this might be happening, given that closeFetches() and closeFeeds() are both guaranteed to be called every invocation of TensorFlowInferenceInterface.run()?", "body": "@faifai21 Thanks for the detailed report! Are you able to determine exactly how much the memory use increases per inference pass? If you change the size of your input array, does the rate of increase vary proportionally? I'm just curious if something else in the run() call could be allocating memory that never gets cleaned up.\r\n\r\n@asimshankar Any idea why this might be happening, given that closeFetches() and closeFeeds() are both guaranteed to be called every invocation of TensorFlowInferenceInterface.run()?"}