{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13869", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13869/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13869/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13869/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13869", "id": 267313661, "node_id": "MDU6SXNzdWUyNjczMTM2NjE=", "number": 13869, "title": "conv2d_transpose crashing, \"NotFoundError: No algorithm worked!\", only with batch size >=2^16", "user": {"login": "sullivak", "id": 3643255, "node_id": "MDQ6VXNlcjM2NDMyNTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/3643255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sullivak", "html_url": "https://github.com/sullivak", "followers_url": "https://api.github.com/users/sullivak/followers", "following_url": "https://api.github.com/users/sullivak/following{/other_user}", "gists_url": "https://api.github.com/users/sullivak/gists{/gist_id}", "starred_url": "https://api.github.com/users/sullivak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sullivak/subscriptions", "organizations_url": "https://api.github.com/users/sullivak/orgs", "repos_url": "https://api.github.com/users/sullivak/repos", "events_url": "https://api.github.com/users/sullivak/events{/privacy}", "received_events_url": "https://api.github.com/users/sullivak/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-20T21:46:31Z", "updated_at": "2018-11-08T11:27:36Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm getting a crash running a simple autoencoder network, stack trace below. Interestingly if I trim the batch size of the input (validate_vecs_normed) to 65535, everything is fine. E.g.<br>\n<code>validate_vecs_normed.shape (65536, 75)</code> crashes, <code>validate_vecs_normed.shape (65536, 75)</code> does not. The size of input is less than 20 MB so should be plenty of room with a 12GB card.</p>\n<pre><code>Traceback (most recent call last):\n  [...snip...] \n    recon, batch_cost = sess.run([decoded, cost], feed_dict={x_in_unrav: validate_vecs_normed})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n    options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'conv2d_transpose', defined at:\n  [...snip...]\n    saver = tf.train.import_meta_graph(os.path.join(model_dir, model_meta_format.format(fold_ind)))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\n    **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): No algorithm worked!\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<p>Possibly related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"240990720\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11327\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11327/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/11327\">#11327</a> or <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"225502749\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9576\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9576/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/9576\">#9576</a> but not sure. One other thing, when running same code on tensorflow 1.0.1, there was no crash, but the return value \"decoded\" was all zeros when running a large batch size, and normal with smaller. I'm not sure if it's the same 65535/65536 threshold, I hadn't found it at that point.</p>\n<p>Just for fun I ran with <code>TF_USE_CUDNN=0</code> but that crashes with <code>UnimplementedError (see above for traceback): Conv2D for GPU is not currently supported without cudnn</code></p>\n<p>OS: Ubuntu 16.04.<br>\nRunning docker image based on tensorflow/tensorflow:1.3.0-devel-gpu (so python 2.7.12, CUDA v8.0, cuDNN v6.0), with nvidia-docker 17.05.0-ce.<br>\nDocker image tensorflow/tensorflow:1.0.1-gpu returns all zeros instead of crashing.<br>\nGPUs: 2x Titan X (Pascal).</p>", "body_text": "I'm getting a crash running a simple autoencoder network, stack trace below. Interestingly if I trim the batch size of the input (validate_vecs_normed) to 65535, everything is fine. E.g.\nvalidate_vecs_normed.shape (65536, 75) crashes, validate_vecs_normed.shape (65536, 75) does not. The size of input is less than 20 MB so should be plenty of room with a 12GB card.\nTraceback (most recent call last):\n  [...snip...] \n    recon, batch_cost = sess.run([decoded, cost], feed_dict={x_in_unrav: validate_vecs_normed})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n    options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'conv2d_transpose', defined at:\n  [...snip...]\n    saver = tf.train.import_meta_graph(os.path.join(model_dir, model_meta_format.format(fold_ind)))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\n    **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): No algorithm worked!\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nPossibly related to #11327 or #9576 but not sure. One other thing, when running same code on tensorflow 1.0.1, there was no crash, but the return value \"decoded\" was all zeros when running a large batch size, and normal with smaller. I'm not sure if it's the same 65535/65536 threshold, I hadn't found it at that point.\nJust for fun I ran with TF_USE_CUDNN=0 but that crashes with UnimplementedError (see above for traceback): Conv2D for GPU is not currently supported without cudnn\nOS: Ubuntu 16.04.\nRunning docker image based on tensorflow/tensorflow:1.3.0-devel-gpu (so python 2.7.12, CUDA v8.0, cuDNN v6.0), with nvidia-docker 17.05.0-ce.\nDocker image tensorflow/tensorflow:1.0.1-gpu returns all zeros instead of crashing.\nGPUs: 2x Titan X (Pascal).", "body": "I'm getting a crash running a simple autoencoder network, stack trace below. Interestingly if I trim the batch size of the input (validate_vecs_normed) to 65535, everything is fine. E.g.\r\n`validate_vecs_normed.shape (65536, 75)` crashes, `validate_vecs_normed.shape (65536, 75)` does not. The size of input is less than 20 MB so should be plenty of room with a 12GB card.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  [...snip...] \r\n    recon, batch_cost = sess.run([decoded, cost], feed_dict={x_in_unrav: validate_vecs_normed})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\r\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\r\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'conv2d_transpose', defined at:\r\n  [...snip...]\r\n    saver = tf.train.import_meta_graph(os.path.join(model_dir, model_meta_format.format(fold_ind)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\r\n    **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): No algorithm worked!\r\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\r\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\nPossibly related to #11327 or #9576 but not sure. One other thing, when running same code on tensorflow 1.0.1, there was no crash, but the return value \"decoded\" was all zeros when running a large batch size, and normal with smaller. I'm not sure if it's the same 65535/65536 threshold, I hadn't found it at that point.\r\n\r\nJust for fun I ran with `TF_USE_CUDNN=0` but that crashes with `UnimplementedError (see above for traceback): Conv2D for GPU is not currently supported without cudnn`\r\n\r\nOS: Ubuntu 16.04. \r\nRunning docker image based on tensorflow/tensorflow:1.3.0-devel-gpu (so python 2.7.12, CUDA v8.0, cuDNN v6.0), with nvidia-docker 17.05.0-ce. \r\nDocker image tensorflow/tensorflow:1.0.1-gpu returns all zeros instead of crashing.\r\nGPUs: 2x Titan X (Pascal).\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}