{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/199626010", "pull_request_review_id": 133724668, "id": 199626010, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTYyNjAxMA==", "diff_hunk": "@@ -790,118 +918,23 @@ tensorflow::Status BinaryCompute(const TRT_ShapedWeights& iweights_l,\n   return tensorflow::Status::OK();\n }\n \n-tensorflow::Status ConstantFoldUnary(\n-    Converter& ctx, const tensorflow::NodeDef& node_def,\n-    const std::vector<TRT_TensorOrWeights>& inputs,\n-    std::vector<TRT_TensorOrWeights>* outputs) {\n-  TRT_ShapedWeights weights_input = inputs.at(0).weights();\n-\n-  // Allocate output weights\n-  TRT_ShapedWeights weights_output = ctx.get_temp_weights_like(weights_input);\n-\n-  // FIXME assume type matches input weights\n-  // Get trt type & shape\n-  // Maybe this part has to be moved into the block of rsqrt later\n-  // Check type consistency\n-  CHECK_EQ(weights_input.type_,\n-           TFAttrs(node_def).get<tensorflow::DataType>(\"T\"));\n-\n-  LambdaFactory unary_op;\n-  if (node_def.op() == \"Rsqrt\") {\n-    // Compute rsqrt\n-    unary_op.op = LambdaFactory::OP_CATEGORY::RSQRT;\n-    auto ret = UnaryCompute(weights_input, &weights_output, unary_op);\n-    // Pass the output\n-    if (ret == tensorflow::Status::OK()) {\n-      outputs->push_back(TRT_TensorOrWeights(weights_output));\n-    }\n-    return ret;\n-  } else {\n-    return tensorflow::errors::Unimplemented(\"Binary op not supported: \" +\n-                                             node_def.op());\n-  }\n-}\n-\n-// TODO(jie,ben) broadcast is needed yet not implemented\n-// Let's get the simple stuff working first. Maybe we should fall back to TF\n-//   approach for constant folding\n-tensorflow::Status ConstantFoldBinary(\n-    Converter& ctx, const tensorflow::NodeDef& node_def,\n-    const std::vector<TRT_TensorOrWeights>& inputs,\n-    std::vector<TRT_TensorOrWeights>* outputs) {\n-  TRT_ShapedWeights weights_input_l = inputs.at(0).weights();\n-  TRT_ShapedWeights weights_input_r = inputs.at(1).weights();\n-\n-  // Check type consistency\n-  CHECK_EQ(weights_input_l.type_, weights_input_r.type_);\n-\n-  if (weights_input_l.shape_.nbDims != weights_input_r.shape_.nbDims)\n-    return tensorflow::errors::Unimplemented(\n-        \"Binary op implicit broadcast not supported: \" + node_def.op());\n-\n-  // TODO(jie): constant fold should really fall back to TF.\n-  int num_dims = weights_input_l.shape_.nbDims;\n-  nvinfer1::Dims output_shape;\n-  output_shape.nbDims = num_dims;\n-  VLOG(2) << \"nb_dims: \" << num_dims\n-          << \", the other: \" << weights_input_r.shape_.nbDims;\n-  for (int i = 0; i < num_dims; i++) {\n-    if (weights_input_l.shape_.d[i] == weights_input_r.shape_.d[i]) {\n-      output_shape.d[i] = weights_input_l.shape_.d[i];\n-    } else if (weights_input_l.shape_.d[i] == 1 ||\n-               weights_input_r.shape_.d[i] == 1) {\n-      output_shape.d[i] =\n-          std::max(weights_input_l.shape_.d[i], weights_input_r.shape_.d[i]);\n-    } else {\n-      return tensorflow::errors::Unimplemented(\n-          \"Binary op with incompatible shape at, \" + node_def.op());\n-    }\n-    VLOG(2) << \"left: \" << weights_input_l.shape_.d[i]\n-            << \"right: \" << weights_input_r.shape_.d[i]\n-            << \"output: \" << output_shape.d[i];\n-  }\n-\n-  // FIXME assume type matches input weights\n-  // Get trt type & shape\n-  TFAttrs attrs(node_def);\n-  // Maybe this part has to be moved into the block of rsqrt later\n-  tensorflow::DataType dtype = attrs.get<tensorflow::DataType>(\"T\");\n-\n-  // Allocate output weights\n-  TRT_ShapedWeights weights_output = ctx.get_temp_weights(dtype, output_shape);\n-\n-  LambdaFactory binary_op;\n-  if (node_def.op() == \"Sub\") {\n-    binary_op.op = LambdaFactory::OP_CATEGORY::SUB;\n-  } else if (node_def.op() == \"Mul\") {\n-    binary_op.op = LambdaFactory::OP_CATEGORY::MUL;\n-  } else if (node_def.op() == \"Add\") {\n-    binary_op.op = LambdaFactory::OP_CATEGORY::ADD;\n-  } else {\n-    return tensorflow::errors::Unimplemented(\"Binary op not supported: \" +\n-                                             node_def.op());\n-  }\n-  auto ret = BinaryCompute(weights_input_l, weights_input_r, &weights_output,\n-                           binary_op);\n-\n-  // Pass the output\n-  if (ret == tensorflow::Status::OK()) {\n-    outputs->push_back(TRT_TensorOrWeights(weights_output));\n-  }\n-\n-  return ret;\n-}\n-\n // TODO(jie): broadcast is needed yet not implemented.\n // Only implemented channel wise for the time being\n tensorflow::Status BinaryTensorOpWeight(\n     Converter& ctx, const tensorflow::NodeDef& node_def,\n     const nvinfer1::ITensor* tensor, TRT_ShapedWeights weights,\n-    std::vector<TRT_TensorOrWeights>* outputs) {\n+    std::vector<TRT_TensorOrWeights>* outputs, bool swapped_inputs) {", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": null, "original_position": 317, "commit_id": "2de343c329ff252ed0cb419f29c3ce3765b3da84", "original_commit_id": "2629729eef55f27d03a1be661bd827d5176afd51", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "Please follow the convention by making all input parameters appear before any output parameter.\r\nAlso, what does swapped_inputs means? Please comment.", "created_at": "2018-07-02T21:26:53Z", "updated_at": "2018-07-11T18:33:32Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20350#discussion_r199626010", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20350", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/199626010"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20350#discussion_r199626010"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20350"}}, "body_html": "<p>Please follow the convention by making all input parameters appear before any output parameter.<br>\nAlso, what does swapped_inputs means? Please comment.</p>", "body_text": "Please follow the convention by making all input parameters appear before any output parameter.\nAlso, what does swapped_inputs means? Please comment."}