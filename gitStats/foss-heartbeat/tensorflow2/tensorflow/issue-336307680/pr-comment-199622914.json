{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/199622914", "pull_request_review_id": 133724668, "id": 199622914, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTYyMjkxNA==", "diff_hunk": "@@ -75,13 +102,108 @@ inline tensorflow::Status ConvertDType(tensorflow::DataType tf_dtype,\n     case tensorflow::DataType::DT_HALF:\n       *trt_dtype = nvinfer1::DataType::kHALF;\n       break;\n+#if NV_TENSORRT_MAJOR > 3\n+    case tensorflow::DataType::DT_INT32:\n+      *trt_dtype = nvinfer1::DataType::kINT32;\n+      break;\n+#endif\n     default:\n       return tensorflow::errors::InvalidArgument(\n           \"Unsupported data type \" + tensorflow::DataTypeString(tf_dtype));\n   }\n   return tensorflow::Status::OK();\n }\n \n+// return whether or not the broadcast is feasible;\n+bool TensorRTGetBroadcastShape(const nvinfer1::Dims& operand_l,\n+                               const bool operand_l_is_tensor,\n+                               const nvinfer1::Dims& operand_r,\n+                               const bool operand_r_is_tensor,\n+                               nvinfer1::Dims* operand_l_new_shape,\n+                               nvinfer1::Dims* operand_r_new_shape) {\n+  /*******************************************************************************\n+    TensorRT Elementwise op supports broadcast but requires both tensor to be of\n+    Identical rank\n+\n+    We consider case of: i. Tensor op Const; ii. Tensor op Tensor\n+    note: const op const (constant folding) should fallback to TensorFlow\n+\n+    broadcast scheme:\n+    T: 1 3 5          (tensor would not have batch dimension)\n+    W: 1 1 3 1        (weight would have all explicit dimensions)\n+    i. fill in explicit dimensions\n+    -> T: -1 1 3 5  (we put a -1 for batch dimension)\n+    -> W:  1 1 3 1\n+    ii. compare broadcast feasibility\n+\n+    we cannot support these since TensorRT does not allow manipulation on batch\n+  dimension, we cannot generate output with proper shape\n+    T: 3 5 1\n+    W: 1 1 1 1 3 5 1\n+    -> T: 1 1 1 -1 3 5 1\n+    -> W: 1 1 1  1 3 5 1\n+  *******************************************************************************/\n+  static const int max_nb_dims = nvinfer1::Dims::MAX_DIMS + 1;\n+  const size_t element_size = sizeof(operand_l.d[0]);\n+\n+  // fill in dimensions\n+  int l_s[max_nb_dims];\n+  std::fill(l_s, l_s + max_nb_dims, 1);\n+  int l_d = operand_l_is_tensor ? operand_l.nbDims + 1 : operand_l.nbDims;\n+  int r_s[max_nb_dims];\n+  std::fill(r_s, r_s + max_nb_dims, 1);\n+  int r_d = operand_r_is_tensor ? operand_r.nbDims + 1 : operand_r.nbDims;\n+\n+  int max_d = std::max(l_d, r_d);\n+  std::memcpy(l_s + max_d - operand_l.nbDims, operand_l.d,\n+              operand_l.nbDims * element_size);\n+  std::memcpy(r_s + max_d - operand_r.nbDims, operand_r.d,\n+              operand_r.nbDims * element_size);\n+\n+  // set -1 for batch dimension, since batch size is not supposed to be\n+  // broadcasted\n+  if (operand_l_is_tensor) {\n+    if (max_d != l_d) {  // if broadcast beyond batch dimension, fail\n+      return false;\n+    }\n+    l_s[0] = -1;", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": 104, "original_position": 102, "commit_id": "2de343c329ff252ed0cb419f29c3ce3765b3da84", "original_commit_id": "2629729eef55f27d03a1be661bd827d5176afd51", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "It seems this does not align with what the comment says:\r\n```\r\n+    T: 3 5 1\r\n+    -> T: 1 1 1 -1 3 5 1\r\n```\r\nbut here it assigns -1 to first element.", "created_at": "2018-07-02T21:13:38Z", "updated_at": "2018-07-11T18:33:32Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20350#discussion_r199622914", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20350", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/199622914"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20350#discussion_r199622914"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20350"}}, "body_html": "<p>It seems this does not align with what the comment says:</p>\n<pre><code>+    T: 3 5 1\n+    -&gt; T: 1 1 1 -1 3 5 1\n</code></pre>\n<p>but here it assigns -1 to first element.</p>", "body_text": "It seems this does not align with what the comment says:\n+    T: 3 5 1\n+    -> T: 1 1 1 -1 3 5 1\n\nbut here it assigns -1 to first element."}