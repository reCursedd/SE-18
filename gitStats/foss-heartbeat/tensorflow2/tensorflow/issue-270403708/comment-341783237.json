{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341783237", "html_url": "https://github.com/tensorflow/tensorflow/issues/14162#issuecomment-341783237", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14162", "id": 341783237, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTc4MzIzNw==", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T18:06:14Z", "updated_at": "2017-11-03T18:18:32Z", "author_association": "NONE", "body_html": "<p>Hello, I made a simple test as below</p>\n<pre><code>  TRAIN_STEP_NUM = 5\n  x_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n  y_train = [0, -1, -2, -3, -4, -5, -6, -7, -8, -9]\n\n  dataset = tf.data.Dataset.from_tensor_slices(\n\t  (tf.cast(x_train, tf.float32), tf.cast(y_train, tf.float32)))\n\n  with tf.Graph().as_default():\n\titerator = dataset.batch(1).make_one_shot_iterator()\n\tx, y = iterator.get_next()\n\t\n\tW = tf.Variable([.3], dtype=tf.float32)\n\tb = tf.Variable([-.3], dtype=tf.float32)\n\tlinear_model = W*x + b\n\tloss = tf.reduce_sum(tf.square(linear_model - y))\n\n\tloss = tf.where(\n\t  tf.greater(tf.train.get_or_create_global_step(), TRAIN_STEP_NUM),\n\t\t\t\t loss, tf.stop_gradient(loss))\n\tslim.losses.add_loss(loss)\n  \n\toptimizer = tf.train.GradientDescentOptimizer(0.01)\n\ttrain_op = slim.learning.create_train_op(loss, optimizer)\n\n\tslim.learning.train(train_op, './tmp',\n\t\t\t\t\t\tlog_every_n_steps=1,\n\t\t\t\t\t\tnumber_of_steps=10)\n</code></pre>\n<p>The outputs are:</p>\n<pre><code>INFO:tensorflow:Starting Session.\nINFO:tensorflow:global_step/sec: inf\nINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\nINFO:tensorflow:Starting Queues.\nINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\nINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\nINFO:tensorflow:global step 3: loss = 6.7600 (0.000 sec/step)\nINFO:tensorflow:global step 4: loss = 15.2100 (0.000 sec/step)\nINFO:tensorflow:global step 5: loss = 27.0400 (0.016 sec/step)\nINFO:tensorflow:global step 6: loss = 42.2500 (0.000 sec/step)\nINFO:tensorflow:global step 7: loss = 60.8400 (0.000 sec/step)\nINFO:tensorflow:global step 8: loss = 0.0433 (0.000 sec/step)\nINFO:tensorflow:global step 9: loss = 0.0126 (0.000 sec/step)\nINFO:tensorflow:global step 10: loss = 0.0068 (0.000 sec/step)\nINFO:tensorflow:Stopping Training.\nINFO:tensorflow:Finished training! Saving model to disk.\n</code></pre>\n<p>If I disabled the \"tf.where\" line, then the outputs are:</p>\n<pre><code>INFO:tensorflow:Starting Session.\nINFO:tensorflow:global_step/sec: inf\nINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\nINFO:tensorflow:Starting Queues.\nINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\nINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\nINFO:tensorflow:global step 3: loss = 5.8467 (0.000 sec/step)\nINFO:tensorflow:global step 4: loss = 9.2253 (0.000 sec/step)\nINFO:tensorflow:global step 5: loss = 8.2057 (0.000 sec/step)\nINFO:tensorflow:global step 6: loss = 3.7965 (0.000 sec/step)\nINFO:tensorflow:global step 7: loss = 0.7162 (0.000 sec/step)\nINFO:tensorflow:global step 8: loss = 0.0489 (0.000 sec/step)\nINFO:tensorflow:global step 9: loss = 0.0143 (0.000 sec/step)\nINFO:tensorflow:global step 10: loss = 0.0077 (0.000 sec/step)\nINFO:tensorflow:Stopping Training.\nINFO:tensorflow:Finished training! Saving model to disk.\n</code></pre>\n<p>It seems that the stop_gradient did not freeze the updating of parameters, leading to loss change; however, it did influence somewhat unexpected, making the loss different. What should I do to freeze the updating in 5 steps? Thank you.</p>", "body_text": "Hello, I made a simple test as below\n  TRAIN_STEP_NUM = 5\n  x_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n  y_train = [0, -1, -2, -3, -4, -5, -6, -7, -8, -9]\n\n  dataset = tf.data.Dataset.from_tensor_slices(\n\t  (tf.cast(x_train, tf.float32), tf.cast(y_train, tf.float32)))\n\n  with tf.Graph().as_default():\n\titerator = dataset.batch(1).make_one_shot_iterator()\n\tx, y = iterator.get_next()\n\t\n\tW = tf.Variable([.3], dtype=tf.float32)\n\tb = tf.Variable([-.3], dtype=tf.float32)\n\tlinear_model = W*x + b\n\tloss = tf.reduce_sum(tf.square(linear_model - y))\n\n\tloss = tf.where(\n\t  tf.greater(tf.train.get_or_create_global_step(), TRAIN_STEP_NUM),\n\t\t\t\t loss, tf.stop_gradient(loss))\n\tslim.losses.add_loss(loss)\n  \n\toptimizer = tf.train.GradientDescentOptimizer(0.01)\n\ttrain_op = slim.learning.create_train_op(loss, optimizer)\n\n\tslim.learning.train(train_op, './tmp',\n\t\t\t\t\t\tlog_every_n_steps=1,\n\t\t\t\t\t\tnumber_of_steps=10)\n\nThe outputs are:\nINFO:tensorflow:Starting Session.\nINFO:tensorflow:global_step/sec: inf\nINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\nINFO:tensorflow:Starting Queues.\nINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\nINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\nINFO:tensorflow:global step 3: loss = 6.7600 (0.000 sec/step)\nINFO:tensorflow:global step 4: loss = 15.2100 (0.000 sec/step)\nINFO:tensorflow:global step 5: loss = 27.0400 (0.016 sec/step)\nINFO:tensorflow:global step 6: loss = 42.2500 (0.000 sec/step)\nINFO:tensorflow:global step 7: loss = 60.8400 (0.000 sec/step)\nINFO:tensorflow:global step 8: loss = 0.0433 (0.000 sec/step)\nINFO:tensorflow:global step 9: loss = 0.0126 (0.000 sec/step)\nINFO:tensorflow:global step 10: loss = 0.0068 (0.000 sec/step)\nINFO:tensorflow:Stopping Training.\nINFO:tensorflow:Finished training! Saving model to disk.\n\nIf I disabled the \"tf.where\" line, then the outputs are:\nINFO:tensorflow:Starting Session.\nINFO:tensorflow:global_step/sec: inf\nINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\nINFO:tensorflow:Starting Queues.\nINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\nINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\nINFO:tensorflow:global step 3: loss = 5.8467 (0.000 sec/step)\nINFO:tensorflow:global step 4: loss = 9.2253 (0.000 sec/step)\nINFO:tensorflow:global step 5: loss = 8.2057 (0.000 sec/step)\nINFO:tensorflow:global step 6: loss = 3.7965 (0.000 sec/step)\nINFO:tensorflow:global step 7: loss = 0.7162 (0.000 sec/step)\nINFO:tensorflow:global step 8: loss = 0.0489 (0.000 sec/step)\nINFO:tensorflow:global step 9: loss = 0.0143 (0.000 sec/step)\nINFO:tensorflow:global step 10: loss = 0.0077 (0.000 sec/step)\nINFO:tensorflow:Stopping Training.\nINFO:tensorflow:Finished training! Saving model to disk.\n\nIt seems that the stop_gradient did not freeze the updating of parameters, leading to loss change; however, it did influence somewhat unexpected, making the loss different. What should I do to freeze the updating in 5 steps? Thank you.", "body": "Hello, I made a simple test as below\r\n\r\n\t  TRAIN_STEP_NUM = 5\r\n\t  x_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n\t  y_train = [0, -1, -2, -3, -4, -5, -6, -7, -8, -9]\r\n\r\n\t  dataset = tf.data.Dataset.from_tensor_slices(\r\n\t\t  (tf.cast(x_train, tf.float32), tf.cast(y_train, tf.float32)))\r\n\r\n\t  with tf.Graph().as_default():\r\n\t\titerator = dataset.batch(1).make_one_shot_iterator()\r\n\t\tx, y = iterator.get_next()\r\n\t\t\r\n\t\tW = tf.Variable([.3], dtype=tf.float32)\r\n\t\tb = tf.Variable([-.3], dtype=tf.float32)\r\n\t\tlinear_model = W*x + b\r\n\t\tloss = tf.reduce_sum(tf.square(linear_model - y))\r\n\r\n\t\tloss = tf.where(\r\n\t\t  tf.greater(tf.train.get_or_create_global_step(), TRAIN_STEP_NUM),\r\n\t\t\t\t\t loss, tf.stop_gradient(loss))\r\n\t\tslim.losses.add_loss(loss)\r\n\t  \r\n\t\toptimizer = tf.train.GradientDescentOptimizer(0.01)\r\n\t\ttrain_op = slim.learning.create_train_op(loss, optimizer)\r\n\r\n\t\tslim.learning.train(train_op, './tmp',\r\n\t\t\t\t\t\t\tlog_every_n_steps=1,\r\n\t\t\t\t\t\t\tnumber_of_steps=10)\r\n\r\nThe outputs are:\r\n\r\n\tINFO:tensorflow:Starting Session.\r\n\tINFO:tensorflow:global_step/sec: inf\r\n\tINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\r\n\tINFO:tensorflow:Starting Queues.\r\n\tINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\r\n\tINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 3: loss = 6.7600 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 4: loss = 15.2100 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 5: loss = 27.0400 (0.016 sec/step)\r\n\tINFO:tensorflow:global step 6: loss = 42.2500 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 7: loss = 60.8400 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 8: loss = 0.0433 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 9: loss = 0.0126 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 10: loss = 0.0068 (0.000 sec/step)\r\n\tINFO:tensorflow:Stopping Training.\r\n\tINFO:tensorflow:Finished training! Saving model to disk.\r\n\r\nIf I disabled the \"tf.where\" line, then the outputs are:\r\n\r\n\tINFO:tensorflow:Starting Session.\r\n\tINFO:tensorflow:global_step/sec: inf\r\n\tINFO:tensorflow:Saving checkpoint to path ./tmp\\model.ckpt\r\n\tINFO:tensorflow:Starting Queues.\r\n\tINFO:tensorflow:global step 1: loss = 0.0000 (0.016 sec/step)\r\n\tINFO:tensorflow:global step 2: loss = 1.6900 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 3: loss = 5.8467 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 4: loss = 9.2253 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 5: loss = 8.2057 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 6: loss = 3.7965 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 7: loss = 0.7162 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 8: loss = 0.0489 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 9: loss = 0.0143 (0.000 sec/step)\r\n\tINFO:tensorflow:global step 10: loss = 0.0077 (0.000 sec/step)\r\n\tINFO:tensorflow:Stopping Training.\r\n\tINFO:tensorflow:Finished training! Saving model to disk.\r\n\r\nIt seems that the stop_gradient did not freeze the updating of parameters, leading to loss change; however, it did influence somewhat unexpected, making the loss different. What should I do to freeze the updating in 5 steps? Thank you."}