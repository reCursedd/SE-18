{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341593090", "html_url": "https://github.com/tensorflow/tensorflow/issues/14162#issuecomment-341593090", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14162", "id": 341593090, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTU5MzA5MA==", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T00:01:59Z", "updated_at": "2017-11-03T15:24:43Z", "author_association": "NONE", "body_html": "<p>Hello, I just made some simple tests and find that the tf.where may not satisfy my need. I want the gradient of a loss to stop back propagation in early rounds, but continue the BP in later rounds.</p>\n<p>As loss is usually defined before a session is really created, if I use codes like<br>\nloss = tf.where(math_ops.greater_equal(global_step, STEP_THRESHOLD), loss, tf.stop_gradient(loss))</p>\n<p>I found that the gradient still passes when global_step &lt; STEP_THRESHOLD. This does not meet my requirement.</p>\n<p>Is there anyway to do this (using global_step to control \"stop_gradient\")? Or a new resume_gradient feature is still needed? Thank you.</p>", "body_text": "Hello, I just made some simple tests and find that the tf.where may not satisfy my need. I want the gradient of a loss to stop back propagation in early rounds, but continue the BP in later rounds.\nAs loss is usually defined before a session is really created, if I use codes like\nloss = tf.where(math_ops.greater_equal(global_step, STEP_THRESHOLD), loss, tf.stop_gradient(loss))\nI found that the gradient still passes when global_step < STEP_THRESHOLD. This does not meet my requirement.\nIs there anyway to do this (using global_step to control \"stop_gradient\")? Or a new resume_gradient feature is still needed? Thank you.", "body": "Hello, I just made some simple tests and find that the tf.where may not satisfy my need. I want the gradient of a loss to stop back propagation in early rounds, but continue the BP in later rounds. \r\n\r\nAs loss is usually defined before a session is really created, if I use codes like\r\n    loss = tf.where(math_ops.greater_equal(global_step, STEP_THRESHOLD), loss, tf.stop_gradient(loss))\r\n\r\nI found that the gradient still passes when global_step < STEP_THRESHOLD. This does not meet my requirement. \r\n\r\nIs there anyway to do this (using global_step to control \"stop_gradient\")? Or a new resume_gradient feature is still needed? Thank you."}