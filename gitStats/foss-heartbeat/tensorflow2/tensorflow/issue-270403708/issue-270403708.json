{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14162", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14162/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14162/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14162/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14162", "id": 270403708, "node_id": "MDU6SXNzdWUyNzA0MDM3MDg=", "number": 14162, "title": "[Feature Request] tf.resume_gradient", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-11-01T18:23:36Z", "updated_at": "2017-11-06T16:45:15Z", "closed_at": "2017-11-06T16:45:15Z", "author_association": "NONE", "body_html": "<p>tf.stop_gradient provides a convenient way for various uses. However, sometimes, it may be useful to resume the gradient passing after some conditions are met.</p>\n<p>For example, in Faster-RCNN algorithm, the RPN layers' gradients are stopped because they may cause unstable results in early rounds. However, this indicates that these layers' weights would be kept the initial random values forever. If we could resume the gradients back propagation after several steps, the results may be better. Therefore, I believe that the resume_gradient function would be useful. Thank you.</p>", "body_text": "tf.stop_gradient provides a convenient way for various uses. However, sometimes, it may be useful to resume the gradient passing after some conditions are met.\nFor example, in Faster-RCNN algorithm, the RPN layers' gradients are stopped because they may cause unstable results in early rounds. However, this indicates that these layers' weights would be kept the initial random values forever. If we could resume the gradients back propagation after several steps, the results may be better. Therefore, I believe that the resume_gradient function would be useful. Thank you.", "body": "tf.stop_gradient provides a convenient way for various uses. However, sometimes, it may be useful to resume the gradient passing after some conditions are met. \r\n\r\nFor example, in Faster-RCNN algorithm, the RPN layers' gradients are stopped because they may cause unstable results in early rounds. However, this indicates that these layers' weights would be kept the initial random values forever. If we could resume the gradients back propagation after several steps, the results may be better. Therefore, I believe that the resume_gradient function would be useful. Thank you."}