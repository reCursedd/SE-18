{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4246", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4246/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4246/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4246/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4246", "id": 175438172, "node_id": "MDU6SXNzdWUxNzU0MzgxNzI=", "number": 4246, "title": "tf.train.Coordinator not closing threads", "user": {"login": "tomrunia", "id": 5536129, "node_id": "MDQ6VXNlcjU1MzYxMjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5536129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomrunia", "html_url": "https://github.com/tomrunia", "followers_url": "https://api.github.com/users/tomrunia/followers", "following_url": "https://api.github.com/users/tomrunia/following{/other_user}", "gists_url": "https://api.github.com/users/tomrunia/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomrunia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomrunia/subscriptions", "organizations_url": "https://api.github.com/users/tomrunia/orgs", "repos_url": "https://api.github.com/users/tomrunia/repos", "events_url": "https://api.github.com/users/tomrunia/events{/privacy}", "received_events_url": "https://api.github.com/users/tomrunia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-09-07T08:04:55Z", "updated_at": "2016-09-08T13:41:47Z", "closed_at": "2016-09-08T13:41:47Z", "author_association": "NONE", "body_html": "<p>I am using a number of threads to feed training examples to a <code>tf.RandomShuffleQueue</code>, however gracefully closing the threads seems not to be working nicely. This is related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"151393738\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2130\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2130/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2130\">#2130</a> and <a href=\"http://stackoverflow.com/questions/36210162/tensorflow-stopping-threads-via-coordinator-seems-not-to-work\" rel=\"nofollow\">this question on StackOverflow</a> however I am wondering whether there is going to be a better way of closing the threads.</p>\n<p>The problem is that there are pending enqueue operations when you request the threads to stop. You can force these to be cancelled using <code>queue.close(cancel_pending_enqueues=True)</code>, but then the threads raise an <code>tf.errors.CancelledError</code>. As a work around you need to surround the enqueue operation with a try/except block, however the whole process is not very elegant. I am wondering whether there are any plans to improve the process of closing the threads using the Coordinator.</p>\n<p>The code below illustrates a working example, however as you can see it is not a very nice solution.</p>\n<div class=\"highlight highlight-source-python\"><pre>q <span class=\"pl-k\">=</span> tf.RandomShuffleQueue( <span class=\"pl-c1\">...</span> )\nenqueue_op <span class=\"pl-k\">=</span> q.enqueue_many([<span class=\"pl-c1\">self</span>.queue_inputs, <span class=\"pl-c1\">self</span>.queue_targets])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">load_and_enqueue</span>( ... ):\n\n    <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> coord.should_stop():\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> ...</span>\n        feed_dict <span class=\"pl-k\">=</span> {\n            queue_inputs:  inputs,\n            queue_targets: targets\n         }\n\n         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Catch the exception that arises when you ask to close pending enqueue operations</span>\n         <span class=\"pl-k\">try</span>:\n             sess.run(enqueue_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed_dict)\n         <span class=\"pl-k\">except</span> tf.errors.CancelledError:\n             <span class=\"pl-k\">return</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Coordinator for threads</span>\ncoord <span class=\"pl-k\">=</span> tf.train.Coordinator()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Start a thread to enqueue data asynchronously, and hide I/O latency.</span>\nthreads <span class=\"pl-k\">=</span> [threading.Thread(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>load_and_enqueue, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">...</span>,)) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">4</span>)]\n\n<span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> threads: t.start()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ... training loop</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Ask the threads to stop and wait until they do it</span>\nsess.run(q.close(<span class=\"pl-v\">cancel_pending_enqueues</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\ncoord.request_stop()\ncoord.join(threads, <span class=\"pl-v\">stop_grace_period_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n\nsess.close()</pre></div>", "body_text": "I am using a number of threads to feed training examples to a tf.RandomShuffleQueue, however gracefully closing the threads seems not to be working nicely. This is related to #2130 and this question on StackOverflow however I am wondering whether there is going to be a better way of closing the threads.\nThe problem is that there are pending enqueue operations when you request the threads to stop. You can force these to be cancelled using queue.close(cancel_pending_enqueues=True), but then the threads raise an tf.errors.CancelledError. As a work around you need to surround the enqueue operation with a try/except block, however the whole process is not very elegant. I am wondering whether there are any plans to improve the process of closing the threads using the Coordinator.\nThe code below illustrates a working example, however as you can see it is not a very nice solution.\nq = tf.RandomShuffleQueue( ... )\nenqueue_op = q.enqueue_many([self.queue_inputs, self.queue_targets])\n\ndef load_and_enqueue( ... ):\n\n    while not coord.should_stop():\n        # ...\n        feed_dict = {\n            queue_inputs:  inputs,\n            queue_targets: targets\n         }\n\n         # Catch the exception that arises when you ask to close pending enqueue operations\n         try:\n             sess.run(enqueue_op, feed_dict=feed_dict)\n         except tf.errors.CancelledError:\n             return\n\n# Coordinator for threads\ncoord = tf.train.Coordinator()\n\n# Start a thread to enqueue data asynchronously, and hide I/O latency.\nthreads = [threading.Thread(target=load_and_enqueue, args=(...,)) for i in range(4)]\n\nfor t in threads: t.start()\n\n# ... training loop\n\n# Ask the threads to stop and wait until they do it\nsess.run(q.close(cancel_pending_enqueues=True))\ncoord.request_stop()\ncoord.join(threads, stop_grace_period_secs=5)\n\nsess.close()", "body": "I am using a number of threads to feed training examples to a `tf.RandomShuffleQueue`, however gracefully closing the threads seems not to be working nicely. This is related to https://github.com/tensorflow/tensorflow/issues/2130 and [this question on StackOverflow](http://stackoverflow.com/questions/36210162/tensorflow-stopping-threads-via-coordinator-seems-not-to-work) however I am wondering whether there is going to be a better way of closing the threads.\n\nThe problem is that there are pending enqueue operations when you request the threads to stop. You can force these to be cancelled using `queue.close(cancel_pending_enqueues=True)`, but then the threads raise an `tf.errors.CancelledError`. As a work around you need to surround the enqueue operation with a try/except block, however the whole process is not very elegant. I am wondering whether there are any plans to improve the process of closing the threads using the Coordinator. \n\nThe code below illustrates a working example, however as you can see it is not a very nice solution.\n\n``` python\n\nq = tf.RandomShuffleQueue( ... )\nenqueue_op = q.enqueue_many([self.queue_inputs, self.queue_targets])\n\ndef load_and_enqueue( ... ):\n\n    while not coord.should_stop():\n        # ...\n        feed_dict = {\n            queue_inputs:  inputs,\n            queue_targets: targets\n         }\n\n         # Catch the exception that arises when you ask to close pending enqueue operations\n         try:\n             sess.run(enqueue_op, feed_dict=feed_dict)\n         except tf.errors.CancelledError:\n             return\n\n# Coordinator for threads\ncoord = tf.train.Coordinator()\n\n# Start a thread to enqueue data asynchronously, and hide I/O latency.\nthreads = [threading.Thread(target=load_and_enqueue, args=(...,)) for i in range(4)]\n\nfor t in threads: t.start()\n\n# ... training loop\n\n# Ask the threads to stop and wait until they do it\nsess.run(q.close(cancel_pending_enqueues=True))\ncoord.request_stop()\ncoord.join(threads, stop_grace_period_secs=5)\n\nsess.close()\n```\n"}