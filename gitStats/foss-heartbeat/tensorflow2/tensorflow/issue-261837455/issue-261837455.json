{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13419", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13419/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13419/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13419/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13419", "id": 261837455, "node_id": "MDU6SXNzdWUyNjE4Mzc0NTU=", "number": 13419, "title": "BUG: variables outside won't update in DNNLinearCombinedRegressor", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-30T11:14:24Z", "updated_at": "2017-12-27T17:58:27Z", "closed_at": "2017-12-27T17:58:27Z", "author_association": "MEMBER", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac 10.11.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Variables outside won't update for <code>DNNLinearCombinedRegressor</code>, while everything is OK for <code>DNNRegressor</code>.</p>\n<p>The bug stems from that only variables in dnn / linear scope will be updated in the code below:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/estimator/canned/dnn_linear_combined.py#L214-L227\">tensorflow/tensorflow/python/estimator/canned/dnn_linear_combined.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 214 to 227\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/107cc777af7880c140d089e44ad898a6ba929286\">107cc77</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L214\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"214\"></td>\n          <td id=\"LC214\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> dnn_logits <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L215\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"215\"></td>\n          <td id=\"LC215\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   train_ops.append( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L216\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"216\"></td>\n          <td id=\"LC216\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       dnn_optimizer.minimize( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L217\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"217\"></td>\n          <td id=\"LC217\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           loss, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L218\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"218\"></td>\n          <td id=\"LC218\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>ops.get_collection( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L219\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"219\"></td>\n          <td id=\"LC219\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">               ops.GraphKeys.<span class=\"pl-c1\">TRAINABLE_VARIABLES</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L220\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"220\"></td>\n          <td id=\"LC220\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">               <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>dnn_parent_scope))) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L221\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"221\"></td>\n          <td id=\"LC221\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> linear_logits <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L222\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"222\"></td>\n          <td id=\"LC222\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   train_ops.append( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L223\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"223\"></td>\n          <td id=\"LC223\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       linear_optimizer.minimize( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L224\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"224\"></td>\n          <td id=\"LC224\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           loss, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L225\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"225\"></td>\n          <td id=\"LC225\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>ops.get_collection( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L226\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"226\"></td>\n          <td id=\"LC226\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">               ops.GraphKeys.<span class=\"pl-c1\">TRAINABLE_VARIABLES</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L227\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"227\"></td>\n          <td id=\"LC227\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">               <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>linear_parent_scope))) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<h3>Source code / logs</h3>\n<p>This is a tiny code to see whether <code>w</code> is updated or not.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow <span class=\"pl-k\">import</span> feature_column <span class=\"pl-k\">as</span> fc\n<span class=\"pl-k\">from</span> tensorflow.python.summary <span class=\"pl-k\">import</span> summary\n\ntf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">DEBUG</span>)\n\n\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">input_fn</span>():\n    x <span class=\"pl-k\">=</span> tf.constant(np.random.randn(<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    w <span class=\"pl-k\">=</span> tf.Variable(np.array([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>]).reshape((<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test/w<span class=\"pl-pds\">\"</span></span>)\n    summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test/w_0_0<span class=\"pl-pds\">\"</span></span>, w[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">0</span>])\n    summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test/w_1_0<span class=\"pl-pds\">\"</span></span>, w[<span class=\"pl-c1\">1</span>][<span class=\"pl-c1\">0</span>])\n    summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test/w_2_0<span class=\"pl-pds\">\"</span></span>, w[<span class=\"pl-c1\">2</span>][<span class=\"pl-c1\">0</span>])\n    summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test/w_3_0<span class=\"pl-pds\">\"</span></span>, w[<span class=\"pl-c1\">3</span>][<span class=\"pl-c1\">0</span>])\n\n    y <span class=\"pl-k\">=</span> tf.matmul(x, w)\n    label <span class=\"pl-k\">=</span> tf.constant(np.random.randint(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">BATCH_SIZE</span>,)))\n\n    <span class=\"pl-k\">return</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: y}, label\n\nf <span class=\"pl-k\">=</span> fc.numeric_column(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">gen_estimator</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">cls</span></span>, <span class=\"pl-smi\">model_dir</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">cls</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dnn<span class=\"pl-pds\">\"</span></span>:\n        <span class=\"pl-k\">return</span> tf.estimator.DNNRegressor(\n                <span class=\"pl-v\">feature_columns</span><span class=\"pl-k\">=</span>[f],\n                <span class=\"pl-v\">hidden_units</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>],\n                <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span>model_dir)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> tf.estimator.DNNLinearCombinedRegressor(\n                <span class=\"pl-v\">dnn_feature_columns</span><span class=\"pl-k\">=</span>[f],\n                <span class=\"pl-v\">dnn_hidden_units</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>],\n                <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span>model_dir)\n\ngen_estimator(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dnn<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/tmp/tf/facai/test_dnn<span class=\"pl-pds\">\"</span></span>).train(input_fn, <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\ngen_estimator(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>deep_and_wide<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/tmp/tf/facai/test_wide_and_deep<span class=\"pl-pds\">\"</span></span>).train(input_fn, <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)</pre></div>\n<h5>Results</h5>\n<ul>\n<li>For <code>DNNRegressor</code>, <code>w</code> variables are updated:</li>\n</ul>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1112263/31045300-20944e50-a613-11e7-9f5d-611b5fde5de8.png\"><img width=\"688\" alt=\"dnn\" src=\"https://user-images.githubusercontent.com/1112263/31045300-20944e50-a613-11e7-9f5d-611b5fde5de8.png\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li>For <code>DNNLinearCombinedRegressor</code>, <code>w</code> keeps constant:</li>\n</ul>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1112263/31045305-5a821192-a613-11e7-8610-7a6d312e2552.png\"><img width=\"690\" alt=\"deep_and_wide\" src=\"https://user-images.githubusercontent.com/1112263/31045305-5a821192-a613-11e7-8610-7a6d312e2552.png\" style=\"max-width:100%;\"></a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.11.6\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.3.0\nPython version: 3.5\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nVariables outside won't update for DNNLinearCombinedRegressor, while everything is OK for DNNRegressor.\nThe bug stems from that only variables in dnn / linear scope will be updated in the code below:\n\n  \n    \n      tensorflow/tensorflow/python/estimator/canned/dnn_linear_combined.py\n    \n    \n        Lines 214 to 227\n      in\n      107cc77\n    \n    \n    \n    \n\n        \n          \n           if dnn_logits is not None: \n        \n\n        \n          \n             train_ops.append( \n        \n\n        \n          \n                 dnn_optimizer.minimize( \n        \n\n        \n          \n                     loss, \n        \n\n        \n          \n                     var_list=ops.get_collection( \n        \n\n        \n          \n                         ops.GraphKeys.TRAINABLE_VARIABLES, \n        \n\n        \n          \n                         scope=dnn_parent_scope))) \n        \n\n        \n          \n           if linear_logits is not None: \n        \n\n        \n          \n             train_ops.append( \n        \n\n        \n          \n                 linear_optimizer.minimize( \n        \n\n        \n          \n                     loss, \n        \n\n        \n          \n                     var_list=ops.get_collection( \n        \n\n        \n          \n                         ops.GraphKeys.TRAINABLE_VARIABLES, \n        \n\n        \n          \n                         scope=linear_parent_scope))) \n        \n    \n  \n\n\nSource code / logs\nThis is a tiny code to see whether w is updated or not.\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import feature_column as fc\nfrom tensorflow.python.summary import summary\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\n\n\nBATCH_SIZE = 4\n\n\ndef input_fn():\n    x = tf.constant(np.random.randn(BATCH_SIZE, 4), dtype=tf.float32)\n\n    w = tf.Variable(np.array([1, 2, 3, 4]).reshape((4, 1)), dtype=tf.float32, name=\"test/w\")\n    summary.scalar(\"test/w_0_0\", w[0][0])\n    summary.scalar(\"test/w_1_0\", w[1][0])\n    summary.scalar(\"test/w_2_0\", w[2][0])\n    summary.scalar(\"test/w_3_0\", w[3][0])\n\n    y = tf.matmul(x, w)\n    label = tf.constant(np.random.randint(0, 1, size=(BATCH_SIZE,)))\n\n    return {\"y\": y}, label\n\nf = fc.numeric_column(\"y\")\n\ndef gen_estimator(cls, model_dir):\n    if cls == \"dnn\":\n        return tf.estimator.DNNRegressor(\n                feature_columns=[f],\n                hidden_units=[2],\n                model_dir=model_dir)\n    else:\n        return tf.estimator.DNNLinearCombinedRegressor(\n                dnn_feature_columns=[f],\n                dnn_hidden_units=[2],\n                model_dir=model_dir)\n\ngen_estimator(\"dnn\", model_dir=\"/tmp/tf/facai/test_dnn\").train(input_fn, steps=1000)\ngen_estimator(\"deep_and_wide\", model_dir=\"/tmp/tf/facai/test_wide_and_deep\").train(input_fn, steps=1000)\nResults\n\nFor DNNRegressor, w variables are updated:\n\n\n\nFor DNNLinearCombinedRegressor, w keeps constant:", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\n\r\nVariables outside won't update for `DNNLinearCombinedRegressor`, while everything is OK for `DNNRegressor`.\r\n\r\nThe bug stems from that only variables in dnn / linear scope will be updated in the code below:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/estimator/canned/dnn_linear_combined.py#L214-L227\r\n\r\n\r\n### Source code / logs\r\n\r\nThis is a tiny code to see whether `w` is updated or not.\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column as fc\r\nfrom tensorflow.python.summary import summary\r\n\r\ntf.logging.set_verbosity(tf.logging.DEBUG)\r\n\r\n\r\nBATCH_SIZE = 4\r\n\r\n\r\ndef input_fn():\r\n    x = tf.constant(np.random.randn(BATCH_SIZE, 4), dtype=tf.float32)\r\n\r\n    w = tf.Variable(np.array([1, 2, 3, 4]).reshape((4, 1)), dtype=tf.float32, name=\"test/w\")\r\n    summary.scalar(\"test/w_0_0\", w[0][0])\r\n    summary.scalar(\"test/w_1_0\", w[1][0])\r\n    summary.scalar(\"test/w_2_0\", w[2][0])\r\n    summary.scalar(\"test/w_3_0\", w[3][0])\r\n\r\n    y = tf.matmul(x, w)\r\n    label = tf.constant(np.random.randint(0, 1, size=(BATCH_SIZE,)))\r\n\r\n    return {\"y\": y}, label\r\n\r\nf = fc.numeric_column(\"y\")\r\n\r\ndef gen_estimator(cls, model_dir):\r\n    if cls == \"dnn\":\r\n        return tf.estimator.DNNRegressor(\r\n                feature_columns=[f],\r\n                hidden_units=[2],\r\n                model_dir=model_dir)\r\n    else:\r\n        return tf.estimator.DNNLinearCombinedRegressor(\r\n                dnn_feature_columns=[f],\r\n                dnn_hidden_units=[2],\r\n                model_dir=model_dir)\r\n\r\ngen_estimator(\"dnn\", model_dir=\"/tmp/tf/facai/test_dnn\").train(input_fn, steps=1000)\r\ngen_estimator(\"deep_and_wide\", model_dir=\"/tmp/tf/facai/test_wide_and_deep\").train(input_fn, steps=1000)\r\n```\r\n\r\n##### Results\r\n\r\n+ For `DNNRegressor`, `w` variables are updated:\r\n\r\n<img width=\"688\" alt=\"dnn\" src=\"https://user-images.githubusercontent.com/1112263/31045300-20944e50-a613-11e7-9f5d-611b5fde5de8.png\">\r\n\r\n+ For `DNNLinearCombinedRegressor`, `w` keeps constant:\r\n\r\n<img width=\"690\" alt=\"deep_and_wide\" src=\"https://user-images.githubusercontent.com/1112263/31045305-5a821192-a613-11e7-8610-7a6d312e2552.png\">\r\n\r\n\r\n"}