{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323344297", "html_url": "https://github.com/tensorflow/tensorflow/issues/12314#issuecomment-323344297", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12314", "id": 323344297, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzM0NDI5Nw==", "user": {"login": "carlthome", "id": 1595907, "node_id": "MDQ6VXNlcjE1OTU5MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1595907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlthome", "html_url": "https://github.com/carlthome", "followers_url": "https://api.github.com/users/carlthome/followers", "following_url": "https://api.github.com/users/carlthome/following{/other_user}", "gists_url": "https://api.github.com/users/carlthome/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlthome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlthome/subscriptions", "organizations_url": "https://api.github.com/users/carlthome/orgs", "repos_url": "https://api.github.com/users/carlthome/repos", "events_url": "https://api.github.com/users/carlthome/events{/privacy}", "received_events_url": "https://api.github.com/users/carlthome/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-18T12:47:12Z", "updated_at": "2017-08-18T14:02:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15696327\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andydavis1\">@andydavis1</a>, cool. <g-emoji class=\"g-emoji\" alias=\"+1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44d.png\">\ud83d\udc4d</g-emoji></p>\n<p>FFT is an essential tool for signal processing. I hope it can be made a higher priority. <g-emoji class=\"g-emoji\" alias=\"koala\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f428.png\">\ud83d\udc28</g-emoji></p>\n<p>Considering <a href=\"https://github.com/tensorflow/tensorflow/blob/96af8b0b9738fb147b3b3ffe555f952b2f9e8ccf/tensorflow/core/kernels/fft_ops.cc#L116\">there's already GPU and CPU support for FFT in TensorFlow</a>, it seems like it should mostly be a matter of creating some additional classes for the XLA op?</p>\n<p>I'd love a brief description on how to go about adding XLA ops (as mentioned by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7657273\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/learyg\">@learyg</a> <a href=\"https://github.com/tensorflow/tensorflow/issues/11905#issuecomment-319277435\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11905/hovercard\">here</a>).</p>\n<p>Just skimming the source, I guess it's something like this:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">FftOp</span> : <span class=\"pl-k\">public</span> <span class=\"pl-en\">XlaOpKernel</span> {\n <span class=\"pl-k\">public:</span>\n  <span class=\"pl-k\">explicit</span> <span class=\"pl-en\">FftOp</span>(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {}\n\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">Compile</span>(XlaOpKernelContext* ctx) <span class=\"pl-k\">override</span> {\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> y = tf.fft(x)</span>\n    <span class=\"pl-k\">auto</span> x = ctx-&gt;<span class=\"pl-c1\">Input</span>(<span class=\"pl-c1\">0</span>);\n    <span class=\"pl-k\">auto</span> y = ctx-&gt;<span class=\"pl-c1\">builder</span>()-&gt;<span class=\"pl-c1\">Fft</span>(x);\n    ctx-&gt;<span class=\"pl-c1\">SetOutput</span>(<span class=\"pl-c1\">0</span>, y);\n  }\n}<span class=\"pl-ii\"></span></pre></div>\n<p>and this</p>\n<div class=\"highlight highlight-source-c++\"><pre>ComputationDataHandle <span class=\"pl-en\">ComputationBuilder::Fft</span>(\n    <span class=\"pl-k\">const</span> ComputationDataHandle&amp; operand) {\n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">UnaryOp</span>(UNOP_FFT, operand);\n}</pre></div>\n<p>etc.</p>", "body_text": "@andydavis1, cool. \ud83d\udc4d\nFFT is an essential tool for signal processing. I hope it can be made a higher priority. \ud83d\udc28\nConsidering there's already GPU and CPU support for FFT in TensorFlow, it seems like it should mostly be a matter of creating some additional classes for the XLA op?\nI'd love a brief description on how to go about adding XLA ops (as mentioned by @learyg here).\nJust skimming the source, I guess it's something like this:\nclass FftOp : public XlaOpKernel {\n public:\n  explicit FftOp(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {}\n\n  void Compile(XlaOpKernelContext* ctx) override {\n    // y = tf.fft(x)\n    auto x = ctx->Input(0);\n    auto y = ctx->builder()->Fft(x);\n    ctx->SetOutput(0, y);\n  }\n}\nand this\nComputationDataHandle ComputationBuilder::Fft(\n    const ComputationDataHandle& operand) {\n  return UnaryOp(UNOP_FFT, operand);\n}\netc.", "body": "@andydavis1, cool. :+1: \r\n\r\nFFT is an essential tool for signal processing. I hope it can be made a higher priority. :koala:\r\n\r\nConsidering [there's already GPU and CPU support for FFT in TensorFlow](https://github.com/tensorflow/tensorflow/blob/96af8b0b9738fb147b3b3ffe555f952b2f9e8ccf/tensorflow/core/kernels/fft_ops.cc#L116), it seems like it should mostly be a matter of creating some additional classes for the XLA op?\r\n\r\nI'd love a brief description on how to go about adding XLA ops (as mentioned by @learyg [here](https://github.com/tensorflow/tensorflow/issues/11905#issuecomment-319277435)).\r\n\r\nJust skimming the source, I guess it's something like this:\r\n```cc\r\nclass FftOp : public XlaOpKernel {\r\n public:\r\n  explicit FftOp(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {}\r\n\r\n  void Compile(XlaOpKernelContext* ctx) override {\r\n    // y = tf.fft(x)\r\n    auto x = ctx->Input(0);\r\n    auto y = ctx->builder()->Fft(x);\r\n    ctx->SetOutput(0, y);\r\n  }\r\n}\r\n```\r\nand this \r\n```cc\r\nComputationDataHandle ComputationBuilder::Fft(\r\n    const ComputationDataHandle& operand) {\r\n  return UnaryOp(UNOP_FFT, operand);\r\n}\r\n```\r\netc."}