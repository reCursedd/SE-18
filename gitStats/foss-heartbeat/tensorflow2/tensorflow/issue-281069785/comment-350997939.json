{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/350997939", "html_url": "https://github.com/tensorflow/tensorflow/issues/15278#issuecomment-350997939", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15278", "id": 350997939, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDk5NzkzOQ==", "user": {"login": "sleighsoft", "id": 9438971, "node_id": "MDQ6VXNlcjk0Mzg5NzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/9438971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sleighsoft", "html_url": "https://github.com/sleighsoft", "followers_url": "https://api.github.com/users/sleighsoft/followers", "following_url": "https://api.github.com/users/sleighsoft/following{/other_user}", "gists_url": "https://api.github.com/users/sleighsoft/gists{/gist_id}", "starred_url": "https://api.github.com/users/sleighsoft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sleighsoft/subscriptions", "organizations_url": "https://api.github.com/users/sleighsoft/orgs", "repos_url": "https://api.github.com/users/sleighsoft/repos", "events_url": "https://api.github.com/users/sleighsoft/events{/privacy}", "received_events_url": "https://api.github.com/users/sleighsoft/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-12T09:41:20Z", "updated_at": "2017-12-12T09:41:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I cannot share the whole code, it is rather large.<br>\nBut it comes from the <code>TrainingHelper</code></p>\n<div class=\"highlight highlight-source-python\"><pre>embedded_input <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(\n            <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>embedding,\n            <span class=\"pl-v\">ids</span><span class=\"pl-k\">=</span>input_data)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Internally it has a `sample` method that uses argmax</span>\nhelper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(\n              <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>embedded_input,\n              <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_length,\n              <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.time_major)\n\nrnn_initial_state <span class=\"pl-k\">=</span> tf.zeros([batch_size, <span class=\"pl-c1\">self</span>.cell_\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\n          rnn_initial_state, rnn_initial_state)\n\nrnn_initial_state = tf.zeros([batch_size, <span class=\"pl-c1\">self</span>.cell_size])\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\n    rnn_initial_state, rnn_initial_state)\n\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    <span class=\"pl-v\">cell</span><span class=\"pl-k\">=</span>rnn_cell,\n    <span class=\"pl-v\">helper</span><span class=\"pl-k\">=</span>helper,\n    <span class=\"pl-v\">initial_state</span><span class=\"pl-k\">=</span>rnn_initial_state,\n    <span class=\"pl-v\">output_layer</span><span class=\"pl-k\">=</span>output_projection)\nfinal_outputs, state, _ = tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span><span class=\"pl-k\">=</span>decoder,\n    <span class=\"pl-v\">maximum_iterations</span><span class=\"pl-k\">=</span>maximum_iterations,\n    <span class=\"pl-v\">output_time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.time_major,\n    <span class=\"pl-v\">swap_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope)</pre></div>", "body_text": "I cannot share the whole code, it is rather large.\nBut it comes from the TrainingHelper\nembedded_input = tf.nn.embedding_lookup(\n            params=embedding,\n            ids=input_data)\n\n# Internally it has a `sample` method that uses argmax\nhelper = tf.contrib.seq2seq.TrainingHelper(\n              inputs=embedded_input,\n              sequence_length=sequence_length,\n              time_major=self.time_major)\n\nrnn_initial_state = tf.zeros([batch_size, self.cell_\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\n          rnn_initial_state, rnn_initial_state)\n\nrnn_initial_state = tf.zeros([batch_size, self.cell_size])\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\n    rnn_initial_state, rnn_initial_state)\n\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    cell=rnn_cell,\n    helper=helper,\n    initial_state=rnn_initial_state,\n    output_layer=output_projection)\nfinal_outputs, state, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder=decoder,\n    maximum_iterations=maximum_iterations,\n    output_time_major=self.time_major,\n    swap_memory=True,\n    scope=scope)", "body": "I cannot share the whole code, it is rather large.\r\nBut it comes from the `TrainingHelper`\r\n\r\n```python\r\nembedded_input = tf.nn.embedding_lookup(\r\n            params=embedding,\r\n            ids=input_data)\r\n\r\n# Internally it has a `sample` method that uses argmax\r\nhelper = tf.contrib.seq2seq.TrainingHelper(\r\n              inputs=embedded_input,\r\n              sequence_length=sequence_length,\r\n              time_major=self.time_major)\r\n\r\nrnn_initial_state = tf.zeros([batch_size, self.cell_\r\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\r\n          rnn_initial_state, rnn_initial_state)\r\n\r\nrnn_initial_state = tf.zeros([batch_size, self.cell_size])\r\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\r\n    rnn_initial_state, rnn_initial_state)\r\n\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell=rnn_cell,\r\n    helper=helper,\r\n    initial_state=rnn_initial_state,\r\n    output_layer=output_projection)\r\nfinal_outputs, state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder=decoder,\r\n    maximum_iterations=maximum_iterations,\r\n    output_time_major=self.time_major,\r\n    swap_memory=True,\r\n    scope=scope)\r\n```"}