{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/349299316", "html_url": "https://github.com/tensorflow/tensorflow/issues/15046#issuecomment-349299316", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15046", "id": 349299316, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTI5OTMxNg==", "user": {"login": "ngc92", "id": 7938269, "node_id": "MDQ6VXNlcjc5MzgyNjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/7938269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngc92", "html_url": "https://github.com/ngc92", "followers_url": "https://api.github.com/users/ngc92/followers", "following_url": "https://api.github.com/users/ngc92/following{/other_user}", "gists_url": "https://api.github.com/users/ngc92/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngc92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngc92/subscriptions", "organizations_url": "https://api.github.com/users/ngc92/orgs", "repos_url": "https://api.github.com/users/ngc92/repos", "events_url": "https://api.github.com/users/ngc92/events{/privacy}", "received_events_url": "https://api.github.com/users/ngc92/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-05T13:09:21Z", "updated_at": "2017-12-05T13:09:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>that wouldn't change anything. <code>weights + 0</code> is still differentiable w.r.t. <code>weights</code>. Also, just to clarify, my question is not how to stop <code>tf.losses.*</code> to derive at the weights, that would be a simple <code>tf.stop_gradient</code>, but whether it is conceptually correct that I would need to do so.</p>\n<p>Having a weighted loss <code>L(x, w)</code> with values <code>x</code> and weights <code>w</code> both depending on a parameter vector <code>P</code>, which of the following would be considered correct</p>\n<pre><code>dL/dP = dL/dx * dx/dP\n</code></pre>\n<p>or</p>\n<pre><code>dL/dP = dL/dx * dx/dP + dL/dw * dw/dP\n</code></pre>\n<p>I would argue that the first one corresponds to the interpretation of  <code>w</code> as weights.</p>", "body_text": "that wouldn't change anything. weights + 0 is still differentiable w.r.t. weights. Also, just to clarify, my question is not how to stop tf.losses.* to derive at the weights, that would be a simple tf.stop_gradient, but whether it is conceptually correct that I would need to do so.\nHaving a weighted loss L(x, w) with values x and weights w both depending on a parameter vector P, which of the following would be considered correct\ndL/dP = dL/dx * dx/dP\n\nor\ndL/dP = dL/dx * dx/dP + dL/dw * dw/dP\n\nI would argue that the first one corresponds to the interpretation of  w as weights.", "body": "that wouldn't change anything. `weights + 0` is still differentiable w.r.t. `weights`. Also, just to clarify, my question is not how to stop `tf.losses.*` to derive at the weights, that would be a simple `tf.stop_gradient`, but whether it is conceptually correct that I would need to do so.\r\n\r\nHaving a weighted loss `L(x, w)` with values `x` and weights `w` both depending on a parameter vector `P`, which of the following would be considered correct\r\n```\r\ndL/dP = dL/dx * dx/dP\r\n```\r\nor\r\n```\r\ndL/dP = dL/dx * dx/dP + dL/dw * dw/dP\r\n```\r\n\r\nI would argue that the first one corresponds to the interpretation of  `w` as weights."}