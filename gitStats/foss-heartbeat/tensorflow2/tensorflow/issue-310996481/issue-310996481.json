{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18216", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18216/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18216/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18216/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18216", "id": 310996481, "node_id": "MDU6SXNzdWUzMTA5OTY0ODE=", "number": 18216, "title": " tf.space_to_depth alters input type if its quantized.  But why?", "user": {"login": "imatensor", "id": 37885429, "node_id": "MDQ6VXNlcjM3ODg1NDI5", "avatar_url": "https://avatars2.githubusercontent.com/u/37885429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/imatensor", "html_url": "https://github.com/imatensor", "followers_url": "https://api.github.com/users/imatensor/followers", "following_url": "https://api.github.com/users/imatensor/following{/other_user}", "gists_url": "https://api.github.com/users/imatensor/gists{/gist_id}", "starred_url": "https://api.github.com/users/imatensor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/imatensor/subscriptions", "organizations_url": "https://api.github.com/users/imatensor/orgs", "repos_url": "https://api.github.com/users/imatensor/repos", "events_url": "https://api.github.com/users/imatensor/events{/privacy}", "received_events_url": "https://api.github.com/users/imatensor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-04-03T20:58:39Z", "updated_at": "2018-08-18T06:10:32Z", "closed_at": "2018-08-18T06:10:32Z", "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:  stock from the tf.space_to_depth() example as given on tensorflow.org, with output directed to tf.quantize()</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>: binary</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:</p>\n</li>\n</ul>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>tf.GIT_VERSION<br>\n\"b'unknown'\"<br>\ntf.VERSION<br>\n'1.6.0'</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<ul>\n<li><strong>Python version</strong>: , Python 3.5.4 Tk 8.6.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>: dst = tf.dequantize(qstd,qmin,qmax)  see the script below please.</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>tf.depth_to_space() doesnt return the input dtype when it is dtype=tf.qint8.  The type is changed to tf.int8.</p>\n<h3>Source code / logs</h3>\n<blockquote>\n<blockquote>\n<blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>Source:</p>\n<h1></h1>\n<p>import tensorflow as tf<br>\nsess = tf.Session()</p>\n<p>dt=tf.float32<br>\nbox=2<br>\nslist = [(1,2,2,1),(1,2,2,3),(1,4,4,1),(2,2,4,1,)]<br>\ndlist = [(1,1,1,4),(1,1,1,12),(1,2,2,4)]<br>\nsshape=2</p>\n<p>space_t=tf.random_uniform(slist[sshape],minval=-10, maxval=10,dtype=tf.float32)</p>\n<p>print(sess.run(space_t))</p>\n<p>ftin = tf.reshape(space_t,[-1])<br>\nmax=ftin[tf.argmax(ftin)]<br>\nmin=ftin[tf.argmin(ftin)]</p>\n<p>aq_op = tf.quantize(space_t,min,max,tf.qint8,name=\"TFQuantize\")<br>\naq_space_t = sess.run(aq_op)</p>\n<h1>The following qstd op should yield a type the same as aq_space_t.output, but does not, according to documentation.</h1>\n<p>qstd = tf.space_to_depth(aq_space_t.output, box,name = 'TestOut')<br>\nout1 = sess.run(qstd)<br>\nqftin = tf.reshape(out1,[-1])<br>\nqmax=ftin[tf.argmax(qftin)]<br>\nqmin=ftin[tf.argmin(qftin)]</p>\n<p>dst = tf.dequantize(qstd,qmin,qmax)   ### This is the failing command<br>\nout2 = sess.run(dqstd)</p>\n<h1>The NON quantized original space_to_depth</h1>\n<p>std = tf.space_to_depth(space_t, box,name = 'TestOut')</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>LOG:<br>\n== RESTART: C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py ==<br>\n[[[[-6.970806 ]<br>\n[-8.151844 ]<br>\n[ 1.0674906]<br>\n[-2.021196 ]]</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>[[-6.90624  ]<br>\n[ 7.7964478]<br>\n[ 8.286434 ]<br>\n[ 5.0095863]]</p>\n<p>[[-0.5847721]<br>\n[ 7.9649334]<br>\n[-1.1554356]<br>\n[ 8.961611 ]]</p>\n<p>[[ 1.2922192]<br>\n[-5.9919786]<br>\n[-5.741205 ]<br>\n[ 5.86102  ]]]]<br>\nTraceback (most recent call last):<br>\nFile \"C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py\", line 33, in <br>\ndst = tf.dequantize(qstd,qmin,qmax)<br>\nFile \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1319, in dequantize<br>\nmode=mode, name=name)<br>\nFile \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper<br>\nparam_name=input_name)<br>\nFile \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint<br>\n\", \".join(dtypes.as_dtype(x).name for x in allowed_list)))<br>\nTypeError: Value passed to parameter 'input' has DataType int8 not in list of allowed values: qint8, quint8, qint32, qint16, quint16</p>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):  stock from the tf.space_to_depth() example as given on tensorflow.org, with output directed to tf.quantize()\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\n\nTensorFlow installed from (source or binary): binary\n\n\nTensorFlow version (use command below):\n\n\n\n\n\ntf.GIT_VERSION\n\"b'unknown'\"\ntf.VERSION\n'1.6.0'\n\n\n\n\nPython version: , Python 3.5.4 Tk 8.6.4\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce: dst = tf.dequantize(qstd,qmin,qmax)  see the script below please.\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\ntf.depth_to_space() doesnt return the input dtype when it is dtype=tf.qint8.  The type is changed to tf.int8.\nSource code / logs\n\n\n\n\n\n\nSource:\n\nimport tensorflow as tf\nsess = tf.Session()\ndt=tf.float32\nbox=2\nslist = [(1,2,2,1),(1,2,2,3),(1,4,4,1),(2,2,4,1,)]\ndlist = [(1,1,1,4),(1,1,1,12),(1,2,2,4)]\nsshape=2\nspace_t=tf.random_uniform(slist[sshape],minval=-10, maxval=10,dtype=tf.float32)\nprint(sess.run(space_t))\nftin = tf.reshape(space_t,[-1])\nmax=ftin[tf.argmax(ftin)]\nmin=ftin[tf.argmin(ftin)]\naq_op = tf.quantize(space_t,min,max,tf.qint8,name=\"TFQuantize\")\naq_space_t = sess.run(aq_op)\nThe following qstd op should yield a type the same as aq_space_t.output, but does not, according to documentation.\nqstd = tf.space_to_depth(aq_space_t.output, box,name = 'TestOut')\nout1 = sess.run(qstd)\nqftin = tf.reshape(out1,[-1])\nqmax=ftin[tf.argmax(qftin)]\nqmin=ftin[tf.argmin(qftin)]\ndst = tf.dequantize(qstd,qmin,qmax)   ### This is the failing command\nout2 = sess.run(dqstd)\nThe NON quantized original space_to_depth\nstd = tf.space_to_depth(space_t, box,name = 'TestOut')\n\n\n\nLOG:\n== RESTART: C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py ==\n[[[[-6.970806 ]\n[-8.151844 ]\n[ 1.0674906]\n[-2.021196 ]]\n\n\n\n[[-6.90624  ]\n[ 7.7964478]\n[ 8.286434 ]\n[ 5.0095863]]\n[[-0.5847721]\n[ 7.9649334]\n[-1.1554356]\n[ 8.961611 ]]\n[[ 1.2922192]\n[-5.9919786]\n[-5.741205 ]\n[ 5.86102  ]]]]\nTraceback (most recent call last):\nFile \"C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py\", line 33, in \ndst = tf.dequantize(qstd,qmin,qmax)\nFile \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1319, in dequantize\nmode=mode, name=name)\nFile \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper\nparam_name=input_name)\nFile \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint\n\", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\nTypeError: Value passed to parameter 'input' has DataType int8 not in list of allowed values: qint8, quint8, qint32, qint16, quint16", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  stock from the tf.space_to_depth() example as given on tensorflow.org, with output directed to tf.quantize()\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:\r\n>>> tf.GIT_VERSION\r\n\"b'unknown'\"\r\n>>> tf.VERSION\r\n'1.6.0'\r\n- **Python version**: , Python 3.5.4 Tk 8.6.4 \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: dst = tf.dequantize(qstd,qmin,qmax)  see the script below please.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\ntf.depth_to_space() doesnt return the input dtype when it is dtype=tf.qint8.  The type is changed to tf.int8.\r\n\r\n### Source code / logs\r\n>>> \r\nSource:\r\n\r\n#\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\ndt=tf.float32\r\nbox=2\r\nslist = [(1,2,2,1),(1,2,2,3),(1,4,4,1),(2,2,4,1,)]\r\ndlist = [(1,1,1,4),(1,1,1,12),(1,2,2,4)]\r\nsshape=2\r\n\r\nspace_t=tf.random_uniform(slist[sshape],minval=-10, maxval=10,dtype=tf.float32)\r\n\r\nprint(sess.run(space_t))\r\n\r\nftin = tf.reshape(space_t,[-1])\r\nmax=ftin[tf.argmax(ftin)]\r\nmin=ftin[tf.argmin(ftin)]\r\n\r\naq_op = tf.quantize(space_t,min,max,tf.qint8,name=\"TFQuantize\")\r\naq_space_t = sess.run(aq_op)\r\n\r\n# The following qstd op should yield a type the same as aq_space_t.output, but does not, according to documentation.\r\nqstd = tf.space_to_depth(aq_space_t.output, box,name = 'TestOut')\r\nout1 = sess.run(qstd)\r\nqftin = tf.reshape(out1,[-1])\r\nqmax=ftin[tf.argmax(qftin)]\r\nqmin=ftin[tf.argmin(qftin)]\r\n\r\ndst = tf.dequantize(qstd,qmin,qmax)   ### This is the failing command\r\nout2 = sess.run(dqstd)\r\n\r\n# The NON quantized original space_to_depth \r\nstd = tf.space_to_depth(space_t, box,name = 'TestOut')\r\n\r\n>>> LOG:\r\n== RESTART: C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py ==\r\n[[[[-6.970806 ]\r\n   [-8.151844 ]\r\n   [ 1.0674906]\r\n   [-2.021196 ]]\r\n\r\n  [[-6.90624  ]\r\n   [ 7.7964478]\r\n   [ 8.286434 ]\r\n   [ 5.0095863]]\r\n\r\n  [[-0.5847721]\r\n   [ 7.9649334]\r\n   [-1.1554356]\r\n   [ 8.961611 ]]\r\n\r\n  [[ 1.2922192]\r\n   [-5.9919786]\r\n   [-5.741205 ]\r\n   [ 5.86102  ]]]]\r\nTraceback (most recent call last):\r\n  File \"C:/Users/c_jrost/projects/tensorTest/py_tests/autoQuantize3.py\", line 33, in <module>\r\n    dst = tf.dequantize(qstd,qmin,qmax)\r\n  File \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1319, in dequantize\r\n    mode=mode, name=name)\r\n  File \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"c:\\Users\\c_jrost\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'input' has DataType int8 not in list of allowed values: qint8, quint8, qint32, qint16, quint16\r\n\r\n"}