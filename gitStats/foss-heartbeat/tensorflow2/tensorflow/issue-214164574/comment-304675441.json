{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/304675441", "html_url": "https://github.com/tensorflow/tensorflow/issues/8404#issuecomment-304675441", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8404", "id": 304675441, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDY3NTQ0MQ==", "user": {"login": "Lakedaemon", "id": 103175, "node_id": "MDQ6VXNlcjEwMzE3NQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/103175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lakedaemon", "html_url": "https://github.com/Lakedaemon", "followers_url": "https://api.github.com/users/Lakedaemon/followers", "following_url": "https://api.github.com/users/Lakedaemon/following{/other_user}", "gists_url": "https://api.github.com/users/Lakedaemon/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lakedaemon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lakedaemon/subscriptions", "organizations_url": "https://api.github.com/users/Lakedaemon/orgs", "repos_url": "https://api.github.com/users/Lakedaemon/repos", "events_url": "https://api.github.com/users/Lakedaemon/events{/privacy}", "received_events_url": "https://api.github.com/users/Lakedaemon/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-29T14:31:51Z", "updated_at": "2017-05-29T14:31:51Z", "author_association": "NONE", "body_html": "<p>ok, built tensorflow from sources (30 minutes O.O)<br>\nretrained my model (took mostly as long for 1 epoch as the prebuilt tensorflow binary, despite the added cpu instructions ? I didn't add any optimisation flag (there is no documentation for that anyway))<br>\nbuilt summarize_graph (it takes sooooooooooooooooooooo long to compile C++ :/, like 8 minutes for a simple utility)</p>\n<p>Also, tried this command <code>bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\ --in_graph=tensorflow_inception_graph.pb \\ --out_graph=optimized_inception_graph.pb \\ --inputs='Mul' \\ --outputs='softmax' \\ --transforms=' strip_unused_nodes(type=float, shape=\"1,299,299,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'</code></p>\n<p>loaded the .pb file, exported as .pbtext to see the differences (went from 370kb to 29kb, nice).<br>\nThere should have been quite a lot of optimizations in there...</p>\n<p>Yet, the DT_BOOL &amp; keras leraning-phase stuff is still in there (obviously, because the keras_learning_phase placeholder hasn't been replaced by a constant op)</p>\n<p>And... looking at the doc for the transform_graph tools, I don't see any transformation that allows one to replace a placeholder op (with a single bool) by a const op... :/<br>\nsigh.... Am I supposed to write a custom transform function for that ?</p>\n<p>And if I do, will the switch op disappear with an opmtimising phase ?</p>\n<p>/me begins to think that he'll throw the towel and just build tensorflow for android with CPU:BOOL kernel<br>\n(why is there support for GPU:BOOL and not for CPU:BOOL anyway ?)</p>", "body_text": "ok, built tensorflow from sources (30 minutes O.O)\nretrained my model (took mostly as long for 1 epoch as the prebuilt tensorflow binary, despite the added cpu instructions ? I didn't add any optimisation flag (there is no documentation for that anyway))\nbuilt summarize_graph (it takes sooooooooooooooooooooo long to compile C++ :/, like 8 minutes for a simple utility)\nAlso, tried this command bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\ --in_graph=tensorflow_inception_graph.pb \\ --out_graph=optimized_inception_graph.pb \\ --inputs='Mul' \\ --outputs='softmax' \\ --transforms=' strip_unused_nodes(type=float, shape=\"1,299,299,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'\nloaded the .pb file, exported as .pbtext to see the differences (went from 370kb to 29kb, nice).\nThere should have been quite a lot of optimizations in there...\nYet, the DT_BOOL & keras leraning-phase stuff is still in there (obviously, because the keras_learning_phase placeholder hasn't been replaced by a constant op)\nAnd... looking at the doc for the transform_graph tools, I don't see any transformation that allows one to replace a placeholder op (with a single bool) by a const op... :/\nsigh.... Am I supposed to write a custom transform function for that ?\nAnd if I do, will the switch op disappear with an opmtimising phase ?\n/me begins to think that he'll throw the towel and just build tensorflow for android with CPU:BOOL kernel\n(why is there support for GPU:BOOL and not for CPU:BOOL anyway ?)", "body": "ok, built tensorflow from sources (30 minutes O.O)\r\nretrained my model (took mostly as long for 1 epoch as the prebuilt tensorflow binary, despite the added cpu instructions ? I didn't add any optimisation flag (there is no documentation for that anyway))\r\nbuilt summarize_graph (it takes sooooooooooooooooooooo long to compile C++ :/, like 8 minutes for a simple utility)\r\n\r\nAlso, tried this command `bazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=tensorflow_inception_graph.pb \\\r\n--out_graph=optimized_inception_graph.pb \\\r\n--inputs='Mul' \\\r\n--outputs='softmax' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms'`\r\n\r\nloaded the .pb file, exported as .pbtext to see the differences (went from 370kb to 29kb, nice).\r\nThere should have been quite a lot of optimizations in there...\r\n\r\nYet, the DT_BOOL & keras leraning-phase stuff is still in there (obviously, because the keras_learning_phase placeholder hasn't been replaced by a constant op)\r\n\r\nAnd... looking at the doc for the transform_graph tools, I don't see any transformation that allows one to replace a placeholder op (with a single bool) by a const op... :/\r\nsigh.... Am I supposed to write a custom transform function for that ?\r\n\r\nAnd if I do, will the switch op disappear with an opmtimising phase ?\r\n\r\n/me begins to think that he'll throw the towel and just build tensorflow for android with CPU:BOOL kernel\r\n(why is there support for GPU:BOOL and not for CPU:BOOL anyway ?)"}