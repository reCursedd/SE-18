{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19936", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19936/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19936/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19936/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19936", "id": 331523980, "node_id": "MDU6SXNzdWUzMzE1MjM5ODA=", "number": 19936, "title": "Min/Max value, naming scope, and tf.contrib.quantize", "user": {"login": "scottcjt", "id": 34140249, "node_id": "MDQ6VXNlcjM0MTQwMjQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/34140249?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scottcjt", "html_url": "https://github.com/scottcjt", "followers_url": "https://api.github.com/users/scottcjt/followers", "following_url": "https://api.github.com/users/scottcjt/following{/other_user}", "gists_url": "https://api.github.com/users/scottcjt/gists{/gist_id}", "starred_url": "https://api.github.com/users/scottcjt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scottcjt/subscriptions", "organizations_url": "https://api.github.com/users/scottcjt/orgs", "repos_url": "https://api.github.com/users/scottcjt/repos", "events_url": "https://api.github.com/users/scottcjt/events{/privacy}", "received_events_url": "https://api.github.com/users/scottcjt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-06-12T10:12:06Z", "updated_at": "2018-11-12T04:12:46Z", "closed_at": "2018-08-02T22:59:06Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 18.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Pip package</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: non-related to GPU</li>\n<li><strong>GPU model and memory</strong>: non-related to GPU</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am assuming to use <code>create_training_graph()</code> to insert min/max vars into my graph, train it, and then reuse variables to re-route an inference graph with <code>create_eval_graph()</code>. By this way I could reuse the min/max information gathered in the training process.</p>\n<p>So I wrote the following code to try them out:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_build</span>():\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>A simple matmul() model.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> with tf.variable_scope('YEAH'): **************************</span>\n    w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>W<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>], tf.float32)\n    b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">64</span>], tf.float32)\n    x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">64</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input<span class=\"pl-pds\">'</span></span>)\n    y <span class=\"pl-k\">=</span> tf.matmul(x, w) <span class=\"pl-k\">+</span> b\n    o <span class=\"pl-k\">=</span> tf.identity(y, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>output<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> x, o\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Train<span class=\"pl-pds\">'</span></span>):\n    _build()\n    tf.contrib.quantize.create_training_graph(<span class=\"pl-v\">quant_delay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> train the model...</span>\n    <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> inference graph</span>\n    tf.get_variable_scope().reuse_variables()\n    _build()\n    tf.contrib.quantize.create_eval_graph()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> export...</span>\n    <span class=\"pl-k\">pass</span></pre></div>\n<p>And it raises exception:</p>\n<pre><code>ValueError: Variable act_quant_1/min does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\n</code></pre>\n<p>However, if I change the <code>_build()</code> function:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_build</span>():\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>A simple matmul() model.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH<span class=\"pl-pds\">'</span></span>):\n        w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>W<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>], tf.float32)\n        b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">64</span>], tf.float32)\n        x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">64</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input<span class=\"pl-pds\">'</span></span>)\n        y <span class=\"pl-k\">=</span> tf.matmul(x, w) <span class=\"pl-k\">+</span> b\n        o <span class=\"pl-k\">=</span> tf.identity(y, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>output<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> x, o</pre></div>\n<p>...to solely apply a variable scope for all nodes, the error disappears. And the variables seems fine.</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">2</span>]: tf.global_variables()\nOut[<span class=\"pl-c1\">2</span>]: \n[<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/W:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>) dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/b:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/weights_quant/min:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/weights_quant/max:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>fake_quantization_step:0<span class=\"pl-pds\">'</span></span> shape=() dtype=int64_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/act_quant/min:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/act_quant/max:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/act_quant/YEAH/act_quant/min/biased:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/act_quant/YEAH/act_quant/min/local_step:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/act_quant/YEAH/act_quant/max/biased:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>,\n <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>YEAH/act_quant/YEAH/act_quant/max/local_step:0<span class=\"pl-pds\">'</span></span> shape=() dtype=float32_ref<span class=\"pl-k\">&gt;</span>]</pre></div>\n<p>What is the correct usage for <code>create_training_graph()</code> and <code>create_eval_graph()</code>? Is my expectation correct? It looks like <code>create_xxx_graph()</code> searches for vacant scope instead of accepting possible reuse.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\nTensorFlow installed from (source or binary): Pip package\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: non-related to GPU\nGPU model and memory: non-related to GPU\nExact command to reproduce:\n\nDescribe the problem\nI am assuming to use create_training_graph() to insert min/max vars into my graph, train it, and then reuse variables to re-route an inference graph with create_eval_graph(). By this way I could reuse the min/max information gathered in the training process.\nSo I wrote the following code to try them out:\nimport tensorflow as tf\n\ndef _build():\n    \"\"\"A simple matmul() model.\"\"\"\n    # with tf.variable_scope('YEAH'): **************************\n    w = tf.get_variable('W', [64, 64], tf.float32)\n    b = tf.get_variable('b', [64], tf.float32)\n    x = tf.placeholder(tf.float32, [1, 64], 'input')\n    y = tf.matmul(x, w) + b\n    o = tf.identity(y, 'output')\n    return x, o\n\nwith tf.name_scope('Train'):\n    _build()\n    tf.contrib.quantize.create_training_graph(quant_delay=1000)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # train the model...\n    pass\n\n    # inference graph\n    tf.get_variable_scope().reuse_variables()\n    _build()\n    tf.contrib.quantize.create_eval_graph()\n\n    # export...\n    pass\nAnd it raises exception:\nValueError: Variable act_quant_1/min does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\n\nHowever, if I change the _build() function:\ndef _build():\n    \"\"\"A simple matmul() model.\"\"\"\n    with tf.variable_scope('YEAH'):\n        w = tf.get_variable('W', [64, 64], tf.float32)\n        b = tf.get_variable('b', [64], tf.float32)\n        x = tf.placeholder(tf.float32, [1, 64], 'input')\n        y = tf.matmul(x, w) + b\n        o = tf.identity(y, 'output')\n    return x, o\n...to solely apply a variable scope for all nodes, the error disappears. And the variables seems fine.\nIn [2]: tf.global_variables()\nOut[2]: \n[<tf.Variable 'YEAH/W:0' shape=(64, 64) dtype=float32_ref>,\n <tf.Variable 'YEAH/b:0' shape=(64,) dtype=float32_ref>,\n <tf.Variable 'YEAH/weights_quant/min:0' shape=() dtype=float32_ref>,\n <tf.Variable 'YEAH/weights_quant/max:0' shape=() dtype=float32_ref>,\n <tf.Variable 'fake_quantization_step:0' shape=() dtype=int64_ref>,\n <tf.Variable 'YEAH/act_quant/min:0' shape=() dtype=float32_ref>,\n <tf.Variable 'YEAH/act_quant/max:0' shape=() dtype=float32_ref>,\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/min/biased:0' shape=() dtype=float32_ref>,\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/min/local_step:0' shape=() dtype=float32_ref>,\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/max/biased:0' shape=() dtype=float32_ref>,\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/max/local_step:0' shape=() dtype=float32_ref>]\nWhat is the correct usage for create_training_graph() and create_eval_graph()? Is my expectation correct? It looks like create_xxx_graph() searches for vacant scope instead of accepting possible reuse.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: Pip package\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: non-related to GPU\r\n- **GPU model and memory**: non-related to GPU\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI am assuming to use `create_training_graph()` to insert min/max vars into my graph, train it, and then reuse variables to re-route an inference graph with `create_eval_graph()`. By this way I could reuse the min/max information gathered in the training process.\r\n\r\nSo I wrote the following code to try them out:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef _build():\r\n    \"\"\"A simple matmul() model.\"\"\"\r\n    # with tf.variable_scope('YEAH'): **************************\r\n    w = tf.get_variable('W', [64, 64], tf.float32)\r\n    b = tf.get_variable('b', [64], tf.float32)\r\n    x = tf.placeholder(tf.float32, [1, 64], 'input')\r\n    y = tf.matmul(x, w) + b\r\n    o = tf.identity(y, 'output')\r\n    return x, o\r\n\r\nwith tf.name_scope('Train'):\r\n    _build()\r\n    tf.contrib.quantize.create_training_graph(quant_delay=1000)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    # train the model...\r\n    pass\r\n\r\n    # inference graph\r\n    tf.get_variable_scope().reuse_variables()\r\n    _build()\r\n    tf.contrib.quantize.create_eval_graph()\r\n\r\n    # export...\r\n    pass\r\n```\r\n\r\nAnd it raises exception:\r\n\r\n```\r\nValueError: Variable act_quant_1/min does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\n```\r\n\r\nHowever, if I change the `_build()` function:\r\n\r\n```python\r\ndef _build():\r\n    \"\"\"A simple matmul() model.\"\"\"\r\n    with tf.variable_scope('YEAH'):\r\n        w = tf.get_variable('W', [64, 64], tf.float32)\r\n        b = tf.get_variable('b', [64], tf.float32)\r\n        x = tf.placeholder(tf.float32, [1, 64], 'input')\r\n        y = tf.matmul(x, w) + b\r\n        o = tf.identity(y, 'output')\r\n    return x, o\r\n```\r\n\r\n...to solely apply a variable scope for all nodes, the error disappears. And the variables seems fine.\r\n\r\n```python\r\nIn [2]: tf.global_variables()\r\nOut[2]: \r\n[<tf.Variable 'YEAH/W:0' shape=(64, 64) dtype=float32_ref>,\r\n <tf.Variable 'YEAH/b:0' shape=(64,) dtype=float32_ref>,\r\n <tf.Variable 'YEAH/weights_quant/min:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'YEAH/weights_quant/max:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'fake_quantization_step:0' shape=() dtype=int64_ref>,\r\n <tf.Variable 'YEAH/act_quant/min:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'YEAH/act_quant/max:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/min/biased:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/min/local_step:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/max/biased:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'YEAH/act_quant/YEAH/act_quant/max/local_step:0' shape=() dtype=float32_ref>]\r\n```\r\n\r\nWhat is the correct usage for `create_training_graph()` and `create_eval_graph()`? Is my expectation correct? It looks like `create_xxx_graph()` searches for vacant scope instead of accepting possible reuse.\r\n"}