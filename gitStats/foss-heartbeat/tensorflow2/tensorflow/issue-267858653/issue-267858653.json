{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13932", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13932/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13932/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13932/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13932", "id": 267858653, "node_id": "MDU6SXNzdWUyNjc4NTg2NTM=", "number": 13932, "title": "Non-determinism from `tf.data.Dataset.map` with random ops", "user": {"login": "dusenberrymw", "id": 5431337, "node_id": "MDQ6VXNlcjU0MzEzMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5431337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dusenberrymw", "html_url": "https://github.com/dusenberrymw", "followers_url": "https://api.github.com/users/dusenberrymw/followers", "following_url": "https://api.github.com/users/dusenberrymw/following{/other_user}", "gists_url": "https://api.github.com/users/dusenberrymw/gists{/gist_id}", "starred_url": "https://api.github.com/users/dusenberrymw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dusenberrymw/subscriptions", "organizations_url": "https://api.github.com/users/dusenberrymw/orgs", "repos_url": "https://api.github.com/users/dusenberrymw/repos", "events_url": "https://api.github.com/users/dusenberrymw/events{/privacy}", "received_events_url": "https://api.github.com/users/dusenberrymw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-24T00:26:33Z", "updated_at": "2018-01-04T19:13:34Z", "closed_at": "2018-01-04T19:13:34Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes -- please see the minimal reproducible example script below.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: <code>pip3 install tf-nightly</code> (also happens when built from source)</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)</li>\n<li><strong>CUDA/cuDNN version</strong>: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)</li>\n<li><strong>GPU model and memory</strong>: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)</li>\n<li><strong>Exact command to reproduce</strong>: See minimal reproducible example below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The new <code>tf.data.Dataset</code> API contains a <code>map</code> function with a <code>num_parallel_calls</code> parameter, which allows elements to be processed in parallel by multiple threads.  Although not explicitly mentioned in the API docs, prior discussions (such as a comment from <a href=\"https://github.com/tensorflow/tensorflow/issues/13847#issuecomment-338772693\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/13847/hovercard\">today</a>) have indicated that the <code>map</code> function should be deterministic (w.r.t. the graph seed) even if <code>num_parallel_calls &gt; 1</code>.  I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below).  However, if the the function being mapped contains a random op, the results become non-deterministic for all values of <code>num_parallel_calls &gt; 1</code>.  This is unexpected, and prevents training experiments from being reproducible, unless <code>num_parallel_calls == 1</code>.  Also, please note that the example below serves as a minimal example to reproduce the issue.  The real scenario involves running data augmentation during training.</p>\n<h3>Source code / logs</h3>\n<ol>\n<li><code>pip3 install tf-nightly</code></li>\n<li>Run the following code to observe that <code>map</code> functions with only <em>non-random</em> ops are <em>deterministic</em> for <em>all</em> values of <code>num_parallel_calls</code>, which is the <em>expected</em> behavior:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">threads</span>):\n  np.random.seed(<span class=\"pl-c1\">42</span>)\n  tf.set_random_seed(<span class=\"pl-c1\">42</span>)\n  images <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>).astype(np.float32)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_data</span>():\n    dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(images)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> some initial dataset</span>\n    dataset <span class=\"pl-k\">=</span> dataset.map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>threads)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> this works fine always</span>\n    dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">32</span>)\n    x <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator().get_next()\n    <span class=\"pl-k\">return</span> x\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> execution 1</span>\n  x <span class=\"pl-k\">=</span> get_data()\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    x_batch1 <span class=\"pl-k\">=</span> sess.run(x)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> clear out everything</span>\n  tf.reset_default_graph()\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> execution 2</span>\n  x <span class=\"pl-k\">=</span> get_data()\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    x_batch2 <span class=\"pl-k\">=</span> sess.run(x)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> results should be equivalent</span>\n  <span class=\"pl-k\">assert</span> np.allclose(x_batch1, x_batch2)\n\ntest(<span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> works with 1 thread!</span>\ntest(<span class=\"pl-c1\">15</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> works with &gt;1 threads!</span></pre></div>\n<ol start=\"3\">\n<li>Run the following code to observe that <code>map</code> functions with <em>random</em> ops are deterministic if <code>num_parallel_calls == 1</code>, but are <em>non-deterministic</em> for values of <code>num_parallel_calls &gt; 1</code>, which seems to me to be an <em>unexpected</em> behavior:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">threads</span>):\n  np.random.seed(<span class=\"pl-c1\">42</span>)\n  tf.set_random_seed(<span class=\"pl-c1\">42</span>)\n  images <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>).astype(np.float32)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_data</span>():\n    dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(images)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> some initial dataset</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ONLY DIFFERENCE IS THE BELOW LINE:</span>\n    dataset <span class=\"pl-k\">=</span> dataset.map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">image</span>: tf.image.random_hue(image, <span class=\"pl-c1\">0.04</span>, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">42</span>), <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>threads)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ONLY DIFFERENCE IS THE ABOVE LINE ^^^:</span>\n    dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">32</span>)\n    x <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator().get_next()\n    <span class=\"pl-k\">return</span> x\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> execution 1</span>\n  x <span class=\"pl-k\">=</span> get_data()\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    x_batch1 <span class=\"pl-k\">=</span> sess.run(x)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> clear out everything</span>\n  tf.reset_default_graph()\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> execution 2</span>\n  x <span class=\"pl-k\">=</span> get_data()\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    x_batch2 <span class=\"pl-k\">=</span> sess.run(x)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> results should be equivalent</span>\n  <span class=\"pl-k\">assert</span> np.allclose(x_batch1, x_batch2)\n\ntest(<span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> works with 1 thread!</span>\ntest(<span class=\"pl-c1\">15</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> fails with &gt;1 threads!</span></pre></div>\n<ol start=\"4\">\n<li>Observe that swapping out the <code>map</code> line above with an entirely different random op such as <code>dataset = dataset.map(lambda x: x * tf.random_normal([64, 64, 3], seed=42), num_parallel_calls=threads)</code> is also <em>non-deterministic</em> for values of <code>num_parallel_calls &gt; 1</code>.</li>\n</ol>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes -- please see the minimal reproducible example script below.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)\nTensorFlow installed from (source or binary): pip3 install tf-nightly (also happens when built from source)\nTensorFlow version (use command below): v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023\nPython version: 3.6.3\nBazel version (if compiling from source): N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)\nCUDA/cuDNN version: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)\nGPU model and memory: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)\nExact command to reproduce: See minimal reproducible example below\n\nDescribe the problem\nThe new tf.data.Dataset API contains a map function with a num_parallel_calls parameter, which allows elements to be processed in parallel by multiple threads.  Although not explicitly mentioned in the API docs, prior discussions (such as a comment from today) have indicated that the map function should be deterministic (w.r.t. the graph seed) even if num_parallel_calls > 1.  I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below).  However, if the the function being mapped contains a random op, the results become non-deterministic for all values of num_parallel_calls > 1.  This is unexpected, and prevents training experiments from being reproducible, unless num_parallel_calls == 1.  Also, please note that the example below serves as a minimal example to reproduce the issue.  The real scenario involves running data augmentation during training.\nSource code / logs\n\npip3 install tf-nightly\nRun the following code to observe that map functions with only non-random ops are deterministic for all values of num_parallel_calls, which is the expected behavior:\n\nimport numpy as np\nimport tensorflow as tf\n\ndef test(threads):\n  np.random.seed(42)\n  tf.set_random_seed(42)\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\n\n  def get_data():\n    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset\n    dataset = dataset.map(lambda x: x * 2, num_parallel_calls=threads)  # this works fine always\n    dataset = dataset.batch(32)\n    x = dataset.make_one_shot_iterator().get_next()\n    return x\n\n  # execution 1\n  x = get_data()\n  with tf.Session() as sess:\n    x_batch1 = sess.run(x)\n\n  # clear out everything\n  tf.reset_default_graph()\n\n  # execution 2\n  x = get_data()\n  with tf.Session() as sess:\n    x_batch2 = sess.run(x)\n\n  # results should be equivalent\n  assert np.allclose(x_batch1, x_batch2)\n\ntest(1)  # works with 1 thread!\ntest(15)  # works with >1 threads!\n\nRun the following code to observe that map functions with random ops are deterministic if num_parallel_calls == 1, but are non-deterministic for values of num_parallel_calls > 1, which seems to me to be an unexpected behavior:\n\nimport numpy as np\nimport tensorflow as tf\n\ndef test(threads):\n  np.random.seed(42)\n  tf.set_random_seed(42)\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\n\n  def get_data():\n    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset\n    # ONLY DIFFERENCE IS THE BELOW LINE:\n    dataset = dataset.map(lambda image: tf.image.random_hue(image, 0.04, seed=42), num_parallel_calls=threads)\n    # ONLY DIFFERENCE IS THE ABOVE LINE ^^^:\n    dataset = dataset.batch(32)\n    x = dataset.make_one_shot_iterator().get_next()\n    return x\n\n  # execution 1\n  x = get_data()\n  with tf.Session() as sess:\n    x_batch1 = sess.run(x)\n\n  # clear out everything\n  tf.reset_default_graph()\n\n  # execution 2\n  x = get_data()\n  with tf.Session() as sess:\n    x_batch2 = sess.run(x)\n\n  # results should be equivalent\n  assert np.allclose(x_batch1, x_batch2)\n\ntest(1)  # works with 1 thread!\ntest(15)  # fails with >1 threads!\n\nObserve that swapping out the map line above with an entirely different random op such as dataset = dataset.map(lambda x: x * tf.random_normal([64, 64, 3], seed=42), num_parallel_calls=threads) is also non-deterministic for values of num_parallel_calls > 1.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes -- please see the minimal reproducible example script below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)\r\n- **TensorFlow installed from (source or binary)**: `pip3 install tf-nightly` (also happens when built from source)\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)\r\n- **CUDA/cuDNN version**: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)\r\n- **GPU model and memory**: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)\r\n- **Exact command to reproduce**: See minimal reproducible example below\r\n\r\n### Describe the problem\r\nThe new `tf.data.Dataset` API contains a `map` function with a `num_parallel_calls` parameter, which allows elements to be processed in parallel by multiple threads.  Although not explicitly mentioned in the API docs, prior discussions (such as a comment from [today](https://github.com/tensorflow/tensorflow/issues/13847#issuecomment-338772693)) have indicated that the `map` function should be deterministic (w.r.t. the graph seed) even if `num_parallel_calls > 1`.  I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below).  However, if the the function being mapped contains a random op, the results become non-deterministic for all values of `num_parallel_calls > 1`.  This is unexpected, and prevents training experiments from being reproducible, unless `num_parallel_calls == 1`.  Also, please note that the example below serves as a minimal example to reproduce the issue.  The real scenario involves running data augmentation during training.\r\n\r\n### Source code / logs\r\n1. `pip3 install tf-nightly`\r\n2.  Run the following code to observe that `map` functions with only *non-random* ops are *deterministic* for *all* values of `num_parallel_calls`, which is the *expected* behavior:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test(threads):\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\r\n\r\n  def get_data():\r\n    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset\r\n    dataset = dataset.map(lambda x: x * 2, num_parallel_calls=threads)  # this works fine always\r\n    dataset = dataset.batch(32)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return x\r\n\r\n  # execution 1\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch1 = sess.run(x)\r\n\r\n  # clear out everything\r\n  tf.reset_default_graph()\r\n\r\n  # execution 2\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch2 = sess.run(x)\r\n\r\n  # results should be equivalent\r\n  assert np.allclose(x_batch1, x_batch2)\r\n\r\ntest(1)  # works with 1 thread!\r\ntest(15)  # works with >1 threads!\r\n```\r\n\r\n3. Run the following code to observe that `map` functions with *random* ops are deterministic if `num_parallel_calls == 1`, but are *non-deterministic* for values of `num_parallel_calls > 1`, which seems to me to be an *unexpected* behavior:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test(threads):\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\r\n\r\n  def get_data():\r\n    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset\r\n    # ONLY DIFFERENCE IS THE BELOW LINE:\r\n    dataset = dataset.map(lambda image: tf.image.random_hue(image, 0.04, seed=42), num_parallel_calls=threads)\r\n    # ONLY DIFFERENCE IS THE ABOVE LINE ^^^:\r\n    dataset = dataset.batch(32)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return x\r\n\r\n  # execution 1\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch1 = sess.run(x)\r\n\r\n  # clear out everything\r\n  tf.reset_default_graph()\r\n\r\n  # execution 2\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch2 = sess.run(x)\r\n\r\n  # results should be equivalent\r\n  assert np.allclose(x_batch1, x_batch2)\r\n\r\ntest(1)  # works with 1 thread!\r\ntest(15)  # fails with >1 threads!\r\n```\r\n\r\n4. Observe that swapping out the `map` line above with an entirely different random op such as `dataset = dataset.map(lambda x: x * tf.random_normal([64, 64, 3], seed=42), num_parallel_calls=threads)` is also *non-deterministic* for values of `num_parallel_calls > 1`."}