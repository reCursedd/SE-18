{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341263301", "html_url": "https://github.com/tensorflow/tensorflow/issues/13932#issuecomment-341263301", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13932", "id": 341263301, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTI2MzMwMQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T22:33:23Z", "updated_at": "2017-11-01T22:33:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Unfortunately, this is \"expected behavior\" due to the way <code>tf.random_uniform()</code> (used inside <code>tf.image.random_hue()</code>) and the other RNG ops are implemented. The parallel invocations of map will race to access the mutable RNG state inside the op, and different invocations will see a non-deterministically chosen element of the same sequence. Currently, the only way to ensure deterministic results from <code>Dataset.map()</code> that contains an RNG op is to set <code>num_parallel_calls=1</code>.</p>\n<p>In principle, you could slice your <code>map()</code> function so that the random number generation in a serial fashion, and the compute-intensive part of the op in a parallel map. For example, it's possible to do this manually for <code>tf.image.random_hue()</code>, because it is simply a composition of <code>tf.adjust_hue(..., tf.random_uniform(...))</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">threads</span>):\n  np.random.seed(<span class=\"pl-c1\">42</span>)\n  tf.set_random_seed(<span class=\"pl-c1\">42</span>)\n  images <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>).astype(np.float32)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_data</span>():\n    dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(images)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Perform the random number generation in a single-threaded map().</span>\n    dataset <span class=\"pl-k\">=</span> dataset.map(\n        <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">image</span>: (image, tf.random_uniform([], <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.04</span>, <span class=\"pl-c1\">0.04</span>, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">42</span>)),\n        <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Perform the compute-intensive hue adjustment in a multi-threaded map().</span>\n    dataset <span class=\"pl-k\">=</span> dataset.map(\n        <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">image</span>, <span class=\"pl-smi\">adjustment</span>: tf.image.adjust_hue(image, adjustment),\n        <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>threads)\n    dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">32</span>)\n    x <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator().get_next()\n    <span class=\"pl-k\">return</span> x\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> execution 1</span>\n  x <span class=\"pl-k\">=</span> get_data()\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    x_batch1 <span class=\"pl-k\">=</span> sess.run(x)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> clear out everything</span>\n  tf.reset_default_graph()\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> execution 2</span>\n  x <span class=\"pl-k\">=</span> get_data()\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    x_batch2 <span class=\"pl-k\">=</span> sess.run(x)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> results should be equivalent</span>\n  <span class=\"pl-k\">assert</span> np.allclose(x_batch1, x_batch2)\n\ntest(<span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> works with 1 thread!</span>\ntest(<span class=\"pl-c1\">15</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> works with &gt;1 threads!</span></pre></div>\n<p>However, this manual approach might not scale to a real program. In our CNN benchmarks, we've been using a sequence number to deterministically map \"random\" perturbations onto input images. In future we might consider doing this kind of slicing automatically, but that's probably some way off.</p>\n<p>Hope this helps though!</p>", "body_text": "Unfortunately, this is \"expected behavior\" due to the way tf.random_uniform() (used inside tf.image.random_hue()) and the other RNG ops are implemented. The parallel invocations of map will race to access the mutable RNG state inside the op, and different invocations will see a non-deterministically chosen element of the same sequence. Currently, the only way to ensure deterministic results from Dataset.map() that contains an RNG op is to set num_parallel_calls=1.\nIn principle, you could slice your map() function so that the random number generation in a serial fashion, and the compute-intensive part of the op in a parallel map. For example, it's possible to do this manually for tf.image.random_hue(), because it is simply a composition of tf.adjust_hue(..., tf.random_uniform(...)):\nimport numpy as np\nimport tensorflow as tf\n\ndef test(threads):\n  np.random.seed(42)\n  tf.set_random_seed(42)\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\n\n  def get_data():\n    dataset = tf.data.Dataset.from_tensor_slices(images)\n    # Perform the random number generation in a single-threaded map().\n    dataset = dataset.map(\n        lambda image: (image, tf.random_uniform([], -0.04, 0.04, seed=42)),\n        num_parallel_calls=1)\n    # Perform the compute-intensive hue adjustment in a multi-threaded map().\n    dataset = dataset.map(\n        lambda image, adjustment: tf.image.adjust_hue(image, adjustment),\n        num_parallel_calls=threads)\n    dataset = dataset.batch(32)\n    x = dataset.make_one_shot_iterator().get_next()\n    return x\n\n  # execution 1\n  x = get_data()\n  with tf.Session() as sess:\n    x_batch1 = sess.run(x)\n\n  # clear out everything\n  tf.reset_default_graph()\n\n  # execution 2\n  x = get_data()\n  with tf.Session() as sess:\n    x_batch2 = sess.run(x)\n\n  # results should be equivalent\n  assert np.allclose(x_batch1, x_batch2)\n\ntest(1)  # works with 1 thread!\ntest(15)  # works with >1 threads!\nHowever, this manual approach might not scale to a real program. In our CNN benchmarks, we've been using a sequence number to deterministically map \"random\" perturbations onto input images. In future we might consider doing this kind of slicing automatically, but that's probably some way off.\nHope this helps though!", "body": "Unfortunately, this is \"expected behavior\" due to the way `tf.random_uniform()` (used inside `tf.image.random_hue()`) and the other RNG ops are implemented. The parallel invocations of map will race to access the mutable RNG state inside the op, and different invocations will see a non-deterministically chosen element of the same sequence. Currently, the only way to ensure deterministic results from `Dataset.map()` that contains an RNG op is to set `num_parallel_calls=1`.\r\n\r\nIn principle, you could slice your `map()` function so that the random number generation in a serial fashion, and the compute-intensive part of the op in a parallel map. For example, it's possible to do this manually for `tf.image.random_hue()`, because it is simply a composition of `tf.adjust_hue(..., tf.random_uniform(...))`:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test(threads):\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  images = np.random.rand(100, 64, 64, 3).astype(np.float32)\r\n\r\n  def get_data():\r\n    dataset = tf.data.Dataset.from_tensor_slices(images)\r\n    # Perform the random number generation in a single-threaded map().\r\n    dataset = dataset.map(\r\n        lambda image: (image, tf.random_uniform([], -0.04, 0.04, seed=42)),\r\n        num_parallel_calls=1)\r\n    # Perform the compute-intensive hue adjustment in a multi-threaded map().\r\n    dataset = dataset.map(\r\n        lambda image, adjustment: tf.image.adjust_hue(image, adjustment),\r\n        num_parallel_calls=threads)\r\n    dataset = dataset.batch(32)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return x\r\n\r\n  # execution 1\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch1 = sess.run(x)\r\n\r\n  # clear out everything\r\n  tf.reset_default_graph()\r\n\r\n  # execution 2\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch2 = sess.run(x)\r\n\r\n  # results should be equivalent\r\n  assert np.allclose(x_batch1, x_batch2)\r\n\r\ntest(1)  # works with 1 thread!\r\ntest(15)  # works with >1 threads!\r\n```\r\n\r\nHowever, this manual approach might not scale to a real program. In our CNN benchmarks, we've been using a sequence number to deterministically map \"random\" perturbations onto input images. In future we might consider doing this kind of slicing automatically, but that's probably some way off.\r\n\r\nHope this helps though!"}