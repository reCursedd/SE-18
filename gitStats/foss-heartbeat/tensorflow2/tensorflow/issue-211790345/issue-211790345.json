{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8061", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8061/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8061/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8061/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8061", "id": 211790345, "node_id": "MDU6SXNzdWUyMTE3OTAzNDU=", "number": 8061, "title": "Multi-GPU Inference using FIFO Queues issue in TF v1.0.0", "user": {"login": "agupta74", "id": 21690396, "node_id": "MDQ6VXNlcjIxNjkwMzk2", "avatar_url": "https://avatars2.githubusercontent.com/u/21690396?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agupta74", "html_url": "https://github.com/agupta74", "followers_url": "https://api.github.com/users/agupta74/followers", "following_url": "https://api.github.com/users/agupta74/following{/other_user}", "gists_url": "https://api.github.com/users/agupta74/gists{/gist_id}", "starred_url": "https://api.github.com/users/agupta74/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agupta74/subscriptions", "organizations_url": "https://api.github.com/users/agupta74/orgs", "repos_url": "https://api.github.com/users/agupta74/repos", "events_url": "https://api.github.com/users/agupta74/events{/privacy}", "received_events_url": "https://api.github.com/users/agupta74/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-03-03T20:12:27Z", "updated_at": "2017-08-18T20:56:13Z", "closed_at": "2017-03-07T23:48:31Z", "author_association": "NONE", "body_html": "<p>Here is an example multi-gpu inference code which produces incorrect Output in TF v1.0.0 but produces correct output in v0.11.0 (single GPU produces correct output in both versions). Please refer to the attached code for more details.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/817729/InferenceTest.txt\">InferenceTest.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/817730/Inference.txt\">Inference.txt</a></p>\n<h2><strong>Correct Output (TF v0.11) -- 2 GPUs</strong><br>\npython InferenceTest.py 2</h2>\n<p>I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally<br>\n**************** Using 2 GPUs *****************<br>\nTensorFlow version:  0.11.0<br>\nGPU ID List:  [0, 1]<br>\nDevice:  /gpu:0<br>\nDevice:  /gpu:1<br>\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:<br>\nname: Tesla M40<br>\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112<br>\npciBusID 0000:00:05.0<br>\nTotal memory: 11.21GiB<br>\nFree memory: 11.09GiB<br>\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x37b9dc0<br>\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:<br>\nname: Tesla M40<br>\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112<br>\npciBusID 0000:00:09.0<br>\nTotal memory: 11.21GiB<br>\nFree memory: 11.09GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 1<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y N<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   N Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)<br>\nInput Value 0<br>\nInput Value 1<br>\nInput Value 2<br>\nInput Value 3<br>\nInput Value 4<br>\nInput Value 5<br>\nInput Value 6<br>\nInput Value 7<br>\nInput Value 8<br>\nInput Value 9<br>\nInput Queue Size=  10<br>\nResults Queue Size=  0<br>\nProcessing: Input Queue Size=  8<br>\nProcessing: Results Queue Size=  2<br>\nProcessing: Input Queue Size=  6<br>\nProcessing: Results Queue Size=  4<br>\nProcessing: Input Queue Size=  4<br>\nProcessing: Results Queue Size=  6<br>\nProcessing: Input Queue Size=  2<br>\nProcessing: Results Queue Size=  8<br>\nProcessing: Input Queue Size=  0<br>\nProcessing: Results Queue Size=  10<br>\n************* Dequeue Results ********************<br>\nResults Queue Size=  10<br>\nDequeue Results: Results Output  1:<br>\nDequeue Results: Results Queue Size=  9<br>\nDequeue Results: Results Output  2:<br>\nDequeue Results: Results Queue Size=  8<br>\nDequeue Results: Results Output  3:<br>\nDequeue Results: Results Queue Size=  7<br>\nDequeue Results: Results Output  4:<br>\nDequeue Results: Results Queue Size=  6<br>\nDequeue Results: Results Output  6:<br>\nDequeue Results: Results Queue Size=  5<br>\nDequeue Results: Results Output  5:<br>\nDequeue Results: Results Queue Size=  4<br>\nDequeue Results: Results Output  7:<br>\nDequeue Results: Results Queue Size=  3<br>\nDequeue Results: Results Output  8:<br>\nDequeue Results: Results Queue Size=  2<br>\nDequeue Results: Results Output  9:<br>\nDequeue Results: Results Queue Size=  1<br>\nDequeue Results: Results Output  10:<br>\nDequeue Results: Results Queue Size=  0<br>\nFinal Input Queue Size=  0<br>\nFinal Results Queue Size=  0</p>\n<hr>\n<h2><strong>Wrong Output (TF v1.0.0) -- 2 GPUs</strong><br>\npython InferenceTest.py 2</h2>\n<p>I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally<br>\n**************** Using 2 GPUs *****************<br>\nTensorFlow version:  1.0.0<br>\nGPU ID List:  [0, 1]<br>\nDevice:  /gpu:0<br>\nDevice:  /gpu:1<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.<br>\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:<br>\nname: Tesla M40<br>\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112<br>\npciBusID 0000:00:05.0<br>\nTotal memory: 11.21GiB<br>\nFree memory: 11.09GiB<br>\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x1999d60<br>\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties:<br>\nname: Tesla M40<br>\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112<br>\npciBusID 0000:00:09.0<br>\nTotal memory: 11.21GiB<br>\nFree memory: 11.09GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)<br>\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0<br>\nInput Value 0<br>\nInput Value 1<br>\nInput Value 2<br>\nInput Value 3<br>\nInput Value 4<br>\nInput Value 5<br>\nInput Value 6<br>\nInput Value 7<br>\nInput Value 8<br>\nInput Value 9<br>\nInput Queue Size=  10<br>\nResults Queue Size=  0<br>\nProcessing: Input Queue Size=  9<br>\nProcessing: Results Queue Size=  2<br>\nProcessing: Input Queue Size=  8<br>\nProcessing: Results Queue Size=  4<br>\nProcessing: Input Queue Size=  7<br>\nProcessing: Results Queue Size=  6<br>\nProcessing: Input Queue Size=  6<br>\nProcessing: Results Queue Size=  8<br>\nProcessing: Input Queue Size=  5<br>\nProcessing: Results Queue Size=  10<br>\nProcessing: Input Queue Size=  4<br>\nProcessing: Results Queue Size=  12<br>\nProcessing: Input Queue Size=  3<br>\nProcessing: Results Queue Size=  14<br>\nProcessing: Input Queue Size=  2<br>\nProcessing: Results Queue Size=  16<br>\nProcessing: Input Queue Size=  1<br>\nProcessing: Results Queue Size=  18<br>\nProcessing: Input Queue Size=  0<br>\nProcessing: Results Queue Size=  20<br>\n************* Dequeue Results ********************<br>\nResults Queue Size=  20<br>\nDequeue Results: Results Output  1:<br>\nDequeue Results: Results Queue Size=  19<br>\nDequeue Results: Results Output  1:<br>\nDequeue Results: Results Queue Size=  18<br>\nDequeue Results: Results Output  2:<br>\nDequeue Results: Results Queue Size=  17<br>\nDequeue Results: Results Output  2:<br>\nDequeue Results: Results Queue Size=  16<br>\nDequeue Results: Results Output  3:<br>\nDequeue Results: Results Queue Size=  15<br>\nDequeue Results: Results Output  3:<br>\nDequeue Results: Results Queue Size=  14<br>\nDequeue Results: Results Output  4:<br>\nDequeue Results: Results Queue Size=  13<br>\nDequeue Results: Results Output  4:<br>\nDequeue Results: Results Queue Size=  12<br>\nDequeue Results: Results Output  5:<br>\nDequeue Results: Results Queue Size=  11<br>\nDequeue Results: Results Output  5:<br>\nDequeue Results: Results Queue Size=  10<br>\nDequeue Results: Results Output  6:<br>\nDequeue Results: Results Queue Size=  9<br>\nDequeue Results: Results Output  6:<br>\nDequeue Results: Results Queue Size=  8<br>\nDequeue Results: Results Output  7:<br>\nDequeue Results: Results Queue Size=  7<br>\nDequeue Results: Results Output  7:<br>\nDequeue Results: Results Queue Size=  6<br>\nDequeue Results: Results Output  8:<br>\nDequeue Results: Results Queue Size=  5<br>\nDequeue Results: Results Output  8:<br>\nDequeue Results: Results Queue Size=  4<br>\nDequeue Results: Results Output  9:<br>\nDequeue Results: Results Queue Size=  3<br>\nDequeue Results: Results Output  9:<br>\nDequeue Results: Results Queue Size=  2<br>\nDequeue Results: Results Output  10:<br>\nDequeue Results: Results Queue Size=  1<br>\nDequeue Results: Results Output  10:<br>\nDequeue Results: Results Queue Size=  0<br>\nFinal Input Queue Size=  0<br>\nFinal Results Queue Size=  0</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None</p>\n<h3>Environment info</h3>\n<p>Operating System: CentOS 7.2.1511</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/21690396/23567127/a771fd10-0009-11e7-88bb-daa27a1b28c2.PNG\"><img src=\"https://cloud.githubusercontent.com/assets/21690396/23567127/a771fd10-0009-11e7-88bb-daa27a1b28c2.PNG\" alt=\"cudaversion\" style=\"max-width:100%;\"></a></p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed: <a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl</a></li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<p>See the output above for 2 different tensor flow versions</p>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)</li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<h3>What other attempted solutions have you tried?</h3>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>", "body_text": "Here is an example multi-gpu inference code which produces incorrect Output in TF v1.0.0 but produces correct output in v0.11.0 (single GPU produces correct output in both versions). Please refer to the attached code for more details.\nInferenceTest.txt\nInference.txt\nCorrect Output (TF v0.11) -- 2 GPUs\npython InferenceTest.py 2\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n**************** Using 2 GPUs *****************\nTensorFlow version:  0.11.0\nGPU ID List:  [0, 1]\nDevice:  /gpu:0\nDevice:  /gpu:1\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\nname: Tesla M40\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\npciBusID 0000:00:05.0\nTotal memory: 11.21GiB\nFree memory: 11.09GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x37b9dc0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties:\nname: Tesla M40\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\npciBusID 0000:00:09.0\nTotal memory: 11.21GiB\nFree memory: 11.09GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y N\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   N Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)\nInput Value 0\nInput Value 1\nInput Value 2\nInput Value 3\nInput Value 4\nInput Value 5\nInput Value 6\nInput Value 7\nInput Value 8\nInput Value 9\nInput Queue Size=  10\nResults Queue Size=  0\nProcessing: Input Queue Size=  8\nProcessing: Results Queue Size=  2\nProcessing: Input Queue Size=  6\nProcessing: Results Queue Size=  4\nProcessing: Input Queue Size=  4\nProcessing: Results Queue Size=  6\nProcessing: Input Queue Size=  2\nProcessing: Results Queue Size=  8\nProcessing: Input Queue Size=  0\nProcessing: Results Queue Size=  10\n************* Dequeue Results ********************\nResults Queue Size=  10\nDequeue Results: Results Output  1:\nDequeue Results: Results Queue Size=  9\nDequeue Results: Results Output  2:\nDequeue Results: Results Queue Size=  8\nDequeue Results: Results Output  3:\nDequeue Results: Results Queue Size=  7\nDequeue Results: Results Output  4:\nDequeue Results: Results Queue Size=  6\nDequeue Results: Results Output  6:\nDequeue Results: Results Queue Size=  5\nDequeue Results: Results Output  5:\nDequeue Results: Results Queue Size=  4\nDequeue Results: Results Output  7:\nDequeue Results: Results Queue Size=  3\nDequeue Results: Results Output  8:\nDequeue Results: Results Queue Size=  2\nDequeue Results: Results Output  9:\nDequeue Results: Results Queue Size=  1\nDequeue Results: Results Output  10:\nDequeue Results: Results Queue Size=  0\nFinal Input Queue Size=  0\nFinal Results Queue Size=  0\n\nWrong Output (TF v1.0.0) -- 2 GPUs\npython InferenceTest.py 2\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n**************** Using 2 GPUs *****************\nTensorFlow version:  1.0.0\nGPU ID List:  [0, 1]\nDevice:  /gpu:0\nDevice:  /gpu:1\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\nname: Tesla M40\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\npciBusID 0000:00:05.0\nTotal memory: 11.21GiB\nFree memory: 11.09GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x1999d60\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties:\nname: Tesla M40\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\npciBusID 0000:00:09.0\nTotal memory: 11.21GiB\nFree memory: 11.09GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\nInput Value 0\nInput Value 1\nInput Value 2\nInput Value 3\nInput Value 4\nInput Value 5\nInput Value 6\nInput Value 7\nInput Value 8\nInput Value 9\nInput Queue Size=  10\nResults Queue Size=  0\nProcessing: Input Queue Size=  9\nProcessing: Results Queue Size=  2\nProcessing: Input Queue Size=  8\nProcessing: Results Queue Size=  4\nProcessing: Input Queue Size=  7\nProcessing: Results Queue Size=  6\nProcessing: Input Queue Size=  6\nProcessing: Results Queue Size=  8\nProcessing: Input Queue Size=  5\nProcessing: Results Queue Size=  10\nProcessing: Input Queue Size=  4\nProcessing: Results Queue Size=  12\nProcessing: Input Queue Size=  3\nProcessing: Results Queue Size=  14\nProcessing: Input Queue Size=  2\nProcessing: Results Queue Size=  16\nProcessing: Input Queue Size=  1\nProcessing: Results Queue Size=  18\nProcessing: Input Queue Size=  0\nProcessing: Results Queue Size=  20\n************* Dequeue Results ********************\nResults Queue Size=  20\nDequeue Results: Results Output  1:\nDequeue Results: Results Queue Size=  19\nDequeue Results: Results Output  1:\nDequeue Results: Results Queue Size=  18\nDequeue Results: Results Output  2:\nDequeue Results: Results Queue Size=  17\nDequeue Results: Results Output  2:\nDequeue Results: Results Queue Size=  16\nDequeue Results: Results Output  3:\nDequeue Results: Results Queue Size=  15\nDequeue Results: Results Output  3:\nDequeue Results: Results Queue Size=  14\nDequeue Results: Results Output  4:\nDequeue Results: Results Queue Size=  13\nDequeue Results: Results Output  4:\nDequeue Results: Results Queue Size=  12\nDequeue Results: Results Output  5:\nDequeue Results: Results Queue Size=  11\nDequeue Results: Results Output  5:\nDequeue Results: Results Queue Size=  10\nDequeue Results: Results Output  6:\nDequeue Results: Results Queue Size=  9\nDequeue Results: Results Output  6:\nDequeue Results: Results Queue Size=  8\nDequeue Results: Results Output  7:\nDequeue Results: Results Queue Size=  7\nDequeue Results: Results Output  7:\nDequeue Results: Results Queue Size=  6\nDequeue Results: Results Output  8:\nDequeue Results: Results Queue Size=  5\nDequeue Results: Results Output  8:\nDequeue Results: Results Queue Size=  4\nDequeue Results: Results Output  9:\nDequeue Results: Results Queue Size=  3\nDequeue Results: Results Output  9:\nDequeue Results: Results Queue Size=  2\nDequeue Results: Results Output  10:\nDequeue Results: Results Queue Size=  1\nDequeue Results: Results Output  10:\nDequeue Results: Results Queue Size=  0\nFinal Input Queue Size=  0\nFinal Results Queue Size=  0\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone\nEnvironment info\nOperating System: CentOS 7.2.1511\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nSee the output above for 2 different tensor flow versions\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).", "body": "Here is an example multi-gpu inference code which produces incorrect Output in TF v1.0.0 but produces correct output in v0.11.0 (single GPU produces correct output in both versions). Please refer to the attached code for more details.\r\n\r\n[InferenceTest.txt](https://github.com/tensorflow/tensorflow/files/817729/InferenceTest.txt)\r\n[Inference.txt](https://github.com/tensorflow/tensorflow/files/817730/Inference.txt)\r\n\r\n\r\n**Correct Output (TF v0.11) -- 2 GPUs** \r\npython InferenceTest.py 2\r\n-------------------------\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n**************** Using 2 GPUs *****************\r\nTensorFlow version:  0.11.0\r\nGPU ID List:  [0, 1]\r\nDevice:  /gpu:0\r\nDevice:  /gpu:1\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:05.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x37b9dc0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:09.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)\r\nInput Value 0\r\nInput Value 1\r\nInput Value 2\r\nInput Value 3\r\nInput Value 4\r\nInput Value 5\r\nInput Value 6\r\nInput Value 7\r\nInput Value 8\r\nInput Value 9\r\nInput Queue Size=  10\r\nResults Queue Size=  0\r\nProcessing: Input Queue Size=  8\r\nProcessing: Results Queue Size=  2\r\nProcessing: Input Queue Size=  6\r\nProcessing: Results Queue Size=  4\r\nProcessing: Input Queue Size=  4\r\nProcessing: Results Queue Size=  6\r\nProcessing: Input Queue Size=  2\r\nProcessing: Results Queue Size=  8\r\nProcessing: Input Queue Size=  0\r\nProcessing: Results Queue Size=  10\r\n************* Dequeue Results ********************\r\nResults Queue Size=  10\r\nDequeue Results: Results Output  1:\r\nDequeue Results: Results Queue Size=  9\r\nDequeue Results: Results Output  2:\r\nDequeue Results: Results Queue Size=  8\r\nDequeue Results: Results Output  3:\r\nDequeue Results: Results Queue Size=  7\r\nDequeue Results: Results Output  4:\r\nDequeue Results: Results Queue Size=  6\r\nDequeue Results: Results Output  6:\r\nDequeue Results: Results Queue Size=  5\r\nDequeue Results: Results Output  5:\r\nDequeue Results: Results Queue Size=  4\r\nDequeue Results: Results Output  7:\r\nDequeue Results: Results Queue Size=  3\r\nDequeue Results: Results Output  8:\r\nDequeue Results: Results Queue Size=  2\r\nDequeue Results: Results Output  9:\r\nDequeue Results: Results Queue Size=  1\r\nDequeue Results: Results Output  10:\r\nDequeue Results: Results Queue Size=  0\r\nFinal Input Queue Size=  0\r\nFinal Results Queue Size=  0\r\n\r\n***************************************************************************************************\r\n**Wrong Output (TF v1.0.0) -- 2 GPUs** \r\npython InferenceTest.py 2\r\n-------------------------\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n**************** Using 2 GPUs *****************\r\nTensorFlow version:  1.0.0\r\nGPU ID List:  [0, 1]\r\nDevice:  /gpu:0\r\nDevice:  /gpu:1\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:05.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x1999d60\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:09.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nInput Value 0\r\nInput Value 1\r\nInput Value 2\r\nInput Value 3\r\nInput Value 4\r\nInput Value 5\r\nInput Value 6\r\nInput Value 7\r\nInput Value 8\r\nInput Value 9\r\nInput Queue Size=  10\r\nResults Queue Size=  0\r\nProcessing: Input Queue Size=  9\r\nProcessing: Results Queue Size=  2\r\nProcessing: Input Queue Size=  8\r\nProcessing: Results Queue Size=  4\r\nProcessing: Input Queue Size=  7\r\nProcessing: Results Queue Size=  6\r\nProcessing: Input Queue Size=  6\r\nProcessing: Results Queue Size=  8\r\nProcessing: Input Queue Size=  5\r\nProcessing: Results Queue Size=  10\r\nProcessing: Input Queue Size=  4\r\nProcessing: Results Queue Size=  12\r\nProcessing: Input Queue Size=  3\r\nProcessing: Results Queue Size=  14\r\nProcessing: Input Queue Size=  2\r\nProcessing: Results Queue Size=  16\r\nProcessing: Input Queue Size=  1\r\nProcessing: Results Queue Size=  18\r\nProcessing: Input Queue Size=  0\r\nProcessing: Results Queue Size=  20\r\n************* Dequeue Results ********************\r\nResults Queue Size=  20\r\nDequeue Results: Results Output  1:\r\nDequeue Results: Results Queue Size=  19\r\nDequeue Results: Results Output  1:\r\nDequeue Results: Results Queue Size=  18\r\nDequeue Results: Results Output  2:\r\nDequeue Results: Results Queue Size=  17\r\nDequeue Results: Results Output  2:\r\nDequeue Results: Results Queue Size=  16\r\nDequeue Results: Results Output  3:\r\nDequeue Results: Results Queue Size=  15\r\nDequeue Results: Results Output  3:\r\nDequeue Results: Results Queue Size=  14\r\nDequeue Results: Results Output  4:\r\nDequeue Results: Results Queue Size=  13\r\nDequeue Results: Results Output  4:\r\nDequeue Results: Results Queue Size=  12\r\nDequeue Results: Results Output  5:\r\nDequeue Results: Results Queue Size=  11\r\nDequeue Results: Results Output  5:\r\nDequeue Results: Results Queue Size=  10\r\nDequeue Results: Results Output  6:\r\nDequeue Results: Results Queue Size=  9\r\nDequeue Results: Results Output  6:\r\nDequeue Results: Results Queue Size=  8\r\nDequeue Results: Results Output  7:\r\nDequeue Results: Results Queue Size=  7\r\nDequeue Results: Results Output  7:\r\nDequeue Results: Results Queue Size=  6\r\nDequeue Results: Results Output  8:\r\nDequeue Results: Results Queue Size=  5\r\nDequeue Results: Results Output  8:\r\nDequeue Results: Results Queue Size=  4\r\nDequeue Results: Results Output  9:\r\nDequeue Results: Results Queue Size=  3\r\nDequeue Results: Results Output  9:\r\nDequeue Results: Results Queue Size=  2\r\nDequeue Results: Results Output  10:\r\nDequeue Results: Results Queue Size=  1\r\nDequeue Results: Results Output  10:\r\nDequeue Results: Results Queue Size=  0\r\nFinal Input Queue Size=  0\r\nFinal Results Queue Size=  0\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: CentOS 7.2.1511\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n![cudaversion](https://cloud.githubusercontent.com/assets/21690396/23567127/a771fd10-0009-11e7-88bb-daa27a1b28c2.PNG)\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl \r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nSee the output above for 2 different tensor flow versions\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n"}