{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/228889889", "html_url": "https://github.com/tensorflow/tensorflow/pull/2847#issuecomment-228889889", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2847", "id": 228889889, "node_id": "MDEyOklzc3VlQ29tbWVudDIyODg4OTg4OQ==", "user": {"login": "pumpikano", "id": 603889, "node_id": "MDQ6VXNlcjYwMzg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/603889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pumpikano", "html_url": "https://github.com/pumpikano", "followers_url": "https://api.github.com/users/pumpikano/followers", "following_url": "https://api.github.com/users/pumpikano/following{/other_user}", "gists_url": "https://api.github.com/users/pumpikano/gists{/gist_id}", "starred_url": "https://api.github.com/users/pumpikano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pumpikano/subscriptions", "organizations_url": "https://api.github.com/users/pumpikano/orgs", "repos_url": "https://api.github.com/users/pumpikano/repos", "events_url": "https://api.github.com/users/pumpikano/events{/privacy}", "received_events_url": "https://api.github.com/users/pumpikano/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-27T22:03:34Z", "updated_at": "2016-06-27T22:03:34Z", "author_association": "NONE", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a>, I wasn't aware of <code>tf.Graph.gradient_override_map</code>. For my use case, I need to provide a scalar tensor to scale the gradients. To do this, I need this tensor to exist already and use it in the gradient function. I came up with this approach, which I think is acceptable, if a little hacky:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">FlipGradientsBuilder</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>.num_calls <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">l</span>):\n        grad_name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FlipGradient<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">self</span>.num_calls\n        <span class=\"pl-en\">@ops.RegisterGradient</span>(grad_name)\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">_flip_gradients</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n            <span class=\"pl-k\">return</span> [tf.neg(grad) <span class=\"pl-k\">*</span> l]\n\n        g <span class=\"pl-k\">=</span> tf.get_default_graph()\n        <span class=\"pl-k\">with</span> g.gradient_override_map({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Identity<span class=\"pl-pds\">\"</span></span>: grad_name}):\n            y <span class=\"pl-k\">=</span> tf.identity(x)\n\n        <span class=\"pl-c1\">self</span>.num_calls <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        <span class=\"pl-k\">return</span> y\n\nflip_gradients <span class=\"pl-k\">=</span> FlipGradientsBuilder()</pre></div>\n<p>This allows me to call <code>y = flip_gradients(x, l)</code> any number of times when building the graph with minimal overhead as far as I can tell. I wanted to share this solution in case others find it useful.</p>", "body_text": "Thanks @girving, I wasn't aware of tf.Graph.gradient_override_map. For my use case, I need to provide a scalar tensor to scale the gradients. To do this, I need this tensor to exist already and use it in the gradient function. I came up with this approach, which I think is acceptable, if a little hacky:\nclass FlipGradientsBuilder(object):\n    def __init__(self):\n        self.num_calls = 0\n\n    def __call__(self, x, l):\n        grad_name = \"FlipGradient%d\" % self.num_calls\n        @ops.RegisterGradient(grad_name)\n        def _flip_gradients(op, grad):\n            return [tf.neg(grad) * l]\n\n        g = tf.get_default_graph()\n        with g.gradient_override_map({\"Identity\": grad_name}):\n            y = tf.identity(x)\n\n        self.num_calls += 1\n        return y\n\nflip_gradients = FlipGradientsBuilder()\nThis allows me to call y = flip_gradients(x, l) any number of times when building the graph with minimal overhead as far as I can tell. I wanted to share this solution in case others find it useful.", "body": "Thanks @girving, I wasn't aware of `tf.Graph.gradient_override_map`. For my use case, I need to provide a scalar tensor to scale the gradients. To do this, I need this tensor to exist already and use it in the gradient function. I came up with this approach, which I think is acceptable, if a little hacky:\n\n``` python\nclass FlipGradientsBuilder(object):\n    def __init__(self):\n        self.num_calls = 0\n\n    def __call__(self, x, l):\n        grad_name = \"FlipGradient%d\" % self.num_calls\n        @ops.RegisterGradient(grad_name)\n        def _flip_gradients(op, grad):\n            return [tf.neg(grad) * l]\n\n        g = tf.get_default_graph()\n        with g.gradient_override_map({\"Identity\": grad_name}):\n            y = tf.identity(x)\n\n        self.num_calls += 1\n        return y\n\nflip_gradients = FlipGradientsBuilder()\n```\n\nThis allows me to call `y = flip_gradients(x, l)` any number of times when building the graph with minimal overhead as far as I can tell. I wanted to share this solution in case others find it useful.\n"}