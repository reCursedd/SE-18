{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11582", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11582/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11582/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11582/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11582", "id": 243769479, "node_id": "MDU6SXNzdWUyNDM3Njk0Nzk=", "number": 11582, "title": "Issue while using AttentionWrapper.", "user": {"login": "PratsBhatt", "id": 15447437, "node_id": "MDQ6VXNlcjE1NDQ3NDM3", "avatar_url": "https://avatars3.githubusercontent.com/u/15447437?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PratsBhatt", "html_url": "https://github.com/PratsBhatt", "followers_url": "https://api.github.com/users/PratsBhatt/followers", "following_url": "https://api.github.com/users/PratsBhatt/following{/other_user}", "gists_url": "https://api.github.com/users/PratsBhatt/gists{/gist_id}", "starred_url": "https://api.github.com/users/PratsBhatt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PratsBhatt/subscriptions", "organizations_url": "https://api.github.com/users/PratsBhatt/orgs", "repos_url": "https://api.github.com/users/PratsBhatt/repos", "events_url": "https://api.github.com/users/PratsBhatt/events{/privacy}", "received_events_url": "https://api.github.com/users/PratsBhatt/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-18T16:06:41Z", "updated_at": "2017-07-21T20:29:08Z", "closed_at": "2017-07-21T20:29:08Z", "author_association": "NONE", "body_html": "<p>I am getting issue related to miss match of state and output. But I am unable to figure the issue.<br>\nIt would be really appreciated if someone can guide me. Thanks in advance.<br>\nI am using tensorfow-gpu==1.2.1, with 1080 Ti graphics.</p>\n<p>Error is as below:<br>\nValueError: Shapes (8, 522) and (8, 512) are incompatible</p>\n<p>Error occurs in the file \"attention_wrapper.py\" in the method named \"call\" at line 708</p>\n<p>cell_output, next_cell_state = self._cell(cell_inputs, cell_state)</p>\n<p>I was able to figure out that it is adding the attention_size to the shape and so there is a mismatch.<br>\nBut I have no idea how to fix it.<br>\nThe code is as below, hyper-parameters are declared as below (test purpose).<br>\n`<br>\nbatch_size= 8<br>\nnumber_of_units_per_layer= 512<br>\nnumber_of_layers = 3<br>\nattn_size= 10<br>\ndef build_decoder_cell(enc_output, enc_state, source_sequence_length, attn_size, batch_size):</p>\n<pre><code>encoder_outputs = enc_output\nencoder_last_state = enc_state\nencoder_inputs_length = source_sequence_length\n\nattention_mechanism = attention_wrapper.LuongAttention(\n        num_units=attn_size, memory=encoder_outputs,\n        memory_sequence_length=encoder_inputs_length,\n        scale=True,\n        name='LuongAttention' )\n\n# Building decoder_cell\ndecoder_cell_list = [\n    build_single_cell() for i in range(num_layers)]\n\ndecoder_initial_state = encoder_last_state\n\ndef attn_decoder_input_fn(inputs, attention):\n    #if not self.attn_input_feeding:\n    #    return inputs\n\n    # Essential when use_residual=True\n    _input_layer = Dense(size, dtype=tf.float32,\n                        name='attn_input_feeding')\n    return _input_layer(array_ops.concat([inputs, attention], -1))\n\n\n# AttentionWrapper wraps RNNCell with the attention_mechanism\n# Note: We implement Attention mechanism only on the top decoder layer\ndecoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\n    cell=decoder_cell_list[-1],\n    attention_mechanism=attention_mechanism,\n    attention_layer_size=attn_size,\n    #cell_input_fn=attn_decoder_input_fn,\n    initial_cell_state=encoder_last_state[-1],\n    alignment_history=False,\n    name='Attention_Wrapper')\n\n# To be compatible with AttentionWrapper, the encoder last state\n# of the top layer should be converted into the AttentionWrapperState form\n# We can easily do this by calling AttentionWrapper.zero_state\n\n# Also if beamsearch decoding is used, the batch_size argument in .zero_state\n# should be ${decoder_beam_width} times to the origianl batch_size\n#batch_size = self.batch_size if not self.use_beamsearch_decode \\\n#    else self.batch_size * self.beam_width\ninitial_state = [state for state in encoder_last_state]\n\ninitial_state[-1] = decoder_cell_list[-1].zero_state(\n    batch_size=batch_size, dtype=tf.float32)\ndecoder_initial_state = tuple(initial_state)\n\nreturn tf.contrib.rnn.MultiRNNCell(decoder_cell_list), decoder_initial_state`\n</code></pre>\n<p>Thank you once again.</p>", "body_text": "I am getting issue related to miss match of state and output. But I am unable to figure the issue.\nIt would be really appreciated if someone can guide me. Thanks in advance.\nI am using tensorfow-gpu==1.2.1, with 1080 Ti graphics.\nError is as below:\nValueError: Shapes (8, 522) and (8, 512) are incompatible\nError occurs in the file \"attention_wrapper.py\" in the method named \"call\" at line 708\ncell_output, next_cell_state = self._cell(cell_inputs, cell_state)\nI was able to figure out that it is adding the attention_size to the shape and so there is a mismatch.\nBut I have no idea how to fix it.\nThe code is as below, hyper-parameters are declared as below (test purpose).\n`\nbatch_size= 8\nnumber_of_units_per_layer= 512\nnumber_of_layers = 3\nattn_size= 10\ndef build_decoder_cell(enc_output, enc_state, source_sequence_length, attn_size, batch_size):\nencoder_outputs = enc_output\nencoder_last_state = enc_state\nencoder_inputs_length = source_sequence_length\n\nattention_mechanism = attention_wrapper.LuongAttention(\n        num_units=attn_size, memory=encoder_outputs,\n        memory_sequence_length=encoder_inputs_length,\n        scale=True,\n        name='LuongAttention' )\n\n# Building decoder_cell\ndecoder_cell_list = [\n    build_single_cell() for i in range(num_layers)]\n\ndecoder_initial_state = encoder_last_state\n\ndef attn_decoder_input_fn(inputs, attention):\n    #if not self.attn_input_feeding:\n    #    return inputs\n\n    # Essential when use_residual=True\n    _input_layer = Dense(size, dtype=tf.float32,\n                        name='attn_input_feeding')\n    return _input_layer(array_ops.concat([inputs, attention], -1))\n\n\n# AttentionWrapper wraps RNNCell with the attention_mechanism\n# Note: We implement Attention mechanism only on the top decoder layer\ndecoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\n    cell=decoder_cell_list[-1],\n    attention_mechanism=attention_mechanism,\n    attention_layer_size=attn_size,\n    #cell_input_fn=attn_decoder_input_fn,\n    initial_cell_state=encoder_last_state[-1],\n    alignment_history=False,\n    name='Attention_Wrapper')\n\n# To be compatible with AttentionWrapper, the encoder last state\n# of the top layer should be converted into the AttentionWrapperState form\n# We can easily do this by calling AttentionWrapper.zero_state\n\n# Also if beamsearch decoding is used, the batch_size argument in .zero_state\n# should be ${decoder_beam_width} times to the origianl batch_size\n#batch_size = self.batch_size if not self.use_beamsearch_decode \\\n#    else self.batch_size * self.beam_width\ninitial_state = [state for state in encoder_last_state]\n\ninitial_state[-1] = decoder_cell_list[-1].zero_state(\n    batch_size=batch_size, dtype=tf.float32)\ndecoder_initial_state = tuple(initial_state)\n\nreturn tf.contrib.rnn.MultiRNNCell(decoder_cell_list), decoder_initial_state`\n\nThank you once again.", "body": "I am getting issue related to miss match of state and output. But I am unable to figure the issue.\r\nIt would be really appreciated if someone can guide me. Thanks in advance.\r\nI am using tensorfow-gpu==1.2.1, with 1080 Ti graphics.\r\n\r\nError is as below:\r\nValueError: Shapes (8, 522) and (8, 512) are incompatible\r\n\r\nError occurs in the file \"attention_wrapper.py\" in the method named \"call\" at line 708\r\n\r\ncell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n\r\nI was able to figure out that it is adding the attention_size to the shape and so there is a mismatch.\r\nBut I have no idea how to fix it.\r\nThe code is as below, hyper-parameters are declared as below (test purpose).\r\n`\r\nbatch_size= 8\r\nnumber_of_units_per_layer= 512\r\nnumber_of_layers = 3\r\nattn_size= 10\r\ndef build_decoder_cell(enc_output, enc_state, source_sequence_length, attn_size, batch_size):\r\n\r\n    encoder_outputs = enc_output\r\n    encoder_last_state = enc_state\r\n    encoder_inputs_length = source_sequence_length\r\n\r\n    attention_mechanism = attention_wrapper.LuongAttention(\r\n            num_units=attn_size, memory=encoder_outputs,\r\n            memory_sequence_length=encoder_inputs_length,\r\n            scale=True,\r\n            name='LuongAttention' )\r\n\r\n    # Building decoder_cell\r\n    decoder_cell_list = [\r\n        build_single_cell() for i in range(num_layers)]\r\n\r\n    decoder_initial_state = encoder_last_state\r\n\r\n    def attn_decoder_input_fn(inputs, attention):\r\n        #if not self.attn_input_feeding:\r\n        #    return inputs\r\n\r\n        # Essential when use_residual=True\r\n        _input_layer = Dense(size, dtype=tf.float32,\r\n                            name='attn_input_feeding')\r\n        return _input_layer(array_ops.concat([inputs, attention], -1))\r\n\r\n\r\n    # AttentionWrapper wraps RNNCell with the attention_mechanism\r\n    # Note: We implement Attention mechanism only on the top decoder layer\r\n    decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\r\n        cell=decoder_cell_list[-1],\r\n        attention_mechanism=attention_mechanism,\r\n        attention_layer_size=attn_size,\r\n        #cell_input_fn=attn_decoder_input_fn,\r\n        initial_cell_state=encoder_last_state[-1],\r\n        alignment_history=False,\r\n        name='Attention_Wrapper')\r\n\r\n    # To be compatible with AttentionWrapper, the encoder last state\r\n    # of the top layer should be converted into the AttentionWrapperState form\r\n    # We can easily do this by calling AttentionWrapper.zero_state\r\n\r\n    # Also if beamsearch decoding is used, the batch_size argument in .zero_state\r\n    # should be ${decoder_beam_width} times to the origianl batch_size\r\n    #batch_size = self.batch_size if not self.use_beamsearch_decode \\\r\n    #    else self.batch_size * self.beam_width\r\n    initial_state = [state for state in encoder_last_state]\r\n\r\n    initial_state[-1] = decoder_cell_list[-1].zero_state(\r\n        batch_size=batch_size, dtype=tf.float32)\r\n    decoder_initial_state = tuple(initial_state)\r\n\r\n    return tf.contrib.rnn.MultiRNNCell(decoder_cell_list), decoder_initial_state`\r\n\r\nThank you once again."}