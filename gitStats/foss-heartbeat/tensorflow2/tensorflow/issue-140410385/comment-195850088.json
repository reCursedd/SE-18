{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/195850088", "html_url": "https://github.com/tensorflow/tensorflow/issues/1477#issuecomment-195850088", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1477", "id": 195850088, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTg1MDA4OA==", "user": {"login": "mznyc", "id": 17802524, "node_id": "MDQ6VXNlcjE3ODAyNTI0", "avatar_url": "https://avatars1.githubusercontent.com/u/17802524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mznyc", "html_url": "https://github.com/mznyc", "followers_url": "https://api.github.com/users/mznyc/followers", "following_url": "https://api.github.com/users/mznyc/following{/other_user}", "gists_url": "https://api.github.com/users/mznyc/gists{/gist_id}", "starred_url": "https://api.github.com/users/mznyc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mznyc/subscriptions", "organizations_url": "https://api.github.com/users/mznyc/orgs", "repos_url": "https://api.github.com/users/mznyc/repos", "events_url": "https://api.github.com/users/mznyc/events{/privacy}", "received_events_url": "https://api.github.com/users/mznyc/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-13T02:05:09Z", "updated_at": "2016-03-13T02:21:40Z", "author_association": "NONE", "body_html": "<p>I was able to train to the following steps. But then on my computer it seems the progress just stopped at step 12680 without update for almost 10 minutes now. Not sure if this is a related problem. one thing noticed is that one CPU usage is almost 100% yet GPU is mostly not busy. Could this be memory issue? My GPU memory is only 2G.</p>\n<p>Console output:</p>\n<p>2016-03-12 20:53:11.140474: step 12650, loss = 0.92 (352.5 examples/sec; 0.363 sec/batch)<br>\n2016-03-12 20:53:14.753551: step 12660, loss = 0.77 (368.4 examples/sec; 0.347 sec/batch)<br>\n2016-03-12 20:53:18.339782: step 12670, loss = 0.79 (344.2 examples/sec; 0.372 sec/batch)<br>\n2016-03-12 20:53:21.947387: step 12680, loss = 0.75 (353.8 examples/sec; 0.362 sec/batch)<br>\n(program was stuck here for a long time. After 20 minutes I just killed it).</p>\n<p>GPU usage report:<br>\n+------------------------------------------------------+<br>\n| NVIDIA-SMI 352.79     Driver Version: 352.79         |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  GeForce 840M        Off  | 0000:04:00.0     Off |                  N/A |<br>\n| N/A   50C    P0    N/A /  N/A |   1915MiB /  2047MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID  Type  Process name                               Usage      |<br>\n|=============================================================================|<br>\n|    0      1135    G   /usr/bin/X                                     117MiB |<br>\n|    0      1852    G   compiz                                          43MiB |<br>\n|    0      2310    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    75MiB |<br>\n|    0     27004    C   python                                        1670MiB |<br>\n+-----------------------------------------------------------------------------+</p>", "body_text": "I was able to train to the following steps. But then on my computer it seems the progress just stopped at step 12680 without update for almost 10 minutes now. Not sure if this is a related problem. one thing noticed is that one CPU usage is almost 100% yet GPU is mostly not busy. Could this be memory issue? My GPU memory is only 2G.\nConsole output:\n2016-03-12 20:53:11.140474: step 12650, loss = 0.92 (352.5 examples/sec; 0.363 sec/batch)\n2016-03-12 20:53:14.753551: step 12660, loss = 0.77 (368.4 examples/sec; 0.347 sec/batch)\n2016-03-12 20:53:18.339782: step 12670, loss = 0.79 (344.2 examples/sec; 0.372 sec/batch)\n2016-03-12 20:53:21.947387: step 12680, loss = 0.75 (353.8 examples/sec; 0.362 sec/batch)\n(program was stuck here for a long time. After 20 minutes I just killed it).\nGPU usage report:\n+------------------------------------------------------+\n| NVIDIA-SMI 352.79     Driver Version: 352.79         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce 840M        Off  | 0000:04:00.0     Off |                  N/A |\n| N/A   50C    P0    N/A /  N/A |   1915MiB /  2047MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1135    G   /usr/bin/X                                     117MiB |\n|    0      1852    G   compiz                                          43MiB |\n|    0      2310    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    75MiB |\n|    0     27004    C   python                                        1670MiB |\n+-----------------------------------------------------------------------------+", "body": "I was able to train to the following steps. But then on my computer it seems the progress just stopped at step 12680 without update for almost 10 minutes now. Not sure if this is a related problem. one thing noticed is that one CPU usage is almost 100% yet GPU is mostly not busy. Could this be memory issue? My GPU memory is only 2G. \n\nConsole output:\n\n2016-03-12 20:53:11.140474: step 12650, loss = 0.92 (352.5 examples/sec; 0.363 sec/batch)\n2016-03-12 20:53:14.753551: step 12660, loss = 0.77 (368.4 examples/sec; 0.347 sec/batch)\n2016-03-12 20:53:18.339782: step 12670, loss = 0.79 (344.2 examples/sec; 0.372 sec/batch)\n2016-03-12 20:53:21.947387: step 12680, loss = 0.75 (353.8 examples/sec; 0.362 sec/batch)\n(program was stuck here for a long time. After 20 minutes I just killed it). \n\nGPU usage report:\n+------------------------------------------------------+  \n| NVIDIA-SMI 352.79     Driver Version: 352.79         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce 840M        Off  | 0000:04:00.0     Off |                  N/A |\n| N/A   50C    P0    N/A /  N/A |   1915MiB /  2047MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1135    G   /usr/bin/X                                     117MiB |\n|    0      1852    G   compiz                                          43MiB |\n|    0      2310    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    75MiB |\n|    0     27004    C   python                                        1670MiB |\n+-----------------------------------------------------------------------------+\n"}