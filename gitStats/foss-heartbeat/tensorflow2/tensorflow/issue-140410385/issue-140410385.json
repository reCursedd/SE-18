{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1477", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1477/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1477/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1477/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1477", "id": 140410385, "node_id": "MDU6SXNzdWUxNDA0MTAzODU=", "number": 1477, "title": "CUDA_ERROR_MISALIGNED_ADDRESS running CIFAR10 on CUDA7.5 with cuDNN v3, built from source", "user": {"login": "mznyc", "id": 17802524, "node_id": "MDQ6VXNlcjE3ODAyNTI0", "avatar_url": "https://avatars1.githubusercontent.com/u/17802524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mznyc", "html_url": "https://github.com/mznyc", "followers_url": "https://api.github.com/users/mznyc/followers", "following_url": "https://api.github.com/users/mznyc/following{/other_user}", "gists_url": "https://api.github.com/users/mznyc/gists{/gist_id}", "starred_url": "https://api.github.com/users/mznyc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mznyc/subscriptions", "organizations_url": "https://api.github.com/users/mznyc/orgs", "repos_url": "https://api.github.com/users/mznyc/repos", "events_url": "https://api.github.com/users/mznyc/events{/privacy}", "received_events_url": "https://api.github.com/users/mznyc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-03-12T19:34:56Z", "updated_at": "2016-03-13T02:21:40Z", "closed_at": "2016-03-13T01:27:46Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04.4 LTS, NVIDIA GeForce 840M</p>\n<p>Installed from sources,  commit hash:<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/30b52579f6d66071ac7cdc7179e2c4aae3c9cb88/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/30b52579f6d66071ac7cdc7179e2c4aae3c9cb88\"><tt>30b5257</tt></a></p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>run python  cifar10_train.py</li>\n<li>program errorr out after a while<br>\n3.</li>\n</ol>\n<h3>What have you tried?</h3>\n<ol>\n<li>restart machine and rerun several times. The same issue.</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).</p>\n<p>2016-03-12 14:02:02.607728: step 4010, loss = 1.22 (421.6 examples/sec; 0.304 sec/batch)<br>\n2016-03-12 14:02:05.731932: step 4020, loss = 1.18 (425.2 examples/sec; 0.301 sec/batch)<br>\n2016-03-12 14:02:08.874431: step 4030, loss = 1.21 (440.1 examples/sec; 0.291 sec/batch)<br>\n2016-03-12 14:02:12.053320: step 4040, loss = 1.30 (367.8 examples/sec; 0.348 sec/batch)<br>\n2016-03-12 14:02:15.304938: step 4050, loss = 1.17 (391.1 examples/sec; 0.327 sec/batch)<br>\n2016-03-12 14:02:18.456706: step 4060, loss = 1.23 (416.5 examples/sec; 0.307 sec/batch)<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8333348 get requests, put_count=8333340 evicted_count=1000 eviction_rate=0.00012 and unsatisfied allocation rate=0.00013296<br>\n2016-03-12 14:02:21.675055: step 4070, loss = 1.19 (424.8 examples/sec; 0.301 sec/batch)<br>\n2016-03-12 14:02:24.810989: step 4080, loss = 1.26 (412.4 examples/sec; 0.310 sec/batch)<br>\n2016-03-12 14:02:28.031250: step 4090, loss = 1.21 (424.3 examples/sec; 0.302 sec/batch)<br>\n2016-03-12 14:02:31.328527: step 4100, loss = 1.13 (396.7 examples/sec; 0.323 sec/batch)<br>\n2016-03-12 14:02:35.028202: step 4110, loss = 1.23 (414.8 examples/sec; 0.309 sec/batch)<br>\n2016-03-12 14:02:38.254790: step 4120, loss = 1.35 (364.6 examples/sec; 0.351 sec/batch)<br>\n2016-03-12 14:02:41.484692: step 4130, loss = 1.47 (415.9 examples/sec; 0.308 sec/batch)<br>\n2016-03-12 14:02:44.718649: step 4140, loss = 1.26 (406.7 examples/sec; 0.315 sec/batch)<br>\n2016-03-12 14:02:47.988837: step 4150, loss = 1.07 (408.5 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:02:51.287844: step 4160, loss = 1.17 (404.1 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:02:54.654917: step 4170, loss = 1.33 (404.0 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:02:57.782336: step 4180, loss = 1.18 (404.1 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:03:01.022658: step 4190, loss = 1.15 (409.9 examples/sec; 0.312 sec/batch)<br>\n2016-03-12 14:03:04.232610: step 4200, loss = 1.24 (365.7 examples/sec; 0.350 sec/batch)<br>\n2016-03-12 14:03:07.854789: step 4210, loss = 1.11 (400.8 examples/sec; 0.319 sec/batch)<br>\n2016-03-12 14:03:11.121573: step 4220, loss = 1.31 (371.6 examples/sec; 0.344 sec/batch)<br>\n2016-03-12 14:03:14.307746: step 4230, loss = 1.46 (382.4 examples/sec; 0.335 sec/batch)<br>\n2016-03-12 14:03:17.566388: step 4240, loss = 1.14 (425.0 examples/sec; 0.301 sec/batch)<br>\n2016-03-12 14:03:20.750270: step 4250, loss = 1.21 (412.2 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:03:24.010454: step 4260, loss = 1.16 (401.9 examples/sec; 0.319 sec/batch)<br>\n2016-03-12 14:03:27.315912: step 4270, loss = 1.16 (351.9 examples/sec; 0.364 sec/batch)<br>\n2016-03-12 14:03:30.528823: step 4280, loss = 1.24 (397.8 examples/sec; 0.322 sec/batch)<br>\n2016-03-12 14:03:33.777292: step 4290, loss = 1.45 (393.2 examples/sec; 0.326 sec/batch)<br>\n2016-03-12 14:03:37.022320: step 4300, loss = 1.19 (396.9 examples/sec; 0.323 sec/batch)<br>\n2016-03-12 14:03:40.659591: step 4310, loss = 1.36 (415.7 examples/sec; 0.308 sec/batch)<br>\n2016-03-12 14:03:43.828922: step 4320, loss = 1.18 (396.4 examples/sec; 0.323 sec/batch)<br>\n2016-03-12 14:03:47.227371: step 4330, loss = 0.96 (377.0 examples/sec; 0.340 sec/batch)<br>\n2016-03-12 14:03:50.400040: step 4340, loss = 1.13 (415.3 examples/sec; 0.308 sec/batch)<br>\n2016-03-12 14:03:53.671297: step 4350, loss = 1.12 (405.1 examples/sec; 0.316 sec/batch)<br>\n2016-03-12 14:03:56.815646: step 4360, loss = 1.09 (414.5 examples/sec; 0.309 sec/batch)<br>\n2016-03-12 14:04:00.007053: step 4370, loss = 1.22 (382.2 examples/sec; 0.335 sec/batch)<br>\n2016-03-12 14:04:03.242795: step 4380, loss = 1.19 (414.5 examples/sec; 0.309 sec/batch)<br>\n2016-03-12 14:04:06.424938: step 4390, loss = 1.13 (392.6 examples/sec; 0.326 sec/batch)<br>\n2016-03-12 14:04:09.578756: step 4400, loss = 1.02 (402.1 examples/sec; 0.318 sec/batch)<br>\n2016-03-12 14:04:13.179099: step 4410, loss = 1.34 (409.3 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:04:16.348951: step 4420, loss = 1.21 (408.8 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:04:19.599094: step 4430, loss = 1.10 (420.9 examples/sec; 0.304 sec/batch)<br>\n2016-03-12 14:04:22.736091: step 4440, loss = 1.11 (407.3 examples/sec; 0.314 sec/batch)<br>\n2016-03-12 14:04:25.979362: step 4450, loss = 1.27 (389.7 examples/sec; 0.328 sec/batch)<br>\n2016-03-12 14:04:29.173873: step 4460, loss = 1.18 (387.9 examples/sec; 0.330 sec/batch)<br>\n2016-03-12 14:04:32.369984: step 4470, loss = 1.36 (422.4 examples/sec; 0.303 sec/batch)<br>\n2016-03-12 14:04:35.565128: step 4480, loss = 1.19 (409.5 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:04:38.783202: step 4490, loss = 1.14 (391.8 examples/sec; 0.327 sec/batch)<br>\n2016-03-12 14:04:41.968468: step 4500, loss = 1.20 (389.9 examples/sec; 0.328 sec/batch)<br>\n2016-03-12 14:04:45.529786: step 4510, loss = 1.14 (397.3 examples/sec; 0.322 sec/batch)<br>\n2016-03-12 14:04:48.693115: step 4520, loss = 1.12 (393.7 examples/sec; 0.325 sec/batch)<br>\n2016-03-12 14:04:51.892441: step 4530, loss = 1.25 (399.6 examples/sec; 0.320 sec/batch)<br>\n2016-03-12 14:04:55.102551: step 4540, loss = 1.26 (411.5 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:04:58.258904: step 4550, loss = 1.27 (395.5 examples/sec; 0.324 sec/batch)<br>\n2016-03-12 14:05:01.485362: step 4560, loss = 1.08 (404.6 examples/sec; 0.316 sec/batch)<br>\n2016-03-12 14:05:04.711979: step 4570, loss = 1.25 (365.1 examples/sec; 0.351 sec/batch)<br>\n2016-03-12 14:05:07.864737: step 4580, loss = 1.31 (390.1 examples/sec; 0.328 sec/batch)<br>\n2016-03-12 14:05:11.075490: step 4590, loss = 1.21 (385.9 examples/sec; 0.332 sec/batch)<br>\n2016-03-12 14:05:14.205806: step 4600, loss = 0.96 (419.8 examples/sec; 0.305 sec/batch)<br>\n2016-03-12 14:05:17.764024: step 4610, loss = 1.16 (419.8 examples/sec; 0.305 sec/batch)<br>\n2016-03-12 14:05:20.992224: step 4620, loss = 1.17 (407.9 examples/sec; 0.314 sec/batch)<br>\n2016-03-12 14:05:24.182559: step 4630, loss = 1.20 (406.1 examples/sec; 0.315 sec/batch)<br>\n2016-03-12 14:05:27.346289: step 4640, loss = 1.30 (421.5 examples/sec; 0.304 sec/batch)<br>\n2016-03-12 14:05:30.537761: step 4650, loss = 1.05 (403.8 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:05:33.755113: step 4660, loss = 1.10 (394.2 examples/sec; 0.325 sec/batch)<br>\n2016-03-12 14:05:36.904973: step 4670, loss = 1.20 (402.2 examples/sec; 0.318 sec/batch)<br>\n2016-03-12 14:05:40.113832: step 4680, loss = 1.10 (407.4 examples/sec; 0.314 sec/batch)<br>\n2016-03-12 14:05:43.286804: step 4690, loss = 1.05 (406.8 examples/sec; 0.315 sec/batch)<br>\n2016-03-12 14:05:46.492911: step 4700, loss = 1.19 (427.0 examples/sec; 0.300 sec/batch)<br>\n2016-03-12 14:05:50.100880: step 4710, loss = 1.09 (414.9 examples/sec; 0.309 sec/batch)<br>\n2016-03-12 14:05:53.295059: step 4720, loss = 1.16 (403.4 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:05:56.461129: step 4730, loss = 1.03 (418.4 examples/sec; 0.306 sec/batch)<br>\n2016-03-12 14:05:59.686046: step 4740, loss = 1.01 (392.7 examples/sec; 0.326 sec/batch)<br>\n2016-03-12 14:06:02.874850: step 4750, loss = 1.11 (403.2 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:06:06.063848: step 4760, loss = 1.15 (421.9 examples/sec; 0.303 sec/batch)<br>\n2016-03-12 14:06:09.311339: step 4770, loss = 1.00 (401.5 examples/sec; 0.319 sec/batch)<br>\n2016-03-12 14:06:12.400291: step 4780, loss = 1.03 (412.0 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:06:15.594995: step 4790, loss = 1.08 (372.0 examples/sec; 0.344 sec/batch)<br>\n2016-03-12 14:06:18.774854: step 4800, loss = 1.31 (378.7 examples/sec; 0.338 sec/batch)<br>\n2016-03-12 14:06:22.455985: step 4810, loss = 1.17 (359.9 examples/sec; 0.356 sec/batch)<br>\n2016-03-12 14:06:25.655368: step 4820, loss = 1.04 (420.3 examples/sec; 0.305 sec/batch)<br>\n2016-03-12 14:06:28.822332: step 4830, loss = 1.26 (404.5 examples/sec; 0.316 sec/batch)<br>\n2016-03-12 14:06:32.007472: step 4840, loss = 1.01 (411.2 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:06:35.283917: step 4850, loss = 1.41 (434.8 examples/sec; 0.294 sec/batch)<br>\n2016-03-12 14:06:38.506687: step 4860, loss = 1.03 (401.3 examples/sec; 0.319 sec/batch)<br>\n2016-03-12 14:06:41.663434: step 4870, loss = 1.12 (395.4 examples/sec; 0.324 sec/batch)<br>\n2016-03-12 14:06:44.828674: step 4880, loss = 1.21 (405.5 examples/sec; 0.316 sec/batch)<br>\n2016-03-12 14:06:48.007878: step 4890, loss = 1.09 (410.4 examples/sec; 0.312 sec/batch)<br>\n2016-03-12 14:06:51.222651: step 4900, loss = 1.00 (396.6 examples/sec; 0.323 sec/batch)<br>\n2016-03-12 14:06:54.860536: step 4910, loss = 1.20 (383.4 examples/sec; 0.334 sec/batch)<br>\n2016-03-12 14:06:58.063615: step 4920, loss = 1.17 (410.9 examples/sec; 0.312 sec/batch)<br>\n2016-03-12 14:07:01.184445: step 4930, loss = 1.05 (397.0 examples/sec; 0.322 sec/batch)<br>\n2016-03-12 14:07:04.394785: step 4940, loss = 1.06 (371.3 examples/sec; 0.345 sec/batch)<br>\n2016-03-12 14:07:07.583337: step 4950, loss = 1.17 (394.5 examples/sec; 0.324 sec/batch)<br>\n2016-03-12 14:07:10.724218: step 4960, loss = 1.09 (397.4 examples/sec; 0.322 sec/batch)<br>\n2016-03-12 14:07:13.931494: step 4970, loss = 1.04 (389.5 examples/sec; 0.329 sec/batch)<br>\n2016-03-12 14:07:17.116672: step 4980, loss = 1.30 (405.9 examples/sec; 0.315 sec/batch)<br>\n2016-03-12 14:07:20.301438: step 4990, loss = 1.26 (427.9 examples/sec; 0.299 sec/batch)<br>\n2016-03-12 14:07:23.464006: step 5000, loss = 1.18 (380.4 examples/sec; 0.337 sec/batch)<br>\n2016-03-12 14:07:27.304553: step 5010, loss = 1.21 (378.2 examples/sec; 0.338 sec/batch)<br>\n2016-03-12 14:07:30.435311: step 5020, loss = 0.96 (420.0 examples/sec; 0.305 sec/batch)<br>\n2016-03-12 14:07:33.525055: step 5030, loss = 1.50 (426.5 examples/sec; 0.300 sec/batch)<br>\n2016-03-12 14:07:36.664561: step 5040, loss = 1.09 (400.9 examples/sec; 0.319 sec/batch)<br>\n2016-03-12 14:07:39.873659: step 5050, loss = 1.09 (422.8 examples/sec; 0.303 sec/batch)<br>\n2016-03-12 14:07:43.060801: step 5060, loss = 1.00 (409.5 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:07:46.216866: step 5070, loss = 1.22 (424.8 examples/sec; 0.301 sec/batch)<br>\n2016-03-12 14:07:49.424010: step 5080, loss = 1.20 (396.9 examples/sec; 0.323 sec/batch)<br>\n2016-03-12 14:07:52.615564: step 5090, loss = 1.03 (393.3 examples/sec; 0.325 sec/batch)<br>\n2016-03-12 14:07:55.773694: step 5100, loss = 1.43 (421.5 examples/sec; 0.304 sec/batch)<br>\n2016-03-12 14:07:59.381470: step 5110, loss = 1.00 (420.5 examples/sec; 0.304 sec/batch)<br>\n2016-03-12 14:08:02.574327: step 5120, loss = 1.08 (412.5 examples/sec; 0.310 sec/batch)<br>\n2016-03-12 14:08:05.758566: step 5130, loss = 0.94 (413.8 examples/sec; 0.309 sec/batch)<br>\n2016-03-12 14:08:08.940488: step 5140, loss = 1.07 (414.1 examples/sec; 0.309 sec/batch)<br>\n2016-03-12 14:08:12.128661: step 5150, loss = 1.15 (374.6 examples/sec; 0.342 sec/batch)<br>\n2016-03-12 14:08:15.321906: step 5160, loss = 1.05 (388.2 examples/sec; 0.330 sec/batch)<br>\n2016-03-12 14:08:18.465653: step 5170, loss = 1.05 (394.7 examples/sec; 0.324 sec/batch)<br>\n2016-03-12 14:08:21.695237: step 5180, loss = 1.17 (405.3 examples/sec; 0.316 sec/batch)<br>\n2016-03-12 14:08:24.817479: step 5190, loss = 1.37 (404.4 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:08:28.045635: step 5200, loss = 1.06 (411.1 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:08:31.625961: step 5210, loss = 0.98 (393.0 examples/sec; 0.326 sec/batch)<br>\n2016-03-12 14:08:34.803401: step 5220, loss = 1.16 (404.8 examples/sec; 0.316 sec/batch)<br>\n2016-03-12 14:08:38.064956: step 5230, loss = 1.22 (386.5 examples/sec; 0.331 sec/batch)<br>\n2016-03-12 14:08:42.036235: step 5240, loss = 0.93 (309.9 examples/sec; 0.413 sec/batch)<br>\n2016-03-12 14:08:45.755935: step 5250, loss = 1.09 (332.2 examples/sec; 0.385 sec/batch)<br>\n2016-03-12 14:08:48.961625: step 5260, loss = 1.24 (413.0 examples/sec; 0.310 sec/batch)<br>\n2016-03-12 14:08:52.615963: step 5270, loss = 1.15 (318.3 examples/sec; 0.402 sec/batch)<br>\n2016-03-12 14:08:56.692628: step 5280, loss = 1.28 (331.9 examples/sec; 0.386 sec/batch)<br>\n2016-03-12 14:09:00.013660: step 5290, loss = 1.18 (429.9 examples/sec; 0.298 sec/batch)<br>\n2016-03-12 14:09:03.394530: step 5300, loss = 0.97 (307.2 examples/sec; 0.417 sec/batch)<br>\n2016-03-12 14:09:07.474397: step 5310, loss = 1.28 (250.3 examples/sec; 0.511 sec/batch)<br>\n2016-03-12 14:09:11.289658: step 5320, loss = 1.03 (307.2 examples/sec; 0.417 sec/batch)<br>\n2016-03-12 14:09:14.478318: step 5330, loss = 0.94 (399.5 examples/sec; 0.320 sec/batch)<br>\n2016-03-12 14:09:17.677547: step 5340, loss = 1.04 (382.3 examples/sec; 0.335 sec/batch)<br>\n2016-03-12 14:09:20.832572: step 5350, loss = 1.08 (433.3 examples/sec; 0.295 sec/batch)<br>\n2016-03-12 14:09:24.024293: step 5360, loss = 1.07 (398.9 examples/sec; 0.321 sec/batch)<br>\n2016-03-12 14:09:27.197269: step 5370, loss = 1.06 (416.3 examples/sec; 0.307 sec/batch)<br>\n2016-03-12 14:09:30.623629: step 5380, loss = 1.04 (324.2 examples/sec; 0.395 sec/batch)<br>\n2016-03-12 14:09:34.399873: step 5390, loss = 1.22 (241.2 examples/sec; 0.531 sec/batch)<br>\n2016-03-12 14:09:38.530928: step 5400, loss = 1.13 (270.8 examples/sec; 0.473 sec/batch)<br>\n2016-03-12 14:09:42.934882: step 5410, loss = 0.90 (268.9 examples/sec; 0.476 sec/batch)<br>\n2016-03-12 14:09:47.006334: step 5420, loss = 0.95 (347.2 examples/sec; 0.369 sec/batch)<br>\n2016-03-12 14:09:50.212403: step 5430, loss = 1.19 (441.3 examples/sec; 0.290 sec/batch)<br>\n2016-03-12 14:09:53.401629: step 5440, loss = 1.11 (417.6 examples/sec; 0.307 sec/batch)<br>\n2016-03-12 14:09:56.592893: step 5450, loss = 1.09 (413.3 examples/sec; 0.310 sec/batch)<br>\n2016-03-12 14:09:59.731023: step 5460, loss = 1.14 (392.4 examples/sec; 0.326 sec/batch)<br>\n2016-03-12 14:10:02.961094: step 5470, loss = 1.07 (413.1 examples/sec; 0.310 sec/batch)<br>\n2016-03-12 14:10:06.190783: step 5480, loss = 1.17 (380.0 examples/sec; 0.337 sec/batch)<br>\n2016-03-12 14:10:09.358733: step 5490, loss = 1.06 (390.4 examples/sec; 0.328 sec/batch)<br>\n2016-03-12 14:10:12.525256: step 5500, loss = 1.03 (423.3 examples/sec; 0.302 sec/batch)<br>\n2016-03-12 14:10:16.103487: step 5510, loss = 0.98 (418.0 examples/sec; 0.306 sec/batch)<br>\n2016-03-12 14:10:19.302679: step 5520, loss = 1.08 (395.4 examples/sec; 0.324 sec/batch)<br>\n2016-03-12 14:10:22.487560: step 5530, loss = 1.16 (423.4 examples/sec; 0.302 sec/batch)<br>\n2016-03-12 14:10:25.632955: step 5540, loss = 1.20 (416.8 examples/sec; 0.307 sec/batch)<br>\n2016-03-12 14:10:28.837518: step 5550, loss = 1.03 (409.2 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:10:32.006405: step 5560, loss = 1.06 (399.6 examples/sec; 0.320 sec/batch)<br>\n2016-03-12 14:10:35.165191: step 5570, loss = 0.95 (442.7 examples/sec; 0.289 sec/batch)<br>\n2016-03-12 14:10:38.384486: step 5580, loss = 1.17 (439.7 examples/sec; 0.291 sec/batch)<br>\n2016-03-12 14:10:41.575061: step 5590, loss = 1.10 (368.8 examples/sec; 0.347 sec/batch)<br>\n2016-03-12 14:10:44.729184: step 5600, loss = 0.89 (423.4 examples/sec; 0.302 sec/batch)<br>\n2016-03-12 14:10:48.343008: step 5610, loss = 1.09 (416.0 examples/sec; 0.308 sec/batch)<br>\n2016-03-12 14:10:51.464913: step 5620, loss = 1.18 (388.3 examples/sec; 0.330 sec/batch)<br>\n2016-03-12 14:10:54.620341: step 5630, loss = 1.05 (421.4 examples/sec; 0.304 sec/batch)<br>\n2016-03-12 14:10:57.837872: step 5640, loss = 1.17 (419.3 examples/sec; 0.305 sec/batch)<br>\n2016-03-12 14:11:01.090029: step 5650, loss = 1.13 (411.3 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:11:04.395310: step 5660, loss = 1.05 (399.1 examples/sec; 0.321 sec/batch)<br>\n2016-03-12 14:11:07.784719: step 5670, loss = 1.05 (366.3 examples/sec; 0.349 sec/batch)<br>\n2016-03-12 14:11:11.604344: step 5680, loss = 0.99 (380.1 examples/sec; 0.337 sec/batch)<br>\n2016-03-12 14:11:15.243982: step 5690, loss = 1.10 (332.7 examples/sec; 0.385 sec/batch)<br>\n2016-03-12 14:11:19.011393: step 5700, loss = 1.14 (388.5 examples/sec; 0.330 sec/batch)<br>\n2016-03-12 14:11:22.897067: step 5710, loss = 1.10 (389.7 examples/sec; 0.328 sec/batch)<br>\n2016-03-12 14:11:26.257571: step 5720, loss = 1.10 (392.0 examples/sec; 0.327 sec/batch)<br>\n2016-03-12 14:11:29.648071: step 5730, loss = 1.05 (378.9 examples/sec; 0.338 sec/batch)<br>\n2016-03-12 14:11:33.027954: step 5740, loss = 1.25 (349.1 examples/sec; 0.367 sec/batch)<br>\n2016-03-12 14:11:36.389940: step 5750, loss = 0.88 (388.9 examples/sec; 0.329 sec/batch)<br>\n2016-03-12 14:11:39.740017: step 5760, loss = 1.15 (386.6 examples/sec; 0.331 sec/batch)<br>\n2016-03-12 14:11:43.111565: step 5770, loss = 0.99 (384.3 examples/sec; 0.333 sec/batch)<br>\n2016-03-12 14:11:46.472003: step 5780, loss = 1.01 (387.7 examples/sec; 0.330 sec/batch)<br>\n2016-03-12 14:11:49.950465: step 5790, loss = 0.89 (363.1 examples/sec; 0.352 sec/batch)<br>\n2016-03-12 14:11:53.508106: step 5800, loss = 1.23 (348.4 examples/sec; 0.367 sec/batch)<br>\n2016-03-12 14:11:57.342506: step 5810, loss = 1.13 (407.0 examples/sec; 0.314 sec/batch)<br>\n2016-03-12 14:12:01.130117: step 5820, loss = 1.17 (409.0 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:12:04.352433: step 5830, loss = 1.05 (399.8 examples/sec; 0.320 sec/batch)<br>\n2016-03-12 14:12:07.511581: step 5840, loss = 1.05 (435.9 examples/sec; 0.294 sec/batch)<br>\n2016-03-12 14:12:10.730863: step 5850, loss = 1.02 (412.3 examples/sec; 0.310 sec/batch)<br>\n2016-03-12 14:12:13.959593: step 5860, loss = 1.10 (355.5 examples/sec; 0.360 sec/batch)<br>\n2016-03-12 14:12:17.241367: step 5870, loss = 1.10 (441.1 examples/sec; 0.290 sec/batch)<br>\n2016-03-12 14:12:20.945846: step 5880, loss = 0.96 (339.6 examples/sec; 0.377 sec/batch)<br>\n2016-03-12 14:12:24.060260: step 5890, loss = 1.14 (398.4 examples/sec; 0.321 sec/batch)<br>\n2016-03-12 14:12:27.196011: step 5900, loss = 1.02 (425.1 examples/sec; 0.301 sec/batch)<br>\n2016-03-12 14:12:30.779291: step 5910, loss = 1.06 (420.0 examples/sec; 0.305 sec/batch)<br>\n2016-03-12 14:12:33.974906: step 5920, loss = 1.10 (403.1 examples/sec; 0.318 sec/batch)<br>\n2016-03-12 14:12:37.188986: step 5930, loss = 1.03 (381.9 examples/sec; 0.335 sec/batch)<br>\n2016-03-12 14:12:40.549341: step 5940, loss = 0.97 (399.6 examples/sec; 0.320 sec/batch)<br>\n2016-03-12 14:12:43.776408: step 5950, loss = 1.10 (386.8 examples/sec; 0.331 sec/batch)<br>\n2016-03-12 14:12:46.910332: step 5960, loss = 1.15 (383.1 examples/sec; 0.334 sec/batch)<br>\n2016-03-12 14:12:50.070772: step 5970, loss = 0.97 (436.3 examples/sec; 0.293 sec/batch)<br>\n2016-03-12 14:12:53.283043: step 5980, loss = 0.91 (400.2 examples/sec; 0.320 sec/batch)<br>\n2016-03-12 14:12:57.333740: step 5990, loss = 1.07 (389.0 examples/sec; 0.329 sec/batch)<br>\n2016-03-12 14:13:00.647640: step 6000, loss = 0.97 (371.8 examples/sec; 0.344 sec/batch)<br>\n2016-03-12 14:13:04.837750: step 6010, loss = 1.17 (304.9 examples/sec; 0.420 sec/batch)<br>\n2016-03-12 14:13:08.816024: step 6020, loss = 1.07 (336.6 examples/sec; 0.380 sec/batch)<br>\n2016-03-12 14:13:12.670182: step 6030, loss = 1.00 (260.2 examples/sec; 0.492 sec/batch)<br>\n2016-03-12 14:13:16.635798: step 6040, loss = 0.98 (295.0 examples/sec; 0.434 sec/batch)<br>\n2016-03-12 14:13:20.590717: step 6050, loss = 1.05 (436.9 examples/sec; 0.293 sec/batch)<br>\n2016-03-12 14:13:24.389427: step 6060, loss = 1.12 (303.6 examples/sec; 0.422 sec/batch)<br>\n2016-03-12 14:13:27.919392: step 6070, loss = 1.12 (403.5 examples/sec; 0.317 sec/batch)<br>\n2016-03-12 14:13:31.241030: step 6080, loss = 1.19 (374.7 examples/sec; 0.342 sec/batch)<br>\n2016-03-12 14:13:36.332882: step 6090, loss = 1.19 (342.0 examples/sec; 0.374 sec/batch)<br>\n2016-03-12 14:13:39.823059: step 6100, loss = 0.99 (363.2 examples/sec; 0.352 sec/batch)<br>\n2016-03-12 14:13:44.250455: step 6110, loss = 1.10 (352.2 examples/sec; 0.363 sec/batch)<br>\n2016-03-12 14:13:47.462684: step 6120, loss = 0.93 (429.3 examples/sec; 0.298 sec/batch)<br>\n2016-03-12 14:13:50.718381: step 6130, loss = 1.24 (405.0 examples/sec; 0.316 sec/batch)<br>\n2016-03-12 14:13:53.870993: step 6140, loss = 0.91 (382.4 examples/sec; 0.335 sec/batch)<br>\n2016-03-12 14:13:56.999939: step 6150, loss = 1.09 (415.8 examples/sec; 0.308 sec/batch)<br>\n2016-03-12 14:14:00.147406: step 6160, loss = 1.03 (408.5 examples/sec; 0.313 sec/batch)<br>\n2016-03-12 14:14:03.290188: step 6170, loss = 1.04 (417.4 examples/sec; 0.307 sec/batch)<br>\n2016-03-12 14:14:06.453738: step 6180, loss = 0.88 (422.1 examples/sec; 0.303 sec/batch)<br>\n2016-03-12 14:14:09.616241: step 6190, loss = 0.98 (393.4 examples/sec; 0.325 sec/batch)<br>\n2016-03-12 14:14:12.722891: step 6200, loss = 1.06 (413.3 examples/sec; 0.310 sec/batch)<br>\n2016-03-12 14:14:16.362099: step 6210, loss = 1.01 (433.5 examples/sec; 0.295 sec/batch)<br>\n2016-03-12 14:14:19.472411: step 6220, loss = 1.02 (436.2 examples/sec; 0.293 sec/batch)<br>\n2016-03-12 14:14:22.663895: step 6230, loss = 1.07 (416.9 examples/sec; 0.307 sec/batch)<br>\n2016-03-12 14:14:26.117081: step 6240, loss = 1.17 (386.0 examples/sec; 0.332 sec/batch)<br>\n2016-03-12 14:14:29.231502: step 6250, loss = 0.90 (411.4 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:14:32.351659: step 6260, loss = 0.97 (409.6 examples/sec; 0.312 sec/batch)<br>\n2016-03-12 14:14:35.495727: step 6270, loss = 0.98 (401.1 examples/sec; 0.319 sec/batch)<br>\n2016-03-12 14:14:38.677070: step 6280, loss = 1.17 (390.6 examples/sec; 0.328 sec/batch)<br>\n2016-03-12 14:14:41.822706: step 6290, loss = 0.97 (436.9 examples/sec; 0.293 sec/batch)<br>\n2016-03-12 14:14:44.955144: step 6300, loss = 0.91 (442.9 examples/sec; 0.289 sec/batch)<br>\n2016-03-12 14:14:48.526671: step 6310, loss = 0.89 (356.7 examples/sec; 0.359 sec/batch)<br>\n2016-03-12 14:14:51.623469: step 6320, loss = 1.15 (395.3 examples/sec; 0.324 sec/batch)<br>\n2016-03-12 14:14:54.781165: step 6330, loss = 1.25 (387.0 examples/sec; 0.331 sec/batch)<br>\n2016-03-12 14:14:57.893973: step 6340, loss = 1.13 (426.3 examples/sec; 0.300 sec/batch)<br>\n2016-03-12 14:15:01.030509: step 6350, loss = 0.92 (411.7 examples/sec; 0.311 sec/batch)<br>\n2016-03-12 14:15:04.212174: step 6360, loss = 0.96 (414.8 examples/sec; 0.309 sec/batch)<br>\n2016-03-12 14:15:07.610402: step 6370, loss = 1.07 (397.9 examples/sec; 0.322 sec/batch)<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1088] could not wait stream on event: CUDA_ERROR_MISALIGNED_ADDRESS<br>\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7012acd00<br>\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_MISALIGNED_ADDRESS; host dst: 0x7fb6276cff80; GPU src: 0x7011c2300; size: 1=0x1<br>\nI tensorflow/stream_executor/stream.cc:826] stream 0x2f7bdd0 did not wait for stream: 0x2f7a8b0<br>\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:193] Unexpected Event status: 1<br>\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7011e2d00<br>\nAborted (core dumped)</p>", "body_text": "Environment info\nOperating System: Ubuntu 14.04.4 LTS, NVIDIA GeForce 840M\nInstalled from sources,  commit hash:\n30b5257\nSteps to reproduce\n\nrun python  cifar10_train.py\nprogram errorr out after a while\n3.\n\nWhat have you tried?\n\nrestart machine and rerun several times. The same issue.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).\n2016-03-12 14:02:02.607728: step 4010, loss = 1.22 (421.6 examples/sec; 0.304 sec/batch)\n2016-03-12 14:02:05.731932: step 4020, loss = 1.18 (425.2 examples/sec; 0.301 sec/batch)\n2016-03-12 14:02:08.874431: step 4030, loss = 1.21 (440.1 examples/sec; 0.291 sec/batch)\n2016-03-12 14:02:12.053320: step 4040, loss = 1.30 (367.8 examples/sec; 0.348 sec/batch)\n2016-03-12 14:02:15.304938: step 4050, loss = 1.17 (391.1 examples/sec; 0.327 sec/batch)\n2016-03-12 14:02:18.456706: step 4060, loss = 1.23 (416.5 examples/sec; 0.307 sec/batch)\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8333348 get requests, put_count=8333340 evicted_count=1000 eviction_rate=0.00012 and unsatisfied allocation rate=0.00013296\n2016-03-12 14:02:21.675055: step 4070, loss = 1.19 (424.8 examples/sec; 0.301 sec/batch)\n2016-03-12 14:02:24.810989: step 4080, loss = 1.26 (412.4 examples/sec; 0.310 sec/batch)\n2016-03-12 14:02:28.031250: step 4090, loss = 1.21 (424.3 examples/sec; 0.302 sec/batch)\n2016-03-12 14:02:31.328527: step 4100, loss = 1.13 (396.7 examples/sec; 0.323 sec/batch)\n2016-03-12 14:02:35.028202: step 4110, loss = 1.23 (414.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:02:38.254790: step 4120, loss = 1.35 (364.6 examples/sec; 0.351 sec/batch)\n2016-03-12 14:02:41.484692: step 4130, loss = 1.47 (415.9 examples/sec; 0.308 sec/batch)\n2016-03-12 14:02:44.718649: step 4140, loss = 1.26 (406.7 examples/sec; 0.315 sec/batch)\n2016-03-12 14:02:47.988837: step 4150, loss = 1.07 (408.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:02:51.287844: step 4160, loss = 1.17 (404.1 examples/sec; 0.317 sec/batch)\n2016-03-12 14:02:54.654917: step 4170, loss = 1.33 (404.0 examples/sec; 0.317 sec/batch)\n2016-03-12 14:02:57.782336: step 4180, loss = 1.18 (404.1 examples/sec; 0.317 sec/batch)\n2016-03-12 14:03:01.022658: step 4190, loss = 1.15 (409.9 examples/sec; 0.312 sec/batch)\n2016-03-12 14:03:04.232610: step 4200, loss = 1.24 (365.7 examples/sec; 0.350 sec/batch)\n2016-03-12 14:03:07.854789: step 4210, loss = 1.11 (400.8 examples/sec; 0.319 sec/batch)\n2016-03-12 14:03:11.121573: step 4220, loss = 1.31 (371.6 examples/sec; 0.344 sec/batch)\n2016-03-12 14:03:14.307746: step 4230, loss = 1.46 (382.4 examples/sec; 0.335 sec/batch)\n2016-03-12 14:03:17.566388: step 4240, loss = 1.14 (425.0 examples/sec; 0.301 sec/batch)\n2016-03-12 14:03:20.750270: step 4250, loss = 1.21 (412.2 examples/sec; 0.311 sec/batch)\n2016-03-12 14:03:24.010454: step 4260, loss = 1.16 (401.9 examples/sec; 0.319 sec/batch)\n2016-03-12 14:03:27.315912: step 4270, loss = 1.16 (351.9 examples/sec; 0.364 sec/batch)\n2016-03-12 14:03:30.528823: step 4280, loss = 1.24 (397.8 examples/sec; 0.322 sec/batch)\n2016-03-12 14:03:33.777292: step 4290, loss = 1.45 (393.2 examples/sec; 0.326 sec/batch)\n2016-03-12 14:03:37.022320: step 4300, loss = 1.19 (396.9 examples/sec; 0.323 sec/batch)\n2016-03-12 14:03:40.659591: step 4310, loss = 1.36 (415.7 examples/sec; 0.308 sec/batch)\n2016-03-12 14:03:43.828922: step 4320, loss = 1.18 (396.4 examples/sec; 0.323 sec/batch)\n2016-03-12 14:03:47.227371: step 4330, loss = 0.96 (377.0 examples/sec; 0.340 sec/batch)\n2016-03-12 14:03:50.400040: step 4340, loss = 1.13 (415.3 examples/sec; 0.308 sec/batch)\n2016-03-12 14:03:53.671297: step 4350, loss = 1.12 (405.1 examples/sec; 0.316 sec/batch)\n2016-03-12 14:03:56.815646: step 4360, loss = 1.09 (414.5 examples/sec; 0.309 sec/batch)\n2016-03-12 14:04:00.007053: step 4370, loss = 1.22 (382.2 examples/sec; 0.335 sec/batch)\n2016-03-12 14:04:03.242795: step 4380, loss = 1.19 (414.5 examples/sec; 0.309 sec/batch)\n2016-03-12 14:04:06.424938: step 4390, loss = 1.13 (392.6 examples/sec; 0.326 sec/batch)\n2016-03-12 14:04:09.578756: step 4400, loss = 1.02 (402.1 examples/sec; 0.318 sec/batch)\n2016-03-12 14:04:13.179099: step 4410, loss = 1.34 (409.3 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:16.348951: step 4420, loss = 1.21 (408.8 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:19.599094: step 4430, loss = 1.10 (420.9 examples/sec; 0.304 sec/batch)\n2016-03-12 14:04:22.736091: step 4440, loss = 1.11 (407.3 examples/sec; 0.314 sec/batch)\n2016-03-12 14:04:25.979362: step 4450, loss = 1.27 (389.7 examples/sec; 0.328 sec/batch)\n2016-03-12 14:04:29.173873: step 4460, loss = 1.18 (387.9 examples/sec; 0.330 sec/batch)\n2016-03-12 14:04:32.369984: step 4470, loss = 1.36 (422.4 examples/sec; 0.303 sec/batch)\n2016-03-12 14:04:35.565128: step 4480, loss = 1.19 (409.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:38.783202: step 4490, loss = 1.14 (391.8 examples/sec; 0.327 sec/batch)\n2016-03-12 14:04:41.968468: step 4500, loss = 1.20 (389.9 examples/sec; 0.328 sec/batch)\n2016-03-12 14:04:45.529786: step 4510, loss = 1.14 (397.3 examples/sec; 0.322 sec/batch)\n2016-03-12 14:04:48.693115: step 4520, loss = 1.12 (393.7 examples/sec; 0.325 sec/batch)\n2016-03-12 14:04:51.892441: step 4530, loss = 1.25 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:04:55.102551: step 4540, loss = 1.26 (411.5 examples/sec; 0.311 sec/batch)\n2016-03-12 14:04:58.258904: step 4550, loss = 1.27 (395.5 examples/sec; 0.324 sec/batch)\n2016-03-12 14:05:01.485362: step 4560, loss = 1.08 (404.6 examples/sec; 0.316 sec/batch)\n2016-03-12 14:05:04.711979: step 4570, loss = 1.25 (365.1 examples/sec; 0.351 sec/batch)\n2016-03-12 14:05:07.864737: step 4580, loss = 1.31 (390.1 examples/sec; 0.328 sec/batch)\n2016-03-12 14:05:11.075490: step 4590, loss = 1.21 (385.9 examples/sec; 0.332 sec/batch)\n2016-03-12 14:05:14.205806: step 4600, loss = 0.96 (419.8 examples/sec; 0.305 sec/batch)\n2016-03-12 14:05:17.764024: step 4610, loss = 1.16 (419.8 examples/sec; 0.305 sec/batch)\n2016-03-12 14:05:20.992224: step 4620, loss = 1.17 (407.9 examples/sec; 0.314 sec/batch)\n2016-03-12 14:05:24.182559: step 4630, loss = 1.20 (406.1 examples/sec; 0.315 sec/batch)\n2016-03-12 14:05:27.346289: step 4640, loss = 1.30 (421.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:05:30.537761: step 4650, loss = 1.05 (403.8 examples/sec; 0.317 sec/batch)\n2016-03-12 14:05:33.755113: step 4660, loss = 1.10 (394.2 examples/sec; 0.325 sec/batch)\n2016-03-12 14:05:36.904973: step 4670, loss = 1.20 (402.2 examples/sec; 0.318 sec/batch)\n2016-03-12 14:05:40.113832: step 4680, loss = 1.10 (407.4 examples/sec; 0.314 sec/batch)\n2016-03-12 14:05:43.286804: step 4690, loss = 1.05 (406.8 examples/sec; 0.315 sec/batch)\n2016-03-12 14:05:46.492911: step 4700, loss = 1.19 (427.0 examples/sec; 0.300 sec/batch)\n2016-03-12 14:05:50.100880: step 4710, loss = 1.09 (414.9 examples/sec; 0.309 sec/batch)\n2016-03-12 14:05:53.295059: step 4720, loss = 1.16 (403.4 examples/sec; 0.317 sec/batch)\n2016-03-12 14:05:56.461129: step 4730, loss = 1.03 (418.4 examples/sec; 0.306 sec/batch)\n2016-03-12 14:05:59.686046: step 4740, loss = 1.01 (392.7 examples/sec; 0.326 sec/batch)\n2016-03-12 14:06:02.874850: step 4750, loss = 1.11 (403.2 examples/sec; 0.317 sec/batch)\n2016-03-12 14:06:06.063848: step 4760, loss = 1.15 (421.9 examples/sec; 0.303 sec/batch)\n2016-03-12 14:06:09.311339: step 4770, loss = 1.00 (401.5 examples/sec; 0.319 sec/batch)\n2016-03-12 14:06:12.400291: step 4780, loss = 1.03 (412.0 examples/sec; 0.311 sec/batch)\n2016-03-12 14:06:15.594995: step 4790, loss = 1.08 (372.0 examples/sec; 0.344 sec/batch)\n2016-03-12 14:06:18.774854: step 4800, loss = 1.31 (378.7 examples/sec; 0.338 sec/batch)\n2016-03-12 14:06:22.455985: step 4810, loss = 1.17 (359.9 examples/sec; 0.356 sec/batch)\n2016-03-12 14:06:25.655368: step 4820, loss = 1.04 (420.3 examples/sec; 0.305 sec/batch)\n2016-03-12 14:06:28.822332: step 4830, loss = 1.26 (404.5 examples/sec; 0.316 sec/batch)\n2016-03-12 14:06:32.007472: step 4840, loss = 1.01 (411.2 examples/sec; 0.311 sec/batch)\n2016-03-12 14:06:35.283917: step 4850, loss = 1.41 (434.8 examples/sec; 0.294 sec/batch)\n2016-03-12 14:06:38.506687: step 4860, loss = 1.03 (401.3 examples/sec; 0.319 sec/batch)\n2016-03-12 14:06:41.663434: step 4870, loss = 1.12 (395.4 examples/sec; 0.324 sec/batch)\n2016-03-12 14:06:44.828674: step 4880, loss = 1.21 (405.5 examples/sec; 0.316 sec/batch)\n2016-03-12 14:06:48.007878: step 4890, loss = 1.09 (410.4 examples/sec; 0.312 sec/batch)\n2016-03-12 14:06:51.222651: step 4900, loss = 1.00 (396.6 examples/sec; 0.323 sec/batch)\n2016-03-12 14:06:54.860536: step 4910, loss = 1.20 (383.4 examples/sec; 0.334 sec/batch)\n2016-03-12 14:06:58.063615: step 4920, loss = 1.17 (410.9 examples/sec; 0.312 sec/batch)\n2016-03-12 14:07:01.184445: step 4930, loss = 1.05 (397.0 examples/sec; 0.322 sec/batch)\n2016-03-12 14:07:04.394785: step 4940, loss = 1.06 (371.3 examples/sec; 0.345 sec/batch)\n2016-03-12 14:07:07.583337: step 4950, loss = 1.17 (394.5 examples/sec; 0.324 sec/batch)\n2016-03-12 14:07:10.724218: step 4960, loss = 1.09 (397.4 examples/sec; 0.322 sec/batch)\n2016-03-12 14:07:13.931494: step 4970, loss = 1.04 (389.5 examples/sec; 0.329 sec/batch)\n2016-03-12 14:07:17.116672: step 4980, loss = 1.30 (405.9 examples/sec; 0.315 sec/batch)\n2016-03-12 14:07:20.301438: step 4990, loss = 1.26 (427.9 examples/sec; 0.299 sec/batch)\n2016-03-12 14:07:23.464006: step 5000, loss = 1.18 (380.4 examples/sec; 0.337 sec/batch)\n2016-03-12 14:07:27.304553: step 5010, loss = 1.21 (378.2 examples/sec; 0.338 sec/batch)\n2016-03-12 14:07:30.435311: step 5020, loss = 0.96 (420.0 examples/sec; 0.305 sec/batch)\n2016-03-12 14:07:33.525055: step 5030, loss = 1.50 (426.5 examples/sec; 0.300 sec/batch)\n2016-03-12 14:07:36.664561: step 5040, loss = 1.09 (400.9 examples/sec; 0.319 sec/batch)\n2016-03-12 14:07:39.873659: step 5050, loss = 1.09 (422.8 examples/sec; 0.303 sec/batch)\n2016-03-12 14:07:43.060801: step 5060, loss = 1.00 (409.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:07:46.216866: step 5070, loss = 1.22 (424.8 examples/sec; 0.301 sec/batch)\n2016-03-12 14:07:49.424010: step 5080, loss = 1.20 (396.9 examples/sec; 0.323 sec/batch)\n2016-03-12 14:07:52.615564: step 5090, loss = 1.03 (393.3 examples/sec; 0.325 sec/batch)\n2016-03-12 14:07:55.773694: step 5100, loss = 1.43 (421.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:07:59.381470: step 5110, loss = 1.00 (420.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:08:02.574327: step 5120, loss = 1.08 (412.5 examples/sec; 0.310 sec/batch)\n2016-03-12 14:08:05.758566: step 5130, loss = 0.94 (413.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:08:08.940488: step 5140, loss = 1.07 (414.1 examples/sec; 0.309 sec/batch)\n2016-03-12 14:08:12.128661: step 5150, loss = 1.15 (374.6 examples/sec; 0.342 sec/batch)\n2016-03-12 14:08:15.321906: step 5160, loss = 1.05 (388.2 examples/sec; 0.330 sec/batch)\n2016-03-12 14:08:18.465653: step 5170, loss = 1.05 (394.7 examples/sec; 0.324 sec/batch)\n2016-03-12 14:08:21.695237: step 5180, loss = 1.17 (405.3 examples/sec; 0.316 sec/batch)\n2016-03-12 14:08:24.817479: step 5190, loss = 1.37 (404.4 examples/sec; 0.317 sec/batch)\n2016-03-12 14:08:28.045635: step 5200, loss = 1.06 (411.1 examples/sec; 0.311 sec/batch)\n2016-03-12 14:08:31.625961: step 5210, loss = 0.98 (393.0 examples/sec; 0.326 sec/batch)\n2016-03-12 14:08:34.803401: step 5220, loss = 1.16 (404.8 examples/sec; 0.316 sec/batch)\n2016-03-12 14:08:38.064956: step 5230, loss = 1.22 (386.5 examples/sec; 0.331 sec/batch)\n2016-03-12 14:08:42.036235: step 5240, loss = 0.93 (309.9 examples/sec; 0.413 sec/batch)\n2016-03-12 14:08:45.755935: step 5250, loss = 1.09 (332.2 examples/sec; 0.385 sec/batch)\n2016-03-12 14:08:48.961625: step 5260, loss = 1.24 (413.0 examples/sec; 0.310 sec/batch)\n2016-03-12 14:08:52.615963: step 5270, loss = 1.15 (318.3 examples/sec; 0.402 sec/batch)\n2016-03-12 14:08:56.692628: step 5280, loss = 1.28 (331.9 examples/sec; 0.386 sec/batch)\n2016-03-12 14:09:00.013660: step 5290, loss = 1.18 (429.9 examples/sec; 0.298 sec/batch)\n2016-03-12 14:09:03.394530: step 5300, loss = 0.97 (307.2 examples/sec; 0.417 sec/batch)\n2016-03-12 14:09:07.474397: step 5310, loss = 1.28 (250.3 examples/sec; 0.511 sec/batch)\n2016-03-12 14:09:11.289658: step 5320, loss = 1.03 (307.2 examples/sec; 0.417 sec/batch)\n2016-03-12 14:09:14.478318: step 5330, loss = 0.94 (399.5 examples/sec; 0.320 sec/batch)\n2016-03-12 14:09:17.677547: step 5340, loss = 1.04 (382.3 examples/sec; 0.335 sec/batch)\n2016-03-12 14:09:20.832572: step 5350, loss = 1.08 (433.3 examples/sec; 0.295 sec/batch)\n2016-03-12 14:09:24.024293: step 5360, loss = 1.07 (398.9 examples/sec; 0.321 sec/batch)\n2016-03-12 14:09:27.197269: step 5370, loss = 1.06 (416.3 examples/sec; 0.307 sec/batch)\n2016-03-12 14:09:30.623629: step 5380, loss = 1.04 (324.2 examples/sec; 0.395 sec/batch)\n2016-03-12 14:09:34.399873: step 5390, loss = 1.22 (241.2 examples/sec; 0.531 sec/batch)\n2016-03-12 14:09:38.530928: step 5400, loss = 1.13 (270.8 examples/sec; 0.473 sec/batch)\n2016-03-12 14:09:42.934882: step 5410, loss = 0.90 (268.9 examples/sec; 0.476 sec/batch)\n2016-03-12 14:09:47.006334: step 5420, loss = 0.95 (347.2 examples/sec; 0.369 sec/batch)\n2016-03-12 14:09:50.212403: step 5430, loss = 1.19 (441.3 examples/sec; 0.290 sec/batch)\n2016-03-12 14:09:53.401629: step 5440, loss = 1.11 (417.6 examples/sec; 0.307 sec/batch)\n2016-03-12 14:09:56.592893: step 5450, loss = 1.09 (413.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:09:59.731023: step 5460, loss = 1.14 (392.4 examples/sec; 0.326 sec/batch)\n2016-03-12 14:10:02.961094: step 5470, loss = 1.07 (413.1 examples/sec; 0.310 sec/batch)\n2016-03-12 14:10:06.190783: step 5480, loss = 1.17 (380.0 examples/sec; 0.337 sec/batch)\n2016-03-12 14:10:09.358733: step 5490, loss = 1.06 (390.4 examples/sec; 0.328 sec/batch)\n2016-03-12 14:10:12.525256: step 5500, loss = 1.03 (423.3 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:16.103487: step 5510, loss = 0.98 (418.0 examples/sec; 0.306 sec/batch)\n2016-03-12 14:10:19.302679: step 5520, loss = 1.08 (395.4 examples/sec; 0.324 sec/batch)\n2016-03-12 14:10:22.487560: step 5530, loss = 1.16 (423.4 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:25.632955: step 5540, loss = 1.20 (416.8 examples/sec; 0.307 sec/batch)\n2016-03-12 14:10:28.837518: step 5550, loss = 1.03 (409.2 examples/sec; 0.313 sec/batch)\n2016-03-12 14:10:32.006405: step 5560, loss = 1.06 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:10:35.165191: step 5570, loss = 0.95 (442.7 examples/sec; 0.289 sec/batch)\n2016-03-12 14:10:38.384486: step 5580, loss = 1.17 (439.7 examples/sec; 0.291 sec/batch)\n2016-03-12 14:10:41.575061: step 5590, loss = 1.10 (368.8 examples/sec; 0.347 sec/batch)\n2016-03-12 14:10:44.729184: step 5600, loss = 0.89 (423.4 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:48.343008: step 5610, loss = 1.09 (416.0 examples/sec; 0.308 sec/batch)\n2016-03-12 14:10:51.464913: step 5620, loss = 1.18 (388.3 examples/sec; 0.330 sec/batch)\n2016-03-12 14:10:54.620341: step 5630, loss = 1.05 (421.4 examples/sec; 0.304 sec/batch)\n2016-03-12 14:10:57.837872: step 5640, loss = 1.17 (419.3 examples/sec; 0.305 sec/batch)\n2016-03-12 14:11:01.090029: step 5650, loss = 1.13 (411.3 examples/sec; 0.311 sec/batch)\n2016-03-12 14:11:04.395310: step 5660, loss = 1.05 (399.1 examples/sec; 0.321 sec/batch)\n2016-03-12 14:11:07.784719: step 5670, loss = 1.05 (366.3 examples/sec; 0.349 sec/batch)\n2016-03-12 14:11:11.604344: step 5680, loss = 0.99 (380.1 examples/sec; 0.337 sec/batch)\n2016-03-12 14:11:15.243982: step 5690, loss = 1.10 (332.7 examples/sec; 0.385 sec/batch)\n2016-03-12 14:11:19.011393: step 5700, loss = 1.14 (388.5 examples/sec; 0.330 sec/batch)\n2016-03-12 14:11:22.897067: step 5710, loss = 1.10 (389.7 examples/sec; 0.328 sec/batch)\n2016-03-12 14:11:26.257571: step 5720, loss = 1.10 (392.0 examples/sec; 0.327 sec/batch)\n2016-03-12 14:11:29.648071: step 5730, loss = 1.05 (378.9 examples/sec; 0.338 sec/batch)\n2016-03-12 14:11:33.027954: step 5740, loss = 1.25 (349.1 examples/sec; 0.367 sec/batch)\n2016-03-12 14:11:36.389940: step 5750, loss = 0.88 (388.9 examples/sec; 0.329 sec/batch)\n2016-03-12 14:11:39.740017: step 5760, loss = 1.15 (386.6 examples/sec; 0.331 sec/batch)\n2016-03-12 14:11:43.111565: step 5770, loss = 0.99 (384.3 examples/sec; 0.333 sec/batch)\n2016-03-12 14:11:46.472003: step 5780, loss = 1.01 (387.7 examples/sec; 0.330 sec/batch)\n2016-03-12 14:11:49.950465: step 5790, loss = 0.89 (363.1 examples/sec; 0.352 sec/batch)\n2016-03-12 14:11:53.508106: step 5800, loss = 1.23 (348.4 examples/sec; 0.367 sec/batch)\n2016-03-12 14:11:57.342506: step 5810, loss = 1.13 (407.0 examples/sec; 0.314 sec/batch)\n2016-03-12 14:12:01.130117: step 5820, loss = 1.17 (409.0 examples/sec; 0.313 sec/batch)\n2016-03-12 14:12:04.352433: step 5830, loss = 1.05 (399.8 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:07.511581: step 5840, loss = 1.05 (435.9 examples/sec; 0.294 sec/batch)\n2016-03-12 14:12:10.730863: step 5850, loss = 1.02 (412.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:12:13.959593: step 5860, loss = 1.10 (355.5 examples/sec; 0.360 sec/batch)\n2016-03-12 14:12:17.241367: step 5870, loss = 1.10 (441.1 examples/sec; 0.290 sec/batch)\n2016-03-12 14:12:20.945846: step 5880, loss = 0.96 (339.6 examples/sec; 0.377 sec/batch)\n2016-03-12 14:12:24.060260: step 5890, loss = 1.14 (398.4 examples/sec; 0.321 sec/batch)\n2016-03-12 14:12:27.196011: step 5900, loss = 1.02 (425.1 examples/sec; 0.301 sec/batch)\n2016-03-12 14:12:30.779291: step 5910, loss = 1.06 (420.0 examples/sec; 0.305 sec/batch)\n2016-03-12 14:12:33.974906: step 5920, loss = 1.10 (403.1 examples/sec; 0.318 sec/batch)\n2016-03-12 14:12:37.188986: step 5930, loss = 1.03 (381.9 examples/sec; 0.335 sec/batch)\n2016-03-12 14:12:40.549341: step 5940, loss = 0.97 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:43.776408: step 5950, loss = 1.10 (386.8 examples/sec; 0.331 sec/batch)\n2016-03-12 14:12:46.910332: step 5960, loss = 1.15 (383.1 examples/sec; 0.334 sec/batch)\n2016-03-12 14:12:50.070772: step 5970, loss = 0.97 (436.3 examples/sec; 0.293 sec/batch)\n2016-03-12 14:12:53.283043: step 5980, loss = 0.91 (400.2 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:57.333740: step 5990, loss = 1.07 (389.0 examples/sec; 0.329 sec/batch)\n2016-03-12 14:13:00.647640: step 6000, loss = 0.97 (371.8 examples/sec; 0.344 sec/batch)\n2016-03-12 14:13:04.837750: step 6010, loss = 1.17 (304.9 examples/sec; 0.420 sec/batch)\n2016-03-12 14:13:08.816024: step 6020, loss = 1.07 (336.6 examples/sec; 0.380 sec/batch)\n2016-03-12 14:13:12.670182: step 6030, loss = 1.00 (260.2 examples/sec; 0.492 sec/batch)\n2016-03-12 14:13:16.635798: step 6040, loss = 0.98 (295.0 examples/sec; 0.434 sec/batch)\n2016-03-12 14:13:20.590717: step 6050, loss = 1.05 (436.9 examples/sec; 0.293 sec/batch)\n2016-03-12 14:13:24.389427: step 6060, loss = 1.12 (303.6 examples/sec; 0.422 sec/batch)\n2016-03-12 14:13:27.919392: step 6070, loss = 1.12 (403.5 examples/sec; 0.317 sec/batch)\n2016-03-12 14:13:31.241030: step 6080, loss = 1.19 (374.7 examples/sec; 0.342 sec/batch)\n2016-03-12 14:13:36.332882: step 6090, loss = 1.19 (342.0 examples/sec; 0.374 sec/batch)\n2016-03-12 14:13:39.823059: step 6100, loss = 0.99 (363.2 examples/sec; 0.352 sec/batch)\n2016-03-12 14:13:44.250455: step 6110, loss = 1.10 (352.2 examples/sec; 0.363 sec/batch)\n2016-03-12 14:13:47.462684: step 6120, loss = 0.93 (429.3 examples/sec; 0.298 sec/batch)\n2016-03-12 14:13:50.718381: step 6130, loss = 1.24 (405.0 examples/sec; 0.316 sec/batch)\n2016-03-12 14:13:53.870993: step 6140, loss = 0.91 (382.4 examples/sec; 0.335 sec/batch)\n2016-03-12 14:13:56.999939: step 6150, loss = 1.09 (415.8 examples/sec; 0.308 sec/batch)\n2016-03-12 14:14:00.147406: step 6160, loss = 1.03 (408.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:14:03.290188: step 6170, loss = 1.04 (417.4 examples/sec; 0.307 sec/batch)\n2016-03-12 14:14:06.453738: step 6180, loss = 0.88 (422.1 examples/sec; 0.303 sec/batch)\n2016-03-12 14:14:09.616241: step 6190, loss = 0.98 (393.4 examples/sec; 0.325 sec/batch)\n2016-03-12 14:14:12.722891: step 6200, loss = 1.06 (413.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:14:16.362099: step 6210, loss = 1.01 (433.5 examples/sec; 0.295 sec/batch)\n2016-03-12 14:14:19.472411: step 6220, loss = 1.02 (436.2 examples/sec; 0.293 sec/batch)\n2016-03-12 14:14:22.663895: step 6230, loss = 1.07 (416.9 examples/sec; 0.307 sec/batch)\n2016-03-12 14:14:26.117081: step 6240, loss = 1.17 (386.0 examples/sec; 0.332 sec/batch)\n2016-03-12 14:14:29.231502: step 6250, loss = 0.90 (411.4 examples/sec; 0.311 sec/batch)\n2016-03-12 14:14:32.351659: step 6260, loss = 0.97 (409.6 examples/sec; 0.312 sec/batch)\n2016-03-12 14:14:35.495727: step 6270, loss = 0.98 (401.1 examples/sec; 0.319 sec/batch)\n2016-03-12 14:14:38.677070: step 6280, loss = 1.17 (390.6 examples/sec; 0.328 sec/batch)\n2016-03-12 14:14:41.822706: step 6290, loss = 0.97 (436.9 examples/sec; 0.293 sec/batch)\n2016-03-12 14:14:44.955144: step 6300, loss = 0.91 (442.9 examples/sec; 0.289 sec/batch)\n2016-03-12 14:14:48.526671: step 6310, loss = 0.89 (356.7 examples/sec; 0.359 sec/batch)\n2016-03-12 14:14:51.623469: step 6320, loss = 1.15 (395.3 examples/sec; 0.324 sec/batch)\n2016-03-12 14:14:54.781165: step 6330, loss = 1.25 (387.0 examples/sec; 0.331 sec/batch)\n2016-03-12 14:14:57.893973: step 6340, loss = 1.13 (426.3 examples/sec; 0.300 sec/batch)\n2016-03-12 14:15:01.030509: step 6350, loss = 0.92 (411.7 examples/sec; 0.311 sec/batch)\n2016-03-12 14:15:04.212174: step 6360, loss = 0.96 (414.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:15:07.610402: step 6370, loss = 1.07 (397.9 examples/sec; 0.322 sec/batch)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1088] could not wait stream on event: CUDA_ERROR_MISALIGNED_ADDRESS\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7012acd00\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_MISALIGNED_ADDRESS; host dst: 0x7fb6276cff80; GPU src: 0x7011c2300; size: 1=0x1\nI tensorflow/stream_executor/stream.cc:826] stream 0x2f7bdd0 did not wait for stream: 0x2f7a8b0\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:193] Unexpected Event status: 1\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7011e2d00\nAborted (core dumped)", "body": "### Environment info\n\nOperating System: Ubuntu 14.04.4 LTS, NVIDIA GeForce 840M\n\nInstalled from sources,  commit hash:\n30b52579f6d66071ac7cdc7179e2c4aae3c9cb88\n### Steps to reproduce\n1. run python  cifar10_train.py\n2. program errorr out after a while\n   3.\n### What have you tried?\n1. restart machine and rerun several times. The same issue.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n2016-03-12 14:02:02.607728: step 4010, loss = 1.22 (421.6 examples/sec; 0.304 sec/batch)\n2016-03-12 14:02:05.731932: step 4020, loss = 1.18 (425.2 examples/sec; 0.301 sec/batch)\n2016-03-12 14:02:08.874431: step 4030, loss = 1.21 (440.1 examples/sec; 0.291 sec/batch)\n2016-03-12 14:02:12.053320: step 4040, loss = 1.30 (367.8 examples/sec; 0.348 sec/batch)\n2016-03-12 14:02:15.304938: step 4050, loss = 1.17 (391.1 examples/sec; 0.327 sec/batch)\n2016-03-12 14:02:18.456706: step 4060, loss = 1.23 (416.5 examples/sec; 0.307 sec/batch)\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8333348 get requests, put_count=8333340 evicted_count=1000 eviction_rate=0.00012 and unsatisfied allocation rate=0.00013296\n2016-03-12 14:02:21.675055: step 4070, loss = 1.19 (424.8 examples/sec; 0.301 sec/batch)\n2016-03-12 14:02:24.810989: step 4080, loss = 1.26 (412.4 examples/sec; 0.310 sec/batch)\n2016-03-12 14:02:28.031250: step 4090, loss = 1.21 (424.3 examples/sec; 0.302 sec/batch)\n2016-03-12 14:02:31.328527: step 4100, loss = 1.13 (396.7 examples/sec; 0.323 sec/batch)\n2016-03-12 14:02:35.028202: step 4110, loss = 1.23 (414.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:02:38.254790: step 4120, loss = 1.35 (364.6 examples/sec; 0.351 sec/batch)\n2016-03-12 14:02:41.484692: step 4130, loss = 1.47 (415.9 examples/sec; 0.308 sec/batch)\n2016-03-12 14:02:44.718649: step 4140, loss = 1.26 (406.7 examples/sec; 0.315 sec/batch)\n2016-03-12 14:02:47.988837: step 4150, loss = 1.07 (408.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:02:51.287844: step 4160, loss = 1.17 (404.1 examples/sec; 0.317 sec/batch)\n2016-03-12 14:02:54.654917: step 4170, loss = 1.33 (404.0 examples/sec; 0.317 sec/batch)\n2016-03-12 14:02:57.782336: step 4180, loss = 1.18 (404.1 examples/sec; 0.317 sec/batch)\n2016-03-12 14:03:01.022658: step 4190, loss = 1.15 (409.9 examples/sec; 0.312 sec/batch)\n2016-03-12 14:03:04.232610: step 4200, loss = 1.24 (365.7 examples/sec; 0.350 sec/batch)\n2016-03-12 14:03:07.854789: step 4210, loss = 1.11 (400.8 examples/sec; 0.319 sec/batch)\n2016-03-12 14:03:11.121573: step 4220, loss = 1.31 (371.6 examples/sec; 0.344 sec/batch)\n2016-03-12 14:03:14.307746: step 4230, loss = 1.46 (382.4 examples/sec; 0.335 sec/batch)\n2016-03-12 14:03:17.566388: step 4240, loss = 1.14 (425.0 examples/sec; 0.301 sec/batch)\n2016-03-12 14:03:20.750270: step 4250, loss = 1.21 (412.2 examples/sec; 0.311 sec/batch)\n2016-03-12 14:03:24.010454: step 4260, loss = 1.16 (401.9 examples/sec; 0.319 sec/batch)\n2016-03-12 14:03:27.315912: step 4270, loss = 1.16 (351.9 examples/sec; 0.364 sec/batch)\n2016-03-12 14:03:30.528823: step 4280, loss = 1.24 (397.8 examples/sec; 0.322 sec/batch)\n2016-03-12 14:03:33.777292: step 4290, loss = 1.45 (393.2 examples/sec; 0.326 sec/batch)\n2016-03-12 14:03:37.022320: step 4300, loss = 1.19 (396.9 examples/sec; 0.323 sec/batch)\n2016-03-12 14:03:40.659591: step 4310, loss = 1.36 (415.7 examples/sec; 0.308 sec/batch)\n2016-03-12 14:03:43.828922: step 4320, loss = 1.18 (396.4 examples/sec; 0.323 sec/batch)\n2016-03-12 14:03:47.227371: step 4330, loss = 0.96 (377.0 examples/sec; 0.340 sec/batch)\n2016-03-12 14:03:50.400040: step 4340, loss = 1.13 (415.3 examples/sec; 0.308 sec/batch)\n2016-03-12 14:03:53.671297: step 4350, loss = 1.12 (405.1 examples/sec; 0.316 sec/batch)\n2016-03-12 14:03:56.815646: step 4360, loss = 1.09 (414.5 examples/sec; 0.309 sec/batch)\n2016-03-12 14:04:00.007053: step 4370, loss = 1.22 (382.2 examples/sec; 0.335 sec/batch)\n2016-03-12 14:04:03.242795: step 4380, loss = 1.19 (414.5 examples/sec; 0.309 sec/batch)\n2016-03-12 14:04:06.424938: step 4390, loss = 1.13 (392.6 examples/sec; 0.326 sec/batch)\n2016-03-12 14:04:09.578756: step 4400, loss = 1.02 (402.1 examples/sec; 0.318 sec/batch)\n2016-03-12 14:04:13.179099: step 4410, loss = 1.34 (409.3 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:16.348951: step 4420, loss = 1.21 (408.8 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:19.599094: step 4430, loss = 1.10 (420.9 examples/sec; 0.304 sec/batch)\n2016-03-12 14:04:22.736091: step 4440, loss = 1.11 (407.3 examples/sec; 0.314 sec/batch)\n2016-03-12 14:04:25.979362: step 4450, loss = 1.27 (389.7 examples/sec; 0.328 sec/batch)\n2016-03-12 14:04:29.173873: step 4460, loss = 1.18 (387.9 examples/sec; 0.330 sec/batch)\n2016-03-12 14:04:32.369984: step 4470, loss = 1.36 (422.4 examples/sec; 0.303 sec/batch)\n2016-03-12 14:04:35.565128: step 4480, loss = 1.19 (409.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:38.783202: step 4490, loss = 1.14 (391.8 examples/sec; 0.327 sec/batch)\n2016-03-12 14:04:41.968468: step 4500, loss = 1.20 (389.9 examples/sec; 0.328 sec/batch)\n2016-03-12 14:04:45.529786: step 4510, loss = 1.14 (397.3 examples/sec; 0.322 sec/batch)\n2016-03-12 14:04:48.693115: step 4520, loss = 1.12 (393.7 examples/sec; 0.325 sec/batch)\n2016-03-12 14:04:51.892441: step 4530, loss = 1.25 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:04:55.102551: step 4540, loss = 1.26 (411.5 examples/sec; 0.311 sec/batch)\n2016-03-12 14:04:58.258904: step 4550, loss = 1.27 (395.5 examples/sec; 0.324 sec/batch)\n2016-03-12 14:05:01.485362: step 4560, loss = 1.08 (404.6 examples/sec; 0.316 sec/batch)\n2016-03-12 14:05:04.711979: step 4570, loss = 1.25 (365.1 examples/sec; 0.351 sec/batch)\n2016-03-12 14:05:07.864737: step 4580, loss = 1.31 (390.1 examples/sec; 0.328 sec/batch)\n2016-03-12 14:05:11.075490: step 4590, loss = 1.21 (385.9 examples/sec; 0.332 sec/batch)\n2016-03-12 14:05:14.205806: step 4600, loss = 0.96 (419.8 examples/sec; 0.305 sec/batch)\n2016-03-12 14:05:17.764024: step 4610, loss = 1.16 (419.8 examples/sec; 0.305 sec/batch)\n2016-03-12 14:05:20.992224: step 4620, loss = 1.17 (407.9 examples/sec; 0.314 sec/batch)\n2016-03-12 14:05:24.182559: step 4630, loss = 1.20 (406.1 examples/sec; 0.315 sec/batch)\n2016-03-12 14:05:27.346289: step 4640, loss = 1.30 (421.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:05:30.537761: step 4650, loss = 1.05 (403.8 examples/sec; 0.317 sec/batch)\n2016-03-12 14:05:33.755113: step 4660, loss = 1.10 (394.2 examples/sec; 0.325 sec/batch)\n2016-03-12 14:05:36.904973: step 4670, loss = 1.20 (402.2 examples/sec; 0.318 sec/batch)\n2016-03-12 14:05:40.113832: step 4680, loss = 1.10 (407.4 examples/sec; 0.314 sec/batch)\n2016-03-12 14:05:43.286804: step 4690, loss = 1.05 (406.8 examples/sec; 0.315 sec/batch)\n2016-03-12 14:05:46.492911: step 4700, loss = 1.19 (427.0 examples/sec; 0.300 sec/batch)\n2016-03-12 14:05:50.100880: step 4710, loss = 1.09 (414.9 examples/sec; 0.309 sec/batch)\n2016-03-12 14:05:53.295059: step 4720, loss = 1.16 (403.4 examples/sec; 0.317 sec/batch)\n2016-03-12 14:05:56.461129: step 4730, loss = 1.03 (418.4 examples/sec; 0.306 sec/batch)\n2016-03-12 14:05:59.686046: step 4740, loss = 1.01 (392.7 examples/sec; 0.326 sec/batch)\n2016-03-12 14:06:02.874850: step 4750, loss = 1.11 (403.2 examples/sec; 0.317 sec/batch)\n2016-03-12 14:06:06.063848: step 4760, loss = 1.15 (421.9 examples/sec; 0.303 sec/batch)\n2016-03-12 14:06:09.311339: step 4770, loss = 1.00 (401.5 examples/sec; 0.319 sec/batch)\n2016-03-12 14:06:12.400291: step 4780, loss = 1.03 (412.0 examples/sec; 0.311 sec/batch)\n2016-03-12 14:06:15.594995: step 4790, loss = 1.08 (372.0 examples/sec; 0.344 sec/batch)\n2016-03-12 14:06:18.774854: step 4800, loss = 1.31 (378.7 examples/sec; 0.338 sec/batch)\n2016-03-12 14:06:22.455985: step 4810, loss = 1.17 (359.9 examples/sec; 0.356 sec/batch)\n2016-03-12 14:06:25.655368: step 4820, loss = 1.04 (420.3 examples/sec; 0.305 sec/batch)\n2016-03-12 14:06:28.822332: step 4830, loss = 1.26 (404.5 examples/sec; 0.316 sec/batch)\n2016-03-12 14:06:32.007472: step 4840, loss = 1.01 (411.2 examples/sec; 0.311 sec/batch)\n2016-03-12 14:06:35.283917: step 4850, loss = 1.41 (434.8 examples/sec; 0.294 sec/batch)\n2016-03-12 14:06:38.506687: step 4860, loss = 1.03 (401.3 examples/sec; 0.319 sec/batch)\n2016-03-12 14:06:41.663434: step 4870, loss = 1.12 (395.4 examples/sec; 0.324 sec/batch)\n2016-03-12 14:06:44.828674: step 4880, loss = 1.21 (405.5 examples/sec; 0.316 sec/batch)\n2016-03-12 14:06:48.007878: step 4890, loss = 1.09 (410.4 examples/sec; 0.312 sec/batch)\n2016-03-12 14:06:51.222651: step 4900, loss = 1.00 (396.6 examples/sec; 0.323 sec/batch)\n2016-03-12 14:06:54.860536: step 4910, loss = 1.20 (383.4 examples/sec; 0.334 sec/batch)\n2016-03-12 14:06:58.063615: step 4920, loss = 1.17 (410.9 examples/sec; 0.312 sec/batch)\n2016-03-12 14:07:01.184445: step 4930, loss = 1.05 (397.0 examples/sec; 0.322 sec/batch)\n2016-03-12 14:07:04.394785: step 4940, loss = 1.06 (371.3 examples/sec; 0.345 sec/batch)\n2016-03-12 14:07:07.583337: step 4950, loss = 1.17 (394.5 examples/sec; 0.324 sec/batch)\n2016-03-12 14:07:10.724218: step 4960, loss = 1.09 (397.4 examples/sec; 0.322 sec/batch)\n2016-03-12 14:07:13.931494: step 4970, loss = 1.04 (389.5 examples/sec; 0.329 sec/batch)\n2016-03-12 14:07:17.116672: step 4980, loss = 1.30 (405.9 examples/sec; 0.315 sec/batch)\n2016-03-12 14:07:20.301438: step 4990, loss = 1.26 (427.9 examples/sec; 0.299 sec/batch)\n2016-03-12 14:07:23.464006: step 5000, loss = 1.18 (380.4 examples/sec; 0.337 sec/batch)\n2016-03-12 14:07:27.304553: step 5010, loss = 1.21 (378.2 examples/sec; 0.338 sec/batch)\n2016-03-12 14:07:30.435311: step 5020, loss = 0.96 (420.0 examples/sec; 0.305 sec/batch)\n2016-03-12 14:07:33.525055: step 5030, loss = 1.50 (426.5 examples/sec; 0.300 sec/batch)\n2016-03-12 14:07:36.664561: step 5040, loss = 1.09 (400.9 examples/sec; 0.319 sec/batch)\n2016-03-12 14:07:39.873659: step 5050, loss = 1.09 (422.8 examples/sec; 0.303 sec/batch)\n2016-03-12 14:07:43.060801: step 5060, loss = 1.00 (409.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:07:46.216866: step 5070, loss = 1.22 (424.8 examples/sec; 0.301 sec/batch)\n2016-03-12 14:07:49.424010: step 5080, loss = 1.20 (396.9 examples/sec; 0.323 sec/batch)\n2016-03-12 14:07:52.615564: step 5090, loss = 1.03 (393.3 examples/sec; 0.325 sec/batch)\n2016-03-12 14:07:55.773694: step 5100, loss = 1.43 (421.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:07:59.381470: step 5110, loss = 1.00 (420.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:08:02.574327: step 5120, loss = 1.08 (412.5 examples/sec; 0.310 sec/batch)\n2016-03-12 14:08:05.758566: step 5130, loss = 0.94 (413.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:08:08.940488: step 5140, loss = 1.07 (414.1 examples/sec; 0.309 sec/batch)\n2016-03-12 14:08:12.128661: step 5150, loss = 1.15 (374.6 examples/sec; 0.342 sec/batch)\n2016-03-12 14:08:15.321906: step 5160, loss = 1.05 (388.2 examples/sec; 0.330 sec/batch)\n2016-03-12 14:08:18.465653: step 5170, loss = 1.05 (394.7 examples/sec; 0.324 sec/batch)\n2016-03-12 14:08:21.695237: step 5180, loss = 1.17 (405.3 examples/sec; 0.316 sec/batch)\n2016-03-12 14:08:24.817479: step 5190, loss = 1.37 (404.4 examples/sec; 0.317 sec/batch)\n2016-03-12 14:08:28.045635: step 5200, loss = 1.06 (411.1 examples/sec; 0.311 sec/batch)\n2016-03-12 14:08:31.625961: step 5210, loss = 0.98 (393.0 examples/sec; 0.326 sec/batch)\n2016-03-12 14:08:34.803401: step 5220, loss = 1.16 (404.8 examples/sec; 0.316 sec/batch)\n2016-03-12 14:08:38.064956: step 5230, loss = 1.22 (386.5 examples/sec; 0.331 sec/batch)\n2016-03-12 14:08:42.036235: step 5240, loss = 0.93 (309.9 examples/sec; 0.413 sec/batch)\n2016-03-12 14:08:45.755935: step 5250, loss = 1.09 (332.2 examples/sec; 0.385 sec/batch)\n2016-03-12 14:08:48.961625: step 5260, loss = 1.24 (413.0 examples/sec; 0.310 sec/batch)\n2016-03-12 14:08:52.615963: step 5270, loss = 1.15 (318.3 examples/sec; 0.402 sec/batch)\n2016-03-12 14:08:56.692628: step 5280, loss = 1.28 (331.9 examples/sec; 0.386 sec/batch)\n2016-03-12 14:09:00.013660: step 5290, loss = 1.18 (429.9 examples/sec; 0.298 sec/batch)\n2016-03-12 14:09:03.394530: step 5300, loss = 0.97 (307.2 examples/sec; 0.417 sec/batch)\n2016-03-12 14:09:07.474397: step 5310, loss = 1.28 (250.3 examples/sec; 0.511 sec/batch)\n2016-03-12 14:09:11.289658: step 5320, loss = 1.03 (307.2 examples/sec; 0.417 sec/batch)\n2016-03-12 14:09:14.478318: step 5330, loss = 0.94 (399.5 examples/sec; 0.320 sec/batch)\n2016-03-12 14:09:17.677547: step 5340, loss = 1.04 (382.3 examples/sec; 0.335 sec/batch)\n2016-03-12 14:09:20.832572: step 5350, loss = 1.08 (433.3 examples/sec; 0.295 sec/batch)\n2016-03-12 14:09:24.024293: step 5360, loss = 1.07 (398.9 examples/sec; 0.321 sec/batch)\n2016-03-12 14:09:27.197269: step 5370, loss = 1.06 (416.3 examples/sec; 0.307 sec/batch)\n2016-03-12 14:09:30.623629: step 5380, loss = 1.04 (324.2 examples/sec; 0.395 sec/batch)\n2016-03-12 14:09:34.399873: step 5390, loss = 1.22 (241.2 examples/sec; 0.531 sec/batch)\n2016-03-12 14:09:38.530928: step 5400, loss = 1.13 (270.8 examples/sec; 0.473 sec/batch)\n2016-03-12 14:09:42.934882: step 5410, loss = 0.90 (268.9 examples/sec; 0.476 sec/batch)\n2016-03-12 14:09:47.006334: step 5420, loss = 0.95 (347.2 examples/sec; 0.369 sec/batch)\n2016-03-12 14:09:50.212403: step 5430, loss = 1.19 (441.3 examples/sec; 0.290 sec/batch)\n2016-03-12 14:09:53.401629: step 5440, loss = 1.11 (417.6 examples/sec; 0.307 sec/batch)\n2016-03-12 14:09:56.592893: step 5450, loss = 1.09 (413.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:09:59.731023: step 5460, loss = 1.14 (392.4 examples/sec; 0.326 sec/batch)\n2016-03-12 14:10:02.961094: step 5470, loss = 1.07 (413.1 examples/sec; 0.310 sec/batch)\n2016-03-12 14:10:06.190783: step 5480, loss = 1.17 (380.0 examples/sec; 0.337 sec/batch)\n2016-03-12 14:10:09.358733: step 5490, loss = 1.06 (390.4 examples/sec; 0.328 sec/batch)\n2016-03-12 14:10:12.525256: step 5500, loss = 1.03 (423.3 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:16.103487: step 5510, loss = 0.98 (418.0 examples/sec; 0.306 sec/batch)\n2016-03-12 14:10:19.302679: step 5520, loss = 1.08 (395.4 examples/sec; 0.324 sec/batch)\n2016-03-12 14:10:22.487560: step 5530, loss = 1.16 (423.4 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:25.632955: step 5540, loss = 1.20 (416.8 examples/sec; 0.307 sec/batch)\n2016-03-12 14:10:28.837518: step 5550, loss = 1.03 (409.2 examples/sec; 0.313 sec/batch)\n2016-03-12 14:10:32.006405: step 5560, loss = 1.06 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:10:35.165191: step 5570, loss = 0.95 (442.7 examples/sec; 0.289 sec/batch)\n2016-03-12 14:10:38.384486: step 5580, loss = 1.17 (439.7 examples/sec; 0.291 sec/batch)\n2016-03-12 14:10:41.575061: step 5590, loss = 1.10 (368.8 examples/sec; 0.347 sec/batch)\n2016-03-12 14:10:44.729184: step 5600, loss = 0.89 (423.4 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:48.343008: step 5610, loss = 1.09 (416.0 examples/sec; 0.308 sec/batch)\n2016-03-12 14:10:51.464913: step 5620, loss = 1.18 (388.3 examples/sec; 0.330 sec/batch)\n2016-03-12 14:10:54.620341: step 5630, loss = 1.05 (421.4 examples/sec; 0.304 sec/batch)\n2016-03-12 14:10:57.837872: step 5640, loss = 1.17 (419.3 examples/sec; 0.305 sec/batch)\n2016-03-12 14:11:01.090029: step 5650, loss = 1.13 (411.3 examples/sec; 0.311 sec/batch)\n2016-03-12 14:11:04.395310: step 5660, loss = 1.05 (399.1 examples/sec; 0.321 sec/batch)\n2016-03-12 14:11:07.784719: step 5670, loss = 1.05 (366.3 examples/sec; 0.349 sec/batch)\n2016-03-12 14:11:11.604344: step 5680, loss = 0.99 (380.1 examples/sec; 0.337 sec/batch)\n2016-03-12 14:11:15.243982: step 5690, loss = 1.10 (332.7 examples/sec; 0.385 sec/batch)\n2016-03-12 14:11:19.011393: step 5700, loss = 1.14 (388.5 examples/sec; 0.330 sec/batch)\n2016-03-12 14:11:22.897067: step 5710, loss = 1.10 (389.7 examples/sec; 0.328 sec/batch)\n2016-03-12 14:11:26.257571: step 5720, loss = 1.10 (392.0 examples/sec; 0.327 sec/batch)\n2016-03-12 14:11:29.648071: step 5730, loss = 1.05 (378.9 examples/sec; 0.338 sec/batch)\n2016-03-12 14:11:33.027954: step 5740, loss = 1.25 (349.1 examples/sec; 0.367 sec/batch)\n2016-03-12 14:11:36.389940: step 5750, loss = 0.88 (388.9 examples/sec; 0.329 sec/batch)\n2016-03-12 14:11:39.740017: step 5760, loss = 1.15 (386.6 examples/sec; 0.331 sec/batch)\n2016-03-12 14:11:43.111565: step 5770, loss = 0.99 (384.3 examples/sec; 0.333 sec/batch)\n2016-03-12 14:11:46.472003: step 5780, loss = 1.01 (387.7 examples/sec; 0.330 sec/batch)\n2016-03-12 14:11:49.950465: step 5790, loss = 0.89 (363.1 examples/sec; 0.352 sec/batch)\n2016-03-12 14:11:53.508106: step 5800, loss = 1.23 (348.4 examples/sec; 0.367 sec/batch)\n2016-03-12 14:11:57.342506: step 5810, loss = 1.13 (407.0 examples/sec; 0.314 sec/batch)\n2016-03-12 14:12:01.130117: step 5820, loss = 1.17 (409.0 examples/sec; 0.313 sec/batch)\n2016-03-12 14:12:04.352433: step 5830, loss = 1.05 (399.8 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:07.511581: step 5840, loss = 1.05 (435.9 examples/sec; 0.294 sec/batch)\n2016-03-12 14:12:10.730863: step 5850, loss = 1.02 (412.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:12:13.959593: step 5860, loss = 1.10 (355.5 examples/sec; 0.360 sec/batch)\n2016-03-12 14:12:17.241367: step 5870, loss = 1.10 (441.1 examples/sec; 0.290 sec/batch)\n2016-03-12 14:12:20.945846: step 5880, loss = 0.96 (339.6 examples/sec; 0.377 sec/batch)\n2016-03-12 14:12:24.060260: step 5890, loss = 1.14 (398.4 examples/sec; 0.321 sec/batch)\n2016-03-12 14:12:27.196011: step 5900, loss = 1.02 (425.1 examples/sec; 0.301 sec/batch)\n2016-03-12 14:12:30.779291: step 5910, loss = 1.06 (420.0 examples/sec; 0.305 sec/batch)\n2016-03-12 14:12:33.974906: step 5920, loss = 1.10 (403.1 examples/sec; 0.318 sec/batch)\n2016-03-12 14:12:37.188986: step 5930, loss = 1.03 (381.9 examples/sec; 0.335 sec/batch)\n2016-03-12 14:12:40.549341: step 5940, loss = 0.97 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:43.776408: step 5950, loss = 1.10 (386.8 examples/sec; 0.331 sec/batch)\n2016-03-12 14:12:46.910332: step 5960, loss = 1.15 (383.1 examples/sec; 0.334 sec/batch)\n2016-03-12 14:12:50.070772: step 5970, loss = 0.97 (436.3 examples/sec; 0.293 sec/batch)\n2016-03-12 14:12:53.283043: step 5980, loss = 0.91 (400.2 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:57.333740: step 5990, loss = 1.07 (389.0 examples/sec; 0.329 sec/batch)\n2016-03-12 14:13:00.647640: step 6000, loss = 0.97 (371.8 examples/sec; 0.344 sec/batch)\n2016-03-12 14:13:04.837750: step 6010, loss = 1.17 (304.9 examples/sec; 0.420 sec/batch)\n2016-03-12 14:13:08.816024: step 6020, loss = 1.07 (336.6 examples/sec; 0.380 sec/batch)\n2016-03-12 14:13:12.670182: step 6030, loss = 1.00 (260.2 examples/sec; 0.492 sec/batch)\n2016-03-12 14:13:16.635798: step 6040, loss = 0.98 (295.0 examples/sec; 0.434 sec/batch)\n2016-03-12 14:13:20.590717: step 6050, loss = 1.05 (436.9 examples/sec; 0.293 sec/batch)\n2016-03-12 14:13:24.389427: step 6060, loss = 1.12 (303.6 examples/sec; 0.422 sec/batch)\n2016-03-12 14:13:27.919392: step 6070, loss = 1.12 (403.5 examples/sec; 0.317 sec/batch)\n2016-03-12 14:13:31.241030: step 6080, loss = 1.19 (374.7 examples/sec; 0.342 sec/batch)\n2016-03-12 14:13:36.332882: step 6090, loss = 1.19 (342.0 examples/sec; 0.374 sec/batch)\n2016-03-12 14:13:39.823059: step 6100, loss = 0.99 (363.2 examples/sec; 0.352 sec/batch)\n2016-03-12 14:13:44.250455: step 6110, loss = 1.10 (352.2 examples/sec; 0.363 sec/batch)\n2016-03-12 14:13:47.462684: step 6120, loss = 0.93 (429.3 examples/sec; 0.298 sec/batch)\n2016-03-12 14:13:50.718381: step 6130, loss = 1.24 (405.0 examples/sec; 0.316 sec/batch)\n2016-03-12 14:13:53.870993: step 6140, loss = 0.91 (382.4 examples/sec; 0.335 sec/batch)\n2016-03-12 14:13:56.999939: step 6150, loss = 1.09 (415.8 examples/sec; 0.308 sec/batch)\n2016-03-12 14:14:00.147406: step 6160, loss = 1.03 (408.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:14:03.290188: step 6170, loss = 1.04 (417.4 examples/sec; 0.307 sec/batch)\n2016-03-12 14:14:06.453738: step 6180, loss = 0.88 (422.1 examples/sec; 0.303 sec/batch)\n2016-03-12 14:14:09.616241: step 6190, loss = 0.98 (393.4 examples/sec; 0.325 sec/batch)\n2016-03-12 14:14:12.722891: step 6200, loss = 1.06 (413.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:14:16.362099: step 6210, loss = 1.01 (433.5 examples/sec; 0.295 sec/batch)\n2016-03-12 14:14:19.472411: step 6220, loss = 1.02 (436.2 examples/sec; 0.293 sec/batch)\n2016-03-12 14:14:22.663895: step 6230, loss = 1.07 (416.9 examples/sec; 0.307 sec/batch)\n2016-03-12 14:14:26.117081: step 6240, loss = 1.17 (386.0 examples/sec; 0.332 sec/batch)\n2016-03-12 14:14:29.231502: step 6250, loss = 0.90 (411.4 examples/sec; 0.311 sec/batch)\n2016-03-12 14:14:32.351659: step 6260, loss = 0.97 (409.6 examples/sec; 0.312 sec/batch)\n2016-03-12 14:14:35.495727: step 6270, loss = 0.98 (401.1 examples/sec; 0.319 sec/batch)\n2016-03-12 14:14:38.677070: step 6280, loss = 1.17 (390.6 examples/sec; 0.328 sec/batch)\n2016-03-12 14:14:41.822706: step 6290, loss = 0.97 (436.9 examples/sec; 0.293 sec/batch)\n2016-03-12 14:14:44.955144: step 6300, loss = 0.91 (442.9 examples/sec; 0.289 sec/batch)\n2016-03-12 14:14:48.526671: step 6310, loss = 0.89 (356.7 examples/sec; 0.359 sec/batch)\n2016-03-12 14:14:51.623469: step 6320, loss = 1.15 (395.3 examples/sec; 0.324 sec/batch)\n2016-03-12 14:14:54.781165: step 6330, loss = 1.25 (387.0 examples/sec; 0.331 sec/batch)\n2016-03-12 14:14:57.893973: step 6340, loss = 1.13 (426.3 examples/sec; 0.300 sec/batch)\n2016-03-12 14:15:01.030509: step 6350, loss = 0.92 (411.7 examples/sec; 0.311 sec/batch)\n2016-03-12 14:15:04.212174: step 6360, loss = 0.96 (414.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:15:07.610402: step 6370, loss = 1.07 (397.9 examples/sec; 0.322 sec/batch)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1088] could not wait stream on event: CUDA_ERROR_MISALIGNED_ADDRESS\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7012acd00\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_MISALIGNED_ADDRESS; host dst: 0x7fb6276cff80; GPU src: 0x7011c2300; size: 1=0x1\nI tensorflow/stream_executor/stream.cc:826] stream 0x2f7bdd0 did not wait for stream: 0x2f7a8b0\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:193] Unexpected Event status: 1\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7011e2d00\nAborted (core dumped)\n"}