{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/230111068", "html_url": "https://github.com/tensorflow/tensorflow/issues/3101#issuecomment-230111068", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3101", "id": 230111068, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMDExMTA2OA==", "user": {"login": "RobRomijnders", "id": 16174021, "node_id": "MDQ6VXNlcjE2MTc0MDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/16174021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RobRomijnders", "html_url": "https://github.com/RobRomijnders", "followers_url": "https://api.github.com/users/RobRomijnders/followers", "following_url": "https://api.github.com/users/RobRomijnders/following{/other_user}", "gists_url": "https://api.github.com/users/RobRomijnders/gists{/gist_id}", "starred_url": "https://api.github.com/users/RobRomijnders/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RobRomijnders/subscriptions", "organizations_url": "https://api.github.com/users/RobRomijnders/orgs", "repos_url": "https://api.github.com/users/RobRomijnders/repos", "events_url": "https://api.github.com/users/RobRomijnders/events{/privacy}", "received_events_url": "https://api.github.com/users/RobRomijnders/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-02T16:49:04Z", "updated_at": "2016-07-02T16:52:10Z", "author_association": "NONE", "body_html": "<p>Yes, it's a feature request.</p>\n<p>For exaple, squeeze(), reduce_sum() an any other reduce_() function, they all have arguments where you define the axes to be operated on.</p>\n<p>For reduce_sum(), you define which axes ought to be reduced and summed. Why not generalize the SoftMax like that too?</p>\n<p>At the moment, the softmax API is<br>\n<code>tf.nn.softmax(logits,name=None)</code></p>\n<p>My feature request is to to<br>\n<code>tf.nn.softmax(logits, softmax_dim = [1], name=None)</code></p>\n<p>Now softmax_dim is the list of axes to be softmax-ed</p>\n<p>for example, softmax_dim would be</p>\n<ul>\n<li>[1] for a batch of features</li>\n<li>[1,2] for a batch of images</li>\n<li>[1,2,3] for a batch of volumes, like a batch of CT-scan masks</li>\n<li>and so on</li>\n</ul>", "body_text": "Yes, it's a feature request.\nFor exaple, squeeze(), reduce_sum() an any other reduce_() function, they all have arguments where you define the axes to be operated on.\nFor reduce_sum(), you define which axes ought to be reduced and summed. Why not generalize the SoftMax like that too?\nAt the moment, the softmax API is\ntf.nn.softmax(logits,name=None)\nMy feature request is to to\ntf.nn.softmax(logits, softmax_dim = [1], name=None)\nNow softmax_dim is the list of axes to be softmax-ed\nfor example, softmax_dim would be\n\n[1] for a batch of features\n[1,2] for a batch of images\n[1,2,3] for a batch of volumes, like a batch of CT-scan masks\nand so on", "body": "Yes, it's a feature request.\n\nFor exaple, squeeze(), reduce_sum() an any other reduce_() function, they all have arguments where you define the axes to be operated on.\n\nFor reduce_sum(), you define which axes ought to be reduced and summed. Why not generalize the SoftMax like that too?\n\nAt the moment, the softmax API is\n   `tf.nn.softmax(logits,name=None)`\n\nMy feature request is to to\n    `tf.nn.softmax(logits, softmax_dim = [1], name=None)`\n\nNow softmax_dim is the list of axes to be softmax-ed\n\nfor example, softmax_dim would be\n- [1] for a batch of features\n- [1,2] for a batch of images\n- [1,2,3] for a batch of volumes, like a batch of CT-scan masks\n- and so on\n"}