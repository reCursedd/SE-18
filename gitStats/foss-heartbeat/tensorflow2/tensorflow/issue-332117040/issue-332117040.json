{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19996", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19996/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19996/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19996/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19996", "id": 332117040, "node_id": "MDU6SXNzdWUzMzIxMTcwNDA=", "number": 19996, "title": "Feature Request: untruncated normal in variance scaling initializer", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-13T18:44:33Z", "updated_at": "2018-06-28T01:04:44Z", "closed_at": "2018-06-28T01:04:44Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: N/A</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: N/A</li>\n<li><strong>TensorFlow version (use command below)</strong>: N/A</li>\n<li><strong>Python version</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>:N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:N/A</li>\n<li><strong>CUDA/cuDNN version</strong>:N/A</li>\n<li><strong>GPU model and memory</strong>:N/A</li>\n<li><strong>Exact command to reproduce</strong>:N/A</li>\n</ul>\n<p>Currently the \"normal\" mode of <code>variance_scaling_initializer</code> actually produces truncated normal distribution.<br>\nDespite that I saw Google suggest in several places that a truncated normal might be better than a true normal distribution, truncated normal is NOT what is used in the initialization literature such as:</p>\n<p>[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. 2010.<br>\n[2] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. 2015.</p>\n<p>I found that this difference is affecting some of my experiments. To be more consistent with literature, I would request a new mode in variance_scaling_initializer that produces untruncated normal distribution. I'm OK to implement it If someone can give it a good name.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\nTensorFlow installed from (source or binary): N/A\nTensorFlow version (use command below): N/A\nPython version: N/A\nBazel version (if compiling from source):N/A\nGCC/Compiler version (if compiling from source):N/A\nCUDA/cuDNN version:N/A\nGPU model and memory:N/A\nExact command to reproduce:N/A\n\nCurrently the \"normal\" mode of variance_scaling_initializer actually produces truncated normal distribution.\nDespite that I saw Google suggest in several places that a truncated normal might be better than a true normal distribution, truncated normal is NOT what is used in the initialization literature such as:\n[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. 2010.\n[2] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. 2015.\nI found that this difference is affecting some of my experiments. To be more consistent with literature, I would request a new mode in variance_scaling_initializer that produces untruncated normal distribution. I'm OK to implement it If someone can give it a good name.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\nCurrently the \"normal\" mode of `variance_scaling_initializer` actually produces truncated normal distribution. \r\nDespite that I saw Google suggest in several places that a truncated normal might be better than a true normal distribution, truncated normal is NOT what is used in the initialization literature such as:\r\n\r\n[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. 2010.\r\n[2] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. 2015.\r\n\r\nI found that this difference is affecting some of my experiments. To be more consistent with literature, I would request a new mode in variance_scaling_initializer that produces untruncated normal distribution. I'm OK to implement it If someone can give it a good name."}