{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16403", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16403/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16403/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16403/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16403", "id": 291559361, "node_id": "MDU6SXNzdWUyOTE1NTkzNjE=", "number": 16403, "title": "1D Convolution in Tensorflow Serving", "user": {"login": "deichbrise", "id": 11631450, "node_id": "MDQ6VXNlcjExNjMxNDUw", "avatar_url": "https://avatars3.githubusercontent.com/u/11631450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deichbrise", "html_url": "https://github.com/deichbrise", "followers_url": "https://api.github.com/users/deichbrise/followers", "following_url": "https://api.github.com/users/deichbrise/following{/other_user}", "gists_url": "https://api.github.com/users/deichbrise/gists{/gist_id}", "starred_url": "https://api.github.com/users/deichbrise/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deichbrise/subscriptions", "organizations_url": "https://api.github.com/users/deichbrise/orgs", "repos_url": "https://api.github.com/users/deichbrise/repos", "events_url": "https://api.github.com/users/deichbrise/events{/privacy}", "received_events_url": "https://api.github.com/users/deichbrise/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-25T12:50:42Z", "updated_at": "2018-01-26T01:44:48Z", "closed_at": "2018-01-26T01:32:27Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Arch Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: tensorflow binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0, 7.0</li>\n<li><strong>GPU model and memory</strong>: GTX 1050</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The Problem is a little bit hard to reproduce, I guess because so many steps are involved.<br>\nSo, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:</p>\n<p>`<br>\ninput = Input(shape=(200, 8))<br>\nx = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(input)<br>\nx = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(x)<br>\nx = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)<br>\nx = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)<br>\nx = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)<br>\nx = Conv1D(filters=2, kernel_size=1, activation=\"softmax\")(x)</p>\n<p>`</p>\n<p>Now, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:</p>\n<p>`<br>\nsession = K.get_session()</p>\n<pre><code>    signature = tf.saved_model.signature_def_utils.build_signature_def(\n        inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},\n        outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},\n        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n    )\n\n    b = tf.saved_model.builder.SavedModelBuilder(filename)\n    legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n    b.add_meta_graph_and_variables(session,\n                                   [tf.saved_model.tag_constants.SERVING],\n                                   signature_def_map={\n                                       tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},\n                                   legacy_init_op=legacy_init_op)\n    b.save()\n</code></pre>\n<p>`</p>\n<p>If I am loading the model via python, everything works as expected.</p>\n<p>Now I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:</p>\n<p>`<br>\nTensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()<br>\n.setSize(data.length).build();</p>\n<pre><code>    TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()\n            .setSize(data[0].length).build();\n\n    TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()\n            .setSize(data[0][0].length).build();\n\n    TensorShapeProto shape = TensorShapeProto.newBuilder()\n            .addDim(dim1).addDim(dim2).addDim(dim3).build();\n\n    TensorProto.Builder builder = TensorProto.newBuilder()\n            .setDtype(DataType.DT_FLOAT)\n            .setTensorShape(shape);\n\n    for(int i = 0; i &lt; data.length; i++) {\n        for(int j = 0; j &lt; data[0].length; j++) {\n            for(int k = 0; k &lt; data[0][0].length; k++) {\n                builder.addFloatVal(data[k][j][i]);\n            }\n        }\n    }\n\n    return builder.build();\n</code></pre>\n<p>`</p>\n<p>And do the predicition like this:</p>\n<p>`<br>\npublic class ModelClientImpl implements ModelClient {</p>\n<pre><code>private String host;\nprivate Integer port;\nprivate ManagedChannel channel;\nprivate PredictionServiceGrpc.PredictionServiceBlockingStub stub;\n\npublic void init() {\n    channel = ManagedChannelBuilder\n            .forAddress(getHost(), getPort())\n            .usePlaintext(true)\n            .build();\n\n    stub = PredictionServiceGrpc.newBlockingStub(channel);\n}\n\n@Override\npublic Map&lt;String, TensorProto&gt; predict(final String signatureName, Map&lt;String, TensorProto&gt; inputs) {\n    final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));\n\n    return response.getOutputsMap();\n}\n\nprotected Predict.PredictRequest createRequest(final String signatureName, final Map&lt;String, TensorProto&gt; inputs) {\n    final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()\n            .setName(signatureName)\n            .setSignatureName(\"serving_default\").build();\n\n    final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()\n            .setModelSpec(modelSpec)\n            .putAllInputs(inputs);\n\n    return builder.build();\n}\n\npublic String getHost() {\n    return host;\n}\n\npublic void setHost(String host) {\n    this.host = host;\n}\n\npublic Integer getPort() {\n    return port;\n}\n\npublic void setPort(Integer port) {\n    this.port = port;\n}\n\n@Override\npublic void close() throws Exception {\n    channel.shutdown().awaitTermination(5, TimeUnit.DAYS);\n}\n</code></pre>\n<p>}</p>\n<p>`</p>\n<p>But the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?</p>", "body_text": "System information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\nTensorFlow installed from (source or binary): tensorflow binary\nTensorFlow version (use command below): 1.4.0\nPython version: 3.6\nCUDA/cuDNN version: 9.0, 7.0\nGPU model and memory: GTX 1050\n\nDescribe the problem\nThe Problem is a little bit hard to reproduce, I guess because so many steps are involved.\nSo, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:\n`\ninput = Input(shape=(200, 8))\nx = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(input)\nx = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(x)\nx = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\nx = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\nx = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\nx = Conv1D(filters=2, kernel_size=1, activation=\"softmax\")(x)\n`\nNow, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:\n`\nsession = K.get_session()\n    signature = tf.saved_model.signature_def_utils.build_signature_def(\n        inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},\n        outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},\n        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n    )\n\n    b = tf.saved_model.builder.SavedModelBuilder(filename)\n    legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n    b.add_meta_graph_and_variables(session,\n                                   [tf.saved_model.tag_constants.SERVING],\n                                   signature_def_map={\n                                       tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},\n                                   legacy_init_op=legacy_init_op)\n    b.save()\n\n`\nIf I am loading the model via python, everything works as expected.\nNow I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:\n`\nTensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()\n.setSize(data.length).build();\n    TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()\n            .setSize(data[0].length).build();\n\n    TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()\n            .setSize(data[0][0].length).build();\n\n    TensorShapeProto shape = TensorShapeProto.newBuilder()\n            .addDim(dim1).addDim(dim2).addDim(dim3).build();\n\n    TensorProto.Builder builder = TensorProto.newBuilder()\n            .setDtype(DataType.DT_FLOAT)\n            .setTensorShape(shape);\n\n    for(int i = 0; i < data.length; i++) {\n        for(int j = 0; j < data[0].length; j++) {\n            for(int k = 0; k < data[0][0].length; k++) {\n                builder.addFloatVal(data[k][j][i]);\n            }\n        }\n    }\n\n    return builder.build();\n\n`\nAnd do the predicition like this:\n`\npublic class ModelClientImpl implements ModelClient {\nprivate String host;\nprivate Integer port;\nprivate ManagedChannel channel;\nprivate PredictionServiceGrpc.PredictionServiceBlockingStub stub;\n\npublic void init() {\n    channel = ManagedChannelBuilder\n            .forAddress(getHost(), getPort())\n            .usePlaintext(true)\n            .build();\n\n    stub = PredictionServiceGrpc.newBlockingStub(channel);\n}\n\n@Override\npublic Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {\n    final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));\n\n    return response.getOutputsMap();\n}\n\nprotected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {\n    final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()\n            .setName(signatureName)\n            .setSignatureName(\"serving_default\").build();\n\n    final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()\n            .setModelSpec(modelSpec)\n            .putAllInputs(inputs);\n\n    return builder.build();\n}\n\npublic String getHost() {\n    return host;\n}\n\npublic void setHost(String host) {\n    this.host = host;\n}\n\npublic Integer getPort() {\n    return port;\n}\n\npublic void setPort(Integer port) {\n    this.port = port;\n}\n\n@Override\npublic void close() throws Exception {\n    channel.shutdown().awaitTermination(5, TimeUnit.DAYS);\n}\n\n}\n`\nBut the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: tensorflow binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 9.0, 7.0\r\n- **GPU model and memory**: GTX 1050\r\n\r\n### Describe the problem\r\nThe Problem is a little bit hard to reproduce, I guess because so many steps are involved.\r\nSo, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:\r\n\r\n`\r\n           input = Input(shape=(200, 8))\r\n            x = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(input)\r\n            x = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=2, kernel_size=1, activation=\"softmax\")(x)\r\n\r\n`\r\n\r\nNow, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:\r\n\r\n`\r\nsession = K.get_session()\r\n\r\n        signature = tf.saved_model.signature_def_utils.build_signature_def(\r\n            inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},\r\n            outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},\r\n            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\r\n        )\r\n\r\n        b = tf.saved_model.builder.SavedModelBuilder(filename)\r\n        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\r\n        b.add_meta_graph_and_variables(session,\r\n                                       [tf.saved_model.tag_constants.SERVING],\r\n                                       signature_def_map={\r\n                                           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},\r\n                                       legacy_init_op=legacy_init_op)\r\n        b.save()\r\n`\r\n\r\nIf I am loading the model via python, everything works as expected.\r\n\r\nNow I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:\r\n\r\n`\r\nTensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()\r\n                .setSize(data.length).build();\r\n\r\n        TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()\r\n                .setSize(data[0].length).build();\r\n\r\n        TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()\r\n                .setSize(data[0][0].length).build();\r\n\r\n        TensorShapeProto shape = TensorShapeProto.newBuilder()\r\n                .addDim(dim1).addDim(dim2).addDim(dim3).build();\r\n\r\n        TensorProto.Builder builder = TensorProto.newBuilder()\r\n                .setDtype(DataType.DT_FLOAT)\r\n                .setTensorShape(shape);\r\n\r\n        for(int i = 0; i < data.length; i++) {\r\n            for(int j = 0; j < data[0].length; j++) {\r\n                for(int k = 0; k < data[0][0].length; k++) {\r\n                    builder.addFloatVal(data[k][j][i]);\r\n                }\r\n            }\r\n        }\r\n\r\n        return builder.build();\r\n`\r\n\r\nAnd do the predicition like this:\r\n\r\n`\r\npublic class ModelClientImpl implements ModelClient {\r\n\r\n    private String host;\r\n    private Integer port;\r\n    private ManagedChannel channel;\r\n    private PredictionServiceGrpc.PredictionServiceBlockingStub stub;\r\n\r\n    public void init() {\r\n        channel = ManagedChannelBuilder\r\n                .forAddress(getHost(), getPort())\r\n                .usePlaintext(true)\r\n                .build();\r\n\r\n        stub = PredictionServiceGrpc.newBlockingStub(channel);\r\n    }\r\n\r\n    @Override\r\n    public Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {\r\n        final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));\r\n\r\n        return response.getOutputsMap();\r\n    }\r\n\r\n    protected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {\r\n        final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()\r\n                .setName(signatureName)\r\n                .setSignatureName(\"serving_default\").build();\r\n\r\n        final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()\r\n                .setModelSpec(modelSpec)\r\n                .putAllInputs(inputs);\r\n\r\n        return builder.build();\r\n    }\r\n\r\n    public String getHost() {\r\n        return host;\r\n    }\r\n\r\n    public void setHost(String host) {\r\n        this.host = host;\r\n    }\r\n\r\n    public Integer getPort() {\r\n        return port;\r\n    }\r\n\r\n    public void setPort(Integer port) {\r\n        this.port = port;\r\n    }\r\n\r\n    @Override\r\n    public void close() throws Exception {\r\n        channel.shutdown().awaitTermination(5, TimeUnit.DAYS);\r\n    }\r\n}\r\n\r\n`\r\n\r\nBut the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?\r\n"}