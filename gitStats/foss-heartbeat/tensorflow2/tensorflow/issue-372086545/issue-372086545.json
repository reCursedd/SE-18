{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23108", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23108/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23108/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23108/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23108", "id": 372086545, "node_id": "MDU6SXNzdWUzNzIwODY1NDU=", "number": 23108, "title": "I get error when I want to load my Model", "user": {"login": "hamidrezafazlali", "id": 32719272, "node_id": "MDQ6VXNlcjMyNzE5Mjcy", "avatar_url": "https://avatars1.githubusercontent.com/u/32719272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hamidrezafazlali", "html_url": "https://github.com/hamidrezafazlali", "followers_url": "https://api.github.com/users/hamidrezafazlali/followers", "following_url": "https://api.github.com/users/hamidrezafazlali/following{/other_user}", "gists_url": "https://api.github.com/users/hamidrezafazlali/gists{/gist_id}", "starred_url": "https://api.github.com/users/hamidrezafazlali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hamidrezafazlali/subscriptions", "organizations_url": "https://api.github.com/users/hamidrezafazlali/orgs", "repos_url": "https://api.github.com/users/hamidrezafazlali/repos", "events_url": "https://api.github.com/users/hamidrezafazlali/events{/privacy}", "received_events_url": "https://api.github.com/users/hamidrezafazlali/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-10-19T19:26:45Z", "updated_at": "2018-11-05T17:21:02Z", "closed_at": "2018-11-05T17:21:02Z", "author_association": "NONE", "body_html": "<p>I am building a deep CNN and I get my graph files by running my code on a GPU on cluster. The training procedure work perfect. Then I move my graph files to my laptop to build this network on my laptop which has CPU. But when I try to load the model I receive the following error:</p>\n<p>Traceback (most recent call last): File \"dev_test.py\", line 28, in new_saver = tf.train.import_meta_graph('./3/Model_Arch3/Deep_CNN_Color_Arch8.ckpt-178000.meta') File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1810, in import_meta_graph **kwargs) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 660, in import_scoped_meta_graph producer_op_list=producer_op_list) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named ParseSingleExample in defined operations.</p>\n<p>The strange thing is that when I train this network on my laptop using my CPU, then I can load the model without any problem!</p>\n<p>Also I can build the graph and load the model successfully on the GPU cluster where I trained it!</p>\n<p>I tried the dir(tf.contrib) solution that was suggested in other question, but it did not work for me.</p>\n<p>I really appreciate if someone helps me</p>", "body_text": "I am building a deep CNN and I get my graph files by running my code on a GPU on cluster. The training procedure work perfect. Then I move my graph files to my laptop to build this network on my laptop which has CPU. But when I try to load the model I receive the following error:\nTraceback (most recent call last): File \"dev_test.py\", line 28, in new_saver = tf.train.import_meta_graph('./3/Model_Arch3/Deep_CNN_Color_Arch8.ckpt-178000.meta') File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1810, in import_meta_graph **kwargs) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 660, in import_scoped_meta_graph producer_op_list=producer_op_list) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named ParseSingleExample in defined operations.\nThe strange thing is that when I train this network on my laptop using my CPU, then I can load the model without any problem!\nAlso I can build the graph and load the model successfully on the GPU cluster where I trained it!\nI tried the dir(tf.contrib) solution that was suggested in other question, but it did not work for me.\nI really appreciate if someone helps me", "body": "I am building a deep CNN and I get my graph files by running my code on a GPU on cluster. The training procedure work perfect. Then I move my graph files to my laptop to build this network on my laptop which has CPU. But when I try to load the model I receive the following error:\r\n\r\nTraceback (most recent call last): File \"dev_test.py\", line 28, in new_saver = tf.train.import_meta_graph('./3/Model_Arch3/Deep_CNN_Color_Arch8.ckpt-178000.meta') File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1810, in import_meta_graph **kwargs) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 660, in import_scoped_meta_graph producer_op_list=producer_op_list) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named ParseSingleExample in defined operations.\r\n\r\nThe strange thing is that when I train this network on my laptop using my CPU, then I can load the model without any problem!\r\n\r\nAlso I can build the graph and load the model successfully on the GPU cluster where I trained it! \r\n\r\nI tried the dir(tf.contrib) solution that was suggested in other question, but it did not work for me.\r\n\r\nI really appreciate if someone helps me"}