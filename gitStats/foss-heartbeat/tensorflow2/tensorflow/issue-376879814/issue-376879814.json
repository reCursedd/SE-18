{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23459", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23459/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23459/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23459/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23459", "id": 376879814, "node_id": "MDU6SXNzdWUzNzY4Nzk4MTQ=", "number": 23459, "title": "AOT compilation of an Estimator", "user": {"login": "volvador", "id": 15655730, "node_id": "MDQ6VXNlcjE1NjU1NzMw", "avatar_url": "https://avatars1.githubusercontent.com/u/15655730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/volvador", "html_url": "https://github.com/volvador", "followers_url": "https://api.github.com/users/volvador/followers", "following_url": "https://api.github.com/users/volvador/following{/other_user}", "gists_url": "https://api.github.com/users/volvador/gists{/gist_id}", "starred_url": "https://api.github.com/users/volvador/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/volvador/subscriptions", "organizations_url": "https://api.github.com/users/volvador/orgs", "repos_url": "https://api.github.com/users/volvador/repos", "events_url": "https://api.github.com/users/volvador/events{/privacy}", "received_events_url": "https://api.github.com/users/volvador/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-02T16:24:39Z", "updated_at": "2018-11-06T14:49:24Z", "closed_at": "2018-11-06T14:49:23Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 18.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: NA</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.11</li>\n<li><strong>Python version</strong>:2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:NA</li>\n<li><strong>CUDA/cuDNN version</strong>:NA</li>\n<li><strong>GPU model and memory</strong>:NA</li>\n<li><strong>Exact command to reproduce</strong>:NA</li>\n</ul>\n<p>I find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to <code>tf.compile</code> the resulting predictor, produce the <code>.so</code> and link it to one's project..</p>\n<p>So I have my calib class in which I define a simple linear estimator</p>\n<pre><code>self.model = tf.estimator.LinearRegressor(\n        feature_columns=self.feature_columns,\n        model_dir = self.model_dir)\n</code></pre>\n<p>After training, I want to<br>\n1- get the trained model with optimal parameters (load it in my variable self.model)<br>\n2- extract the graph and freeze it<br>\n3- tf.compile that graph</p>\n<p>I could not find any way to do parts 1- and 2-.<br>\nCan you please point me to a good way to it?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):1.11\nPython version:2.7\nBazel version (if compiling from source):NA\nGCC/Compiler version (if compiling from source):NA\nCUDA/cuDNN version:NA\nGPU model and memory:NA\nExact command to reproduce:NA\n\nI find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to tf.compile the resulting predictor, produce the .so and link it to one's project..\nSo I have my calib class in which I define a simple linear estimator\nself.model = tf.estimator.LinearRegressor(\n        feature_columns=self.feature_columns,\n        model_dir = self.model_dir)\n\nAfter training, I want to\n1- get the trained model with optimal parameters (load it in my variable self.model)\n2- extract the graph and freeze it\n3- tf.compile that graph\nI could not find any way to do parts 1- and 2-.\nCan you please point me to a good way to it?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.11\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:NA\r\n\r\nI find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to `tf.compile` the resulting predictor, produce the `.so` and link it to one's project..\r\n\r\nSo I have my calib class in which I define a simple linear estimator\r\n\r\n    self.model = tf.estimator.LinearRegressor(\r\n            feature_columns=self.feature_columns,\r\n            model_dir = self.model_dir)\r\n\r\nAfter training, I want to \r\n1- get the trained model with optimal parameters (load it in my variable self.model)\r\n2- extract the graph and freeze it\r\n3- tf.compile that graph\r\n\r\nI could not find any way to do parts 1- and 2-. \r\nCan you please point me to a good way to it?"}