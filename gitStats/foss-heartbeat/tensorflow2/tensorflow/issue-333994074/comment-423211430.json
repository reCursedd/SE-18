{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423211430", "html_url": "https://github.com/tensorflow/tensorflow/issues/20141#issuecomment-423211430", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20141", "id": 423211430, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzIxMTQzMA==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T14:47:17Z", "updated_at": "2018-09-20T14:47:17Z", "author_association": "MEMBER", "body_html": "<p>The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet.</p>\n<p>It can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: <a href=\"https://www.tensorflow.org/performance/post_training_quantization\" rel=\"nofollow\">https://www.tensorflow.org/performance/post_training_quantization</a></p>", "body_text": "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet.\nIt can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: https://www.tensorflow.org/performance/post_training_quantization", "body": "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet.\r\n\r\nIt can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: https://www.tensorflow.org/performance/post_training_quantization"}