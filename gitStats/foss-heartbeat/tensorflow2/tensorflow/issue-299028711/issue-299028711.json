{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17169", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17169/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17169/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17169/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17169", "id": 299028711, "node_id": "MDU6SXNzdWUyOTkwMjg3MTE=", "number": 17169, "title": "tf.contrib.layers output_collections inconsistency", "user": {"login": "dodiknikola", "id": 5052480, "node_id": "MDQ6VXNlcjUwNTI0ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5052480?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dodiknikola", "html_url": "https://github.com/dodiknikola", "followers_url": "https://api.github.com/users/dodiknikola/followers", "following_url": "https://api.github.com/users/dodiknikola/following{/other_user}", "gists_url": "https://api.github.com/users/dodiknikola/gists{/gist_id}", "starred_url": "https://api.github.com/users/dodiknikola/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dodiknikola/subscriptions", "organizations_url": "https://api.github.com/users/dodiknikola/orgs", "repos_url": "https://api.github.com/users/dodiknikola/repos", "events_url": "https://api.github.com/users/dodiknikola/events{/privacy}", "received_events_url": "https://api.github.com/users/dodiknikola/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-02-21T16:05:00Z", "updated_at": "2018-03-28T20:26:13Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Please correct me if I'm wrong, but there seems to be an inconsistency between the pooling and the convolution layers in <code>tf.contrib.layers</code> when it comes to adding variables to the <code>outputs_collection</code>, and using <code>tf.get_variable_scope().reuse_variables()</code>.</p>\n<p>Here is the relevant part of the model definition.</p>\n<pre><code>def build_network(input):\n    with tf.variable_scope('scope') as sc:\n        ...\n        net = slim.max_pool2d(net, [2, 2], scope='pool')\n        ...\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        ...\n        skip = slim.conv2d(end_points[sc.name + '/pool'], out_channels, [1, 1], scope='skip')\n        ...\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        return net, end_points\n</code></pre>\n<p>And here is my test, which fails:</p>\n<pre><code>network, end_points = build_network(input)\ntf.get_variable_scope().reuse_variables()\nnetwork, end_points = build_network(input)\n</code></pre>\n<p>The reason the test fails is that whereas my convolutions all end up with the same names in the <code>end_points</code> dictionary both times (e.g. <code>scope/conv1/conv1_1</code>), the max_pooling layers end up with different names (e.g. the first time the network is built we get <code>scope/pool</code>, and the second time we get <code>scope_1/pool</code>). This means that the <code>end_points['scope/pool']</code> fails since the dictionary contains <code>'scope_1/pool'</code> as a key.</p>\n<p>I've done some breakpoint digging and it turns out that the behavior <a href=\"https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L1063\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L2266\">here</a> differs due to convolutions using variable scopes and pooling using name scopes.</p>\n<p>Now I can potentially fix my code by using <code>sc.original_name_scope</code> instead of <code>sc.name</code>, but then the end_points dictionary is still wrong. I could manually fix the dictionary, but this seems hacky. I feel like it could be made such that the behaviour is consistent across the different layers. What do you think?</p>", "body_text": "Please correct me if I'm wrong, but there seems to be an inconsistency between the pooling and the convolution layers in tf.contrib.layers when it comes to adding variables to the outputs_collection, and using tf.get_variable_scope().reuse_variables().\nHere is the relevant part of the model definition.\ndef build_network(input):\n    with tf.variable_scope('scope') as sc:\n        ...\n        net = slim.max_pool2d(net, [2, 2], scope='pool')\n        ...\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        ...\n        skip = slim.conv2d(end_points[sc.name + '/pool'], out_channels, [1, 1], scope='skip')\n        ...\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        return net, end_points\n\nAnd here is my test, which fails:\nnetwork, end_points = build_network(input)\ntf.get_variable_scope().reuse_variables()\nnetwork, end_points = build_network(input)\n\nThe reason the test fails is that whereas my convolutions all end up with the same names in the end_points dictionary both times (e.g. scope/conv1/conv1_1), the max_pooling layers end up with different names (e.g. the first time the network is built we get scope/pool, and the second time we get scope_1/pool). This means that the end_points['scope/pool'] fails since the dictionary contains 'scope_1/pool' as a key.\nI've done some breakpoint digging and it turns out that the behavior here and here differs due to convolutions using variable scopes and pooling using name scopes.\nNow I can potentially fix my code by using sc.original_name_scope instead of sc.name, but then the end_points dictionary is still wrong. I could manually fix the dictionary, but this seems hacky. I feel like it could be made such that the behaviour is consistent across the different layers. What do you think?", "body": "Please correct me if I'm wrong, but there seems to be an inconsistency between the pooling and the convolution layers in `tf.contrib.layers` when it comes to adding variables to the `outputs_collection`, and using `tf.get_variable_scope().reuse_variables()`.\r\n\r\nHere is the relevant part of the model definition.\r\n```\r\ndef build_network(input):\r\n    with tf.variable_scope('scope') as sc:\r\n        ...\r\n        net = slim.max_pool2d(net, [2, 2], scope='pool')\r\n        ...\r\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\r\n        ...\r\n        skip = slim.conv2d(end_points[sc.name + '/pool'], out_channels, [1, 1], scope='skip')\r\n        ...\r\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\r\n        return net, end_points\r\n```\r\n\r\nAnd here is my test, which fails:\r\n```\r\nnetwork, end_points = build_network(input)\r\ntf.get_variable_scope().reuse_variables()\r\nnetwork, end_points = build_network(input)\r\n```\r\n\r\nThe reason the test fails is that whereas my convolutions all end up with the same names in the `end_points` dictionary both times (e.g. `scope/conv1/conv1_1`), the max_pooling layers end up with different names (e.g. the first time the network is built we get `scope/pool`, and the second time we get `scope_1/pool`). This means that the `end_points['scope/pool']` fails since the dictionary contains `'scope_1/pool'` as a key.\r\n\r\nI've done some breakpoint digging and it turns out that the behavior [here](https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L1063) and [here](https://github.com/tensorflow/tensorflow/blob/d100729c309cb22baf1630d9f39cf60516c58cdf/tensorflow/contrib/layers/python/layers/layers.py#L2266) differs due to convolutions using variable scopes and pooling using name scopes.\r\n\r\nNow I can potentially fix my code by using `sc.original_name_scope` instead of `sc.name`, but then the end_points dictionary is still wrong. I could manually fix the dictionary, but this seems hacky. I feel like it could be made such that the behaviour is consistent across the different layers. What do you think?"}