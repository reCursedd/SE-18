{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/349721000", "html_url": "https://github.com/tensorflow/tensorflow/issues/15115#issuecomment-349721000", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15115", "id": 349721000, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTcyMTAwMA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-06T17:51:17Z", "updated_at": "2017-12-06T17:51:17Z", "author_association": "MEMBER", "body_html": "<p><code>accuracy</code> is recomputed everytime it is evaluated.</p>\n<p>How <code>tf.metrics.accuracy</code> works in general is that it maintains two variables, <code>total</code> and <code>count</code>, each which starts at 0. Whenever <code>accuracy</code> is evaluated, it returns <code>total / count</code> (or 0 if count is 0), and does not modify <code>total</code> or <code>count</code>. <code>accuracy</code> also does not look <code>labels</code> or <code>predictions</code>. It just does a division.</p>\n<p>When <code>update_op</code> is evaluated, it uses <code>labels</code> and <code>predictions</code> to increase <code>total</code> and <code>count</code>. It increases <code>total</code> by the number of predictions that match the labels, and increases <code>count</code> by the total number of predictions. The idea is that you run <code>update_op</code> for every new batch of labels and predictions you get. Then, whenever you want to check to accuracy of all the batches you've seen so far, you evaluate <code>accuracy</code>, which will have the current accuracy of the batches seen so far. <code>accuracy</code> will change only when you run <code>update_op</code>, since only running <code>update_op</code> modifies <code>total</code> and <code>count</code>.</p>\n<p>In your Estimators case, <code>update_op</code> is never run during training, only during evaluation. The Estimator will automatically call the <code>update_op</code> every batch when you return the EstimatorSpec with <code>eval_metric_ops</code> during evaluation, but it does not during training.</p>", "body_text": "accuracy is recomputed everytime it is evaluated.\nHow tf.metrics.accuracy works in general is that it maintains two variables, total and count, each which starts at 0. Whenever accuracy is evaluated, it returns total / count (or 0 if count is 0), and does not modify total or count. accuracy also does not look labels or predictions. It just does a division.\nWhen update_op is evaluated, it uses labels and predictions to increase total and count. It increases total by the number of predictions that match the labels, and increases count by the total number of predictions. The idea is that you run update_op for every new batch of labels and predictions you get. Then, whenever you want to check to accuracy of all the batches you've seen so far, you evaluate accuracy, which will have the current accuracy of the batches seen so far. accuracy will change only when you run update_op, since only running update_op modifies total and count.\nIn your Estimators case, update_op is never run during training, only during evaluation. The Estimator will automatically call the update_op every batch when you return the EstimatorSpec with eval_metric_ops during evaluation, but it does not during training.", "body": "`accuracy` is recomputed everytime it is evaluated.\r\n\r\nHow `tf.metrics.accuracy` works in general is that it maintains two variables, `total` and `count`, each which starts at 0. Whenever `accuracy` is evaluated, it returns `total / count` (or 0 if count is 0), and does not modify `total` or `count`. `accuracy` also does not look `labels` or `predictions`. It just does a division.\r\n\r\nWhen `update_op` is evaluated, it uses `labels` and `predictions` to increase `total` and `count`. It increases `total` by the number of predictions that match the labels, and increases `count` by the total number of predictions. The idea is that you run `update_op` for every new batch of labels and predictions you get. Then, whenever you want to check to accuracy of all the batches you've seen so far, you evaluate `accuracy`, which will have the current accuracy of the batches seen so far. `accuracy` will change only when you run `update_op`, since only running `update_op` modifies `total` and `count`.\r\n\r\nIn your Estimators case, `update_op` is never run during training, only during evaluation. The Estimator will automatically call the `update_op` every batch when you return the EstimatorSpec with `eval_metric_ops` during evaluation, but it does not during training."}