{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/389252977", "html_url": "https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-389252977", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18861", "id": 389252977, "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTI1Mjk3Nw==", "user": {"login": "rundembear", "id": 7693798, "node_id": "MDQ6VXNlcjc2OTM3OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7693798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rundembear", "html_url": "https://github.com/rundembear", "followers_url": "https://api.github.com/users/rundembear/followers", "following_url": "https://api.github.com/users/rundembear/following{/other_user}", "gists_url": "https://api.github.com/users/rundembear/gists{/gist_id}", "starred_url": "https://api.github.com/users/rundembear/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rundembear/subscriptions", "organizations_url": "https://api.github.com/users/rundembear/orgs", "repos_url": "https://api.github.com/users/rundembear/repos", "events_url": "https://api.github.com/users/rundembear/events{/privacy}", "received_events_url": "https://api.github.com/users/rundembear/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-15T17:43:02Z", "updated_at": "2018-05-15T17:43:02Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=31743510\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aaroey\">@aaroey</a> Thank you very much for your explanation. I don't feel like our language is quite lining up so I'm not sure whether your concern is warranted.  I am not doing anything to 'rename' the GPUs. As far as I know /gpu:3 is still /gpu:3 but I have created a tensorflow session in which reference to /gpu:0 will be redirected to /gpu:3. So if this mechanism were working as described the problem you raise would be a non-issue. If the graph has been optimized so that all of the operations are efficiently scheduled to run on /gpu:0, but within a session this is remapped then I should still be running the same graph on all GPUs.</p>\n<p>I have a feeling that what you are saying is that this mechanism should never have existed and there isn't actually a use case for which it makes sense, but perhaps I am missing something?</p>\n<p>Or are you saying that there are operations that occur outside of any sessions that see all of the GPUs, and thus might do the wrong thing? But is this really a problem if the entire graph is assigned to GPU 0?</p>\n<p>Let me clarify what our use case is to explain why this not being supported is rather unfortunate. We are carrying out Bayesian Inference and distributing our sampling across multiple GPUs. The thing is, we are running exactly the same graph on every GPU. If we do things as you suggest then I need to actually create N copies of the graph with each graph being run on a different GPU. Alternatively I can rearchitect my code into a master/servant model using multiprocessing and have each process manipulate CUDA_VISIBLE_DEVICES so that it can only see one GPU. While you may have decided that this is not something that you want to support this is exactly what the documentation says this mechanism is for.</p>", "body_text": "@aaroey Thank you very much for your explanation. I don't feel like our language is quite lining up so I'm not sure whether your concern is warranted.  I am not doing anything to 'rename' the GPUs. As far as I know /gpu:3 is still /gpu:3 but I have created a tensorflow session in which reference to /gpu:0 will be redirected to /gpu:3. So if this mechanism were working as described the problem you raise would be a non-issue. If the graph has been optimized so that all of the operations are efficiently scheduled to run on /gpu:0, but within a session this is remapped then I should still be running the same graph on all GPUs.\nI have a feeling that what you are saying is that this mechanism should never have existed and there isn't actually a use case for which it makes sense, but perhaps I am missing something?\nOr are you saying that there are operations that occur outside of any sessions that see all of the GPUs, and thus might do the wrong thing? But is this really a problem if the entire graph is assigned to GPU 0?\nLet me clarify what our use case is to explain why this not being supported is rather unfortunate. We are carrying out Bayesian Inference and distributing our sampling across multiple GPUs. The thing is, we are running exactly the same graph on every GPU. If we do things as you suggest then I need to actually create N copies of the graph with each graph being run on a different GPU. Alternatively I can rearchitect my code into a master/servant model using multiprocessing and have each process manipulate CUDA_VISIBLE_DEVICES so that it can only see one GPU. While you may have decided that this is not something that you want to support this is exactly what the documentation says this mechanism is for.", "body": "@aaroey Thank you very much for your explanation. I don't feel like our language is quite lining up so I'm not sure whether your concern is warranted.  I am not doing anything to 'rename' the GPUs. As far as I know /gpu:3 is still /gpu:3 but I have created a tensorflow session in which reference to /gpu:0 will be redirected to /gpu:3. So if this mechanism were working as described the problem you raise would be a non-issue. If the graph has been optimized so that all of the operations are efficiently scheduled to run on /gpu:0, but within a session this is remapped then I should still be running the same graph on all GPUs. \r\n\r\nI have a feeling that what you are saying is that this mechanism should never have existed and there isn't actually a use case for which it makes sense, but perhaps I am missing something?\r\n\r\nOr are you saying that there are operations that occur outside of any sessions that see all of the GPUs, and thus might do the wrong thing? But is this really a problem if the entire graph is assigned to GPU 0?\r\n\r\nLet me clarify what our use case is to explain why this not being supported is rather unfortunate. We are carrying out Bayesian Inference and distributing our sampling across multiple GPUs. The thing is, we are running exactly the same graph on every GPU. If we do things as you suggest then I need to actually create N copies of the graph with each graph being run on a different GPU. Alternatively I can rearchitect my code into a master/servant model using multiprocessing and have each process manipulate CUDA_VISIBLE_DEVICES so that it can only see one GPU. While you may have decided that this is not something that you want to support this is exactly what the documentation says this mechanism is for. "}