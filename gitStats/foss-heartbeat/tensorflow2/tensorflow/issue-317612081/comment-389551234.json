{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/389551234", "html_url": "https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-389551234", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18861", "id": 389551234, "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTU1MTIzNA==", "user": {"login": "rundembear", "id": 7693798, "node_id": "MDQ6VXNlcjc2OTM3OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7693798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rundembear", "html_url": "https://github.com/rundembear", "followers_url": "https://api.github.com/users/rundembear/followers", "following_url": "https://api.github.com/users/rundembear/following{/other_user}", "gists_url": "https://api.github.com/users/rundembear/gists{/gist_id}", "starred_url": "https://api.github.com/users/rundembear/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rundembear/subscriptions", "organizations_url": "https://api.github.com/users/rundembear/orgs", "repos_url": "https://api.github.com/users/rundembear/repos", "events_url": "https://api.github.com/users/rundembear/events{/privacy}", "received_events_url": "https://api.github.com/users/rundembear/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-16T15:01:45Z", "updated_at": "2018-05-16T15:01:45Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=31743510\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aaroey\">@aaroey</a>  One more question. I just tried reading multiple copies of my graph using this:</p>\n<pre><code>    with open(graphfile, 'rb') as f:\n        graph_text = f.read()\n\n    for g in range(len(graph_object.devices)):\n        graph_object.graphs.append(tf.Graph())\n        with tf.device(graph_object.devices[g]):\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(graph_text)\n            with graph_object.graphs[g].as_default():\n                tf.import_graph_def(graph_def, name='')\n</code></pre>\n<p>I ran this on 3 GPUs, so the devices list was:  ['/gpu:0', '/gpu:1', '/gpu:2']</p>\n<p>Doing this places all of the load onto GPU 0.</p>\n<p>So when you said this:</p>\n<p>\"In order to apply the same graph to different gpu, we can use with tf.device() with different device name when building/importing the graph.\"</p>\n<p>either I am misunderstanding what you meant, or this mechanism does not work as you had hoped.</p>\n<p>If I am missing something, please correct what I am doing wrong; Otherwise it looks like the only way to accomplish what I want to do is to use multi-processing. That certainly is an option, just wanted to try this idea first.</p>\n<p>BTW, I also tried using the 'with tf.device()' when I created the session and when I ran the session. Still sends everything to GPU 0. Sigh.</p>", "body_text": "@aaroey  One more question. I just tried reading multiple copies of my graph using this:\n    with open(graphfile, 'rb') as f:\n        graph_text = f.read()\n\n    for g in range(len(graph_object.devices)):\n        graph_object.graphs.append(tf.Graph())\n        with tf.device(graph_object.devices[g]):\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(graph_text)\n            with graph_object.graphs[g].as_default():\n                tf.import_graph_def(graph_def, name='')\n\nI ran this on 3 GPUs, so the devices list was:  ['/gpu:0', '/gpu:1', '/gpu:2']\nDoing this places all of the load onto GPU 0.\nSo when you said this:\n\"In order to apply the same graph to different gpu, we can use with tf.device() with different device name when building/importing the graph.\"\neither I am misunderstanding what you meant, or this mechanism does not work as you had hoped.\nIf I am missing something, please correct what I am doing wrong; Otherwise it looks like the only way to accomplish what I want to do is to use multi-processing. That certainly is an option, just wanted to try this idea first.\nBTW, I also tried using the 'with tf.device()' when I created the session and when I ran the session. Still sends everything to GPU 0. Sigh.", "body": "@aaroey  One more question. I just tried reading multiple copies of my graph using this:\r\n\r\n        with open(graphfile, 'rb') as f:\r\n            graph_text = f.read()\r\n\r\n        for g in range(len(graph_object.devices)):\r\n            graph_object.graphs.append(tf.Graph())\r\n            with tf.device(graph_object.devices[g]):\r\n                graph_def = tf.GraphDef()\r\n                graph_def.ParseFromString(graph_text)\r\n                with graph_object.graphs[g].as_default():\r\n                    tf.import_graph_def(graph_def, name='')\r\n\r\n\r\nI ran this on 3 GPUs, so the devices list was:  ['/gpu:0', '/gpu:1', '/gpu:2']\r\n\r\nDoing this places all of the load onto GPU 0. \r\n\r\nSo when you said this: \r\n\r\n\"In order to apply the same graph to different gpu, we can use with tf.device() with different device name when building/importing the graph.\"\r\n\r\neither I am misunderstanding what you meant, or this mechanism does not work as you had hoped. \r\n\r\nIf I am missing something, please correct what I am doing wrong; Otherwise it looks like the only way to accomplish what I want to do is to use multi-processing. That certainly is an option, just wanted to try this idea first. \r\n\r\nBTW, I also tried using the 'with tf.device()' when I created the session and when I ran the session. Still sends everything to GPU 0. Sigh."}