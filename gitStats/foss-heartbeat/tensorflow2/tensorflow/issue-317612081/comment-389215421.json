{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/389215421", "html_url": "https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-389215421", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18861", "id": 389215421, "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTIxNTQyMQ==", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-15T15:45:20Z", "updated_at": "2018-05-15T15:45:20Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7693798\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rundembear\">@rundembear</a>,</p>\n<p>Thanks for pointing out this. After some discussion, we think that the way you configure TF gpu device is not truly supported by TF, even in v1.3. If I understand correctly, your program has multiple threads, each creates a device with same name (\"/gpu:0\") but pointing to different physical gpu hardware. This will cause problem when any code/lib wants to access the gpu hardware using the gpu id in the device name, as all access will be directed to the first gpu, not the gpu that the device pointing to. There are actually several code paths like this in TF, for example, grappler will extract the id from the device name and use it to call methods like <code>cudaGetDeviceProperties()</code>, so regardless which actual device your graph is using, grappler will always use the information from the first physical gpu to do the optimization. There are other use cases, including NCCL, TensorRT integration, and maybe others that I don't know.</p>\n<p>Above all, the way how you configure TF gpu may work under certain circumstances (like what you observed), but is a way that TF doesn't intend to support. In general, TF doesn't fully support different config proto within same process, and different sessions should use same gpu config. As a result, I would suggest, if possible, to list all available gpu ids in <code>visible_device_list</code>, and use one for each graph. Or we can discuss more if you have any questions.</p>\n<p>Thanks.</p>", "body_text": "Hi @rundembear,\nThanks for pointing out this. After some discussion, we think that the way you configure TF gpu device is not truly supported by TF, even in v1.3. If I understand correctly, your program has multiple threads, each creates a device with same name (\"/gpu:0\") but pointing to different physical gpu hardware. This will cause problem when any code/lib wants to access the gpu hardware using the gpu id in the device name, as all access will be directed to the first gpu, not the gpu that the device pointing to. There are actually several code paths like this in TF, for example, grappler will extract the id from the device name and use it to call methods like cudaGetDeviceProperties(), so regardless which actual device your graph is using, grappler will always use the information from the first physical gpu to do the optimization. There are other use cases, including NCCL, TensorRT integration, and maybe others that I don't know.\nAbove all, the way how you configure TF gpu may work under certain circumstances (like what you observed), but is a way that TF doesn't intend to support. In general, TF doesn't fully support different config proto within same process, and different sessions should use same gpu config. As a result, I would suggest, if possible, to list all available gpu ids in visible_device_list, and use one for each graph. Or we can discuss more if you have any questions.\nThanks.", "body": "Hi @rundembear,\r\n\r\nThanks for pointing out this. After some discussion, we think that the way you configure TF gpu device is not truly supported by TF, even in v1.3. If I understand correctly, your program has multiple threads, each creates a device with same name (\"/gpu:0\") but pointing to different physical gpu hardware. This will cause problem when any code/lib wants to access the gpu hardware using the gpu id in the device name, as all access will be directed to the first gpu, not the gpu that the device pointing to. There are actually several code paths like this in TF, for example, grappler will extract the id from the device name and use it to call methods like `cudaGetDeviceProperties()`, so regardless which actual device your graph is using, grappler will always use the information from the first physical gpu to do the optimization. There are other use cases, including NCCL, TensorRT integration, and maybe others that I don't know.\r\n\r\nAbove all, the way how you configure TF gpu may work under certain circumstances (like what you observed), but is a way that TF doesn't intend to support. In general, TF doesn't fully support different config proto within same process, and different sessions should use same gpu config. As a result, I would suggest, if possible, to list all available gpu ids in `visible_device_list`, and use one for each graph. Or we can discuss more if you have any questions.\r\n\r\nThanks."}