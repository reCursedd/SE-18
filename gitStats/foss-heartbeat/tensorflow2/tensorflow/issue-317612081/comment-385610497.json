{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/385610497", "html_url": "https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-385610497", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18861", "id": 385610497, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTYxMDQ5Nw==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-01T06:43:35Z", "updated_at": "2018-05-01T06:43:35Z", "author_association": "MEMBER", "body_html": "<p>Unfortunately, though <code>visible_deivces_list</code> is included in <code>ConfigProto</code>, it is actually a per-process setting. In fact, this is true of almost all options inside the <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/core/protobuf/config.proto#L123\"><code>GPUOptions</code></a> protocol buffer.</p>\n<p>Apologies for that.</p>\n<p>In the mean time, a possible work around is to have the models themselves explicitly use different GPUs. You can do so by explicitly constructing the models as so (e.g.., using <code>with tf.device('/gpu:1')</code> when creating the graph in a Python program), or editing the graph before creating the session (a similar workaround <a href=\"https://stackoverflow.com/a/47915987/6708503\" rel=\"nofollow\">in Java was suggested on StackOverflow</a>).</p>\n<p>Hope that helps in the short term.</p>\n<p>Assigning to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1192265\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yifeif\">@yifeif</a> - who is looking into at least making this subtlety more explicit and then perhaps even overcoming this limitation.</p>\n<p>Also CCing <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1994308\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/akshayka\">@akshayka</a> who was looking into more dynamic assignment of devices to models at runtime.</p>", "body_text": "Unfortunately, though visible_deivces_list is included in ConfigProto, it is actually a per-process setting. In fact, this is true of almost all options inside the GPUOptions protocol buffer.\nApologies for that.\nIn the mean time, a possible work around is to have the models themselves explicitly use different GPUs. You can do so by explicitly constructing the models as so (e.g.., using with tf.device('/gpu:1') when creating the graph in a Python program), or editing the graph before creating the session (a similar workaround in Java was suggested on StackOverflow).\nHope that helps in the short term.\nAssigning to @yifeif - who is looking into at least making this subtlety more explicit and then perhaps even overcoming this limitation.\nAlso CCing @akshayka who was looking into more dynamic assignment of devices to models at runtime.", "body": "Unfortunately, though `visible_deivces_list` is included in `ConfigProto`, it is actually a per-process setting. In fact, this is true of almost all options inside the [`GPUOptions`](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/core/protobuf/config.proto#L123) protocol buffer. \r\n\r\nApologies for that.\r\n\r\nIn the mean time, a possible work around is to have the models themselves explicitly use different GPUs. You can do so by explicitly constructing the models as so (e.g.., using `with tf.device('/gpu:1')` when creating the graph in a Python program), or editing the graph before creating the session (a similar workaround [in Java was suggested on StackOverflow](https://stackoverflow.com/a/47915987/6708503)).\r\n\r\nHope that helps in the short term.\r\n\r\nAssigning to @yifeif - who is looking into at least making this subtlety more explicit and then perhaps even overcoming this limitation.\r\n\r\nAlso CCing @akshayka who was looking into more dynamic assignment of devices to models at runtime."}