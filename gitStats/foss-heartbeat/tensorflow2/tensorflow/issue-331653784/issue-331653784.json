{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19945", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19945/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19945/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19945/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19945", "id": 331653784, "node_id": "MDU6SXNzdWUzMzE2NTM3ODQ=", "number": 19945, "title": "The argument \"num_parallel_calls\" in tf.data.Dataset.map() doesn't work in eager execution.", "user": {"login": "DHZS", "id": 10752166, "node_id": "MDQ6VXNlcjEwNzUyMTY2", "avatar_url": "https://avatars1.githubusercontent.com/u/10752166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DHZS", "html_url": "https://github.com/DHZS", "followers_url": "https://api.github.com/users/DHZS/followers", "following_url": "https://api.github.com/users/DHZS/following{/other_user}", "gists_url": "https://api.github.com/users/DHZS/gists{/gist_id}", "starred_url": "https://api.github.com/users/DHZS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DHZS/subscriptions", "organizations_url": "https://api.github.com/users/DHZS/orgs", "repos_url": "https://api.github.com/users/DHZS/repos", "events_url": "https://api.github.com/users/DHZS/events{/privacy}", "received_events_url": "https://api.github.com/users/DHZS/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-06-12T16:03:10Z", "updated_at": "2018-07-16T21:52:05Z", "closed_at": "2018-07-16T21:52:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nWindows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\npip</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.8.0</li>\n<li><strong>Python version</strong>:<br>\n3.6.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nNo</li>\n<li><strong>GPU model and memory</strong>:<br>\nNo</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nNo</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>I use <code>tf.py_func</code>(<code>tfe.py_func</code> has the same problem) in <code>tf.data.Dataset.map()</code> function to pre-process my training data in eager execution.</p>\n<p>In my model, the batch size is 16, and it costs about 30ms to process a piece of data. So it takes 16*30=480ms to process a batch of training data. The network trained with a batch of data costs about 300ms.</p>\n<p>I followed this guide (<a href=\"https://www.tensorflow.org/performance/datasets_performance\" rel=\"nofollow\">https://www.tensorflow.org/performance/datasets_performance</a>) and try to build  an efficient input pipeline.</p>\n<p>First, I use <code>prefetch(1)</code> after <code>batch(16)</code>, and it works(480ms per batch).<br>\nThen, I use <code>map(map_func, num_parallel_calls=4)</code> to pre-process the data in parallel. But it doesn't work. It costs 480ms per batch.<br>\nFurthermore, I try it without eager execution. And it works well(300ms per batch).</p>\n<h3>Source code / logs</h3>\n<p>Using Eager Execution</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.eager.python <span class=\"pl-k\">import</span> tfe\ntf.enable_eager_execution()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    data <span class=\"pl-k\">=</span> tf.range(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">map_fn</span>(<span class=\"pl-smi\">n</span>):\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">fn</span>(<span class=\"pl-smi\">nn</span>):\n            time.sleep(<span class=\"pl-c1\">0.03</span>)\n            <span class=\"pl-k\">return</span> tf.constant(<span class=\"pl-c1\">1</span>.)\n        <span class=\"pl-k\">return</span> tfe.py_func(fn, [n], [tf.float32])\n\n    dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(data)\n    dataset <span class=\"pl-k\">=</span> dataset.repeat()\n    dataset <span class=\"pl-k\">=</span> dataset.map(map_fn, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>).batch(<span class=\"pl-c1\">16</span>).prefetch(<span class=\"pl-c1\">1</span>)\n\n    t <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> dataset:\n        time.sleep(<span class=\"pl-c1\">0.3</span>)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>time: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format((time.time() <span class=\"pl-k\">-</span> t) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>))\n        t <span class=\"pl-k\">=</span> time.time()\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<p>output:</p>\n<blockquote>\n<p>time: 880.5060386657715<br>\ntime: 487.31040954589844<br>\ntime: 487.4300956726074<br>\ntime: 487.37573623657227<br>\n...</p>\n</blockquote>\n<p>Without Eager Execution</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    data <span class=\"pl-k\">=</span> tf.range(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">map_fn</span>(<span class=\"pl-smi\">n</span>):\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">fn</span>(<span class=\"pl-smi\">nn</span>):\n            time.sleep(<span class=\"pl-c1\">0.03</span>)\n            <span class=\"pl-k\">return</span> np.array([<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n        <span class=\"pl-k\">return</span> tf.py_func(fn, [n], [np.float32])\n\n    dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(data)\n    dataset <span class=\"pl-k\">=</span> dataset.repeat()\n    dataset <span class=\"pl-k\">=</span> dataset.map(map_fn, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>).batch(<span class=\"pl-c1\">16</span>).prefetch(<span class=\"pl-c1\">1</span>)\n\n    next_data <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator().get_next()\n\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    t <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n        time.sleep(<span class=\"pl-c1\">0.3</span>)\n        d <span class=\"pl-k\">=</span> sess.run(next_data)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>time: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format((time.time() <span class=\"pl-k\">-</span> t) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>))\n        t <span class=\"pl-k\">=</span> time.time()\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<p>output:</p>\n<blockquote>\n<p>time: 435.55521965026855<br>\ntime: 301.36632919311523<br>\ntime: 301.2561798095703<br>\ntime: 301.67603492736816<br>\n...</p>\n</blockquote>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nWindows 10\nTensorFlow installed from (source or binary):\npip\nTensorFlow version (use command below):\n1.8.0\nPython version:\n3.6.4\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nNo\nGPU model and memory:\nNo\nExact command to reproduce:\nNo\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nI use tf.py_func(tfe.py_func has the same problem) in tf.data.Dataset.map() function to pre-process my training data in eager execution.\nIn my model, the batch size is 16, and it costs about 30ms to process a piece of data. So it takes 16*30=480ms to process a batch of training data. The network trained with a batch of data costs about 300ms.\nI followed this guide (https://www.tensorflow.org/performance/datasets_performance) and try to build  an efficient input pipeline.\nFirst, I use prefetch(1) after batch(16), and it works(480ms per batch).\nThen, I use map(map_func, num_parallel_calls=4) to pre-process the data in parallel. But it doesn't work. It costs 480ms per batch.\nFurthermore, I try it without eager execution. And it works well(300ms per batch).\nSource code / logs\nUsing Eager Execution\nimport time\nimport tensorflow as tf\nfrom tensorflow.contrib.eager.python import tfe\ntf.enable_eager_execution()\n\n\ndef main():\n    data = tf.range(0, 100, dtype=tf.float32)\n\n    def map_fn(n):\n        def fn(nn):\n            time.sleep(0.03)\n            return tf.constant(1.)\n        return tfe.py_func(fn, [n], [tf.float32])\n\n    dataset = tf.data.Dataset.from_tensor_slices(data)\n    dataset = dataset.repeat()\n    dataset = dataset.map(map_fn, num_parallel_calls=4).batch(16).prefetch(1)\n\n    t = time.time()\n    for i in dataset:\n        time.sleep(0.3)\n        print('time: {}'.format((time.time() - t) * 1000))\n        t = time.time()\n\n\nif __name__ == '__main__':\n    main()\noutput:\n\ntime: 880.5060386657715\ntime: 487.31040954589844\ntime: 487.4300956726074\ntime: 487.37573623657227\n...\n\nWithout Eager Execution\nimport time\nimport numpy as np\nimport tensorflow as tf\n\n\ndef main():\n    data = tf.range(0, 100, dtype=tf.float32)\n\n    def map_fn(n):\n        def fn(nn):\n            time.sleep(0.03)\n            return np.array([1], dtype=np.float32)\n        return tf.py_func(fn, [n], [np.float32])\n\n    dataset = tf.data.Dataset.from_tensor_slices(data)\n    dataset = dataset.repeat()\n    dataset = dataset.map(map_fn, num_parallel_calls=4).batch(16).prefetch(1)\n\n    next_data = dataset.make_one_shot_iterator().get_next()\n\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    t = time.time()\n    for i in range(10000):\n        time.sleep(0.3)\n        d = sess.run(next_data)\n        print('time: {}'.format((time.time() - t) * 1000))\n        t = time.time()\n\n\nif __name__ == '__main__':\n    main()\noutput:\n\ntime: 435.55521965026855\ntime: 301.36632919311523\ntime: 301.2561798095703\ntime: 301.67603492736816\n...", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nNo\r\n- **GPU model and memory**:\r\nNo\r\n- **Exact command to reproduce**:\r\nNo\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI use `tf.py_func`(`tfe.py_func` has the same problem) in `tf.data.Dataset.map()` function to pre-process my training data in eager execution.\r\n\r\nIn my model, the batch size is 16, and it costs about 30ms to process a piece of data. So it takes 16*30=480ms to process a batch of training data. The network trained with a batch of data costs about 300ms.\r\n\r\nI followed this guide (https://www.tensorflow.org/performance/datasets_performance) and try to build  an efficient input pipeline.\r\n\r\nFirst, I use `prefetch(1)` after `batch(16)`, and it works(480ms per batch).\r\nThen, I use `map(map_func, num_parallel_calls=4)` to pre-process the data in parallel. But it doesn't work. It costs 480ms per batch.\r\nFurthermore, I try it without eager execution. And it works well(300ms per batch). \r\n\r\n\r\n### Source code / logs\r\nUsing Eager Execution\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.eager.python import tfe\r\ntf.enable_eager_execution()\r\n\r\n\r\ndef main():\r\n    data = tf.range(0, 100, dtype=tf.float32)\r\n\r\n    def map_fn(n):\r\n        def fn(nn):\r\n            time.sleep(0.03)\r\n            return tf.constant(1.)\r\n        return tfe.py_func(fn, [n], [tf.float32])\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(data)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.map(map_fn, num_parallel_calls=4).batch(16).prefetch(1)\r\n\r\n    t = time.time()\r\n    for i in dataset:\r\n        time.sleep(0.3)\r\n        print('time: {}'.format((time.time() - t) * 1000))\r\n        t = time.time()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\noutput:\r\n>time: 880.5060386657715\r\ntime: 487.31040954589844\r\ntime: 487.4300956726074\r\ntime: 487.37573623657227\r\n...\r\n\r\n\r\n\r\nWithout Eager Execution\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    data = tf.range(0, 100, dtype=tf.float32)\r\n\r\n    def map_fn(n):\r\n        def fn(nn):\r\n            time.sleep(0.03)\r\n            return np.array([1], dtype=np.float32)\r\n        return tf.py_func(fn, [n], [np.float32])\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(data)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.map(map_fn, num_parallel_calls=4).batch(16).prefetch(1)\r\n\r\n    next_data = dataset.make_one_shot_iterator().get_next()\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    t = time.time()\r\n    for i in range(10000):\r\n        time.sleep(0.3)\r\n        d = sess.run(next_data)\r\n        print('time: {}'.format((time.time() - t) * 1000))\r\n        t = time.time()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\noutput:\r\n> time: 435.55521965026855\r\ntime: 301.36632919311523\r\ntime: 301.2561798095703\r\ntime: 301.67603492736816\r\n..."}