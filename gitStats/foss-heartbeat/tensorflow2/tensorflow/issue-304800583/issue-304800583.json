{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17679", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17679/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17679/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17679/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17679", "id": 304800583, "node_id": "MDU6SXNzdWUzMDQ4MDA1ODM=", "number": 17679, "title": "Error with combination of 'numpy_input_fn' and 'tf.contrib.seq2seq.AttentionWrapper'", "user": {"login": "yanghoonkim", "id": 9985986, "node_id": "MDQ6VXNlcjk5ODU5ODY=", "avatar_url": "https://avatars2.githubusercontent.com/u/9985986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yanghoonkim", "html_url": "https://github.com/yanghoonkim", "followers_url": "https://api.github.com/users/yanghoonkim/followers", "following_url": "https://api.github.com/users/yanghoonkim/following{/other_user}", "gists_url": "https://api.github.com/users/yanghoonkim/gists{/gist_id}", "starred_url": "https://api.github.com/users/yanghoonkim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yanghoonkim/subscriptions", "organizations_url": "https://api.github.com/users/yanghoonkim/orgs", "repos_url": "https://api.github.com/users/yanghoonkim/repos", "events_url": "https://api.github.com/users/yanghoonkim/events{/privacy}", "received_events_url": "https://api.github.com/users/yanghoonkim/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-03-13T14:58:17Z", "updated_at": "2018-09-08T16:15:30Z", "closed_at": "2018-09-08T16:15:30Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4 and 1.6 both</li>\n<li><strong>Python version</strong>: 2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8 for tensorflow 1.4 and 9.0 for tensorflow 1.6</li>\n<li><strong>GPU model and memory</strong>: gtx 1080ti</li>\n<li><strong>Exact command to reproduce</strong>: I would like to send my code via email.</li>\n</ul>\n<p><strong>Problem description:</strong><br>\nI am now modeling an architecture that is related to encoder-decoder model. So, first of all, I wrote my own source code of encoder-decoder model with tensorflow api. the abstract structure of my source code(<code>model_fn</code> which is used for <code>tf.contrib.learn.Experiment</code>) is as follows:</p>\n<pre><code>def model_fn(features, labels, mode, params):\n    \n    start_token = 1 \n    end_token = 2\n    # should pay attention to this batch_size from params\n    batch_size = params['batch_size']\n\n    input = features['input'] # [batch, length]\n    input_length = features['input_length']\n    \n    if mode != tf.estimator.ModeKeys.PREDICT:\n        target = features['target'] # label\n        target_length = features['target_length']\n    \n    # Embedding for sentence, question and rnn encoding of sentence\n    with tf.variable_scope('SharedScope'):\n        # Embedded inputs\n        # [batch, input_length] -&gt; [batch, input_length, hidden_size]\n        embd_input = embed_op(input, params, name = 'embedded_input')\n        embd_target = embed_op(target, params, name = 'embedded_target')\n\n        # Build encoder cell\n        encoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n\n        # Run Dynamic RNN\n        #   encoder_outputs: [max_time, batch_size, num_units]\n        #   encoder_state: [batch_size, num_units]\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n            encoder_cell, embd_input,\n            sequence_length=input_length,\n            dtype = tf.float32    \n            )   \n        \n    with tf.variable_scope('SharedScope/EmbeddingScope', reuse = True):\n        embedding_target = tf.get_variable('embedding_target')\n    \n    # Rnn decoding of sentence with attention \n    with tf.variable_scope('Decoder'):\n        # Memory for attention\n        attention_states = encoder_outputs\n\n        # Create an attention mechanism\n        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n                hidden_size, attention_states,\n                memory_sequence_length=input_length)\n\n        # Build decoder cell\n        decoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n                decoder_cell, attention_mechanism,\n                attention_layer_size= hidden_size)\n\n        # Helper for decoder cell\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            maxlen_target = params['maxlen_target'] * tf.ones(batch_size], dtype = tf.int32)\n            helper = tf.contrib.seq2seq.TrainingHelper(\n                    embd_target, maxlen_target\n                    )\n        else: # EVAL &amp; TEST\n            start_tokens = start_token * tf.ones([batch_size], dtype = tf.int32)\n            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                    embedding_target, start_tokens, end_token\n                    )\n        # Decoder\n        initial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\n        projection_q = tf.layers.Dense(target_voca_size, use_bias = True)\n\n        decoder = tf.contrib.seq2seq.BasicDecoder(\n            decoder_cell, helper, initial_state,\n            output_layer=None)\n\n        # Dynamic decoding\n        max_iter = params['maxlen_target_dev'] \n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = None)\n        else: # Test\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = max_iter)\n\n        logits_q = projection_q(outputs.rnn_output)\n.\n.\n.\n</code></pre>\n<p>and the data(<code>features</code>) are feeded through <code>tf.estimator.inputs.numpy_input_fn</code>:<br>\nFor evaluation data:</p>\n<pre><code>    # Evaluation input function for estimator\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x = {\"input\": eval_input, 'target': eval_target, \n            'input_length': eval_input_length, 'len_q': eval_target_length},\n        y = None,\n        batch_size = model_params['batch_size'], # batch size that i specified\n        num_epochs=1,\n        shuffle=False)  \n</code></pre>\n<p><strong>When <code>batch_size</code> that I specified has lower value than 99(not including), it works fine</strong>, I mean, when I run <code>neural_network_experiment.train_and_evaluate()</code>, it trains well and also evaluate without an error. <strong>However, when I set <code>batch_size</code> bigger than 98, There is always an error only when evaluating(no matter with training period)</strong> :</p>\n<p><code>InvalidArgumentError (see above for traceback): assertion failed: [When applying AttentionWrapper attention_wrapper_1: Non-matching batch sizes between the memory (encoder output) and the query (decoder output).  Are you using the BeamSearchDecoder?  You may need to tile your memory input via the tf.contrib.seq2seq.tile_batch function with argument multiple=beam_width.] [Condition x == y did not hold element-wise:] [x (QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x:0) = ] [99] [y (QuestionGeneration/LuongAttention/strided_slice_3:0) = ] [14] [[Node: QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/All/_389, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_0, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_1, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_2, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x/_391, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_4, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Equal/Enter/_393)]] [[Node: QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert/_384 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_397_QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]</code></p>\n<p>I solved this error by change the <code>batch_size</code> value in <code>model_fn</code> by <code>attention_mechanism._batch_size</code>.<br>\nThe problem may caused from the redundant data that can't be divided by <code>batch_size</code>( if i have 1004 lines of data and my <code>batch_size</code> is 10, than 4 lines data will be left). So when I change the <code>batch_size</code> value in <code>model_fn</code> to <code>attention_mechanism._batch_size</code> from specified value, the last iteration's batch size will be 4 and no error anymore. About the error message above, I used batch size of 99, and there will be 14 lines of data left. Then, I want to ask for 2 questions that may related to error:</p>\n<ol>\n<li>when I fixed the <code>batch_size</code> by specified value smaller than 99, why no error(both training and evaluate)</li>\n<li>when I fixed the 'batch_size' by specified value bigger than 98, why no error in training period and does have error in evaluation period.</li>\n</ol>\n<p>I think there may be some errors in tensorflow api related to this. Or maybe I was wrong in some part.</p>\n<p>Also, I hope to see the details of <code>numpy_input_fn</code>, such as: when used with <code>tf.estimator</code>, How will it treat the redundant data that can't be divided by batch_size</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4 and 1.6 both\nPython version: 2\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 8 for tensorflow 1.4 and 9.0 for tensorflow 1.6\nGPU model and memory: gtx 1080ti\nExact command to reproduce: I would like to send my code via email.\n\nProblem description:\nI am now modeling an architecture that is related to encoder-decoder model. So, first of all, I wrote my own source code of encoder-decoder model with tensorflow api. the abstract structure of my source code(model_fn which is used for tf.contrib.learn.Experiment) is as follows:\ndef model_fn(features, labels, mode, params):\n    \n    start_token = 1 \n    end_token = 2\n    # should pay attention to this batch_size from params\n    batch_size = params['batch_size']\n\n    input = features['input'] # [batch, length]\n    input_length = features['input_length']\n    \n    if mode != tf.estimator.ModeKeys.PREDICT:\n        target = features['target'] # label\n        target_length = features['target_length']\n    \n    # Embedding for sentence, question and rnn encoding of sentence\n    with tf.variable_scope('SharedScope'):\n        # Embedded inputs\n        # [batch, input_length] -> [batch, input_length, hidden_size]\n        embd_input = embed_op(input, params, name = 'embedded_input')\n        embd_target = embed_op(target, params, name = 'embedded_target')\n\n        # Build encoder cell\n        encoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n\n        # Run Dynamic RNN\n        #   encoder_outputs: [max_time, batch_size, num_units]\n        #   encoder_state: [batch_size, num_units]\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n            encoder_cell, embd_input,\n            sequence_length=input_length,\n            dtype = tf.float32    \n            )   \n        \n    with tf.variable_scope('SharedScope/EmbeddingScope', reuse = True):\n        embedding_target = tf.get_variable('embedding_target')\n    \n    # Rnn decoding of sentence with attention \n    with tf.variable_scope('Decoder'):\n        # Memory for attention\n        attention_states = encoder_outputs\n\n        # Create an attention mechanism\n        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n                hidden_size, attention_states,\n                memory_sequence_length=input_length)\n\n        # Build decoder cell\n        decoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n                decoder_cell, attention_mechanism,\n                attention_layer_size= hidden_size)\n\n        # Helper for decoder cell\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            maxlen_target = params['maxlen_target'] * tf.ones(batch_size], dtype = tf.int32)\n            helper = tf.contrib.seq2seq.TrainingHelper(\n                    embd_target, maxlen_target\n                    )\n        else: # EVAL & TEST\n            start_tokens = start_token * tf.ones([batch_size], dtype = tf.int32)\n            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                    embedding_target, start_tokens, end_token\n                    )\n        # Decoder\n        initial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\n        projection_q = tf.layers.Dense(target_voca_size, use_bias = True)\n\n        decoder = tf.contrib.seq2seq.BasicDecoder(\n            decoder_cell, helper, initial_state,\n            output_layer=None)\n\n        # Dynamic decoding\n        max_iter = params['maxlen_target_dev'] \n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = None)\n        else: # Test\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = max_iter)\n\n        logits_q = projection_q(outputs.rnn_output)\n.\n.\n.\n\nand the data(features) are feeded through tf.estimator.inputs.numpy_input_fn:\nFor evaluation data:\n    # Evaluation input function for estimator\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x = {\"input\": eval_input, 'target': eval_target, \n            'input_length': eval_input_length, 'len_q': eval_target_length},\n        y = None,\n        batch_size = model_params['batch_size'], # batch size that i specified\n        num_epochs=1,\n        shuffle=False)  \n\nWhen batch_size that I specified has lower value than 99(not including), it works fine, I mean, when I run neural_network_experiment.train_and_evaluate(), it trains well and also evaluate without an error. However, when I set batch_size bigger than 98, There is always an error only when evaluating(no matter with training period) :\nInvalidArgumentError (see above for traceback): assertion failed: [When applying AttentionWrapper attention_wrapper_1: Non-matching batch sizes between the memory (encoder output) and the query (decoder output).  Are you using the BeamSearchDecoder?  You may need to tile your memory input via the tf.contrib.seq2seq.tile_batch function with argument multiple=beam_width.] [Condition x == y did not hold element-wise:] [x (QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x:0) = ] [99] [y (QuestionGeneration/LuongAttention/strided_slice_3:0) = ] [14] [[Node: QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/All/_389, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_0, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_1, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_2, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x/_391, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_4, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Equal/Enter/_393)]] [[Node: QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert/_384 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_397_QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\nI solved this error by change the batch_size value in model_fn by attention_mechanism._batch_size.\nThe problem may caused from the redundant data that can't be divided by batch_size( if i have 1004 lines of data and my batch_size is 10, than 4 lines data will be left). So when I change the batch_size value in model_fn to attention_mechanism._batch_size from specified value, the last iteration's batch size will be 4 and no error anymore. About the error message above, I used batch size of 99, and there will be 14 lines of data left. Then, I want to ask for 2 questions that may related to error:\n\nwhen I fixed the batch_size by specified value smaller than 99, why no error(both training and evaluate)\nwhen I fixed the 'batch_size' by specified value bigger than 98, why no error in training period and does have error in evaluation period.\n\nI think there may be some errors in tensorflow api related to this. Or maybe I was wrong in some part.\nAlso, I hope to see the details of numpy_input_fn, such as: when used with tf.estimator, How will it treat the redundant data that can't be divided by batch_size", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4 and 1.6 both\r\n- **Python version**: 2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8 for tensorflow 1.4 and 9.0 for tensorflow 1.6\r\n- **GPU model and memory**: gtx 1080ti\r\n- **Exact command to reproduce**: I would like to send my code via email.\r\n\r\n**Problem description:**\r\nI am now modeling an architecture that is related to encoder-decoder model. So, first of all, I wrote my own source code of encoder-decoder model with tensorflow api. the abstract structure of my source code(`model_fn` which is used for `tf.contrib.learn.Experiment`) is as follows:\r\n\r\n```\r\ndef model_fn(features, labels, mode, params):\r\n    \r\n    start_token = 1 \r\n    end_token = 2\r\n    # should pay attention to this batch_size from params\r\n    batch_size = params['batch_size']\r\n\r\n    input = features['input'] # [batch, length]\r\n    input_length = features['input_length']\r\n    \r\n    if mode != tf.estimator.ModeKeys.PREDICT:\r\n        target = features['target'] # label\r\n        target_length = features['target_length']\r\n    \r\n    # Embedding for sentence, question and rnn encoding of sentence\r\n    with tf.variable_scope('SharedScope'):\r\n        # Embedded inputs\r\n        # [batch, input_length] -> [batch, input_length, hidden_size]\r\n        embd_input = embed_op(input, params, name = 'embedded_input')\r\n        embd_target = embed_op(target, params, name = 'embedded_target')\r\n\r\n        # Build encoder cell\r\n        encoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n\r\n        # Run Dynamic RNN\r\n        #   encoder_outputs: [max_time, batch_size, num_units]\r\n        #   encoder_state: [batch_size, num_units]\r\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n            encoder_cell, embd_input,\r\n            sequence_length=input_length,\r\n            dtype = tf.float32    \r\n            )   \r\n        \r\n    with tf.variable_scope('SharedScope/EmbeddingScope', reuse = True):\r\n        embedding_target = tf.get_variable('embedding_target')\r\n    \r\n    # Rnn decoding of sentence with attention \r\n    with tf.variable_scope('Decoder'):\r\n        # Memory for attention\r\n        attention_states = encoder_outputs\r\n\r\n        # Create an attention mechanism\r\n        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n                hidden_size, attention_states,\r\n                memory_sequence_length=input_length)\r\n\r\n        # Build decoder cell\r\n        decoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n\r\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n                decoder_cell, attention_mechanism,\r\n                attention_layer_size= hidden_size)\r\n\r\n        # Helper for decoder cell\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            maxlen_target = params['maxlen_target'] * tf.ones(batch_size], dtype = tf.int32)\r\n            helper = tf.contrib.seq2seq.TrainingHelper(\r\n                    embd_target, maxlen_target\r\n                    )\r\n        else: # EVAL & TEST\r\n            start_tokens = start_token * tf.ones([batch_size], dtype = tf.int32)\r\n            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n                    embedding_target, start_tokens, end_token\r\n                    )\r\n        # Decoder\r\n        initial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\r\n        projection_q = tf.layers.Dense(target_voca_size, use_bias = True)\r\n\r\n        decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            decoder_cell, helper, initial_state,\r\n            output_layer=None)\r\n\r\n        # Dynamic decoding\r\n        max_iter = params['maxlen_target_dev'] \r\n\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = None)\r\n        else: # Test\r\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = max_iter)\r\n\r\n        logits_q = projection_q(outputs.rnn_output)\r\n.\r\n.\r\n.\r\n```\r\n\r\nand the data(`features`) are feeded through `tf.estimator.inputs.numpy_input_fn`:\r\nFor evaluation data:\r\n\r\n```\r\n    # Evaluation input function for estimator\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x = {\"input\": eval_input, 'target': eval_target, \r\n            'input_length': eval_input_length, 'len_q': eval_target_length},\r\n        y = None,\r\n        batch_size = model_params['batch_size'], # batch size that i specified\r\n        num_epochs=1,\r\n        shuffle=False)  \r\n```\r\n\r\n\r\n**When `batch_size` that I specified has lower value than 99(not including), it works fine**, I mean, when I run `neural_network_experiment.train_and_evaluate()`, it trains well and also evaluate without an error. **However, when I set `batch_size` bigger than 98, There is always an error only when evaluating(no matter with training period)** : \r\n\r\n`\r\nInvalidArgumentError (see above for traceback): assertion failed: [When applying AttentionWrapper attention_wrapper_1: Non-matching batch sizes between the memory (encoder output) and the query (decoder output).  Are you using\r\nthe BeamSearchDecoder?  You may need to tile your memory input via the tf.contrib.seq2seq.tile_batch function with argument multiple=beam_width.] [Condition x == y did not hold element-wise:] [x (QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x:0) = ] [99] [y (QuestionGeneration/LuongAttention/strided_slice_3:0) = ] [14]\r\n         [[Node: QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/All/_389, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_0, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_1, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_2, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x/_391, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_4, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Equal/Enter/_393)]]\r\n         [[Node: QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert/_384 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_397_QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n`\r\n\r\nI solved this error by change the `batch_size` value in `model_fn` by `attention_mechanism._batch_size`.\r\nThe problem may caused from the redundant data that can't be divided by `batch_size`( if i have 1004 lines of data and my `batch_size` is 10, than 4 lines data will be left). So when I change the `batch_size` value in `model_fn` to `attention_mechanism._batch_size` from specified value, the last iteration's batch size will be 4 and no error anymore. About the error message above, I used batch size of 99, and there will be 14 lines of data left. Then, I want to ask for 2 questions that may related to error:\r\n\r\n1. when I fixed the `batch_size` by specified value smaller than 99, why no error(both training and evaluate)\r\n2. when I fixed the 'batch_size' by specified value bigger than 98, why no error in training period and does have error in evaluation period.\r\n\r\nI think there may be some errors in tensorflow api related to this. Or maybe I was wrong in some part.\r\n\r\nAlso, I hope to see the details of `numpy_input_fn`, such as: when used with `tf.estimator`, How will it treat the redundant data that can't be divided by batch_size"}