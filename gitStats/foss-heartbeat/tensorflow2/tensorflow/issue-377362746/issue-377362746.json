{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23522", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23522/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23522/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23522/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23522", "id": 377362746, "node_id": "MDU6SXNzdWUzNzczNjI3NDY=", "number": 23522, "title": "Conversion from pb to tflite fails", "user": {"login": "Noltibus", "id": 15614838, "node_id": "MDQ6VXNlcjE1NjE0ODM4", "avatar_url": "https://avatars3.githubusercontent.com/u/15614838?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Noltibus", "html_url": "https://github.com/Noltibus", "followers_url": "https://api.github.com/users/Noltibus/followers", "following_url": "https://api.github.com/users/Noltibus/following{/other_user}", "gists_url": "https://api.github.com/users/Noltibus/gists{/gist_id}", "starred_url": "https://api.github.com/users/Noltibus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Noltibus/subscriptions", "organizations_url": "https://api.github.com/users/Noltibus/orgs", "repos_url": "https://api.github.com/users/Noltibus/repos", "events_url": "https://api.github.com/users/Noltibus/events{/privacy}", "received_events_url": "https://api.github.com/users/Noltibus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "gargn", "id": 1900612, "node_id": "MDQ6VXNlcjE5MDA2MTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1900612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gargn", "html_url": "https://github.com/gargn", "followers_url": "https://api.github.com/users/gargn/followers", "following_url": "https://api.github.com/users/gargn/following{/other_user}", "gists_url": "https://api.github.com/users/gargn/gists{/gist_id}", "starred_url": "https://api.github.com/users/gargn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gargn/subscriptions", "organizations_url": "https://api.github.com/users/gargn/orgs", "repos_url": "https://api.github.com/users/gargn/repos", "events_url": "https://api.github.com/users/gargn/events{/privacy}", "received_events_url": "https://api.github.com/users/gargn/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gargn", "id": 1900612, "node_id": "MDQ6VXNlcjE5MDA2MTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1900612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gargn", "html_url": "https://github.com/gargn", "followers_url": "https://api.github.com/users/gargn/followers", "following_url": "https://api.github.com/users/gargn/following{/other_user}", "gists_url": "https://api.github.com/users/gargn/gists{/gist_id}", "starred_url": "https://api.github.com/users/gargn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gargn/subscriptions", "organizations_url": "https://api.github.com/users/gargn/orgs", "repos_url": "https://api.github.com/users/gargn/repos", "events_url": "https://api.github.com/users/gargn/events{/privacy}", "received_events_url": "https://api.github.com/users/gargn/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-05T11:40:21Z", "updated_at": "2018-11-15T10:17:18Z", "closed_at": "2018-11-15T10:17:17Z", "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below): v1.11.0-0-gc19e29306c</li>\n<li>Python version: python 3.5</li>\n<li>Bazel version (if compiling from source): -</li>\n<li>GCC/Compiler version (if compiling from source): -</li>\n<li>CUDA/cuDNN version: only CPU</li>\n<li>GPU model and memory: -</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nI built a Neural network with the following code:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n\nclass AlexNet(object):\n    \"\"\"Implementation of the AlexNet.\"\"\"\n\n    def __init__(self, x, keep_prob, num_classes, skip_layer,\n                 weights_path='DEFAULT'):\n        \"\"\"Create the graph of the AlexNet model.\n        Args:\n            x: Placeholder for the input tensor.\n            keep_prob: Dropout probability.\n            num_classes: Number of classes in the dataset.\n            skip_layer: List of names of the layer, that get trained from\n                scratch\n            weights_path: Complete path to the pretrained weight file, if it\n                isn't in the same folder as this code\n        \"\"\"\n        # Parse input arguments into class variables\n        self.X = x\n        self.NUM_CLASSES = num_classes\n        self.KEEP_PROB = keep_prob\n        self.SKIP_LAYER = skip_layer\n\n        if weights_path == 'DEFAULT':\n            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\n        else:\n            self.WEIGHTS_PATH = weights_path\n\n        # Call the create function to build the computational graph of AlexNet\n        self.create()\n\n    def create(self):\n        \"\"\"Create the network graph.\"\"\"\n        # 1st Layer: Conv (w ReLu) -&gt; Lrn -&gt; Pool\n        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')\n        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\n\n        # 2nd Layer: Conv (w ReLu)  -&gt; Lrn -&gt; Pool with 2 groups\n        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\n        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')\n        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\n\n        # 3rd Layer: Conv (w ReLu)\n        self.conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\n\n        # 4th Layer: Conv (w ReLu) splitted into two groups\n        conv4 = conv(self.conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\n\n        # 5th Layer: Conv (w ReLu) -&gt; Pool splitted into two groups\n        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\n        self.pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n\n        # 6th Layer: Flatten -&gt; FC (w ReLu) -&gt; Dropout\n        flattened = tf.reshape(self.pool5, [-1, 6*6*256])\n        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n        dropout6 = dropout(fc6, self.KEEP_PROB)\n\n        # 7th Layer: FC (w ReLu) -&gt; Dropout\n        fc7 = fc(dropout6, 4096, 4096, name='fc7')\n        dropout7 = dropout(fc7, self.KEEP_PROB)\n\n        # 8th Layer: FC and return unscaled activations\n        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu=False, name='fc8')\n\n    def load_initial_weights(self, session):\n        \"\"\"Load weights from file into network.\n        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n        come as a dict of lists (e.g. weights['conv1'] is a list) and not as\n        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &amp;\n        'biases') we need a special load function\n        \"\"\"\n        # Load the weights into memory\n        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()\n\n        # Loop over all layer names stored in the weights dict\n        for op_name in weights_dict:\n\n            # Check if layer should be trained from scratch\n            if op_name not in self.SKIP_LAYER:\n\n                with tf.variable_scope(op_name, reuse=True):\n\n                    # Assign weights/biases to their corresponding tf variable\n                    for data in weights_dict[op_name]:\n\n                        # Biases\n                        if len(data.shape) == 1:\n                            var = tf.get_variable('biases', trainable=False)\n                            session.run(var.assign(data))\n\n                        # Weights\n                        else:\n                            var = tf.get_variable('weights', trainable=False)\n                            session.run(var.assign(data))\n\n\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n         padding='SAME', groups=1):\n    \"\"\"Create a convolution layer.\n    Adapted from: https://github.com/ethereon/caffe-tensorflow\n    \"\"\"\n    # Get number of input channels\n    input_channels = int(x.get_shape()[-1])\n\n    # Create lambda function for the convolution\n    convolve = lambda i, k: tf.nn.conv2d(i, k,\n                                         strides=[1, stride_y, stride_x, 1],\n                                         padding=padding)\n\n    with tf.variable_scope(name) as scope:\n        # Create tf variables for the weights and biases of the conv layer\n        weights = tf.get_variable('weights', shape=[filter_height,\n                                                    filter_width,\n                                                    input_channels/groups,\n                                                    num_filters])\n        biases = tf.get_variable('biases', shape=[num_filters])\n\n    if groups == 1:\n        conv = convolve(x, weights)\n\n    # In the cases of multiple groups, split inputs &amp; weights and\n    else:\n        # Split input and weights and convolve them separately\n        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\n        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\n                                 value=weights)\n        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\n\n        # Concat the convolved output together again\n        conv = tf.concat(axis=3, values=output_groups)\n\n    # Add biases\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\n\n    # Apply relu function\n    relu = tf.nn.relu(bias, name=scope.name)\n\n    return relu\n\n\ndef fc(x, num_in, num_out, name, relu=True):\n    \"\"\"Create a fully connected layer.\"\"\"\n    with tf.variable_scope(name) as scope:\n\n        # Create tf variables for the weights and biases\n        weights = tf.get_variable('weights', shape=[num_in, num_out],\n                                  trainable=True)\n        biases = tf.get_variable('biases', [num_out], trainable=True)\n\n        # Matrix multiply weights and inputs and add bias\n        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n\n    if relu:\n        # Apply ReLu non linearity\n        relu = tf.nn.relu(act)\n        return relu\n    else:\n        return act\n\n\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\n             padding='SAME'):\n    \"\"\"Create a max pooling layer.\"\"\"\n    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n                          strides=[1, stride_y, stride_x, 1],\n                          padding=padding, name=name)\n\n\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\n    \"\"\"Create a local response normalization layer.\"\"\"\n    return tf.nn.local_response_normalization(x, depth_radius=radius,\n                                              alpha=alpha, beta=beta,\n                                              bias=bias, name=name)\n\n\ndef dropout(x, keep_prob):\n    \"\"\"Create a dropout layer.\"\"\"\n    return tf.nn.dropout(x, keep_prob)\n</code></pre>\n<p>I loaded the weights from a .npy file and saved the resulting pb file. I saved the resulting pb file with this code:<br>\nfrom tensorflow.python.tools import freeze_graph</p>\n<pre><code>SAVED_MODEL_PATH = \"save_path/\"\nMODEL_NAME = \"cut_net_pool5\"\n\ninput_graph = SAVED_MODEL_PATH + MODEL_NAME + '.pb'\n# any other saver to use other than default\ninput_saver = \"\"\n# earlier definition file format text or binary\ninput_binary = True\n# checkpoint file to merge with graph definition\ninput_checkpoint = SAVED_MODEL_PATH + MODEL_NAME + '.ckpt'\n# output nodes inn our model\noutput_node_names = 'output'\nrestore_op_name = 'save/restore_all'\nfilename_tensor_name = 'save/Const:0'\n# output path\noutput_graph = SAVED_MODEL_PATH + '2frozen_' + MODEL_NAME + '.pb'\n# default True\nclear_devices = True\ninitializer_nodes = \"\"\nvariable_names_blacklist = \"\"\n\nfreeze_graph.freeze_graph(\n    input_graph,\n    input_saver,\n    input_binary,\n    input_checkpoint,\n    output_node_names,\n    restore_op_name,\n    filename_tensor_name,\n    output_graph,\n    clear_devices,\n    initializer_nodes,\n    variable_names_blacklist\n)\n</code></pre>\n<p><strong>Describe the expected behavior</strong><br>\nI tried to export the resulting .pb file to tflite via this command in the terminal:</p>\n<p><code>tflite_convert --output_file=test.tflite --graph_def_file=frozen.pb --input_arrays=Placeholder --output_arrays=output </code></p>\n<p><strong>Code to reproduce the issue</strong><br>\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p><strong>Other info / logs</strong><br>\nThe command fails with the following message:</p>\n<p><code>tensorflow/contrib/lite/toco/tooling_util.cc:981] Check failed: name.substr(colon_pos +1).find_first_not_of(\"0123456789\") == string::npos (1 vs. 18446744073709551615)Array name must only have digits after colon\\nAborted (core dumped)\\n</code></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.11.0-0-gc19e29306c\nPython version: python 3.5\nBazel version (if compiling from source): -\nGCC/Compiler version (if compiling from source): -\nCUDA/cuDNN version: only CPU\nGPU model and memory: -\n\nDescribe the current behavior\nI built a Neural network with the following code:\nimport tensorflow as tf\nimport numpy as np\n\n\nclass AlexNet(object):\n    \"\"\"Implementation of the AlexNet.\"\"\"\n\n    def __init__(self, x, keep_prob, num_classes, skip_layer,\n                 weights_path='DEFAULT'):\n        \"\"\"Create the graph of the AlexNet model.\n        Args:\n            x: Placeholder for the input tensor.\n            keep_prob: Dropout probability.\n            num_classes: Number of classes in the dataset.\n            skip_layer: List of names of the layer, that get trained from\n                scratch\n            weights_path: Complete path to the pretrained weight file, if it\n                isn't in the same folder as this code\n        \"\"\"\n        # Parse input arguments into class variables\n        self.X = x\n        self.NUM_CLASSES = num_classes\n        self.KEEP_PROB = keep_prob\n        self.SKIP_LAYER = skip_layer\n\n        if weights_path == 'DEFAULT':\n            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\n        else:\n            self.WEIGHTS_PATH = weights_path\n\n        # Call the create function to build the computational graph of AlexNet\n        self.create()\n\n    def create(self):\n        \"\"\"Create the network graph.\"\"\"\n        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')\n        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\n\n        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\n        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')\n        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\n\n        # 3rd Layer: Conv (w ReLu)\n        self.conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\n\n        # 4th Layer: Conv (w ReLu) splitted into two groups\n        conv4 = conv(self.conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\n\n        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\n        self.pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n\n        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n        flattened = tf.reshape(self.pool5, [-1, 6*6*256])\n        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n        dropout6 = dropout(fc6, self.KEEP_PROB)\n\n        # 7th Layer: FC (w ReLu) -> Dropout\n        fc7 = fc(dropout6, 4096, 4096, name='fc7')\n        dropout7 = dropout(fc7, self.KEEP_PROB)\n\n        # 8th Layer: FC and return unscaled activations\n        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu=False, name='fc8')\n\n    def load_initial_weights(self, session):\n        \"\"\"Load weights from file into network.\n        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n        come as a dict of lists (e.g. weights['conv1'] is a list) and not as\n        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &\n        'biases') we need a special load function\n        \"\"\"\n        # Load the weights into memory\n        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()\n\n        # Loop over all layer names stored in the weights dict\n        for op_name in weights_dict:\n\n            # Check if layer should be trained from scratch\n            if op_name not in self.SKIP_LAYER:\n\n                with tf.variable_scope(op_name, reuse=True):\n\n                    # Assign weights/biases to their corresponding tf variable\n                    for data in weights_dict[op_name]:\n\n                        # Biases\n                        if len(data.shape) == 1:\n                            var = tf.get_variable('biases', trainable=False)\n                            session.run(var.assign(data))\n\n                        # Weights\n                        else:\n                            var = tf.get_variable('weights', trainable=False)\n                            session.run(var.assign(data))\n\n\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n         padding='SAME', groups=1):\n    \"\"\"Create a convolution layer.\n    Adapted from: https://github.com/ethereon/caffe-tensorflow\n    \"\"\"\n    # Get number of input channels\n    input_channels = int(x.get_shape()[-1])\n\n    # Create lambda function for the convolution\n    convolve = lambda i, k: tf.nn.conv2d(i, k,\n                                         strides=[1, stride_y, stride_x, 1],\n                                         padding=padding)\n\n    with tf.variable_scope(name) as scope:\n        # Create tf variables for the weights and biases of the conv layer\n        weights = tf.get_variable('weights', shape=[filter_height,\n                                                    filter_width,\n                                                    input_channels/groups,\n                                                    num_filters])\n        biases = tf.get_variable('biases', shape=[num_filters])\n\n    if groups == 1:\n        conv = convolve(x, weights)\n\n    # In the cases of multiple groups, split inputs & weights and\n    else:\n        # Split input and weights and convolve them separately\n        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\n        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\n                                 value=weights)\n        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\n\n        # Concat the convolved output together again\n        conv = tf.concat(axis=3, values=output_groups)\n\n    # Add biases\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\n\n    # Apply relu function\n    relu = tf.nn.relu(bias, name=scope.name)\n\n    return relu\n\n\ndef fc(x, num_in, num_out, name, relu=True):\n    \"\"\"Create a fully connected layer.\"\"\"\n    with tf.variable_scope(name) as scope:\n\n        # Create tf variables for the weights and biases\n        weights = tf.get_variable('weights', shape=[num_in, num_out],\n                                  trainable=True)\n        biases = tf.get_variable('biases', [num_out], trainable=True)\n\n        # Matrix multiply weights and inputs and add bias\n        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n\n    if relu:\n        # Apply ReLu non linearity\n        relu = tf.nn.relu(act)\n        return relu\n    else:\n        return act\n\n\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\n             padding='SAME'):\n    \"\"\"Create a max pooling layer.\"\"\"\n    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n                          strides=[1, stride_y, stride_x, 1],\n                          padding=padding, name=name)\n\n\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\n    \"\"\"Create a local response normalization layer.\"\"\"\n    return tf.nn.local_response_normalization(x, depth_radius=radius,\n                                              alpha=alpha, beta=beta,\n                                              bias=bias, name=name)\n\n\ndef dropout(x, keep_prob):\n    \"\"\"Create a dropout layer.\"\"\"\n    return tf.nn.dropout(x, keep_prob)\n\nI loaded the weights from a .npy file and saved the resulting pb file. I saved the resulting pb file with this code:\nfrom tensorflow.python.tools import freeze_graph\nSAVED_MODEL_PATH = \"save_path/\"\nMODEL_NAME = \"cut_net_pool5\"\n\ninput_graph = SAVED_MODEL_PATH + MODEL_NAME + '.pb'\n# any other saver to use other than default\ninput_saver = \"\"\n# earlier definition file format text or binary\ninput_binary = True\n# checkpoint file to merge with graph definition\ninput_checkpoint = SAVED_MODEL_PATH + MODEL_NAME + '.ckpt'\n# output nodes inn our model\noutput_node_names = 'output'\nrestore_op_name = 'save/restore_all'\nfilename_tensor_name = 'save/Const:0'\n# output path\noutput_graph = SAVED_MODEL_PATH + '2frozen_' + MODEL_NAME + '.pb'\n# default True\nclear_devices = True\ninitializer_nodes = \"\"\nvariable_names_blacklist = \"\"\n\nfreeze_graph.freeze_graph(\n    input_graph,\n    input_saver,\n    input_binary,\n    input_checkpoint,\n    output_node_names,\n    restore_op_name,\n    filename_tensor_name,\n    output_graph,\n    clear_devices,\n    initializer_nodes,\n    variable_names_blacklist\n)\n\nDescribe the expected behavior\nI tried to export the resulting .pb file to tflite via this command in the terminal:\ntflite_convert --output_file=test.tflite --graph_def_file=frozen.pb --input_arrays=Placeholder --output_arrays=output \nCode to reproduce the issue\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\nOther info / logs\nThe command fails with the following message:\ntensorflow/contrib/lite/toco/tooling_util.cc:981] Check failed: name.substr(colon_pos +1).find_first_not_of(\"0123456789\") == string::npos (1 vs. 18446744073709551615)Array name must only have digits after colon\\nAborted (core dumped)\\n", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c\r\n- Python version: python 3.5\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: only CPU\r\n- GPU model and memory: - \r\n\r\n**Describe the current behavior**\r\nI built a Neural network with the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass AlexNet(object):\r\n    \"\"\"Implementation of the AlexNet.\"\"\"\r\n\r\n    def __init__(self, x, keep_prob, num_classes, skip_layer,\r\n                 weights_path='DEFAULT'):\r\n        \"\"\"Create the graph of the AlexNet model.\r\n        Args:\r\n            x: Placeholder for the input tensor.\r\n            keep_prob: Dropout probability.\r\n            num_classes: Number of classes in the dataset.\r\n            skip_layer: List of names of the layer, that get trained from\r\n                scratch\r\n            weights_path: Complete path to the pretrained weight file, if it\r\n                isn't in the same folder as this code\r\n        \"\"\"\r\n        # Parse input arguments into class variables\r\n        self.X = x\r\n        self.NUM_CLASSES = num_classes\r\n        self.KEEP_PROB = keep_prob\r\n        self.SKIP_LAYER = skip_layer\r\n\r\n        if weights_path == 'DEFAULT':\r\n            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\r\n        else:\r\n            self.WEIGHTS_PATH = weights_path\r\n\r\n        # Call the create function to build the computational graph of AlexNet\r\n        self.create()\r\n\r\n    def create(self):\r\n        \"\"\"Create the network graph.\"\"\"\r\n        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\r\n        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\r\n        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')\r\n        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\r\n\r\n        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\r\n        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\r\n        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')\r\n        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\r\n\r\n        # 3rd Layer: Conv (w ReLu)\r\n        self.conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\r\n\r\n        # 4th Layer: Conv (w ReLu) splitted into two groups\r\n        conv4 = conv(self.conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\r\n\r\n        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\r\n        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\r\n        self.pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\r\n\r\n        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\r\n        flattened = tf.reshape(self.pool5, [-1, 6*6*256])\r\n        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\r\n        dropout6 = dropout(fc6, self.KEEP_PROB)\r\n\r\n        # 7th Layer: FC (w ReLu) -> Dropout\r\n        fc7 = fc(dropout6, 4096, 4096, name='fc7')\r\n        dropout7 = dropout(fc7, self.KEEP_PROB)\r\n\r\n        # 8th Layer: FC and return unscaled activations\r\n        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu=False, name='fc8')\r\n\r\n    def load_initial_weights(self, session):\r\n        \"\"\"Load weights from file into network.\r\n        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\r\n        come as a dict of lists (e.g. weights['conv1'] is a list) and not as\r\n        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &\r\n        'biases') we need a special load function\r\n        \"\"\"\r\n        # Load the weights into memory\r\n        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()\r\n\r\n        # Loop over all layer names stored in the weights dict\r\n        for op_name in weights_dict:\r\n\r\n            # Check if layer should be trained from scratch\r\n            if op_name not in self.SKIP_LAYER:\r\n\r\n                with tf.variable_scope(op_name, reuse=True):\r\n\r\n                    # Assign weights/biases to their corresponding tf variable\r\n                    for data in weights_dict[op_name]:\r\n\r\n                        # Biases\r\n                        if len(data.shape) == 1:\r\n                            var = tf.get_variable('biases', trainable=False)\r\n                            session.run(var.assign(data))\r\n\r\n                        # Weights\r\n                        else:\r\n                            var = tf.get_variable('weights', trainable=False)\r\n                            session.run(var.assign(data))\r\n\r\n\r\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\r\n         padding='SAME', groups=1):\r\n    \"\"\"Create a convolution layer.\r\n    Adapted from: https://github.com/ethereon/caffe-tensorflow\r\n    \"\"\"\r\n    # Get number of input channels\r\n    input_channels = int(x.get_shape()[-1])\r\n\r\n    # Create lambda function for the convolution\r\n    convolve = lambda i, k: tf.nn.conv2d(i, k,\r\n                                         strides=[1, stride_y, stride_x, 1],\r\n                                         padding=padding)\r\n\r\n    with tf.variable_scope(name) as scope:\r\n        # Create tf variables for the weights and biases of the conv layer\r\n        weights = tf.get_variable('weights', shape=[filter_height,\r\n                                                    filter_width,\r\n                                                    input_channels/groups,\r\n                                                    num_filters])\r\n        biases = tf.get_variable('biases', shape=[num_filters])\r\n\r\n    if groups == 1:\r\n        conv = convolve(x, weights)\r\n\r\n    # In the cases of multiple groups, split inputs & weights and\r\n    else:\r\n        # Split input and weights and convolve them separately\r\n        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\r\n        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\r\n                                 value=weights)\r\n        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\r\n\r\n        # Concat the convolved output together again\r\n        conv = tf.concat(axis=3, values=output_groups)\r\n\r\n    # Add biases\r\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\r\n\r\n    # Apply relu function\r\n    relu = tf.nn.relu(bias, name=scope.name)\r\n\r\n    return relu\r\n\r\n\r\ndef fc(x, num_in, num_out, name, relu=True):\r\n    \"\"\"Create a fully connected layer.\"\"\"\r\n    with tf.variable_scope(name) as scope:\r\n\r\n        # Create tf variables for the weights and biases\r\n        weights = tf.get_variable('weights', shape=[num_in, num_out],\r\n                                  trainable=True)\r\n        biases = tf.get_variable('biases', [num_out], trainable=True)\r\n\r\n        # Matrix multiply weights and inputs and add bias\r\n        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\r\n\r\n    if relu:\r\n        # Apply ReLu non linearity\r\n        relu = tf.nn.relu(act)\r\n        return relu\r\n    else:\r\n        return act\r\n\r\n\r\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\r\n             padding='SAME'):\r\n    \"\"\"Create a max pooling layer.\"\"\"\r\n    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\r\n                          strides=[1, stride_y, stride_x, 1],\r\n                          padding=padding, name=name)\r\n\r\n\r\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\r\n    \"\"\"Create a local response normalization layer.\"\"\"\r\n    return tf.nn.local_response_normalization(x, depth_radius=radius,\r\n                                              alpha=alpha, beta=beta,\r\n                                              bias=bias, name=name)\r\n\r\n\r\ndef dropout(x, keep_prob):\r\n    \"\"\"Create a dropout layer.\"\"\"\r\n    return tf.nn.dropout(x, keep_prob)\r\n```\r\nI loaded the weights from a .npy file and saved the resulting pb file. I saved the resulting pb file with this code:\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\n```\r\nSAVED_MODEL_PATH = \"save_path/\"\r\nMODEL_NAME = \"cut_net_pool5\"\r\n\r\ninput_graph = SAVED_MODEL_PATH + MODEL_NAME + '.pb'\r\n# any other saver to use other than default\r\ninput_saver = \"\"\r\n# earlier definition file format text or binary\r\ninput_binary = True\r\n# checkpoint file to merge with graph definition\r\ninput_checkpoint = SAVED_MODEL_PATH + MODEL_NAME + '.ckpt'\r\n# output nodes inn our model\r\noutput_node_names = 'output'\r\nrestore_op_name = 'save/restore_all'\r\nfilename_tensor_name = 'save/Const:0'\r\n# output path\r\noutput_graph = SAVED_MODEL_PATH + '2frozen_' + MODEL_NAME + '.pb'\r\n# default True\r\nclear_devices = True\r\ninitializer_nodes = \"\"\r\nvariable_names_blacklist = \"\"\r\n\r\nfreeze_graph.freeze_graph(\r\n    input_graph,\r\n    input_saver,\r\n    input_binary,\r\n    input_checkpoint,\r\n    output_node_names,\r\n    restore_op_name,\r\n    filename_tensor_name,\r\n    output_graph,\r\n    clear_devices,\r\n    initializer_nodes,\r\n    variable_names_blacklist\r\n)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI tried to export the resulting .pb file to tflite via this command in the terminal:\r\n\r\n`tflite_convert --output_file=test.tflite --graph_def_file=frozen.pb --input_arrays=Placeholder --output_arrays=output `\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nThe command fails with the following message:\r\n\r\n`tensorflow/contrib/lite/toco/tooling_util.cc:981] Check failed: name.substr(colon_pos +1).find_first_not_of(\"0123456789\") == string::npos (1 vs. 18446744073709551615)Array name must only have digits after colon\\nAborted (core dumped)\\n`\r\n"}