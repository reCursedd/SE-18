{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/172740099", "pull_request_review_id": 101806360, "id": 172740099, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3Mjc0MDA5OQ==", "diff_hunk": "@@ -1330,6 +1330,151 @@ def assert_shape_match(inp, out):\n     return (res_outputs, new_state)\n \n \n+class LayerNormBasicGRUCell(rnn_cell_impl._LayerRNNCell):\n+  \"\"\"GRU unit with layer normalization.\n+\n+    This class adds layer normalization to a\n+    basic GRU unit. Layer normalization implementation is based on:\n+\n+      https://arxiv.org/abs/1607.06450.\n+\n+    \"Layer Normalization\"\n+    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+    and is applied before the internal nonlinearities.\n+    \"\"\"\n+\n+  def __init__(self, num_units,\n+               activation=math_ops.tanh,\n+               layer_norm=True,\n+               norm_gain=1.0,\n+               norm_shift=0.0,\n+               reuse=None):\n+    \"\"\"Initializes the cell.\n+\n+        Args:\n+          num_units: int, The number of units in the GRU cell.\n+          activation: Activation function of the inner states.\n+          layer_norm: If `True`, layer normalization will be applied.\n+          norm_gain: float, The layer normalization gain initial value. If\n+            `layer_norm` has been set to `False`, this argument will be ignored.\n+          norm_shift: float, The layer normalization shift initial value. If\n+            `layer_norm` has been set to `False`, this argument will be ignored.\n+        \"\"\"\n+\n+    super(LayerNormBasicGRUCell, self).__init__(_reuse=reuse)\n+\n+    self._num_units = num_units\n+    self._activation = activation\n+    self._layer_norm = layer_norm\n+    self._g = norm_gain\n+    self._b = norm_shift\n+    self._reuse = reuse\n+\n+  @property\n+  def state_size(self):\n+    return self._num_units\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  def _linear(self, args, kernel, bias):\n+    out = math_ops.matmul(args, kernel)\n+    if not self._layer_norm:\n+      out = nn_ops.bias_add(out, bias)\n+    return out\n+\n+  def _norm(self, inp, scope):\n+    return layers.layer_norm(inp, reuse=True, scope=scope)", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 60, "commit_id": "249443a3e718ca5367aac1a2f34b1ba42a79e195", "original_commit_id": "134510d9d80fbb7bb62b24f310c05573bd8fc642", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "add some extra documentation describing that the gamma and beta variables used by this layer_norm come from the ones you built in `build`.  is there a way to pass them in explicitly instead?  that would be cleaner.\r\n\r\nbtw, are you sure you set the `begin_norm_axis` and `begin_params_axis` correctly to [layer_norm](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L2058)?", "created_at": "2018-03-07T04:42:32Z", "updated_at": "2018-09-10T15:16:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14578#discussion_r172740099", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14578", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/172740099"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14578#discussion_r172740099"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14578"}}, "body_html": "<p>add some extra documentation describing that the gamma and beta variables used by this layer_norm come from the ones you built in <code>build</code>.  is there a way to pass them in explicitly instead?  that would be cleaner.</p>\n<p>btw, are you sure you set the <code>begin_norm_axis</code> and <code>begin_params_axis</code> correctly to <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L2058\">layer_norm</a>?</p>", "body_text": "add some extra documentation describing that the gamma and beta variables used by this layer_norm come from the ones you built in build.  is there a way to pass them in explicitly instead?  that would be cleaner.\nbtw, are you sure you set the begin_norm_axis and begin_params_axis correctly to layer_norm?"}