{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/353120108", "html_url": "https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-353120108", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11275", "id": 353120108, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzEyMDEwOA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-20T16:57:19Z", "updated_at": "2017-12-20T16:57:19Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">It is a goal; but because XLA will JIT a graph and perform memory layout\n*before* any execution, you will have to provide a \"maximum max_time\" for\nthe memory layout (even if the loop will not calculate up to this maximum\nsize).  When working with dynamic_rnn, for example, this will mean you will\nhave to pad your batched sequence inputs to some specific max_time size(s),\nlike 10, 50, 100, ...; to ensure XLA doesn't try to JIT compile for every\npossible max(sequence_length) across your minibatches.  The tensorflow NMT\ntutorial shows how to bucket batches by sequence lengths, but does not\ncurrently pad the input max_times to the bucket boundaries.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Dec 20, 2017 at 8:49 AM, leod ***@***.***&gt; wrote:\n As a follow-up question, is it within XLA's goals to work with seq2seq\n models at some point? Will it be possible to use XLA on graphs that have\n variable length inputs and outputs? Right now, it seems that tfcompile\n does not like fetches with dimension -1.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"240455025\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11275\" href=\"https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-353117651\">#11275 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim2kBtt6Pd9UOPkYyfUgpZtuBgwDeks5tCTqkgaJpZM4ONgEv\">https://github.com/notifications/unsubscribe-auth/ABtim2kBtt6Pd9UOPkYyfUgpZtuBgwDeks5tCTqkgaJpZM4ONgEv</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "It is a goal; but because XLA will JIT a graph and perform memory layout\n*before* any execution, you will have to provide a \"maximum max_time\" for\nthe memory layout (even if the loop will not calculate up to this maximum\nsize).  When working with dynamic_rnn, for example, this will mean you will\nhave to pad your batched sequence inputs to some specific max_time size(s),\nlike 10, 50, 100, ...; to ensure XLA doesn't try to JIT compile for every\npossible max(sequence_length) across your minibatches.  The tensorflow NMT\ntutorial shows how to bucket batches by sequence lengths, but does not\ncurrently pad the input max_times to the bucket boundaries.\n\u2026\nOn Wed, Dec 20, 2017 at 8:49 AM, leod ***@***.***> wrote:\n As a follow-up question, is it within XLA's goals to work with seq2seq\n models at some point? Will it be possible to use XLA on graphs that have\n variable length inputs and outputs? Right now, it seems that tfcompile\n does not like fetches with dimension -1.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#11275 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim2kBtt6Pd9UOPkYyfUgpZtuBgwDeks5tCTqkgaJpZM4ONgEv>\n .", "body": "It is a goal; but because XLA will JIT a graph and perform memory layout\n*before* any execution, you will have to provide a \"maximum max_time\" for\nthe memory layout (even if the loop will not calculate up to this maximum\nsize).  When working with dynamic_rnn, for example, this will mean you will\nhave to pad your batched sequence inputs to some specific max_time size(s),\nlike 10, 50, 100, ...; to ensure XLA doesn't try to JIT compile for every\npossible max(sequence_length) across your minibatches.  The tensorflow NMT\ntutorial shows how to bucket batches by sequence lengths, but does not\ncurrently pad the input max_times to the bucket boundaries.\n\n\nOn Wed, Dec 20, 2017 at 8:49 AM, leod <notifications@github.com> wrote:\n\n> As a follow-up question, is it within XLA's goals to work with seq2seq\n> models at some point? Will it be possible to use XLA on graphs that have\n> variable length inputs and outputs? Right now, it seems that tfcompile\n> does not like fetches with dimension -1.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-353117651>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2kBtt6Pd9UOPkYyfUgpZtuBgwDeks5tCTqkgaJpZM4ONgEv>\n> .\n>\n"}