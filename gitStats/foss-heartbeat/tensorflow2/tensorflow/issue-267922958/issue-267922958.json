{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13939", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13939/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13939/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13939/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13939", "id": 267922958, "node_id": "MDU6SXNzdWUyNjc5MjI5NTg=", "number": 13939, "title": "Quantization make graph slower during inference.", "user": {"login": "YannickBlais", "id": 13531697, "node_id": "MDQ6VXNlcjEzNTMxNjk3", "avatar_url": "https://avatars3.githubusercontent.com/u/13531697?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YannickBlais", "html_url": "https://github.com/YannickBlais", "followers_url": "https://api.github.com/users/YannickBlais/followers", "following_url": "https://api.github.com/users/YannickBlais/following{/other_user}", "gists_url": "https://api.github.com/users/YannickBlais/gists{/gist_id}", "starred_url": "https://api.github.com/users/YannickBlais/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YannickBlais/subscriptions", "organizations_url": "https://api.github.com/users/YannickBlais/orgs", "repos_url": "https://api.github.com/users/YannickBlais/repos", "events_url": "https://api.github.com/users/YannickBlais/events{/privacy}", "received_events_url": "https://api.github.com/users/YannickBlais/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2017-10-24T07:19:04Z", "updated_at": "2018-01-31T19:25:00Z", "closed_at": "2018-01-31T19:25:00Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from</strong>: using TF source code (GPU build), can provide docker to reproduce environment conditions if necessary</li>\n<li><strong>TensorFlow version</strong>: using r1.3 branch, version 1.3.1</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.6.1</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/6.0</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080 Ti, 11170 MB</li>\n<li><strong>Exact command to reproduce</strong>:<br>\n/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --in_graph=/quantization/VGG16/frozen_model.pb   --outputs=\"Validation_segmentation/Validation/decoder/Softmax\" --out_graph=/quantization/VGG16/optimized_model.pb   --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"384,1248,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes'</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hi, I compressed a graph using transform_graph tool but the resulting graph is actually slower during inference. I am compressing a graph similar to the one presented in this article: <a href=\"https://arxiv.org/pdf/1612.07695.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1612.07695.pdf</a>, which has VGG16 as an encoder in input and a classification decoder with a Softmax in output. Inference uses same python script for both graph (original and quantized) and make an average of 100 inferences. Original graph takes ~0.1s for inference, quantized graph takes 70s! If I perform quantization without quantize_nodes, inference takes ~0.3s.</p>\n<p>I understand that this quantization is still in a work in progress and maybe was more aimed at improving inference on mobile devices, but I'm surprised that it is actually so much slower, so that's why I'm logging it as a bug here. (I posted this on stackoverflow but didn't get any answer...)</p>\n<p>The graph takes ~500Mb, let me know if I should attach it to this ticket (or include an external link?)</p>\n<h3>Source code / logs</h3>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/1409825/quantization_logs.txt\">quantization_logs.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1409824/tf_env.txt\">tf_env.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1409864/inference.py.txt\">inference.py.txt</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from: using TF source code (GPU build), can provide docker to reproduce environment conditions if necessary\nTensorFlow version: using r1.3 branch, version 1.3.1\nPython version: 2.7\nBazel version (if compiling from source): 0.6.1\nCUDA/cuDNN version: 8.0/6.0\nGPU model and memory: GeForce GTX 1080 Ti, 11170 MB\nExact command to reproduce:\n/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --in_graph=/quantization/VGG16/frozen_model.pb   --outputs=\"Validation_segmentation/Validation/decoder/Softmax\" --out_graph=/quantization/VGG16/optimized_model.pb   --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"384,1248,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes'\n\nDescribe the problem\nHi, I compressed a graph using transform_graph tool but the resulting graph is actually slower during inference. I am compressing a graph similar to the one presented in this article: https://arxiv.org/pdf/1612.07695.pdf, which has VGG16 as an encoder in input and a classification decoder with a Softmax in output. Inference uses same python script for both graph (original and quantized) and make an average of 100 inferences. Original graph takes ~0.1s for inference, quantized graph takes 70s! If I perform quantization without quantize_nodes, inference takes ~0.3s.\nI understand that this quantization is still in a work in progress and maybe was more aimed at improving inference on mobile devices, but I'm surprised that it is actually so much slower, so that's why I'm logging it as a bug here. (I posted this on stackoverflow but didn't get any answer...)\nThe graph takes ~500Mb, let me know if I should attach it to this ticket (or include an external link?)\nSource code / logs\nquantization_logs.txt\ntf_env.txt\ninference.py.txt", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: using TF source code (GPU build), can provide docker to reproduce environment conditions if necessary\r\n- **TensorFlow version**: using r1.3 branch, version 1.3.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: GeForce GTX 1080 Ti, 11170 MB\r\n- **Exact command to reproduce**: \r\n/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --in_graph=/quantization/VGG16/frozen_model.pb   --outputs=\"Validation_segmentation/Validation/decoder/Softmax\" --out_graph=/quantization/VGG16/optimized_model.pb   --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"384,1248,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes'\r\n\r\n\r\n### Describe the problem\r\nHi, I compressed a graph using transform_graph tool but the resulting graph is actually slower during inference. I am compressing a graph similar to the one presented in this article: https://arxiv.org/pdf/1612.07695.pdf, which has VGG16 as an encoder in input and a classification decoder with a Softmax in output. Inference uses same python script for both graph (original and quantized) and make an average of 100 inferences. Original graph takes ~0.1s for inference, quantized graph takes 70s! If I perform quantization without quantize_nodes, inference takes ~0.3s.\r\n\r\nI understand that this quantization is still in a work in progress and maybe was more aimed at improving inference on mobile devices, but I'm surprised that it is actually so much slower, so that's why I'm logging it as a bug here. (I posted this on stackoverflow but didn't get any answer...)\r\n\r\nThe graph takes ~500Mb, let me know if I should attach it to this ticket (or include an external link?)\r\n\r\n### Source code / logs\r\n[quantization_logs.txt](https://github.com/tensorflow/tensorflow/files/1409825/quantization_logs.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1409824/tf_env.txt)\r\n[inference.py.txt](https://github.com/tensorflow/tensorflow/files/1409864/inference.py.txt)\r\n\r\n"}