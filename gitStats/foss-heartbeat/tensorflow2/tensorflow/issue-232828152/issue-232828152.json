{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10369", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10369/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10369/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10369/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10369", "id": 232828152, "node_id": "MDU6SXNzdWUyMzI4MjgxNTI=", "number": 10369, "title": "Deadlock in MapDataset", "user": {"login": "snnn", "id": 856316, "node_id": "MDQ6VXNlcjg1NjMxNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/856316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snnn", "html_url": "https://github.com/snnn", "followers_url": "https://api.github.com/users/snnn/followers", "following_url": "https://api.github.com/users/snnn/following{/other_user}", "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snnn/subscriptions", "organizations_url": "https://api.github.com/users/snnn/orgs", "repos_url": "https://api.github.com/users/snnn/repos", "events_url": "https://api.github.com/users/snnn/events{/privacy}", "received_events_url": "https://api.github.com/users/snnn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-06-01T10:03:57Z", "updated_at": "2017-06-03T07:46:58Z", "closed_at": "2017-06-03T07:46:58Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nno</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 14.04</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>: source</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/95d90ab2e0994127ffc42b80e16a3f532895cf6d/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/95d90ab2e0994127ffc42b80e16a3f532895cf6d\"><tt>95d90ab</tt></a></p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\n0.5.0</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nNone</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nNone</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\npython map_dataset_op_test.py</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The process hangs forever</p>\n<h3>Source code / logs</h3>\n<p>Below is from map_dataset_op_test.py:</p>\n<pre><code>\"\"\"Tests for the experimental input pipeline ops.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.contrib.data.python.ops import dataset_ops\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.platform import test\n\n\nclass MapDatasetTest(test.TestCase):\n\n  def _buildParallelMapDataset(self, components, count, num_threads,\n\t\t\t\t\t\t\t   output_buffer_size):\n\tdef _map_fn(x, y, z):\n\t  return math_ops.square(x), math_ops.square(y), math_ops.square(z)\n\treturn (dataset_ops.Dataset.from_tensor_slices(components).map(\n\t\t_map_fn, num_threads=num_threads, output_buffer_size=output_buffer_size)\n\t\t\t.repeat(count))\n\n  def testParallelMapDataset(self):\n\t\"\"\"Test an dataset that maps a TF function across its input elements.\"\"\"\n\t# The pipeline is TensorSliceDataset -&gt; ParallelMapDataset(square_3) -&gt;\n\t# RepeatDataset(count).\n\tcomponents = [np.arange(7),\n\t\t\t\t  np.array([[1, 2, 3]]) * np.arange(7)[:, np.newaxis],\n\t\t\t\t  np.array(37.0) * np.arange(7)]\n\tcount = array_ops.placeholder(dtypes.int64, shape=[])\n\tnum_threads = array_ops.placeholder(dtypes.int32, shape=[])\n\toutput_buffer_size = array_ops.placeholder(dtypes.int64, shape=[])\n\n\tdataset = self._buildParallelMapDataset(components, count, num_threads,\n\t\t\t\t\t\t\t\t\t\t\toutput_buffer_size)\n\titerator = dataset.make_initializable_iterator()\n\tinit_op = iterator.initializer\n\tget_next = iterator.get_next()\n\n\tself.assertEqual([c.shape[1:] for c in components],\n\t\t\t\t\t [t.shape for t in get_next])\n\n\twith self.test_session() as sess:\n\t  def do_test(num_threads_val, output_buffer_size_val):\n\t\t# Test single-threaded access to the iterator.\n\t\tsess.run(init_op, feed_dict={\n\t\t\tcount: 14,\n\t\t\tnum_threads: num_threads_val,\n\t\t\toutput_buffer_size: output_buffer_size_val})\n\t\tfor _ in range(14):\n\t\t  for i in range(7):\n\t\t\tresult = sess.run(get_next)\n\t\t\tfor component, result_component in zip(components, result):\n\t\t\t  self.assertAllEqual(component[i]**2, result_component)\n\t\twith self.assertRaises(errors.OutOfRangeError):\n\t\t  sess.run(get_next)\n\n\t\t# Test multi-threaded access to the same iterator.\n\t\tsess.run(init_op, feed_dict={\n\t\t\tcount: 18,\n\t\t\tnum_threads: num_threads_val,\n\t\t\toutput_buffer_size: output_buffer_size_val})\n\t\tresults = []\n\t\tdef iterator_thread():\n\t\t  while True:\n\t\t\ttry:\n\t\t\t  results.append(sess.run(get_next))\n\t\t\texcept errors.OutOfRangeError:\n\t\t\t  return\n\t\tthreads = [self.checkedThread(target=iterator_thread) for _ in range(8)]\n\t\tfor t in threads:\n\t\t  t.start()\n\t\tfor t in threads:\n\t\t  t.join()\n\n\t\t# `results` will contain the same elements components**2\n\t\t# repeated 18 times, but in a non-deterministic order. Sort the\n\t\t# results, and assert that each element of components**2 is\n\t\t# produced 18 times.\n\t\tresults.sort(key=lambda x: x[0])\n\t\tfor i in range(7):\n\t\t  for j in range(18):\n\t\t\tfor component, result_component in zip(components,\n\t\t\t\t\t\t\t\t\t\t\t\t   results[i * 18 + j]):\n\t\t\t  self.assertAllEqual(component[i]**2, result_component)\n\n\t  for num_threads_val, output_buffer_size_val in [\n\t\t  (1, 1), (1, 2), (2, 2), (2, 4), (8, 8), (8, 16)]:\n\t\tdo_test(num_threads_val, output_buffer_size_val)\n\nif __name__ == \"__main__\":\n  test.main()\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/1044353/s.txt\">bt.txt</a></p>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nno\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 14.04\n\n\nTensorFlow installed from (source or binary): source\n\n\nTensorFlow version (use command below):\n95d90ab\n\n\nBazel version (if compiling from source):\n0.5.0\n\n\nCUDA/cuDNN version:\nNone\n\n\nGPU model and memory:\nNone\n\n\nExact command to reproduce:\npython map_dataset_op_test.py\n\n\nDescribe the problem\nThe process hangs forever\nSource code / logs\nBelow is from map_dataset_op_test.py:\n\"\"\"Tests for the experimental input pipeline ops.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.contrib.data.python.ops import dataset_ops\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.platform import test\n\n\nclass MapDatasetTest(test.TestCase):\n\n  def _buildParallelMapDataset(self, components, count, num_threads,\n\t\t\t\t\t\t\t   output_buffer_size):\n\tdef _map_fn(x, y, z):\n\t  return math_ops.square(x), math_ops.square(y), math_ops.square(z)\n\treturn (dataset_ops.Dataset.from_tensor_slices(components).map(\n\t\t_map_fn, num_threads=num_threads, output_buffer_size=output_buffer_size)\n\t\t\t.repeat(count))\n\n  def testParallelMapDataset(self):\n\t\"\"\"Test an dataset that maps a TF function across its input elements.\"\"\"\n\t# The pipeline is TensorSliceDataset -> ParallelMapDataset(square_3) ->\n\t# RepeatDataset(count).\n\tcomponents = [np.arange(7),\n\t\t\t\t  np.array([[1, 2, 3]]) * np.arange(7)[:, np.newaxis],\n\t\t\t\t  np.array(37.0) * np.arange(7)]\n\tcount = array_ops.placeholder(dtypes.int64, shape=[])\n\tnum_threads = array_ops.placeholder(dtypes.int32, shape=[])\n\toutput_buffer_size = array_ops.placeholder(dtypes.int64, shape=[])\n\n\tdataset = self._buildParallelMapDataset(components, count, num_threads,\n\t\t\t\t\t\t\t\t\t\t\toutput_buffer_size)\n\titerator = dataset.make_initializable_iterator()\n\tinit_op = iterator.initializer\n\tget_next = iterator.get_next()\n\n\tself.assertEqual([c.shape[1:] for c in components],\n\t\t\t\t\t [t.shape for t in get_next])\n\n\twith self.test_session() as sess:\n\t  def do_test(num_threads_val, output_buffer_size_val):\n\t\t# Test single-threaded access to the iterator.\n\t\tsess.run(init_op, feed_dict={\n\t\t\tcount: 14,\n\t\t\tnum_threads: num_threads_val,\n\t\t\toutput_buffer_size: output_buffer_size_val})\n\t\tfor _ in range(14):\n\t\t  for i in range(7):\n\t\t\tresult = sess.run(get_next)\n\t\t\tfor component, result_component in zip(components, result):\n\t\t\t  self.assertAllEqual(component[i]**2, result_component)\n\t\twith self.assertRaises(errors.OutOfRangeError):\n\t\t  sess.run(get_next)\n\n\t\t# Test multi-threaded access to the same iterator.\n\t\tsess.run(init_op, feed_dict={\n\t\t\tcount: 18,\n\t\t\tnum_threads: num_threads_val,\n\t\t\toutput_buffer_size: output_buffer_size_val})\n\t\tresults = []\n\t\tdef iterator_thread():\n\t\t  while True:\n\t\t\ttry:\n\t\t\t  results.append(sess.run(get_next))\n\t\t\texcept errors.OutOfRangeError:\n\t\t\t  return\n\t\tthreads = [self.checkedThread(target=iterator_thread) for _ in range(8)]\n\t\tfor t in threads:\n\t\t  t.start()\n\t\tfor t in threads:\n\t\t  t.join()\n\n\t\t# `results` will contain the same elements components**2\n\t\t# repeated 18 times, but in a non-deterministic order. Sort the\n\t\t# results, and assert that each element of components**2 is\n\t\t# produced 18 times.\n\t\tresults.sort(key=lambda x: x[0])\n\t\tfor i in range(7):\n\t\t  for j in range(18):\n\t\t\tfor component, result_component in zip(components,\n\t\t\t\t\t\t\t\t\t\t\t\t   results[i * 18 + j]):\n\t\t\t  self.assertAllEqual(component[i]**2, result_component)\n\n\t  for num_threads_val, output_buffer_size_val in [\n\t\t  (1, 1), (1, 2), (2, 2), (2, 4), (8, 8), (8, 16)]:\n\t\tdo_test(num_threads_val, output_buffer_size_val)\n\nif __name__ == \"__main__\":\n  test.main()\n\nbt.txt", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n\r\n- **TensorFlow installed from (source or binary)**: source\r\n\r\n- **TensorFlow version (use command below)**:\r\n95d90ab2e0994127ffc42b80e16a3f532895cf6d\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.5.0\r\n\r\n- **CUDA/cuDNN version**:\r\nNone\r\n\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\npython map_dataset_op_test.py\r\n\r\n\r\n### Describe the problem\r\nThe process hangs forever\r\n\r\n### Source code / logs\r\nBelow is from map_dataset_op_test.py:\r\n\r\n\t\"\"\"Tests for the experimental input pipeline ops.\"\"\"\r\n\tfrom __future__ import absolute_import\r\n\tfrom __future__ import division\r\n\tfrom __future__ import print_function\r\n\r\n\timport numpy as np\r\n\r\n\tfrom tensorflow.contrib.data.python.ops import dataset_ops\r\n\tfrom tensorflow.python.framework import constant_op\r\n\tfrom tensorflow.python.framework import dtypes\r\n\tfrom tensorflow.python.framework import errors\r\n\tfrom tensorflow.python.ops import array_ops\r\n\tfrom tensorflow.python.ops import data_flow_ops\r\n\tfrom tensorflow.python.ops import lookup_ops\r\n\tfrom tensorflow.python.ops import math_ops\r\n\tfrom tensorflow.python.ops import random_ops\r\n\tfrom tensorflow.python.ops import string_ops\r\n\tfrom tensorflow.python.ops import variable_scope\r\n\tfrom tensorflow.python.platform import test\r\n\r\n\r\n\tclass MapDatasetTest(test.TestCase):\r\n\r\n\t  def _buildParallelMapDataset(self, components, count, num_threads,\r\n\t\t\t\t\t\t\t\t   output_buffer_size):\r\n\t\tdef _map_fn(x, y, z):\r\n\t\t  return math_ops.square(x), math_ops.square(y), math_ops.square(z)\r\n\t\treturn (dataset_ops.Dataset.from_tensor_slices(components).map(\r\n\t\t\t_map_fn, num_threads=num_threads, output_buffer_size=output_buffer_size)\r\n\t\t\t\t.repeat(count))\r\n\r\n\t  def testParallelMapDataset(self):\r\n\t\t\"\"\"Test an dataset that maps a TF function across its input elements.\"\"\"\r\n\t\t# The pipeline is TensorSliceDataset -> ParallelMapDataset(square_3) ->\r\n\t\t# RepeatDataset(count).\r\n\t\tcomponents = [np.arange(7),\r\n\t\t\t\t\t  np.array([[1, 2, 3]]) * np.arange(7)[:, np.newaxis],\r\n\t\t\t\t\t  np.array(37.0) * np.arange(7)]\r\n\t\tcount = array_ops.placeholder(dtypes.int64, shape=[])\r\n\t\tnum_threads = array_ops.placeholder(dtypes.int32, shape=[])\r\n\t\toutput_buffer_size = array_ops.placeholder(dtypes.int64, shape=[])\r\n\r\n\t\tdataset = self._buildParallelMapDataset(components, count, num_threads,\r\n\t\t\t\t\t\t\t\t\t\t\t\toutput_buffer_size)\r\n\t\titerator = dataset.make_initializable_iterator()\r\n\t\tinit_op = iterator.initializer\r\n\t\tget_next = iterator.get_next()\r\n\r\n\t\tself.assertEqual([c.shape[1:] for c in components],\r\n\t\t\t\t\t\t [t.shape for t in get_next])\r\n\r\n\t\twith self.test_session() as sess:\r\n\t\t  def do_test(num_threads_val, output_buffer_size_val):\r\n\t\t\t# Test single-threaded access to the iterator.\r\n\t\t\tsess.run(init_op, feed_dict={\r\n\t\t\t\tcount: 14,\r\n\t\t\t\tnum_threads: num_threads_val,\r\n\t\t\t\toutput_buffer_size: output_buffer_size_val})\r\n\t\t\tfor _ in range(14):\r\n\t\t\t  for i in range(7):\r\n\t\t\t\tresult = sess.run(get_next)\r\n\t\t\t\tfor component, result_component in zip(components, result):\r\n\t\t\t\t  self.assertAllEqual(component[i]**2, result_component)\r\n\t\t\twith self.assertRaises(errors.OutOfRangeError):\r\n\t\t\t  sess.run(get_next)\r\n\r\n\t\t\t# Test multi-threaded access to the same iterator.\r\n\t\t\tsess.run(init_op, feed_dict={\r\n\t\t\t\tcount: 18,\r\n\t\t\t\tnum_threads: num_threads_val,\r\n\t\t\t\toutput_buffer_size: output_buffer_size_val})\r\n\t\t\tresults = []\r\n\t\t\tdef iterator_thread():\r\n\t\t\t  while True:\r\n\t\t\t\ttry:\r\n\t\t\t\t  results.append(sess.run(get_next))\r\n\t\t\t\texcept errors.OutOfRangeError:\r\n\t\t\t\t  return\r\n\t\t\tthreads = [self.checkedThread(target=iterator_thread) for _ in range(8)]\r\n\t\t\tfor t in threads:\r\n\t\t\t  t.start()\r\n\t\t\tfor t in threads:\r\n\t\t\t  t.join()\r\n\r\n\t\t\t# `results` will contain the same elements components**2\r\n\t\t\t# repeated 18 times, but in a non-deterministic order. Sort the\r\n\t\t\t# results, and assert that each element of components**2 is\r\n\t\t\t# produced 18 times.\r\n\t\t\tresults.sort(key=lambda x: x[0])\r\n\t\t\tfor i in range(7):\r\n\t\t\t  for j in range(18):\r\n\t\t\t\tfor component, result_component in zip(components,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t   results[i * 18 + j]):\r\n\t\t\t\t  self.assertAllEqual(component[i]**2, result_component)\r\n\r\n\t\t  for num_threads_val, output_buffer_size_val in [\r\n\t\t\t  (1, 1), (1, 2), (2, 2), (2, 4), (8, 8), (8, 16)]:\r\n\t\t\tdo_test(num_threads_val, output_buffer_size_val)\r\n\r\n\tif __name__ == \"__main__\":\r\n\t  test.main()\r\n\r\n[bt.txt](https://github.com/tensorflow/tensorflow/files/1044353/s.txt)\r\n\r\n\t\r\n"}