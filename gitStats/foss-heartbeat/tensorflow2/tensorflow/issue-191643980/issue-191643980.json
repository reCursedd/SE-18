{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5850", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5850/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5850/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5850/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5850", "id": 191643980, "node_id": "MDU6SXNzdWUxOTE2NDM5ODA=", "number": 5850, "title": "adding zero-sized layer costs huge amount of memory, but without increasing total # of trainable parameters", "user": {"login": "xuancong84", "id": 10172392, "node_id": "MDQ6VXNlcjEwMTcyMzky", "avatar_url": "https://avatars0.githubusercontent.com/u/10172392?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuancong84", "html_url": "https://github.com/xuancong84", "followers_url": "https://api.github.com/users/xuancong84/followers", "following_url": "https://api.github.com/users/xuancong84/following{/other_user}", "gists_url": "https://api.github.com/users/xuancong84/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuancong84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuancong84/subscriptions", "organizations_url": "https://api.github.com/users/xuancong84/orgs", "repos_url": "https://api.github.com/users/xuancong84/repos", "events_url": "https://api.github.com/users/xuancong84/events{/privacy}", "received_events_url": "https://api.github.com/users/xuancong84/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2016-11-25T07:59:34Z", "updated_at": "2017-06-16T16:50:06Z", "closed_at": "2017-06-16T16:50:06Z", "author_association": "NONE", "body_html": "<p>In order to make code generic, my code does append a layer with a variable-sized layer which might be of length 0, i.e.<br>\n<code> tv_concat = tf.concat(1, [tf.reshape(tf.slice(X,[0,time_step,0],[-1,Ncontext,-1]), [tp_current_batch_size,embed_size*Ncontext]), tv_hh])</code></p>\n<p>In the above code, X is a tensor of shape [batch_size, max_time_step, state_size], tv_hh of shape [batch_size, state_size]. Depending on the value of Ncontext, a differently sized layer will be concatenated with the existing layer tv_hh. So when Ncontext=0, a zero-size layer will be concatenated so that tv_concat will be the same as tv_hh. I have explicitly checked and confirmed that tv_concat is indeed of the same shape as tv_hh. I have also checked that the total number of trainable parameters are the same.</p>\n<p>However, when Ncontext=0, the graphics memory consumption is &gt;3G, if I use tv_concat=tv_hh, the graphics memory consumption is only 2G.</p>\n<p>So is this behaviour expected?</p>", "body_text": "In order to make code generic, my code does append a layer with a variable-sized layer which might be of length 0, i.e.\n tv_concat = tf.concat(1, [tf.reshape(tf.slice(X,[0,time_step,0],[-1,Ncontext,-1]), [tp_current_batch_size,embed_size*Ncontext]), tv_hh])\nIn the above code, X is a tensor of shape [batch_size, max_time_step, state_size], tv_hh of shape [batch_size, state_size]. Depending on the value of Ncontext, a differently sized layer will be concatenated with the existing layer tv_hh. So when Ncontext=0, a zero-size layer will be concatenated so that tv_concat will be the same as tv_hh. I have explicitly checked and confirmed that tv_concat is indeed of the same shape as tv_hh. I have also checked that the total number of trainable parameters are the same.\nHowever, when Ncontext=0, the graphics memory consumption is >3G, if I use tv_concat=tv_hh, the graphics memory consumption is only 2G.\nSo is this behaviour expected?", "body": "In order to make code generic, my code does append a layer with a variable-sized layer which might be of length 0, i.e.\r\n`\r\ntv_concat = tf.concat(1, [tf.reshape(tf.slice(X,[0,time_step,0],[-1,Ncontext,-1]), [tp_current_batch_size,embed_size*Ncontext]), tv_hh])`\r\n\r\nIn the above code, X is a tensor of shape [batch_size, max_time_step, state_size], tv_hh of shape [batch_size, state_size]. Depending on the value of Ncontext, a differently sized layer will be concatenated with the existing layer tv_hh. So when Ncontext=0, a zero-size layer will be concatenated so that tv_concat will be the same as tv_hh. I have explicitly checked and confirmed that tv_concat is indeed of the same shape as tv_hh. I have also checked that the total number of trainable parameters are the same.\r\n\r\nHowever, when Ncontext=0, the graphics memory consumption is >3G, if I use tv_concat=tv_hh, the graphics memory consumption is only 2G.\r\n\r\nSo is this behaviour expected?"}