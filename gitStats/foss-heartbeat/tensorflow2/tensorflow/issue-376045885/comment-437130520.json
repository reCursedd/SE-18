{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/437130520", "html_url": "https://github.com/tensorflow/tensorflow/issues/23407#issuecomment-437130520", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407", "id": 437130520, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzEzMDUyMA==", "user": {"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-08T19:43:42Z", "updated_at": "2018-11-08T20:59:10Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8427788\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/galeone\">@galeone</a>,</p>\n<p>I tried to reproduce the problems, but I'm not able to. The only changes I made were to add this loop (and print the losses).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">_</span>):\n  tf.enable_eager_execution()\n\n  (train_images, train_labels), _ <span class=\"pl-k\">=</span> k.datasets.fashion_mnist.load_data()\n\n  train_images <span class=\"pl-k\">=</span> train_images <span class=\"pl-k\">/</span> <span class=\"pl-c1\">255.0</span>\n  train_images <span class=\"pl-k\">=</span> train_images.astype(np.float32)\n  train_images <span class=\"pl-k\">=</span> np.expand_dims(train_images, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n  train_dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(\n      (train_images, train_labels)).shuffle(<span class=\"pl-c1\">10000</span>).repeat().batch(<span class=\"pl-c1\">128</span>).map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>: (tf.image.resize_images(x, [<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>]), y))\n\n  global_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\n  generator_optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">2e-4</span>, <span class=\"pl-c1\">0.5</span>)\n  discriminator_optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">2e-4</span>, <span class=\"pl-c1\">0.5</span>)\n  discriminator <span class=\"pl-k\">=</span> Discriminator()\n  g_encoder <span class=\"pl-k\">=</span> Encoder()\n  g_decoder <span class=\"pl-k\">=</span> Decoder()\n  encoder <span class=\"pl-k\">=</span> Encoder()\n\n  it <span class=\"pl-k\">=</span> <span class=\"pl-c1\">iter</span>(train_dataset)\n  <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n    x, _ <span class=\"pl-k\">=</span> it.next()\n    train(encoder, g_encoder, g_decoder, generator_optimizer, discriminator,\n          discriminator_optimizer, global_step, x)</pre></div>\n<p>I get after 2000 steps:</p>\n<pre><code>step &lt;tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1000&gt;, g: 4.06344842911, d: 1.27861332893                                                                                                                                                                                   \nstep &lt;tf.Variable 'global_step:0' shape=() dtype=int64, numpy=2000&gt;, g: 3.98709654808, d: 0.57758295536\n</code></pre>\n<p>Note that I tried this with/without a GPU, and on 1.11 as well as nightly and wasn't able to reproduce it.</p>\n<p>Perhaps you can paste your full code so that I can reproduce this better?</p>", "body_text": "Hi @galeone,\nI tried to reproduce the problems, but I'm not able to. The only changes I made were to add this loop (and print the losses).\ndef main(_):\n  tf.enable_eager_execution()\n\n  (train_images, train_labels), _ = k.datasets.fashion_mnist.load_data()\n\n  train_images = train_images / 255.0\n  train_images = train_images.astype(np.float32)\n  train_images = np.expand_dims(train_images, -1)\n\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n      (train_images, train_labels)).shuffle(10000).repeat().batch(128).map(lambda x, y: (tf.image.resize_images(x, [32, 32]), y))\n\n  global_step = tf.train.get_or_create_global_step()\n  generator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\n  discriminator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\n  discriminator = Discriminator()\n  g_encoder = Encoder()\n  g_decoder = Decoder()\n  encoder = Encoder()\n\n  it = iter(train_dataset)\n  for i in range(10000):\n    x, _ = it.next()\n    train(encoder, g_encoder, g_decoder, generator_optimizer, discriminator,\n          discriminator_optimizer, global_step, x)\nI get after 2000 steps:\nstep <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1000>, g: 4.06344842911, d: 1.27861332893                                                                                                                                                                                   \nstep <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=2000>, g: 3.98709654808, d: 0.57758295536\n\nNote that I tried this with/without a GPU, and on 1.11 as well as nightly and wasn't able to reproduce it.\nPerhaps you can paste your full code so that I can reproduce this better?", "body": "Hi @galeone,\r\n\r\nI tried to reproduce the problems, but I'm not able to. The only changes I made were to add this loop (and print the losses).  \r\n```python\r\ndef main(_):\r\n  tf.enable_eager_execution()\r\n\r\n  (train_images, train_labels), _ = k.datasets.fashion_mnist.load_data()\r\n\r\n  train_images = train_images / 255.0\r\n  train_images = train_images.astype(np.float32)\r\n  train_images = np.expand_dims(train_images, -1)\r\n\r\n  train_dataset = tf.data.Dataset.from_tensor_slices(\r\n      (train_images, train_labels)).shuffle(10000).repeat().batch(128).map(lambda x, y: (tf.image.resize_images(x, [32, 32]), y))\r\n\r\n  global_step = tf.train.get_or_create_global_step()\r\n  generator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\r\n  discriminator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\r\n  discriminator = Discriminator()\r\n  g_encoder = Encoder()\r\n  g_decoder = Decoder()\r\n  encoder = Encoder()\r\n\r\n  it = iter(train_dataset)\r\n  for i in range(10000):\r\n    x, _ = it.next()\r\n    train(encoder, g_encoder, g_decoder, generator_optimizer, discriminator,\r\n          discriminator_optimizer, global_step, x)\r\n```\r\n\r\nI get after 2000 steps:\r\n```\r\nstep <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=1000>, g: 4.06344842911, d: 1.27861332893                                                                                                                                                                                   \r\nstep <tf.Variable 'global_step:0' shape=() dtype=int64, numpy=2000>, g: 3.98709654808, d: 0.57758295536\r\n```\r\n\r\nNote that I tried this with/without a GPU, and on 1.11 as well as nightly and wasn't able to reproduce it. \r\n\r\nPerhaps you can paste your full code so that I can reproduce this better?"}