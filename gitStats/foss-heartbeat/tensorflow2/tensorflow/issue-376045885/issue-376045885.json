{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23407", "id": 376045885, "node_id": "MDU6SXNzdWUzNzYwNDU4ODU=", "number": 23407, "title": "Tensorflow eager version fails, while Tensorflow static graph works", "user": {"login": "galeone", "id": 8427788, "node_id": "MDQ6VXNlcjg0Mjc3ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/8427788?v=4", "gravatar_id": "", "url": "https://api.github.com/users/galeone", "html_url": "https://github.com/galeone", "followers_url": "https://api.github.com/users/galeone/followers", "following_url": "https://api.github.com/users/galeone/following{/other_user}", "gists_url": "https://api.github.com/users/galeone/gists{/gist_id}", "starred_url": "https://api.github.com/users/galeone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/galeone/subscriptions", "organizations_url": "https://api.github.com/users/galeone/orgs", "repos_url": "https://api.github.com/users/galeone/repos", "events_url": "https://api.github.com/users/galeone/events{/privacy}", "received_events_url": "https://api.github.com/users/galeone/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2018-10-31T16:17:08Z", "updated_at": "2018-11-21T18:43:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux</li>\n<li>TensorFlow installed from (source or binary): repository</li>\n<li>TensorFlow version (use command below): 1.11</li>\n<li>Python version: 3.7</li>\n<li>CUDA/cuDNN version: cuda 10, cudnn 7</li>\n<li>GPU model and memory: nvidia 1080ti</li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>I'm porting a ML model ( <a href=\"https://github.com/samet-akcay/ganomaly\">https://github.com/samet-akcay/ganomaly</a> ) implemented in pytorch to tensorflow (using the keras layers, knowing that tf 2.0 will come soon).</p>\n<p>The first implementation was using the eager version to do the train, but the model collapses, nothing works.</p>\n<p>The same model definition has been reused but it has been used to first define a static graph and then train the model: it works perfectly.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>The static graph version and the eager version should have the same behavior.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<h3>Model description (same for both eager and static)</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Dict\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.keras <span class=\"pl-k\">as</span> k\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nconv_initializer <span class=\"pl-k\">=</span> k.initializers.random_normal(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">0.02</span>)\nbatchnorm_inizializer <span class=\"pl-k\">=</span> k.initializers.random_normal(<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">0.02</span>)\n\neps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-5</span>\nmomentum <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.99</span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Decoder</span>(<span class=\"pl-e\">k</span>.<span class=\"pl-e\">models</span>.<span class=\"pl-e\">Model</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Decoder (Generator) Network</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">output_depth</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>):\n        <span class=\"pl-c1\">super</span>(Decoder, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">100</span>),\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n        <span class=\"pl-c1\">self</span>.batchnorm1 <span class=\"pl-k\">=</span> k.layers.BatchNormalization(\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>eps,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum,\n            <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n            <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n        <span class=\"pl-c1\">self</span>.batchnorm2 <span class=\"pl-k\">=</span> k.layers.BatchNormalization(\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>eps,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum,\n            <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n            <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv3 <span class=\"pl-k\">=</span> k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n        <span class=\"pl-c1\">self</span>.batchnorm3 <span class=\"pl-k\">=</span> k.layers.BatchNormalization(\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>eps,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum,\n            <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n            <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv4 <span class=\"pl-k\">=</span> k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span>output_depth,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(\"X.SHAPE: \", x.shape)</span>\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batchnorm1(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>training)\n        x <span class=\"pl-k\">=</span> tf.nn.relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv2(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batchnorm2(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>training)\n        x <span class=\"pl-k\">=</span> tf.nn.relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv3(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batchnorm3(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>training)\n        x <span class=\"pl-k\">=</span> tf.nn.relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv4(x)\n        x <span class=\"pl-k\">=</span> tf.nn.tanh(x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> image</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(\"Decoder call output size: \", x.shape)</span>\n\n        <span class=\"pl-k\">return</span> x\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Encoder</span>(<span class=\"pl-e\">k</span>.<span class=\"pl-e\">models</span>.<span class=\"pl-e\">Model</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">latent_dimensions</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>):\n        <span class=\"pl-c1\">super</span>(Encoder, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.conv0 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">1</span>),\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n        <span class=\"pl-c1\">self</span>.batchnorm1 <span class=\"pl-k\">=</span> k.layers.BatchNormalization(\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>eps,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum,\n            <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n            <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n        <span class=\"pl-c1\">self</span>.batchnorm2 <span class=\"pl-k\">=</span> k.layers.BatchNormalization(\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>eps,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum,\n            <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n            <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv3 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span>latent_dimensions,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>valid<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> x = self.conv0(x, input_shape=x.shape[1:])</span>\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv0(x)\n        x <span class=\"pl-k\">=</span> tf.nn.leaky_relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batchnorm1(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>training)\n        x <span class=\"pl-k\">=</span> tf.nn.leaky_relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv2(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batchnorm2(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>training)\n        x <span class=\"pl-k\">=</span> tf.nn.leaky_relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv3(x)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> x = tf.nn.tanh(x)       # latent space unitary sphere [-1,1] <span class=\"pl-k\">TODO</span>: temporary?</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(\"Encoder call output size: \", x.shape)</span>\n\n        <span class=\"pl-k\">return</span> x\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Discriminator</span>(<span class=\"pl-e\">k</span>.<span class=\"pl-e\">models</span>.<span class=\"pl-e\">Model</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Discriminator, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.conv0 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n        <span class=\"pl-c1\">self</span>.batchnorm1 <span class=\"pl-k\">=</span> k.layers.BatchNormalization(\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>eps,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum,\n            <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n            <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n        <span class=\"pl-c1\">self</span>.batchnorm2 <span class=\"pl-k\">=</span> k.layers.BatchNormalization(\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>eps,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>momentum,\n            <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n            <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>batchnorm_inizializer,\n        )\n\n        <span class=\"pl-c1\">self</span>.conv3 <span class=\"pl-k\">=</span> k.layers.Conv2D(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>),\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>valid<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>conv_initializer,\n            <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv0(x)\n        x <span class=\"pl-k\">=</span> tf.nn.leaky_relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batchnorm1(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>training)\n        x <span class=\"pl-k\">=</span> tf.nn.leaky_relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv2(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batchnorm2(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>training)\n        x <span class=\"pl-k\">=</span> tf.nn.leaky_relu(x)\n\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv3(x)\n\n        <span class=\"pl-k\">return</span> x</pre></div>\n<p>Also, the following variable definitions are shared in both the eager and static implementations</p>\n<div class=\"highlight highlight-source-python\"><pre>            global_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\n            generator_optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">2e-4</span>, <span class=\"pl-c1\">0.5</span>)\n            discriminator_optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">2e-4</span>, <span class=\"pl-c1\">0.5</span>)\n            discirminator <span class=\"pl-k\">=</span> Discriminator()\n            g_encoder <span class=\"pl-k\">=</span> Encoder()\n            g_decoder <span class=\"pl-k\">=</span> Decoder()\n            encoder <span class=\"pl-k\">=</span> Encoder()</pre></div>\n<h3>Eager training</h3>\n<p>I show just a single training update, that's run in a training loop</p>\n<div class=\"highlight highlight-source-python\"><pre>            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Discriminator training</span>\n            <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n\n                discriminator.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                disc_x <span class=\"pl-k\">=</span> tf.squeeze(discriminator(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n\n                disc_real_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator loss on result disc_x</span>\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.ones_like(disc_x), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x\n                )\n                g_encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n                g_decoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> recreate the data (=&gt; x_hat), starting from real data x</span>\n                z <span class=\"pl-k\">=</span> g_encoder(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not training</span>\n                x_hat <span class=\"pl-k\">=</span> g_decoder(z, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not training</span>\n\n                disc_x_hat <span class=\"pl-k\">=</span> tf.squeeze(discriminator(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n                disc_gen_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator loss on result disc_x_hat</span>\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.zeros_like(disc_x_hat), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x_hat\n                )\n                disc_loss <span class=\"pl-k\">=</span> disc_real_loss <span class=\"pl-k\">+</span> disc_gen_loss\n\n            discriminator_gradients <span class=\"pl-k\">=</span> tape.gradient(\n                disc_loss, discriminator.trainable_variables\n            )\n\n            discriminator_optimizer.apply_gradients(\n                <span class=\"pl-c1\">zip</span>(discriminator_gradients, discriminator.trainable_variables)\n            )\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Generator Training</span>\n            <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> err_g_bce</span>\n                g_encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                g_decoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                z <span class=\"pl-k\">=</span> g_encoder(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                x_hat <span class=\"pl-k\">=</span> g_decoder(z, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                disc_x_hat <span class=\"pl-k\">=</span> tf.squeeze(\n                    discriminator(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]\n                ) \n                bce_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.ones_like(disc_x_hat),\n                    <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x_hat,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> G wants to generate reals so ones_like</span>\n                )\n\n                l1_loss <span class=\"pl-k\">=</span> tf.losses.absolute_difference(x, x_hat)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> err_g_enc</span>\n                z_hat <span class=\"pl-k\">=</span> encoder(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                l2_loss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(z, z_hat)\n\n                gen_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span><span class=\"pl-k\">*</span> bce_loss <span class=\"pl-k\">+</span> <span class=\"pl-c1\">50</span> <span class=\"pl-k\">*</span> l1_loss <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">*</span> l2_loss\n                \n            trainable_variable_list <span class=\"pl-k\">=</span> (\n                g_encoder.trainable_variables\n                <span class=\"pl-k\">+</span> g_decoder.trainable_variables\n                <span class=\"pl-k\">+</span> encoder.trainable_variables\n            )\n\n            generator_gradients <span class=\"pl-k\">=</span> tape.gradient(gen_loss, trainable_variable_list)\n\n            generator_optimizer.apply_gradients(\n                <span class=\"pl-c1\">zip</span>(generator_gradients, trainable_variable_list),\n                <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step,\n            )</pre></div>\n<h3>Static graph training</h3>\n<p>First I show the graph definition,then how is used in the training loop</p>\n<h4>Graph def</h4>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Discriminator on real</span>\n    D_x <span class=\"pl-k\">=</span> discriminator(x)\n    D_x <span class=\"pl-k\">=</span> tf.squeeze(D_x, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate fake</span>\n    z <span class=\"pl-k\">=</span> g_encoder(x)\n    x_hat <span class=\"pl-k\">=</span> g_decoder(z)\n\n    D_x_hat <span class=\"pl-k\">=</span> discriminator(x_hat)\n    D_x_hat <span class=\"pl-k\">=</span> tf.squeeze(D_x_hat, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># Discriminator</span>\n    d_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy( \n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.ones_like(D_x), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>D_x) <span class=\"pl-k\">+</span> tf.losses.sigmoid_cross_entropy(\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.zeros_like(D_x_hat), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>D_x_hat)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> encode x_hat to z_hat</span>\n    z_hat <span class=\"pl-k\">=</span> encoder(x_hat)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># Generator</span>\n    bce_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(tf.ones_like(D_x_hat), D_x_hat)\n    l1_loss <span class=\"pl-k\">=</span> tf.losses.absolute_difference(x, x_hat)\n    l2_loss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(z, z_hat)\n\n    g_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">*</span> bce_loss <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">*</span> l1_loss <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">*</span> l2_loss\n    global_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the D train op</span>\n    train_d <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(\n        lr, <span class=\"pl-v\">beta1</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>).minimize(\n            d_loss, <span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>D.trainable_variables)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> train G_e G_d E</span>\n    train_g <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(\n        lr, <span class=\"pl-v\">beta1</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>).minimize(\n            g_loss,\n            <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step,\n            <span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>g_encoder.trainable_variables <span class=\"pl-k\">+</span> g_decoder.trainable_variables <span class=\"pl-k\">+</span>\n            encoder.trainable_variables)</pre></div>\n<p>And this is what's inside the training loop (executed in a MonitoredSession):</p>\n<div class=\"highlight highlight-source-python\"><pre>            <span class=\"pl-c\"><span class=\"pl-c\">#</span> extract from tf.data.Dataset, x is a placeholder and x_ is the iterator.get_next</span>\n            real <span class=\"pl-k\">=</span> sess.run(x_) \n            feed_dict <span class=\"pl-k\">=</span> {x: real}\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> train D</span>\n            _, d_loss_value <span class=\"pl-k\">=</span> sess.run([train_d, d_loss], feed_dict)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> train G+E</span>\n            _, g_loss_value, step <span class=\"pl-k\">=</span> sess.run([train_g, g_loss, global_step],\n                                             feed_dict)</pre></div>\n<p>The model definition is the same, the loss definition is the same, the training steps and the same, the only difference is the eager mode enabled or disabled. The results with eager on are:</p>\n<p>D loss: collapses to zero, that's wrong since its aim is to stay around 0.5<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8427788/47802503-b2a31100-dd30-11e8-8d53-ef024f30879d.png\"><img src=\"https://user-images.githubusercontent.com/8427788/47802503-b2a31100-dd30-11e8-8d53-ef024f30879d.png\" alt=\"bad d\" style=\"max-width:100%;\"></a></p>\n<p>Generated images: wrong, bad reconstructions since D collapsed<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8427788/47802519-bafb4c00-dd30-11e8-9a70-16b2c27d69b7.png\"><img src=\"https://user-images.githubusercontent.com/8427788/47802519-bafb4c00-dd30-11e8-9a70-16b2c27d69b7.png\" alt=\"bad_gen\" style=\"max-width:100%;\"></a></p>\n<p>While when eager is off, the discriminator loss looks correct and the generated output are the one expected:</p>\n<p>D loss:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8427788/47802443-8e473480-dd30-11e8-8525-38ec50040a16.png\"><img src=\"https://user-images.githubusercontent.com/8427788/47802443-8e473480-dd30-11e8-8525-38ec50040a16.png\" alt=\"good d\" style=\"max-width:100%;\"></a></p>\n<p>Generated output:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8427788/47802455-930be880-dd30-11e8-85c3-efdd87311530.png\"><img src=\"https://user-images.githubusercontent.com/8427788/47802455-930be880-dd30-11e8-85c3-efdd87311530.png\" alt=\"good_gen\" style=\"max-width:100%;\"></a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\nTensorFlow installed from (source or binary): repository\nTensorFlow version (use command below): 1.11\nPython version: 3.7\nCUDA/cuDNN version: cuda 10, cudnn 7\nGPU model and memory: nvidia 1080ti\n\nDescribe the current behavior\nI'm porting a ML model ( https://github.com/samet-akcay/ganomaly ) implemented in pytorch to tensorflow (using the keras layers, knowing that tf 2.0 will come soon).\nThe first implementation was using the eager version to do the train, but the model collapses, nothing works.\nThe same model definition has been reused but it has been used to first define a static graph and then train the model: it works perfectly.\nDescribe the expected behavior\nThe static graph version and the eager version should have the same behavior.\nCode to reproduce the issue\nModel description (same for both eager and static)\nfrom typing import Dict\nimport tensorflow as tf\nimport tensorflow.keras as k\nimport numpy as np\n\nconv_initializer = k.initializers.random_normal(0.0, 0.02)\nbatchnorm_inizializer = k.initializers.random_normal(1.0, 0.02)\n\neps = 1e-5\nmomentum = 0.99\n\n\nclass Decoder(k.models.Model):\n    \"\"\"\n    Decoder (Generator) Network\n    \"\"\"\n\n    def __init__(self, output_depth: int = 1):\n        super(Decoder, self).__init__()\n\n        self.conv1 = k.layers.Conv2DTranspose(\n            filters=256,\n            kernel_size=(4, 4),\n            strides=(1, 1),\n            kernel_initializer=conv_initializer,\n            input_shape=(-1, 1, 1, 100),\n            use_bias=False,\n        )\n        self.batchnorm1 = k.layers.BatchNormalization(\n            epsilon=eps,\n            momentum=momentum,\n            beta_initializer=batchnorm_inizializer,\n            gamma_initializer=batchnorm_inizializer,\n        )\n\n        self.conv2 = k.layers.Conv2DTranspose(\n            filters=128,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n        self.batchnorm2 = k.layers.BatchNormalization(\n            epsilon=eps,\n            momentum=momentum,\n            beta_initializer=batchnorm_inizializer,\n            gamma_initializer=batchnorm_inizializer,\n        )\n\n        self.conv3 = k.layers.Conv2DTranspose(\n            filters=64,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n        self.batchnorm3 = k.layers.BatchNormalization(\n            epsilon=eps,\n            momentum=momentum,\n            beta_initializer=batchnorm_inizializer,\n            gamma_initializer=batchnorm_inizializer,\n        )\n\n        self.conv4 = k.layers.Conv2DTranspose(\n            filters=output_depth,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n\n    def call(self, x, training=True):\n        # print(\"X.SHAPE: \", x.shape)\n\n        x = self.conv1(x)\n        x = self.batchnorm1(x, training=training)\n        x = tf.nn.relu(x)\n\n        x = self.conv2(x)\n        x = self.batchnorm2(x, training=training)\n        x = tf.nn.relu(x)\n\n        x = self.conv3(x)\n        x = self.batchnorm3(x, training=training)\n        x = tf.nn.relu(x)\n\n        x = self.conv4(x)\n        x = tf.nn.tanh(x)  # image\n\n        # print(\"Decoder call output size: \", x.shape)\n\n        return x\n\n\nclass Encoder(k.models.Model):\n\n    def __init__(self, latent_dimensions: int = 100):\n        super(Encoder, self).__init__()\n\n        self.conv0 = k.layers.Conv2D(\n            filters=64,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            input_shape=(-1, 32, 32, 1),\n            use_bias=False,\n        )\n\n        self.conv1 = k.layers.Conv2D(\n            filters=128,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n        self.batchnorm1 = k.layers.BatchNormalization(\n            epsilon=eps,\n            momentum=momentum,\n            beta_initializer=batchnorm_inizializer,\n            gamma_initializer=batchnorm_inizializer,\n        )\n\n        self.conv2 = k.layers.Conv2D(\n            filters=256,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n        self.batchnorm2 = k.layers.BatchNormalization(\n            epsilon=eps,\n            momentum=momentum,\n            beta_initializer=batchnorm_inizializer,\n            gamma_initializer=batchnorm_inizializer,\n        )\n\n        self.conv3 = k.layers.Conv2D(\n            filters=latent_dimensions,\n            kernel_size=(4, 4),\n            strides=(1, 1),\n            padding=\"valid\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n\n    def call(self, x, training=True):\n        # x = self.conv0(x, input_shape=x.shape[1:])\n\n        x = self.conv0(x)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv1(x)\n        x = self.batchnorm1(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv2(x)\n        x = self.batchnorm2(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv3(x)\n        # x = tf.nn.tanh(x)       # latent space unitary sphere [-1,1] TODO: temporary?\n\n        # print(\"Encoder call output size: \", x.shape)\n\n        return x\n\n\nclass Discriminator(k.models.Model):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.conv0 = k.layers.Conv2D(\n            filters=64,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n\n        self.conv1 = k.layers.Conv2D(\n            filters=128,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n        self.batchnorm1 = k.layers.BatchNormalization(\n            epsilon=eps,\n            momentum=momentum,\n            beta_initializer=batchnorm_inizializer,\n            gamma_initializer=batchnorm_inizializer,\n        )\n\n        self.conv2 = k.layers.Conv2D(\n            filters=256,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n        self.batchnorm2 = k.layers.BatchNormalization(\n            epsilon=eps,\n            momentum=momentum,\n            beta_initializer=batchnorm_inizializer,\n            gamma_initializer=batchnorm_inizializer,\n        )\n\n        self.conv3 = k.layers.Conv2D(\n            filters=1,\n            kernel_size=(4, 4),\n            strides=(1, 1),\n            padding=\"valid\",\n            kernel_initializer=conv_initializer,\n            use_bias=False,\n        )\n\n    def call(self, x, training=True):\n        x = self.conv0(x)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv1(x)\n        x = self.batchnorm1(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv2(x)\n        x = self.batchnorm2(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.conv3(x)\n\n        return x\nAlso, the following variable definitions are shared in both the eager and static implementations\n            global_step = tf.train.get_or_create_global_step()\n            generator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\n            discriminator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\n            discirminator = Discriminator()\n            g_encoder = Encoder()\n            g_decoder = Decoder()\n            encoder = Encoder()\nEager training\nI show just a single training update, that's run in a training loop\n            # Discriminator training\n            with tf.GradientTape() as tape:\n\n                discriminator.trainable = True\n                disc_x = tf.squeeze(discriminator(x, training=False), axis=[1, 2])\n\n                disc_real_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x\n                    multi_class_labels=tf.ones_like(disc_x), logits=disc_x\n                )\n                g_encoder.trainable = False\n                g_decoder.trainable = False\n                # recreate the data (=> x_hat), starting from real data x\n                z = g_encoder(x, training=True)  # Not training\n                x_hat = g_decoder(z, training=True)  # Not training\n\n                disc_x_hat = tf.squeeze(discriminator(x_hat, training=False), axis=[1, 2])\n                disc_gen_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x_hat\n                    multi_class_labels=tf.zeros_like(disc_x_hat), logits=disc_x_hat\n                )\n                disc_loss = disc_real_loss + disc_gen_loss\n\n            discriminator_gradients = tape.gradient(\n                disc_loss, discriminator.trainable_variables\n            )\n\n            discriminator_optimizer.apply_gradients(\n                zip(discriminator_gradients, discriminator.trainable_variables)\n            )\n\n            # Generator Training\n            with tf.GradientTape() as tape:\n\n                # err_g_bce\n                g_encoder.trainable = True\n                g_decoder.trainable = True\n                encoder.trainable = True\n                z = g_encoder(x, training=True)\n                x_hat = g_decoder(z, training=True)\n                disc_x_hat = tf.squeeze(\n                    discriminator(x_hat, training=False), axis=[1, 2]\n                ) \n                bce_loss = tf.losses.sigmoid_cross_entropy(\n                    multi_class_labels=tf.ones_like(disc_x_hat),\n                    logits=disc_x_hat,  # G wants to generate reals so ones_like\n                )\n\n                l1_loss = tf.losses.absolute_difference(x, x_hat)\n                # err_g_enc\n                z_hat = encoder(x_hat, training=True)\n                l2_loss = tf.losses.mean_squared_error(z, z_hat)\n\n                gen_loss = 1* bce_loss + 50 * l1_loss + 1 * l2_loss\n                \n            trainable_variable_list = (\n                g_encoder.trainable_variables\n                + g_decoder.trainable_variables\n                + encoder.trainable_variables\n            )\n\n            generator_gradients = tape.gradient(gen_loss, trainable_variable_list)\n\n            generator_optimizer.apply_gradients(\n                zip(generator_gradients, trainable_variable_list),\n                global_step=global_step,\n            )\nStatic graph training\nFirst I show the graph definition,then how is used in the training loop\nGraph def\n    # Discriminator on real\n    D_x = discriminator(x)\n    D_x = tf.squeeze(D_x, axis=[1, 2])\n\n    # Generate fake\n    z = g_encoder(x)\n    x_hat = g_decoder(z)\n\n    D_x_hat = discriminator(x_hat)\n    D_x_hat = tf.squeeze(D_x_hat, axis=[1, 2])\n    ## Discriminator\n    d_loss = tf.losses.sigmoid_cross_entropy( \n                    multi_class_labels=tf.ones_like(D_x), logits=D_x) + tf.losses.sigmoid_cross_entropy(\n                    multi_class_labels=tf.zeros_like(D_x_hat), logits=D_x_hat)\n\n    # encode x_hat to z_hat\n    z_hat = encoder(x_hat)\n\n    ## Generator\n    bce_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(D_x_hat), D_x_hat)\n    l1_loss = tf.losses.absolute_difference(x, x_hat)\n    l2_loss = tf.losses.mean_squared_error(z, z_hat)\n\n    g_loss = 1 * bce_loss + 1 * l1_loss + 1 * l2_loss\n    global_step = tf.train.get_or_create_global_step()\n\n    # Define the D train op\n    train_d = tf.train.AdamOptimizer(\n        lr, beta1=0.5).minimize(\n            d_loss, var_list=D.trainable_variables)\n\n    # train G_e G_d E\n    train_g = tf.train.AdamOptimizer(\n        lr, beta1=0.5).minimize(\n            g_loss,\n            global_step=global_step,\n            var_list=g_encoder.trainable_variables + g_decoder.trainable_variables +\n            encoder.trainable_variables)\nAnd this is what's inside the training loop (executed in a MonitoredSession):\n            # extract from tf.data.Dataset, x is a placeholder and x_ is the iterator.get_next\n            real = sess.run(x_) \n            feed_dict = {x: real}\n\n            # train D\n            _, d_loss_value = sess.run([train_d, d_loss], feed_dict)\n\n            # train G+E\n            _, g_loss_value, step = sess.run([train_g, g_loss, global_step],\n                                             feed_dict)\nThe model definition is the same, the loss definition is the same, the training steps and the same, the only difference is the eager mode enabled or disabled. The results with eager on are:\nD loss: collapses to zero, that's wrong since its aim is to stay around 0.5\n\nGenerated images: wrong, bad reconstructions since D collapsed\n\nWhile when eager is off, the discriminator loss looks correct and the generated output are the one expected:\nD loss:\n\nGenerated output:", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): repository\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: cuda 10, cudnn 7\r\n- GPU model and memory: nvidia 1080ti\r\n\r\n**Describe the current behavior**\r\n\r\nI'm porting a ML model ( https://github.com/samet-akcay/ganomaly ) implemented in pytorch to tensorflow (using the keras layers, knowing that tf 2.0 will come soon).\r\n\r\nThe first implementation was using the eager version to do the train, but the model collapses, nothing works.\r\n\r\nThe same model definition has been reused but it has been used to first define a static graph and then train the model: it works perfectly.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe static graph version and the eager version should have the same behavior.\r\n\r\n**Code to reproduce the issue**\r\n\r\n### Model description (same for both eager and static)\r\n\r\n```python\r\nfrom typing import Dict\r\nimport tensorflow as tf\r\nimport tensorflow.keras as k\r\nimport numpy as np\r\n\r\nconv_initializer = k.initializers.random_normal(0.0, 0.02)\r\nbatchnorm_inizializer = k.initializers.random_normal(1.0, 0.02)\r\n\r\neps = 1e-5\r\nmomentum = 0.99\r\n\r\n\r\nclass Decoder(k.models.Model):\r\n    \"\"\"\r\n    Decoder (Generator) Network\r\n    \"\"\"\r\n\r\n    def __init__(self, output_depth: int = 1):\r\n        super(Decoder, self).__init__()\r\n\r\n        self.conv1 = k.layers.Conv2DTranspose(\r\n            filters=256,\r\n            kernel_size=(4, 4),\r\n            strides=(1, 1),\r\n            kernel_initializer=conv_initializer,\r\n            input_shape=(-1, 1, 1, 100),\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm1 = k.layers.BatchNormalization(\r\n            epsilon=eps,\r\n            momentum=momentum,\r\n            beta_initializer=batchnorm_inizializer,\r\n            gamma_initializer=batchnorm_inizializer,\r\n        )\r\n\r\n        self.conv2 = k.layers.Conv2DTranspose(\r\n            filters=128,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm2 = k.layers.BatchNormalization(\r\n            epsilon=eps,\r\n            momentum=momentum,\r\n            beta_initializer=batchnorm_inizializer,\r\n            gamma_initializer=batchnorm_inizializer,\r\n        )\r\n\r\n        self.conv3 = k.layers.Conv2DTranspose(\r\n            filters=64,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm3 = k.layers.BatchNormalization(\r\n            epsilon=eps,\r\n            momentum=momentum,\r\n            beta_initializer=batchnorm_inizializer,\r\n            gamma_initializer=batchnorm_inizializer,\r\n        )\r\n\r\n        self.conv4 = k.layers.Conv2DTranspose(\r\n            filters=output_depth,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n\r\n    def call(self, x, training=True):\r\n        # print(\"X.SHAPE: \", x.shape)\r\n\r\n        x = self.conv1(x)\r\n        x = self.batchnorm1(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.batchnorm3(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv4(x)\r\n        x = tf.nn.tanh(x)  # image\r\n\r\n        # print(\"Decoder call output size: \", x.shape)\r\n\r\n        return x\r\n\r\n\r\nclass Encoder(k.models.Model):\r\n\r\n    def __init__(self, latent_dimensions: int = 100):\r\n        super(Encoder, self).__init__()\r\n\r\n        self.conv0 = k.layers.Conv2D(\r\n            filters=64,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            input_shape=(-1, 32, 32, 1),\r\n            use_bias=False,\r\n        )\r\n\r\n        self.conv1 = k.layers.Conv2D(\r\n            filters=128,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm1 = k.layers.BatchNormalization(\r\n            epsilon=eps,\r\n            momentum=momentum,\r\n            beta_initializer=batchnorm_inizializer,\r\n            gamma_initializer=batchnorm_inizializer,\r\n        )\r\n\r\n        self.conv2 = k.layers.Conv2D(\r\n            filters=256,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm2 = k.layers.BatchNormalization(\r\n            epsilon=eps,\r\n            momentum=momentum,\r\n            beta_initializer=batchnorm_inizializer,\r\n            gamma_initializer=batchnorm_inizializer,\r\n        )\r\n\r\n        self.conv3 = k.layers.Conv2D(\r\n            filters=latent_dimensions,\r\n            kernel_size=(4, 4),\r\n            strides=(1, 1),\r\n            padding=\"valid\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n\r\n    def call(self, x, training=True):\r\n        # x = self.conv0(x, input_shape=x.shape[1:])\r\n\r\n        x = self.conv0(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv1(x)\r\n        x = self.batchnorm1(x, training=training)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x, training=training)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv3(x)\r\n        # x = tf.nn.tanh(x)       # latent space unitary sphere [-1,1] TODO: temporary?\r\n\r\n        # print(\"Encoder call output size: \", x.shape)\r\n\r\n        return x\r\n\r\n\r\nclass Discriminator(k.models.Model):\r\n\r\n    def __init__(self):\r\n        super(Discriminator, self).__init__()\r\n\r\n        self.conv0 = k.layers.Conv2D(\r\n            filters=64,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n\r\n        self.conv1 = k.layers.Conv2D(\r\n            filters=128,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm1 = k.layers.BatchNormalization(\r\n            epsilon=eps,\r\n            momentum=momentum,\r\n            beta_initializer=batchnorm_inizializer,\r\n            gamma_initializer=batchnorm_inizializer,\r\n        )\r\n\r\n        self.conv2 = k.layers.Conv2D(\r\n            filters=256,\r\n            kernel_size=(4, 4),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm2 = k.layers.BatchNormalization(\r\n            epsilon=eps,\r\n            momentum=momentum,\r\n            beta_initializer=batchnorm_inizializer,\r\n            gamma_initializer=batchnorm_inizializer,\r\n        )\r\n\r\n        self.conv3 = k.layers.Conv2D(\r\n            filters=1,\r\n            kernel_size=(4, 4),\r\n            strides=(1, 1),\r\n            padding=\"valid\",\r\n            kernel_initializer=conv_initializer,\r\n            use_bias=False,\r\n        )\r\n\r\n    def call(self, x, training=True):\r\n        x = self.conv0(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv1(x)\r\n        x = self.batchnorm1(x, training=training)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x, training=training)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv3(x)\r\n\r\n        return x\r\n```\r\n\r\nAlso, the following variable definitions are shared in both the eager and static implementations\r\n\r\n```python\r\n            global_step = tf.train.get_or_create_global_step()\r\n            generator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\r\n            discriminator_optimizer = tf.train.AdamOptimizer(2e-4, 0.5)\r\n            discirminator = Discriminator()\r\n            g_encoder = Encoder()\r\n            g_decoder = Decoder()\r\n            encoder = Encoder()\r\n```\r\n\r\n### Eager training\r\n\r\nI show just a single training update, that's run in a training loop\r\n```python\r\n            # Discriminator training\r\n            with tf.GradientTape() as tape:\r\n\r\n                discriminator.trainable = True\r\n                disc_x = tf.squeeze(discriminator(x, training=False), axis=[1, 2])\r\n\r\n                disc_real_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x\r\n                    multi_class_labels=tf.ones_like(disc_x), logits=disc_x\r\n                )\r\n                g_encoder.trainable = False\r\n                g_decoder.trainable = False\r\n                # recreate the data (=> x_hat), starting from real data x\r\n                z = g_encoder(x, training=True)  # Not training\r\n                x_hat = g_decoder(z, training=True)  # Not training\r\n\r\n                disc_x_hat = tf.squeeze(discriminator(x_hat, training=False), axis=[1, 2])\r\n                disc_gen_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x_hat\r\n                    multi_class_labels=tf.zeros_like(disc_x_hat), logits=disc_x_hat\r\n                )\r\n                disc_loss = disc_real_loss + disc_gen_loss\r\n\r\n            discriminator_gradients = tape.gradient(\r\n                disc_loss, discriminator.trainable_variables\r\n            )\r\n\r\n            discriminator_optimizer.apply_gradients(\r\n                zip(discriminator_gradients, discriminator.trainable_variables)\r\n            )\r\n\r\n            # Generator Training\r\n            with tf.GradientTape() as tape:\r\n\r\n                # err_g_bce\r\n                g_encoder.trainable = True\r\n                g_decoder.trainable = True\r\n                encoder.trainable = True\r\n                z = g_encoder(x, training=True)\r\n                x_hat = g_decoder(z, training=True)\r\n                disc_x_hat = tf.squeeze(\r\n                    discriminator(x_hat, training=False), axis=[1, 2]\r\n                ) \r\n                bce_loss = tf.losses.sigmoid_cross_entropy(\r\n                    multi_class_labels=tf.ones_like(disc_x_hat),\r\n                    logits=disc_x_hat,  # G wants to generate reals so ones_like\r\n                )\r\n\r\n                l1_loss = tf.losses.absolute_difference(x, x_hat)\r\n                # err_g_enc\r\n                z_hat = encoder(x_hat, training=True)\r\n                l2_loss = tf.losses.mean_squared_error(z, z_hat)\r\n\r\n                gen_loss = 1* bce_loss + 50 * l1_loss + 1 * l2_loss\r\n                \r\n            trainable_variable_list = (\r\n                g_encoder.trainable_variables\r\n                + g_decoder.trainable_variables\r\n                + encoder.trainable_variables\r\n            )\r\n\r\n            generator_gradients = tape.gradient(gen_loss, trainable_variable_list)\r\n\r\n            generator_optimizer.apply_gradients(\r\n                zip(generator_gradients, trainable_variable_list),\r\n                global_step=global_step,\r\n            )\r\n```\r\n\r\n### Static graph training\r\nFirst I show the graph definition,then how is used in the training loop\r\n\r\n#### Graph def\r\n```python\r\n\r\n    # Discriminator on real\r\n    D_x = discriminator(x)\r\n    D_x = tf.squeeze(D_x, axis=[1, 2])\r\n\r\n    # Generate fake\r\n    z = g_encoder(x)\r\n    x_hat = g_decoder(z)\r\n\r\n    D_x_hat = discriminator(x_hat)\r\n    D_x_hat = tf.squeeze(D_x_hat, axis=[1, 2])\r\n    ## Discriminator\r\n    d_loss = tf.losses.sigmoid_cross_entropy( \r\n                    multi_class_labels=tf.ones_like(D_x), logits=D_x) + tf.losses.sigmoid_cross_entropy(\r\n                    multi_class_labels=tf.zeros_like(D_x_hat), logits=D_x_hat)\r\n\r\n    # encode x_hat to z_hat\r\n    z_hat = encoder(x_hat)\r\n\r\n    ## Generator\r\n    bce_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(D_x_hat), D_x_hat)\r\n    l1_loss = tf.losses.absolute_difference(x, x_hat)\r\n    l2_loss = tf.losses.mean_squared_error(z, z_hat)\r\n\r\n    g_loss = 1 * bce_loss + 1 * l1_loss + 1 * l2_loss\r\n    global_step = tf.train.get_or_create_global_step()\r\n\r\n    # Define the D train op\r\n    train_d = tf.train.AdamOptimizer(\r\n        lr, beta1=0.5).minimize(\r\n            d_loss, var_list=D.trainable_variables)\r\n\r\n    # train G_e G_d E\r\n    train_g = tf.train.AdamOptimizer(\r\n        lr, beta1=0.5).minimize(\r\n            g_loss,\r\n            global_step=global_step,\r\n            var_list=g_encoder.trainable_variables + g_decoder.trainable_variables +\r\n            encoder.trainable_variables)\r\n```\r\n\r\nAnd this is what's inside the training loop (executed in a MonitoredSession):\r\n\r\n```python\r\n            # extract from tf.data.Dataset, x is a placeholder and x_ is the iterator.get_next\r\n            real = sess.run(x_) \r\n            feed_dict = {x: real}\r\n\r\n            # train D\r\n            _, d_loss_value = sess.run([train_d, d_loss], feed_dict)\r\n\r\n            # train G+E\r\n            _, g_loss_value, step = sess.run([train_g, g_loss, global_step],\r\n                                             feed_dict)\r\n````\r\n\r\nThe model definition is the same, the loss definition is the same, the training steps and the same, the only difference is the eager mode enabled or disabled. The results with eager on are:\r\n\r\nD loss: collapses to zero, that's wrong since its aim is to stay around 0.5\r\n![bad d](https://user-images.githubusercontent.com/8427788/47802503-b2a31100-dd30-11e8-8d53-ef024f30879d.png)\r\n\r\nGenerated images: wrong, bad reconstructions since D collapsed\r\n![bad_gen](https://user-images.githubusercontent.com/8427788/47802519-bafb4c00-dd30-11e8-9a70-16b2c27d69b7.png)\r\n\r\nWhile when eager is off, the discriminator loss looks correct and the generated output are the one expected:\r\n\r\nD loss:\r\n![good d](https://user-images.githubusercontent.com/8427788/47802443-8e473480-dd30-11e8-8525-38ec50040a16.png)\r\n\r\nGenerated output:\r\n![good_gen](https://user-images.githubusercontent.com/8427788/47802455-930be880-dd30-11e8-85c3-efdd87311530.png)\r\n\r\n"}