{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436700755", "html_url": "https://github.com/tensorflow/tensorflow/issues/23407#issuecomment-436700755", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407", "id": 436700755, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjcwMDc1NQ==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T17:08:51Z", "updated_at": "2018-11-07T17:08:51Z", "author_association": "MEMBER", "body_html": "<p>I meant replacing your static graph training code block with</p>\n<div class=\"highlight highlight-source-python\"><pre>            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Discriminator training</span>\n            x <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-c1\">...</span>.)\n            <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n\n                discriminator.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                disc_x <span class=\"pl-k\">=</span> tf.squeeze(discriminator(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n\n                disc_real_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator loss on result disc_x</span>\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.ones_like(disc_x), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x\n                )\n                g_encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n                g_decoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> recreate the data (=&gt; x_hat), starting from real data x</span>\n                z <span class=\"pl-k\">=</span> g_encoder(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not training</span>\n                x_hat <span class=\"pl-k\">=</span> g_decoder(z, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not training</span>\n\n                disc_x_hat <span class=\"pl-k\">=</span> tf.squeeze(discriminator(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n                disc_gen_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator loss on result disc_x_hat</span>\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.zeros_like(disc_x_hat), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x_hat\n                )\n                disc_loss <span class=\"pl-k\">=</span> disc_real_loss <span class=\"pl-k\">+</span> disc_gen_loss\n\n            discriminator_gradients <span class=\"pl-k\">=</span> tape.gradient(\n                disc_loss, discriminator.trainable_variables\n            )\n\n            d_loss <span class=\"pl-k\">=</span> disc_loss\n            train_d <span class=\"pl-k\">=</span> discriminator_optimizer.apply_gradients(\n                <span class=\"pl-c1\">zip</span>(discriminator_gradients, discriminator.trainable_variables)\n            )\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Generator Training</span>\n            <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> err_g_bce</span>\n                g_encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                g_decoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                z <span class=\"pl-k\">=</span> g_encoder(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                x_hat <span class=\"pl-k\">=</span> g_decoder(z, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                disc_x_hat <span class=\"pl-k\">=</span> tf.squeeze(\n                    discriminator(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]\n                ) \n                bce_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.ones_like(disc_x_hat),\n                    <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x_hat,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> G wants to generate reals so ones_like</span>\n                )\n\n                l1_loss <span class=\"pl-k\">=</span> tf.losses.absolute_difference(x, x_hat)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> err_g_enc</span>\n                z_hat <span class=\"pl-k\">=</span> encoder(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                l2_loss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(z, z_hat)\n\n                gen_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span><span class=\"pl-k\">*</span> bce_loss <span class=\"pl-k\">+</span> <span class=\"pl-c1\">50</span> <span class=\"pl-k\">*</span> l1_loss <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">*</span> l2_loss\n                \n            trainable_variable_list <span class=\"pl-k\">=</span> (\n                g_encoder.trainable_variables\n                <span class=\"pl-k\">+</span> g_decoder.trainable_variables\n                <span class=\"pl-k\">+</span> encoder.trainable_variables\n            )\n\n            generator_gradients <span class=\"pl-k\">=</span> tape.gradient(gen_loss, trainable_variable_list)\n\n           g_loss <span class=\"pl-k\">=</span> gen_loss\n            train_g <span class=\"pl-k\">=</span> generator_optimizer.apply_gradients(\n                <span class=\"pl-c1\">zip</span>(generator_gradients, trainable_variable_list),\n                <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step,\n            )</pre></div>\n<p>and then running your session.run loop for training unchanged:</p>\n<div class=\"highlight highlight-source-python\"><pre>            <span class=\"pl-c\"><span class=\"pl-c\">#</span> extract from tf.data.Dataset, x is a placeholder and x_ is the iterator.get_next</span>\n            real <span class=\"pl-k\">=</span> sess.run(x_) \n            feed_dict <span class=\"pl-k\">=</span> {x: real}\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> train D</span>\n            _, d_loss_value <span class=\"pl-k\">=</span> sess.run([train_d, d_loss], feed_dict)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> train G+E</span>\n            _, g_loss_value, step <span class=\"pl-k\">=</span> sess.run([train_g, g_loss, global_step],\n                                             feed_dict)</pre></div>", "body_text": "I meant replacing your static graph training code block with\n            # Discriminator training\n            x = tf.placeholder(....)\n            with tf.GradientTape() as tape:\n\n                discriminator.trainable = True\n                disc_x = tf.squeeze(discriminator(x, training=False), axis=[1, 2])\n\n                disc_real_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x\n                    multi_class_labels=tf.ones_like(disc_x), logits=disc_x\n                )\n                g_encoder.trainable = False\n                g_decoder.trainable = False\n                # recreate the data (=> x_hat), starting from real data x\n                z = g_encoder(x, training=True)  # Not training\n                x_hat = g_decoder(z, training=True)  # Not training\n\n                disc_x_hat = tf.squeeze(discriminator(x_hat, training=False), axis=[1, 2])\n                disc_gen_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x_hat\n                    multi_class_labels=tf.zeros_like(disc_x_hat), logits=disc_x_hat\n                )\n                disc_loss = disc_real_loss + disc_gen_loss\n\n            discriminator_gradients = tape.gradient(\n                disc_loss, discriminator.trainable_variables\n            )\n\n            d_loss = disc_loss\n            train_d = discriminator_optimizer.apply_gradients(\n                zip(discriminator_gradients, discriminator.trainable_variables)\n            )\n\n            # Generator Training\n            with tf.GradientTape() as tape:\n\n                # err_g_bce\n                g_encoder.trainable = True\n                g_decoder.trainable = True\n                encoder.trainable = True\n                z = g_encoder(x, training=True)\n                x_hat = g_decoder(z, training=True)\n                disc_x_hat = tf.squeeze(\n                    discriminator(x_hat, training=False), axis=[1, 2]\n                ) \n                bce_loss = tf.losses.sigmoid_cross_entropy(\n                    multi_class_labels=tf.ones_like(disc_x_hat),\n                    logits=disc_x_hat,  # G wants to generate reals so ones_like\n                )\n\n                l1_loss = tf.losses.absolute_difference(x, x_hat)\n                # err_g_enc\n                z_hat = encoder(x_hat, training=True)\n                l2_loss = tf.losses.mean_squared_error(z, z_hat)\n\n                gen_loss = 1* bce_loss + 50 * l1_loss + 1 * l2_loss\n                \n            trainable_variable_list = (\n                g_encoder.trainable_variables\n                + g_decoder.trainable_variables\n                + encoder.trainable_variables\n            )\n\n            generator_gradients = tape.gradient(gen_loss, trainable_variable_list)\n\n           g_loss = gen_loss\n            train_g = generator_optimizer.apply_gradients(\n                zip(generator_gradients, trainable_variable_list),\n                global_step=global_step,\n            )\nand then running your session.run loop for training unchanged:\n            # extract from tf.data.Dataset, x is a placeholder and x_ is the iterator.get_next\n            real = sess.run(x_) \n            feed_dict = {x: real}\n\n            # train D\n            _, d_loss_value = sess.run([train_d, d_loss], feed_dict)\n\n            # train G+E\n            _, g_loss_value, step = sess.run([train_g, g_loss, global_step],\n                                             feed_dict)", "body": "I meant replacing your static graph training code block with\r\n\r\n\r\n```python\r\n            # Discriminator training\r\n            x = tf.placeholder(....)\r\n            with tf.GradientTape() as tape:\r\n\r\n                discriminator.trainable = True\r\n                disc_x = tf.squeeze(discriminator(x, training=False), axis=[1, 2])\r\n\r\n                disc_real_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x\r\n                    multi_class_labels=tf.ones_like(disc_x), logits=disc_x\r\n                )\r\n                g_encoder.trainable = False\r\n                g_decoder.trainable = False\r\n                # recreate the data (=> x_hat), starting from real data x\r\n                z = g_encoder(x, training=True)  # Not training\r\n                x_hat = g_decoder(z, training=True)  # Not training\r\n\r\n                disc_x_hat = tf.squeeze(discriminator(x_hat, training=False), axis=[1, 2])\r\n                disc_gen_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x_hat\r\n                    multi_class_labels=tf.zeros_like(disc_x_hat), logits=disc_x_hat\r\n                )\r\n                disc_loss = disc_real_loss + disc_gen_loss\r\n\r\n            discriminator_gradients = tape.gradient(\r\n                disc_loss, discriminator.trainable_variables\r\n            )\r\n\r\n            d_loss = disc_loss\r\n            train_d = discriminator_optimizer.apply_gradients(\r\n                zip(discriminator_gradients, discriminator.trainable_variables)\r\n            )\r\n\r\n            # Generator Training\r\n            with tf.GradientTape() as tape:\r\n\r\n                # err_g_bce\r\n                g_encoder.trainable = True\r\n                g_decoder.trainable = True\r\n                encoder.trainable = True\r\n                z = g_encoder(x, training=True)\r\n                x_hat = g_decoder(z, training=True)\r\n                disc_x_hat = tf.squeeze(\r\n                    discriminator(x_hat, training=False), axis=[1, 2]\r\n                ) \r\n                bce_loss = tf.losses.sigmoid_cross_entropy(\r\n                    multi_class_labels=tf.ones_like(disc_x_hat),\r\n                    logits=disc_x_hat,  # G wants to generate reals so ones_like\r\n                )\r\n\r\n                l1_loss = tf.losses.absolute_difference(x, x_hat)\r\n                # err_g_enc\r\n                z_hat = encoder(x_hat, training=True)\r\n                l2_loss = tf.losses.mean_squared_error(z, z_hat)\r\n\r\n                gen_loss = 1* bce_loss + 50 * l1_loss + 1 * l2_loss\r\n                \r\n            trainable_variable_list = (\r\n                g_encoder.trainable_variables\r\n                + g_decoder.trainable_variables\r\n                + encoder.trainable_variables\r\n            )\r\n\r\n            generator_gradients = tape.gradient(gen_loss, trainable_variable_list)\r\n\r\n           g_loss = gen_loss\r\n            train_g = generator_optimizer.apply_gradients(\r\n                zip(generator_gradients, trainable_variable_list),\r\n                global_step=global_step,\r\n            )\r\n```\r\n\r\nand then running your session.run loop for training unchanged:\r\n```python\r\n            # extract from tf.data.Dataset, x is a placeholder and x_ is the iterator.get_next\r\n            real = sess.run(x_) \r\n            feed_dict = {x: real}\r\n\r\n            # train D\r\n            _, d_loss_value = sess.run([train_d, d_loss], feed_dict)\r\n\r\n            # train G+E\r\n            _, g_loss_value, step = sess.run([train_g, g_loss, global_step],\r\n                                             feed_dict)\r\n ```\r\n"}