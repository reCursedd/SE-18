{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/437279234", "html_url": "https://github.com/tensorflow/tensorflow/issues/23407#issuecomment-437279234", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407", "id": 437279234, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzI3OTIzNA==", "user": {"login": "galeone", "id": 8427788, "node_id": "MDQ6VXNlcjg0Mjc3ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/8427788?v=4", "gravatar_id": "", "url": "https://api.github.com/users/galeone", "html_url": "https://github.com/galeone", "followers_url": "https://api.github.com/users/galeone/followers", "following_url": "https://api.github.com/users/galeone/following{/other_user}", "gists_url": "https://api.github.com/users/galeone/gists{/gist_id}", "starred_url": "https://api.github.com/users/galeone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/galeone/subscriptions", "organizations_url": "https://api.github.com/users/galeone/orgs", "repos_url": "https://api.github.com/users/galeone/repos", "events_url": "https://api.github.com/users/galeone/events{/privacy}", "received_events_url": "https://api.github.com/users/galeone/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-09T07:56:03Z", "updated_at": "2018-11-09T07:56:03Z", "author_association": "NONE", "body_html": "<p>Yes, no problem.</p>\n<p>The models are the one posted earlier and are inside the <code>models/ganomaly.py</code> file.</p>\n<p>The dataset definition (common to both version too) is placed inside <code>ops/input_fn/fashion_mnist.py</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Load Fashion MNIST Dataset.<span class=\"pl-pds\">\"\"\"</span></span>\n\n<span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Callable, Dict, Tuple\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> sklearn.model_selection <span class=\"pl-k\">as</span> sk\n<span class=\"pl-k\">from</span> skimage.transform <span class=\"pl-k\">import</span> resize\n<span class=\"pl-k\">from</span> matplotlib <span class=\"pl-k\">import</span> pyplot <span class=\"pl-k\">as</span> plt\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.keras <span class=\"pl-k\">as</span> k\n\nDataset <span class=\"pl-k\">=</span> Dict[<span class=\"pl-c1\">str</span>, np.ndarray]\nCompleteDataset <span class=\"pl-k\">=</span> Dict[<span class=\"pl-c1\">str</span>, Dataset]\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">arrays_dataset_to_generator</span>(<span class=\"pl-smi\">dataset</span>: Dataset) -&gt; Callable:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> DOCUMENT</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_generator</span>():\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(dataset[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>])):\n            <span class=\"pl-k\">yield</span> dataset[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>][i], dataset[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>][i]\n\n    <span class=\"pl-k\">return</span> _generator\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">extract_label</span>(\n    <span class=\"pl-smi\">features</span>: np.ndarray, <span class=\"pl-smi\">labels</span>: np.ndarray, <span class=\"pl-smi\">target_label</span>: <span class=\"pl-c1\">int</span>\n) -&gt; Tuple[Dataset, Dataset]:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Extract all features whose label is ``target_labels``.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        features: Dataset features</span>\n<span class=\"pl-s\">        labels: Dataset labels</span>\n<span class=\"pl-s\">        target_label: The label used as filter</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        All the features with the ``target_labels`` as label.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    bool_filter <span class=\"pl-k\">=</span> labels <span class=\"pl-k\">==</span> target_label\n    normal_features <span class=\"pl-k\">=</span> features[<span class=\"pl-k\">~</span>bool_filter]\n    normal_labels <span class=\"pl-k\">=</span> labels[<span class=\"pl-k\">~</span>bool_filter]\n    anomalous_features <span class=\"pl-k\">=</span> features[bool_filter]\n    anomalous_labels <span class=\"pl-k\">=</span> labels[bool_filter]\n    <span class=\"pl-k\">return</span> (\n        {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>: normal_features, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: normal_labels},\n        {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>: anomalous_features, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: anomalous_labels},\n    )\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_process_features</span>(<span class=\"pl-smi\">feature</span>, <span class=\"pl-smi\">image_size</span>):\n    feature <span class=\"pl-k\">=</span> tf.squeeze(tf.image.resize_images(tf.expand_dims(tf.expand_dims(feature, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>), <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)),\n                         <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> resize change to float32</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Normalize features between -1 and 1</span>\n    feature <span class=\"pl-k\">=</span> (feature <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-k\">return</span> feature.numpy()\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">FashionMNIST</span>:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    DigitsMNIST Dataset.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Attributes:</span>\n<span class=\"pl-s\">        train_datasets (Dict[str, np.ndarray]): Dictionary of features for training..</span>\n<span class=\"pl-s\">            The dictionary has keys in the form of ``non_{label}`` meaning that each</span>\n<span class=\"pl-s\">            key holds the 80% of the features whose labels is not ``{label}``</span>\n<span class=\"pl-s\">        test_datasets (Dict[str, np.ndarray]): Dictionary of features for testing.</span>\n<span class=\"pl-s\">            The dictionary has keys in the form of ``only_{label}`` meaning that each</span>\n<span class=\"pl-s\">            key holds the all the features whose label is ``{label}`` and 20% of all</span>\n<span class=\"pl-s\">            the other features as non-anomalies</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    train_datasets: CompleteDataset\n    test_datasets: CompleteDataset\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> def __init__(self, image_size: Tuple[int, int, int] = (28, 28, 1)) -&gt; None:</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">image_size</span>: Tuple[<span class=\"pl-c1\">int</span>, <span class=\"pl-c1\">int</span>, <span class=\"pl-c1\">int</span>] <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">1</span>)) -&gt; <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Fetch the dataset and partition it for training and testing.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Retrieve the dataset from the Keras builtin, split each class 80-20 and</span>\n<span class=\"pl-s\">        using these splits build all the datasets required for training and testing.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Args:</span>\n<span class=\"pl-s\">            image_size: Size of each image</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Returns:</span>\n<span class=\"pl-s\">            None.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-c1\">self</span>.image_size <span class=\"pl-k\">=</span> image_size\n        <span class=\"pl-c1\">self</span>.train_datasets <span class=\"pl-k\">=</span> {}\n        <span class=\"pl-c1\">self</span>.test_datasets <span class=\"pl-k\">=</span> {}\n\n        mnist_train_set, _ <span class=\"pl-k\">=</span> k.datasets.fashion_mnist.load_data()\n        mnist_x, mnist_y <span class=\"pl-k\">=</span> mnist_train_set\n\n        mnist_x <span class=\"pl-k\">=</span> np.array([_process_features(x, <span class=\"pl-c1\">self</span>.image_size) <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> mnist_x])\n\n        <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(n)</span>\n            normal_data, anomalous_data <span class=\"pl-k\">=</span> extract_label(mnist_x, mnist_y, n)\n            n_train_x, n_test_x, n_train_y, n_test_y <span class=\"pl-k\">=</span> sk.train_test_split(\n                normal_data[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>], normal_data[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>], <span class=\"pl-v\">test_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.2</span>, <span class=\"pl-v\">random_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">42</span>\n            )\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(n_train_x.shape[0], n_test_x.shape[0], anomalous_data[\"x\"].shape[0])</span>\n\n            <span class=\"pl-c1\">self</span>.train_datasets[<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">anomaly_</span><span class=\"pl-c1\">{</span>n<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>] <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>: n_train_x, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: n_train_y}\n            <span class=\"pl-c1\">self</span>.test_datasets[<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">anomaly_</span><span class=\"pl-c1\">{</span>n<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>] <span class=\"pl-k\">=</span> {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>: np.concatenate((n_test_x, anomalous_data[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>])),\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: np.concatenate((n_test_y, anomalous_data[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>])),\n            }\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">to_tf_dataset</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">arrays_dataset</span>: Dataset, <span class=\"pl-smi\">hyper</span>: Dict) -&gt; tf.data.Dataset:\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Convert ``features`` and ``labels`` into a `tf.data.Datasets``.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Args:</span>\n<span class=\"pl-s\">            features: Array of features</span>\n<span class=\"pl-s\">            labels: Array of labels</span>\n<span class=\"pl-s\">            hyper: Dictionary of hyperparameters</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Returns:</span>\n<span class=\"pl-s\">            `tf.data.Dataset` of batch(image, label).</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        generator <span class=\"pl-k\">=</span> arrays_dataset_to_generator(arrays_dataset)\n        dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(generator, (tf.float32, tf.uint8))\n        dataset <span class=\"pl-k\">=</span> (\n            dataset.shuffle(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>buffer_size<span class=\"pl-pds\">\"</span></span>], <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">42</span>)\n            .batch(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>], <span class=\"pl-v\">drop_remainder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n            .prefetch(<span class=\"pl-c1\">1</span>)\n        )\n        dataset <span class=\"pl-k\">=</span> dataset.repeat(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epochs<span class=\"pl-pds\">\"</span></span>]) <span class=\"pl-k\">if</span> hyper.get(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epochs<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">else</span> dataset\n\n        <span class=\"pl-k\">return</span> dataset\n</pre></div>\n<h2>Eager version</h2>\n<p>This is the training definition:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> annotations\n\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Callable, Dict, Optional\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.keras <span class=\"pl-k\">as</span> k\n<span class=\"pl-k\">from</span> models.ganomaly <span class=\"pl-k\">import</span> Discriminator\n<span class=\"pl-k\">import</span> statistics <span class=\"pl-k\">as</span> s\n<span class=\"pl-k\">import</span> copy\n\n<span class=\"pl-c1\">LOGGED</span>: Dict <span class=\"pl-k\">=</span> {}\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">GANomaly</span>:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    GANomaly</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    g_encoder: k.models.Model  <span class=\"pl-c\"><span class=\"pl-c\">#</span> bow-tie encoder (encoder)</span>\n    g_decoder: k.models.Model  <span class=\"pl-c\"><span class=\"pl-c\">#</span> bow-tie decoder (generator)</span>\n    encoder: k.models.Model  <span class=\"pl-c\"><span class=\"pl-c\">#</span> encoder</span>\n    discriminator: k.models.Model  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator</span>\n\n    generator_optimizer: tf.train.AdamOptimizer\n    discriminator_optimizer: tf.train.AdamOptimizer\n\n    dataset: tf.data.Dataset\n\n    checkpoint: tf.train.Checkpoint\n    logging_fn: Optional[Callable]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(\n        <span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,\n        <span class=\"pl-smi\">g_encoder</span>: k.models.Model,\n        <span class=\"pl-smi\">g_decoder</span>: k.models.Model,\n        <span class=\"pl-smi\">encoder</span>: k.models.Model,\n        <span class=\"pl-smi\">discriminator</span>: k.models.Model,\n        <span class=\"pl-smi\">dataset</span>: tf.data.Dataset,\n        <span class=\"pl-smi\">model_dir</span>: <span class=\"pl-c1\">str</span>,\n        <span class=\"pl-smi\">hyper</span>: Dict,\n        <span class=\"pl-smi\">logging_fn</span>: Optional[Callable] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>,\n    ) -&gt; <span class=\"pl-c1\">None</span>:\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Model</span>\n        <span class=\"pl-c1\">self</span>.g_encoder <span class=\"pl-k\">=</span> g_encoder()\n        <span class=\"pl-c1\">self</span>.g_decoder <span class=\"pl-k\">=</span> g_decoder()\n        <span class=\"pl-c1\">self</span>.encoder <span class=\"pl-k\">=</span> encoder()\n        <span class=\"pl-c1\">self</span>.discriminator <span class=\"pl-k\">=</span> discriminator()\n        <span class=\"pl-c1\">self</span>.hyper <span class=\"pl-k\">=</span> hyper\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Optimizers</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> this is for g_encode, g_decoder and encoder</span>\n        <span class=\"pl-c1\">self</span>.global_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\n        <span class=\"pl-c1\">self</span>.learning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>learning_rate<span class=\"pl-pds\">\"</span></span>]\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> with decay, 2 optimizer</span>\n        <span class=\"pl-c1\">self</span>.generator_optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(\n            <span class=\"pl-c1\">self</span>.learning_rate, hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>beta1<span class=\"pl-pds\">\"</span></span>]\n        )\n\n        <span class=\"pl-c1\">self</span>.discriminator_optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(\n            <span class=\"pl-c1\">self</span>.learning_rate, hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>beta1<span class=\"pl-pds\">\"</span></span>]\n        )\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Data</span>\n        <span class=\"pl-c1\">self</span>.dataset <span class=\"pl-k\">=</span> dataset\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Checkpoints</span>\n        <span class=\"pl-c1\">self</span>.checkpoint <span class=\"pl-k\">=</span> tf.train.Checkpoint(\n            <span class=\"pl-v\">generator_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.generator_optimizer,\n            <span class=\"pl-v\">discriminator_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.discriminator_optimizer,\n            <span class=\"pl-v\">g_encoder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.g_encoder,\n            <span class=\"pl-v\">g_decoder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.g_decoder,\n            <span class=\"pl-v\">encoder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.encoder,\n            <span class=\"pl-v\">discriminator</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.discriminator,\n        )\n\n        <span class=\"pl-c1\">self</span>.checkpoint_prefix <span class=\"pl-k\">=</span> os.path.join(model_dir, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ckpt<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.summary_writer <span class=\"pl-k\">=</span> tf.contrib.summary.create_file_writer(\n            model_dir, <span class=\"pl-v\">flush_millis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10000</span>\n        )\n\n        <span class=\"pl-c1\">self</span>.saved_model_dir <span class=\"pl-k\">=</span> model_dir\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Logging</span>\n        <span class=\"pl-c1\">self</span>.logging_fn <span class=\"pl-k\">=</span> logging_fn\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>: reduce complexity</span>\n        <span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,\n        <span class=\"pl-smi\">steps_per_epoch</span>: <span class=\"pl-c1\">int</span>,\n        <span class=\"pl-smi\">batch_size</span>: <span class=\"pl-c1\">int</span>,\n        <span class=\"pl-smi\">noise_dims</span>: <span class=\"pl-c1\">int</span>,\n        <span class=\"pl-smi\">epochs</span>: <span class=\"pl-c1\">float</span>,\n        <span class=\"pl-smi\">checkpoint_frequency</span>: <span class=\"pl-c1\">int</span>,\n        <span class=\"pl-smi\">logging_enabled</span>: <span class=\"pl-c1\">bool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>,\n        <span class=\"pl-smi\">discriminator_passes</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>,\n    ) -&gt; <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> global_step = tf.train.get_or_create_global_step()</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> self.g_enc_dec_weights = []</span>\n        epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        epoch_start <span class=\"pl-k\">=</span> time.time()\n        <span class=\"pl-k\">for</span> x, _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.dataset:\n            tf.Print(<span class=\"pl-c1\">self</span>.global_step, [<span class=\"pl-c1\">self</span>.global_step], <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>GLOBAL STEP<span class=\"pl-pds\">\"</span></span>)\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">self</span>.global_step.numpy()) <span class=\"pl-k\">%</span> steps_per_epoch <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                tf.logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>---------------[NEW EPOCH]---------------<span class=\"pl-pds\">\"</span></span>)\n                tf.logging.info(<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">Current Epoch </span><span class=\"pl-c1\">{</span>epoch <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span><span class=\"pl-c1\">}</span><span class=\"pl-s\"> | Total Epochs: </span><span class=\"pl-c1\">{</span>epochs<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>)\n                tf.logging.info(\n                    <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">Step </span><span class=\"pl-c1\">{</span><span class=\"pl-c1\">self</span>.global_step.numpy()<span class=\"pl-c1\">}</span><span class=\"pl-s\"> </span><span class=\"pl-pds\">\"</span>\n                    <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">| Total Steps: </span><span class=\"pl-c1\">{</span>steps_per_epoch <span class=\"pl-k\">*</span> epochs<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>\n                )\n                tf.logging.info(<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">Epoch time: </span><span class=\"pl-c1\">{</span>time.time() <span class=\"pl-k\">-</span> epoch_start<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>)\n                epoch_start <span class=\"pl-k\">=</span> time.time()\n                epoch <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n            step_start <span class=\"pl-k\">=</span> time.time()\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Discriminator training</span>\n            <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n\n                <span class=\"pl-c1\">self</span>.discriminator.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                disc_x <span class=\"pl-k\">=</span> tf.squeeze(\n                    <span class=\"pl-c1\">self</span>.discriminator(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]\n                )  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator on real data x. Training on.</span>\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> I save the weights here because here I know they are not zero</span>\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">self</span>.global_step.numpy()) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Saving weights...<span class=\"pl-pds\">\"</span></span>)\n                    <span class=\"pl-c1\">self</span>.discriminator.save_weights()\n\n                disc_real_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator loss on result disc_x</span>\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.ones_like(disc_x), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x\n                )\n\n                <span class=\"pl-c1\">self</span>.g_encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n                <span class=\"pl-c1\">self</span>.g_decoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> recreate the data (=&gt; x_hat), starting from real data x</span>\n                z <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.g_encoder(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not training</span>\n                x_hat <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.g_decoder(z, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not training</span>\n\n                disc_x_hat <span class=\"pl-k\">=</span> tf.squeeze(\n                    <span class=\"pl-c1\">self</span>.discriminator(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]\n                )  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator on recreated data. Training on.</span>\n                disc_gen_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> discriminator loss on result disc_x_hat</span>\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.zeros_like(disc_x_hat), <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x_hat\n                )\n                disc_loss <span class=\"pl-k\">=</span> disc_real_loss <span class=\"pl-k\">+</span> disc_gen_loss\n\n            discriminator_gradients <span class=\"pl-k\">=</span> tape.gradient(\n                disc_loss, <span class=\"pl-c1\">self</span>.discriminator.trainable_variables\n            )\n\n            <span class=\"pl-c1\">self</span>.discriminator_optimizer.apply_gradients(\n                <span class=\"pl-c1\">zip</span>(discriminator_gradients, <span class=\"pl-c1\">self</span>.discriminator.trainable_variables)\n            )\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Generator Training</span>\n            <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> err_g_bce</span>\n                <span class=\"pl-c1\">self</span>.g_encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                <span class=\"pl-c1\">self</span>.g_decoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                <span class=\"pl-c1\">self</span>.encoder.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n                z <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.g_encoder(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                x_hat <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.g_decoder(z, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                disc_x_hat <span class=\"pl-k\">=</span> tf.squeeze(\n                    <span class=\"pl-c1\">self</span>.discriminator(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]\n                )  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Training false</span>\n                bce_loss <span class=\"pl-k\">=</span> tf.losses.sigmoid_cross_entropy(\n                    <span class=\"pl-v\">multi_class_labels</span><span class=\"pl-k\">=</span>tf.ones_like(disc_x_hat),\n                    <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>disc_x_hat,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> G wants to generate reals so ones_like</span>\n                )\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(\"disc_x_hat.shape::::::::\", disc_x_hat.shape)</span>\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> err_g_l1l</span>\n                l1_loss <span class=\"pl-k\">=</span> tf.losses.absolute_difference(x, x_hat)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> err_g_enc</span>\n                z_hat <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.encoder(x_hat, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                l2_loss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(z, z_hat)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> final generator loss</span>\n                gen_loss <span class=\"pl-k\">=</span> (\n                    <span class=\"pl-c1\">self</span>.hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>adversarial_w<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">*</span> bce_loss\n                    <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>contextual_w<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">*</span> l1_loss\n                    <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>encoder_w<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">*</span> l2_loss\n                )\n\n            trainable_variable_list <span class=\"pl-k\">=</span> (\n                <span class=\"pl-c1\">self</span>.g_encoder.trainable_variables\n                <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.g_decoder.trainable_variables\n                <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.encoder.trainable_variables\n            )\n            generator_gradients <span class=\"pl-k\">=</span> tape.gradient(gen_loss, trainable_variable_list)\n\n            <span class=\"pl-c1\">self</span>.generator_optimizer.apply_gradients(\n                <span class=\"pl-c1\">zip</span>(generator_gradients, trainable_variable_list),\n                <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.global_step,\n            )\n\n            <span class=\"pl-k\">if</span> disc_loss <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1e-4</span>:\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RESET WEIGHTS<span class=\"pl-pds\">\"</span></span>)\n                <span class=\"pl-c1\">self</span>.discriminator.reset_weights()\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> self.discriminator = Discriminator()</span>\n\n            step_time_delta <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> step_start\n\n            <span class=\"pl-c1\">LOGGED</span>.update(\n                {  <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">HACK</span>: Find better way</span>\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>generated_data<span class=\"pl-pds\">\"</span></span>: x_hat,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>real_data<span class=\"pl-pds\">\"</span></span>: x,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>encoded_real<span class=\"pl-pds\">\"</span></span>: z,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gen_loss<span class=\"pl-pds\">\"</span></span>: gen_loss,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>disc_loss<span class=\"pl-pds\">\"</span></span>: disc_loss,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gen_bce_loss<span class=\"pl-pds\">\"</span></span>: bce_loss,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gen_cont_loss<span class=\"pl-pds\">\"</span></span>: l1_loss,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gen_enc_loss<span class=\"pl-pds\">\"</span></span>: l2_loss,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>step<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">self</span>.global_step,\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> \"learning_rate\": self.learning_rate(),</span>\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>learning_rate<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">self</span>.learning_rate,\n                }\n            )\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>: Move the logging to a separate function</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>: Divide Epoch-wise logging from Step-wise logging</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>: Add support for metrics</span>\n\n            <span class=\"pl-k\">if</span> logging_enabled:\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.logging_fn:\n                    <span class=\"pl-c1\">self</span>.logging_fn(<span class=\"pl-c1\">self</span>.summary_writer, <span class=\"pl-c1\">LOGGED</span>)\n                <span class=\"pl-k\">else</span>:\n                    tf.logging.error(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Logging enabled but no logging_fn was provided.<span class=\"pl-pds\">\"</span></span>)\n\n        tf.logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span># ############ [Training Complete] ##############<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.checkpoint.save(<span class=\"pl-v\">file_prefix</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.checkpoint_prefix)</pre></div>\n<p>And this is the file that runs the train:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">Run the model.</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Optional\n\n<span class=\"pl-k\">import</span> fire\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">from</span> models.anogan <span class=\"pl-k\">import</span> AnoGAN\n<span class=\"pl-k\">import</span> GANomaly <span class=\"pl-k\">as</span> ganomaly\n<span class=\"pl-k\">import</span> models.ganomaly <span class=\"pl-k\">as</span> m_ganomaly\n\ntf.enable_eager_execution()\ntf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">gan</span>: <span class=\"pl-c1\">str</span>, <span class=\"pl-smi\">mnist_type</span>: Optional[<span class=\"pl-c1\">str</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>) -&gt; <span class=\"pl-c1\">None</span>:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Execute the training script.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>main -- checking fire arguments<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">if</span> gan.lower() <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dummy<span class=\"pl-pds\">\"</span></span>:\n        tf.logging.info(<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">Starting DummyGAN</span><span class=\"pl-pds\">\"</span>)\n        <span class=\"pl-k\">from</span> models.dummy <span class=\"pl-k\">import</span> Generator, Discriminator, stepwise_train_logging\n        <span class=\"pl-k\">from</span> ops.input_fn.dummy <span class=\"pl-k\">import</span> DummyData\n\n        hyper <span class=\"pl-k\">=</span> {\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mean<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">10.0</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>std<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">0.01</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>points<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">10000</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1000</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epochs<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">2200</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>buffer_size<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">10000</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>noise_dims<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">100</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>learning_rate<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">0.0002</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>beta1<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">0.5</span>,\n        }\n\n        config <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>checkpoint_frequency<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">500</span>}\n\n        dataset <span class=\"pl-k\">=</span> DummyData(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mean<span class=\"pl-pds\">\"</span></span>], hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>std<span class=\"pl-pds\">\"</span></span>], hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>points<span class=\"pl-pds\">\"</span></span>])\n        train_dataset <span class=\"pl-k\">=</span> dataset.to_tf_dataset(\n            dataset.train_features,\n            <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>],\n            <span class=\"pl-v\">buffer_size</span><span class=\"pl-k\">=</span>hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>buffer_size<span class=\"pl-pds\">\"</span></span>],\n            <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span>hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epochs<span class=\"pl-pds\">\"</span></span>],\n        )\n\n        dummy_gan <span class=\"pl-k\">=</span> AnoGAN(\n            Generator,\n            Discriminator,\n            train_dataset,\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>logs/dummy<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">logging_fn</span><span class=\"pl-k\">=</span>stepwise_train_logging,\n            <span class=\"pl-v\">hyper</span><span class=\"pl-k\">=</span>hyper,\n        )\n\n        dummy_gan.train(\n            <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>points<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">/</span> hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>]),\n            <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>]),\n            <span class=\"pl-v\">noise_dims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>noise_dims<span class=\"pl-pds\">\"</span></span>]),\n            <span class=\"pl-v\">checkpoint_frequency</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(config[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>checkpoint_frequency<span class=\"pl-pds\">\"</span></span>]),\n            <span class=\"pl-v\">logging_enabled</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span>hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epochs<span class=\"pl-pds\">\"</span></span>],\n        )\n\n    <span class=\"pl-k\">elif</span> (gan.lower() <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mnist<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">and</span> mnist_type <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Category: MNIST<span class=\"pl-pds\">\"</span></span>)\n        tf.logging.info(<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">Starting MnistGAN</span><span class=\"pl-pds\">\"</span>)\n        <span class=\"pl-k\">from</span> models.ganomaly <span class=\"pl-k\">import</span> stepwise_train_logging\n        <span class=\"pl-k\">from</span> ops.input_fn.fashion_mnist <span class=\"pl-k\">import</span> FashionMNIST\n        <span class=\"pl-k\">from</span> ops.input_fn.mnist <span class=\"pl-k\">import</span> DigitsMNIST\n\n        config <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>checkpoint_frequency<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1</span>}\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> ----------------------------------- fashion mnist</span>\n        <span class=\"pl-k\">if</span> mnist_type.lower() <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fashion<span class=\"pl-pds\">\"</span></span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Sub-Category: FASHION<span class=\"pl-pds\">\"</span></span>)\n\n            hyper <span class=\"pl-k\">=</span> {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">64</span>,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epochs<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">2000</span>,     <span class=\"pl-c\"><span class=\"pl-c\">#</span> 500</span>\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>buffer_size<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">6000</span>,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>noise_dims<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">100</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 128</span>\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dataset_length<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">6000</span>,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>learning_rate<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">0.0002</span>,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>beta1<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">0.5</span>,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>adversarial_w<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1</span>,    <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1</span>\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>contextual_w<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">50</span>,     <span class=\"pl-c\"><span class=\"pl-c\">#</span> 50</span>\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>encoder_w<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1</span>,        <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1</span>\n            }\n\n            dataset <span class=\"pl-k\">=</span> FashionMNIST()\n            n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            train_dataset <span class=\"pl-k\">=</span> dataset.to_tf_dataset(\n                dataset.train_datasets[<span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">anomaly_</span><span class=\"pl-c1\">{</span>n<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>],\n                <span class=\"pl-v\">hyper</span><span class=\"pl-k\">=</span>hyper,\n            )\n\n            dummy_gan <span class=\"pl-k\">=</span> ganomaly.GANomaly(\n                m_ganomaly.Encoder,\n                m_ganomaly.Decoder,\n                m_ganomaly.Encoder,\n                m_ganomaly.Discriminator,\n                train_dataset,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>logs/ganomaly<span class=\"pl-pds\">\"</span></span>,\n                <span class=\"pl-v\">logging_fn</span><span class=\"pl-k\">=</span>stepwise_train_logging,\n                <span class=\"pl-v\">hyper</span><span class=\"pl-k\">=</span>hyper,\n            )\n\n            dummy_gan.train(\n                <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dataset_length<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">/</span> hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>]),\n                <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_size<span class=\"pl-pds\">\"</span></span>]),\n                <span class=\"pl-v\">noise_dims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>noise_dims<span class=\"pl-pds\">\"</span></span>]),\n                <span class=\"pl-v\">checkpoint_frequency</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(config[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>checkpoint_frequency<span class=\"pl-pds\">\"</span></span>]),\n                <span class=\"pl-v\">logging_enabled</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span>hyper[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epochs<span class=\"pl-pds\">\"</span></span>],\n            )\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    fire.Fire(main)</pre></div>\n<p>^ This version produces the D collapse.</p>\n<p>While inside a session, (with ops generated by the eager as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> told me to do or using the static graph definition as I showed in the first post) it does not.</p>", "body_text": "Yes, no problem.\nThe models are the one posted earlier and are inside the models/ganomaly.py file.\nThe dataset definition (common to both version too) is placed inside ops/input_fn/fashion_mnist.py:\n\"\"\"Load Fashion MNIST Dataset.\"\"\"\n\nfrom typing import Callable, Dict, Tuple\n\nimport numpy as np\nimport sklearn.model_selection as sk\nfrom skimage.transform import resize\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as k\n\nDataset = Dict[str, np.ndarray]\nCompleteDataset = Dict[str, Dataset]\n\n\ndef arrays_dataset_to_generator(dataset: Dataset) -> Callable:\n    # DOCUMENT\n    def _generator():\n        for i in range(len(dataset[\"x\"])):\n            yield dataset[\"x\"][i], dataset[\"y\"][i]\n\n    return _generator\n\n\ndef extract_label(\n    features: np.ndarray, labels: np.ndarray, target_label: int\n) -> Tuple[Dataset, Dataset]:\n    \"\"\"\n    Extract all features whose label is ``target_labels``.\n\n    Args:\n        features: Dataset features\n        labels: Dataset labels\n        target_label: The label used as filter\n\n    Returns:\n        All the features with the ``target_labels`` as label.\n\n    \"\"\"\n    bool_filter = labels == target_label\n    normal_features = features[~bool_filter]\n    normal_labels = labels[~bool_filter]\n    anomalous_features = features[bool_filter]\n    anomalous_labels = labels[bool_filter]\n    return (\n        {\"x\": normal_features, \"y\": normal_labels},\n        {\"x\": anomalous_features, \"y\": anomalous_labels},\n    )\n\n\ndef _process_features(feature, image_size):\n    feature = tf.squeeze(tf.image.resize_images(tf.expand_dims(tf.expand_dims(feature, axis=2), axis=0), size=(32, 32)),\n                         axis=0)  # resize change to float32\n\n    # Normalize features between -1 and 1\n    feature = (feature * 2) - 1\n\n    return feature.numpy()\n\n\nclass FashionMNIST:\n    \"\"\"\n    DigitsMNIST Dataset.\n\n    Attributes:\n        train_datasets (Dict[str, np.ndarray]): Dictionary of features for training..\n            The dictionary has keys in the form of ``non_{label}`` meaning that each\n            key holds the 80% of the features whose labels is not ``{label}``\n        test_datasets (Dict[str, np.ndarray]): Dictionary of features for testing.\n            The dictionary has keys in the form of ``only_{label}`` meaning that each\n            key holds the all the features whose label is ``{label}`` and 20% of all\n            the other features as non-anomalies\n\n    \"\"\"\n\n    train_datasets: CompleteDataset\n    test_datasets: CompleteDataset\n\n    # def __init__(self, image_size: Tuple[int, int, int] = (28, 28, 1)) -> None:\n    def __init__(self, image_size: Tuple[int, int, int] = (32, 32, 1)) -> None:\n        \"\"\"\n        Fetch the dataset and partition it for training and testing.\n\n        Retrieve the dataset from the Keras builtin, split each class 80-20 and\n        using these splits build all the datasets required for training and testing.\n\n        Args:\n            image_size: Size of each image\n\n        Returns:\n            None.\n\n        \"\"\"\n        self.image_size = image_size\n        self.train_datasets = {}\n        self.test_datasets = {}\n\n        mnist_train_set, _ = k.datasets.fashion_mnist.load_data()\n        mnist_x, mnist_y = mnist_train_set\n\n        mnist_x = np.array([_process_features(x, self.image_size) for x in mnist_x])\n\n        for n in range(10):\n            # print(n)\n            normal_data, anomalous_data = extract_label(mnist_x, mnist_y, n)\n            n_train_x, n_test_x, n_train_y, n_test_y = sk.train_test_split(\n                normal_data[\"x\"], normal_data[\"y\"], test_size=0.2, random_state=42\n            )\n\n            # print(n_train_x.shape[0], n_test_x.shape[0], anomalous_data[\"x\"].shape[0])\n\n            self.train_datasets[f\"anomaly_{n}\"] = {\"x\": n_train_x, \"y\": n_train_y}\n            self.test_datasets[f\"anomaly_{n}\"] = {\n                \"x\": np.concatenate((n_test_x, anomalous_data[\"x\"])),\n                \"y\": np.concatenate((n_test_y, anomalous_data[\"y\"])),\n            }\n\n    def to_tf_dataset(self, arrays_dataset: Dataset, hyper: Dict) -> tf.data.Dataset:\n        \"\"\"\n        Convert ``features`` and ``labels`` into a `tf.data.Datasets``.\n\n        Args:\n            features: Array of features\n            labels: Array of labels\n            hyper: Dictionary of hyperparameters\n\n        Returns:\n            `tf.data.Dataset` of batch(image, label).\n\n        \"\"\"\n        generator = arrays_dataset_to_generator(arrays_dataset)\n        dataset = tf.data.Dataset.from_generator(generator, (tf.float32, tf.uint8))\n        dataset = (\n            dataset.shuffle(hyper[\"buffer_size\"], seed=42)\n            .batch(hyper[\"batch_size\"], drop_remainder=True)\n            .prefetch(1)\n        )\n        dataset = dataset.repeat(hyper[\"epochs\"]) if hyper.get(\"epochs\") else dataset\n\n        return dataset\n\nEager version\nThis is the training definition:\nfrom __future__ import annotations\n\nimport os\nimport time\nfrom typing import Callable, Dict, Optional\n\nimport tensorflow as tf\nimport tensorflow.keras as k\nfrom models.ganomaly import Discriminator\nimport statistics as s\nimport copy\n\nLOGGED: Dict = {}\n\n\nclass GANomaly:\n    \"\"\"\n    GANomaly\n    \"\"\"\n\n    g_encoder: k.models.Model  # bow-tie encoder (encoder)\n    g_decoder: k.models.Model  # bow-tie decoder (generator)\n    encoder: k.models.Model  # encoder\n    discriminator: k.models.Model  # discriminator\n\n    generator_optimizer: tf.train.AdamOptimizer\n    discriminator_optimizer: tf.train.AdamOptimizer\n\n    dataset: tf.data.Dataset\n\n    checkpoint: tf.train.Checkpoint\n    logging_fn: Optional[Callable]\n\n    def __init__(\n        self,\n        g_encoder: k.models.Model,\n        g_decoder: k.models.Model,\n        encoder: k.models.Model,\n        discriminator: k.models.Model,\n        dataset: tf.data.Dataset,\n        model_dir: str,\n        hyper: Dict,\n        logging_fn: Optional[Callable] = None,\n    ) -> None:\n\n        # Model\n        self.g_encoder = g_encoder()\n        self.g_decoder = g_decoder()\n        self.encoder = encoder()\n        self.discriminator = discriminator()\n        self.hyper = hyper\n\n        # Optimizers\n\n        # this is for g_encode, g_decoder and encoder\n        self.global_step = tf.train.get_or_create_global_step()\n        self.learning_rate = self.hyper[\"learning_rate\"]\n\n        # with decay, 2 optimizer\n        self.generator_optimizer = tf.train.AdamOptimizer(\n            self.learning_rate, hyper[\"beta1\"]\n        )\n\n        self.discriminator_optimizer = tf.train.AdamOptimizer(\n            self.learning_rate, hyper[\"beta1\"]\n        )\n        # Data\n        self.dataset = dataset\n\n        # Checkpoints\n        self.checkpoint = tf.train.Checkpoint(\n            generator_optimizer=self.generator_optimizer,\n            discriminator_optimizer=self.discriminator_optimizer,\n            g_encoder=self.g_encoder,\n            g_decoder=self.g_decoder,\n            encoder=self.encoder,\n            discriminator=self.discriminator,\n        )\n\n        self.checkpoint_prefix = os.path.join(model_dir, \"ckpt\")\n        self.summary_writer = tf.contrib.summary.create_file_writer(\n            model_dir, flush_millis=10000\n        )\n\n        self.saved_model_dir = model_dir\n\n        # Logging\n        self.logging_fn = logging_fn\n\n    def train(  # TODO: reduce complexity\n        self,\n        steps_per_epoch: int,\n        batch_size: int,\n        noise_dims: int,\n        epochs: float,\n        checkpoint_frequency: int,\n        logging_enabled: bool = False,\n        discriminator_passes: int = 2,\n    ) -> None:\n        # global_step = tf.train.get_or_create_global_step()\n\n        # self.g_enc_dec_weights = []\n        epoch = 0\n        epoch_start = time.time()\n        for x, _ in self.dataset:\n            tf.Print(self.global_step, [self.global_step], \"GLOBAL STEP\")\n            if int(self.global_step.numpy()) % steps_per_epoch == 0:\n                tf.logging.info(\"---------------[NEW EPOCH]---------------\")\n                tf.logging.info(f\"Current Epoch {epoch + 1} | Total Epochs: {epochs}\")\n                tf.logging.info(\n                    f\"Step {self.global_step.numpy()} \"\n                    f\"| Total Steps: {steps_per_epoch * epochs}\"\n                )\n                tf.logging.info(f\"Epoch time: {time.time() - epoch_start}\")\n                epoch_start = time.time()\n                epoch += 1\n            step_start = time.time()\n\n            # Discriminator training\n            with tf.GradientTape() as tape:\n\n                self.discriminator.trainable = True\n                disc_x = tf.squeeze(\n                    self.discriminator(x, training=False), axis=[1, 2]\n                )  # discriminator on real data x. Training on.\n\n                # I save the weights here because here I know they are not zero\n                if int(self.global_step.numpy()) == 0:\n                    print(\"Saving weights...\")\n                    self.discriminator.save_weights()\n\n                disc_real_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x\n                    multi_class_labels=tf.ones_like(disc_x), logits=disc_x\n                )\n\n                self.g_encoder.trainable = False\n                self.g_decoder.trainable = False\n                # recreate the data (=> x_hat), starting from real data x\n                z = self.g_encoder(x, training=True)  # Not training\n                x_hat = self.g_decoder(z, training=True)  # Not training\n\n                disc_x_hat = tf.squeeze(\n                    self.discriminator(x_hat, training=False), axis=[1, 2]\n                )  # discriminator on recreated data. Training on.\n                disc_gen_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x_hat\n                    multi_class_labels=tf.zeros_like(disc_x_hat), logits=disc_x_hat\n                )\n                disc_loss = disc_real_loss + disc_gen_loss\n\n            discriminator_gradients = tape.gradient(\n                disc_loss, self.discriminator.trainable_variables\n            )\n\n            self.discriminator_optimizer.apply_gradients(\n                zip(discriminator_gradients, self.discriminator.trainable_variables)\n            )\n\n            # Generator Training\n            with tf.GradientTape() as tape:\n\n                # err_g_bce\n                self.g_encoder.trainable = True\n                self.g_decoder.trainable = True\n                self.encoder.trainable = True\n                z = self.g_encoder(x, training=True)\n                x_hat = self.g_decoder(z, training=True)\n                disc_x_hat = tf.squeeze(\n                    self.discriminator(x_hat, training=False), axis=[1, 2]\n                )  # Training false\n                bce_loss = tf.losses.sigmoid_cross_entropy(\n                    multi_class_labels=tf.ones_like(disc_x_hat),\n                    logits=disc_x_hat,  # G wants to generate reals so ones_like\n                )\n\n                # print(\"disc_x_hat.shape::::::::\", disc_x_hat.shape)\n\n                # err_g_l1l\n                l1_loss = tf.losses.absolute_difference(x, x_hat)\n\n                # err_g_enc\n                z_hat = self.encoder(x_hat, training=True)\n                l2_loss = tf.losses.mean_squared_error(z, z_hat)\n\n                # final generator loss\n                gen_loss = (\n                    self.hyper[\"adversarial_w\"] * bce_loss\n                    + self.hyper[\"contextual_w\"] * l1_loss\n                    + self.hyper[\"encoder_w\"] * l2_loss\n                )\n\n            trainable_variable_list = (\n                self.g_encoder.trainable_variables\n                + self.g_decoder.trainable_variables\n                + self.encoder.trainable_variables\n            )\n            generator_gradients = tape.gradient(gen_loss, trainable_variable_list)\n\n            self.generator_optimizer.apply_gradients(\n                zip(generator_gradients, trainable_variable_list),\n                global_step=self.global_step,\n            )\n\n            if disc_loss < 1e-4:\n                print(\"RESET WEIGHTS\")\n                self.discriminator.reset_weights()\n                # self.discriminator = Discriminator()\n\n            step_time_delta = time.time() - step_start\n\n            LOGGED.update(\n                {  # HACK: Find better way\n                    \"generated_data\": x_hat,\n                    \"real_data\": x,\n                    \"encoded_real\": z,\n                    \"gen_loss\": gen_loss,\n                    \"disc_loss\": disc_loss,\n                    \"gen_bce_loss\": bce_loss,\n                    \"gen_cont_loss\": l1_loss,\n                    \"gen_enc_loss\": l2_loss,\n                    \"step\": self.global_step,\n                    # \"learning_rate\": self.learning_rate(),\n                    \"learning_rate\": self.learning_rate,\n                }\n            )\n\n            # TODO: Move the logging to a separate function\n            # TODO: Divide Epoch-wise logging from Step-wise logging\n            # TODO: Add support for metrics\n\n            if logging_enabled:\n                if self.logging_fn:\n                    self.logging_fn(self.summary_writer, LOGGED)\n                else:\n                    tf.logging.error(\"Logging enabled but no logging_fn was provided.\")\n\n        tf.logging.info(\"# ############ [Training Complete] ##############\")\n        self.checkpoint.save(file_prefix=self.checkpoint_prefix)\nAnd this is the file that runs the train:\n\"\"\"\nRun the model.\n\"\"\"\nfrom typing import Optional\n\nimport fire\nimport tensorflow as tf\n\nfrom models.anogan import AnoGAN\nimport GANomaly as ganomaly\nimport models.ganomaly as m_ganomaly\n\ntf.enable_eager_execution()\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef main(gan: str, mnist_type: Optional[str] = None) -> None:\n    \"\"\"Execute the training script.\"\"\"\n    print(\"main -- checking fire arguments\")\n    if gan.lower() == \"dummy\":\n        tf.logging.info(f\"Starting DummyGAN\")\n        from models.dummy import Generator, Discriminator, stepwise_train_logging\n        from ops.input_fn.dummy import DummyData\n\n        hyper = {\n            \"mean\": 10.0,\n            \"std\": 0.01,\n            \"points\": 10000,\n            \"batch_size\": 1000,\n            \"epochs\": 2200,\n            \"buffer_size\": 10000,\n            \"noise_dims\": 100,\n            \"learning_rate\": 0.0002,\n            \"beta1\": 0.5,\n        }\n\n        config = {\"checkpoint_frequency\": 500}\n\n        dataset = DummyData(hyper[\"mean\"], hyper[\"std\"], hyper[\"points\"])\n        train_dataset = dataset.to_tf_dataset(\n            dataset.train_features,\n            batch_size=hyper[\"batch_size\"],\n            buffer_size=hyper[\"buffer_size\"],\n            epochs=hyper[\"epochs\"],\n        )\n\n        dummy_gan = AnoGAN(\n            Generator,\n            Discriminator,\n            train_dataset,\n            \"logs/dummy\",\n            logging_fn=stepwise_train_logging,\n            hyper=hyper,\n        )\n\n        dummy_gan.train(\n            steps_per_epoch=int(hyper[\"points\"] / hyper[\"batch_size\"]),\n            batch_size=int(hyper[\"batch_size\"]),\n            noise_dims=int(hyper[\"noise_dims\"]),\n            checkpoint_frequency=int(config[\"checkpoint_frequency\"]),\n            logging_enabled=True,\n            epochs=hyper[\"epochs\"],\n        )\n\n    elif (gan.lower() == \"mnist\") and mnist_type is not None:\n        print(\"Category: MNIST\")\n        tf.logging.info(f\"Starting MnistGAN\")\n        from models.ganomaly import stepwise_train_logging\n        from ops.input_fn.fashion_mnist import FashionMNIST\n        from ops.input_fn.mnist import DigitsMNIST\n\n        config = {\"checkpoint_frequency\": 1}\n\n        # ----------------------------------- fashion mnist\n        if mnist_type.lower() == \"fashion\":\n            print(\"Sub-Category: FASHION\")\n\n            hyper = {\n                \"batch_size\": 64,\n                \"epochs\": 2000,     # 500\n                \"buffer_size\": 6000,\n                \"noise_dims\": 100,  # 128\n                \"dataset_length\": 6000,\n                \"learning_rate\": 0.0002,\n                \"beta1\": 0.5,\n                \"adversarial_w\": 1,    # 1\n                \"contextual_w\": 50,     # 50\n                \"encoder_w\": 1,        # 1\n            }\n\n            dataset = FashionMNIST()\n            n = 0\n            train_dataset = dataset.to_tf_dataset(\n                dataset.train_datasets[f\"anomaly_{n}\"],\n                hyper=hyper,\n            )\n\n            dummy_gan = ganomaly.GANomaly(\n                m_ganomaly.Encoder,\n                m_ganomaly.Decoder,\n                m_ganomaly.Encoder,\n                m_ganomaly.Discriminator,\n                train_dataset,\n                \"logs/ganomaly\",\n                logging_fn=stepwise_train_logging,\n                hyper=hyper,\n            )\n\n            dummy_gan.train(\n                steps_per_epoch=int(hyper[\"dataset_length\"] / hyper[\"batch_size\"]),\n                batch_size=int(hyper[\"batch_size\"]),\n                noise_dims=int(hyper[\"noise_dims\"]),\n                checkpoint_frequency=int(config[\"checkpoint_frequency\"]),\n                logging_enabled=True,\n                epochs=hyper[\"epochs\"],\n            )\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n^ This version produces the D collapse.\nWhile inside a session, (with ops generated by the eager as @alextp told me to do or using the static graph definition as I showed in the first post) it does not.", "body": "Yes, no problem.\r\n\r\nThe models are the one posted earlier and are inside the `models/ganomaly.py` file.\r\n\r\nThe dataset definition (common to both version too) is placed inside `ops/input_fn/fashion_mnist.py`:\r\n\r\n```python\r\n\"\"\"Load Fashion MNIST Dataset.\"\"\"\r\n\r\nfrom typing import Callable, Dict, Tuple\r\n\r\nimport numpy as np\r\nimport sklearn.model_selection as sk\r\nfrom skimage.transform import resize\r\nfrom matplotlib import pyplot as plt\r\nimport tensorflow as tf\r\nimport tensorflow.keras as k\r\n\r\nDataset = Dict[str, np.ndarray]\r\nCompleteDataset = Dict[str, Dataset]\r\n\r\n\r\ndef arrays_dataset_to_generator(dataset: Dataset) -> Callable:\r\n    # DOCUMENT\r\n    def _generator():\r\n        for i in range(len(dataset[\"x\"])):\r\n            yield dataset[\"x\"][i], dataset[\"y\"][i]\r\n\r\n    return _generator\r\n\r\n\r\ndef extract_label(\r\n    features: np.ndarray, labels: np.ndarray, target_label: int\r\n) -> Tuple[Dataset, Dataset]:\r\n    \"\"\"\r\n    Extract all features whose label is ``target_labels``.\r\n\r\n    Args:\r\n        features: Dataset features\r\n        labels: Dataset labels\r\n        target_label: The label used as filter\r\n\r\n    Returns:\r\n        All the features with the ``target_labels`` as label.\r\n\r\n    \"\"\"\r\n    bool_filter = labels == target_label\r\n    normal_features = features[~bool_filter]\r\n    normal_labels = labels[~bool_filter]\r\n    anomalous_features = features[bool_filter]\r\n    anomalous_labels = labels[bool_filter]\r\n    return (\r\n        {\"x\": normal_features, \"y\": normal_labels},\r\n        {\"x\": anomalous_features, \"y\": anomalous_labels},\r\n    )\r\n\r\n\r\ndef _process_features(feature, image_size):\r\n    feature = tf.squeeze(tf.image.resize_images(tf.expand_dims(tf.expand_dims(feature, axis=2), axis=0), size=(32, 32)),\r\n                         axis=0)  # resize change to float32\r\n\r\n    # Normalize features between -1 and 1\r\n    feature = (feature * 2) - 1\r\n\r\n    return feature.numpy()\r\n\r\n\r\nclass FashionMNIST:\r\n    \"\"\"\r\n    DigitsMNIST Dataset.\r\n\r\n    Attributes:\r\n        train_datasets (Dict[str, np.ndarray]): Dictionary of features for training..\r\n            The dictionary has keys in the form of ``non_{label}`` meaning that each\r\n            key holds the 80% of the features whose labels is not ``{label}``\r\n        test_datasets (Dict[str, np.ndarray]): Dictionary of features for testing.\r\n            The dictionary has keys in the form of ``only_{label}`` meaning that each\r\n            key holds the all the features whose label is ``{label}`` and 20% of all\r\n            the other features as non-anomalies\r\n\r\n    \"\"\"\r\n\r\n    train_datasets: CompleteDataset\r\n    test_datasets: CompleteDataset\r\n\r\n    # def __init__(self, image_size: Tuple[int, int, int] = (28, 28, 1)) -> None:\r\n    def __init__(self, image_size: Tuple[int, int, int] = (32, 32, 1)) -> None:\r\n        \"\"\"\r\n        Fetch the dataset and partition it for training and testing.\r\n\r\n        Retrieve the dataset from the Keras builtin, split each class 80-20 and\r\n        using these splits build all the datasets required for training and testing.\r\n\r\n        Args:\r\n            image_size: Size of each image\r\n\r\n        Returns:\r\n            None.\r\n\r\n        \"\"\"\r\n        self.image_size = image_size\r\n        self.train_datasets = {}\r\n        self.test_datasets = {}\r\n\r\n        mnist_train_set, _ = k.datasets.fashion_mnist.load_data()\r\n        mnist_x, mnist_y = mnist_train_set\r\n\r\n        mnist_x = np.array([_process_features(x, self.image_size) for x in mnist_x])\r\n\r\n        for n in range(10):\r\n            # print(n)\r\n            normal_data, anomalous_data = extract_label(mnist_x, mnist_y, n)\r\n            n_train_x, n_test_x, n_train_y, n_test_y = sk.train_test_split(\r\n                normal_data[\"x\"], normal_data[\"y\"], test_size=0.2, random_state=42\r\n            )\r\n\r\n            # print(n_train_x.shape[0], n_test_x.shape[0], anomalous_data[\"x\"].shape[0])\r\n\r\n            self.train_datasets[f\"anomaly_{n}\"] = {\"x\": n_train_x, \"y\": n_train_y}\r\n            self.test_datasets[f\"anomaly_{n}\"] = {\r\n                \"x\": np.concatenate((n_test_x, anomalous_data[\"x\"])),\r\n                \"y\": np.concatenate((n_test_y, anomalous_data[\"y\"])),\r\n            }\r\n\r\n    def to_tf_dataset(self, arrays_dataset: Dataset, hyper: Dict) -> tf.data.Dataset:\r\n        \"\"\"\r\n        Convert ``features`` and ``labels`` into a `tf.data.Datasets``.\r\n\r\n        Args:\r\n            features: Array of features\r\n            labels: Array of labels\r\n            hyper: Dictionary of hyperparameters\r\n\r\n        Returns:\r\n            `tf.data.Dataset` of batch(image, label).\r\n\r\n        \"\"\"\r\n        generator = arrays_dataset_to_generator(arrays_dataset)\r\n        dataset = tf.data.Dataset.from_generator(generator, (tf.float32, tf.uint8))\r\n        dataset = (\r\n            dataset.shuffle(hyper[\"buffer_size\"], seed=42)\r\n            .batch(hyper[\"batch_size\"], drop_remainder=True)\r\n            .prefetch(1)\r\n        )\r\n        dataset = dataset.repeat(hyper[\"epochs\"]) if hyper.get(\"epochs\") else dataset\r\n\r\n        return dataset\r\n\r\n```\r\n\r\n## Eager version\r\n\r\nThis is the training definition: \r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nimport os\r\nimport time\r\nfrom typing import Callable, Dict, Optional\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras as k\r\nfrom models.ganomaly import Discriminator\r\nimport statistics as s\r\nimport copy\r\n\r\nLOGGED: Dict = {}\r\n\r\n\r\nclass GANomaly:\r\n    \"\"\"\r\n    GANomaly\r\n    \"\"\"\r\n\r\n    g_encoder: k.models.Model  # bow-tie encoder (encoder)\r\n    g_decoder: k.models.Model  # bow-tie decoder (generator)\r\n    encoder: k.models.Model  # encoder\r\n    discriminator: k.models.Model  # discriminator\r\n\r\n    generator_optimizer: tf.train.AdamOptimizer\r\n    discriminator_optimizer: tf.train.AdamOptimizer\r\n\r\n    dataset: tf.data.Dataset\r\n\r\n    checkpoint: tf.train.Checkpoint\r\n    logging_fn: Optional[Callable]\r\n\r\n    def __init__(\r\n        self,\r\n        g_encoder: k.models.Model,\r\n        g_decoder: k.models.Model,\r\n        encoder: k.models.Model,\r\n        discriminator: k.models.Model,\r\n        dataset: tf.data.Dataset,\r\n        model_dir: str,\r\n        hyper: Dict,\r\n        logging_fn: Optional[Callable] = None,\r\n    ) -> None:\r\n\r\n        # Model\r\n        self.g_encoder = g_encoder()\r\n        self.g_decoder = g_decoder()\r\n        self.encoder = encoder()\r\n        self.discriminator = discriminator()\r\n        self.hyper = hyper\r\n\r\n        # Optimizers\r\n\r\n        # this is for g_encode, g_decoder and encoder\r\n        self.global_step = tf.train.get_or_create_global_step()\r\n        self.learning_rate = self.hyper[\"learning_rate\"]\r\n\r\n        # with decay, 2 optimizer\r\n        self.generator_optimizer = tf.train.AdamOptimizer(\r\n            self.learning_rate, hyper[\"beta1\"]\r\n        )\r\n\r\n        self.discriminator_optimizer = tf.train.AdamOptimizer(\r\n            self.learning_rate, hyper[\"beta1\"]\r\n        )\r\n        # Data\r\n        self.dataset = dataset\r\n\r\n        # Checkpoints\r\n        self.checkpoint = tf.train.Checkpoint(\r\n            generator_optimizer=self.generator_optimizer,\r\n            discriminator_optimizer=self.discriminator_optimizer,\r\n            g_encoder=self.g_encoder,\r\n            g_decoder=self.g_decoder,\r\n            encoder=self.encoder,\r\n            discriminator=self.discriminator,\r\n        )\r\n\r\n        self.checkpoint_prefix = os.path.join(model_dir, \"ckpt\")\r\n        self.summary_writer = tf.contrib.summary.create_file_writer(\r\n            model_dir, flush_millis=10000\r\n        )\r\n\r\n        self.saved_model_dir = model_dir\r\n\r\n        # Logging\r\n        self.logging_fn = logging_fn\r\n\r\n    def train(  # TODO: reduce complexity\r\n        self,\r\n        steps_per_epoch: int,\r\n        batch_size: int,\r\n        noise_dims: int,\r\n        epochs: float,\r\n        checkpoint_frequency: int,\r\n        logging_enabled: bool = False,\r\n        discriminator_passes: int = 2,\r\n    ) -> None:\r\n        # global_step = tf.train.get_or_create_global_step()\r\n\r\n        # self.g_enc_dec_weights = []\r\n        epoch = 0\r\n        epoch_start = time.time()\r\n        for x, _ in self.dataset:\r\n            tf.Print(self.global_step, [self.global_step], \"GLOBAL STEP\")\r\n            if int(self.global_step.numpy()) % steps_per_epoch == 0:\r\n                tf.logging.info(\"---------------[NEW EPOCH]---------------\")\r\n                tf.logging.info(f\"Current Epoch {epoch + 1} | Total Epochs: {epochs}\")\r\n                tf.logging.info(\r\n                    f\"Step {self.global_step.numpy()} \"\r\n                    f\"| Total Steps: {steps_per_epoch * epochs}\"\r\n                )\r\n                tf.logging.info(f\"Epoch time: {time.time() - epoch_start}\")\r\n                epoch_start = time.time()\r\n                epoch += 1\r\n            step_start = time.time()\r\n\r\n            # Discriminator training\r\n            with tf.GradientTape() as tape:\r\n\r\n                self.discriminator.trainable = True\r\n                disc_x = tf.squeeze(\r\n                    self.discriminator(x, training=False), axis=[1, 2]\r\n                )  # discriminator on real data x. Training on.\r\n\r\n                # I save the weights here because here I know they are not zero\r\n                if int(self.global_step.numpy()) == 0:\r\n                    print(\"Saving weights...\")\r\n                    self.discriminator.save_weights()\r\n\r\n                disc_real_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x\r\n                    multi_class_labels=tf.ones_like(disc_x), logits=disc_x\r\n                )\r\n\r\n                self.g_encoder.trainable = False\r\n                self.g_decoder.trainable = False\r\n                # recreate the data (=> x_hat), starting from real data x\r\n                z = self.g_encoder(x, training=True)  # Not training\r\n                x_hat = self.g_decoder(z, training=True)  # Not training\r\n\r\n                disc_x_hat = tf.squeeze(\r\n                    self.discriminator(x_hat, training=False), axis=[1, 2]\r\n                )  # discriminator on recreated data. Training on.\r\n                disc_gen_loss = tf.losses.sigmoid_cross_entropy(  # discriminator loss on result disc_x_hat\r\n                    multi_class_labels=tf.zeros_like(disc_x_hat), logits=disc_x_hat\r\n                )\r\n                disc_loss = disc_real_loss + disc_gen_loss\r\n\r\n            discriminator_gradients = tape.gradient(\r\n                disc_loss, self.discriminator.trainable_variables\r\n            )\r\n\r\n            self.discriminator_optimizer.apply_gradients(\r\n                zip(discriminator_gradients, self.discriminator.trainable_variables)\r\n            )\r\n\r\n            # Generator Training\r\n            with tf.GradientTape() as tape:\r\n\r\n                # err_g_bce\r\n                self.g_encoder.trainable = True\r\n                self.g_decoder.trainable = True\r\n                self.encoder.trainable = True\r\n                z = self.g_encoder(x, training=True)\r\n                x_hat = self.g_decoder(z, training=True)\r\n                disc_x_hat = tf.squeeze(\r\n                    self.discriminator(x_hat, training=False), axis=[1, 2]\r\n                )  # Training false\r\n                bce_loss = tf.losses.sigmoid_cross_entropy(\r\n                    multi_class_labels=tf.ones_like(disc_x_hat),\r\n                    logits=disc_x_hat,  # G wants to generate reals so ones_like\r\n                )\r\n\r\n                # print(\"disc_x_hat.shape::::::::\", disc_x_hat.shape)\r\n\r\n                # err_g_l1l\r\n                l1_loss = tf.losses.absolute_difference(x, x_hat)\r\n\r\n                # err_g_enc\r\n                z_hat = self.encoder(x_hat, training=True)\r\n                l2_loss = tf.losses.mean_squared_error(z, z_hat)\r\n\r\n                # final generator loss\r\n                gen_loss = (\r\n                    self.hyper[\"adversarial_w\"] * bce_loss\r\n                    + self.hyper[\"contextual_w\"] * l1_loss\r\n                    + self.hyper[\"encoder_w\"] * l2_loss\r\n                )\r\n\r\n            trainable_variable_list = (\r\n                self.g_encoder.trainable_variables\r\n                + self.g_decoder.trainable_variables\r\n                + self.encoder.trainable_variables\r\n            )\r\n            generator_gradients = tape.gradient(gen_loss, trainable_variable_list)\r\n\r\n            self.generator_optimizer.apply_gradients(\r\n                zip(generator_gradients, trainable_variable_list),\r\n                global_step=self.global_step,\r\n            )\r\n\r\n            if disc_loss < 1e-4:\r\n                print(\"RESET WEIGHTS\")\r\n                self.discriminator.reset_weights()\r\n                # self.discriminator = Discriminator()\r\n\r\n            step_time_delta = time.time() - step_start\r\n\r\n            LOGGED.update(\r\n                {  # HACK: Find better way\r\n                    \"generated_data\": x_hat,\r\n                    \"real_data\": x,\r\n                    \"encoded_real\": z,\r\n                    \"gen_loss\": gen_loss,\r\n                    \"disc_loss\": disc_loss,\r\n                    \"gen_bce_loss\": bce_loss,\r\n                    \"gen_cont_loss\": l1_loss,\r\n                    \"gen_enc_loss\": l2_loss,\r\n                    \"step\": self.global_step,\r\n                    # \"learning_rate\": self.learning_rate(),\r\n                    \"learning_rate\": self.learning_rate,\r\n                }\r\n            )\r\n\r\n            # TODO: Move the logging to a separate function\r\n            # TODO: Divide Epoch-wise logging from Step-wise logging\r\n            # TODO: Add support for metrics\r\n\r\n            if logging_enabled:\r\n                if self.logging_fn:\r\n                    self.logging_fn(self.summary_writer, LOGGED)\r\n                else:\r\n                    tf.logging.error(\"Logging enabled but no logging_fn was provided.\")\r\n\r\n        tf.logging.info(\"# ############ [Training Complete] ##############\")\r\n        self.checkpoint.save(file_prefix=self.checkpoint_prefix)\r\n```\r\n\r\nAnd this is the file that runs the train:\r\n\r\n```python\r\n\"\"\"\r\nRun the model.\r\n\"\"\"\r\nfrom typing import Optional\r\n\r\nimport fire\r\nimport tensorflow as tf\r\n\r\nfrom models.anogan import AnoGAN\r\nimport GANomaly as ganomaly\r\nimport models.ganomaly as m_ganomaly\r\n\r\ntf.enable_eager_execution()\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n\r\ndef main(gan: str, mnist_type: Optional[str] = None) -> None:\r\n    \"\"\"Execute the training script.\"\"\"\r\n    print(\"main -- checking fire arguments\")\r\n    if gan.lower() == \"dummy\":\r\n        tf.logging.info(f\"Starting DummyGAN\")\r\n        from models.dummy import Generator, Discriminator, stepwise_train_logging\r\n        from ops.input_fn.dummy import DummyData\r\n\r\n        hyper = {\r\n            \"mean\": 10.0,\r\n            \"std\": 0.01,\r\n            \"points\": 10000,\r\n            \"batch_size\": 1000,\r\n            \"epochs\": 2200,\r\n            \"buffer_size\": 10000,\r\n            \"noise_dims\": 100,\r\n            \"learning_rate\": 0.0002,\r\n            \"beta1\": 0.5,\r\n        }\r\n\r\n        config = {\"checkpoint_frequency\": 500}\r\n\r\n        dataset = DummyData(hyper[\"mean\"], hyper[\"std\"], hyper[\"points\"])\r\n        train_dataset = dataset.to_tf_dataset(\r\n            dataset.train_features,\r\n            batch_size=hyper[\"batch_size\"],\r\n            buffer_size=hyper[\"buffer_size\"],\r\n            epochs=hyper[\"epochs\"],\r\n        )\r\n\r\n        dummy_gan = AnoGAN(\r\n            Generator,\r\n            Discriminator,\r\n            train_dataset,\r\n            \"logs/dummy\",\r\n            logging_fn=stepwise_train_logging,\r\n            hyper=hyper,\r\n        )\r\n\r\n        dummy_gan.train(\r\n            steps_per_epoch=int(hyper[\"points\"] / hyper[\"batch_size\"]),\r\n            batch_size=int(hyper[\"batch_size\"]),\r\n            noise_dims=int(hyper[\"noise_dims\"]),\r\n            checkpoint_frequency=int(config[\"checkpoint_frequency\"]),\r\n            logging_enabled=True,\r\n            epochs=hyper[\"epochs\"],\r\n        )\r\n\r\n    elif (gan.lower() == \"mnist\") and mnist_type is not None:\r\n        print(\"Category: MNIST\")\r\n        tf.logging.info(f\"Starting MnistGAN\")\r\n        from models.ganomaly import stepwise_train_logging\r\n        from ops.input_fn.fashion_mnist import FashionMNIST\r\n        from ops.input_fn.mnist import DigitsMNIST\r\n\r\n        config = {\"checkpoint_frequency\": 1}\r\n\r\n        # ----------------------------------- fashion mnist\r\n        if mnist_type.lower() == \"fashion\":\r\n            print(\"Sub-Category: FASHION\")\r\n\r\n            hyper = {\r\n                \"batch_size\": 64,\r\n                \"epochs\": 2000,     # 500\r\n                \"buffer_size\": 6000,\r\n                \"noise_dims\": 100,  # 128\r\n                \"dataset_length\": 6000,\r\n                \"learning_rate\": 0.0002,\r\n                \"beta1\": 0.5,\r\n                \"adversarial_w\": 1,    # 1\r\n                \"contextual_w\": 50,     # 50\r\n                \"encoder_w\": 1,        # 1\r\n            }\r\n\r\n            dataset = FashionMNIST()\r\n            n = 0\r\n            train_dataset = dataset.to_tf_dataset(\r\n                dataset.train_datasets[f\"anomaly_{n}\"],\r\n                hyper=hyper,\r\n            )\r\n\r\n            dummy_gan = ganomaly.GANomaly(\r\n                m_ganomaly.Encoder,\r\n                m_ganomaly.Decoder,\r\n                m_ganomaly.Encoder,\r\n                m_ganomaly.Discriminator,\r\n                train_dataset,\r\n                \"logs/ganomaly\",\r\n                logging_fn=stepwise_train_logging,\r\n                hyper=hyper,\r\n            )\r\n\r\n            dummy_gan.train(\r\n                steps_per_epoch=int(hyper[\"dataset_length\"] / hyper[\"batch_size\"]),\r\n                batch_size=int(hyper[\"batch_size\"]),\r\n                noise_dims=int(hyper[\"noise_dims\"]),\r\n                checkpoint_frequency=int(config[\"checkpoint_frequency\"]),\r\n                logging_enabled=True,\r\n                epochs=hyper[\"epochs\"],\r\n            )\r\n\r\nif __name__ == \"__main__\":\r\n    fire.Fire(main)\r\n```\r\n\r\n^ This version produces the D collapse.\r\n\r\nWhile inside a session, (with ops generated by the eager as @alextp told me to do or using the static graph definition as I showed in the first post) it does not."}