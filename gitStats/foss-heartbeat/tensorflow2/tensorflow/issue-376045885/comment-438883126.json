{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/438883126", "html_url": "https://github.com/tensorflow/tensorflow/issues/23407#issuecomment-438883126", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23407", "id": 438883126, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODg4MzEyNg==", "user": {"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-15T01:33:58Z", "updated_at": "2018-11-15T01:33:58Z", "author_association": "MEMBER", "body_html": "<p>Ah, so the loss is similar for me with/without scaling (which at the very least means we are on the same page with regards to reproducibility :). The loss is small, but I'd expect 0.0s or NaNs if the model had collapsed, so I was saying that this hadn't collapsed.</p>\n<p>Is it possible that the datasets and preprocessing steps are different between graph and eager? If so, can you try the graph version with an identical dataset?</p>", "body_text": "Ah, so the loss is similar for me with/without scaling (which at the very least means we are on the same page with regards to reproducibility :). The loss is small, but I'd expect 0.0s or NaNs if the model had collapsed, so I was saying that this hadn't collapsed.\nIs it possible that the datasets and preprocessing steps are different between graph and eager? If so, can you try the graph version with an identical dataset?", "body": "Ah, so the loss is similar for me with/without scaling (which at the very least means we are on the same page with regards to reproducibility :). The loss is small, but I'd expect 0.0s or NaNs if the model had collapsed, so I was saying that this hadn't collapsed.\r\n\r\nIs it possible that the datasets and preprocessing steps are different between graph and eager? If so, can you try the graph version with an identical dataset?"}