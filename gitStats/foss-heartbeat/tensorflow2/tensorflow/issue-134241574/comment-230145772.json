{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/230145772", "html_url": "https://github.com/tensorflow/tensorflow/issues/1140#issuecomment-230145772", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1140", "id": 230145772, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMDE0NTc3Mg==", "user": {"login": "ibab", "id": 890531, "node_id": "MDQ6VXNlcjg5MDUzMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/890531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ibab", "html_url": "https://github.com/ibab", "followers_url": "https://api.github.com/users/ibab/followers", "following_url": "https://api.github.com/users/ibab/following{/other_user}", "gists_url": "https://api.github.com/users/ibab/gists{/gist_id}", "starred_url": "https://api.github.com/users/ibab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ibab/subscriptions", "organizations_url": "https://api.github.com/users/ibab/orgs", "repos_url": "https://api.github.com/users/ibab/repos", "events_url": "https://api.github.com/users/ibab/events{/privacy}", "received_events_url": "https://api.github.com/users/ibab/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-03T10:13:21Z", "updated_at": "2016-07-03T10:15:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11607205\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samjabrahams\">@samjabrahams</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2789456\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/siddharth-agrawal\">@siddharth-agrawal</a> I think the reason is that if there is no registered GPU kernel for an op, then that op will give you an error if you execute it inside a <code>with tf.device('/gpu:0'):</code> block, which is quite annoying.</p>\n<p>There are a few situations where it doesn't really make sense to have a real GPU op, and there might be ops for which a proper GPU kernel hasn't been written yet and we want to substitute an existing CPU op.<br>\nThe solution to the above problem is to register a \"fake\" GPU kernel that actually just executes the regular CPU op.<br>\nThis is done by placing all input/output tensors into host memory using the <code>.HostMemory(\"x\")</code> kernel registration method and registering the CPU op as the implementation.<br>\nWe have to use <code>.HostMemory</code>, because otherwise TensorFlow will automatically copy the data into GPU memory, where our CPU kernel can't reach it.</p>\n<p>A good example that I've found is the <code>InvertPermutation</code> kernel here: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/kernels/transpose_op.cc#L77\">tensorflow/tensorflow/core/kernels/transpose_op.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 77\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/d42facc3cc9611f0c9722c81551a7404a0bd3f6b\">d42facc</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L77\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"77\"></td>\n          <td id=\"LC77\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-en\">REGISTER_KERNEL_BUILDER</span>(Name(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>InvertPermutation<span class=\"pl-pds\">\"</span></span>) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nIt's expected that it will be called on an extremely small array (with length equal to the rank of some tensor), so we don't want it to be a real GPU kernel, as we want to avoid needlessly copying our tiny array to the device (and the result back to the host).<br>\nAt the same time, we want to be able to run inside a <code>tf.device('/gpu:0')</code> block, so we make a fake GPU kernel registration.</p>", "body_text": "@samjabrahams @siddharth-agrawal I think the reason is that if there is no registered GPU kernel for an op, then that op will give you an error if you execute it inside a with tf.device('/gpu:0'): block, which is quite annoying.\nThere are a few situations where it doesn't really make sense to have a real GPU op, and there might be ops for which a proper GPU kernel hasn't been written yet and we want to substitute an existing CPU op.\nThe solution to the above problem is to register a \"fake\" GPU kernel that actually just executes the regular CPU op.\nThis is done by placing all input/output tensors into host memory using the .HostMemory(\"x\") kernel registration method and registering the CPU op as the implementation.\nWe have to use .HostMemory, because otherwise TensorFlow will automatically copy the data into GPU memory, where our CPU kernel can't reach it.\nA good example that I've found is the InvertPermutation kernel here: \n  \n    \n      tensorflow/tensorflow/core/kernels/transpose_op.cc\n    \n    \n         Line 77\n      in\n      d42facc\n    \n    \n    \n    \n\n        \n          \n           REGISTER_KERNEL_BUILDER(Name(\"InvertPermutation\") \n        \n    \n  \n\n\nIt's expected that it will be called on an extremely small array (with length equal to the rank of some tensor), so we don't want it to be a real GPU kernel, as we want to avoid needlessly copying our tiny array to the device (and the result back to the host).\nAt the same time, we want to be able to run inside a tf.device('/gpu:0') block, so we make a fake GPU kernel registration.", "body": "@samjabrahams @siddharth-agrawal I think the reason is that if there is no registered GPU kernel for an op, then that op will give you an error if you execute it inside a `with tf.device('/gpu:0'):` block, which is quite annoying.\n\nThere are a few situations where it doesn't really make sense to have a real GPU op, and there might be ops for which a proper GPU kernel hasn't been written yet and we want to substitute an existing CPU op.\nThe solution to the above problem is to register a \"fake\" GPU kernel that actually just executes the regular CPU op.\nThis is done by placing all input/output tensors into host memory using the `.HostMemory(\"x\")` kernel registration method and registering the CPU op as the implementation.\nWe have to use `.HostMemory`, because otherwise TensorFlow will automatically copy the data into GPU memory, where our CPU kernel can't reach it.\n\nA good example that I've found is the `InvertPermutation` kernel here: https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/kernels/transpose_op.cc#L77\nIt's expected that it will be called on an extremely small array (with length equal to the rank of some tensor), so we don't want it to be a real GPU kernel, as we want to avoid needlessly copying our tiny array to the device (and the result back to the host).\nAt the same time, we want to be able to run inside a `tf.device('/gpu:0')` block, so we make a fake GPU kernel registration.\n"}