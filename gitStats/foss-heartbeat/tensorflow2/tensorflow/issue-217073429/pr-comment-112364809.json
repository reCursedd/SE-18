{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/112364809", "pull_request_review_id": 33672465, "id": 112364809, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMjM2NDgwOQ==", "diff_hunk": "@@ -0,0 +1,108 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/io_ops.cc.\n+\n+#include <memory>\n+#include <zmq.h>\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+\n+namespace tensorflow {\n+\n+class PollZmqOp : public AsyncOpKernel {\n+ public:\n+  explicit PollZmqOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"address\", &address_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"timeout\", &timeout_));\n+    // Create a context and connect a REQ socket\n+    context_ = zmq_ctx_new();\n+    OP_REQUIRES(context, context_ != nullptr,\n+                errors::Internal(\"Failed to initialize context.\"));\n+    socket_ = zmq_socket(context_, ZMQ_REQ);\n+    OP_REQUIRES(context, socket_ != nullptr,\n+                errors::Internal(\"Failed to initialize socket.\"));\n+    OP_REQUIRES(context,\n+                zmq_setsockopt(socket_, ZMQ_RCVTIMEO,\n+                               &timeout_, sizeof(timeout_)) == 0,\n+                errors::Internal(\"Failed to set timeout.\"));\n+    OP_REQUIRES(context, zmq_connect(socket_, &address_[0]) == 0,\n+                errors::Internal(\"Failed to connect to \", address_, \".\"));\n+  }\n+\n+  ~PollZmqOp() {\n+    if (socket_ != nullptr) {\n+      zmq_close(socket_);\n+    }\n+    if (context_ != nullptr) {\n+      zmq_ctx_destroy(context_);\n+    }\n+  }\n+\n+  using AsyncOpKernel::AsyncOpKernel;\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor* input;\n+    OP_REQUIRES_OK(context, context->input(\"request\", &input));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(input->shape()),\n+                errors::InvalidArgument(\n+                    \"Input message tensor must be scalar, but had shape: \",\n+                    input->shape().DebugString()));\n+\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context, context->allocate_output(\"reply\",\n+                                                      TensorShape({}), &output));\n+\n+    // Get the message and response as string scalars\n+    auto request = input->scalar<string>()();\n+    auto &reply = output->scalar<string>()();\n+\n+    // Lock for exclusive access (destructor releases)\n+    mutex_lock lock(mu_);\n+\n+    // Prepare the message\n+    zmq_msg_t request_msg;\n+    OP_REQUIRES(context, zmq_msg_init_size(&request_msg, request.size()) == 0,\n+                errors::Internal(\"Failed to initialize request.\"));\n+    memmove(zmq_msg_data(&request_msg), &request[0], request.size());\n+    // Send the message (this also takes care of clean up)\n+    OP_REQUIRES(context, zmq_msg_send(&request_msg, socket_, 0) == request.size(),\n+                errors::Internal(\"Failed to send reply.\"));\n+\n+    // Get the response\n+    zmq_msg_t reply_msg;\n+    OP_REQUIRES(context, zmq_msg_init(&reply_msg) == 0,\n+                errors::Internal(\"Failed to initialize reply.\"));\n+    OP_REQUIRES(context, zmq_msg_recv(&reply_msg, socket_, 0) != -1,\n+                errors::Internal(\"Failed to receive reply within \", timeout_, \"ms.\"));\n+    // Copy the data and clean up\n+    reply.resize(zmq_msg_size(&reply_msg));\n+    memmove(&reply[0], zmq_msg_data(&reply_msg), zmq_msg_size(&reply_msg));\n+    OP_REQUIRES(context, zmq_msg_close(&reply_msg) == 0,\n+                errors::Internal(\"Failed to close reply message.\"));\n+    done();", "path": "tensorflow/contrib/io/kernels/zmq_op.cc", "position": null, "original_position": 95, "commit_id": "d70ab8bd61f5dae7ab26713b4f12f9231b244f82", "original_commit_id": "557cadb5bf0170599aeaf46b9713654a0ea0445f", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "body": "Since the `done()` callback is always* called synchronous, this is effectively a synchronous `OpKernel`, and the `AsyncOpKernel` is just adding complexity for no gain.\r\n\r\nHowever, since this is a polling method and there's no loop, I assume that `zmq_msg_recv()` can block. We discourage writing kernels that block a thread-pool thread because there is only a finite number of them and blocking operations leads straightforwardly to deadlock.\r\n\r\nAre there asynchronous variants of the `zmq_*()` methods that could call the `done()` callback on completion? Or would it make sense for this kernel to own a thread that performs the blocking iterations on behalf of the op thread-pool, and executes `done()` when a message arrives?\r\n\r\n(* In fact, if any of the `OP_REQUIRES()` macros fails, `done()` will never be called and the execution will hang. It's important (though not well documented) that you use `OP_REQUIRES_ASYNC()` and `OP_REQUIRES_OK_ASYNC()` in a `ComputeAsync()` method.)", "created_at": "2017-04-20T04:46:08Z", "updated_at": "2017-04-20T20:24:27Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8728#discussion_r112364809", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8728", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/112364809"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8728#discussion_r112364809"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8728"}}, "body_html": "<p>Since the <code>done()</code> callback is always* called synchronous, this is effectively a synchronous <code>OpKernel</code>, and the <code>AsyncOpKernel</code> is just adding complexity for no gain.</p>\n<p>However, since this is a polling method and there's no loop, I assume that <code>zmq_msg_recv()</code> can block. We discourage writing kernels that block a thread-pool thread because there is only a finite number of them and blocking operations leads straightforwardly to deadlock.</p>\n<p>Are there asynchronous variants of the <code>zmq_*()</code> methods that could call the <code>done()</code> callback on completion? Or would it make sense for this kernel to own a thread that performs the blocking iterations on behalf of the op thread-pool, and executes <code>done()</code> when a message arrives?</p>\n<p>(* In fact, if any of the <code>OP_REQUIRES()</code> macros fails, <code>done()</code> will never be called and the execution will hang. It's important (though not well documented) that you use <code>OP_REQUIRES_ASYNC()</code> and <code>OP_REQUIRES_OK_ASYNC()</code> in a <code>ComputeAsync()</code> method.)</p>", "body_text": "Since the done() callback is always* called synchronous, this is effectively a synchronous OpKernel, and the AsyncOpKernel is just adding complexity for no gain.\nHowever, since this is a polling method and there's no loop, I assume that zmq_msg_recv() can block. We discourage writing kernels that block a thread-pool thread because there is only a finite number of them and blocking operations leads straightforwardly to deadlock.\nAre there asynchronous variants of the zmq_*() methods that could call the done() callback on completion? Or would it make sense for this kernel to own a thread that performs the blocking iterations on behalf of the op thread-pool, and executes done() when a message arrives?\n(* In fact, if any of the OP_REQUIRES() macros fails, done() will never be called and the execution will hang. It's important (though not well documented) that you use OP_REQUIRES_ASYNC() and OP_REQUIRES_OK_ASYNC() in a ComputeAsync() method.)"}