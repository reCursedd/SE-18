{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19204", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19204/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19204/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19204/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/19204", "id": 322020423, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg3MjU5NDU3", "number": 19204, "title": "Use passed name for leaky relu tensor op", "user": {"login": "jdgumz", "id": 9907395, "node_id": "MDQ6VXNlcjk5MDczOTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/9907395?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdgumz", "html_url": "https://github.com/jdgumz", "followers_url": "https://api.github.com/users/jdgumz/followers", "following_url": "https://api.github.com/users/jdgumz/following{/other_user}", "gists_url": "https://api.github.com/users/jdgumz/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdgumz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdgumz/subscriptions", "organizations_url": "https://api.github.com/users/jdgumz/orgs", "repos_url": "https://api.github.com/users/jdgumz/repos", "events_url": "https://api.github.com/users/jdgumz/events{/privacy}", "received_events_url": "https://api.github.com/users/jdgumz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136613, "node_id": "MDU6TGFiZWwzMDAxMzY2MTM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20no", "name": "cla: no", "color": "eb6420", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-10T17:46:50Z", "updated_at": "2018-05-10T20:42:44Z", "closed_at": "2018-05-10T20:41:39Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19204", "html_url": "https://github.com/tensorflow/tensorflow/pull/19204", "diff_url": "https://github.com/tensorflow/tensorflow/pull/19204.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/19204.patch"}, "body_html": "<p>I was experimenting with different activation functions for the final layer of my graph recently when I noticed that the output graph was failing to save because it couldn't find a tensor by a name I had provided, e.g. (<code>my_final_tensor_op</code>).</p>\n<p>It worked correctly with an activation function like <code>tf.nn.sigmoid</code>:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\nsigmoid_tensor = tf.nn.sigmoid(sample_values, name='my_final_tensor_op')\nsigmoid_tensor.name\n&gt;&gt;&gt; 'my_final_tensor_op:0'\n</code></pre>\n<p>But for <code>tf.nn.leaky_relu</code> I noticed that a <code>/Maximum</code> gets appended to whatever <code>name</code> value is passed:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\nleaky_relu_tensor = tf.nn.leaky_relu(sample_values, name='my_final_tensor_op')\nleaky_relu_tensor.name\n&gt;&gt;&gt; 'my_final_tensor_op/Maximum:0'\n</code></pre>\n<p>I suspect the reason why is that the <code>name</code> parameter <a href=\"https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/nn_ops.py#L1604\">is not passed in the call</a> to the <code>math_ops.maximum</code> function:</p>\n<pre><code>return math_ops.maximum(alpha * features, features)\n</code></pre>\n<p>Compare this to the case of <code>tf.nn.sigmoid</code>, which <a href=\"https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/math_ops.py#L2342\">does pass in</a> the <code>name</code> parameter into the function call that it returns:</p>\n<pre><code>return gen_math_ops.sigmoid(x, name=name)\n</code></pre>\n<p>This PR makes the change to have the <code>leaky_relu</code> function pass <code>name</code> to the <code>math_ops.maximum</code> function so that the desired name for the op carries down. I also added a unit test that addresses this specific functionality.</p>\n<p>One potential issue that could come up is if there's a lot of existing code that expects the <code>/Maximum</code> string to be appended, such as in the case where no <code>name</code> is set and the tensor op's name becomes <code>LeakyRelu/Maximum:0</code>. If that's the case, I would at least like to change the method's documentation so that the caller is aware of the <code>/Maximum</code> string concatenation side effect.</p>", "body_text": "I was experimenting with different activation functions for the final layer of my graph recently when I noticed that the output graph was failing to save because it couldn't find a tensor by a name I had provided, e.g. (my_final_tensor_op).\nIt worked correctly with an activation function like tf.nn.sigmoid:\nimport tensorflow as tf\nimport numpy as np\n\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\nsigmoid_tensor = tf.nn.sigmoid(sample_values, name='my_final_tensor_op')\nsigmoid_tensor.name\n>>> 'my_final_tensor_op:0'\n\nBut for tf.nn.leaky_relu I noticed that a /Maximum gets appended to whatever name value is passed:\nimport tensorflow as tf\nimport numpy as np\n\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\nleaky_relu_tensor = tf.nn.leaky_relu(sample_values, name='my_final_tensor_op')\nleaky_relu_tensor.name\n>>> 'my_final_tensor_op/Maximum:0'\n\nI suspect the reason why is that the name parameter is not passed in the call to the math_ops.maximum function:\nreturn math_ops.maximum(alpha * features, features)\n\nCompare this to the case of tf.nn.sigmoid, which does pass in the name parameter into the function call that it returns:\nreturn gen_math_ops.sigmoid(x, name=name)\n\nThis PR makes the change to have the leaky_relu function pass name to the math_ops.maximum function so that the desired name for the op carries down. I also added a unit test that addresses this specific functionality.\nOne potential issue that could come up is if there's a lot of existing code that expects the /Maximum string to be appended, such as in the case where no name is set and the tensor op's name becomes LeakyRelu/Maximum:0. If that's the case, I would at least like to change the method's documentation so that the caller is aware of the /Maximum string concatenation side effect.", "body": "I was experimenting with different activation functions for the final layer of my graph recently when I noticed that the output graph was failing to save because it couldn't find a tensor by a name I had provided, e.g. (`my_final_tensor_op`).\r\n\r\nIt worked correctly with an activation function like `tf.nn.sigmoid`:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\r\nsigmoid_tensor = tf.nn.sigmoid(sample_values, name='my_final_tensor_op')\r\nsigmoid_tensor.name\r\n>>> 'my_final_tensor_op:0'\r\n```\r\n\r\nBut for `tf.nn.leaky_relu` I noticed that a `/Maximum` gets appended to whatever `name` value is passed:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\r\nleaky_relu_tensor = tf.nn.leaky_relu(sample_values, name='my_final_tensor_op')\r\nleaky_relu_tensor.name\r\n>>> 'my_final_tensor_op/Maximum:0'\r\n```\r\n\r\nI suspect the reason why is that the `name` parameter [is not passed in the call](https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/nn_ops.py#L1604) to the `math_ops.maximum` function:\r\n```\r\nreturn math_ops.maximum(alpha * features, features)\r\n```\r\n\r\nCompare this to the case of `tf.nn.sigmoid`, which [does pass in](https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/math_ops.py#L2342) the `name` parameter into the function call that it returns:\r\n```\r\nreturn gen_math_ops.sigmoid(x, name=name)\r\n```\r\n\r\nThis PR makes the change to have the `leaky_relu` function pass `name` to the `math_ops.maximum` function so that the desired name for the op carries down. I also added a unit test that addresses this specific functionality.\r\n\r\nOne potential issue that could come up is if there's a lot of existing code that expects the `/Maximum` string to be appended, such as in the case where no `name` is set and the tensor op's name becomes `LeakyRelu/Maximum:0`. If that's the case, I would at least like to change the method's documentation so that the caller is aware of the `/Maximum` string concatenation side effect."}