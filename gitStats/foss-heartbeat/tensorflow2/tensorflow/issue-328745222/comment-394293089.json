{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/394293089", "html_url": "https://github.com/tensorflow/tensorflow/issues/19710#issuecomment-394293089", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19710", "id": 394293089, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDI5MzA4OQ==", "user": {"login": "MrWanter", "id": 18298163, "node_id": "MDQ6VXNlcjE4Mjk4MTYz", "avatar_url": "https://avatars0.githubusercontent.com/u/18298163?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MrWanter", "html_url": "https://github.com/MrWanter", "followers_url": "https://api.github.com/users/MrWanter/followers", "following_url": "https://api.github.com/users/MrWanter/following{/other_user}", "gists_url": "https://api.github.com/users/MrWanter/gists{/gist_id}", "starred_url": "https://api.github.com/users/MrWanter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MrWanter/subscriptions", "organizations_url": "https://api.github.com/users/MrWanter/orgs", "repos_url": "https://api.github.com/users/MrWanter/repos", "events_url": "https://api.github.com/users/MrWanter/events{/privacy}", "received_events_url": "https://api.github.com/users/MrWanter/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-04T09:32:10Z", "updated_at": "2018-06-04T09:33:01Z", "author_association": "NONE", "body_html": "<pre><code>batch_size_training = 4\nbucket_boundaries_ = [0.3, 0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 1.7]\nbucket_batch_sizes_ = (len(bucket_boundaries_)+1)*[batch_size_training]\n\ndef element_length_function(elem):\n    height, width = tf.cast(tf.shape(elem['image'])[0], tf.float32), \\\n                    tf.cast(tf.shape(elem['image'])[1], tf.float32)\n    ratio = width/height\n    return ratio\ndef bucket_by_sequence_length(element_length_func = element_length_function,\n                              bucket_boundaries= bucket_boundaries_,\n                              bucket_batch_sizes = bucket_batch_sizes_,\n                              padded_shapes=None,\n                              padding_values=None,\n                              pad_to_bucket_boundary=False):\n\n    with ops.name_scope(\"bucket_by_seq_length\"):\n        if len(bucket_batch_sizes) != (len(bucket_boundaries) + 1):\n            raise ValueError(\n                \"len(bucket_batch_sizes) must equal len(bucket_boundaries) + 1\")\n\n        batch_sizes = constant_op.constant(bucket_batch_sizes, dtype=dtypes.int64)\n\n        def element_to_bucket_id(*args):\n            \"\"\"Return int64 id of the length bucket for this element.\"\"\"\n            seq_length = element_length_func(*args)\n\n            boundaries = list(bucket_boundaries)\n            buckets_min = [np.iinfo(np.int32).min] + boundaries\n            buckets_max = boundaries + [np.iinfo(np.int32).max]\n            conditions_c = math_ops.logical_and(\n                math_ops.less_equal(buckets_min, seq_length),\n                math_ops.less(seq_length, buckets_max))\n            bucket_id = math_ops.reduce_min(array_ops.where(conditions_c))\n\n            return bucket_id\n\n\n\n        def window_size_fn(bucket_id):\n            # The window size is set to the batch size for this bucket\n            window_size = batch_sizes[bucket_id]\n            return window_size\n\n\n        def make_padded_shapes(shapes, none_filler=None):\n            padded = []\n            for shape in nest.flatten(shapes):\n                shape = tensor_shape.TensorShape(shape)\n                shape = [\n                    none_filler if d.value is None else d\n                    for d in shape\n                ]\n                padded.append(shape)\n            return nest.pack_sequence_as(shapes, padded)\n\n\n        def batching_fn(bucket_id, grouped_dataset):\n            \"\"\"Batch elements in dataset.\"\"\"\n            batch_size = batch_sizes[bucket_id]\n            none_filler = None\n            if pad_to_bucket_boundary:\n                err_msg = (\"When pad_to_bucket_boundary=True, elements must have \"\n                           \"length &lt;= max(bucket_boundaries).\")\n                check = check_ops.assert_less(\n                    bucket_id,\n                    constant_op.constant(len(bucket_batch_sizes) - 1,\n                                         dtype=dtypes.int64),\n                    message=err_msg)\n                with ops.control_dependencies([check]):\n                    boundaries = constant_op.constant(bucket_boundaries,\n                                                      dtype=dtypes.int64)\n                    bucket_boundary = boundaries[bucket_id]\n                    none_filler = bucket_boundary\n            shapes = make_padded_shapes(\n                padded_shapes or grouped_dataset.output_shapes,\n                none_filler=none_filler)\n            return grouped_dataset.padded_batch(batch_size, shapes, padding_values)\n\n\n        def _apply_fn(dataset):\n            return dataset.apply(\n                tf.contrib.data.group_by_window(element_to_bucket_id, batching_fn,\n                                window_size_func=window_size_fn))\n\n        return _apply_fn\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>, I tried to modify the  bucket sequences by length, but it only generate one image a time, can you pls point where I'm wrong?</p>", "body_text": "batch_size_training = 4\nbucket_boundaries_ = [0.3, 0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 1.7]\nbucket_batch_sizes_ = (len(bucket_boundaries_)+1)*[batch_size_training]\n\ndef element_length_function(elem):\n    height, width = tf.cast(tf.shape(elem['image'])[0], tf.float32), \\\n                    tf.cast(tf.shape(elem['image'])[1], tf.float32)\n    ratio = width/height\n    return ratio\ndef bucket_by_sequence_length(element_length_func = element_length_function,\n                              bucket_boundaries= bucket_boundaries_,\n                              bucket_batch_sizes = bucket_batch_sizes_,\n                              padded_shapes=None,\n                              padding_values=None,\n                              pad_to_bucket_boundary=False):\n\n    with ops.name_scope(\"bucket_by_seq_length\"):\n        if len(bucket_batch_sizes) != (len(bucket_boundaries) + 1):\n            raise ValueError(\n                \"len(bucket_batch_sizes) must equal len(bucket_boundaries) + 1\")\n\n        batch_sizes = constant_op.constant(bucket_batch_sizes, dtype=dtypes.int64)\n\n        def element_to_bucket_id(*args):\n            \"\"\"Return int64 id of the length bucket for this element.\"\"\"\n            seq_length = element_length_func(*args)\n\n            boundaries = list(bucket_boundaries)\n            buckets_min = [np.iinfo(np.int32).min] + boundaries\n            buckets_max = boundaries + [np.iinfo(np.int32).max]\n            conditions_c = math_ops.logical_and(\n                math_ops.less_equal(buckets_min, seq_length),\n                math_ops.less(seq_length, buckets_max))\n            bucket_id = math_ops.reduce_min(array_ops.where(conditions_c))\n\n            return bucket_id\n\n\n\n        def window_size_fn(bucket_id):\n            # The window size is set to the batch size for this bucket\n            window_size = batch_sizes[bucket_id]\n            return window_size\n\n\n        def make_padded_shapes(shapes, none_filler=None):\n            padded = []\n            for shape in nest.flatten(shapes):\n                shape = tensor_shape.TensorShape(shape)\n                shape = [\n                    none_filler if d.value is None else d\n                    for d in shape\n                ]\n                padded.append(shape)\n            return nest.pack_sequence_as(shapes, padded)\n\n\n        def batching_fn(bucket_id, grouped_dataset):\n            \"\"\"Batch elements in dataset.\"\"\"\n            batch_size = batch_sizes[bucket_id]\n            none_filler = None\n            if pad_to_bucket_boundary:\n                err_msg = (\"When pad_to_bucket_boundary=True, elements must have \"\n                           \"length <= max(bucket_boundaries).\")\n                check = check_ops.assert_less(\n                    bucket_id,\n                    constant_op.constant(len(bucket_batch_sizes) - 1,\n                                         dtype=dtypes.int64),\n                    message=err_msg)\n                with ops.control_dependencies([check]):\n                    boundaries = constant_op.constant(bucket_boundaries,\n                                                      dtype=dtypes.int64)\n                    bucket_boundary = boundaries[bucket_id]\n                    none_filler = bucket_boundary\n            shapes = make_padded_shapes(\n                padded_shapes or grouped_dataset.output_shapes,\n                none_filler=none_filler)\n            return grouped_dataset.padded_batch(batch_size, shapes, padding_values)\n\n\n        def _apply_fn(dataset):\n            return dataset.apply(\n                tf.contrib.data.group_by_window(element_to_bucket_id, batching_fn,\n                                window_size_func=window_size_fn))\n\n        return _apply_fn\n\n@mrry, I tried to modify the  bucket sequences by length, but it only generate one image a time, can you pls point where I'm wrong?", "body": "```\r\nbatch_size_training = 4\r\nbucket_boundaries_ = [0.3, 0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 1.7]\r\nbucket_batch_sizes_ = (len(bucket_boundaries_)+1)*[batch_size_training]\r\n\r\ndef element_length_function(elem):\r\n    height, width = tf.cast(tf.shape(elem['image'])[0], tf.float32), \\\r\n                    tf.cast(tf.shape(elem['image'])[1], tf.float32)\r\n    ratio = width/height\r\n    return ratio\r\ndef bucket_by_sequence_length(element_length_func = element_length_function,\r\n                              bucket_boundaries= bucket_boundaries_,\r\n                              bucket_batch_sizes = bucket_batch_sizes_,\r\n                              padded_shapes=None,\r\n                              padding_values=None,\r\n                              pad_to_bucket_boundary=False):\r\n\r\n    with ops.name_scope(\"bucket_by_seq_length\"):\r\n        if len(bucket_batch_sizes) != (len(bucket_boundaries) + 1):\r\n            raise ValueError(\r\n                \"len(bucket_batch_sizes) must equal len(bucket_boundaries) + 1\")\r\n\r\n        batch_sizes = constant_op.constant(bucket_batch_sizes, dtype=dtypes.int64)\r\n\r\n        def element_to_bucket_id(*args):\r\n            \"\"\"Return int64 id of the length bucket for this element.\"\"\"\r\n            seq_length = element_length_func(*args)\r\n\r\n            boundaries = list(bucket_boundaries)\r\n            buckets_min = [np.iinfo(np.int32).min] + boundaries\r\n            buckets_max = boundaries + [np.iinfo(np.int32).max]\r\n            conditions_c = math_ops.logical_and(\r\n                math_ops.less_equal(buckets_min, seq_length),\r\n                math_ops.less(seq_length, buckets_max))\r\n            bucket_id = math_ops.reduce_min(array_ops.where(conditions_c))\r\n\r\n            return bucket_id\r\n\r\n\r\n\r\n        def window_size_fn(bucket_id):\r\n            # The window size is set to the batch size for this bucket\r\n            window_size = batch_sizes[bucket_id]\r\n            return window_size\r\n\r\n\r\n        def make_padded_shapes(shapes, none_filler=None):\r\n            padded = []\r\n            for shape in nest.flatten(shapes):\r\n                shape = tensor_shape.TensorShape(shape)\r\n                shape = [\r\n                    none_filler if d.value is None else d\r\n                    for d in shape\r\n                ]\r\n                padded.append(shape)\r\n            return nest.pack_sequence_as(shapes, padded)\r\n\r\n\r\n        def batching_fn(bucket_id, grouped_dataset):\r\n            \"\"\"Batch elements in dataset.\"\"\"\r\n            batch_size = batch_sizes[bucket_id]\r\n            none_filler = None\r\n            if pad_to_bucket_boundary:\r\n                err_msg = (\"When pad_to_bucket_boundary=True, elements must have \"\r\n                           \"length <= max(bucket_boundaries).\")\r\n                check = check_ops.assert_less(\r\n                    bucket_id,\r\n                    constant_op.constant(len(bucket_batch_sizes) - 1,\r\n                                         dtype=dtypes.int64),\r\n                    message=err_msg)\r\n                with ops.control_dependencies([check]):\r\n                    boundaries = constant_op.constant(bucket_boundaries,\r\n                                                      dtype=dtypes.int64)\r\n                    bucket_boundary = boundaries[bucket_id]\r\n                    none_filler = bucket_boundary\r\n            shapes = make_padded_shapes(\r\n                padded_shapes or grouped_dataset.output_shapes,\r\n                none_filler=none_filler)\r\n            return grouped_dataset.padded_batch(batch_size, shapes, padding_values)\r\n\r\n\r\n        def _apply_fn(dataset):\r\n            return dataset.apply(\r\n                tf.contrib.data.group_by_window(element_to_bucket_id, batching_fn,\r\n                                window_size_func=window_size_fn))\r\n\r\n        return _apply_fn\r\n```\r\n\r\n@mrry, I tried to modify the  bucket sequences by length, but it only generate one image a time, can you pls point where I'm wrong?"}