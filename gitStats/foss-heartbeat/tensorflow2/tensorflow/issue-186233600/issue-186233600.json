{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5299", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5299/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5299/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5299/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5299", "id": 186233600, "node_id": "MDU6SXNzdWUxODYyMzM2MDA=", "number": 5299, "title": "How can I make distributed tensorFlow support failover?", "user": {"login": "wangyang0918", "id": 15904523, "node_id": "MDQ6VXNlcjE1OTA0NTIz", "avatar_url": "https://avatars1.githubusercontent.com/u/15904523?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangyang0918", "html_url": "https://github.com/wangyang0918", "followers_url": "https://api.github.com/users/wangyang0918/followers", "following_url": "https://api.github.com/users/wangyang0918/following{/other_user}", "gists_url": "https://api.github.com/users/wangyang0918/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangyang0918/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangyang0918/subscriptions", "organizations_url": "https://api.github.com/users/wangyang0918/orgs", "repos_url": "https://api.github.com/users/wangyang0918/repos", "events_url": "https://api.github.com/users/wangyang0918/events{/privacy}", "received_events_url": "https://api.github.com/users/wangyang0918/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-10-31T08:58:06Z", "updated_at": "2016-11-01T04:47:06Z", "closed_at": "2016-10-31T20:39:45Z", "author_association": "NONE", "body_html": "<p>I create a 4 nodes tensorflow cluster, 2 worker, 2 ps. When the worker or ps fails, I relaunch it with the same configuration on the machine. However, it cannot continue to work from the checkpoint.<br>\nDoes distributed tensorflow still not support failover?</p>\n<p>This file \"model.ckpt-24\" is only on the machine( task 0 of worker), the trace is as follows.</p>\n<p>Traceback (most recent call last):<br>\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in <br>\ntf.app.run()<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv[:1] + flags_passthrough))<br>\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 87, in main<br>\nwith sv.managed_session(server.target) as sess:<br>\nFile \"/usr/lib64/python2.7/contextlib.py\", line 17, in <strong>enter</strong><br>\nreturn self.gen.next()<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session<br>\nself.stop(close_summary_writer=close_summary_writer)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 797, in stop<br>\nstop_grace_period_secs=self._stop_grace_secs)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join<br>\nsix.reraise(*self._exc_info_to_raise)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session<br>\nstart_standard_services=start_standard_services)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 715, in prepare_or_wait_for_session<br>\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 227, in prepare_session<br>\nconfig=config)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 173, in _restore_checkpoint<br>\nsaver.restore(sess, ckpt.model_checkpoint_path)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1345, in restore<br>\n{self.saver_def.filename_tensor_name: save_path})<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run<br>\nfeed_dict_string, options, run_metadata)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run<br>\ntarget_list, options, run_metadata)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003<br>\n[[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]</p>\n<p>Caused by op u'save/restore_slice_1', defined at:<br>\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in <br>\ntf.app.run()<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv[:1] + flags_passthrough))<br>\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 71, in main<br>\nsaver = tf.train.Saver()<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 986, in <strong>init</strong><br>\nself.build()<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1015, in build<br>\nrestore_sequentially=self._restore_sequentially)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 620, in build<br>\nrestore_sequentially, reshape)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 357, in _AddRestoreOps<br>\ntensors = self.restore_op(filename_tensor, saveable, preferred_shard)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 270, in restore_op<br>\npreferred_shard=preferred_shard))<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 204, in _restore_slice<br>\npreferred_shard, name=name)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice<br>\npreferred_shard=preferred_shard, name=name)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op<br>\nop_def=op_def)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in <strong>init</strong><br>\nself._traceback = _extract_stack()</p>\n<p>InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003<br>\n[[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]</p>", "body_text": "I create a 4 nodes tensorflow cluster, 2 worker, 2 ps. When the worker or ps fails, I relaunch it with the same configuration on the machine. However, it cannot continue to work from the checkpoint.\nDoes distributed tensorflow still not support failover?\nThis file \"model.ckpt-24\" is only on the machine( task 0 of worker), the trace is as follows.\nTraceback (most recent call last):\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in \ntf.app.run()\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv[:1] + flags_passthrough))\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 87, in main\nwith sv.managed_session(server.target) as sess:\nFile \"/usr/lib64/python2.7/contextlib.py\", line 17, in enter\nreturn self.gen.next()\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\nself.stop(close_summary_writer=close_summary_writer)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\nstop_grace_period_secs=self._stop_grace_secs)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\nsix.reraise(*self._exc_info_to_raise)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session\nstart_standard_services=start_standard_services)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 715, in prepare_or_wait_for_session\ninit_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 227, in prepare_session\nconfig=config)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 173, in _restore_checkpoint\nsaver.restore(sess, ckpt.model_checkpoint_path)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1345, in restore\n{self.saver_def.filename_tensor_name: save_path})\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\nrun_metadata_ptr)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\nfeed_dict_string, options, run_metadata)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\ntarget_list, options, run_metadata)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003\n[[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]\nCaused by op u'save/restore_slice_1', defined at:\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in \ntf.app.run()\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv[:1] + flags_passthrough))\nFile \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 71, in main\nsaver = tf.train.Saver()\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 986, in init\nself.build()\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1015, in build\nrestore_sequentially=self._restore_sequentially)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 620, in build\nrestore_sequentially, reshape)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 357, in _AddRestoreOps\ntensors = self.restore_op(filename_tensor, saveable, preferred_shard)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 270, in restore_op\npreferred_shard=preferred_shard))\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 204, in _restore_slice\npreferred_shard, name=name)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\npreferred_shard=preferred_shard, name=name)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\nop_def=op_def)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in init\nself._traceback = _extract_stack()\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003\n[[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]", "body": "I create a 4 nodes tensorflow cluster, 2 worker, 2 ps. When the worker or ps fails, I relaunch it with the same configuration on the machine. However, it cannot continue to work from the checkpoint.\r\nDoes distributed tensorflow still not support failover?\r\n\r\nThis file \"model.ckpt-24\" is only on the machine( task 0 of worker), the trace is as follows.\r\n\r\nTraceback (most recent call last):\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 87, in main\r\n    with sv.managed_session(server.target) as sess:\r\n  File \"/usr/lib64/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session\r\n    start_standard_services=start_standard_services)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 715, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 227, in prepare_session\r\n    config=config)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 173, in _restore_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1345, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003\r\n\t [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]\r\n\r\nCaused by op u'save/restore_slice_1', defined at:\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 71, in main\r\n    saver = tf.train.Saver()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 986, in __init__\r\n    self.build()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1015, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 620, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 357, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 270, in restore_op\r\n    preferred_shard=preferred_shard))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 204, in _restore_slice\r\n    preferred_shard, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\r\n    preferred_shard=preferred_shard, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003\r\n\t [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]"}