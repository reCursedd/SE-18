{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219575880", "html_url": "https://github.com/tensorflow/tensorflow/pull/2374#issuecomment-219575880", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2374", "id": 219575880, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTU3NTg4MA==", "user": {"login": "keiji", "id": 932136, "node_id": "MDQ6VXNlcjkzMjEzNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/932136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keiji", "html_url": "https://github.com/keiji", "followers_url": "https://api.github.com/users/keiji/followers", "following_url": "https://api.github.com/users/keiji/following{/other_user}", "gists_url": "https://api.github.com/users/keiji/gists{/gist_id}", "starred_url": "https://api.github.com/users/keiji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keiji/subscriptions", "organizations_url": "https://api.github.com/users/keiji/orgs", "repos_url": "https://api.github.com/users/keiji/repos", "events_url": "https://api.github.com/users/keiji/events{/privacy}", "received_events_url": "https://api.github.com/users/keiji/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-16T23:13:07Z", "updated_at": "2016-05-16T23:13:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a>,</p>\n<p>conv2's parameter stddev is different from original <a href=\"https://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-conv-local-11pct.cfg\" rel=\"nofollow\">cuda-convnet</a>.<br>\nI think this may be a typo, because <code>1e-4</code> is same value with conv1 layer.</p>\n<p>Currently parameter(1e-4) cause of vanishing-gradient at trying <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html\" rel=\"nofollow\">EXERCISE</a>.</p>\n<hr>\n<p>The original  <a href=\"https://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-conv-local-11pct.cfg\" rel=\"nofollow\">cuda-convnet</a>'s conv2 initW is <code>0.01</code>.</p>\n<pre><code>[conv2]\ntype=conv\ninputs=rnorm1\nfilters=64\npadding=2\nstride=1\nfilterSize=5\nchannels=64\nneuron=relu\ninitW=0.01\npartialSum=8\nsharedBiases=1\n</code></pre>\n<p>And cifar10.py's conv2 stddev is <code>1e-4</code>.</p>\n<pre><code>  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-4, wd=0.0)\n</code></pre>\n<p>I think this may be a typo, because <code>1e-4</code> is same value with conv1 layer.</p>\n<p><a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html\" rel=\"nofollow\">TensorFlow document</a> is questioning</p>\n<pre><code>EXERCISE: The model architecture in inference() differs slightly from the CIFAR-10 model specified in \ncuda-convnet. In particular, the top layers of Alex's original model are locally connected and not fully \nconnected. Try editing the architecture to exactly reproduce the locally connected architecture in the top \nlayer.\n</code></pre>\n<p>I'm trying this exercise(unfortunately I cannot find model answer on web).<br>\nI tried replace local3 and local4 to convolutional layer. And if conv2's <code>stddev</code> was <code>1e-4</code>, vanishing-gradient occurred at conv2 layer.</p>", "body_text": "Hi @girving,\nconv2's parameter stddev is different from original cuda-convnet.\nI think this may be a typo, because 1e-4 is same value with conv1 layer.\nCurrently parameter(1e-4) cause of vanishing-gradient at trying EXERCISE.\n\nThe original  cuda-convnet's conv2 initW is 0.01.\n[conv2]\ntype=conv\ninputs=rnorm1\nfilters=64\npadding=2\nstride=1\nfilterSize=5\nchannels=64\nneuron=relu\ninitW=0.01\npartialSum=8\nsharedBiases=1\n\nAnd cifar10.py's conv2 stddev is 1e-4.\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-4, wd=0.0)\n\nI think this may be a typo, because 1e-4 is same value with conv1 layer.\nTensorFlow document is questioning\nEXERCISE: The model architecture in inference() differs slightly from the CIFAR-10 model specified in \ncuda-convnet. In particular, the top layers of Alex's original model are locally connected and not fully \nconnected. Try editing the architecture to exactly reproduce the locally connected architecture in the top \nlayer.\n\nI'm trying this exercise(unfortunately I cannot find model answer on web).\nI tried replace local3 and local4 to convolutional layer. And if conv2's stddev was 1e-4, vanishing-gradient occurred at conv2 layer.", "body": "Hi @girving,\n\nconv2's parameter stddev is different from original [cuda-convnet](https://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-conv-local-11pct.cfg).\nI think this may be a typo, because `1e-4` is same value with conv1 layer.\n\nCurrently parameter(1e-4) cause of vanishing-gradient at trying [EXERCISE](https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html).\n\n---\n\nThe original  [cuda-convnet](https://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-conv-local-11pct.cfg)'s conv2 initW is `0.01`.\n\n```\n[conv2]\ntype=conv\ninputs=rnorm1\nfilters=64\npadding=2\nstride=1\nfilterSize=5\nchannels=64\nneuron=relu\ninitW=0.01\npartialSum=8\nsharedBiases=1\n```\n\nAnd cifar10.py's conv2 stddev is `1e-4`.\n\n```\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-4, wd=0.0)\n```\n\nI think this may be a typo, because `1e-4` is same value with conv1 layer.\n\n[TensorFlow document](https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html) is questioning\n\n```\nEXERCISE: The model architecture in inference() differs slightly from the CIFAR-10 model specified in \ncuda-convnet. In particular, the top layers of Alex's original model are locally connected and not fully \nconnected. Try editing the architecture to exactly reproduce the locally connected architecture in the top \nlayer.\n```\n\nI'm trying this exercise(unfortunately I cannot find model answer on web).\nI tried replace local3 and local4 to convolutional layer. And if conv2's `stddev` was `1e-4`, vanishing-gradient occurred at conv2 layer.\n"}