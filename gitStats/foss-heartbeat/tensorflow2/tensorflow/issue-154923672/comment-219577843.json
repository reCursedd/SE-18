{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219577843", "html_url": "https://github.com/tensorflow/tensorflow/pull/2374#issuecomment-219577843", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2374", "id": 219577843, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTU3Nzg0Mw==", "user": {"login": "keiji", "id": 932136, "node_id": "MDQ6VXNlcjkzMjEzNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/932136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keiji", "html_url": "https://github.com/keiji", "followers_url": "https://api.github.com/users/keiji/followers", "following_url": "https://api.github.com/users/keiji/following{/other_user}", "gists_url": "https://api.github.com/users/keiji/gists{/gist_id}", "starred_url": "https://api.github.com/users/keiji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keiji/subscriptions", "organizations_url": "https://api.github.com/users/keiji/orgs", "repos_url": "https://api.github.com/users/keiji/repos", "events_url": "https://api.github.com/users/keiji/events{/privacy}", "received_events_url": "https://api.github.com/users/keiji/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-16T23:25:12Z", "updated_at": "2016-05-16T23:30:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here's my current <code>inference</code> code.</p>\n<pre><code>def inference(images):\n  \"\"\"Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  \"\"\"\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope('conv1') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],\n                                         stddev=1e-4, wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding='SAME', name='pool1')\n\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm1')\n\n  # conv2\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-2, wd=0.0)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm2')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n\n  # local3\n  with tf.variable_scope('local3') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[64]))\n    bias = tf.nn.bias_add(conv, biases)\n    local3 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope('local4') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 32],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(local3, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[32]))\n    bias = tf.nn.bias_add(conv, biases)\n    local4 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local4)\n\n  # softmax, i.e. softmax(WX + b)\n  with tf.variable_scope('softmax_linear') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(local4, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n\n    weights = _variable_with_weight_decay('weights', [dim, NUM_CLASSES],\n                                          stddev=1 / float(NUM_CLASSES), wd=0.0)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.nn.softmax(tf.nn.bias_add(tf.matmul(reshape, weights), biases), name=scope.name)\n\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n</code></pre>\n<p>If weight stddev was <code>1e-4</code> at conv2, evaluation accuracies is just 10%.</p>\n<pre><code>2016-05-17 08:29:43.386243: precision @ 1 = 0.100\n</code></pre>\n<p>And tensorboard shows vanishing-gradient at conv2 layer.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/932136/15306759/41f20334-1b82-11e6-937b-11af16397d6d.png\"><img width=\"751\" alt=\"screen-shot-2016-05-16-at-03 28 57\" src=\"https://cloud.githubusercontent.com/assets/932136/15306759/41f20334-1b82-11e6-937b-11af16397d6d.png\" style=\"max-width:100%;\"></a></p>", "body_text": "Here's my current inference code.\ndef inference(images):\n  \"\"\"Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  \"\"\"\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope('conv1') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],\n                                         stddev=1e-4, wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding='SAME', name='pool1')\n\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm1')\n\n  # conv2\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-2, wd=0.0)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm2')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n\n  # local3\n  with tf.variable_scope('local3') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[64]))\n    bias = tf.nn.bias_add(conv, biases)\n    local3 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope('local4') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 32],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(local3, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[32]))\n    bias = tf.nn.bias_add(conv, biases)\n    local4 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local4)\n\n  # softmax, i.e. softmax(WX + b)\n  with tf.variable_scope('softmax_linear') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(local4, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n\n    weights = _variable_with_weight_decay('weights', [dim, NUM_CLASSES],\n                                          stddev=1 / float(NUM_CLASSES), wd=0.0)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.nn.softmax(tf.nn.bias_add(tf.matmul(reshape, weights), biases), name=scope.name)\n\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n\nIf weight stddev was 1e-4 at conv2, evaluation accuracies is just 10%.\n2016-05-17 08:29:43.386243: precision @ 1 = 0.100\n\nAnd tensorboard shows vanishing-gradient at conv2 layer.", "body": "Here's my current `inference` code.\n\n```\ndef inference(images):\n  \"\"\"Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  \"\"\"\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope('conv1') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],\n                                         stddev=1e-4, wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding='SAME', name='pool1')\n\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm1')\n\n  # conv2\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-2, wd=0.0)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm2')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n\n  # local3\n  with tf.variable_scope('local3') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[64]))\n    bias = tf.nn.bias_add(conv, biases)\n    local3 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope('local4') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 32],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(local3, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[32]))\n    bias = tf.nn.bias_add(conv, biases)\n    local4 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local4)\n\n  # softmax, i.e. softmax(WX + b)\n  with tf.variable_scope('softmax_linear') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(local4, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n\n    weights = _variable_with_weight_decay('weights', [dim, NUM_CLASSES],\n                                          stddev=1 / float(NUM_CLASSES), wd=0.0)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.nn.softmax(tf.nn.bias_add(tf.matmul(reshape, weights), biases), name=scope.name)\n\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n```\n\nIf weight stddev was `1e-4` at conv2, evaluation accuracies is just 10%.\n\n```\n2016-05-17 08:29:43.386243: precision @ 1 = 0.100\n```\n\nAnd tensorboard shows vanishing-gradient at conv2 layer.\n\n<img width=\"751\" alt=\"screen-shot-2016-05-16-at-03 28 57\" src=\"https://cloud.githubusercontent.com/assets/932136/15306759/41f20334-1b82-11e6-937b-11af16397d6d.png\">\n"}