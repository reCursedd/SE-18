{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239969809", "html_url": "https://github.com/tensorflow/tensorflow/issues/3377#issuecomment-239969809", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3377", "id": 239969809, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTk2OTgwOQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-16T00:31:37Z", "updated_at": "2016-08-16T00:31:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>feed_dict has more overhead than just copying CPU to GPU. It also moves the data from Python to TensorFlow among other things. Someone will have to look further to determine whether this is the only issue.</p>\n<p>But even with only one GPU, you can hide data transfer between python and TensorFlow through:</p>\n<p>1.a Having one python thread to read the data, and feed them into a tensorflow queue, managed by queue runner.<br>\n1.b Having your model read from that queue in parallel, instead of through feed_dict. That way, the data copy from Python to TensorFlow is parallelized behind the real training.</p>\n<p>When you have many replicas running on multiple GPU over multiple machines, you want to use the reader class to read the data locally on each replica, instead of through feed_dict. For example:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_input.py#L79\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_input.py#L79</a><br>\n<a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/dataset.py#L95\">https://github.com/tensorflow/models/blob/master/inception/inception/dataset.py#L95</a></p>", "body_text": "feed_dict has more overhead than just copying CPU to GPU. It also moves the data from Python to TensorFlow among other things. Someone will have to look further to determine whether this is the only issue.\nBut even with only one GPU, you can hide data transfer between python and TensorFlow through:\n1.a Having one python thread to read the data, and feed them into a tensorflow queue, managed by queue runner.\n1.b Having your model read from that queue in parallel, instead of through feed_dict. That way, the data copy from Python to TensorFlow is parallelized behind the real training.\nWhen you have many replicas running on multiple GPU over multiple machines, you want to use the reader class to read the data locally on each replica, instead of through feed_dict. For example:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_input.py#L79\nhttps://github.com/tensorflow/models/blob/master/inception/inception/dataset.py#L95", "body": "feed_dict has more overhead than just copying CPU to GPU. It also moves the data from Python to TensorFlow among other things. Someone will have to look further to determine whether this is the only issue. \n\nBut even with only one GPU, you can hide data transfer between python and TensorFlow through: \n\n1.a Having one python thread to read the data, and feed them into a tensorflow queue, managed by queue runner.\n1.b Having your model read from that queue in parallel, instead of through feed_dict. That way, the data copy from Python to TensorFlow is parallelized behind the real training. \n\nWhen you have many replicas running on multiple GPU over multiple machines, you want to use the reader class to read the data locally on each replica, instead of through feed_dict. For example: \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_input.py#L79\nhttps://github.com/tensorflow/models/blob/master/inception/inception/dataset.py#L95\n"}