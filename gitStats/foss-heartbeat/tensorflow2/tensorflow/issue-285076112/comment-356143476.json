{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356143476", "html_url": "https://github.com/tensorflow/tensorflow/issues/15717#issuecomment-356143476", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15717", "id": 356143476, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjE0MzQ3Ng==", "user": {"login": "acbellini", "id": 1515754, "node_id": "MDQ6VXNlcjE1MTU3NTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1515754?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acbellini", "html_url": "https://github.com/acbellini", "followers_url": "https://api.github.com/users/acbellini/followers", "following_url": "https://api.github.com/users/acbellini/following{/other_user}", "gists_url": "https://api.github.com/users/acbellini/gists{/gist_id}", "starred_url": "https://api.github.com/users/acbellini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acbellini/subscriptions", "organizations_url": "https://api.github.com/users/acbellini/orgs", "repos_url": "https://api.github.com/users/acbellini/repos", "events_url": "https://api.github.com/users/acbellini/events{/privacy}", "received_events_url": "https://api.github.com/users/acbellini/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-09T00:44:01Z", "updated_at": "2018-01-09T00:44:50Z", "author_association": "NONE", "body_html": "<p>I did a bit more research on this.</p>\n<p>If you substitute</p>\n<p><code>s.run((_const_a, _mul_const_a, _random_a, _mul_random_a))</code></p>\n<p>with</p>\n<pre><code>    s.run((_mul_const_a.op))\n    s.run((_mul_random_a.op))\n</code></pre>\n<p>The overhead disappears and constant multiplication is much faster. However, if you run:</p>\n<p><code>s.run((_mul_const_a.op, _mul_random_a.op))</code></p>\n<p>then again constant multiplication exhibits the overhead.</p>\n<p>It seems that, while the two parts of the graph are disjoint, retrieving nodes from different portions of the graph causes them to be re-initialized from the source, i.e. transferring again from the initialization data from the CPU to the GPU.</p>\n<p>How can that be explained? Doesn't seem like a desireable feature.</p>", "body_text": "I did a bit more research on this.\nIf you substitute\ns.run((_const_a, _mul_const_a, _random_a, _mul_random_a))\nwith\n    s.run((_mul_const_a.op))\n    s.run((_mul_random_a.op))\n\nThe overhead disappears and constant multiplication is much faster. However, if you run:\ns.run((_mul_const_a.op, _mul_random_a.op))\nthen again constant multiplication exhibits the overhead.\nIt seems that, while the two parts of the graph are disjoint, retrieving nodes from different portions of the graph causes them to be re-initialized from the source, i.e. transferring again from the initialization data from the CPU to the GPU.\nHow can that be explained? Doesn't seem like a desireable feature.", "body": "I did a bit more research on this.\r\n\r\nIf you substitute\r\n\r\n`s.run((_const_a, _mul_const_a, _random_a, _mul_random_a))`\r\n\r\nwith \r\n\r\n```\r\n    s.run((_mul_const_a.op))\r\n    s.run((_mul_random_a.op))\r\n```\r\n\r\nThe overhead disappears and constant multiplication is much faster. However, if you run:\r\n\r\n`s.run((_mul_const_a.op, _mul_random_a.op))`\r\n\r\nthen again constant multiplication exhibits the overhead. \r\n\r\nIt seems that, while the two parts of the graph are disjoint, retrieving nodes from different portions of the graph causes them to be re-initialized from the source, i.e. transferring again from the initialization data from the CPU to the GPU. \r\n\r\nHow can that be explained? Doesn't seem like a desireable feature.\r\n  "}