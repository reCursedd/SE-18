{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14507", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14507/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14507/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14507/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14507", "id": 273288200, "node_id": "MDU6SXNzdWUyNzMyODgyMDA=", "number": 14507, "title": "XLA reports error with 1000 steps of static_bidirectional_rnn", "user": {"login": "linearhit", "id": 8992445, "node_id": "MDQ6VXNlcjg5OTI0NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8992445?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linearhit", "html_url": "https://github.com/linearhit", "followers_url": "https://api.github.com/users/linearhit/followers", "following_url": "https://api.github.com/users/linearhit/following{/other_user}", "gists_url": "https://api.github.com/users/linearhit/gists{/gist_id}", "starred_url": "https://api.github.com/users/linearhit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linearhit/subscriptions", "organizations_url": "https://api.github.com/users/linearhit/orgs", "repos_url": "https://api.github.com/users/linearhit/repos", "events_url": "https://api.github.com/users/linearhit/events{/privacy}", "received_events_url": "https://api.github.com/users/linearhit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-11-13T01:49:50Z", "updated_at": "2018-01-25T21:23:22Z", "closed_at": null, "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2.1 or 1.3</li>\n<li><strong>Python version</strong>: 2.7.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.8.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 5.1</li>\n<li><strong>GPU model and memory</strong>: M40</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>This issue can only be reproduced when XLA works with static_bidirectional_rnn with 1000 steps, and the \"seq_len\" of static_bidirectional_rnn must be assigned, which means it works with \"dynamic calculation\". When the issue is reproduced, it reports:</p>\n<pre><code>2017-11-01 18:47:16.497266: E tensorflow/stream_executor/cuda/cuda_driver.cc:731] failed to load PTX text as a module: CUDA_ERROR_NO_BINARY_FOR_GPU\n2017-11-01 18:47:16.497294: E tensorflow/stream_executor/cuda/cuda_driver.cc:736] error log buffer (163 bytes): ptxas application ptx input, line 7231; error   : Kernel '_fusion_1' exceeds parameter space limit of 4352 bytes\nptxas fatal   : Ptx assembly aborted due to error\n\n</code></pre>\n<p>From my analysis, a fused XLA instruction requires for more than 1000 input parameters. This further leads to a PTX kernel with 1000+ parameters, which is not accepted by the cuda driver.</p>\n<p>This is what I found from the PTX ISA documents:<br>\n<code>The maximum memory size supported by PTX for normal (non-opaque type) parameters is 4352 bytes. Prior to PTX ISA version 1.5, the maximum size was 256 bytes.</code></p>\n<p>Read more at: <a href=\"http://docs.nvidia.com/cuda/parallel-thread-execution/index.html#ixzz4yGwCVOB7\" rel=\"nofollow\">http://docs.nvidia.com/cuda/parallel-thread-execution/index.html#ixzz4yGwCVOB7</a><br>\nFollow us: @GPUComputing on Twitter | NVIDIA on Facebook</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.2.1 or 1.3\nPython version: 2.7.4\nBazel version (if compiling from source): 0.4.5\nGCC/Compiler version (if compiling from source): 4.8.5\nCUDA/cuDNN version: 5.1\nGPU model and memory: M40\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nThis issue can only be reproduced when XLA works with static_bidirectional_rnn with 1000 steps, and the \"seq_len\" of static_bidirectional_rnn must be assigned, which means it works with \"dynamic calculation\". When the issue is reproduced, it reports:\n2017-11-01 18:47:16.497266: E tensorflow/stream_executor/cuda/cuda_driver.cc:731] failed to load PTX text as a module: CUDA_ERROR_NO_BINARY_FOR_GPU\n2017-11-01 18:47:16.497294: E tensorflow/stream_executor/cuda/cuda_driver.cc:736] error log buffer (163 bytes): ptxas application ptx input, line 7231; error   : Kernel '_fusion_1' exceeds parameter space limit of 4352 bytes\nptxas fatal   : Ptx assembly aborted due to error\n\n\nFrom my analysis, a fused XLA instruction requires for more than 1000 input parameters. This further leads to a PTX kernel with 1000+ parameters, which is not accepted by the cuda driver.\nThis is what I found from the PTX ISA documents:\nThe maximum memory size supported by PTX for normal (non-opaque type) parameters is 4352 bytes. Prior to PTX ISA version 1.5, the maximum size was 256 bytes.\nRead more at: http://docs.nvidia.com/cuda/parallel-thread-execution/index.html#ixzz4yGwCVOB7\nFollow us: @GPUComputing on Twitter | NVIDIA on Facebook\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1 or 1.3\r\n- **Python version**: 2.7.4\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: M40\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThis issue can only be reproduced when XLA works with static_bidirectional_rnn with 1000 steps, and the \"seq_len\" of static_bidirectional_rnn must be assigned, which means it works with \"dynamic calculation\". When the issue is reproduced, it reports:\r\n\r\n```\r\n2017-11-01 18:47:16.497266: E tensorflow/stream_executor/cuda/cuda_driver.cc:731] failed to load PTX text as a module: CUDA_ERROR_NO_BINARY_FOR_GPU\r\n2017-11-01 18:47:16.497294: E tensorflow/stream_executor/cuda/cuda_driver.cc:736] error log buffer (163 bytes): ptxas application ptx input, line 7231; error   : Kernel '_fusion_1' exceeds parameter space limit of 4352 bytes\r\nptxas fatal   : Ptx assembly aborted due to error\r\n\r\n```\r\nFrom my analysis, a fused XLA instruction requires for more than 1000 input parameters. This further leads to a PTX kernel with 1000+ parameters, which is not accepted by the cuda driver.\r\n\r\nThis is what I found from the PTX ISA documents:\r\n`The maximum memory size supported by PTX for normal (non-opaque type) parameters is 4352 bytes. Prior to PTX ISA version 1.5, the maximum size was 256 bytes.`\r\n\r\nRead more at: http://docs.nvidia.com/cuda/parallel-thread-execution/index.html#ixzz4yGwCVOB7\r\nFollow us: @GPUComputing on Twitter | NVIDIA on Facebook\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n"}