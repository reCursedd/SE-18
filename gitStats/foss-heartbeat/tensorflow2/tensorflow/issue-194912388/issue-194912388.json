{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6263", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6263/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6263/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6263/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6263", "id": 194912388, "node_id": "MDU6SXNzdWUxOTQ5MTIzODg=", "number": 6263, "title": "Document that tf.train.Supervisor is deprecated", "user": {"login": "taochenshh", "id": 15166943, "node_id": "MDQ6VXNlcjE1MTY2OTQz", "avatar_url": "https://avatars3.githubusercontent.com/u/15166943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taochenshh", "html_url": "https://github.com/taochenshh", "followers_url": "https://api.github.com/users/taochenshh/followers", "following_url": "https://api.github.com/users/taochenshh/following{/other_user}", "gists_url": "https://api.github.com/users/taochenshh/gists{/gist_id}", "starred_url": "https://api.github.com/users/taochenshh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taochenshh/subscriptions", "organizations_url": "https://api.github.com/users/taochenshh/orgs", "repos_url": "https://api.github.com/users/taochenshh/repos", "events_url": "https://api.github.com/users/taochenshh/events{/privacy}", "received_events_url": "https://api.github.com/users/taochenshh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2016-12-12T08:06:22Z", "updated_at": "2017-12-02T09:39:46Z", "closed_at": "2017-12-02T09:39:46Z", "author_association": "NONE", "body_html": "<p>I am using CUDA 8.0, cuDNN 5.1, ubuntu 16.04, GPU: TitanX, tensorflow r0.12.<br>\nAnd I met some problems when using tf.train.Supervisor in distributed training. I have simplified my  code shown as belown:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nserver <span class=\"pl-k\">=</span> tf.train.Server.create_local_server()\nlogs_path <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mnist/logs<span class=\"pl-pds\">\"</span></span>\n\n\nglobal_step <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_step<span class=\"pl-pds\">'</span></span>, [],\n                              <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0</span>),\n                              <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>):\n    W1 <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([<span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">100</span>]))\n    W2 <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([<span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">100</span>]))\n\ninit_op <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Variables initialized ...<span class=\"pl-pds\">\"</span></span>)\nsv <span class=\"pl-k\">=</span> tf.train.Supervisor(<span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                         <span class=\"pl-v\">logdir</span><span class=\"pl-k\">=</span>logs_path,\n                         <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step,\n                         <span class=\"pl-v\">init_op</span><span class=\"pl-k\">=</span>init_op,\n                         <span class=\"pl-v\">save_model_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">600</span>)\n<span class=\"pl-k\">with</span> sv.managed_session(server.target) <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> sv.should_stop():\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>==============<span class=\"pl-pds\">'</span></span>)\nsv.stop()</pre></div>\n<p>The problem is that if I set <code>logdir</code> explicitly in tf.train.Supervisor, then the code above will met error like this:<code>NotFoundError (see above for traceback): Key weights/Variable not found in checkpoint</code>. But if I comment the lines about defining W1 and W2, then the code could work. So I assume there might be come issues in saving and restoring the checkpoint files in <code>tf.train.Supervisor</code> or maybe I did not use <code>tf.train.Supervisor</code> correctly.</p>", "body_text": "I am using CUDA 8.0, cuDNN 5.1, ubuntu 16.04, GPU: TitanX, tensorflow r0.12.\nAnd I met some problems when using tf.train.Supervisor in distributed training. I have simplified my  code shown as belown:\nfrom __future__ import print_function\nimport tensorflow as tf\n\nserver = tf.train.Server.create_local_server()\nlogs_path = \"mnist/logs\"\n\n\nglobal_step = tf.get_variable('global_step', [],\n                              initializer=tf.constant_initializer(0),\n                              trainable=False)\nwith tf.name_scope(\"weights\"):\n    W1 = tf.Variable(tf.random_normal([784, 100]))\n    W2 = tf.Variable(tf.random_normal([784, 100]))\n\ninit_op = tf.global_variables_initializer()\nprint(\"Variables initialized ...\")\nsv = tf.train.Supervisor(is_chief=True,\n                         logdir=logs_path,\n                         global_step=global_step,\n                         init_op=init_op,\n                         save_model_secs=600)\nwith sv.managed_session(server.target) as sess:\n    while not sv.should_stop():\n        print('==============')\nsv.stop()\nThe problem is that if I set logdir explicitly in tf.train.Supervisor, then the code above will met error like this:NotFoundError (see above for traceback): Key weights/Variable not found in checkpoint. But if I comment the lines about defining W1 and W2, then the code could work. So I assume there might be come issues in saving and restoring the checkpoint files in tf.train.Supervisor or maybe I did not use tf.train.Supervisor correctly.", "body": "I am using CUDA 8.0, cuDNN 5.1, ubuntu 16.04, GPU: TitanX, tensorflow r0.12.\r\nAnd I met some problems when using tf.train.Supervisor in distributed training. I have simplified my  code shown as belown:\r\n```python\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\nserver = tf.train.Server.create_local_server()\r\nlogs_path = \"mnist/logs\"\r\n\r\n\r\nglobal_step = tf.get_variable('global_step', [],\r\n                              initializer=tf.constant_initializer(0),\r\n                              trainable=False)\r\nwith tf.name_scope(\"weights\"):\r\n    W1 = tf.Variable(tf.random_normal([784, 100]))\r\n    W2 = tf.Variable(tf.random_normal([784, 100]))\r\n\r\ninit_op = tf.global_variables_initializer()\r\nprint(\"Variables initialized ...\")\r\nsv = tf.train.Supervisor(is_chief=True,\r\n                         logdir=logs_path,\r\n                         global_step=global_step,\r\n                         init_op=init_op,\r\n                         save_model_secs=600)\r\nwith sv.managed_session(server.target) as sess:\r\n    while not sv.should_stop():\r\n        print('==============')\r\nsv.stop()\r\n```\r\nThe problem is that if I set `logdir` explicitly in tf.train.Supervisor, then the code above will met error like this:`NotFoundError (see above for traceback): Key weights/Variable not found in checkpoint`. But if I comment the lines about defining W1 and W2, then the code could work. So I assume there might be come issues in saving and restoring the checkpoint files in `tf.train.Supervisor` or maybe I did not use `tf.train.Supervisor` correctly. \r\n"}