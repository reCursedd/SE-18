{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23511", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23511/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23511/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23511/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23511", "id": 377263402, "node_id": "MDU6SXNzdWUzNzcyNjM0MDI=", "number": 23511, "title": "tf.GradientTape() not support to MaxPool3D", "user": {"login": "so77id", "id": 4089219, "node_id": "MDQ6VXNlcjQwODkyMTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4089219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/so77id", "html_url": "https://github.com/so77id", "followers_url": "https://api.github.com/users/so77id/followers", "following_url": "https://api.github.com/users/so77id/following{/other_user}", "gists_url": "https://api.github.com/users/so77id/gists{/gist_id}", "starred_url": "https://api.github.com/users/so77id/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/so77id/subscriptions", "organizations_url": "https://api.github.com/users/so77id/orgs", "repos_url": "https://api.github.com/users/so77id/repos", "events_url": "https://api.github.com/users/so77id/events{/privacy}", "received_events_url": "https://api.github.com/users/so77id/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547147, "node_id": "MDU6TGFiZWwxMDk3NTQ3MTQ3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:ops", "name": "comp:ops", "color": "0052cc", "default": false}, {"id": 779195454, "node_id": "MDU6TGFiZWw3NzkxOTU0NTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/pending%20merge%20internally", "name": "pending merge internally", "color": "efad99", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-11-05T06:13:47Z", "updated_at": "2018-11-09T19:04:32Z", "closed_at": "2018-11-09T19:04:32Z", "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code: <strong>yes</strong></li>\n<li>OS Platform and Distribution: <strong>Linux Ubuntu 16.04</strong></li>\n<li>TensorFlow installed from (source or binary): <strong>binary, pip3 install</strong></li>\n<li>TensorFlow version (use command below): <strong>1.10</strong></li>\n<li>Python version: <strong>3.5</strong></li>\n<li>Bazel version (if compiling from source): N/A</li>\n<li>GCC/Compiler version (if compiling from source): N/A</li>\n<li>CUDA/cuDNN version: <strong>CUDA Version 9.1.85</strong></li>\n<li>GPU model and memory: <strong>TITAN V, 12066MB</strong></li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nI'm trying to create a simple cnn3d network, which contains a convolutional layer in 3d, a max pooling in 3d and a fully connected layer, all this using eager mode.<br>\nWhen trying to calculate the gradients with an optimizer I get this error:<br>\n<code>TypeError: 'NoneType' object has no attribute '__getitem__'</code><br>\nThen, when trying to remove the MaxPool3D layer or replace with AveragePooling3D, the gradient calculation works without problems.</p>\n<p><strong>Describe the expected behavior</strong><br>\nCalculate the gradients using the eager mode with tf.GradientTape() in a network with a Conv3D layer and a MaxPool3D layer.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import, division, print_function\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> enable eager mode</span>\ntf.enable_eager_execution()\ntf.set_random_seed(<span class=\"pl-c1\">0</span>)\nnp.random.seed(<span class=\"pl-c1\">0</span>)\n\n\nx <span class=\"pl-k\">=</span> tf.random_uniform((<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">3</span>))\ny <span class=\"pl-k\">=</span> tf.random_uniform((<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">5</span>))\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyModel</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">keras</span>.<span class=\"pl-e\">Model</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">super</span>(MyModel, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.conv3d_1 <span class=\"pl-k\">=</span> tf.keras.layers.Conv3D(<span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">6</span>,\n                                           <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>))\n    <span class=\"pl-c1\">self</span>.max_pool_1 <span class=\"pl-k\">=</span> tf.keras.layers.MaxPool3D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))\n    <span class=\"pl-c1\">self</span>.flatten <span class=\"pl-k\">=</span> tf.keras.layers.Flatten()\n    <span class=\"pl-c1\">self</span>.dense_1 <span class=\"pl-k\">=</span> tf.layers.Dense(<span class=\"pl-c1\">5</span>)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Run the model.<span class=\"pl-pds\">\"\"\"</span></span>\n    model <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv3d_1(<span class=\"pl-c1\">input</span>)\n    model <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.max_pool_1(model)\n    model <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.flatten(model)\n    model <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.dense_1(model)\n    \n    <span class=\"pl-k\">return</span> model\n\nmodel <span class=\"pl-k\">=</span> MyModel()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>):\n  logits <span class=\"pl-k\">=</span> model(x)\n  <span class=\"pl-k\">return</span> tf.losses.softmax_cross_entropy(<span class=\"pl-v\">onehot_labels</span><span class=\"pl-k\">=</span>y, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits), logits\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">grad</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">targets</span>):\n  <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n    loss_value, logits <span class=\"pl-k\">=</span> loss(model, inputs, targets)\n  <span class=\"pl-k\">return</span> loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Optimize the model</span>\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\nglobal_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\nloss_value, logits, grads <span class=\"pl-k\">=</span> grad(model, x, y)\noptimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(grads, model.variables), global_step)</pre></div>\n<p><strong>Other info / logs</strong><br>\nThe traceback error:</p>\n<div class=\"highlight highlight-source-python\"><pre>TypeErrorTraceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">4</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">56cdb9bbe4e5</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n     <span class=\"pl-c1\">44</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Optimize the model</span>\n     <span class=\"pl-c1\">45</span> optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">46</span> loss_value, logits, grads <span class=\"pl-k\">=</span> grad(model, x, y)\n     <span class=\"pl-c1\">47</span> optimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(grads, model.variables), global_step)\n\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">4</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">56cdb9bbe4e5</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> grad(model, inputs, targets)\n     <span class=\"pl-c1\">40</span>   <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n     <span class=\"pl-c1\">41</span>     loss_value, logits <span class=\"pl-k\">=</span> loss(model, inputs, targets)\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">42</span>   <span class=\"pl-k\">return</span> loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\n     <span class=\"pl-c1\">43</span> \n     <span class=\"pl-c1\">44</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Optimize the model</span>\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>eager<span class=\"pl-k\">/</span>backprop.pyc <span class=\"pl-k\">in</span> gradient(<span class=\"pl-c1\">self</span>, target, sources, output_gradients)\n    <span class=\"pl-c1\">899</span>         nest.flatten(target),\n    <span class=\"pl-c1\">900</span>         flat_sources,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">901</span>         output_gradients<span class=\"pl-k\">=</span>output_gradients)\n    <span class=\"pl-c1\">902</span> \n    <span class=\"pl-c1\">903</span>     <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">self</span>._persistent:\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>eager<span class=\"pl-k\">/</span>imperative_grad.pyc <span class=\"pl-k\">in</span> imperative_grad(tape, target, sources, output_gradients)\n     <span class=\"pl-c1\">62</span>       target,\n     <span class=\"pl-c1\">63</span>       sources,\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">64</span>       output_gradients)\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>eager<span class=\"pl-k\">/</span>backprop.pyc <span class=\"pl-k\">in</span> _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\n    <span class=\"pl-c1\">115</span>     <span class=\"pl-k\">return</span> [<span class=\"pl-c1\">None</span>] <span class=\"pl-k\">*</span> num_inputs\n    <span class=\"pl-c1\">116</span> \n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">117</span>   <span class=\"pl-k\">return</span> grad_fn(mock_op, <span class=\"pl-k\">*</span>out_grads)\n    <span class=\"pl-c1\">118</span> \n    <span class=\"pl-c1\">119</span> \n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>nn_grad.pyc <span class=\"pl-k\">in</span> _MaxPool3DGrad(op, grad)\n    <span class=\"pl-c1\">180</span>   <span class=\"pl-k\">return</span> gen_nn_ops.max_pool3d_grad(\n    <span class=\"pl-c1\">181</span>       op.inputs[<span class=\"pl-c1\">0</span>],\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">182</span>       op.outputs[<span class=\"pl-c1\">0</span>],\n    <span class=\"pl-c1\">183</span>       grad,\n    <span class=\"pl-c1\">184</span>       <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>op.get_attr(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ksize<span class=\"pl-pds\">\"</span></span>),\n\n<span class=\"pl-c1\">TypeError</span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>NoneType<span class=\"pl-pds\">'</span></span> <span class=\"pl-c1\">object</span> has no attribute <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__getitem__<span class=\"pl-pds\">'</span></span></pre></div>", "body_text": "System information\n\nHave I written custom code: yes\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary, pip3 install\nTensorFlow version (use command below): 1.10\nPython version: 3.5\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: CUDA Version 9.1.85\nGPU model and memory: TITAN V, 12066MB\n\nDescribe the current behavior\nI'm trying to create a simple cnn3d network, which contains a convolutional layer in 3d, a max pooling in 3d and a fully connected layer, all this using eager mode.\nWhen trying to calculate the gradients with an optimizer I get this error:\nTypeError: 'NoneType' object has no attribute '__getitem__'\nThen, when trying to remove the MaxPool3D layer or replace with AveragePooling3D, the gradient calculation works without problems.\nDescribe the expected behavior\nCalculate the gradients using the eager mode with tf.GradientTape() in a network with a Conv3D layer and a MaxPool3D layer.\nCode to reproduce the issue\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\n# enable eager mode\ntf.enable_eager_execution()\ntf.set_random_seed(0)\nnp.random.seed(0)\n\n\nx = tf.random_uniform((10,5,10,10,3))\ny = tf.random_uniform((10, 5))\n\n\nclass MyModel(tf.keras.Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.conv3d_1 = tf.keras.layers.Conv3D(filters=6,\n                                           kernel_size=(3,3,3))\n    self.max_pool_1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))\n    self.flatten = tf.keras.layers.Flatten()\n    self.dense_1 = tf.layers.Dense(5)\n\n  def call(self, input, training=False):\n    \"\"\"Run the model.\"\"\"\n    model = self.conv3d_1(input)\n    model = self.max_pool_1(model)\n    model = self.flatten(model)\n    model = self.dense_1(model)\n    \n    return model\n\nmodel = MyModel()\n\ndef loss(model, x, y):\n  logits = model(x)\n  return tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits), logits\n\ndef grad(model, inputs, targets):\n  with tf.GradientTape() as tape:\n    loss_value, logits = loss(model, inputs, targets)\n  return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\n\n# Optimize the model\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nglobal_step = tf.train.get_or_create_global_step()\nloss_value, logits, grads = grad(model, x, y)\noptimizer.apply_gradients(zip(grads, model.variables), global_step)\nOther info / logs\nThe traceback error:\nTypeErrorTraceback (most recent call last)\n<ipython-input-4-56cdb9bbe4e5> in <module>()\n     44 # Optimize the model\n     45 optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n---> 46 loss_value, logits, grads = grad(model, x, y)\n     47 optimizer.apply_gradients(zip(grads, model.variables), global_step)\n\n<ipython-input-4-56cdb9bbe4e5> in grad(model, inputs, targets)\n     40   with tf.GradientTape() as tape:\n     41     loss_value, logits = loss(model, inputs, targets)\n---> 42   return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\n     43 \n     44 # Optimize the model\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in gradient(self, target, sources, output_gradients)\n    899         nest.flatten(target),\n    900         flat_sources,\n--> 901         output_gradients=output_gradients)\n    902 \n    903     if not self._persistent:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/imperative_grad.pyc in imperative_grad(tape, target, sources, output_gradients)\n     62       target,\n     63       sources,\n---> 64       output_gradients)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\n    115     return [None] * num_inputs\n    116 \n--> 117   return grad_fn(mock_op, *out_grads)\n    118 \n    119 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.pyc in _MaxPool3DGrad(op, grad)\n    180   return gen_nn_ops.max_pool3d_grad(\n    181       op.inputs[0],\n--> 182       op.outputs[0],\n    183       grad,\n    184       ksize=op.get_attr(\"ksize\"),\n\nTypeError: 'NoneType' object has no attribute '__getitem__'", "body": "**System information**\r\n- Have I written custom code: **yes**\r\n- OS Platform and Distribution: **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **binary, pip3 install**\r\n- TensorFlow version (use command below): **1.10**\r\n- Python version: **3.5**\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: **CUDA Version 9.1.85**\r\n- GPU model and memory: **TITAN V, 12066MB**\r\n\r\n**Describe the current behavior**\r\nI'm trying to create a simple cnn3d network, which contains a convolutional layer in 3d, a max pooling in 3d and a fully connected layer, all this using eager mode.\r\nWhen trying to calculate the gradients with an optimizer I get this error:\r\n`TypeError: 'NoneType' object has no attribute '__getitem__'`\r\nThen, when trying to remove the MaxPool3D layer or replace with AveragePooling3D, the gradient calculation works without problems.\r\n\r\n**Describe the expected behavior**\r\nCalculate the gradients using the eager mode with tf.GradientTape() in a network with a Conv3D layer and a MaxPool3D layer.\r\n\r\n**Code to reproduce the issue**\r\n```python3\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport tensorflow as tf\r\n\r\n# enable eager mode\r\ntf.enable_eager_execution()\r\ntf.set_random_seed(0)\r\nnp.random.seed(0)\r\n\r\n\r\nx = tf.random_uniform((10,5,10,10,3))\r\ny = tf.random_uniform((10, 5))\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.conv3d_1 = tf.keras.layers.Conv3D(filters=6,\r\n                                           kernel_size=(3,3,3))\r\n    self.max_pool_1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))\r\n    self.flatten = tf.keras.layers.Flatten()\r\n    self.dense_1 = tf.layers.Dense(5)\r\n\r\n  def call(self, input, training=False):\r\n    \"\"\"Run the model.\"\"\"\r\n    model = self.conv3d_1(input)\r\n    model = self.max_pool_1(model)\r\n    model = self.flatten(model)\r\n    model = self.dense_1(model)\r\n    \r\n    return model\r\n\r\nmodel = MyModel()\r\n\r\ndef loss(model, x, y):\r\n  logits = model(x)\r\n  return tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits), logits\r\n\r\ndef grad(model, inputs, targets):\r\n  with tf.GradientTape() as tape:\r\n    loss_value, logits = loss(model, inputs, targets)\r\n  return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\r\n\r\n# Optimize the model\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\nglobal_step = tf.train.get_or_create_global_step()\r\nloss_value, logits, grads = grad(model, x, y)\r\noptimizer.apply_gradients(zip(grads, model.variables), global_step)\r\n```\r\n\r\n**Other info / logs**\r\nThe traceback error:\r\n\r\n```python3\r\nTypeErrorTraceback (most recent call last)\r\n<ipython-input-4-56cdb9bbe4e5> in <module>()\r\n     44 # Optimize the model\r\n     45 optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n---> 46 loss_value, logits, grads = grad(model, x, y)\r\n     47 optimizer.apply_gradients(zip(grads, model.variables), global_step)\r\n\r\n<ipython-input-4-56cdb9bbe4e5> in grad(model, inputs, targets)\r\n     40   with tf.GradientTape() as tape:\r\n     41     loss_value, logits = loss(model, inputs, targets)\r\n---> 42   return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)\r\n     43 \r\n     44 # Optimize the model\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in gradient(self, target, sources, output_gradients)\r\n    899         nest.flatten(target),\r\n    900         flat_sources,\r\n--> 901         output_gradients=output_gradients)\r\n    902 \r\n    903     if not self._persistent:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/imperative_grad.pyc in imperative_grad(tape, target, sources, output_gradients)\r\n     62       target,\r\n     63       sources,\r\n---> 64       output_gradients)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\r\n    115     return [None] * num_inputs\r\n    116 \r\n--> 117   return grad_fn(mock_op, *out_grads)\r\n    118 \r\n    119 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.pyc in _MaxPool3DGrad(op, grad)\r\n    180   return gen_nn_ops.max_pool3d_grad(\r\n    181       op.inputs[0],\r\n--> 182       op.outputs[0],\r\n    183       grad,\r\n    184       ksize=op.get_attr(\"ksize\"),\r\n\r\nTypeError: 'NoneType' object has no attribute '__getitem__'\r\n```\r\n\r\n"}