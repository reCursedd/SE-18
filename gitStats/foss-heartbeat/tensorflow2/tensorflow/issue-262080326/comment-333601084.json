{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333601084", "html_url": "https://github.com/tensorflow/tensorflow/issues/13446#issuecomment-333601084", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13446", "id": 333601084, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzYwMTA4NA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-02T17:15:59Z", "updated_at": "2017-10-02T17:16:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Right, it is trying very hard to be reproducible, but there's no indication in the code of when it should reshuffle. (Consider that if it did reshuffle each iteration, you wouldn't be able to reproduce the same sequence within the same session.) If you're using <code>tf.set_random_seed()</code>, you need to do a little more work to get different orders on each epoch. The easiest workaround would be to specify an additional seed as a placeholder, which you then feed with a different value on each :</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">input_pipeline</span>(<span class=\"pl-smi\">filenames</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.</span>\n    dataset <span class=\"pl-k\">=</span> (tf.contrib.data.TextLineDataset(filenames)\n               .map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">line</span>: tf.decode_csv(\n                    line, <span class=\"pl-v\">record_defaults</span><span class=\"pl-k\">=</span>[[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1<span class=\"pl-pds\">'</span></span>], [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1<span class=\"pl-pds\">'</span></span>]], <span class=\"pl-v\">field_delim</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>-<span class=\"pl-pds\">'</span></span>))\n               .shuffle(<span class=\"pl-v\">buffer_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Equivalent to min_after_dequeue=10.</span>\n               .batch(batch_size))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Return an *initializable* iterator over the dataset, which will allow us to</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> re-initialize it at the beginning of each epoch.</span>\n    <span class=\"pl-k\">return</span> dataset.make_initializable_iterator()\n\nfilenames<span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1.txt<span class=\"pl-pds\">'</span></span>]\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\nnum_epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\nseed <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>())\niterator <span class=\"pl-k\">=</span> input_pipeline(filenames, batch_size, seed)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> `a1`, `a2`, and `a3` represent the next element to be retrieved from the iterator.</span>\na1, a2, a3 <span class=\"pl-k\">=</span> iterator.get_next()\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_epochs):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Resets the iterator at the beginning of an epoch.</span>\n        sess.run(iterator.initializer, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{seed: epoch})\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch:<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (epoch))\n        <span class=\"pl-k\">try</span>:\n            <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n                a, b, c <span class=\"pl-k\">=</span> sess.run([a1, a2, a3])\n                <span class=\"pl-c1\">print</span>(a, b, c)\n        <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> This will be raised when you reach the end of an epoch (i.e. the</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> iterator has no more elements).</span>\n            <span class=\"pl-k\">pass</span></pre></div>", "body_text": "Right, it is trying very hard to be reproducible, but there's no indication in the code of when it should reshuffle. (Consider that if it did reshuffle each iteration, you wouldn't be able to reproduce the same sequence within the same session.) If you're using tf.set_random_seed(), you need to do a little more work to get different orders on each epoch. The easiest workaround would be to specify an additional seed as a placeholder, which you then feed with a different value on each :\ndef input_pipeline(filenames, batch_size, seed=None):\n    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\n    dataset = (tf.contrib.data.TextLineDataset(filenames)\n               .map(lambda line: tf.decode_csv(\n                    line, record_defaults=[['1'], ['1'], ['1']], field_delim='-'))\n               .shuffle(buffer_size=10, seed=seed)  # Equivalent to min_after_dequeue=10.\n               .batch(batch_size))\n\n    # Return an *initializable* iterator over the dataset, which will allow us to\n    # re-initialize it at the beginning of each epoch.\n    return dataset.make_initializable_iterator()\n\nfilenames=['1.txt']\nbatch_size = 3\nnum_epochs = 3\nseed = tf.placeholder(tf.int64, shape=())\niterator = input_pipeline(filenames, batch_size, seed)\n\n# `a1`, `a2`, and `a3` represent the next element to be retrieved from the iterator.\na1, a2, a3 = iterator.get_next()\n\nwith tf.Session() as sess:\n    for epoch in range(num_epochs):\n        # Resets the iterator at the beginning of an epoch.\n        sess.run(iterator.initializer, feed_dict={seed: epoch})\n        print('epoch:%d' % (epoch))\n        try:\n            while True:\n                a, b, c = sess.run([a1, a2, a3])\n                print(a, b, c)\n        except tf.errors.OutOfRangeError:\n            # This will be raised when you reach the end of an epoch (i.e. the\n            # iterator has no more elements).\n            pass", "body": "Right, it is trying very hard to be reproducible, but there's no indication in the code of when it should reshuffle. (Consider that if it did reshuffle each iteration, you wouldn't be able to reproduce the same sequence within the same session.) If you're using `tf.set_random_seed()`, you need to do a little more work to get different orders on each epoch. The easiest workaround would be to specify an additional seed as a placeholder, which you then feed with a different value on each :\r\n\r\n```python\r\ndef input_pipeline(filenames, batch_size, seed=None):\r\n    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\r\n    dataset = (tf.contrib.data.TextLineDataset(filenames)\r\n               .map(lambda line: tf.decode_csv(\r\n                    line, record_defaults=[['1'], ['1'], ['1']], field_delim='-'))\r\n               .shuffle(buffer_size=10, seed=seed)  # Equivalent to min_after_dequeue=10.\r\n               .batch(batch_size))\r\n\r\n    # Return an *initializable* iterator over the dataset, which will allow us to\r\n    # re-initialize it at the beginning of each epoch.\r\n    return dataset.make_initializable_iterator()\r\n\r\nfilenames=['1.txt']\r\nbatch_size = 3\r\nnum_epochs = 3\r\nseed = tf.placeholder(tf.int64, shape=())\r\niterator = input_pipeline(filenames, batch_size, seed)\r\n\r\n# `a1`, `a2`, and `a3` represent the next element to be retrieved from the iterator.\r\na1, a2, a3 = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    for epoch in range(num_epochs):\r\n        # Resets the iterator at the beginning of an epoch.\r\n        sess.run(iterator.initializer, feed_dict={seed: epoch})\r\n        print('epoch:%d' % (epoch))\r\n        try:\r\n            while True:\r\n                a, b, c = sess.run([a1, a2, a3])\r\n                print(a, b, c)\r\n        except tf.errors.OutOfRangeError:\r\n            # This will be raised when you reach the end of an epoch (i.e. the\r\n            # iterator has no more elements).\r\n            pass\r\n```"}