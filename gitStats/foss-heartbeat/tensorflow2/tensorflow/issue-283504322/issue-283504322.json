{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15518", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15518/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15518/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15518/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15518", "id": 283504322, "node_id": "MDU6SXNzdWUyODM1MDQzMjI=", "number": 15518, "title": "Possible memory leak with tf.py_func() with distributed Tensorflow?", "user": {"login": "Hong-Xiang", "id": 11002856, "node_id": "MDQ6VXNlcjExMDAyODU2", "avatar_url": "https://avatars2.githubusercontent.com/u/11002856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hong-Xiang", "html_url": "https://github.com/Hong-Xiang", "followers_url": "https://api.github.com/users/Hong-Xiang/followers", "following_url": "https://api.github.com/users/Hong-Xiang/following{/other_user}", "gists_url": "https://api.github.com/users/Hong-Xiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hong-Xiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hong-Xiang/subscriptions", "organizations_url": "https://api.github.com/users/Hong-Xiang/orgs", "repos_url": "https://api.github.com/users/Hong-Xiang/repos", "events_url": "https://api.github.com/users/Hong-Xiang/events{/privacy}", "received_events_url": "https://api.github.com/users/Hong-Xiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-12-20T09:47:28Z", "updated_at": "2017-12-20T19:39:15Z", "closed_at": "2017-12-20T19:39:15Z", "author_association": "NONE", "body_html": "<p>When running Tensorflow as an distributed process to provide data with tf.data, it gradually consumes more and more memory, and finally consumes all memory of the system.</p>\n<p>Scripts to reproduce:<br>\nWe use a dummy dataset which produce [128, 28, 28, 1] tensors.<br>\nCase1: Without distribute, which works fine, it will only consume 429Mb memory, no matter how many batches we run.<br>\nCodes in <code>test1.py</code>:</p>\n<pre><code>#test1.py\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\n\ndef dataset_generator():\n    while True:\n        yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\ndataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\ndataset = dataset.batch(128)\nvalue = dataset.make_one_shot_iterator().get_next()\n\nsess = tf.Session()\nfor _ in tqdm(range(100000), ascii=True):\n    sess.run(value)\n</code></pre>\n<p>Case: With distribute, it will consumes more and more memory while running more and more batches. It consumes 10+Gb with less than 1M batches. Use the following two commands in two processes to run the <code>test2.py</code>:</p>\n<pre><code>CUDA_VISIBLE_DEVICES=\"\" python test2.py dataset\nCUDA_VISIBLE_DEVICES=\"\" python test2.py test\n</code></pre>\n<p>Codes in <code>test2.py</code></p>\n<pre><code># test2.py\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\nimport sys\ndef main(role):\n    def dataset_generator():\n        while True:\n            yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\n    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'], 'test': ['localhost:2002']})\n    if role == 'dataset':\n        server = tf.train.Server(cluster, 'dataset', 0)\n    elif role == 'test':\n        server = tf.train.Server(cluster, 'test', 0)\n    else:\n        raise ValueError(\"Uknown role {}.\".format(role))\n    with tf.device('/job:dataset/task:0')    :\n        dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\n        dataset = dataset.batch(128)\n        value = dataset.make_one_shot_iterator().get_next()\n    if role == 'dataset':\n        server.join()\n    elif role == 'test':\n        sess = tf.Session(target=server.target)\n        for _ in tqdm(range(100000000), ascii=True):\n            sess.run(value)\n            \nif __name__ == \"__main__\":\n    main(sys.argv[1])\n</code></pre>\n<p>Tensorflow: v1.4.0-rc1-11-g130a514 1.4.0<br>\nOS: ubuntu mate 16.04.1<br>\nPython: 3.6.1 (conda 4.3.30)</p>", "body_text": "When running Tensorflow as an distributed process to provide data with tf.data, it gradually consumes more and more memory, and finally consumes all memory of the system.\nScripts to reproduce:\nWe use a dummy dataset which produce [128, 28, 28, 1] tensors.\nCase1: Without distribute, which works fine, it will only consume 429Mb memory, no matter how many batches we run.\nCodes in test1.py:\n#test1.py\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\n\ndef dataset_generator():\n    while True:\n        yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\ndataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\ndataset = dataset.batch(128)\nvalue = dataset.make_one_shot_iterator().get_next()\n\nsess = tf.Session()\nfor _ in tqdm(range(100000), ascii=True):\n    sess.run(value)\n\nCase: With distribute, it will consumes more and more memory while running more and more batches. It consumes 10+Gb with less than 1M batches. Use the following two commands in two processes to run the test2.py:\nCUDA_VISIBLE_DEVICES=\"\" python test2.py dataset\nCUDA_VISIBLE_DEVICES=\"\" python test2.py test\n\nCodes in test2.py\n# test2.py\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\nimport sys\ndef main(role):\n    def dataset_generator():\n        while True:\n            yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\n    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'], 'test': ['localhost:2002']})\n    if role == 'dataset':\n        server = tf.train.Server(cluster, 'dataset', 0)\n    elif role == 'test':\n        server = tf.train.Server(cluster, 'test', 0)\n    else:\n        raise ValueError(\"Uknown role {}.\".format(role))\n    with tf.device('/job:dataset/task:0')    :\n        dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\n        dataset = dataset.batch(128)\n        value = dataset.make_one_shot_iterator().get_next()\n    if role == 'dataset':\n        server.join()\n    elif role == 'test':\n        sess = tf.Session(target=server.target)\n        for _ in tqdm(range(100000000), ascii=True):\n            sess.run(value)\n            \nif __name__ == \"__main__\":\n    main(sys.argv[1])\n\nTensorflow: v1.4.0-rc1-11-g130a514 1.4.0\nOS: ubuntu mate 16.04.1\nPython: 3.6.1 (conda 4.3.30)", "body": "When running Tensorflow as an distributed process to provide data with tf.data, it gradually consumes more and more memory, and finally consumes all memory of the system.\r\n\r\nScripts to reproduce:\r\nWe use a dummy dataset which produce [128, 28, 28, 1] tensors. \r\nCase1: Without distribute, which works fine, it will only consume 429Mb memory, no matter how many batches we run.\r\nCodes in `test1.py`:\r\n```\r\n#test1.py\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\ndef dataset_generator():\r\n    while True:\r\n        yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\r\ndataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\r\ndataset = dataset.batch(128)\r\nvalue = dataset.make_one_shot_iterator().get_next()\r\n\r\nsess = tf.Session()\r\nfor _ in tqdm(range(100000), ascii=True):\r\n    sess.run(value)\r\n```\r\n\r\nCase: With distribute, it will consumes more and more memory while running more and more batches. It consumes 10+Gb with less than 1M batches. Use the following two commands in two processes to run the `test2.py`:\r\n```\r\nCUDA_VISIBLE_DEVICES=\"\" python test2.py dataset\r\nCUDA_VISIBLE_DEVICES=\"\" python test2.py test\r\n```\r\nCodes in `test2.py`\r\n```\r\n# test2.py\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport sys\r\ndef main(role):\r\n    def dataset_generator():\r\n        while True:\r\n            yield np.random.uniform(size=[28, 28, 1]).astype(np.float32)\r\n    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'], 'test': ['localhost:2002']})\r\n    if role == 'dataset':\r\n        server = tf.train.Server(cluster, 'dataset', 0)\r\n    elif role == 'test':\r\n        server = tf.train.Server(cluster, 'test', 0)\r\n    else:\r\n        raise ValueError(\"Uknown role {}.\".format(role))\r\n    with tf.device('/job:dataset/task:0')    :\r\n        dataset = tf.data.Dataset.from_generator(dataset_generator, tf.float32)\r\n        dataset = dataset.batch(128)\r\n        value = dataset.make_one_shot_iterator().get_next()\r\n    if role == 'dataset':\r\n        server.join()\r\n    elif role == 'test':\r\n        sess = tf.Session(target=server.target)\r\n        for _ in tqdm(range(100000000), ascii=True):\r\n            sess.run(value)\r\n            \r\nif __name__ == \"__main__\":\r\n    main(sys.argv[1])\r\n```\r\n\r\nTensorflow: v1.4.0-rc1-11-g130a514 1.4.0\r\nOS: ubuntu mate 16.04.1\r\nPython: 3.6.1 (conda 4.3.30)\r\n\r\n\r\n"}