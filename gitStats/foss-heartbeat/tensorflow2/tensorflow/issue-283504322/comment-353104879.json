{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/353104879", "html_url": "https://github.com/tensorflow/tensorflow/issues/15518#issuecomment-353104879", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15518", "id": 353104879, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzEwNDg3OQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-20T16:05:16Z", "updated_at": "2017-12-20T16:05:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think this is an issue in <code>tf.py_func()</code> (which <code>Dataset.from_generator()</code> uses internally). The following program exhibits the same memory leak:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> tqdm <span class=\"pl-k\">import</span> tqdm\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">role</span>):\n    cluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dataset<span class=\"pl-pds\">'</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>localhost:2001<span class=\"pl-pds\">'</span></span>],\n                                    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>localhost:2002<span class=\"pl-pds\">'</span></span>]})\n    <span class=\"pl-k\">if</span> role <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dataset<span class=\"pl-pds\">'</span></span>:\n        server <span class=\"pl-k\">=</span> tf.train.Server(cluster, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dataset<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">0</span>)\n    <span class=\"pl-k\">elif</span> role <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>:\n        server <span class=\"pl-k\">=</span> tf.train.Server(cluster, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">0</span>)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Uknown role <span class=\"pl-c1\">{}</span>.<span class=\"pl-pds\">\"</span></span>.format(role))\n\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/job:dataset/task:0<span class=\"pl-pds\">'</span></span>):\n        result <span class=\"pl-k\">=</span> tf.py_func(\n            <span class=\"pl-k\">lambda</span>: np.random.uniform(<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">1</span>]).astype(np.float32),\n            <span class=\"pl-v\">inp</span><span class=\"pl-k\">=</span>[], <span class=\"pl-v\">Tout</span><span class=\"pl-k\">=</span>tf.float32)\n\n    <span class=\"pl-k\">if</span> role <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dataset<span class=\"pl-pds\">'</span></span>:\n        server.join()\n    <span class=\"pl-k\">elif</span> role <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>:\n        sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>server.target)\n        <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> tqdm(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100000000</span>), <span class=\"pl-v\">ascii</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n            sess.run(result)\n            \n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    main(sys.argv[<span class=\"pl-c1\">1</span>])</pre></div>\n<p>Note that the memory leak is in the <code>\"/job:dataset/task:0\"</code> process (which executes the <code>tf.py_func()</code> op) and not the <code>\"/job:test/task:0\"</code> process (which creates the session). Also note that this setup isn't intended to be supported, but I suppose the note in  <a href=\"https://www.tensorflow.org/api_docs/python/tf/py_func\" rel=\"nofollow\">the <code>tf.py_func()</code> docs</a> could be read as allowing it:</p>\n<blockquote>\n<p>The operation must run in the same address space as the Python program that calls <code>tf.py_func()</code>. If you are using distributed TensorFlow, you must run a <code>tf.train.Server</code> in the same process as the program that calls <code>tf.py_func()</code> and you must pin the created operation to a device in that server (e.g. using with tf.device():).</p>\n</blockquote>\n<p>(In a sense, the program \"gets lucky\" because both processes call <code>tf.py_func()</code> in the same order, and so the same identifier is used for the registered Python function in each process.)</p>\n<p>I think the source of the leak is the \"decref cache\", which holds references to Python arrays that are passed without copying into the TensorFlow runtime, and can only be cleared when the GIL is held. The <a href=\"https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/lib/core/ndarray_tensor_bridge.cc#L52\"><code>ClearDecrefCache()</code></a> function is only called in the session code (and some TF Eager code): for example at the <a href=\"https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/client/tf_session_helper.cc#L88\">beginning</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/client/tf_session_helper.cc#L151\">end</a> of a <code>sess.run()</code> call. Since the cache is filling up in <code>\"/job:dataset/task:0\"</code> and it is only being cleared in <code>\"/job:test/task:0\"</code>, we see a memory leak in <code>\"/job:dataset/task:0\"</code>.</p>\n<p>I'll assign this to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a>, who added this mechanism, and might be able to suggest a fix or workaround.</p>", "body_text": "I think this is an issue in tf.py_func() (which Dataset.from_generator() uses internally). The following program exhibits the same memory leak:\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\nimport sys\ndef main(role):\n    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'],\n                                    'test': ['localhost:2002']})\n    if role == 'dataset':\n        server = tf.train.Server(cluster, 'dataset', 0)\n    elif role == 'test':\n        server = tf.train.Server(cluster, 'test', 0)\n    else:\n        raise ValueError(\"Uknown role {}.\".format(role))\n\n    with tf.device('/job:dataset/task:0'):\n        result = tf.py_func(\n            lambda: np.random.uniform(size=[28, 28, 1]).astype(np.float32),\n            inp=[], Tout=tf.float32)\n\n    if role == 'dataset':\n        server.join()\n    elif role == 'test':\n        sess = tf.Session(target=server.target)\n        for _ in tqdm(range(100000000), ascii=True):\n            sess.run(result)\n            \nif __name__ == \"__main__\":\n    main(sys.argv[1])\nNote that the memory leak is in the \"/job:dataset/task:0\" process (which executes the tf.py_func() op) and not the \"/job:test/task:0\" process (which creates the session). Also note that this setup isn't intended to be supported, but I suppose the note in  the tf.py_func() docs could be read as allowing it:\n\nThe operation must run in the same address space as the Python program that calls tf.py_func(). If you are using distributed TensorFlow, you must run a tf.train.Server in the same process as the program that calls tf.py_func() and you must pin the created operation to a device in that server (e.g. using with tf.device():).\n\n(In a sense, the program \"gets lucky\" because both processes call tf.py_func() in the same order, and so the same identifier is used for the registered Python function in each process.)\nI think the source of the leak is the \"decref cache\", which holds references to Python arrays that are passed without copying into the TensorFlow runtime, and can only be cleared when the GIL is held. The ClearDecrefCache() function is only called in the session code (and some TF Eager code): for example at the beginning and end of a sess.run() call. Since the cache is filling up in \"/job:dataset/task:0\" and it is only being cleared in \"/job:test/task:0\", we see a memory leak in \"/job:dataset/task:0\".\nI'll assign this to @alextp, who added this mechanism, and might be able to suggest a fix or workaround.", "body": "I think this is an issue in `tf.py_func()` (which `Dataset.from_generator()` uses internally). The following program exhibits the same memory leak:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport sys\r\ndef main(role):\r\n    cluster = tf.train.ClusterSpec({'dataset': ['localhost:2001'],\r\n                                    'test': ['localhost:2002']})\r\n    if role == 'dataset':\r\n        server = tf.train.Server(cluster, 'dataset', 0)\r\n    elif role == 'test':\r\n        server = tf.train.Server(cluster, 'test', 0)\r\n    else:\r\n        raise ValueError(\"Uknown role {}.\".format(role))\r\n\r\n    with tf.device('/job:dataset/task:0'):\r\n        result = tf.py_func(\r\n            lambda: np.random.uniform(size=[28, 28, 1]).astype(np.float32),\r\n            inp=[], Tout=tf.float32)\r\n\r\n    if role == 'dataset':\r\n        server.join()\r\n    elif role == 'test':\r\n        sess = tf.Session(target=server.target)\r\n        for _ in tqdm(range(100000000), ascii=True):\r\n            sess.run(result)\r\n            \r\nif __name__ == \"__main__\":\r\n    main(sys.argv[1])\r\n```\r\n\r\nNote that the memory leak is in the `\"/job:dataset/task:0\"` process (which executes the `tf.py_func()` op) and not the `\"/job:test/task:0\"` process (which creates the session). Also note that this setup isn't intended to be supported, but I suppose the note in  [the `tf.py_func()` docs](https://www.tensorflow.org/api_docs/python/tf/py_func) could be read as allowing it:\r\n\r\n> The operation must run in the same address space as the Python program that calls `tf.py_func()`. If you are using distributed TensorFlow, you must run a `tf.train.Server` in the same process as the program that calls `tf.py_func()` and you must pin the created operation to a device in that server (e.g. using with tf.device():).\r\n\r\n(In a sense, the program \"gets lucky\" because both processes call `tf.py_func()` in the same order, and so the same identifier is used for the registered Python function in each process.)\r\n\r\nI think the source of the leak is the \"decref cache\", which holds references to Python arrays that are passed without copying into the TensorFlow runtime, and can only be cleared when the GIL is held. The [`ClearDecrefCache()`](https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/lib/core/ndarray_tensor_bridge.cc#L52) function is only called in the session code (and some TF Eager code): for example at the [beginning](https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/client/tf_session_helper.cc#L88) and [end](https://github.com/tensorflow/tensorflow/blob/810394550571c5feb333cb6da66afb4b20c3bd85/tensorflow/python/client/tf_session_helper.cc#L151) of a `sess.run()` call. Since the cache is filling up in `\"/job:dataset/task:0\"` and it is only being cleared in `\"/job:test/task:0\"`, we see a memory leak in `\"/job:dataset/task:0\"`.\r\n\r\nI'll assign this to @alextp, who added this mechanism, and might be able to suggest a fix or workaround."}