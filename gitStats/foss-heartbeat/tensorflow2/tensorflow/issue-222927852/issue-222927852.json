{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9322", "id": 222927852, "node_id": "MDU6SXNzdWUyMjI5Mjc4NTI=", "number": 9322, "title": "TensorFlow 60-80% slower than PyTorch on training Wide ResNet", "user": {"login": "taion", "id": 3112159, "node_id": "MDQ6VXNlcjMxMTIxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3112159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taion", "html_url": "https://github.com/taion", "followers_url": "https://api.github.com/users/taion/followers", "following_url": "https://api.github.com/users/taion/following{/other_user}", "gists_url": "https://api.github.com/users/taion/gists{/gist_id}", "starred_url": "https://api.github.com/users/taion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taion/subscriptions", "organizations_url": "https://api.github.com/users/taion/orgs", "repos_url": "https://api.github.com/users/taion/repos", "events_url": "https://api.github.com/users/taion/events{/privacy}", "received_events_url": "https://api.github.com/users/taion/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 39, "created_at": "2017-04-20T02:14:02Z", "updated_at": "2017-05-08T16:20:40Z", "closed_at": "2017-04-20T19:28:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a></p>\n<p>From <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"204598482\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7187\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7187/hovercard?comment_id=295502315&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-295502315\">#7187 (comment)</a></p>\n<p>On an AWS p2.xlarge, using the <code>tensorflow/tensorflow:1.0.1-devel-gpu</code> Docker image as a base, I see ~270 ms per epoch while training a WRN-16-4 without dropout on CIFAR-10.</p>\n<p>Using a PyTorch implementation from <a href=\"https://github.com/xternalz/WideResNet-pytorch\">https://github.com/xternalz/WideResNet-pytorch</a>, I see instead ~150 ms per epoch for the same.</p>\n<p>My implementation of Wide ResNets uses NCHW and fused batch norm. It does use <code>feed_dict</code> for data loading, but I've observed with <code>nvidia-smi</code> that my GPU utilization stays near 100%.</p>\n<hr>\n<p>To reproduce:</p>\n<ul>\n<li>Clone <a href=\"https://github.com/4Catalyzer/dl-papers\">https://github.com/4Catalyzer/dl-papers</a>, and go to that directory.</li>\n<li>Check out the <a href=\"https://github.com/4Catalyzer/dl-papers/tree/benchmark\"><code>benchmark</code></a> branch.</li>\n<li>Build the Docker image, which is based on the Docker hub image above:</li>\n</ul>\n<pre><code>$ docker build -t dl-papers .\n</code></pre>\n<ul>\n<li>Run the Docker image using NVIDIA Docker:</li>\n</ul>\n<pre><code>$ nvidia-docker run --rm -it dl-papers /bin/bash\n</code></pre>\n<ul>\n<li>Run the TF WRN-16-4 training:</li>\n</ul>\n<pre><code># python -m dl_papers.wide_resnet.train cifar10\n</code></pre>\n<ul>\n<li>Observe the logged batch timings, then kill the process.</li>\n<li>In the same Docker container up the PyTorch Wide ResNet example:</li>\n</ul>\n<pre><code># cd ..\n# pip install http://download.pytorch.org/whl/cu80/torch-0.1.11.post5-cp27-none-linux_x86_64.whl\n# pip install torchvision tensorboard_logger\n# git clone https://github.com/xternalz/WideResNet-pytorch.git\n# cd WideResNet-pytorch\n</code></pre>\n<ul>\n<li>Run PyTorch training:</li>\n</ul>\n<pre><code># python train.py --dataset cifar10 --layers 16 --widen-factor 4 -p 1\n</code></pre>\n<ul>\n<li>Observe logged batch timings.</li>\n</ul>", "body_text": "cc @tfboyd\nFrom #7187 (comment)\nOn an AWS p2.xlarge, using the tensorflow/tensorflow:1.0.1-devel-gpu Docker image as a base, I see ~270 ms per epoch while training a WRN-16-4 without dropout on CIFAR-10.\nUsing a PyTorch implementation from https://github.com/xternalz/WideResNet-pytorch, I see instead ~150 ms per epoch for the same.\nMy implementation of Wide ResNets uses NCHW and fused batch norm. It does use feed_dict for data loading, but I've observed with nvidia-smi that my GPU utilization stays near 100%.\n\nTo reproduce:\n\nClone https://github.com/4Catalyzer/dl-papers, and go to that directory.\nCheck out the benchmark branch.\nBuild the Docker image, which is based on the Docker hub image above:\n\n$ docker build -t dl-papers .\n\n\nRun the Docker image using NVIDIA Docker:\n\n$ nvidia-docker run --rm -it dl-papers /bin/bash\n\n\nRun the TF WRN-16-4 training:\n\n# python -m dl_papers.wide_resnet.train cifar10\n\n\nObserve the logged batch timings, then kill the process.\nIn the same Docker container up the PyTorch Wide ResNet example:\n\n# cd ..\n# pip install http://download.pytorch.org/whl/cu80/torch-0.1.11.post5-cp27-none-linux_x86_64.whl\n# pip install torchvision tensorboard_logger\n# git clone https://github.com/xternalz/WideResNet-pytorch.git\n# cd WideResNet-pytorch\n\n\nRun PyTorch training:\n\n# python train.py --dataset cifar10 --layers 16 --widen-factor 4 -p 1\n\n\nObserve logged batch timings.", "body": "cc @tfboyd\r\n\r\nFrom https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-295502315\r\n\r\nOn an AWS p2.xlarge, using the `tensorflow/tensorflow:1.0.1-devel-gpu` Docker image as a base, I see ~270 ms per epoch while training a WRN-16-4 without dropout on CIFAR-10.\r\n\r\nUsing a PyTorch implementation from https://github.com/xternalz/WideResNet-pytorch, I see instead ~150 ms per epoch for the same.\r\n\r\nMy implementation of Wide ResNets uses NCHW and fused batch norm. It does use `feed_dict` for data loading, but I've observed with `nvidia-smi` that my GPU utilization stays near 100%.\r\n\r\n-----\r\n\r\nTo reproduce:\r\n\r\n- Clone https://github.com/4Catalyzer/dl-papers, and go to that directory.\r\n- Check out the [`benchmark`](https://github.com/4Catalyzer/dl-papers/tree/benchmark) branch.\r\n- Build the Docker image, which is based on the Docker hub image above:\r\n```\r\n$ docker build -t dl-papers .\r\n```\r\n- Run the Docker image using NVIDIA Docker:\r\n```\r\n$ nvidia-docker run --rm -it dl-papers /bin/bash\r\n```\r\n- Run the TF WRN-16-4 training:\r\n```\r\n# python -m dl_papers.wide_resnet.train cifar10\r\n```\r\n- Observe the logged batch timings, then kill the process.\r\n- In the same Docker container up the PyTorch Wide ResNet example:\r\n```\r\n# cd ..\r\n# pip install http://download.pytorch.org/whl/cu80/torch-0.1.11.post5-cp27-none-linux_x86_64.whl\r\n# pip install torchvision tensorboard_logger\r\n# git clone https://github.com/xternalz/WideResNet-pytorch.git\r\n# cd WideResNet-pytorch\r\n```\r\n- Run PyTorch training:\r\n```\r\n# python train.py --dataset cifar10 --layers 16 --widen-factor 4 -p 1\r\n```\r\n- Observe logged batch timings."}