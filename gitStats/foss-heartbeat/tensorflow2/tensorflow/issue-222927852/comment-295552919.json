{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295552919", "html_url": "https://github.com/tensorflow/tensorflow/issues/9322#issuecomment-295552919", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322", "id": 295552919, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTU1MjkxOQ==", "user": {"login": "taion", "id": 3112159, "node_id": "MDQ6VXNlcjMxMTIxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3112159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taion", "html_url": "https://github.com/taion", "followers_url": "https://api.github.com/users/taion/followers", "following_url": "https://api.github.com/users/taion/following{/other_user}", "gists_url": "https://api.github.com/users/taion/gists{/gist_id}", "starred_url": "https://api.github.com/users/taion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taion/subscriptions", "organizations_url": "https://api.github.com/users/taion/orgs", "repos_url": "https://api.github.com/users/taion/repos", "events_url": "https://api.github.com/users/taion/events{/privacy}", "received_events_url": "https://api.github.com/users/taion/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T02:28:56Z", "updated_at": "2017-04-20T02:31:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>To rule out issues with my use of <code>feed_dict</code>, I've added a <a href=\"https://github.com/4Catalyzer/dl-papers/tree/benchmark-constant\"><code>benchmark-constant</code></a> branch to my repo that uses constant tensors for the batch inputs and outputs, and removes all the actual data loading logic.</p>\n<p>On this branch, on the same AWS instance, I see batch times of ~246 ms, which is still over 60% slower than what I see with the PyTorch example (and this is non-apples-to-apples, since the PyTorch example is doing real data loading).</p>", "body_text": "To rule out issues with my use of feed_dict, I've added a benchmark-constant branch to my repo that uses constant tensors for the batch inputs and outputs, and removes all the actual data loading logic.\nOn this branch, on the same AWS instance, I see batch times of ~246 ms, which is still over 60% slower than what I see with the PyTorch example (and this is non-apples-to-apples, since the PyTorch example is doing real data loading).", "body": "To rule out issues with my use of `feed_dict`, I've added a [`benchmark-constant`](https://github.com/4Catalyzer/dl-papers/tree/benchmark-constant) branch to my repo that uses constant tensors for the batch inputs and outputs, and removes all the actual data loading logic.\r\n\r\nOn this branch, on the same AWS instance, I see batch times of ~246 ms, which is still over 60% slower than what I see with the PyTorch example (and this is non-apples-to-apples, since the PyTorch example is doing real data loading)."}