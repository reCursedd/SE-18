{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295609987", "html_url": "https://github.com/tensorflow/tensorflow/issues/9322#issuecomment-295609987", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322", "id": 295609987, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTYwOTk4Nw==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T07:29:57Z", "updated_at": "2017-04-20T07:29:57Z", "author_association": "MEMBER", "body_html": "<p>Here are the results on a GTX 1080 (which I also use as my main display card):</p>\n<p>Not apples-to-apples due to TF using a constant.  I used git checkout benchmark-constant with the following commands on Ubuntu 14.04 (a custom internal google build) with CUDA 8.0 and 5.1 cuDNN as one would expect:</p>\n<ul>\n<li>python -m dl_papers.wide_resnet.train cifar10</li>\n<li>python train.py --dataset cifar10 --layers 16 --widen-factor 4 -p 1</li>\n</ul>\n<p>.069 vs .055.  I am running TF 1.1rc2 compiled with CUDA and compute 6.1 as well as avx (which I doubt matters in this instance).</p>\n<p>TensorFlow with the constant<br>\n[2017-04-20 00:15:48,235] INFO in train: epoch 0, batch 176: 0.068s<br>\n[2017-04-20 00:15:48,303] INFO in train: epoch 0, batch 177: 0.068s<br>\n[2017-04-20 00:15:48,372] INFO in train: epoch 0, batch 178: 0.068s<br>\n[2017-04-20 00:15:48,440] INFO in train: epoch 0, batch 179: 0.068s<br>\n[2017-04-20 00:15:48,510] INFO in train: epoch 0, batch 180: 0.069s<br>\n[2017-04-20 00:15:48,581] INFO in train: epoch 0, batch 181: 0.071s<br>\n[2017-04-20 00:15:48,649] INFO in train: epoch 0, batch 182: 0.068s</p>\n<p>PyTorch with real data<br>\nEpoch: [0][83/391]\tTime 0.055 (0.069)\tLoss 1.5680 (1.8102)\tPrec@1 42.188 (31.343)<br>\nEpoch: [0][84/391]\tTime 0.054 (0.069)\tLoss 1.6633 (1.8084)\tPrec@1 39.062 (31.434)<br>\nEpoch: [0][85/391]\tTime 0.053 (0.069)\tLoss 1.6345 (1.8064)\tPrec@1 39.844 (31.532)<br>\nEpoch: [0][86/391]\tTime 0.056 (0.069)\tLoss 1.4338 (1.8021)\tPrec@1 46.875 (31.708)<br>\nEpoch: [0][87/391]\tTime 0.056 (0.069)\tLoss 1.4923 (1.7986)\tPrec@1 43.750 (31.845)<br>\nEpoch: [0][88/391]\tTime 0.057 (0.068)\tLoss 1.6836 (1.7973)\tPrec@1 45.312 (31.996)</p>\n<p>Toby</p>", "body_text": "Here are the results on a GTX 1080 (which I also use as my main display card):\nNot apples-to-apples due to TF using a constant.  I used git checkout benchmark-constant with the following commands on Ubuntu 14.04 (a custom internal google build) with CUDA 8.0 and 5.1 cuDNN as one would expect:\n\npython -m dl_papers.wide_resnet.train cifar10\npython train.py --dataset cifar10 --layers 16 --widen-factor 4 -p 1\n\n.069 vs .055.  I am running TF 1.1rc2 compiled with CUDA and compute 6.1 as well as avx (which I doubt matters in this instance).\nTensorFlow with the constant\n[2017-04-20 00:15:48,235] INFO in train: epoch 0, batch 176: 0.068s\n[2017-04-20 00:15:48,303] INFO in train: epoch 0, batch 177: 0.068s\n[2017-04-20 00:15:48,372] INFO in train: epoch 0, batch 178: 0.068s\n[2017-04-20 00:15:48,440] INFO in train: epoch 0, batch 179: 0.068s\n[2017-04-20 00:15:48,510] INFO in train: epoch 0, batch 180: 0.069s\n[2017-04-20 00:15:48,581] INFO in train: epoch 0, batch 181: 0.071s\n[2017-04-20 00:15:48,649] INFO in train: epoch 0, batch 182: 0.068s\nPyTorch with real data\nEpoch: [0][83/391]\tTime 0.055 (0.069)\tLoss 1.5680 (1.8102)\tPrec@1 42.188 (31.343)\nEpoch: [0][84/391]\tTime 0.054 (0.069)\tLoss 1.6633 (1.8084)\tPrec@1 39.062 (31.434)\nEpoch: [0][85/391]\tTime 0.053 (0.069)\tLoss 1.6345 (1.8064)\tPrec@1 39.844 (31.532)\nEpoch: [0][86/391]\tTime 0.056 (0.069)\tLoss 1.4338 (1.8021)\tPrec@1 46.875 (31.708)\nEpoch: [0][87/391]\tTime 0.056 (0.069)\tLoss 1.4923 (1.7986)\tPrec@1 43.750 (31.845)\nEpoch: [0][88/391]\tTime 0.057 (0.068)\tLoss 1.6836 (1.7973)\tPrec@1 45.312 (31.996)\nToby", "body": "Here are the results on a GTX 1080 (which I also use as my main display card):\r\n\r\nNot apples-to-apples due to TF using a constant.  I used git checkout benchmark-constant with the following commands on Ubuntu 14.04 (a custom internal google build) with CUDA 8.0 and 5.1 cuDNN as one would expect:\r\n- python -m dl_papers.wide_resnet.train cifar10\r\n- python train.py --dataset cifar10 --layers 16 --widen-factor 4 -p 1\r\n\r\n.069 vs .055.  I am running TF 1.1rc2 compiled with CUDA and compute 6.1 as well as avx (which I doubt matters in this instance).\r\n\r\nTensorFlow with the constant\r\n[2017-04-20 00:15:48,235] INFO in train: epoch 0, batch 176: 0.068s\r\n[2017-04-20 00:15:48,303] INFO in train: epoch 0, batch 177: 0.068s\r\n[2017-04-20 00:15:48,372] INFO in train: epoch 0, batch 178: 0.068s\r\n[2017-04-20 00:15:48,440] INFO in train: epoch 0, batch 179: 0.068s\r\n[2017-04-20 00:15:48,510] INFO in train: epoch 0, batch 180: 0.069s\r\n[2017-04-20 00:15:48,581] INFO in train: epoch 0, batch 181: 0.071s\r\n[2017-04-20 00:15:48,649] INFO in train: epoch 0, batch 182: 0.068s\r\n\r\n\r\nPyTorch with real data\r\nEpoch: [0][83/391]\tTime 0.055 (0.069)\tLoss 1.5680 (1.8102)\tPrec@1 42.188 (31.343)\r\nEpoch: [0][84/391]\tTime 0.054 (0.069)\tLoss 1.6633 (1.8084)\tPrec@1 39.062 (31.434)\r\nEpoch: [0][85/391]\tTime 0.053 (0.069)\tLoss 1.6345 (1.8064)\tPrec@1 39.844 (31.532)\r\nEpoch: [0][86/391]\tTime 0.056 (0.069)\tLoss 1.4338 (1.8021)\tPrec@1 46.875 (31.708)\r\nEpoch: [0][87/391]\tTime 0.056 (0.069)\tLoss 1.4923 (1.7986)\tPrec@1 43.750 (31.845)\r\nEpoch: [0][88/391]\tTime 0.057 (0.068)\tLoss 1.6836 (1.7973)\tPrec@1 45.312 (31.996)\r\n\r\nToby\r\n"}