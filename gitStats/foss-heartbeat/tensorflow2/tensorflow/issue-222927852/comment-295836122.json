{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295836122", "html_url": "https://github.com/tensorflow/tensorflow/issues/9322#issuecomment-295836122", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322", "id": 295836122, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTgzNjEyMg==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T18:01:44Z", "updated_at": "2017-04-20T18:38:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a> Currently I don't have other machines to test on, but I ran both several times. TF is always within 5% slower. It's 1.94it/s for TF and around 1.99it/s for pytorch (without extra arguments to change the widen factor, etc). Before enabling the NONFUSED algorithm (cudnn algo No.7, as you can see in the log), TF is about 1.25it/s.<br>\nI'm using Tesla M40, E5-2680v3, tensorflow nightly downloaded yesterday, cuda8.0, cudnn5.1. pytorch also uses cudnn5.1</p>\n<p>I modified my TF code to use constant input and the results are the same. I'm looking at 10-step average time, but the number is quite stable after the warm-up.</p>\n<p>To clarify a bit, my log shows that</p>\n<ol>\n<li>although TF and pytorch use different methods to choose cudnn algo, they chose the same algo for all the convolutions in this model.</li>\n<li>But in some convolution, they invoke cudnn with different shapes. I think that's because TF padding convention is different from others.<br>\nAccording to the doc, for Conv32x32 kernel3 stride2 padSAME, TF will pad 0 at left/top and 1 at right/bottom. If I'm not mistaken, cudnn pad in the opposite way. This explains this line from TF VLOG:</li>\n</ol>\n<pre><code>Conv accepts: 128, 160, (33, 33), 320, (3, 3), (2, 2), (1, 1), 1, 0 -&gt; (1, 0)\n</code></pre>\n<p>where TF pre-pad the image to become (33,33) and then call cudnn (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L562\">code here</a>). I'm just wondering could that be a possible performance issue, because 33 doesn't look like a good number for cache.</p>", "body_text": "@tfboyd Currently I don't have other machines to test on, but I ran both several times. TF is always within 5% slower. It's 1.94it/s for TF and around 1.99it/s for pytorch (without extra arguments to change the widen factor, etc). Before enabling the NONFUSED algorithm (cudnn algo No.7, as you can see in the log), TF is about 1.25it/s.\nI'm using Tesla M40, E5-2680v3, tensorflow nightly downloaded yesterday, cuda8.0, cudnn5.1. pytorch also uses cudnn5.1\nI modified my TF code to use constant input and the results are the same. I'm looking at 10-step average time, but the number is quite stable after the warm-up.\nTo clarify a bit, my log shows that\n\nalthough TF and pytorch use different methods to choose cudnn algo, they chose the same algo for all the convolutions in this model.\nBut in some convolution, they invoke cudnn with different shapes. I think that's because TF padding convention is different from others.\nAccording to the doc, for Conv32x32 kernel3 stride2 padSAME, TF will pad 0 at left/top and 1 at right/bottom. If I'm not mistaken, cudnn pad in the opposite way. This explains this line from TF VLOG:\n\nConv accepts: 128, 160, (33, 33), 320, (3, 3), (2, 2), (1, 1), 1, 0 -> (1, 0)\n\nwhere TF pre-pad the image to become (33,33) and then call cudnn (code here). I'm just wondering could that be a possible performance issue, because 33 doesn't look like a good number for cache.", "body": "@tfboyd Currently I don't have other machines to test on, but I ran both several times. TF is always within 5% slower. It's 1.94it/s for TF and around 1.99it/s for pytorch (without extra arguments to change the widen factor, etc). Before enabling the NONFUSED algorithm (cudnn algo No.7, as you can see in the log), TF is about 1.25it/s. \r\nI'm using Tesla M40, E5-2680v3, tensorflow nightly downloaded yesterday, cuda8.0, cudnn5.1. pytorch also uses cudnn5.1 \r\n\r\nI modified my TF code to use constant input and the results are the same. I'm looking at 10-step average time, but the number is quite stable after the warm-up.\r\n\r\nTo clarify a bit, my log shows that\r\n1. although TF and pytorch use different methods to choose cudnn algo, they chose the same algo for all the convolutions in this model.\r\n2. But in some convolution, they invoke cudnn with different shapes. I think that's because TF padding convention is different from others.\r\nAccording to the doc, for Conv32x32 kernel3 stride2 padSAME, TF will pad 0 at left/top and 1 at right/bottom. If I'm not mistaken, cudnn pad in the opposite way. This explains this line from TF VLOG:\r\n```\r\nConv accepts: 128, 160, (33, 33), 320, (3, 3), (2, 2), (1, 1), 1, 0 -> (1, 0)\r\n```\r\nwhere TF pre-pad the image to become (33,33) and then call cudnn ([code here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L562)). I'm just wondering could that be a possible performance issue, because 33 doesn't look like a good number for cache.\r\n\r\n"}