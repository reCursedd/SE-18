{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295588377", "html_url": "https://github.com/tensorflow/tensorflow/issues/9322#issuecomment-295588377", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322", "id": 295588377, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTU4ODM3Nw==", "user": {"login": "taion", "id": 3112159, "node_id": "MDQ6VXNlcjMxMTIxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3112159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taion", "html_url": "https://github.com/taion", "followers_url": "https://api.github.com/users/taion/followers", "following_url": "https://api.github.com/users/taion/following{/other_user}", "gists_url": "https://api.github.com/users/taion/gists{/gist_id}", "starred_url": "https://api.github.com/users/taion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taion/subscriptions", "organizations_url": "https://api.github.com/users/taion/orgs", "repos_url": "https://api.github.com/users/taion/repos", "events_url": "https://api.github.com/users/taion/events{/privacy}", "received_events_url": "https://api.github.com/users/taion/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T05:43:17Z", "updated_at": "2017-04-20T05:43:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>To clarify:</p>\n<ul>\n<li>Even when using <code>feed_dict</code> for the data, I'm seeing 95%-100% GPU utilization, so I'm fairly confident that I'm not starving my GPU, even when using <code>feed_dict</code> (data augmentation/&amp;c. are still happening in a separate thread)</li>\n<li>When I remove all placeholders and just use constant fake training data (i.e. to remove all GPU memory or whatever pressure from moving data around), I still see TF as ~60% slower than the PyTorch model that's doing actual fitting</li>\n<li>I intentionally wrote this Wide ResNet model from scratch; the ResNet models in <code>tensorflow/models</code> have a number of problems, from inefficiency for the non-TF-Slim versions (using NHWC and not using fused batch norm) to actually implementing the model incorrectly in the TF-Slim versions (not dropping biases on convolutions before batch norms and not using beta and gamma on the batch norm operations, plus the same inefficiencies as above)</li>\n</ul>", "body_text": "To clarify:\n\nEven when using feed_dict for the data, I'm seeing 95%-100% GPU utilization, so I'm fairly confident that I'm not starving my GPU, even when using feed_dict (data augmentation/&c. are still happening in a separate thread)\nWhen I remove all placeholders and just use constant fake training data (i.e. to remove all GPU memory or whatever pressure from moving data around), I still see TF as ~60% slower than the PyTorch model that's doing actual fitting\nI intentionally wrote this Wide ResNet model from scratch; the ResNet models in tensorflow/models have a number of problems, from inefficiency for the non-TF-Slim versions (using NHWC and not using fused batch norm) to actually implementing the model incorrectly in the TF-Slim versions (not dropping biases on convolutions before batch norms and not using beta and gamma on the batch norm operations, plus the same inefficiencies as above)", "body": "To clarify:\r\n\r\n- Even when using `feed_dict` for the data, I'm seeing 95%-100% GPU utilization, so I'm fairly confident that I'm not starving my GPU, even when using `feed_dict` (data augmentation/&c. are still happening in a separate thread)\r\n- When I remove all placeholders and just use constant fake training data (i.e. to remove all GPU memory or whatever pressure from moving data around), I still see TF as ~60% slower than the PyTorch model that's doing actual fitting\r\n- I intentionally wrote this Wide ResNet model from scratch; the ResNet models in `tensorflow/models` have a number of problems, from inefficiency for the non-TF-Slim versions (using NHWC and not using fused batch norm) to actually implementing the model incorrectly in the TF-Slim versions (not dropping biases on convolutions before batch norms and not using beta and gamma on the batch norm operations, plus the same inefficiencies as above)"}