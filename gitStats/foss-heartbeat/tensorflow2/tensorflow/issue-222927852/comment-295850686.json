{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295850686", "html_url": "https://github.com/tensorflow/tensorflow/issues/9322#issuecomment-295850686", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9322", "id": 295850686, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTg1MDY4Ng==", "user": {"login": "taion", "id": 3112159, "node_id": "MDQ6VXNlcjMxMTIxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3112159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taion", "html_url": "https://github.com/taion", "followers_url": "https://api.github.com/users/taion/followers", "following_url": "https://api.github.com/users/taion/following{/other_user}", "gists_url": "https://api.github.com/users/taion/gists{/gist_id}", "starred_url": "https://api.github.com/users/taion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taion/subscriptions", "organizations_url": "https://api.github.com/users/taion/orgs", "repos_url": "https://api.github.com/users/taion/repos", "events_url": "https://api.github.com/users/taion/events{/privacy}", "received_events_url": "https://api.github.com/users/taion/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T18:38:32Z", "updated_at": "2017-04-20T18:39:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'd like to highlight that I think the biggest gap here is in DX. We started migrating from Theano to TensorFlow the first week of this year. This is partly my frustration speaking, but the process looked something like this:</p>\n<ol>\n<li>Write naive model implementations, which end up using NHWC and unfused batch norm.</li>\n<li>See documentation that suggests NCHW is faster for convolutions (this predates the perf guide). Refactor models to support both NCHW and NHWC, because we still need to support CPU inference. Observe that the models run ~6x slower. Give up on this for a bit.</li>\n<li>See documentation that fused batch norm is faster. Switch back from <code>tf.layers.batch_normalization</code> to <code>tf.contrib.layers.batch_norm</code>. This shows a performance improvement.</li>\n<li>Try NCHW again, and now see a performance improvement.<br>\na. Realize earlier issue was due to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"208010826\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7551\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7551/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7551\">#7551</a></li>\n<li>See the performance guide. Convert input pipeline to use queues instead of <code>feed_dict</code>. See negligible speedup.</li>\n<li>Compare to straightforward PyTorch impl, and observe that the PyTorch impl runs much faster.</li>\n<li>Learn through this issue that we need to enable an undocumented environment flag to access the fastest cuDNN mode for 3x3 convolutions.</li>\n</ol>\n<p>As a developer, this is a really suboptimal experience.</p>\n<p>My impression is that the modal TF example or published code is at something like our step (1) above, in that it uses NHWC, uses unfused batch norm, and doesn't enable the non-fused Winograd convolution. Correspondingly, performance is quite far from optimal.</p>\n<p>By contrast, though with a smaller sample size, the PyTorch examples I've seen generally seem to do \"the right thing\" performance-wise, and seem to run quickly out-of-the-box. (Also, the status of the built-in PyTorch layer API makes separate PyTorch examples far more consistent in terms of how the code reads.)</p>\n<p>I'm very grateful for your help in tracking down these issues, but I really wish the out-of-the-box experience were better, and that it didn't take so much work to get to this point.</p>", "body_text": "I'd like to highlight that I think the biggest gap here is in DX. We started migrating from Theano to TensorFlow the first week of this year. This is partly my frustration speaking, but the process looked something like this:\n\nWrite naive model implementations, which end up using NHWC and unfused batch norm.\nSee documentation that suggests NCHW is faster for convolutions (this predates the perf guide). Refactor models to support both NCHW and NHWC, because we still need to support CPU inference. Observe that the models run ~6x slower. Give up on this for a bit.\nSee documentation that fused batch norm is faster. Switch back from tf.layers.batch_normalization to tf.contrib.layers.batch_norm. This shows a performance improvement.\nTry NCHW again, and now see a performance improvement.\na. Realize earlier issue was due to #7551\nSee the performance guide. Convert input pipeline to use queues instead of feed_dict. See negligible speedup.\nCompare to straightforward PyTorch impl, and observe that the PyTorch impl runs much faster.\nLearn through this issue that we need to enable an undocumented environment flag to access the fastest cuDNN mode for 3x3 convolutions.\n\nAs a developer, this is a really suboptimal experience.\nMy impression is that the modal TF example or published code is at something like our step (1) above, in that it uses NHWC, uses unfused batch norm, and doesn't enable the non-fused Winograd convolution. Correspondingly, performance is quite far from optimal.\nBy contrast, though with a smaller sample size, the PyTorch examples I've seen generally seem to do \"the right thing\" performance-wise, and seem to run quickly out-of-the-box. (Also, the status of the built-in PyTorch layer API makes separate PyTorch examples far more consistent in terms of how the code reads.)\nI'm very grateful for your help in tracking down these issues, but I really wish the out-of-the-box experience were better, and that it didn't take so much work to get to this point.", "body": "I'd like to highlight that I think the biggest gap here is in DX. We started migrating from Theano to TensorFlow the first week of this year. This is partly my frustration speaking, but the process looked something like this:\r\n\r\n1. Write naive model implementations, which end up using NHWC and unfused batch norm.\r\n2. See documentation that suggests NCHW is faster for convolutions (this predates the perf guide). Refactor models to support both NCHW and NHWC, because we still need to support CPU inference. Observe that the models run ~6x slower. Give up on this for a bit.\r\n3. See documentation that fused batch norm is faster. Switch back from `tf.layers.batch_normalization` to `tf.contrib.layers.batch_norm`. This shows a performance improvement.\r\n4. Try NCHW again, and now see a performance improvement.\r\n  a. Realize earlier issue was due to https://github.com/tensorflow/tensorflow/issues/7551\r\n5. See the performance guide. Convert input pipeline to use queues instead of `feed_dict`. See negligible speedup.\r\n6. Compare to straightforward PyTorch impl, and observe that the PyTorch impl runs much faster.\r\n7. Learn through this issue that we need to enable an undocumented environment flag to access the fastest cuDNN mode for 3x3 convolutions.\r\n\r\nAs a developer, this is a really suboptimal experience.\r\n\r\nMy impression is that the modal TF example or published code is at something like our step (1) above, in that it uses NHWC, uses unfused batch norm, and doesn't enable the non-fused Winograd convolution. Correspondingly, performance is quite far from optimal.\r\n\r\nBy contrast, though with a smaller sample size, the PyTorch examples I've seen generally seem to do \"the right thing\" performance-wise, and seem to run quickly out-of-the-box. (Also, the status of the built-in PyTorch layer API makes separate PyTorch examples far more consistent in terms of how the code reads.)\r\n\r\nI'm very grateful for your help in tracking down these issues, but I really wish the out-of-the-box experience were better, and that it didn't take so much work to get to this point."}