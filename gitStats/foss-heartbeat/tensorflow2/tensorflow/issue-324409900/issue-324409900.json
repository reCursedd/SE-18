{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19380", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19380/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19380/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19380/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19380", "id": 324409900, "node_id": "MDU6SXNzdWUzMjQ0MDk5MDA=", "number": 19380, "title": "Tensorflow Lite AllocateTensors() fails with custom model", "user": {"login": "phildue", "id": 25346572, "node_id": "MDQ6VXNlcjI1MzQ2NTcy", "avatar_url": "https://avatars2.githubusercontent.com/u/25346572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phildue", "html_url": "https://github.com/phildue", "followers_url": "https://api.github.com/users/phildue/followers", "following_url": "https://api.github.com/users/phildue/following{/other_user}", "gists_url": "https://api.github.com/users/phildue/gists{/gist_id}", "starred_url": "https://api.github.com/users/phildue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phildue/subscriptions", "organizations_url": "https://api.github.com/users/phildue/orgs", "repos_url": "https://api.github.com/users/phildue/repos", "events_url": "https://api.github.com/users/phildue/events{/privacy}", "received_events_url": "https://api.github.com/users/phildue/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-05-18T13:25:39Z", "updated_at": "2018-09-07T15:33:16Z", "closed_at": "2018-08-23T18:22:51Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n<li><strong>Python version</strong>:<br>\n('v1.8.0-0-g93bc2e2072', '1.8.0')</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n</ul>\n<pre><code>Build label: 0.13.0\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)\nBuild timestamp: 1525078013620\nBuild timestamp as int: 1525078013620\n\n</code></pre>\n<ul>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\ngcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1~16.04)</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nNone</li>\n<li><strong>GPU model and memory</strong>:<br>\nNone</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Python-Code:</h3>\n<pre><code>from keras import Input, Model\nfrom keras.layers import Conv2D\nimport keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\n# manually put back imported modules\nimport tempfile\nimport subprocess\n\ntf.contrib.lite.tempfile = tempfile\ntf.contrib.lite.subprocess = subprocess\n\nK.set_learning_phase(0)\ninput = Input((416, 416, 3),name='Placeholder')\nconv1 = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False)(input)\nmodel = Model(input, conv1)\nnetin = [K.placeholder(name=\"Input\", dtype=tf.float32, shape=(416,416,3))]\nnetout = [K.identity(model.outputs[0],\"Prediction\")]\nsess = K.get_session()\n\nconstant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"Prediction\"])\n\ntflite_model = tf.contrib.lite.toco_convert(constant_graph, netin, netout)\nopen('model' + '.tflite', \"wb\").write(tflite_model)\n\n</code></pre>\n<h3>C-Code:</h3>\n<pre><code>//based on tensorflow/contrib/lite/examples/minimal/minimal.cc\n\n#include \"tensorflow/contrib/lite/model.h\"\n#include \"tensorflow/contrib/lite/interpreter.h\"\n#include \"tensorflow/contrib/lite/kernels/register.h\"\n#include \"tensorflow/contrib/lite/kernels/kernel_util.h\"\n#include &lt;cstdio&gt;\n#include &lt;iostream&gt;\n#include \"exp.h\"\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/core.hpp\"\n// Usage: minimal &lt;tflite model&gt;\n\nusing namespace tflite;\n\n#define TFLITE_MINIMAL_CHECK(x) \\\n  if(!(x)) {                                                    \\\n    fprintf(stderr, \"Error at %s:%d\\n\",  __FILE__, __LINE__); \\\n    exit(1); \\\n  }\n\n\nint main(int argc, char *argv[]) {\n  if(argc != 3) {\n    fprintf(stderr, \"Usage: &lt;model&gt; &lt;image&gt;\\n\");\n    return 1;\n  }else{\n      std::cout &lt;&lt; \"Reading model from: \" &lt;&lt; argv[1] &lt;&lt; std::endl;\n      std::cout &lt;&lt; \"Reading image from: \" &lt;&lt; argv[2] &lt;&lt; std::endl;\n  }\n  const char* filename = argv[1];\n    const char* imagefile = argv[2];\n\n  // Load model\n  std::unique_ptr&lt;tflite::FlatBufferModel&gt; model\n      = tflite::FlatBufferModel::BuildFromFile(filename);\n  TFLITE_MINIMAL_CHECK(model != nullptr);\n\n\n\n  // Build the interpreter\n  tflite::ops::builtin::BuiltinOpResolver resolver;\n  resolver.AddCustom(\"Exp\", Register_EXP());\n\n  InterpreterBuilder builder(*model.get(), resolver);\n  std::unique_ptr&lt;Interpreter&gt; interpreter;\n  builder(&amp;interpreter);\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\n    cv::Mat cvimg = cv::imread(imagefile);\n    cv::imshow(\"Input\",cvimg);\n    cv::waitKey(1);\n\n  // Allocate tensor buffers.\n    TFLITE_MINIMAL_CHECK(interpreter-&gt;AllocateTensors() == kTfLiteOk);\n    // Fill input buffers\n\n    int input = interpreter-&gt;inputs()[0];\n    memcpy(interpreter-&gt;typed_input_tensor&lt;float&gt;(input), cvimg.data, cvimg.total() * cvimg.elemSize());\n\n  // Run inference\n    TFLITE_MINIMAL_CHECK(interpreter-&gt;Invoke() == kTfLiteOk);\n\n  // Read output buffers\n  // TODO(user): Insert getting data out code.\n    //int output = interpreter-&gt;outputs()[0];\n    std::cout &lt;&lt; interpreter-&gt;typed_output_tensor&lt;float&gt;(0) &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I convert a keras model (very simplified to reproduce error) to tflite. I verified the graph using tensorboard and it seems fine. However, when I want to run it in C++ the following error occurs during the call of interpreter-&gt;AllocateTensors():</p>\n<pre><code>tensorflow/contrib/lite/kernels/conv.cc:189 input-&gt;dims-&gt;size != 4 (0 != 4)\n</code></pre>\n<p>This does not happen when loading a model from <a href=\"https://www.tensorflow.org/mobile/tflite/demo_android\" rel=\"nofollow\">here</a>. Is this a bug in the graph conversion in the model loader or am I doing sth wrong?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\nPython version:\n('v1.8.0-0-g93bc2e2072', '1.8.0')\nBazel version (if compiling from source):\n\nBuild label: 0.13.0\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)\nBuild timestamp: 1525078013620\nBuild timestamp as int: 1525078013620\n\n\n\nGCC/Compiler version (if compiling from source):\ngcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1~16.04)\nCUDA/cuDNN version:\nNone\nGPU model and memory:\nNone\nExact command to reproduce:\n\nPython-Code:\nfrom keras import Input, Model\nfrom keras.layers import Conv2D\nimport keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\n# manually put back imported modules\nimport tempfile\nimport subprocess\n\ntf.contrib.lite.tempfile = tempfile\ntf.contrib.lite.subprocess = subprocess\n\nK.set_learning_phase(0)\ninput = Input((416, 416, 3),name='Placeholder')\nconv1 = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False)(input)\nmodel = Model(input, conv1)\nnetin = [K.placeholder(name=\"Input\", dtype=tf.float32, shape=(416,416,3))]\nnetout = [K.identity(model.outputs[0],\"Prediction\")]\nsess = K.get_session()\n\nconstant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"Prediction\"])\n\ntflite_model = tf.contrib.lite.toco_convert(constant_graph, netin, netout)\nopen('model' + '.tflite', \"wb\").write(tflite_model)\n\n\nC-Code:\n//based on tensorflow/contrib/lite/examples/minimal/minimal.cc\n\n#include \"tensorflow/contrib/lite/model.h\"\n#include \"tensorflow/contrib/lite/interpreter.h\"\n#include \"tensorflow/contrib/lite/kernels/register.h\"\n#include \"tensorflow/contrib/lite/kernels/kernel_util.h\"\n#include <cstdio>\n#include <iostream>\n#include \"exp.h\"\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/core.hpp\"\n// Usage: minimal <tflite model>\n\nusing namespace tflite;\n\n#define TFLITE_MINIMAL_CHECK(x) \\\n  if(!(x)) {                                                    \\\n    fprintf(stderr, \"Error at %s:%d\\n\",  __FILE__, __LINE__); \\\n    exit(1); \\\n  }\n\n\nint main(int argc, char *argv[]) {\n  if(argc != 3) {\n    fprintf(stderr, \"Usage: <model> <image>\\n\");\n    return 1;\n  }else{\n      std::cout << \"Reading model from: \" << argv[1] << std::endl;\n      std::cout << \"Reading image from: \" << argv[2] << std::endl;\n  }\n  const char* filename = argv[1];\n    const char* imagefile = argv[2];\n\n  // Load model\n  std::unique_ptr<tflite::FlatBufferModel> model\n      = tflite::FlatBufferModel::BuildFromFile(filename);\n  TFLITE_MINIMAL_CHECK(model != nullptr);\n\n\n\n  // Build the interpreter\n  tflite::ops::builtin::BuiltinOpResolver resolver;\n  resolver.AddCustom(\"Exp\", Register_EXP());\n\n  InterpreterBuilder builder(*model.get(), resolver);\n  std::unique_ptr<Interpreter> interpreter;\n  builder(&interpreter);\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\n    cv::Mat cvimg = cv::imread(imagefile);\n    cv::imshow(\"Input\",cvimg);\n    cv::waitKey(1);\n\n  // Allocate tensor buffers.\n    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\n    // Fill input buffers\n\n    int input = interpreter->inputs()[0];\n    memcpy(interpreter->typed_input_tensor<float>(input), cvimg.data, cvimg.total() * cvimg.elemSize());\n\n  // Run inference\n    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\n\n  // Read output buffers\n  // TODO(user): Insert getting data out code.\n    //int output = interpreter->outputs()[0];\n    std::cout << interpreter->typed_output_tensor<float>(0) << std::endl;\n  return 0;\n}\n\nDescribe the problem\nI convert a keras model (very simplified to reproduce error) to tflite. I verified the graph using tensorboard and it seems fine. However, when I want to run it in C++ the following error occurs during the call of interpreter->AllocateTensors():\ntensorflow/contrib/lite/kernels/conv.cc:189 input->dims->size != 4 (0 != 4)\n\nThis does not happen when loading a model from here. Is this a bug in the graph conversion in the model loader or am I doing sth wrong?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n('v1.8.0-0-g93bc2e2072', '1.8.0')\r\n- **Bazel version (if compiling from source)**:\r\n```\r\nBuild label: 0.13.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)\r\nBuild timestamp: 1525078013620\r\nBuild timestamp as int: 1525078013620\r\n\r\n```\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1~16.04) \r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n\r\n### Python-Code:\r\n```\r\nfrom keras import Input, Model\r\nfrom keras.layers import Conv2D\r\nimport keras.backend as K\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\n# manually put back imported modules\r\nimport tempfile\r\nimport subprocess\r\n\r\ntf.contrib.lite.tempfile = tempfile\r\ntf.contrib.lite.subprocess = subprocess\r\n\r\nK.set_learning_phase(0)\r\ninput = Input((416, 416, 3),name='Placeholder')\r\nconv1 = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False)(input)\r\nmodel = Model(input, conv1)\r\nnetin = [K.placeholder(name=\"Input\", dtype=tf.float32, shape=(416,416,3))]\r\nnetout = [K.identity(model.outputs[0],\"Prediction\")]\r\nsess = K.get_session()\r\n\r\nconstant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"Prediction\"])\r\n\r\ntflite_model = tf.contrib.lite.toco_convert(constant_graph, netin, netout)\r\nopen('model' + '.tflite', \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\n### C-Code:\r\n\r\n```\r\n//based on tensorflow/contrib/lite/examples/minimal/minimal.cc\r\n\r\n#include \"tensorflow/contrib/lite/model.h\"\r\n#include \"tensorflow/contrib/lite/interpreter.h\"\r\n#include \"tensorflow/contrib/lite/kernels/register.h\"\r\n#include \"tensorflow/contrib/lite/kernels/kernel_util.h\"\r\n#include <cstdio>\r\n#include <iostream>\r\n#include \"exp.h\"\r\n#include \"opencv2/highgui.hpp\"\r\n#include \"opencv2/core.hpp\"\r\n// Usage: minimal <tflite model>\r\n\r\nusing namespace tflite;\r\n\r\n#define TFLITE_MINIMAL_CHECK(x) \\\r\n  if(!(x)) {                                                    \\\r\n    fprintf(stderr, \"Error at %s:%d\\n\",  __FILE__, __LINE__); \\\r\n    exit(1); \\\r\n  }\r\n\r\n\r\nint main(int argc, char *argv[]) {\r\n  if(argc != 3) {\r\n    fprintf(stderr, \"Usage: <model> <image>\\n\");\r\n    return 1;\r\n  }else{\r\n      std::cout << \"Reading model from: \" << argv[1] << std::endl;\r\n      std::cout << \"Reading image from: \" << argv[2] << std::endl;\r\n  }\r\n  const char* filename = argv[1];\r\n    const char* imagefile = argv[2];\r\n\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model\r\n      = tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n\r\n\r\n  // Build the interpreter\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  resolver.AddCustom(\"Exp\", Register_EXP());\r\n\r\n  InterpreterBuilder builder(*model.get(), resolver);\r\n  std::unique_ptr<Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n    cv::Mat cvimg = cv::imread(imagefile);\r\n    cv::imshow(\"Input\",cvimg);\r\n    cv::waitKey(1);\r\n\r\n  // Allocate tensor buffers.\r\n    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n    // Fill input buffers\r\n\r\n    int input = interpreter->inputs()[0];\r\n    memcpy(interpreter->typed_input_tensor<float>(input), cvimg.data, cvimg.total() * cvimg.elemSize());\r\n\r\n  // Run inference\r\n    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n\r\n  // Read output buffers\r\n  // TODO(user): Insert getting data out code.\r\n    //int output = interpreter->outputs()[0];\r\n    std::cout << interpreter->typed_output_tensor<float>(0) << std::endl;\r\n  return 0;\r\n}\r\n```\r\n### Describe the problem\r\n\r\nI convert a keras model (very simplified to reproduce error) to tflite. I verified the graph using tensorboard and it seems fine. However, when I want to run it in C++ the following error occurs during the call of interpreter->AllocateTensors():\r\n\r\n```\r\ntensorflow/contrib/lite/kernels/conv.cc:189 input->dims->size != 4 (0 != 4)\r\n```\r\nThis does not happen when loading a model from [here](https://www.tensorflow.org/mobile/tflite/demo_android). Is this a bug in the graph conversion in the model loader or am I doing sth wrong?\r\n"}