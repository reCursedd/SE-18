{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23730", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23730/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23730/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23730/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23730", "id": 380599688, "node_id": "MDU6SXNzdWUzODA1OTk2ODg=", "number": 23730, "title": "Multi GPU, GPU to GPU communication stops", "user": {"login": "skang29", "id": 18202810, "node_id": "MDQ6VXNlcjE4MjAyODEw", "avatar_url": "https://avatars0.githubusercontent.com/u/18202810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skang29", "html_url": "https://github.com/skang29", "followers_url": "https://api.github.com/users/skang29/followers", "following_url": "https://api.github.com/users/skang29/following{/other_user}", "gists_url": "https://api.github.com/users/skang29/gists{/gist_id}", "starred_url": "https://api.github.com/users/skang29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skang29/subscriptions", "organizations_url": "https://api.github.com/users/skang29/orgs", "repos_url": "https://api.github.com/users/skang29/repos", "events_url": "https://api.github.com/users/skang29/events{/privacy}", "received_events_url": "https://api.github.com/users/skang29/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-14T09:08:20Z", "updated_at": "2018-11-21T04:51:06Z", "closed_at": "2018-11-17T11:01:40Z", "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: YES</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: NVIDIA DOCKER Image</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.11.0-0-gc19e29306c 1.11.0</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: NVIDIA DOCKER Image</li>\n<li><strong>GPU model and memory</strong>: Titan Xp (8 pcs.)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with:</p>\n<div class=\"highlight highlight-source-shell\"><pre>python -c <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)<span class=\"pl-pds\">\"</span></span></pre></div>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p>Hi, I'm trying to utilize 8 gpus for my GAN training. I use 8 pieces of Titan Xp. Detail of gpus are</p>\n<div class=\"highlight highlight-source-shell\"><pre>GPU <span class=\"pl-k\">|</span> Bus-Id\n0 <span class=\"pl-k\">|</span> 00000000:1B:00.0\n1 <span class=\"pl-k\">|</span> 00000000:1C:00.0\n2 <span class=\"pl-k\">|</span> 00000000:1D:00.0\n3 <span class=\"pl-k\">|</span> 00000000:1E:00.0\n4 <span class=\"pl-k\">|</span> 00000000:3D:00.0\n5 <span class=\"pl-k\">|</span> 00000000:3F:00.0\n6 <span class=\"pl-k\">|</span> 00000000:40:00.0\n7 <span class=\"pl-k\">|</span> 00000000:41:00.0</pre></div>\n<p>After I build model, I use this code to sync every device. Variables in each device has same prefix such as 'Tower_0', 'Tower_1', ..., 'Tower_7' and the 'Tower_0' is the main one.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Creating ops</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">device_sync_op</span>(<span class=\"pl-smi\">prefix</span>, <span class=\"pl-smi\">main_idx</span>):\n    <span class=\"pl-k\">import</span> re\n    all_vars <span class=\"pl-k\">=</span> tf.global_variables() <span class=\"pl-k\">+</span> tf.local_variables()\n    var_by_name <span class=\"pl-k\">=</span> <span class=\"pl-c1\">dict</span>([(v.name, v) <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> all_vars])\n    post_init_ops <span class=\"pl-k\">=</span> []\n    match_re <span class=\"pl-k\">=</span> <span class=\"pl-sr\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"</span>{}<span class=\"pl-c1\">\\d</span><span class=\"pl-k\">+</span><span class=\"pl-pds\">\"</span></span>.format(prefix.format(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>))\n    regex <span class=\"pl-k\">=</span> re.compile(match_re)\n    main_tower_name <span class=\"pl-k\">=</span> prefix.format(main_idx)\n    <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> all_vars:\n        tower_name <span class=\"pl-k\">=</span> regex.search(v.name)\n        <span class=\"pl-k\">if</span> tower_name <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            <span class=\"pl-k\">continue</span>\n\n        <span class=\"pl-k\">if</span> main_tower_name <span class=\"pl-k\">==</span> tower_name:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> no need to copy to main tower</span>\n            <span class=\"pl-k\">continue</span>\n\n        tower_name <span class=\"pl-k\">=</span> tower_name.group()\n\n        copy_from <span class=\"pl-k\">=</span> var_by_name.get(v.name.replace(tower_name, main_tower_name))\n        <span class=\"pl-k\">if</span> v.name <span class=\"pl-k\">==</span> copy_from:\n            <span class=\"pl-k\">continue</span>\n\n        <span class=\"pl-k\">if</span> copy_from <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n            post_init_ops.append(v.assign(copy_from.read_value()))\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">UserWarning</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Cannot find <span class=\"pl-c1\">{}</span> in the graph!<span class=\"pl-pds\">\"</span></span>.format(v.name.replace(tower_name, main_tower_name)))\n\n    <span class=\"pl-k\">return</span> tf.group(<span class=\"pl-k\">*</span>post_init_ops, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sync_variables_from_main_tower<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is the code I use to sync every devices</span>\nsync_op <span class=\"pl-k\">=</span> device_sync_op(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Tower_<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">0</span>)\nsess.run(sync_op)</pre></div>\n<p>This code works perfectly, if I utilize only 2 gpus from different PCI BUS group. As you can see above, GPU 0-3, GPU 4-7 have sequent PCI BUS addresses. So I call each group as PCI BUS group for easy explanation. In other words, if I use gpus (0, 4), (0,5), (0, 6), (0, 7), (1, 4), ..., (3, 7), the code works.<br>\nHowever, if I try to use 2 gpus from same group such as (0, 1), (0, 2), (0, 3), ..., (6, 7), it just doesn't response. I have to kill the process using <strong>kill -9</strong> or reboot computer at the worst. I cannot utilize more than 2 gpus for my model because of this problem.</p>\n<p>Is there any options I have to enable or drivers to install more to solve this problem? Or is it impossible to make each device communicate each other?</p>\n<p>Waiting for your wise solutions.</p>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): NVIDIA DOCKER Image\nTensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\nPython version: 3.5.2\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: NVIDIA DOCKER Image\nGPU model and memory: Titan Xp (8 pcs.)\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with:\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nHi, I'm trying to utilize 8 gpus for my GAN training. I use 8 pieces of Titan Xp. Detail of gpus are\nGPU | Bus-Id\n0 | 00000000:1B:00.0\n1 | 00000000:1C:00.0\n2 | 00000000:1D:00.0\n3 | 00000000:1E:00.0\n4 | 00000000:3D:00.0\n5 | 00000000:3F:00.0\n6 | 00000000:40:00.0\n7 | 00000000:41:00.0\nAfter I build model, I use this code to sync every device. Variables in each device has same prefix such as 'Tower_0', 'Tower_1', ..., 'Tower_7' and the 'Tower_0' is the main one.\n# Creating ops\ndef device_sync_op(prefix, main_idx):\n    import re\n    all_vars = tf.global_variables() + tf.local_variables()\n    var_by_name = dict([(v.name, v) for v in all_vars])\n    post_init_ops = []\n    match_re = r\"{}\\d+\".format(prefix.format(\"\"))\n    regex = re.compile(match_re)\n    main_tower_name = prefix.format(main_idx)\n    for v in all_vars:\n        tower_name = regex.search(v.name)\n        if tower_name is None:\n            continue\n\n        if main_tower_name == tower_name:\n            # no need to copy to main tower\n            continue\n\n        tower_name = tower_name.group()\n\n        copy_from = var_by_name.get(v.name.replace(tower_name, main_tower_name))\n        if v.name == copy_from:\n            continue\n\n        if copy_from is not None:\n            post_init_ops.append(v.assign(copy_from.read_value()))\n        else:\n            UserWarning(\"Cannot find {} in the graph!\".format(v.name.replace(tower_name, main_tower_name)))\n\n    return tf.group(*post_init_ops, name=\"sync_variables_from_main_tower\")\n\n# This is the code I use to sync every devices\nsync_op = device_sync_op(\"Tower_%d\", 0)\nsess.run(sync_op)\nThis code works perfectly, if I utilize only 2 gpus from different PCI BUS group. As you can see above, GPU 0-3, GPU 4-7 have sequent PCI BUS addresses. So I call each group as PCI BUS group for easy explanation. In other words, if I use gpus (0, 4), (0,5), (0, 6), (0, 7), (1, 4), ..., (3, 7), the code works.\nHowever, if I try to use 2 gpus from same group such as (0, 1), (0, 2), (0, 3), ..., (6, 7), it just doesn't response. I have to kill the process using kill -9 or reboot computer at the worst. I cannot utilize more than 2 gpus for my model because of this problem.\nIs there any options I have to enable or drivers to install more to solve this problem? Or is it impossible to make each device communicate each other?\nWaiting for your wise solutions.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: NVIDIA DOCKER Image\r\n- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NVIDIA DOCKER Image\r\n- **GPU model and memory**: Titan Xp (8 pcs.)\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n\r\nHi, I'm trying to utilize 8 gpus for my GAN training. I use 8 pieces of Titan Xp. Detail of gpus are\r\n```bash\r\nGPU | Bus-Id\r\n0 | 00000000:1B:00.0\r\n1 | 00000000:1C:00.0\r\n2 | 00000000:1D:00.0\r\n3 | 00000000:1E:00.0\r\n4 | 00000000:3D:00.0\r\n5 | 00000000:3F:00.0\r\n6 | 00000000:40:00.0\r\n7 | 00000000:41:00.0\r\n```\r\n\r\nAfter I build model, I use this code to sync every device. Variables in each device has same prefix such as 'Tower_0', 'Tower_1', ..., 'Tower_7' and the 'Tower_0' is the main one.\r\n```python\r\n# Creating ops\r\ndef device_sync_op(prefix, main_idx):\r\n    import re\r\n    all_vars = tf.global_variables() + tf.local_variables()\r\n    var_by_name = dict([(v.name, v) for v in all_vars])\r\n    post_init_ops = []\r\n    match_re = r\"{}\\d+\".format(prefix.format(\"\"))\r\n    regex = re.compile(match_re)\r\n    main_tower_name = prefix.format(main_idx)\r\n    for v in all_vars:\r\n        tower_name = regex.search(v.name)\r\n        if tower_name is None:\r\n            continue\r\n\r\n        if main_tower_name == tower_name:\r\n            # no need to copy to main tower\r\n            continue\r\n\r\n        tower_name = tower_name.group()\r\n\r\n        copy_from = var_by_name.get(v.name.replace(tower_name, main_tower_name))\r\n        if v.name == copy_from:\r\n            continue\r\n\r\n        if copy_from is not None:\r\n            post_init_ops.append(v.assign(copy_from.read_value()))\r\n        else:\r\n            UserWarning(\"Cannot find {} in the graph!\".format(v.name.replace(tower_name, main_tower_name)))\r\n\r\n    return tf.group(*post_init_ops, name=\"sync_variables_from_main_tower\")\r\n\r\n# This is the code I use to sync every devices\r\nsync_op = device_sync_op(\"Tower_%d\", 0)\r\nsess.run(sync_op)\r\n```\r\n\r\nThis code works perfectly, if I utilize only 2 gpus from different PCI BUS group. As you can see above, GPU 0-3, GPU 4-7 have sequent PCI BUS addresses. So I call each group as PCI BUS group for easy explanation. In other words, if I use gpus (0, 4), (0,5), (0, 6), (0, 7), (1, 4), ..., (3, 7), the code works.\r\nHowever, if I try to use 2 gpus from same group such as (0, 1), (0, 2), (0, 3), ..., (6, 7), it just doesn't response. I have to kill the process using **kill -9** or reboot computer at the worst. I cannot utilize more than 2 gpus for my model because of this problem.\r\n\r\nIs there any options I have to enable or drivers to install more to solve this problem? Or is it impossible to make each device communicate each other?\r\n\r\nWaiting for your wise solutions."}