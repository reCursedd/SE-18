{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/95630657", "pull_request_review_id": 16190508, "id": 95630657, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk1NjMwNjU3", "diff_hunk": "@@ -2129,6 +2131,97 @@ def legacy_fully_connected(x,\n \n     return _apply_activation(y, activation_fn, output_collections)\n \n+def adaptive_softmax_loss(inputs,\n+                          labels,\n+                          cutoff,\n+                          project_factor=4,\n+                          initializer=None,\n+                          name=None):\n+  \"\"\"Computes and returns the adaptive softmax loss (a improvement of \n+  hierarchical softmax).\n+    \n+  See [Efficient softmax approximation for GPUs](https://arxiv.org/pdf/1609.04309v2.pdf).\n+        \n+  This is a faster way to train a softmax classifier over a huge number of \n+  classes, and can be used for **both training and prediction**. For example, it \n+  can be used for training a Language Model with a very huge vocabulary, and \n+  the trained languaed model can be used in speech recognition, text generation, \n+  and machine translation very efficiently.\n+  \n+  Args:\n+    inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward\n+      activations of the input network.\n+    labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-2}]` and dtype `int32` or\n+      `int64`. Each entry in `labels` must be an index in `[0, num_classes)`.\n+    cutoff: A list indicating the limits of the different clusters.\n+    project_factor: A floating point value greater or equal to 1.0. The projection \n+      factor between two neighboring clusters.\n+    initializer: Initializer for adaptive softmax variables (optional).\n+    name: A name for the operation (optional).\n+\n+  Returns:\n+    loss: A `batch_size` 1-D tensor of the adaptive softmax cross entropy loss.\n+    training_losses: A list of 1-D tensors of adaptive softmax loss for each \n+      cluster, which can be used for calculating the gradients and back \n+      propagation when training.\n+  \"\"\"\n+  input_dim = int(inputs.get_shape()[1])\n+  sample_num = int(inputs.get_shape()[0])\n+  cluster_num = len(cutoff) - 1\n+  with ops.name_scope(name, \"AdaptiveSoftmax\"):\n+    if initializer is None:\n+      stdv = math.sqrt(1. / input_dim)\n+      initializer = init_ops.random_uniform_initializer(-stdv * 0.8, stdv * 0.8)\n+\n+    head_dim = cutoff[0] + cluster_num\n+    head_w = variable_scope.get_variable(\"adaptive_softmax_head_w\", \n+                             [input_dim, head_dim], initializer=initializer)\n+\n+    tail_project_factor = project_factor\n+    tail_w = []\n+    for i in range(cluster_num):\n+      project_dim = max(1, input_dim // tail_project_factor)\n+      tail_dim = cutoff[i + 1] - cutoff[i]\n+      tail_w.append([\n+        variable_scope.get_variable(\"adaptive_softmax_tail{}_proj_w\".format(i+1), \n+                        [input_dim, project_dim], initializer=initializer),\n+        variable_scope.get_variable(\"adaptive_softmax_tail{}_w\".format(i+1), \n+                        [project_dim, tail_dim], initializer=initializer)\n+      ])\n+      tail_project_factor *= project_factor\n+\n+    # Get tail masks and update head labels\n+    training_losses = []\n+    loss = array_ops.zeros([sample_num], dtype=dtypes.float32)\n+    head_labels = labels\n+    for i in range(cluster_num):\n+      mask = math_ops.logical_and(math_ops.greater_equal(labels, cutoff[i]), \n+                                  math_ops.less(labels, cutoff[i + 1]))\n+      \n+      # Update head labels\n+      head_labels = math_ops.select(mask, array_ops.constant([cutoff[0] + i] * \n+                            sample_num), head_labels)\n+\n+      # Compute tail loss\n+      tail_inputs = array_ops.boolean_mask(inputs, mask)\n+      tail_logits = math_ops.matmul(math_ops.matmul(tail_inputs, tail_w[i][0]), \n+                                    tail_w[i][1])\n+      tail_labels = array_ops.boolean_mask(labels - cutoff[i], mask)\n+      tail_loss = nn.sparse_softmax_cross_entropy_with_logits(tail_logits, \n+                                                                  tail_labels)\n+      training_losses.append(tail_loss)\n+      aligned_tail_loss = sparse_tensor.SparseTensor(\n+        array_ops.squeeze(array_ops.where(mask)), tail_loss, [sample_num])\n+      loss += sparse_ops.sparse_tensor_to_dense(aligned_tail_loss)", "path": "tensorflow/contrib/layers/python/layers/layers.py", "position": 103, "original_position": 103, "commit_id": "b98dc09da4f979d85ad27c6ff4c901af1bffdec3", "original_commit_id": "b98dc09da4f979d85ad27c6ff4c901af1bffdec3", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "see if you can use array_ops.scatter_nd here.  that one has a gradient.", "created_at": "2017-01-11T17:38:30Z", "updated_at": "2017-01-11T17:39:27Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6632#discussion_r95630657", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6632", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/95630657"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6632#discussion_r95630657"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6632"}}, "body_html": "<p>see if you can use array_ops.scatter_nd here.  that one has a gradient.</p>", "body_text": "see if you can use array_ops.scatter_nd here.  that one has a gradient."}