{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20131", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20131/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20131/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20131/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20131", "id": 333887732, "node_id": "MDU6SXNzdWUzMzM4ODc3MzI=", "number": 20131, "title": "Computation Paths of different attention mechanism", "user": {"login": "baluyotraf", "id": 7478783, "node_id": "MDQ6VXNlcjc0Nzg3ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7478783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/baluyotraf", "html_url": "https://github.com/baluyotraf", "followers_url": "https://api.github.com/users/baluyotraf/followers", "following_url": "https://api.github.com/users/baluyotraf/following{/other_user}", "gists_url": "https://api.github.com/users/baluyotraf/gists{/gist_id}", "starred_url": "https://api.github.com/users/baluyotraf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/baluyotraf/subscriptions", "organizations_url": "https://api.github.com/users/baluyotraf/orgs", "repos_url": "https://api.github.com/users/baluyotraf/repos", "events_url": "https://api.github.com/users/baluyotraf/events{/privacy}", "received_events_url": "https://api.github.com/users/baluyotraf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-06-20T00:51:36Z", "updated_at": "2018-08-30T23:11:23Z", "closed_at": "2018-08-30T23:11:23Z", "author_association": "NONE", "body_html": "<p>I was checking the computation paths of the different attention mechanism and compared them with the attention wrapper implementation. I noticed that the computation paths follows the Luong implementation more closely. People brought up this issue in <a href=\"https://github.com/tensorflow/tensorflow/issues/9635\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9635/hovercard\">9635</a>  but the thread was closed since the main issue was answered already.</p>\n<p>To summarize there were two attention mechanisms. Luong uses the computation path  h(t)-&gt;a(t)-&gt;c(t)-&gt;h\u02dc(t) while Bahnadau uses the h(t-1)-&gt;a(t)-&gt;c(t)-&gt;h(t).</p>\n<p>I seems that right now this is not a priority to implement as the people in the discussion are saying that there is no significant difference.  However I think it would be better to document that the attention wrapper implementation is more similar the the Luong attention just so people are aware. Also the output_attention does not really change the computation paths so I think the description for the flag should be changed.</p>\n<p>That being said, this is my first time to read deeply into the internal tensorflow code. If just me misunderstanding the implementation, then feel free to point out the things I might have missed. Thanks.</p>\n<p>Edit:<br>\nI looked at the code once again. It seems like the I was just having trouble due to the recursive nature of the code.<br>\nWhen using output_attention=True, the recursion goes to h(t)-&gt;a(t)-&gt;c(t)-&gt;h\u02dc(t).<br>\nWhen using output_attention=False, the recursion goes to c(t)-&gt;h(t)-&gt;a(t+1)-&gt;c(t+1) which is the same if to h(t-1)-&gt;a(t)-&gt;c(t)-&gt;h(t).</p>\n<p>If things work like my edit, feel free to close the issue.</p>\n<p>Edit 2:<br>\nIn <a href=\"https://github.com/tensorflow/tensorflow/issues/9635\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9635/hovercard\">issues 9635</a> other people also saw this the first time I saw it (that the paths are different) so a verification or second opinion will be helpful.</p>", "body_text": "I was checking the computation paths of the different attention mechanism and compared them with the attention wrapper implementation. I noticed that the computation paths follows the Luong implementation more closely. People brought up this issue in 9635  but the thread was closed since the main issue was answered already.\nTo summarize there were two attention mechanisms. Luong uses the computation path  h(t)->a(t)->c(t)->h\u02dc(t) while Bahnadau uses the h(t-1)->a(t)->c(t)->h(t).\nI seems that right now this is not a priority to implement as the people in the discussion are saying that there is no significant difference.  However I think it would be better to document that the attention wrapper implementation is more similar the the Luong attention just so people are aware. Also the output_attention does not really change the computation paths so I think the description for the flag should be changed.\nThat being said, this is my first time to read deeply into the internal tensorflow code. If just me misunderstanding the implementation, then feel free to point out the things I might have missed. Thanks.\nEdit:\nI looked at the code once again. It seems like the I was just having trouble due to the recursive nature of the code.\nWhen using output_attention=True, the recursion goes to h(t)->a(t)->c(t)->h\u02dc(t).\nWhen using output_attention=False, the recursion goes to c(t)->h(t)->a(t+1)->c(t+1) which is the same if to h(t-1)->a(t)->c(t)->h(t).\nIf things work like my edit, feel free to close the issue.\nEdit 2:\nIn issues 9635 other people also saw this the first time I saw it (that the paths are different) so a verification or second opinion will be helpful.", "body": "I was checking the computation paths of the different attention mechanism and compared them with the attention wrapper implementation. I noticed that the computation paths follows the Luong implementation more closely. People brought up this issue in [9635](https://github.com/tensorflow/tensorflow/issues/9635)  but the thread was closed since the main issue was answered already.\r\n\r\nTo summarize there were two attention mechanisms. Luong uses the computation path  h(t)->a(t)->c(t)->h\u02dc(t) while Bahnadau uses the h(t-1)->a(t)->c(t)->h(t).\r\n\r\nI seems that right now this is not a priority to implement as the people in the discussion are saying that there is no significant difference.  However I think it would be better to document that the attention wrapper implementation is more similar the the Luong attention just so people are aware. Also the output_attention does not really change the computation paths so I think the description for the flag should be changed.\r\n\r\nThat being said, this is my first time to read deeply into the internal tensorflow code. If just me misunderstanding the implementation, then feel free to point out the things I might have missed. Thanks.\r\n\r\nEdit:\r\nI looked at the code once again. It seems like the I was just having trouble due to the recursive nature of the code.\r\nWhen using output_attention=True, the recursion goes to h(t)->a(t)->c(t)->h\u02dc(t).\r\nWhen using output_attention=False, the recursion goes to c(t)->h(t)->a(t+1)->c(t+1) which is the same if to h(t-1)->a(t)->c(t)->h(t).\r\n\r\nIf things work like my edit, feel free to close the issue.\r\n\r\nEdit 2:\r\nIn [issues 9635](https://github.com/tensorflow/tensorflow/issues/9635) other people also saw this the first time I saw it (that the paths are different) so a verification or second opinion will be helpful."}