{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13097", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13097/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13097/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13097/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13097", "id": 258289281, "node_id": "MDU6SXNzdWUyNTgyODkyODE=", "number": 13097, "title": "Support 64bit float point gradient", "user": {"login": "ZhongBaby", "id": 8222739, "node_id": "MDQ6VXNlcjgyMjI3Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/8222739?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhongBaby", "html_url": "https://github.com/ZhongBaby", "followers_url": "https://api.github.com/users/ZhongBaby/followers", "following_url": "https://api.github.com/users/ZhongBaby/following{/other_user}", "gists_url": "https://api.github.com/users/ZhongBaby/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhongBaby/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhongBaby/subscriptions", "organizations_url": "https://api.github.com/users/ZhongBaby/orgs", "repos_url": "https://api.github.com/users/ZhongBaby/repos", "events_url": "https://api.github.com/users/ZhongBaby/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhongBaby/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-09-17T08:50:30Z", "updated_at": "2018-04-09T17:47:29Z", "closed_at": "2018-04-09T17:47:29Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.1.0</li>\n<li><strong>Python version</strong>:  3.5.3</li>\n<li><strong>CUDA/cuDNN version</strong>:Cuda 8.0 cudnn 5.1</li>\n<li><strong>GPU model and memory</strong>:GTX 1080Ti 11GB</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I use Tensorflow to train a multilayer Convolutional network.</p>\n<p>Since my network has too many parameters and my GTX 1080Ti has limited memory(11GB), so the batch size cannot exceed 16 otherwise it would cause OutOfMemory exception.</p>\n<p>I want to update parameters using bigger batch size, so I follow <a href=\"https://stackoverflow.com/questions/42156957/how-to-update-model-parameters-with-accumulated-gradients\" rel=\"nofollow\">the answer</a> that is, accumulate and average gradients over multiple batches.</p>\n<p><strong>Scenario 1</strong></p>\n<p>If I use batch size=16, and update parameters after each batch, my network can converge to loss=0.01.</p>\n<p><strong>Scenario 2</strong></p>\n<p>If I use batch size=1, and update parameters after accumulating and averaging gradients after every 16 batches, my network can only converge to loss=0.04.</p>\n<p>Theoreticall the two scenarios should converge to the same loss, but the problem is when the network converge close to the extrema, the magnitude of the gradients is about 1e-5.</p>\n<p>And guess how precise is float32 in Tensorflow? I compute gradients and don't update parameters, they differ after 6 significant digits.</p>\n<p>I want my network continues to converge even the magnitude of the gradients is about 1e-5, the float32 cannot satisfy my needs.</p>\n<p>The obvious solution is to use float64 as the data type of the parameters, but Tensorflow tells me float64 is not supported in Conv2D.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):1.1.0\nPython version:  3.5.3\nCUDA/cuDNN version:Cuda 8.0 cudnn 5.1\nGPU model and memory:GTX 1080Ti 11GB\n\nDescribe the problem\nI use Tensorflow to train a multilayer Convolutional network.\nSince my network has too many parameters and my GTX 1080Ti has limited memory(11GB), so the batch size cannot exceed 16 otherwise it would cause OutOfMemory exception.\nI want to update parameters using bigger batch size, so I follow the answer that is, accumulate and average gradients over multiple batches.\nScenario 1\nIf I use batch size=16, and update parameters after each batch, my network can converge to loss=0.01.\nScenario 2\nIf I use batch size=1, and update parameters after accumulating and averaging gradients after every 16 batches, my network can only converge to loss=0.04.\nTheoreticall the two scenarios should converge to the same loss, but the problem is when the network converge close to the extrema, the magnitude of the gradients is about 1e-5.\nAnd guess how precise is float32 in Tensorflow? I compute gradients and don't update parameters, they differ after 6 significant digits.\nI want my network continues to converge even the magnitude of the gradients is about 1e-5, the float32 cannot satisfy my needs.\nThe obvious solution is to use float64 as the data type of the parameters, but Tensorflow tells me float64 is not supported in Conv2D.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.1.0\r\n- **Python version**:  3.5.3\r\n- **CUDA/cuDNN version**:Cuda 8.0 cudnn 5.1\r\n- **GPU model and memory**:GTX 1080Ti 11GB\r\n\r\n### Describe the problem\r\nI use Tensorflow to train a multilayer Convolutional network.\r\n\r\nSince my network has too many parameters and my GTX 1080Ti has limited memory(11GB), so the batch size cannot exceed 16 otherwise it would cause OutOfMemory exception.\r\n\r\nI want to update parameters using bigger batch size, so I follow [the answer](https://stackoverflow.com/questions/42156957/how-to-update-model-parameters-with-accumulated-gradients) that is, accumulate and average gradients over multiple batches.\r\n\r\n**Scenario 1**\r\n\r\nIf I use batch size=16, and update parameters after each batch, my network can converge to loss=0.01.\r\n\r\n**Scenario 2**\r\n\r\nIf I use batch size=1, and update parameters after accumulating and averaging gradients after every 16 batches, my network can only converge to loss=0.04.\r\n\r\nTheoreticall the two scenarios should converge to the same loss, but the problem is when the network converge close to the extrema, the magnitude of the gradients is about 1e-5.\r\n\r\nAnd guess how precise is float32 in Tensorflow? I compute gradients and don't update parameters, they differ after 6 significant digits.\r\n\r\nI want my network continues to converge even the magnitude of the gradients is about 1e-5, the float32 cannot satisfy my needs.\r\n\r\nThe obvious solution is to use float64 as the data type of the parameters, but Tensorflow tells me float64 is not supported in Conv2D."}