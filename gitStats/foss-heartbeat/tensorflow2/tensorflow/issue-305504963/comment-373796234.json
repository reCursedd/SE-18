{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/373796234", "html_url": "https://github.com/tensorflow/tensorflow/issues/17735#issuecomment-373796234", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17735", "id": 373796234, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Mzc5NjIzNA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-16T17:59:41Z", "updated_at": "2018-03-16T18:00:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15856029\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Raj-08\">@Raj-08</a> tf.assign returns \"ref\", ie link to the variable. So if you implemented backprop for it, the result would be zero since state of the variable is functionally independent of other parts of the graph.</p>\n<p>Do you want the gradients to just propagate the right hand side of the assignment? If so, you could differentiate the right-hand side explicitly, and use control dependencies to force assignment to happen on forward pass</p>\n<pre><code>a = tf.Variable(1)\nb = tf.assign(a, stuff)\nwith tf.control_dependencies([b]):\n  c = tf.identity(stuff)\ntf.gradients(c, ...)  # this works\n</code></pre>", "body_text": "@Raj-08 tf.assign returns \"ref\", ie link to the variable. So if you implemented backprop for it, the result would be zero since state of the variable is functionally independent of other parts of the graph.\nDo you want the gradients to just propagate the right hand side of the assignment? If so, you could differentiate the right-hand side explicitly, and use control dependencies to force assignment to happen on forward pass\na = tf.Variable(1)\nb = tf.assign(a, stuff)\nwith tf.control_dependencies([b]):\n  c = tf.identity(stuff)\ntf.gradients(c, ...)  # this works", "body": "@Raj-08 tf.assign returns \"ref\", ie link to the variable. So if you implemented backprop for it, the result would be zero since state of the variable is functionally independent of other parts of the graph.\r\n\r\nDo you want the gradients to just propagate the right hand side of the assignment? If so, you could differentiate the right-hand side explicitly, and use control dependencies to force assignment to happen on forward pass\r\n\r\n```\r\na = tf.Variable(1)\r\nb = tf.assign(a, stuff)\r\nwith tf.control_dependencies([b]):\r\n  c = tf.identity(stuff)\r\ntf.gradients(c, ...)  # this works\r\n```"}