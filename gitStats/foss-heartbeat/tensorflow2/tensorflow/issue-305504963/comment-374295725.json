{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/374295725", "html_url": "https://github.com/tensorflow/tensorflow/issues/17735#issuecomment-374295725", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17735", "id": 374295725, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NDI5NTcyNQ==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-19T17:23:26Z", "updated_at": "2018-03-19T17:23:26Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15856029\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Raj-08\">@Raj-08</a> what do you mean \"apply my gradients to tensors\"? Either you want to store mutable variables as your layer weights (which are initialized from the predictions of another network), in which case you apply your gradients to the variables but you can't backprop through them, or you want your layer weights to be computed tensors, in which case you should apply the gradients not to them but to the network which is producing them, right?</p>\n<p>Or is there another option I'm missing here?</p>", "body_text": "@Raj-08 what do you mean \"apply my gradients to tensors\"? Either you want to store mutable variables as your layer weights (which are initialized from the predictions of another network), in which case you apply your gradients to the variables but you can't backprop through them, or you want your layer weights to be computed tensors, in which case you should apply the gradients not to them but to the network which is producing them, right?\nOr is there another option I'm missing here?", "body": "@Raj-08 what do you mean \"apply my gradients to tensors\"? Either you want to store mutable variables as your layer weights (which are initialized from the predictions of another network), in which case you apply your gradients to the variables but you can't backprop through them, or you want your layer weights to be computed tensors, in which case you should apply the gradients not to them but to the network which is producing them, right?\r\n\r\nOr is there another option I'm missing here?"}