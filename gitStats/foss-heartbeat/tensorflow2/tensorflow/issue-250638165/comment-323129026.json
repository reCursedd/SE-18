{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323129026", "html_url": "https://github.com/tensorflow/tensorflow/issues/12330#issuecomment-323129026", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12330", "id": 323129026, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzEyOTAyNg==", "user": {"login": "iramusa", "id": 4759395, "node_id": "MDQ6VXNlcjQ3NTkzOTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/4759395?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iramusa", "html_url": "https://github.com/iramusa", "followers_url": "https://api.github.com/users/iramusa/followers", "following_url": "https://api.github.com/users/iramusa/following{/other_user}", "gists_url": "https://api.github.com/users/iramusa/gists{/gist_id}", "starred_url": "https://api.github.com/users/iramusa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iramusa/subscriptions", "organizations_url": "https://api.github.com/users/iramusa/orgs", "repos_url": "https://api.github.com/users/iramusa/repos", "events_url": "https://api.github.com/users/iramusa/events{/privacy}", "received_events_url": "https://api.github.com/users/iramusa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T16:44:04Z", "updated_at": "2017-08-17T17:20:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=498544\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Sohl-Dickstein\">@Sohl-Dickstein</a></p>\n<p>Thank you for suggestion to use tf.squeeze(). Unfortunately this doesn't solve the issue for me.</p>\n<p>Let me be more concrete. This is what I'm trying to do using Keras with tf as backend. First I build a network:</p>\n<pre><code>x = Input(batch_shape=(1, 10))\nv = Dense(10)(x)\ny = Dense(1)(v)\nnet = Model(x, y)\n</code></pre>\n<p>Then I train it, etc. Then I'm interested in hessian of <code>y</code> wrt <code>v</code>. <code>tf.gradients(y, v)</code> works perfectly fine.</p>\n<p>What I attempted:</p>\n<p>(1) Naive: <code>tf.hessians(y, v)</code> throws:</p>\n<pre><code>ValueError: Computing hessians is currently only supported for one-dimensional tensors. Element number 0 of `xs` has 2 dimensions.\n</code></pre>\n<p>(2) Using <code>tf.squeeze</code> to force the right shape of <code>v</code> does not help. I think that's because in the computation graph is <code>v</code> copied into a new tensor and there is no differentiable connection between that tensor and <code>y</code>. <code>tf.reshape(v (-1,))</code> has the same result. What I observe is that:</p>\n<pre><code>v_flat = tf.squeeze(v);\ntf.gradients(y, v_flat)\n</code></pre>\n<p>returns <code>[None]</code></p>\n<p>(3) I can modify my original network the following way:</p>\n<pre><code>x = Input(batch_shape=(1, 10))\nv = Dense(10)(x)\n\nv_flat = tf.squeeze(v)  # added\nv_non_flat = tf.reshape(v_flat, (1, 10))   # added\n\ny = Dense(1)(v_non_flat)\nnet = Model(x, y)\n</code></pre>\n<p>Then, I hope to be able to call <code>tf.hessians(y, v_flat)</code>. Unfortunately, I cannot test that because the above code halts with error:</p>\n<pre><code>TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-14-68462b8d53de&gt; in &lt;module&gt;()\n      7 \n      8 y = Dense(10)(v_non_flat)\n----&gt; 9 net = Model(x, y)\n     10 net.compile(optimizer='adam', loss='mse')\n     11 \n\n/home/ira/code/envs/flexi/lib/python3.5/site-packages/keras/engine/topology.py in __init__(self, input, output, name)\n   1814                 cls_name = self.__class__.__name__\n   1815                 raise TypeError('Output tensors to a ' + cls_name + ' must be '\n-&gt; 1816                                 'Keras tensors. Found: ' + str(x))\n   1817         # Build self.output_layers:\n   1818         for x in self.outputs:\n\nTypeError: Output tensors to a Model must be Keras tensors. Found: Tensor(\"add_5:0\", shape=(1, 1), dtype=float32)\n</code></pre>\n<p>Do you have any further suggestions? Possibly solving the issue from (3) could bring me on track.</p>", "body_text": "@reedwm @Sohl-Dickstein\nThank you for suggestion to use tf.squeeze(). Unfortunately this doesn't solve the issue for me.\nLet me be more concrete. This is what I'm trying to do using Keras with tf as backend. First I build a network:\nx = Input(batch_shape=(1, 10))\nv = Dense(10)(x)\ny = Dense(1)(v)\nnet = Model(x, y)\n\nThen I train it, etc. Then I'm interested in hessian of y wrt v. tf.gradients(y, v) works perfectly fine.\nWhat I attempted:\n(1) Naive: tf.hessians(y, v) throws:\nValueError: Computing hessians is currently only supported for one-dimensional tensors. Element number 0 of `xs` has 2 dimensions.\n\n(2) Using tf.squeeze to force the right shape of v does not help. I think that's because in the computation graph is v copied into a new tensor and there is no differentiable connection between that tensor and y. tf.reshape(v (-1,)) has the same result. What I observe is that:\nv_flat = tf.squeeze(v);\ntf.gradients(y, v_flat)\n\nreturns [None]\n(3) I can modify my original network the following way:\nx = Input(batch_shape=(1, 10))\nv = Dense(10)(x)\n\nv_flat = tf.squeeze(v)  # added\nv_non_flat = tf.reshape(v_flat, (1, 10))   # added\n\ny = Dense(1)(v_non_flat)\nnet = Model(x, y)\n\nThen, I hope to be able to call tf.hessians(y, v_flat). Unfortunately, I cannot test that because the above code halts with error:\nTypeError                                 Traceback (most recent call last)\n<ipython-input-14-68462b8d53de> in <module>()\n      7 \n      8 y = Dense(10)(v_non_flat)\n----> 9 net = Model(x, y)\n     10 net.compile(optimizer='adam', loss='mse')\n     11 \n\n/home/ira/code/envs/flexi/lib/python3.5/site-packages/keras/engine/topology.py in __init__(self, input, output, name)\n   1814                 cls_name = self.__class__.__name__\n   1815                 raise TypeError('Output tensors to a ' + cls_name + ' must be '\n-> 1816                                 'Keras tensors. Found: ' + str(x))\n   1817         # Build self.output_layers:\n   1818         for x in self.outputs:\n\nTypeError: Output tensors to a Model must be Keras tensors. Found: Tensor(\"add_5:0\", shape=(1, 1), dtype=float32)\n\nDo you have any further suggestions? Possibly solving the issue from (3) could bring me on track.", "body": "@reedwm @Sohl-Dickstein \r\n\r\nThank you for suggestion to use tf.squeeze(). Unfortunately this doesn't solve the issue for me.\r\n\r\nLet me be more concrete. This is what I'm trying to do using Keras with tf as backend. First I build a network:\r\n\r\n```\r\nx = Input(batch_shape=(1, 10))\r\nv = Dense(10)(x)\r\ny = Dense(1)(v)\r\nnet = Model(x, y)\r\n```\r\n\r\nThen I train it, etc. Then I'm interested in hessian of `y` wrt `v`. `tf.gradients(y, v)` works perfectly fine.\r\n\r\nWhat I attempted:\r\n\r\n(1) Naive: `tf.hessians(y, v)` throws:\r\n```\r\nValueError: Computing hessians is currently only supported for one-dimensional tensors. Element number 0 of `xs` has 2 dimensions.\r\n```\r\n\r\n(2) Using `tf.squeeze` to force the right shape of `v` does not help. I think that's because in the computation graph is `v` copied into a new tensor and there is no differentiable connection between that tensor and `y`. `tf.reshape(v (-1,))` has the same result. What I observe is that:\r\n```\r\nv_flat = tf.squeeze(v);\r\ntf.gradients(y, v_flat)\r\n```\r\nreturns `[None]`\r\n\r\n(3) I can modify my original network the following way:\r\n```\r\nx = Input(batch_shape=(1, 10))\r\nv = Dense(10)(x)\r\n\r\nv_flat = tf.squeeze(v)  # added\r\nv_non_flat = tf.reshape(v_flat, (1, 10))   # added\r\n\r\ny = Dense(1)(v_non_flat)\r\nnet = Model(x, y)\r\n```\r\nThen, I hope to be able to call `tf.hessians(y, v_flat)`. Unfortunately, I cannot test that because the above code halts with error:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-14-68462b8d53de> in <module>()\r\n      7 \r\n      8 y = Dense(10)(v_non_flat)\r\n----> 9 net = Model(x, y)\r\n     10 net.compile(optimizer='adam', loss='mse')\r\n     11 \r\n\r\n/home/ira/code/envs/flexi/lib/python3.5/site-packages/keras/engine/topology.py in __init__(self, input, output, name)\r\n   1814                 cls_name = self.__class__.__name__\r\n   1815                 raise TypeError('Output tensors to a ' + cls_name + ' must be '\r\n-> 1816                                 'Keras tensors. Found: ' + str(x))\r\n   1817         # Build self.output_layers:\r\n   1818         for x in self.outputs:\r\n\r\nTypeError: Output tensors to a Model must be Keras tensors. Found: Tensor(\"add_5:0\", shape=(1, 1), dtype=float32)\r\n```\r\n\r\nDo you have any further suggestions? Possibly solving the issue from (3) could bring me on track."}