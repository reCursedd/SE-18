{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303781339", "html_url": "https://github.com/tensorflow/tensorflow/issues/10145#issuecomment-303781339", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10145", "id": 303781339, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzc4MTMzOQ==", "user": {"login": "lw394", "id": 15891975, "node_id": "MDQ6VXNlcjE1ODkxOTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/15891975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lw394", "html_url": "https://github.com/lw394", "followers_url": "https://api.github.com/users/lw394/followers", "following_url": "https://api.github.com/users/lw394/following{/other_user}", "gists_url": "https://api.github.com/users/lw394/gists{/gist_id}", "starred_url": "https://api.github.com/users/lw394/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lw394/subscriptions", "organizations_url": "https://api.github.com/users/lw394/orgs", "repos_url": "https://api.github.com/users/lw394/repos", "events_url": "https://api.github.com/users/lw394/events{/privacy}", "received_events_url": "https://api.github.com/users/lw394/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-24T16:40:16Z", "updated_at": "2017-05-24T16:40:16Z", "author_association": "NONE", "body_html": "<p>Ok, I've double checked, and as anticipated it makes no difference.  All 4 workers train at ~1.5steps/sec.  GPU/CPU utilization is the same as before (i.e. 0-5% per GPU at any give time).  This is 33x slower for me than just using a single GPU in the non-distributed setting.</p>\n<p>Specifically, I did away with async_btwgraph_launcher.py completely, and added the following to the end of async_btwgraph_dist_trainer.py:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    output_dir <span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>tfprojects/output_dir_debug<span class=\"pl-pds\">'</span></span>\n\n    cluster_spec <span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2227<span class=\"pl-pds\">\"</span></span>,\n                                ],\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: [\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2223<span class=\"pl-pds\">\"</span></span>,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2224<span class=\"pl-pds\">\"</span></span>,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2225<span class=\"pl-pds\">\"</span></span>,\n                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2226<span class=\"pl-pds\">\"</span></span>\n                    ]\n                }\n    cluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec(cluster_spec)\n    params <span class=\"pl-k\">=</span> HParams(<span class=\"pl-v\">cluster</span><span class=\"pl-k\">=</span>cluster,\n                     <span class=\"pl-v\">job_name</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.job_name,\n                     <span class=\"pl-v\">task_index</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.task_index)\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.job_name<span class=\"pl-k\">==</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> allow each worker to see only 1 of the 4 GPUS</span>\n        os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">\"</span></span>]<span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>(params.task_index)\n\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> hide all 4 GPUS from ps</span>\n        os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">\"</span></span>]<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>\n    train(output_dir, params)</pre></div>\n<p>As in the online example, I then run these 5 commands, each one in a separate terminal session.</p>\n<div class=\"highlight highlight-source-shell\"><pre>python async_btwgraph_dist_trainer.py --job_name=ps --task_index=0</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>python async_btwgraph_dist_trainer.py --job_name=worker --task_index=0</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>python async_btwgraph_dist_trainer.py --job_name=worker --task_index=1</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>python async_btwgraph_dist_trainer.py --job_name=worker --task_index=2</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>python async_btwgraph_dist_trainer.py --job_name=worker --task_index=3</pre></div>", "body_text": "Ok, I've double checked, and as anticipated it makes no difference.  All 4 workers train at ~1.5steps/sec.  GPU/CPU utilization is the same as before (i.e. 0-5% per GPU at any give time).  This is 33x slower for me than just using a single GPU in the non-distributed setting.\nSpecifically, I did away with async_btwgraph_launcher.py completely, and added the following to the end of async_btwgraph_dist_trainer.py:\nif __name__ == '__main__':\n    output_dir ='tfprojects/output_dir_debug'\n\n    cluster_spec ={\"ps\": [\"localhost:2227\",\n                                ],\n                \"worker\": [\n                    \"localhost:2223\",\n                    \"localhost:2224\",\n                    \"localhost:2225\",\n                    \"localhost:2226\"\n                    ]\n                }\n    cluster = tf.train.ClusterSpec(cluster_spec)\n    params = HParams(cluster=cluster,\n                     job_name = FLAGS.job_name,\n                     task_index = FLAGS.task_index)\n    if FLAGS.job_name=='worker':\n        # allow each worker to see only 1 of the 4 GPUS\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(params.task_index)\n\n    else:\n        # hide all 4 GPUS from ps\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n    train(output_dir, params)\nAs in the online example, I then run these 5 commands, each one in a separate terminal session.\npython async_btwgraph_dist_trainer.py --job_name=ps --task_index=0\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=0\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=1\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=2\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=3", "body": "Ok, I've double checked, and as anticipated it makes no difference.  All 4 workers train at ~1.5steps/sec.  GPU/CPU utilization is the same as before (i.e. 0-5% per GPU at any give time).  This is 33x slower for me than just using a single GPU in the non-distributed setting.  \r\n\r\nSpecifically, I did away with async_btwgraph_launcher.py completely, and added the following to the end of async_btwgraph_dist_trainer.py:\r\n```python\r\nif __name__ == '__main__':\r\n    output_dir ='tfprojects/output_dir_debug'\r\n\r\n    cluster_spec ={\"ps\": [\"localhost:2227\",\r\n                                ],\r\n                \"worker\": [\r\n                    \"localhost:2223\",\r\n                    \"localhost:2224\",\r\n                    \"localhost:2225\",\r\n                    \"localhost:2226\"\r\n                    ]\r\n                }\r\n    cluster = tf.train.ClusterSpec(cluster_spec)\r\n    params = HParams(cluster=cluster,\r\n                     job_name = FLAGS.job_name,\r\n                     task_index = FLAGS.task_index)\r\n    if FLAGS.job_name=='worker':\r\n        # allow each worker to see only 1 of the 4 GPUS\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(params.task_index)\r\n\r\n    else:\r\n        # hide all 4 GPUS from ps\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\r\n    train(output_dir, params)\r\n```\r\nAs in the online example, I then run these 5 commands, each one in a separate terminal session.\r\n```bash\r\npython async_btwgraph_dist_trainer.py --job_name=ps --task_index=0\r\n```\r\n\r\n```bash\r\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=0\r\n```\r\n\r\n```bash\r\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=1\r\n```\r\n\r\n```bash\r\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=2\r\n```\r\n\r\n```bash\r\npython async_btwgraph_dist_trainer.py --job_name=worker --task_index=3\r\n```"}