{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10145", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10145/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10145/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10145/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10145", "id": 230824745, "node_id": "MDU6SXNzdWUyMzA4MjQ3NDU=", "number": 10145, "title": "local distributed tensorflow async btw graph multigpu example is extremely slow", "user": {"login": "lw394", "id": 15891975, "node_id": "MDQ6VXNlcjE1ODkxOTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/15891975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lw394", "html_url": "https://github.com/lw394", "followers_url": "https://api.github.com/users/lw394/followers", "following_url": "https://api.github.com/users/lw394/following{/other_user}", "gists_url": "https://api.github.com/users/lw394/gists{/gist_id}", "starred_url": "https://api.github.com/users/lw394/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lw394/subscriptions", "organizations_url": "https://api.github.com/users/lw394/orgs", "repos_url": "https://api.github.com/users/lw394/repos", "events_url": "https://api.github.com/users/lw394/events{/privacy}", "received_events_url": "https://api.github.com/users/lw394/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-05-23T19:56:21Z", "updated_at": "2018-03-01T18:18:56Z", "closed_at": "2017-05-26T04:35:38Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Not really, this primarily a copy and paste of the distributed tensorflow example</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2 GPU</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: Titan X Pascal, 12G, 4 total</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre>python async_btwgraph_launcher.py</pre></div>\n<p>async_btwgraph_launcher.py</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> async_btwgraph_dist_trainer <span class=\"pl-k\">import</span> train\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">from</span> multiprocessing <span class=\"pl-k\">import</span> Process\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">from</span> tensorflow.contrib.training <span class=\"pl-k\">import</span> HParams\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Set up configurations to sweep</span>\noutput_dir <span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>tfprojects/output_dir_debug<span class=\"pl-pds\">'</span></span>\n\ncluster_spec <span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2227<span class=\"pl-pds\">\"</span></span>\n                      ],\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2223<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2224<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2225<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>localhost:2226<span class=\"pl-pds\">\"</span></span>\n        ]\n    }\n\ncluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec(cluster_spec)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">worker</span>(<span class=\"pl-smi\">device</span>):\n    params <span class=\"pl-k\">=</span> HParams(<span class=\"pl-v\">cluster</span><span class=\"pl-k\">=</span>cluster,\n                     <span class=\"pl-v\">job_name</span> <span class=\"pl-k\">=</span> device[<span class=\"pl-c1\">0</span>],\n                     <span class=\"pl-v\">task_index</span> <span class=\"pl-k\">=</span> device[<span class=\"pl-c1\">1</span>])\n\n    <span class=\"pl-k\">if</span> device[<span class=\"pl-c1\">0</span>]<span class=\"pl-k\">==</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> allow each worker to see only 1 of the 4 GPUS</span>\n        os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">\"</span></span>]<span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>(params.task_index)\n\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> hide all 4 GPUS from ps</span>\n        os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">\"</span></span>]<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>\n\n\n    train(output_dir, params)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    devices <span class=\"pl-k\">=</span> [[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>ps<span class=\"pl-pds\">'</span></span>,<span class=\"pl-c1\">0</span>],\n               [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>,<span class=\"pl-c1\">0</span>],\n               [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>,<span class=\"pl-c1\">1</span>],\n               [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>,<span class=\"pl-c1\">2</span>],\n               [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>,<span class=\"pl-c1\">3</span>]\n               ]\n\n    processes <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> devices:\n\n        p <span class=\"pl-k\">=</span> Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>worker, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(d,))\n        p.start()\n        processes.append(p)\n\n    <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> processes:\n        p.join()</pre></div>\n<p>async_btwgraph_dist_trainer.py</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> time\n\ntf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> enables training error print out during training</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn</span>(<span class=\"pl-smi\">features</span>,<span class=\"pl-smi\">labels</span>,<span class=\"pl-smi\">mode</span>,<span class=\"pl-smi\">params</span>):\n\n\n    outputs <span class=\"pl-k\">=</span> layers.fully_connected(\n                    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> features,\n                    <span class=\"pl-v\">num_outputs</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4096</span>)\n    outputs <span class=\"pl-k\">=</span> layers.fully_connected(\n                    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> outputs,\n                    <span class=\"pl-v\">num_outputs</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4096</span>)\n    outputs <span class=\"pl-k\">=</span> layers.fully_connected(\n                    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> outputs,\n                    <span class=\"pl-v\">num_outputs</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>)\n\n    loss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(outputs, labels)\n\n\n    train_op <span class=\"pl-k\">=</span> tf.contrib.layers.optimize_loss(\n              loss, <span class=\"pl-c1\">None</span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Adam<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">learning_rate</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">.0001</span>)\n\n\n    predictions <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>predictions<span class=\"pl-pds\">\"</span></span>:tf.identity(outputs,<span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>predictions<span class=\"pl-pds\">'</span></span>)}\n    <span class=\"pl-k\">return</span> predictions, loss, train_op\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">dumb_input_fn</span>():\n\n    x <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">256</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    y <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">256</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    <span class=\"pl-k\">return</span> [x,y]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">output_dir</span>, <span class=\"pl-smi\">params</span><span class=\"pl-k\">=</span>{}):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>***JOBNAME**:<span class=\"pl-pds\">'</span></span>,params.job_name)\n    cluster <span class=\"pl-k\">=</span> params.cluster\n    job_name <span class=\"pl-k\">=</span> params.job_name\n    task_index <span class=\"pl-k\">=</span> params.task_index\n    gpu <span class=\"pl-k\">=</span> task_index\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create and start a server for the local task.</span>\n    server <span class=\"pl-k\">=</span> tf.train.Server(cluster,\n                           <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span>job_name,\n                           <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span>task_index)\n\n    <span class=\"pl-k\">if</span> job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>:\n        server.join()\n    <span class=\"pl-k\">elif</span> job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>:\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assigns ops to the local worker by default.</span>\n        <span class=\"pl-k\">with</span> tf.device(tf.train.replica_device_setter(\n            <span class=\"pl-v\">worker_device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/job:worker/replica:0/task:<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> task_index,\n            <span class=\"pl-v\">cluster</span><span class=\"pl-k\">=</span>cluster)):\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Build model...</span>\n            x,y <span class=\"pl-k\">=</span> dumb_input_fn()\n            _, _, train_op <span class=\"pl-k\">=</span> model_fn(x,y,<span class=\"pl-c1\">None</span>,params)\n\n        global_step <span class=\"pl-k\">=</span> tf.contrib.framework.get_or_create_global_step()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> The StopAtStepHook handles stopping after running given steps.</span>\n        hooks<span class=\"pl-k\">=</span>[tf.train.StopAtStepHook(<span class=\"pl-v\">last_step</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100000</span>)]\n\n        step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        start_time <span class=\"pl-k\">=</span> time.time()\n\n        <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(<span class=\"pl-v\">master</span><span class=\"pl-k\">=</span>server.target,\n                                               <span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span>(task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>),\n                                               <span class=\"pl-v\">checkpoint_dir</span><span class=\"pl-k\">=</span>output_dir,\n                                               <span class=\"pl-v\">hooks</span><span class=\"pl-k\">=</span>hooks) <span class=\"pl-k\">as</span> mon_sess:\n            <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> mon_sess.should_stop():\n\n                _ <span class=\"pl-k\">=</span> mon_sess.run(train_op)\n\n                <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Step:<span class=\"pl-pds\">\"</span></span>, step,<span class=\"pl-c1\">10</span><span class=\"pl-k\">/</span>(time.time()<span class=\"pl-k\">-</span>start_time),<span class=\"pl-s\"><span class=\"pl-pds\">'</span>steps/sec<span class=\"pl-pds\">'</span></span>)\n                    start_time <span class=\"pl-k\">=</span> time.time()\n\n                step<span class=\"pl-k\">+=</span><span class=\"pl-c1\">1</span>\n</pre></div>\n<h3>Describe the problem</h3>\n<p>I'm trying to use the distributed tensorflow <a href=\"https://www.tensorflow.org/versions/r1.2/deploy/distributed\" rel=\"nofollow\">example</a> to do async between graph replication on a 4 Titan X machine, with 1 GPU per worker.  Without distributed TF and using a single GPU, the same code trains at ~150-200 steps/sec.  As shown at the end of the log below, this distributed trainer clocks at ~ 2 steps/sec.  The 4 GPUs are barely utilized,<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/15891975/26372649/9d8f25cc-3fcc-11e7-87f0-0be4b1a80fb6.png\"><img src=\"https://cloud.githubusercontent.com/assets/15891975/26372649/9d8f25cc-3fcc-11e7-87f0-0be4b1a80fb6.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nwith plenty of  CPU headroom,<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/15891975/26372709/c4203ca8-3fcc-11e7-804f-5daa97bc5b70.png\"><img src=\"https://cloud.githubusercontent.com/assets/15891975/26372709/c4203ca8-3fcc-11e7-804f-5daa97bc5b70.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>Also, if I simply remove the parameter server from this example, but keeping all 4 workers, they all grab GPU:0 maxing it out, and each worker process running at ~50steps/sec, and GPUs 1-3 are unused.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/15891975/26373341/28da56e0-3fcf-11e7-87a7-db64dfac13e4.png\"><img src=\"https://cloud.githubusercontent.com/assets/15891975/26373341/28da56e0-3fcf-11e7-87a7-db64dfac13e4.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nHowever, see that I'm setting os.environ[\"CUDA_VISIBLE_DEVICES\"] to enable only 1 unique GPU per worker.</p>\n<p>Is this expected behavior?<br>\nThanks,<br>\nLuke</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-shell\"><pre>python async_btwgraph_launcher.py\n<span class=\"pl-k\">***</span>JOBNAME<span class=\"pl-k\">**</span>: ps\n2017-05-23 15:06:16.761116: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.761187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.761200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.761212: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.761225: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">***JOBNAME**: worker</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.763172: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.763247: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.763259: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.763269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.763280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n<span class=\"pl-k\">***</span>JOBNAME<span class=\"pl-k\">**</span>: worker\n2017-05-23 15:06:16.765599: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.765671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.765684: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.765693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.765705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">***JOBNAME**: worker</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.767881: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.767951: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.767983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.768005: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.768024: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n<span class=\"pl-k\">***</span>JOBNAME<span class=\"pl-k\">**</span>: worker\n2017-05-23 15:06:16.771552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.771617: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.771638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.771649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-pds\">'</span></span>t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.771660: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.885876: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.885946: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: mlearn2</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.885961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: mlearn2</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.886027: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 375.39.0</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.886800: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39  Tue Jan 31 20:47:00 PST 2017</span>\n<span class=\"pl-s\">GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)</span>\n<span class=\"pl-s\">\"\"\"</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.886835: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.39.0</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.886848: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 375.39.0</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.898031: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2227}</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.898063: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223, 1 -&gt; localhost:2224, 2 -&gt; localhost:2225, 3 -&gt; localhost:2226}</span>\n<span class=\"pl-s\">2017-05-23 15:06:16.908193: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2227</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.817380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:</span>\n<span class=\"pl-s\">name: TITAN X (Pascal)</span>\n<span class=\"pl-s\">major: 6 minor: 1 memoryClockRate (GHz) 1.911</span>\n<span class=\"pl-s\">pciBusID 0000:02:00.0</span>\n<span class=\"pl-s\">Total memory: 11.90GiB</span>\n<span class=\"pl-s\">Free memory: 11.60GiB</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.817433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.817443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.817581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.859150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:</span>\n<span class=\"pl-s\">name: TITAN X (Pascal)</span>\n<span class=\"pl-s\">major: 6 minor: 1 memoryClockRate (GHz) 1.911</span>\n<span class=\"pl-s\">pciBusID 0000:03:00.0</span>\n<span class=\"pl-s\">Total memory: 11.90GiB</span>\n<span class=\"pl-s\">Free memory: 11.76GiB</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.859216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.859227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.859274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.900436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:</span>\n<span class=\"pl-s\">name: TITAN X (Pascal)</span>\n<span class=\"pl-s\">major: 6 minor: 1 memoryClockRate (GHz) 1.911</span>\n<span class=\"pl-s\">pciBusID 0000:81:00.0</span>\n<span class=\"pl-s\">Total memory: 11.90GiB</span>\n<span class=\"pl-s\">Free memory: 11.76GiB</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.900506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.900520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.900562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.922840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:</span>\n<span class=\"pl-s\">name: TITAN X (Pascal)</span>\n<span class=\"pl-s\">major: 6 minor: 1 memoryClockRate (GHz) 1.911</span>\n<span class=\"pl-s\">pciBusID 0000:82:00.0</span>\n<span class=\"pl-s\">Total memory: 11.90GiB</span>\n<span class=\"pl-s\">Free memory: 11.76GiB</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.922898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.922913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.922954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.947847: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2227}</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.947913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223, 1 -&gt; localhost:2224, 2 -&gt; localhost:2225, 3 -&gt; localhost:2226}</span>\n<span class=\"pl-s\">2017-05-23 15:06:18.954688: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.008071: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2227}</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.008132: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223, 1 -&gt; localhost:2224, 2 -&gt; localhost:2225, 3 -&gt; localhost:2226}</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.016316: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2224</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.052548: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2227}</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.052589: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223, 1 -&gt; localhost:2224, 2 -&gt; localhost:2225, 3 -&gt; localhost:2226}</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.056154: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2227}</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.056176: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223, 1 -&gt; localhost:2224, 2 -&gt; localhost:2225, 3 -&gt; localhost:2226}</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.060973: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2225</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.063039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2226</span>\n<span class=\"pl-s\">2017-05-23 15:06:19.444002: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 6bd586237b42120b with config:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">2017-05-23 15:06:19.458159: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 924ffab74f941016 with config:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">2017-05-23 15:06:19.478740: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c85138445cc12f91 with config:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">2017-05-23 15:06:19.481805: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c77091e3456c2d32 with config:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Step: 0 5.582075516520828 steps/sec</span>\n<span class=\"pl-s\">Step: 10 2.1573368439379705 steps/sec</span>\n<span class=\"pl-s\">Step: 20 2.2082990011777794 steps/sec</span>\n<span class=\"pl-s\">Step: 30 2.1863694281593564 steps/sec</span>\n<span class=\"pl-s\">Step: 40 2.254566631295363 steps/sec</span>\n<span class=\"pl-s\">Step: 50 2.2088188362675036 steps/sec</span>\n<span class=\"pl-s\">Step: 60 2.163473831162752 steps/sec</span>\n<span class=\"pl-s\">2017-05-23 15:06:49.920556: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 617cf268cb5a24e3 with config:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">2017-05-23 15:06:49.952686: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 9b89c26b8c702b9f with config:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">2017-05-23 15:06:49.955667: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session b8d674f54212032f with config:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Step: 0 0.30616911528363916 steps/sec</span>\n<span class=\"pl-s\">Step: 0 0.30273764853875 steps/sec</span>\n<span class=\"pl-s\">Step: 0 0.3023278973613949 steps/sec</span>\n<span class=\"pl-s\">Step: 70 1.7908409267412564 steps/sec</span>\n<span class=\"pl-s\">Step: 10 1.4149145268366454 steps/sec</span>\n<span class=\"pl-s\">Step: 10 1.4475748080324984 steps/sec</span>\n<span class=\"pl-s\">Step: 10 1.3904163169606496 steps/sec</span>\n<span class=\"pl-s\">Step: 80 1.486386761414297 steps/sec</span>\n<span class=\"pl-s\">Step: 20 1.4357212501056003 steps/sec</span>\n<span class=\"pl-s\">Step: 20 1.4629177065889272 steps/sec</span>\n<span class=\"pl-s\">Step: 20 1.4312276702523932 steps/sec</span>\n<span class=\"pl-s\">Step: 90 1.4173087487398812 steps/sec</span></pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really, this primarily a copy and paste of the distributed tensorflow example\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.2 GPU\nBazel version (if compiling from source):\nCUDA/cuDNN version: 8.0\nGPU model and memory: Titan X Pascal, 12G, 4 total\nExact command to reproduce:\n\npython async_btwgraph_launcher.py\nasync_btwgraph_launcher.py\nfrom async_btwgraph_dist_trainer import train\nimport os\nfrom multiprocessing import Process\nimport time\nfrom tensorflow.contrib.training import HParams\nimport tensorflow as tf\n# Set up configurations to sweep\noutput_dir ='tfprojects/output_dir_debug'\n\ncluster_spec ={\"ps\": [\"localhost:2227\"\n                      ],\n    \"worker\": [\n        \"localhost:2223\",\n        \"localhost:2224\",\n        \"localhost:2225\",\n        \"localhost:2226\"\n        ]\n    }\n\ncluster = tf.train.ClusterSpec(cluster_spec)\ndef worker(device):\n    params = HParams(cluster=cluster,\n                     job_name = device[0],\n                     task_index = device[1])\n\n    if device[0]=='worker':\n        # allow each worker to see only 1 of the 4 GPUS\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(params.task_index)\n\n    else:\n        # hide all 4 GPUS from ps\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n\n\n    train(output_dir, params)\n\nif __name__ == '__main__':\n    devices = [['ps',0],\n               ['worker',0],\n               ['worker',1],\n               ['worker',2],\n               ['worker',3]\n               ]\n\n    processes = []\n    for d in devices:\n\n        p = Process(target=worker, args=(d,))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\nasync_btwgraph_dist_trainer.py\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport time\n\ntf.logging.set_verbosity(tf.logging.INFO)  # enables training error print out during training\n\ndef model_fn(features,labels,mode,params):\n\n\n    outputs = layers.fully_connected(\n                    inputs = features,\n                    num_outputs = 4096)\n    outputs = layers.fully_connected(\n                    inputs = outputs,\n                    num_outputs = 4096)\n    outputs = layers.fully_connected(\n                    inputs = outputs,\n                    num_outputs = 256)\n\n    loss = tf.losses.mean_squared_error(outputs, labels)\n\n\n    train_op = tf.contrib.layers.optimize_loss(\n              loss, None, optimizer='Adam',\n                        learning_rate = .0001)\n\n\n    predictions = {\"predictions\":tf.identity(outputs,name = 'predictions')}\n    return predictions, loss, train_op\n\n\ndef dumb_input_fn():\n\n    x = tf.random_normal([128,256], dtype=tf.float32)\n    y = tf.random_normal([128,256], dtype=tf.float32)\n\n    return [x,y]\n\n#\ndef train(output_dir, params={}):\n    print('***JOBNAME**:',params.job_name)\n    cluster = params.cluster\n    job_name = params.job_name\n    task_index = params.task_index\n    gpu = task_index\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                           job_name=job_name,\n                           task_index=task_index)\n\n    if job_name == \"ps\":\n        server.join()\n    elif job_name == \"worker\":\n\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/replica:0/task:%d\" % task_index,\n            cluster=cluster)):\n\n            # Build model...\n            x,y = dumb_input_fn()\n            _, _, train_op = model_fn(x,y,None,params)\n\n        global_step = tf.contrib.framework.get_or_create_global_step()\n        # The StopAtStepHook handles stopping after running given steps.\n        hooks=[tf.train.StopAtStepHook(last_step=100000)]\n\n        step = 0\n        start_time = time.time()\n\n        with tf.train.MonitoredTrainingSession(master=server.target,\n                                               is_chief=(task_index == 0),\n                                               checkpoint_dir=output_dir,\n                                               hooks=hooks) as mon_sess:\n            while not mon_sess.should_stop():\n\n                _ = mon_sess.run(train_op)\n\n                if step % 10 == 0:\n                    print(\"Step:\", step,10/(time.time()-start_time),'steps/sec')\n                    start_time = time.time()\n\n                step+=1\n\nDescribe the problem\nI'm trying to use the distributed tensorflow example to do async between graph replication on a 4 Titan X machine, with 1 GPU per worker.  Without distributed TF and using a single GPU, the same code trains at ~150-200 steps/sec.  As shown at the end of the log below, this distributed trainer clocks at ~ 2 steps/sec.  The 4 GPUs are barely utilized,\n\nwith plenty of  CPU headroom,\n\nAlso, if I simply remove the parameter server from this example, but keeping all 4 workers, they all grab GPU:0 maxing it out, and each worker process running at ~50steps/sec, and GPUs 1-3 are unused.\n\nHowever, see that I'm setting os.environ[\"CUDA_VISIBLE_DEVICES\"] to enable only 1 unique GPU per worker.\nIs this expected behavior?\nThanks,\nLuke\nSource code / logs\npython async_btwgraph_launcher.py\n***JOBNAME**: ps\n2017-05-23 15:06:16.761116: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.761187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.761200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.761212: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.761225: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n***JOBNAME**: worker\n2017-05-23 15:06:16.763172: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.763247: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.763259: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.763269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.763280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n***JOBNAME**: worker\n2017-05-23 15:06:16.765599: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.765671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.765684: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.765693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.765705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n***JOBNAME**: worker\n2017-05-23 15:06:16.767881: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.767951: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.767983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.768005: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.768024: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n***JOBNAME**: worker\n2017-05-23 15:06:16.771552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.771617: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.771638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.771649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.771660: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-23 15:06:16.885876: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n2017-05-23 15:06:16.885946: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: mlearn2\n2017-05-23 15:06:16.885961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: mlearn2\n2017-05-23 15:06:16.886027: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 375.39.0\n2017-05-23 15:06:16.886800: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39  Tue Jan 31 20:47:00 PST 2017\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)\n\"\"\"\n2017-05-23 15:06:16.886835: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.39.0\n2017-05-23 15:06:16.886848: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 375.39.0\n2017-05-23 15:06:16.898031: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\n2017-05-23 15:06:16.898063: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\n2017-05-23 15:06:16.908193: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2227\n2017-05-23 15:06:18.817380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\npciBusID 0000:02:00.0\nTotal memory: 11.90GiB\nFree memory: 11.60GiB\n2017-05-23 15:06:18.817433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\n2017-05-23 15:06:18.817443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\n2017-05-23 15:06:18.817581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\n2017-05-23 15:06:18.859150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\npciBusID 0000:03:00.0\nTotal memory: 11.90GiB\nFree memory: 11.76GiB\n2017-05-23 15:06:18.859216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\n2017-05-23 15:06:18.859227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\n2017-05-23 15:06:18.859274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)\n2017-05-23 15:06:18.900436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\npciBusID 0000:81:00.0\nTotal memory: 11.90GiB\nFree memory: 11.76GiB\n2017-05-23 15:06:18.900506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\n2017-05-23 15:06:18.900520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\n2017-05-23 15:06:18.900562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)\n2017-05-23 15:06:18.922840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\npciBusID 0000:82:00.0\nTotal memory: 11.90GiB\nFree memory: 11.76GiB\n2017-05-23 15:06:18.922898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\n2017-05-23 15:06:18.922913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\n2017-05-23 15:06:18.922954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)\n2017-05-23 15:06:18.947847: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\n2017-05-23 15:06:18.947913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\n2017-05-23 15:06:18.954688: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223\n2017-05-23 15:06:19.008071: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\n2017-05-23 15:06:19.008132: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\n2017-05-23 15:06:19.016316: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2224\n2017-05-23 15:06:19.052548: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\n2017-05-23 15:06:19.052589: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\n2017-05-23 15:06:19.056154: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\n2017-05-23 15:06:19.056176: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\n2017-05-23 15:06:19.060973: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2225\n2017-05-23 15:06:19.063039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2226\n2017-05-23 15:06:19.444002: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 6bd586237b42120b with config:\n\n2017-05-23 15:06:19.458159: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 924ffab74f941016 with config:\n\n2017-05-23 15:06:19.478740: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c85138445cc12f91 with config:\n\n2017-05-23 15:06:19.481805: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c77091e3456c2d32 with config:\n\nStep: 0 5.582075516520828 steps/sec\nStep: 10 2.1573368439379705 steps/sec\nStep: 20 2.2082990011777794 steps/sec\nStep: 30 2.1863694281593564 steps/sec\nStep: 40 2.254566631295363 steps/sec\nStep: 50 2.2088188362675036 steps/sec\nStep: 60 2.163473831162752 steps/sec\n2017-05-23 15:06:49.920556: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 617cf268cb5a24e3 with config:\n\n2017-05-23 15:06:49.952686: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 9b89c26b8c702b9f with config:\n\n2017-05-23 15:06:49.955667: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session b8d674f54212032f with config:\n\nStep: 0 0.30616911528363916 steps/sec\nStep: 0 0.30273764853875 steps/sec\nStep: 0 0.3023278973613949 steps/sec\nStep: 70 1.7908409267412564 steps/sec\nStep: 10 1.4149145268366454 steps/sec\nStep: 10 1.4475748080324984 steps/sec\nStep: 10 1.3904163169606496 steps/sec\nStep: 80 1.486386761414297 steps/sec\nStep: 20 1.4357212501056003 steps/sec\nStep: 20 1.4629177065889272 steps/sec\nStep: 20 1.4312276702523932 steps/sec\nStep: 90 1.4173087487398812 steps/sec", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not really, this primarily a copy and paste of the distributed tensorflow example\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2 GPU\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Titan X Pascal, 12G, 4 total\r\n- **Exact command to reproduce**:\r\n\r\n\r\n```bash\r\npython async_btwgraph_launcher.py\r\n```\r\nasync_btwgraph_launcher.py\r\n```python\r\nfrom async_btwgraph_dist_trainer import train\r\nimport os\r\nfrom multiprocessing import Process\r\nimport time\r\nfrom tensorflow.contrib.training import HParams\r\nimport tensorflow as tf\r\n# Set up configurations to sweep\r\noutput_dir ='tfprojects/output_dir_debug'\r\n\r\ncluster_spec ={\"ps\": [\"localhost:2227\"\r\n                      ],\r\n    \"worker\": [\r\n        \"localhost:2223\",\r\n        \"localhost:2224\",\r\n        \"localhost:2225\",\r\n        \"localhost:2226\"\r\n        ]\r\n    }\r\n\r\ncluster = tf.train.ClusterSpec(cluster_spec)\r\ndef worker(device):\r\n    params = HParams(cluster=cluster,\r\n                     job_name = device[0],\r\n                     task_index = device[1])\r\n\r\n    if device[0]=='worker':\r\n        # allow each worker to see only 1 of the 4 GPUS\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(params.task_index)\r\n\r\n    else:\r\n        # hide all 4 GPUS from ps\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\r\n\r\n\r\n    train(output_dir, params)\r\n\r\nif __name__ == '__main__':\r\n    devices = [['ps',0],\r\n               ['worker',0],\r\n               ['worker',1],\r\n               ['worker',2],\r\n               ['worker',3]\r\n               ]\r\n\r\n    processes = []\r\n    for d in devices:\r\n\r\n        p = Process(target=worker, args=(d,))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\r\nasync_btwgraph_dist_trainer.py\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)  # enables training error print out during training\r\n\r\ndef model_fn(features,labels,mode,params):\r\n\r\n\r\n    outputs = layers.fully_connected(\r\n                    inputs = features,\r\n                    num_outputs = 4096)\r\n    outputs = layers.fully_connected(\r\n                    inputs = outputs,\r\n                    num_outputs = 4096)\r\n    outputs = layers.fully_connected(\r\n                    inputs = outputs,\r\n                    num_outputs = 256)\r\n\r\n    loss = tf.losses.mean_squared_error(outputs, labels)\r\n\r\n\r\n    train_op = tf.contrib.layers.optimize_loss(\r\n              loss, None, optimizer='Adam',\r\n                        learning_rate = .0001)\r\n\r\n\r\n    predictions = {\"predictions\":tf.identity(outputs,name = 'predictions')}\r\n    return predictions, loss, train_op\r\n\r\n\r\ndef dumb_input_fn():\r\n\r\n    x = tf.random_normal([128,256], dtype=tf.float32)\r\n    y = tf.random_normal([128,256], dtype=tf.float32)\r\n\r\n    return [x,y]\r\n\r\n#\r\ndef train(output_dir, params={}):\r\n    print('***JOBNAME**:',params.job_name)\r\n    cluster = params.cluster\r\n    job_name = params.job_name\r\n    task_index = params.task_index\r\n    gpu = task_index\r\n    # Create and start a server for the local task.\r\n    server = tf.train.Server(cluster,\r\n                           job_name=job_name,\r\n                           task_index=task_index)\r\n\r\n    if job_name == \"ps\":\r\n        server.join()\r\n    elif job_name == \"worker\":\r\n\r\n        # Assigns ops to the local worker by default.\r\n        with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/replica:0/task:%d\" % task_index,\r\n            cluster=cluster)):\r\n\r\n            # Build model...\r\n            x,y = dumb_input_fn()\r\n            _, _, train_op = model_fn(x,y,None,params)\r\n\r\n        global_step = tf.contrib.framework.get_or_create_global_step()\r\n        # The StopAtStepHook handles stopping after running given steps.\r\n        hooks=[tf.train.StopAtStepHook(last_step=100000)]\r\n\r\n        step = 0\r\n        start_time = time.time()\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=(task_index == 0),\r\n                                               checkpoint_dir=output_dir,\r\n                                               hooks=hooks) as mon_sess:\r\n            while not mon_sess.should_stop():\r\n\r\n                _ = mon_sess.run(train_op)\r\n\r\n                if step % 10 == 0:\r\n                    print(\"Step:\", step,10/(time.time()-start_time),'steps/sec')\r\n                    start_time = time.time()\r\n\r\n                step+=1\r\n\r\n```\r\n### Describe the problem\r\nI'm trying to use the distributed tensorflow [example](https://www.tensorflow.org/versions/r1.2/deploy/distributed) to do async between graph replication on a 4 Titan X machine, with 1 GPU per worker.  Without distributed TF and using a single GPU, the same code trains at ~150-200 steps/sec.  As shown at the end of the log below, this distributed trainer clocks at ~ 2 steps/sec.  The 4 GPUs are barely utilized,\r\n![image](https://cloud.githubusercontent.com/assets/15891975/26372649/9d8f25cc-3fcc-11e7-87f0-0be4b1a80fb6.png)\r\nwith plenty of  CPU headroom,\r\n![image](https://cloud.githubusercontent.com/assets/15891975/26372709/c4203ca8-3fcc-11e7-804f-5daa97bc5b70.png)\r\n\r\nAlso, if I simply remove the parameter server from this example, but keeping all 4 workers, they all grab GPU:0 maxing it out, and each worker process running at ~50steps/sec, and GPUs 1-3 are unused.\r\n![image](https://cloud.githubusercontent.com/assets/15891975/26373341/28da56e0-3fcf-11e7-87a7-db64dfac13e4.png)\r\nHowever, see that I'm setting os.environ[\"CUDA_VISIBLE_DEVICES\"] to enable only 1 unique GPU per worker.  \r\n\r\nIs this expected behavior?\r\nThanks,\r\nLuke \r\n\r\n### Source code / logs\r\n```bash\r\npython async_btwgraph_launcher.py\r\n***JOBNAME**: ps\r\n2017-05-23 15:06:16.761116: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.761187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.761200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.761212: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.761225: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n***JOBNAME**: worker\r\n2017-05-23 15:06:16.763172: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.763247: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.763259: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.763269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.763280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n***JOBNAME**: worker\r\n2017-05-23 15:06:16.765599: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.765671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.765684: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.765693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.765705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n***JOBNAME**: worker\r\n2017-05-23 15:06:16.767881: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.767951: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.767983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.768005: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.768024: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n***JOBNAME**: worker\r\n2017-05-23 15:06:16.771552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.771617: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.771638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.771649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.771660: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-23 15:06:16.885876: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n2017-05-23 15:06:16.885946: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: mlearn2\r\n2017-05-23 15:06:16.885961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: mlearn2\r\n2017-05-23 15:06:16.886027: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 375.39.0\r\n2017-05-23 15:06:16.886800: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39  Tue Jan 31 20:47:00 PST 2017\r\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)\r\n\"\"\"\r\n2017-05-23 15:06:16.886835: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.39.0\r\n2017-05-23 15:06:16.886848: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 375.39.0\r\n2017-05-23 15:06:16.898031: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\r\n2017-05-23 15:06:16.898063: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\r\n2017-05-23 15:06:16.908193: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2227\r\n2017-05-23 15:06:18.817380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.60GiB\r\n2017-05-23 15:06:18.817433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\r\n2017-05-23 15:06:18.817443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\r\n2017-05-23 15:06:18.817581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\r\n2017-05-23 15:06:18.859150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\r\npciBusID 0000:03:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\n2017-05-23 15:06:18.859216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\r\n2017-05-23 15:06:18.859227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\r\n2017-05-23 15:06:18.859274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)\r\n2017-05-23 15:06:18.900436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\r\npciBusID 0000:81:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\n2017-05-23 15:06:18.900506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\r\n2017-05-23 15:06:18.900520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\r\n2017-05-23 15:06:18.900562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)\r\n2017-05-23 15:06:18.922840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Found device 0 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.911\r\npciBusID 0000:82:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\n2017-05-23 15:06:18.922898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:927] DMA: 0\r\n2017-05-23 15:06:18.922913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:937] 0:   Y\r\n2017-05-23 15:06:18.922954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)\r\n2017-05-23 15:06:18.947847: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\r\n2017-05-23 15:06:18.947913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\r\n2017-05-23 15:06:18.954688: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223\r\n2017-05-23 15:06:19.008071: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\r\n2017-05-23 15:06:19.008132: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\r\n2017-05-23 15:06:19.016316: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2224\r\n2017-05-23 15:06:19.052548: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\r\n2017-05-23 15:06:19.052589: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\r\n2017-05-23 15:06:19.056154: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2227}\r\n2017-05-23 15:06:19.056176: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225, 3 -> localhost:2226}\r\n2017-05-23 15:06:19.060973: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2225\r\n2017-05-23 15:06:19.063039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2226\r\n2017-05-23 15:06:19.444002: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 6bd586237b42120b with config:\r\n\r\n2017-05-23 15:06:19.458159: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 924ffab74f941016 with config:\r\n\r\n2017-05-23 15:06:19.478740: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c85138445cc12f91 with config:\r\n\r\n2017-05-23 15:06:19.481805: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c77091e3456c2d32 with config:\r\n\r\nStep: 0 5.582075516520828 steps/sec\r\nStep: 10 2.1573368439379705 steps/sec\r\nStep: 20 2.2082990011777794 steps/sec\r\nStep: 30 2.1863694281593564 steps/sec\r\nStep: 40 2.254566631295363 steps/sec\r\nStep: 50 2.2088188362675036 steps/sec\r\nStep: 60 2.163473831162752 steps/sec\r\n2017-05-23 15:06:49.920556: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 617cf268cb5a24e3 with config:\r\n\r\n2017-05-23 15:06:49.952686: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 9b89c26b8c702b9f with config:\r\n\r\n2017-05-23 15:06:49.955667: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session b8d674f54212032f with config:\r\n\r\nStep: 0 0.30616911528363916 steps/sec\r\nStep: 0 0.30273764853875 steps/sec\r\nStep: 0 0.3023278973613949 steps/sec\r\nStep: 70 1.7908409267412564 steps/sec\r\nStep: 10 1.4149145268366454 steps/sec\r\nStep: 10 1.4475748080324984 steps/sec\r\nStep: 10 1.3904163169606496 steps/sec\r\nStep: 80 1.486386761414297 steps/sec\r\nStep: 20 1.4357212501056003 steps/sec\r\nStep: 20 1.4629177065889272 steps/sec\r\nStep: 20 1.4312276702523932 steps/sec\r\nStep: 90 1.4173087487398812 steps/sec\r\n```"}