{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280080496", "html_url": "https://github.com/tensorflow/tensorflow/issues/492#issuecomment-280080496", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492", "id": 280080496, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDA4MDQ5Ng==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-15T17:34:52Z", "updated_at": "2017-02-15T17:34:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10166968\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kvrd18\">@kvrd18</a>  BTW, for tracking down out of memory errors, here are two tools to make it easier:</p>\n<ol>\n<li>\n<p><a href=\"https://github.com/yaroslavvb/memory_probe_ops\">https://github.com/yaroslavvb/memory_probe_ops</a> -- it's a tensorflow op you can insert somewhere in the graph and evaluate in <code>sess.run</code> to get the amount of memory allocated at that point in time</p>\n</li>\n<li>\n<p><a href=\"https://github.com/yaroslavvb/memory_util\">https://github.com/yaroslavvb/memory_util</a> -- this tool lets you see timeline of all tensor allocations and deallocations.</p>\n</li>\n</ol>\n<p>After looking at out of memory situations from various large networks (densenet, pixelnet), I found no \"low-hanging fruit\" aside from the \"linearize\" tool mentioned above. IE, TensorFlow only allocates memory for tensors that are required for computation, and this memory is released as soon as the tensor has been consumed.</p>\n<p>Out of memory situations tend to occur while computing gradients. If you have a computation with \"k\" operations in a sequence, each operation producing B bytes, then you need to save their outputs in order to compute backprop. This means k*B memory in peak. This is different from forward prop where older tensors can be discarded. So for instance, a network with 100 operations, with each processing 1GB of data only needs 2GB to run forward prop, but 100GB to do backprop.</p>\n<p>There are some \"high-hanging fruit\" I'm looking on atm, in particular, such as discarding some parts of computation, and then recomputing it. One way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example -- <a href=\"https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb\">https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb</a></p>", "body_text": "@kvrd18  BTW, for tracking down out of memory errors, here are two tools to make it easier:\n\n\nhttps://github.com/yaroslavvb/memory_probe_ops -- it's a tensorflow op you can insert somewhere in the graph and evaluate in sess.run to get the amount of memory allocated at that point in time\n\n\nhttps://github.com/yaroslavvb/memory_util -- this tool lets you see timeline of all tensor allocations and deallocations.\n\n\nAfter looking at out of memory situations from various large networks (densenet, pixelnet), I found no \"low-hanging fruit\" aside from the \"linearize\" tool mentioned above. IE, TensorFlow only allocates memory for tensors that are required for computation, and this memory is released as soon as the tensor has been consumed.\nOut of memory situations tend to occur while computing gradients. If you have a computation with \"k\" operations in a sequence, each operation producing B bytes, then you need to save their outputs in order to compute backprop. This means k*B memory in peak. This is different from forward prop where older tensors can be discarded. So for instance, a network with 100 operations, with each processing 1GB of data only needs 2GB to run forward prop, but 100GB to do backprop.\nThere are some \"high-hanging fruit\" I'm looking on atm, in particular, such as discarding some parts of computation, and then recomputing it. One way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example -- https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb", "body": "@kvrd18  BTW, for tracking down out of memory errors, here are two tools to make it easier:\r\n\r\n1. https://github.com/yaroslavvb/memory_probe_ops -- it's a tensorflow op you can insert somewhere in the graph and evaluate in `sess.run` to get the amount of memory allocated at that point in time\r\n\r\n2. https://github.com/yaroslavvb/memory_util -- this tool lets you see timeline of all tensor allocations and deallocations.\r\n\r\nAfter looking at out of memory situations from various large networks (densenet, pixelnet), I found no \"low-hanging fruit\" aside from the \"linearize\" tool mentioned above. IE, TensorFlow only allocates memory for tensors that are required for computation, and this memory is released as soon as the tensor has been consumed.\r\n\r\nOut of memory situations tend to occur while computing gradients. If you have a computation with \"k\" operations in a sequence, each operation producing B bytes, then you need to save their outputs in order to compute backprop. This means k*B memory in peak. This is different from forward prop where older tensors can be discarded. So for instance, a network with 100 operations, with each processing 1GB of data only needs 2GB to run forward prop, but 100GB to do backprop.\r\n\r\nThere are some \"high-hanging fruit\" I'm looking on atm, in particular, such as discarding some parts of computation, and then recomputing it. One way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example -- https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb\r\n"}