{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267775897", "html_url": "https://github.com/tensorflow/tensorflow/issues/492#issuecomment-267775897", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492", "id": 267775897, "node_id": "MDEyOklzc3VlQ29tbWVudDI2Nzc3NTg5Nw==", "user": {"login": "ashtawy", "id": 3431058, "node_id": "MDQ6VXNlcjM0MzEwNTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/3431058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashtawy", "html_url": "https://github.com/ashtawy", "followers_url": "https://api.github.com/users/ashtawy/followers", "following_url": "https://api.github.com/users/ashtawy/following{/other_user}", "gists_url": "https://api.github.com/users/ashtawy/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashtawy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashtawy/subscriptions", "organizations_url": "https://api.github.com/users/ashtawy/orgs", "repos_url": "https://api.github.com/users/ashtawy/repos", "events_url": "https://api.github.com/users/ashtawy/events{/privacy}", "received_events_url": "https://api.github.com/users/ashtawy/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-17T17:40:53Z", "updated_at": "2016-12-17T17:40:53Z", "author_association": "NONE", "body_html": "<p>I had a similar problem and solved it by releasing the GPU memory utilized by another process. Use the command nvidia-smi to check the GPU's Memory-Usage. If it is not 0MiB, kill the process that's allocating the GPU's memory even if that process is setting idle. Then run the script that trains your TF model.</p>\n<p>In many cases, when a process (your TF training program for example) does not end normally, say due to an error, it holds on to the memory allocated for it until the program is manually killed ($kill -9 PID).</p>", "body_text": "I had a similar problem and solved it by releasing the GPU memory utilized by another process. Use the command nvidia-smi to check the GPU's Memory-Usage. If it is not 0MiB, kill the process that's allocating the GPU's memory even if that process is setting idle. Then run the script that trains your TF model.\nIn many cases, when a process (your TF training program for example) does not end normally, say due to an error, it holds on to the memory allocated for it until the program is manually killed ($kill -9 PID).", "body": "I had a similar problem and solved it by releasing the GPU memory utilized by another process. Use the command nvidia-smi to check the GPU's Memory-Usage. If it is not 0MiB, kill the process that's allocating the GPU's memory even if that process is setting idle. Then run the script that trains your TF model.\r\n\r\nIn many cases, when a process (your TF training program for example) does not end normally, say due to an error, it holds on to the memory allocated for it until the program is manually killed ($kill -9 PID)."}