{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285671164", "html_url": "https://github.com/tensorflow/tensorflow/issues/492#issuecomment-285671164", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492", "id": 285671164, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTY3MTE2NA==", "user": {"login": "ericloveland", "id": 26264595, "node_id": "MDQ6VXNlcjI2MjY0NTk1", "avatar_url": "https://avatars2.githubusercontent.com/u/26264595?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericloveland", "html_url": "https://github.com/ericloveland", "followers_url": "https://api.github.com/users/ericloveland/followers", "following_url": "https://api.github.com/users/ericloveland/following{/other_user}", "gists_url": "https://api.github.com/users/ericloveland/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericloveland/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericloveland/subscriptions", "organizations_url": "https://api.github.com/users/ericloveland/orgs", "repos_url": "https://api.github.com/users/ericloveland/repos", "events_url": "https://api.github.com/users/ericloveland/events{/privacy}", "received_events_url": "https://api.github.com/users/ericloveland/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-10T13:40:35Z", "updated_at": "2017-03-10T13:40:35Z", "author_association": "NONE", "body_html": "<p>I think it was an architecture issue... I chose some different params (smaller nmaps, and more layers) and tensorflow happily ran even when it had to start using the swap file.  It appears that nmap size is limited by the memory size of the GPU, while the number of layers is limited by the amount of CPU memory... other things like (batch size, and sample size) being the same.  The seemingly sporadic running out of memory appears to be due to when the model runs up on a larger sample, it sometimes needs to create a larger RNN model/cell under the covers... I think. Thanks!</p>", "body_text": "I think it was an architecture issue... I chose some different params (smaller nmaps, and more layers) and tensorflow happily ran even when it had to start using the swap file.  It appears that nmap size is limited by the memory size of the GPU, while the number of layers is limited by the amount of CPU memory... other things like (batch size, and sample size) being the same.  The seemingly sporadic running out of memory appears to be due to when the model runs up on a larger sample, it sometimes needs to create a larger RNN model/cell under the covers... I think. Thanks!", "body": "I think it was an architecture issue... I chose some different params (smaller nmaps, and more layers) and tensorflow happily ran even when it had to start using the swap file.  It appears that nmap size is limited by the memory size of the GPU, while the number of layers is limited by the amount of CPU memory... other things like (batch size, and sample size) being the same.  The seemingly sporadic running out of memory appears to be due to when the model runs up on a larger sample, it sometimes needs to create a larger RNN model/cell under the covers... I think. Thanks!"}