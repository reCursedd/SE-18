{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280089482", "html_url": "https://github.com/tensorflow/tensorflow/issues/492#issuecomment-280089482", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492", "id": 280089482, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDA4OTQ4Mg==", "user": {"login": "asanakoy", "id": 1690869, "node_id": "MDQ6VXNlcjE2OTA4Njk=", "avatar_url": "https://avatars0.githubusercontent.com/u/1690869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asanakoy", "html_url": "https://github.com/asanakoy", "followers_url": "https://api.github.com/users/asanakoy/followers", "following_url": "https://api.github.com/users/asanakoy/following{/other_user}", "gists_url": "https://api.github.com/users/asanakoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/asanakoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asanakoy/subscriptions", "organizations_url": "https://api.github.com/users/asanakoy/orgs", "repos_url": "https://api.github.com/users/asanakoy/repos", "events_url": "https://api.github.com/users/asanakoy/events{/privacy}", "received_events_url": "https://api.github.com/users/asanakoy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-15T18:05:39Z", "updated_at": "2017-02-15T18:05:39Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>, Thanks for great tips!</p>\n<blockquote>\n<p>One way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example</p>\n</blockquote>\n<p>Thats a bit counterintuitive. How is it controlled? Why does TF discard intermediate values inside the function (despite all intermediate ops will be added to the graph anyhow)?</p>", "body_text": "@yaroslavvb, Thanks for great tips!\n\nOne way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example\n\nThats a bit counterintuitive. How is it controlled? Why does TF discard intermediate values inside the function (despite all intermediate ops will be added to the graph anyhow)?", "body": "@yaroslavvb, Thanks for great tips!\r\n> One way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example\r\n\r\nThats a bit counterintuitive. How is it controlled? Why does TF discard intermediate values inside the function (despite all intermediate ops will be added to the graph anyhow)?"}