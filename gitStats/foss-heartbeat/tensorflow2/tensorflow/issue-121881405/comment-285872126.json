{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285872126", "html_url": "https://github.com/tensorflow/tensorflow/issues/492#issuecomment-285872126", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492", "id": 285872126, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTg3MjEyNg==", "user": {"login": "yaroslavvb2", "id": 25833812, "node_id": "MDQ6VXNlcjI1ODMzODEy", "avatar_url": "https://avatars1.githubusercontent.com/u/25833812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb2", "html_url": "https://github.com/yaroslavvb2", "followers_url": "https://api.github.com/users/yaroslavvb2/followers", "following_url": "https://api.github.com/users/yaroslavvb2/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb2/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb2/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb2/orgs", "repos_url": "https://api.github.com/users/yaroslavvb2/repos", "events_url": "https://api.github.com/users/yaroslavvb2/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb2/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-11T15:04:42Z", "updated_at": "2017-03-11T15:04:42Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Tensorflow uses its own caching allocator, so nvidia-smi gives incorrect\nresult</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Mar 11, 2017 5:35 AM, \"Alexander Botev\" ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> &lt;<a href=\"https://github.com/yaroslavvb\">https://github.com/yaroslavvb</a>&gt; It seems that the\n memory_stats_op.MaxBytesInUse() is wrong. I'm running a single tensorflow\n instance and that being the only thing running on that GPU. Under\n nvidia-smi there are 8459MB memory used on the tensorflow, the op returns\n 4254MB. This seems criminally like the tensorflow op is returning exactly\n half the memory actually being used. I'm using it like this:\n\n memory_stats_ops = tf.load_op_library(memory_stats_ops_loc)\n stats_op = memory_stats_ops.max_bytes_in_use()\n memory = session.run((train, cost, stats_op), feed_dict=feed_dict)[2] // (1024 * 1024)\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"121881405\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/492\" href=\"https://github.com/tensorflow/tensorflow/issues/492#issuecomment-285866983\">#492 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AYoxVJzBid9_o5mfSczGZtDl8QXLjyfZks5rkqMYgaJpZM4G0MRi\">https://github.com/notifications/unsubscribe-auth/AYoxVJzBid9_o5mfSczGZtDl8QXLjyfZks5rkqMYgaJpZM4G0MRi</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Tensorflow uses its own caching allocator, so nvidia-smi gives incorrect\nresult\n\u2026\nOn Mar 11, 2017 5:35 AM, \"Alexander Botev\" ***@***.***> wrote:\n @yaroslavvb <https://github.com/yaroslavvb> It seems that the\n memory_stats_op.MaxBytesInUse() is wrong. I'm running a single tensorflow\n instance and that being the only thing running on that GPU. Under\n nvidia-smi there are 8459MB memory used on the tensorflow, the op returns\n 4254MB. This seems criminally like the tensorflow op is returning exactly\n half the memory actually being used. I'm using it like this:\n\n memory_stats_ops = tf.load_op_library(memory_stats_ops_loc)\n stats_op = memory_stats_ops.max_bytes_in_use()\n memory = session.run((train, cost, stats_op), feed_dict=feed_dict)[2] // (1024 * 1024)\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n <#492 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AYoxVJzBid9_o5mfSczGZtDl8QXLjyfZks5rkqMYgaJpZM4G0MRi>\n .", "body": "Tensorflow uses its own caching allocator, so nvidia-smi gives incorrect\nresult\n\nOn Mar 11, 2017 5:35 AM, \"Alexander Botev\" <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> It seems that the\n> memory_stats_op.MaxBytesInUse() is wrong. I'm running a single tensorflow\n> instance and that being the only thing running on that GPU. Under\n> nvidia-smi there are 8459MB memory used on the tensorflow, the op returns\n> 4254MB. This seems criminally like the tensorflow op is returning exactly\n> half the memory actually being used. I'm using it like this:\n>\n> memory_stats_ops = tf.load_op_library(memory_stats_ops_loc)\n> stats_op = memory_stats_ops.max_bytes_in_use()\n> memory = session.run((train, cost, stats_op), feed_dict=feed_dict)[2] // (1024 * 1024)\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/492#issuecomment-285866983>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYoxVJzBid9_o5mfSczGZtDl8QXLjyfZks5rkqMYgaJpZM4G0MRi>\n> .\n>\n"}