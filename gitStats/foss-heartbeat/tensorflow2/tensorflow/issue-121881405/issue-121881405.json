{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/492/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/492", "id": 121881405, "node_id": "MDU6SXNzdWUxMjE4ODE0MDU=", "number": 492, "title": "memory issues", "user": {"login": "zcyang", "id": 5890073, "node_id": "MDQ6VXNlcjU4OTAwNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5890073?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zcyang", "html_url": "https://github.com/zcyang", "followers_url": "https://api.github.com/users/zcyang/followers", "following_url": "https://api.github.com/users/zcyang/following{/other_user}", "gists_url": "https://api.github.com/users/zcyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/zcyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zcyang/subscriptions", "organizations_url": "https://api.github.com/users/zcyang/orgs", "repos_url": "https://api.github.com/users/zcyang/repos", "events_url": "https://api.github.com/users/zcyang/events{/privacy}", "received_events_url": "https://api.github.com/users/zcyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 55, "created_at": "2015-12-12T21:17:38Z", "updated_at": "2018-05-09T11:37:55Z", "closed_at": "2016-02-19T08:05:41Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>It seems the memory allocation of tensorflow is rather inefficient. I have been running a single layer rnn with 256 batch size, 124 length and dim of 512, it constantly gets memory not enough error for my 4GB 980. In theory, the model size is much less than 1GB</p>\n<p>no matter how large the batch size I set, it always use up all the 4GB memory, which is unreasonable. I have been compiling tensorflow from the source and BFC memory allocator is set as default.</p>\n<p>I think the memory problem was also mentioned here<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116287942\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/soumith/convnet-benchmarks/issues/66\" data-hovercard-type=\"issue\" data-hovercard-url=\"/soumith/convnet-benchmarks/issues/66/hovercard\" href=\"https://github.com/soumith/convnet-benchmarks/issues/66\">soumith/convnet-benchmarks#66</a><br>\nand mentioned by many other users. In compare with Theano and Torch, tensorflow can only experiment with smaller models.</p>\n<p>Are there any solutions to this? This is a major problem that stops me from experimenting with tensorflow.</p>\n<p>Many thanks!!</p>", "body_text": "Hi,\nIt seems the memory allocation of tensorflow is rather inefficient. I have been running a single layer rnn with 256 batch size, 124 length and dim of 512, it constantly gets memory not enough error for my 4GB 980. In theory, the model size is much less than 1GB\nno matter how large the batch size I set, it always use up all the 4GB memory, which is unreasonable. I have been compiling tensorflow from the source and BFC memory allocator is set as default.\nI think the memory problem was also mentioned here\nsoumith/convnet-benchmarks#66\nand mentioned by many other users. In compare with Theano and Torch, tensorflow can only experiment with smaller models.\nAre there any solutions to this? This is a major problem that stops me from experimenting with tensorflow.\nMany thanks!!", "body": "Hi,\n\nIt seems the memory allocation of tensorflow is rather inefficient. I have been running a single layer rnn with 256 batch size, 124 length and dim of 512, it constantly gets memory not enough error for my 4GB 980. In theory, the model size is much less than 1GB\n\nno matter how large the batch size I set, it always use up all the 4GB memory, which is unreasonable. I have been compiling tensorflow from the source and BFC memory allocator is set as default.\n\nI think the memory problem was also mentioned here \nhttps://github.com/soumith/convnet-benchmarks/issues/66\nand mentioned by many other users. In compare with Theano and Torch, tensorflow can only experiment with smaller models.\n\nAre there any solutions to this? This is a major problem that stops me from experimenting with tensorflow.\n\nMany thanks!!\n"}