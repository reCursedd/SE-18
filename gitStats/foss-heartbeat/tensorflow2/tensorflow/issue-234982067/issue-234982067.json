{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10607", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10607/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10607/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10607/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10607", "id": 234982067, "node_id": "MDU6SXNzdWUyMzQ5ODIwNjc=", "number": 10607, "title": "functional add_n: compose a new (differentiable) op from a list of ops without memory footprint", "user": {"login": "bobye", "id": 1040227, "node_id": "MDQ6VXNlcjEwNDAyMjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1040227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bobye", "html_url": "https://github.com/bobye", "followers_url": "https://api.github.com/users/bobye/followers", "following_url": "https://api.github.com/users/bobye/following{/other_user}", "gists_url": "https://api.github.com/users/bobye/gists{/gist_id}", "starred_url": "https://api.github.com/users/bobye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bobye/subscriptions", "organizations_url": "https://api.github.com/users/bobye/orgs", "repos_url": "https://api.github.com/users/bobye/repos", "events_url": "https://api.github.com/users/bobye/events{/privacy}", "received_events_url": "https://api.github.com/users/bobye/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-06-10T05:36:44Z", "updated_at": "2018-11-08T15:29:53Z", "closed_at": "2018-11-08T15:29:53Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>It is a feature request, and should be very related to the current high memory cost of tensorflow.</p>\n<p>In my case, I am building a graph which I should be able to compute gradients. In this graph, I heavily use the <code>add_n</code> op which returns a tensor from a list of tensors. I want to remark that this is very inefficient for two reasons:</p>\n<ol>\n<li>\n<p>Memory wise, it requires to cache all input tensors in the list before it actually aggregates. Technically, such process can be replaced by <code>accumulate_n</code> if one does not care computing its gradient. In facts, I have been tried with <code>add_n</code> with input list size goes to tens or a hundred, it quickly fills up the memory of GPUs.</p>\n</li>\n<li>\n<p>Besides high memory footprint, it also prohibits the use of multi-thread framework of tensorflow, ultimately affecting the efficiency. This is because the idea of <code>add_n</code> actually introduces an extra layer of tensors whose controlled dependency prevents deallocating any of these tensors computed in time. Consider the following example:</p>\n</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>a<span class=\"pl-k\">=</span>tf.Variable(<span class=\"pl-c1\">0.2</span>)\nb<span class=\"pl-k\">=</span>tf.Variable(<span class=\"pl-c1\">0.1</span>)\n\nc<span class=\"pl-k\">=</span>[tf.sin(a), tf.cos(a), tf.sin(b)]\nd<span class=\"pl-k\">=</span>[tf.sin(a), tf.cos(b)]\n\ne<span class=\"pl-k\">=</span>add_n(c)\nf<span class=\"pl-k\">=</span>add_n(d)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ... initialization </span>\nsess.run([e, f])</pre></div>\n<p>In the above example, suppose given variables <code>a</code> and <code>b</code> what we really need is tensors <code>e</code> and <code>f</code>, but the introduction of tensor list <code>c</code> and <code>d</code> occupies what I consider as redundant memory. Note that the computation of tensors in <code>c</code> and <code>d</code> can be made multi-threaded / parallel if we have infinite memory. But the real situation is if the caching tensors in <code>c</code> already eat up all memory, no tenors in <code>d</code> will be executed simultaneously until <code>e</code> is complete (at which time <code>c</code> is released).</p>\n<p>In fact, one can calculate <code>e</code> and its gradients <code>de/da</code>, <code>de/db</code> without explicitly storing any tensors in <code>c</code>. This trick is by introducing a functional which takes a set of ops and their inputs, and return a global summed-up tensor, which may look like:</p>\n<div class=\"highlight highlight-source-python\"><pre>a<span class=\"pl-k\">=</span>tf.Variable(<span class=\"pl-c1\">0.2</span>)\nb<span class=\"pl-k\">=</span>tf.Variable(<span class=\"pl-c1\">0.1</span>)\n\ne_func<span class=\"pl-k\">=</span>add_n_functional([tf.sin, tf.cos, tf.sin])\ne<span class=\"pl-k\">=</span>e_func([a, a, b])\nf_func<span class=\"pl-k\">=</span>add_n_functional([tf.sin, tf.cos])\nf<span class=\"pl-k\">=</span>f_fun([a, b])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ... initialization </span>\nsess.run([e, f])</pre></div>\n<p>Note <code>add_n_functional</code> returns a new op from a list of given ops. With this new functional, we can avoid buffering a lot intermediate tensors. At the same time, <code>e_func</code> and <code>f_func</code> are still differentiable.</p>\n<p>I was thinking how to implement this idea at python level, but it seems to only be feasible via rewriting some C++ parts. I will appreciate if this feature is added to tensorflow in the near future.</p>", "body_text": "Describe the problem\nIt is a feature request, and should be very related to the current high memory cost of tensorflow.\nIn my case, I am building a graph which I should be able to compute gradients. In this graph, I heavily use the add_n op which returns a tensor from a list of tensors. I want to remark that this is very inefficient for two reasons:\n\n\nMemory wise, it requires to cache all input tensors in the list before it actually aggregates. Technically, such process can be replaced by accumulate_n if one does not care computing its gradient. In facts, I have been tried with add_n with input list size goes to tens or a hundred, it quickly fills up the memory of GPUs.\n\n\nBesides high memory footprint, it also prohibits the use of multi-thread framework of tensorflow, ultimately affecting the efficiency. This is because the idea of add_n actually introduces an extra layer of tensors whose controlled dependency prevents deallocating any of these tensors computed in time. Consider the following example:\n\n\na=tf.Variable(0.2)\nb=tf.Variable(0.1)\n\nc=[tf.sin(a), tf.cos(a), tf.sin(b)]\nd=[tf.sin(a), tf.cos(b)]\n\ne=add_n(c)\nf=add_n(d)\n# ... initialization \nsess.run([e, f])\nIn the above example, suppose given variables a and b what we really need is tensors e and f, but the introduction of tensor list c and d occupies what I consider as redundant memory. Note that the computation of tensors in c and d can be made multi-threaded / parallel if we have infinite memory. But the real situation is if the caching tensors in c already eat up all memory, no tenors in d will be executed simultaneously until e is complete (at which time c is released).\nIn fact, one can calculate e and its gradients de/da, de/db without explicitly storing any tensors in c. This trick is by introducing a functional which takes a set of ops and their inputs, and return a global summed-up tensor, which may look like:\na=tf.Variable(0.2)\nb=tf.Variable(0.1)\n\ne_func=add_n_functional([tf.sin, tf.cos, tf.sin])\ne=e_func([a, a, b])\nf_func=add_n_functional([tf.sin, tf.cos])\nf=f_fun([a, b])\n\n# ... initialization \nsess.run([e, f])\nNote add_n_functional returns a new op from a list of given ops. With this new functional, we can avoid buffering a lot intermediate tensors. At the same time, e_func and f_func are still differentiable.\nI was thinking how to implement this idea at python level, but it seems to only be feasible via rewriting some C++ parts. I will appreciate if this feature is added to tensorflow in the near future.", "body": "### Describe the problem\r\n\r\nIt is a feature request, and should be very related to the current high memory cost of tensorflow. \r\n\r\nIn my case, I am building a graph which I should be able to compute gradients. In this graph, I heavily use the `add_n` op which returns a tensor from a list of tensors. I want to remark that this is very inefficient for two reasons:\r\n\r\n1. Memory wise, it requires to cache all input tensors in the list before it actually aggregates. Technically, such process can be replaced by `accumulate_n` if one does not care computing its gradient. In facts, I have been tried with `add_n` with input list size goes to tens or a hundred, it quickly fills up the memory of GPUs. \r\n\r\n2. Besides high memory footprint, it also prohibits the use of multi-thread framework of tensorflow, ultimately affecting the efficiency. This is because the idea of `add_n` actually introduces an extra layer of tensors whose controlled dependency prevents deallocating any of these tensors computed in time. Consider the following example:\r\n\r\n```python\r\na=tf.Variable(0.2)\r\nb=tf.Variable(0.1)\r\n\r\nc=[tf.sin(a), tf.cos(a), tf.sin(b)]\r\nd=[tf.sin(a), tf.cos(b)]\r\n\r\ne=add_n(c)\r\nf=add_n(d)\r\n# ... initialization \r\nsess.run([e, f])\r\n```\r\nIn the above example, suppose given variables `a` and `b` what we really need is tensors `e` and `f`, but the introduction of tensor list `c` and `d` occupies what I consider as redundant memory. Note that the computation of tensors in `c` and `d` can be made multi-threaded / parallel if we have infinite memory. But the real situation is if the caching tensors in `c` already eat up all memory, no tenors in `d` will be executed simultaneously until `e` is complete (at which time `c` is released). \r\n\r\nIn fact, one can calculate `e` and its gradients `de/da`, `de/db` without explicitly storing any tensors in `c`. This trick is by introducing a functional which takes a set of ops and their inputs, and return a global summed-up tensor, which may look like:\r\n\r\n```python\r\n\r\na=tf.Variable(0.2)\r\nb=tf.Variable(0.1)\r\n\r\ne_func=add_n_functional([tf.sin, tf.cos, tf.sin])\r\ne=e_func([a, a, b])\r\nf_func=add_n_functional([tf.sin, tf.cos])\r\nf=f_fun([a, b])\r\n\r\n# ... initialization \r\nsess.run([e, f])\r\n```\r\nNote `add_n_functional` returns a new op from a list of given ops. With this new functional, we can avoid buffering a lot intermediate tensors. At the same time, `e_func` and `f_func` are still differentiable.\r\n\r\nI was thinking how to implement this idea at python level, but it seems to only be feasible via rewriting some C++ parts. I will appreciate if this feature is added to tensorflow in the near future.   \r\n\r\n\r\n\r\n"}