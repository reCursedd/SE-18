{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362066946", "html_url": "https://github.com/tensorflow/tensorflow/issues/16621#issuecomment-362066946", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16621", "id": 362066946, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjA2Njk0Ng==", "user": {"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T20:53:32Z", "updated_at": "2018-01-31T20:53:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't think <code>tf.train.SyncReplicasOptimizer</code> designed to handle multiple optimizers. You can workaround it by creating your own optimizer. something like:</p>\n<pre><code>class MyOptimizer(tf.train.Optimizer):\n  def __init__(...)\n     self.opt_conv = ..\n     self.opt_fc_w = ...\n    ...\n  def apply_gradient(...):\n     extract relevant gradients for each optimizer\n     self.opt_conv.apply_gradient(...)\n     self.opt_fc_w.apply_gradient(...)\n    ...\n\noptimizer = SyncReplicasOptimizer(MyOptimizer(), ...)\n\n</code></pre>", "body_text": "I don't think tf.train.SyncReplicasOptimizer designed to handle multiple optimizers. You can workaround it by creating your own optimizer. something like:\nclass MyOptimizer(tf.train.Optimizer):\n  def __init__(...)\n     self.opt_conv = ..\n     self.opt_fc_w = ...\n    ...\n  def apply_gradient(...):\n     extract relevant gradients for each optimizer\n     self.opt_conv.apply_gradient(...)\n     self.opt_fc_w.apply_gradient(...)\n    ...\n\noptimizer = SyncReplicasOptimizer(MyOptimizer(), ...)", "body": "I don't think `tf.train.SyncReplicasOptimizer` designed to handle multiple optimizers. You can workaround it by creating your own optimizer. something like:\r\n\r\n```\r\nclass MyOptimizer(tf.train.Optimizer):\r\n  def __init__(...)\r\n     self.opt_conv = ..\r\n     self.opt_fc_w = ...\r\n    ...\r\n  def apply_gradient(...):\r\n     extract relevant gradients for each optimizer\r\n     self.opt_conv.apply_gradient(...)\r\n     self.opt_fc_w.apply_gradient(...)\r\n    ...\r\n\r\noptimizer = SyncReplicasOptimizer(MyOptimizer(), ...)\r\n\r\n```\r\n   \r\n     "}