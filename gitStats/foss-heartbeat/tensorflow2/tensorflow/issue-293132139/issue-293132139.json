{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16621", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16621/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16621/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16621/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16621", "id": 293132139, "node_id": "MDU6SXNzdWUyOTMxMzIxMzk=", "number": 16621, "title": "Using tf.train.SyncReplicasOptimizer with multiple optimizers", "user": {"login": "AshAswin", "id": 35997529, "node_id": "MDQ6VXNlcjM1OTk3NTI5", "avatar_url": "https://avatars3.githubusercontent.com/u/35997529?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AshAswin", "html_url": "https://github.com/AshAswin", "followers_url": "https://api.github.com/users/AshAswin/followers", "following_url": "https://api.github.com/users/AshAswin/following{/other_user}", "gists_url": "https://api.github.com/users/AshAswin/gists{/gist_id}", "starred_url": "https://api.github.com/users/AshAswin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AshAswin/subscriptions", "organizations_url": "https://api.github.com/users/AshAswin/orgs", "repos_url": "https://api.github.com/users/AshAswin/repos", "events_url": "https://api.github.com/users/AshAswin/events{/privacy}", "received_events_url": "https://api.github.com/users/AshAswin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-31T11:59:09Z", "updated_at": "2018-02-01T05:02:39Z", "closed_at": "2018-01-31T20:53:32Z", "author_association": "NONE", "body_html": "<p>I am trying to run the DeepLab Resnet (<a href=\"https://github.com/DrSleep/tensorflow-deeplab-resnet\">https://github.com/DrSleep/tensorflow-deeplab-resnet</a>) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(<a href=\"https://github.com/tensorflow/models/tree/master/research/inception\">https://github.com/tensorflow/models/tree/master/research/inception</a>).</p>\n<p>In the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:</p>\n<pre><code>`#Three optimizers declared with different learning rates\n opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)\n opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)\n opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`\n\n#Scope for every optimizer \ngrads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)\ngrads_conv = grads[:len(conv_trainable)]\ngrads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]\ngrads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]\n\n#Gradients applied to various portions of the network\ntrain_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))\ntrain_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))\ntrain_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))\n\n`#tf.group to combine all three operations\ntrain_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`\n</code></pre>\n<p>I don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.</p>", "body_text": "I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).\nIn the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:\n`#Three optimizers declared with different learning rates\n opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)\n opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)\n opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`\n\n#Scope for every optimizer \ngrads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)\ngrads_conv = grads[:len(conv_trainable)]\ngrads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]\ngrads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]\n\n#Gradients applied to various portions of the network\ntrain_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))\ntrain_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))\ntrain_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))\n\n`#tf.group to combine all three operations\ntrain_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`\n\nI don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.", "body": "I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).\r\n\r\nIn the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:\r\n\r\n    `#Three optimizers declared with different learning rates\r\n     opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)\r\n     opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)\r\n     opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`\r\n\r\n    #Scope for every optimizer \r\n    grads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)\r\n    grads_conv = grads[:len(conv_trainable)]\r\n    grads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]\r\n    grads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]\r\n\r\n    #Gradients applied to various portions of the network\r\n    train_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))\r\n    train_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))\r\n    train_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))\r\n \r\n    `#tf.group to combine all three operations\r\n    train_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`\r\n   \r\nI don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.\r\n\r\n\r\n \r\n\r\n\r\n"}