{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/51347550", "pull_request_review_id": null, "id": 51347550, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzQ3NTUw", "diff_hunk": "@@ -496,22 +502,61 @@\n         \"outputId\": \"8af66da6-902d-4719-bedc-7c9fb7ae7948\"\n       },\n       \"source\": [\n+        \"def make_arrays(nb_rows, img_size):\\n\",\n+        \"    if nb_rows:\\n\",\n+        \"        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\\n\",\n+        \"        labels = np.ndarray(nb_rows, dtype=np.int32)\\n\",\n+        \"    else:\\n\",\n+        \"        dataset, labels = None, None\\n\",\n+        \"    return dataset, labels\\n\",\n+        \"\\n\",\n+        \"def merge_datasets(pickle_files, train_size, valid_size=0):\\n\",\n+        \"    num_classes = len(pickle_files)\\n\",\n+        \"    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\\n\",\n+        \"    train_dataset, train_labels = make_arrays(train_size, image_size)\\n\",\n+        \"    vsize_per_class = int(valid_size / num_classes)\\n\",\n+        \"    tsize_per_class = int(train_size / num_classes)\\n\",\n+        \"    \\n\",\n+        \"    start_v, start_t = 0, 0\\n\",\n+        \"    end_v, end_t = vsize_per_class, tsize_per_class\\n\",\n+        \"    for label, pickle_file in enumerate(pickle_files):       \\n\",\n+        \"        try:\\n\",\n+        \"            with open(pickle_file, 'rb') as f:\\n\",\n+        \"                letter_set = pickle.load(f)\\n\",\n+        \"                if valid_dataset is not None:\\n\",\n+        \"                    valid_dataset[start_v:end_v, :, :] = letter_set[:vsize_per_class, :, :]\\n\",\n+        \"                    valid_labels[start_v:end_v] = label\\n\",\n+        \"                    start_v += vsize_per_class\\n\",\n+        \"                    end_v += vsize_per_class\\n\",\n+        \"                train_dataset[start_t:end_t, :, :] = letter_set[vsize_per_class:vsize_per_class+tsize_per_class, :, :]\\n\",\n+        \"                train_labels[start_t:end_t] = label\\n\",\n+        \"                start_t += tsize_per_class\\n\",\n+        \"                end_t += tsize_per_class\\n\",\n+        \"        except Exception as e:\\n\",\n+        \"          print('Unable to process data from', pickle_file, ':', e)\\n\",\n+        \"          raise\\n\",\n+        \"    \\n\",\n+        \"    return valid_dataset, valid_labels, train_dataset, train_labels\\n\",\n+        \"            \\n\",\n+        \"            \\n\",\n         \"train_size = 200000\\n\",\n         \"valid_size = 10000\\n\",\n+        \"test_size = 10000\\n\",\n+        \"\\n\",\n+        \"valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets, train_size, valid_size)\\n\",\n+        \"__, __, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\\n\",\n         \"\\n\",\n-        \"valid_dataset = train_dataset[:valid_size,:,:]\\n\",\n-        \"valid_labels = train_labels[:valid_size]\\n\",\n-        \"train_dataset = train_dataset[valid_size:valid_size+train_size,:,:]\\n\",\n-        \"train_labels = train_labels[valid_size:valid_size+train_size]\\n\",\n-        \"print('Training', train_dataset.shape, train_labels.shape)\\n\",\n-        \"print('Validation', valid_dataset.shape, valid_labels.shape)\"\n+        \"print('Training:', train_dataset.shape, train_labels.shape)\\n\",\n+        \"print('Validation:', valid_dataset.shape, valid_labels.shape)\\n\",\n+        \"print('Testing:', test_dataset.shape, test_labels.shape)\"\n       ],\n       \"outputs\": [\n         {\n           \"output_type\": \"stream\",\n           \"text\": [\n             \"Training (200000, 28, 28) (200000,)\\n\",", "path": "tensorflow/examples/udacity/1_notmnist.ipynb", "position": 333, "original_position": 327, "commit_id": "b52273baaa13ea000cf904b4ea4e215410bca908", "original_commit_id": "8bf5d031678bbfc5c470f5a1b8394eaf3b91b444", "user": {"login": "rthouvenin", "id": 7570541, "node_id": "MDQ6VXNlcjc1NzA1NDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7570541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rthouvenin", "html_url": "https://github.com/rthouvenin", "followers_url": "https://api.github.com/users/rthouvenin/followers", "following_url": "https://api.github.com/users/rthouvenin/following{/other_user}", "gists_url": "https://api.github.com/users/rthouvenin/gists{/gist_id}", "starred_url": "https://api.github.com/users/rthouvenin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rthouvenin/subscriptions", "organizations_url": "https://api.github.com/users/rthouvenin/orgs", "repos_url": "https://api.github.com/users/rthouvenin/repos", "events_url": "https://api.github.com/users/rthouvenin/events{/privacy}", "received_events_url": "https://api.github.com/users/rthouvenin/received_events", "type": "User", "site_admin": false}, "body": "I haven't had the time yet to do the subsequent assignments. But this default training size is already in the current notebook.\n", "created_at": "2016-01-30T17:02:12Z", "updated_at": "2016-01-31T08:01:47Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/940#discussion_r51347550", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/940", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/51347550"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/940#discussion_r51347550"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/940"}}, "body_html": "<p>I haven't had the time yet to do the subsequent assignments. But this default training size is already in the current notebook.</p>", "body_text": "I haven't had the time yet to do the subsequent assignments. But this default training size is already in the current notebook."}