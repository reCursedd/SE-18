{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/165595088", "html_url": "https://github.com/tensorflow/tensorflow/issues/535#issuecomment-165595088", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/535", "id": 165595088, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTU5NTA4OA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-17T22:03:15Z", "updated_at": "2015-12-17T22:03:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Typically we use a much larger amount of computation to train a model with TensorFlow (using one or more GPUs if it is a complex model), because the throughput of examples processed per second is what determines how long it takes to train the network.</p>\n<p>For inference - depending on the model - it is often possible to scale horizontally by using lower power devices, and scaling out to many independent devices (unless the model is too big to fit on a single device). That's what makes it possible to run inference on a mobile device (like in the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\">Android tutorial</a>), and potentially it could also run inference on a Raspberry Pi.</p>", "body_text": "Typically we use a much larger amount of computation to train a model with TensorFlow (using one or more GPUs if it is a complex model), because the throughput of examples processed per second is what determines how long it takes to train the network.\nFor inference - depending on the model - it is often possible to scale horizontally by using lower power devices, and scaling out to many independent devices (unless the model is too big to fit on a single device). That's what makes it possible to run inference on a mobile device (like in the Android tutorial), and potentially it could also run inference on a Raspberry Pi.", "body": "Typically we use a much larger amount of computation to train a model with TensorFlow (using one or more GPUs if it is a complex model), because the throughput of examples processed per second is what determines how long it takes to train the network.\n\nFor inference - depending on the model - it is often possible to scale horizontally by using lower power devices, and scaling out to many independent devices (unless the model is too big to fit on a single device). That's what makes it possible to run inference on a mobile device (like in the [Android tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)), and potentially it could also run inference on a Raspberry Pi.\n"}