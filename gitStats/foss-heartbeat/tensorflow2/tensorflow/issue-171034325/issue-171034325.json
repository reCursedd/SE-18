{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3799", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3799/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3799/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3799/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3799", "id": 171034325, "node_id": "MDU6SXNzdWUxNzEwMzQzMjU=", "number": 3799, "title": "distributed tensorflow on localhosts failed by \u201csocket error, connection refused\u201d", "user": {"login": "tonyrivermsfly", "id": 18235797, "node_id": "MDQ6VXNlcjE4MjM1Nzk3", "avatar_url": "https://avatars0.githubusercontent.com/u/18235797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tonyrivermsfly", "html_url": "https://github.com/tonyrivermsfly", "followers_url": "https://api.github.com/users/tonyrivermsfly/followers", "following_url": "https://api.github.com/users/tonyrivermsfly/following{/other_user}", "gists_url": "https://api.github.com/users/tonyrivermsfly/gists{/gist_id}", "starred_url": "https://api.github.com/users/tonyrivermsfly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tonyrivermsfly/subscriptions", "organizations_url": "https://api.github.com/users/tonyrivermsfly/orgs", "repos_url": "https://api.github.com/users/tonyrivermsfly/repos", "events_url": "https://api.github.com/users/tonyrivermsfly/events{/privacy}", "received_events_url": "https://api.github.com/users/tonyrivermsfly/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-08-14T00:11:33Z", "updated_at": "2016-08-16T15:51:41Z", "closed_at": "2016-08-16T15:51:41Z", "author_association": "NONE", "body_html": "<p>I am experimenting distributed tensorflow using a slight modification of an <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html\" rel=\"nofollow\">official example</a>.</p>\n<p>My experiment code is (you can skip this for now and scroll down to the problem),</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == \"ps\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # Build model...\n            x = tf.placeholder(\"float\", [10, 10], name=\"x\")\n            y = tf.placeholder(\"float\", [10, 1], name=\"y\")\n            initial_w = np.zeros((10, 1))\n            w = tf.Variable(initial_w, name=\"w\", dtype=\"float32\")\n            loss = tf.pow(tf.add(y,-tf.matmul(x,w)),2,name=\"loss\")\n            global_step = tf.Variable(0)\n\n            saver = tf.train.Saver()\n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n\n        # Create a \"supervisor\", which oversees the training process.\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=\"/tmp/train_logs\",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=600)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step = 0\n            while not sv.should_stop() and step &lt; 1000000:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                _, step = sess.run([loss, global_step])\n                print(\"job_name: %s; task_index: %s; step: %d\" % (FLAGS.job_name,FLAGS.task_index,step))\n\n        # Ask for all the services to stop.\n        sv.stop()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()\n</code></pre>\n<p>Then I run the following commands as instructed by the official document, changing the example host names to \"localhost\" (the script is named hello_distributed.py),</p>\n<pre><code>localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=0\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=1\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=0\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=1\n</code></pre>\n<p>The first two lines for running \"ps\" are good. The last two lines get the following \"connection refused\" error.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/18235797/17646308/036f3360-6192-11e6-9e1a-4a4ef88f8264.png\"><img src=\"https://cloud.githubusercontent.com/assets/18235797/17646308/036f3360-6192-11e6-9e1a-4a4ef88f8264.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>Anyone can help with this? Thank you!</p>", "body_text": "I am experimenting distributed tensorflow using a slight modification of an official example.\nMy experiment code is (you can skip this for now and scroll down to the problem),\nimport tensorflow as tf\nimport numpy as np\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == \"ps\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # Build model...\n            x = tf.placeholder(\"float\", [10, 10], name=\"x\")\n            y = tf.placeholder(\"float\", [10, 1], name=\"y\")\n            initial_w = np.zeros((10, 1))\n            w = tf.Variable(initial_w, name=\"w\", dtype=\"float32\")\n            loss = tf.pow(tf.add(y,-tf.matmul(x,w)),2,name=\"loss\")\n            global_step = tf.Variable(0)\n\n            saver = tf.train.Saver()\n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n\n        # Create a \"supervisor\", which oversees the training process.\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=\"/tmp/train_logs\",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=600)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step = 0\n            while not sv.should_stop() and step < 1000000:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                _, step = sess.run([loss, global_step])\n                print(\"job_name: %s; task_index: %s; step: %d\" % (FLAGS.job_name,FLAGS.task_index,step))\n\n        # Ask for all the services to stop.\n        sv.stop()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()\n\nThen I run the following commands as instructed by the official document, changing the example host names to \"localhost\" (the script is named hello_distributed.py),\nlocalhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=0\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=1\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=0\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=1\n\nThe first two lines for running \"ps\" are good. The last two lines get the following \"connection refused\" error.\n\nAnyone can help with this? Thank you!", "body": "I am experimenting distributed tensorflow using a slight modification of an [official example](https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html).\n\nMy experiment code is (you can skip this for now and scroll down to the problem),\n\n```\nimport tensorflow as tf\nimport numpy as np\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == \"ps\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # Build model...\n            x = tf.placeholder(\"float\", [10, 10], name=\"x\")\n            y = tf.placeholder(\"float\", [10, 1], name=\"y\")\n            initial_w = np.zeros((10, 1))\n            w = tf.Variable(initial_w, name=\"w\", dtype=\"float32\")\n            loss = tf.pow(tf.add(y,-tf.matmul(x,w)),2,name=\"loss\")\n            global_step = tf.Variable(0)\n\n            saver = tf.train.Saver()\n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n\n        # Create a \"supervisor\", which oversees the training process.\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=\"/tmp/train_logs\",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=600)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step = 0\n            while not sv.should_stop() and step < 1000000:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                _, step = sess.run([loss, global_step])\n                print(\"job_name: %s; task_index: %s; step: %d\" % (FLAGS.job_name,FLAGS.task_index,step))\n\n        # Ask for all the services to stop.\n        sv.stop()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()\n```\n\nThen I run the following commands as instructed by the official document, changing the example host names to \"localhost\" (the script is named hello_distributed.py),\n\n```\nlocalhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=0\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=1\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=0\n\nsudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=1\n```\n\nThe first two lines for running \"ps\" are good. The last two lines get the following \"connection refused\" error.\n\n![image](https://cloud.githubusercontent.com/assets/18235797/17646308/036f3360-6192-11e6-9e1a-4a4ef88f8264.png)\n\nAnyone can help with this? Thank you!\n"}