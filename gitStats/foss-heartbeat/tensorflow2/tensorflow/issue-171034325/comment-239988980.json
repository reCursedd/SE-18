{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239988980", "html_url": "https://github.com/tensorflow/tensorflow/issues/3799#issuecomment-239988980", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3799", "id": 239988980, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTk4ODk4MA==", "user": {"login": "tonyrivermsfly", "id": 18235797, "node_id": "MDQ6VXNlcjE4MjM1Nzk3", "avatar_url": "https://avatars0.githubusercontent.com/u/18235797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tonyrivermsfly", "html_url": "https://github.com/tonyrivermsfly", "followers_url": "https://api.github.com/users/tonyrivermsfly/followers", "following_url": "https://api.github.com/users/tonyrivermsfly/following{/other_user}", "gists_url": "https://api.github.com/users/tonyrivermsfly/gists{/gist_id}", "starred_url": "https://api.github.com/users/tonyrivermsfly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tonyrivermsfly/subscriptions", "organizations_url": "https://api.github.com/users/tonyrivermsfly/orgs", "repos_url": "https://api.github.com/users/tonyrivermsfly/repos", "events_url": "https://api.github.com/users/tonyrivermsfly/events{/privacy}", "received_events_url": "https://api.github.com/users/tonyrivermsfly/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-16T03:00:27Z", "updated_at": "2016-08-16T03:04:50Z", "author_association": "NONE", "body_html": "<p>Hi guys, thank you for all your help!</p>\n<p>Yes, when it says \"connection refused\", it actually means the program has crashed, and that <strong>means there could be some error in the code</strong>. It turns out I forgot to feed sess.run function.</p>\n<p>In the script I use</p>\n<p><code>_, step = sess.run([loss, global_step])</code></p>\n<p>which has no feed for loss. The correct code is</p>\n<pre><code>                _, step = sess.run([loss, global_step],\n                {\n                  x: np.random.rand(10,10),\n                  y: np.random.rand(10).reshape(-1,1)\n                })\n</code></pre>\n<p>Now everything works! Thank you guys! Special thank you to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6200749\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hholst80\">@hholst80</a> for providing the script.</p>\n<p>The whole working code is posted below,</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == \"ps\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # Build model...\n            x = tf.placeholder(\"float\", [10, 10], name=\"x\")\n            y = tf.placeholder(\"float\", [10, 1], name=\"y\")\n            initial_w = np.zeros((10, 1))\n            w = tf.Variable(initial_w, name=\"w\", dtype=\"float32\")\n            loss = tf.pow(tf.add(y,-tf.matmul(x,w)),2,name=\"loss\")\n            global_step = tf.Variable(0)\n\n            saver = tf.train.Saver()\n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n\n        # Create a \"supervisor\", which oversees the training process.\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=\"/tmp/train_logs\",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=600)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step = 0\n            while not sv.should_stop() and step &lt; 1000000:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                _, step = sess.run([loss, global_step],\n                {\n                  x: np.random.rand(10,10),\n                  y: np.random.rand(10).reshape(-1,1)\n                })\n                print(\"job_name: %s; task_index: %s; step: %d\" % (FLAGS.job_name,FLAGS.task_index,step))\n\n        # Ask for all the services to stop.\n        sv.stop()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()\n</code></pre>", "body_text": "Hi guys, thank you for all your help!\nYes, when it says \"connection refused\", it actually means the program has crashed, and that means there could be some error in the code. It turns out I forgot to feed sess.run function.\nIn the script I use\n_, step = sess.run([loss, global_step])\nwhich has no feed for loss. The correct code is\n                _, step = sess.run([loss, global_step],\n                {\n                  x: np.random.rand(10,10),\n                  y: np.random.rand(10).reshape(-1,1)\n                })\n\nNow everything works! Thank you guys! Special thank you to @hholst80 for providing the script.\nThe whole working code is posted below,\nimport tensorflow as tf\nimport numpy as np\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == \"ps\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # Build model...\n            x = tf.placeholder(\"float\", [10, 10], name=\"x\")\n            y = tf.placeholder(\"float\", [10, 1], name=\"y\")\n            initial_w = np.zeros((10, 1))\n            w = tf.Variable(initial_w, name=\"w\", dtype=\"float32\")\n            loss = tf.pow(tf.add(y,-tf.matmul(x,w)),2,name=\"loss\")\n            global_step = tf.Variable(0)\n\n            saver = tf.train.Saver()\n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n\n        # Create a \"supervisor\", which oversees the training process.\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=\"/tmp/train_logs\",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=600)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step = 0\n            while not sv.should_stop() and step < 1000000:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                _, step = sess.run([loss, global_step],\n                {\n                  x: np.random.rand(10,10),\n                  y: np.random.rand(10).reshape(-1,1)\n                })\n                print(\"job_name: %s; task_index: %s; step: %d\" % (FLAGS.job_name,FLAGS.task_index,step))\n\n        # Ask for all the services to stop.\n        sv.stop()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()", "body": "Hi guys, thank you for all your help!\n\nYes, when it says \"connection refused\", it actually means the program has crashed, and that **means there could be some error in the code**. It turns out I forgot to feed sess.run function.\n\nIn the script I use\n\n`_, step = sess.run([loss, global_step])`\n\nwhich has no feed for loss. The correct code is\n\n```\n                _, step = sess.run([loss, global_step],\n                {\n                  x: np.random.rand(10,10),\n                  y: np.random.rand(10).reshape(-1,1)\n                })\n```\n\nNow everything works! Thank you guys! Special thank you to @hholst80 for providing the script.\n\nThe whole working code is posted below,\n\n```\nimport tensorflow as tf\nimport numpy as np\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == \"ps\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # Build model...\n            x = tf.placeholder(\"float\", [10, 10], name=\"x\")\n            y = tf.placeholder(\"float\", [10, 1], name=\"y\")\n            initial_w = np.zeros((10, 1))\n            w = tf.Variable(initial_w, name=\"w\", dtype=\"float32\")\n            loss = tf.pow(tf.add(y,-tf.matmul(x,w)),2,name=\"loss\")\n            global_step = tf.Variable(0)\n\n            saver = tf.train.Saver()\n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n\n        # Create a \"supervisor\", which oversees the training process.\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=\"/tmp/train_logs\",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=600)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step = 0\n            while not sv.should_stop() and step < 1000000:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                _, step = sess.run([loss, global_step],\n                {\n                  x: np.random.rand(10,10),\n                  y: np.random.rand(10).reshape(-1,1)\n                })\n                print(\"job_name: %s; task_index: %s; step: %d\" % (FLAGS.job_name,FLAGS.task_index,step))\n\n        # Ask for all the services to stop.\n        sv.stop()\n\n\nif __name__ == \"__main__\":\n    tf.app.run()\n```\n"}