{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/327443275", "html_url": "https://github.com/tensorflow/tensorflow/issues/12649#issuecomment-327443275", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12649", "id": 327443275, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzQ0MzI3NQ==", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-06T10:29:57Z", "updated_at": "2017-09-06T10:29:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a> I have tried Inception V3 at varying resolutions that is not 299x299 on my workstation, and it could still work. I think what has to be changed is just the first input node shape and some modification to its auxiliary operations. This is also true for the android implementation, and I've tried till 360x360, beyond which it gives a buffer overflow error. As for the image data, I think generally all the images have around the same range of pixel height and width, and I don't think there are any outlier images (using the imagenet data). In fact, because I feed my image sequentially in order, without shuffling, I've checked the first image doesn't have weird sizes. Also, the model fails at the first image already so it didn't stop halfway while running.</p>\n<p>I've also tried for a significantly larger model, which took around 2x more memory but could still work at higher resolutions - this is the part I'm unsure about. From my experience, increasing the image resolution does improve the results for some models, just as decreasing the resolution would worsen the performance.</p>\n<p>Actually I'd like to print the sizes of var1 and var2 at <code>TensorFlowInferenceInterface.class</code> like you mentioned, but it's compiled and so not accessible directly. Is there a direct way to get the sizes of var1 and var or increase the buffer size?</p>\n<p>Before the <code>inferenceInterface.fetch(outputName, outputs);</code> call happens, the values of the arguments are:</p>\n<ol>\n<li><strong>outputName:</strong> InceptionV3/Predictions/Softmax</li>\n<li><strong>outputs:</strong> [F@c9bf470</li>\n</ol>\n<p>which are output node name and output float array (which is empty at first).</p>", "body_text": "@andrewharp I have tried Inception V3 at varying resolutions that is not 299x299 on my workstation, and it could still work. I think what has to be changed is just the first input node shape and some modification to its auxiliary operations. This is also true for the android implementation, and I've tried till 360x360, beyond which it gives a buffer overflow error. As for the image data, I think generally all the images have around the same range of pixel height and width, and I don't think there are any outlier images (using the imagenet data). In fact, because I feed my image sequentially in order, without shuffling, I've checked the first image doesn't have weird sizes. Also, the model fails at the first image already so it didn't stop halfway while running.\nI've also tried for a significantly larger model, which took around 2x more memory but could still work at higher resolutions - this is the part I'm unsure about. From my experience, increasing the image resolution does improve the results for some models, just as decreasing the resolution would worsen the performance.\nActually I'd like to print the sizes of var1 and var2 at TensorFlowInferenceInterface.class like you mentioned, but it's compiled and so not accessible directly. Is there a direct way to get the sizes of var1 and var or increase the buffer size?\nBefore the inferenceInterface.fetch(outputName, outputs); call happens, the values of the arguments are:\n\noutputName: InceptionV3/Predictions/Softmax\noutputs: [F@c9bf470\n\nwhich are output node name and output float array (which is empty at first).", "body": "@andrewharp I have tried Inception V3 at varying resolutions that is not 299x299 on my workstation, and it could still work. I think what has to be changed is just the first input node shape and some modification to its auxiliary operations. This is also true for the android implementation, and I've tried till 360x360, beyond which it gives a buffer overflow error. As for the image data, I think generally all the images have around the same range of pixel height and width, and I don't think there are any outlier images (using the imagenet data). In fact, because I feed my image sequentially in order, without shuffling, I've checked the first image doesn't have weird sizes. Also, the model fails at the first image already so it didn't stop halfway while running.\r\n\r\nI've also tried for a significantly larger model, which took around 2x more memory but could still work at higher resolutions - this is the part I'm unsure about. From my experience, increasing the image resolution does improve the results for some models, just as decreasing the resolution would worsen the performance. \r\n\r\nActually I'd like to print the sizes of var1 and var2 at `TensorFlowInferenceInterface.class` like you mentioned, but it's compiled and so not accessible directly. Is there a direct way to get the sizes of var1 and var or increase the buffer size?\r\n\r\nBefore the `inferenceInterface.fetch(outputName, outputs);` call happens, the values of the arguments are:\r\n\r\n1. **outputName:** InceptionV3/Predictions/Softmax\r\n2. **outputs:** [F@c9bf470\r\n\r\nwhich are output node name and output float array (which is empty at first).\r\n\r\n\r\n\r\n"}