{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243778132", "html_url": "https://github.com/tensorflow/tensorflow/issues/1996#issuecomment-243778132", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1996", "id": 243778132, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mzc3ODEzMg==", "user": {"login": "klueska", "id": 96419, "node_id": "MDQ6VXNlcjk2NDE5", "avatar_url": "https://avatars1.githubusercontent.com/u/96419?v=4", "gravatar_id": "", "url": "https://api.github.com/users/klueska", "html_url": "https://github.com/klueska", "followers_url": "https://api.github.com/users/klueska/followers", "following_url": "https://api.github.com/users/klueska/following{/other_user}", "gists_url": "https://api.github.com/users/klueska/gists{/gist_id}", "starred_url": "https://api.github.com/users/klueska/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/klueska/subscriptions", "organizations_url": "https://api.github.com/users/klueska/orgs", "repos_url": "https://api.github.com/users/klueska/repos", "events_url": "https://api.github.com/users/klueska/events{/privacy}", "received_events_url": "https://api.github.com/users/klueska/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-31T14:16:37Z", "updated_at": "2016-08-31T14:16:37Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=572167\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/windreamer\">@windreamer</a> I think the reason that the JIRA was probably never resolved is that it's not clear that the fix being proposed is the right one. It may happen to work for your particular use case, but many of mesos's probufs aren't written in a way that is compatible with proto3 clients.  For example, many of mesos's protobufs still contain <code>required</code> fields. We don't want people to blindly do a <code>pip install protobuf</code> (which installs 3.0 by default) and then start writing clients that will break in subtle ways when interacting with proto2 data coming over the wire. If you know of a general workaround for this, I'm sure it would gladly be accepted.</p>\n<p>Regarding problems figuring out how to enable GPU support -- I can help with that. We basically mimic the functionality of nvidia-docker so that anything that runs in nvidia-docker should now be able to run in mesos as well.  Consider the following example:</p>\n<pre><code>$ mesos-master \\\n      --ip=127.0.0.1 \\\n      --work_dir=/var/lib/mesos\n\n$ mesos-agent \\\n      --master=127.0.0.1:5050 \\\n      --work_dir=/var/lib/mesos \\\n      --image_providers=docker \\\n      --executor_environment_variables=\"{}\" \\\n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\"\n\n$ mesos-execute \\\n      --master=127.0.0.1:5050 \\\n      --name=gpu-test \\\n      --docker_image=nvidia/cuda \\\n      --command=\"nvidia-smi\" \\\n      --framework_capabilities=\"GPU_RESOURCES\" \\\n      --resources=\"gpus:1\"\n</code></pre>\n<p>The flags of note here are:</p>\n<pre><code>  mesos-agent: \n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\" \n\n  mesos-execute: \n      --resources=\"gpus:1\" \n      --framework_capabilities=\"GPU_RESOURCES\" \n</code></pre>\n<p>When launching an agent, both the <code>cgroups/devices</code> and the <code>gpu/nvidia</code> isolation flags are required for Nvidia GPU support in Mesos. Likewise, the <code>docker/runtime</code> and <code>filesystem/linux</code> flags are needed to enable running docker images with the unified containerizer.</p>\n<p>The <code>cgroups/devices</code> flag tells the agent to restrict access to a specific set of devices when launching a task (i.e. a subset of the devices listed in <code>/dev</code>). The <code>gpu/nvidia</code> isolation flag allows the agent to grant / revoke access to GPUs on a per-task basis. It also handles automatic injection of the Nvidia libraries / volumes into the container if the label <code>com.nvidia.volumes.needed = nvidia_driver</code> is present in the docker image. The <code>docker/runtime</code> flag allows the agent to parse docker image files and containerize them. The <code>filesystem/linux</code> flag says to use linux specific functionality when creating / entering the new mount namespace for the container filesystem.</p>\n<p>In addition to these agent isolation flags, Mesos requires frameworks that want to consume GPU resources to have the GPU_RESOURCES framework capability set. Without this, the master will not send an offer to a framework if it contains GPUs. The choice to make frameworks explicitly opt-in to this GPU_RESOURCES capability was to keep legacy frameworks from accidentally consuming a bunch of non-GPU resources on any GPU-capable machines in a cluster (and thus blocking your GPU jobs from running). It's not that big a deal if all of your nodes have GPUs, but in a mixed-node environment, it can be a big problem.</p>\n<p>Finally, the  <code>--resources=\"gpus:1\"</code> flag tells the framework to only accept offers that contain at least 1 GPU. This is just an example of consuming a single GPU, you can (and probably should) build your framework to do something more interesting.</p>\n<p>Hopefully you can extrapolate things from there. Let me know if you have any questions.</p>", "body_text": "@windreamer I think the reason that the JIRA was probably never resolved is that it's not clear that the fix being proposed is the right one. It may happen to work for your particular use case, but many of mesos's probufs aren't written in a way that is compatible with proto3 clients.  For example, many of mesos's protobufs still contain required fields. We don't want people to blindly do a pip install protobuf (which installs 3.0 by default) and then start writing clients that will break in subtle ways when interacting with proto2 data coming over the wire. If you know of a general workaround for this, I'm sure it would gladly be accepted.\nRegarding problems figuring out how to enable GPU support -- I can help with that. We basically mimic the functionality of nvidia-docker so that anything that runs in nvidia-docker should now be able to run in mesos as well.  Consider the following example:\n$ mesos-master \\\n      --ip=127.0.0.1 \\\n      --work_dir=/var/lib/mesos\n\n$ mesos-agent \\\n      --master=127.0.0.1:5050 \\\n      --work_dir=/var/lib/mesos \\\n      --image_providers=docker \\\n      --executor_environment_variables=\"{}\" \\\n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\"\n\n$ mesos-execute \\\n      --master=127.0.0.1:5050 \\\n      --name=gpu-test \\\n      --docker_image=nvidia/cuda \\\n      --command=\"nvidia-smi\" \\\n      --framework_capabilities=\"GPU_RESOURCES\" \\\n      --resources=\"gpus:1\"\n\nThe flags of note here are:\n  mesos-agent: \n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\" \n\n  mesos-execute: \n      --resources=\"gpus:1\" \n      --framework_capabilities=\"GPU_RESOURCES\" \n\nWhen launching an agent, both the cgroups/devices and the gpu/nvidia isolation flags are required for Nvidia GPU support in Mesos. Likewise, the docker/runtime and filesystem/linux flags are needed to enable running docker images with the unified containerizer.\nThe cgroups/devices flag tells the agent to restrict access to a specific set of devices when launching a task (i.e. a subset of the devices listed in /dev). The gpu/nvidia isolation flag allows the agent to grant / revoke access to GPUs on a per-task basis. It also handles automatic injection of the Nvidia libraries / volumes into the container if the label com.nvidia.volumes.needed = nvidia_driver is present in the docker image. The docker/runtime flag allows the agent to parse docker image files and containerize them. The filesystem/linux flag says to use linux specific functionality when creating / entering the new mount namespace for the container filesystem.\nIn addition to these agent isolation flags, Mesos requires frameworks that want to consume GPU resources to have the GPU_RESOURCES framework capability set. Without this, the master will not send an offer to a framework if it contains GPUs. The choice to make frameworks explicitly opt-in to this GPU_RESOURCES capability was to keep legacy frameworks from accidentally consuming a bunch of non-GPU resources on any GPU-capable machines in a cluster (and thus blocking your GPU jobs from running). It's not that big a deal if all of your nodes have GPUs, but in a mixed-node environment, it can be a big problem.\nFinally, the  --resources=\"gpus:1\" flag tells the framework to only accept offers that contain at least 1 GPU. This is just an example of consuming a single GPU, you can (and probably should) build your framework to do something more interesting.\nHopefully you can extrapolate things from there. Let me know if you have any questions.", "body": "@windreamer I think the reason that the JIRA was probably never resolved is that it's not clear that the fix being proposed is the right one. It may happen to work for your particular use case, but many of mesos's probufs aren't written in a way that is compatible with proto3 clients.  For example, many of mesos's protobufs still contain `required` fields. We don't want people to blindly do a `pip install protobuf` (which installs 3.0 by default) and then start writing clients that will break in subtle ways when interacting with proto2 data coming over the wire. If you know of a general workaround for this, I'm sure it would gladly be accepted. \n\nRegarding problems figuring out how to enable GPU support -- I can help with that. We basically mimic the functionality of nvidia-docker so that anything that runs in nvidia-docker should now be able to run in mesos as well.  Consider the following example:\n\n```\n$ mesos-master \\\n      --ip=127.0.0.1 \\\n      --work_dir=/var/lib/mesos\n\n$ mesos-agent \\\n      --master=127.0.0.1:5050 \\\n      --work_dir=/var/lib/mesos \\\n      --image_providers=docker \\\n      --executor_environment_variables=\"{}\" \\\n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\"\n\n$ mesos-execute \\\n      --master=127.0.0.1:5050 \\\n      --name=gpu-test \\\n      --docker_image=nvidia/cuda \\\n      --command=\"nvidia-smi\" \\\n      --framework_capabilities=\"GPU_RESOURCES\" \\\n      --resources=\"gpus:1\"\n```\n\nThe flags of note here are: \n\n```\n  mesos-agent: \n      --isolation=\"docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia\" \n\n  mesos-execute: \n      --resources=\"gpus:1\" \n      --framework_capabilities=\"GPU_RESOURCES\" \n```\n\nWhen launching an agent, both the `cgroups/devices` and the `gpu/nvidia` isolation flags are required for Nvidia GPU support in Mesos. Likewise, the `docker/runtime` and `filesystem/linux` flags are needed to enable running docker images with the unified containerizer.\n\nThe `cgroups/devices` flag tells the agent to restrict access to a specific set of devices when launching a task (i.e. a subset of the devices listed in `/dev`). The `gpu/nvidia` isolation flag allows the agent to grant / revoke access to GPUs on a per-task basis. It also handles automatic injection of the Nvidia libraries / volumes into the container if the label `com.nvidia.volumes.needed = nvidia_driver` is present in the docker image. The `docker/runtime` flag allows the agent to parse docker image files and containerize them. The `filesystem/linux` flag says to use linux specific functionality when creating / entering the new mount namespace for the container filesystem. \n\nIn addition to these agent isolation flags, Mesos requires frameworks that want to consume GPU resources to have the GPU_RESOURCES framework capability set. Without this, the master will not send an offer to a framework if it contains GPUs. The choice to make frameworks explicitly opt-in to this GPU_RESOURCES capability was to keep legacy frameworks from accidentally consuming a bunch of non-GPU resources on any GPU-capable machines in a cluster (and thus blocking your GPU jobs from running). It's not that big a deal if all of your nodes have GPUs, but in a mixed-node environment, it can be a big problem.\n\nFinally, the  `--resources=\"gpus:1\"` flag tells the framework to only accept offers that contain at least 1 GPU. This is just an example of consuming a single GPU, you can (and probably should) build your framework to do something more interesting.\n\nHopefully you can extrapolate things from there. Let me know if you have any questions.\n"}