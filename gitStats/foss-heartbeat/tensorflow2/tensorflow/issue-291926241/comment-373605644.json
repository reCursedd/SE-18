{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/373605644", "html_url": "https://github.com/tensorflow/tensorflow/issues/16455#issuecomment-373605644", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16455", "id": 373605644, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzYwNTY0NA==", "user": {"login": "CasiaFan", "id": 10608984, "node_id": "MDQ6VXNlcjEwNjA4OTg0", "avatar_url": "https://avatars3.githubusercontent.com/u/10608984?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CasiaFan", "html_url": "https://github.com/CasiaFan", "followers_url": "https://api.github.com/users/CasiaFan/followers", "following_url": "https://api.github.com/users/CasiaFan/following{/other_user}", "gists_url": "https://api.github.com/users/CasiaFan/gists{/gist_id}", "starred_url": "https://api.github.com/users/CasiaFan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CasiaFan/subscriptions", "organizations_url": "https://api.github.com/users/CasiaFan/orgs", "repos_url": "https://api.github.com/users/CasiaFan/repos", "events_url": "https://api.github.com/users/CasiaFan/events{/privacy}", "received_events_url": "https://api.github.com/users/CasiaFan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-16T05:13:23Z", "updated_at": "2018-03-16T05:13:23Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=852234\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/formigone\">@formigone</a> I think you should put <code>tf.control_dependencies()</code> function inside your estimator TRAIN branch, for the moving average and variance are only need to update during training. Here is my example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">params</span>):\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        is_training <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    <span class=\"pl-k\">else</span>:\n        is_training <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n    input_tensor <span class=\"pl-k\">=</span> features[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>]\n    logits <span class=\"pl-k\">=</span> your_model_fn(input_tensor, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training)\n    probs <span class=\"pl-k\">=</span> tf.nn.softmax(logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_score<span class=\"pl-pds\">\"</span></span>)\n    predictions <span class=\"pl-k\">=</span> tf.argmax(probs, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_label<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> provide a tf.estimator spec for PREDICT</span>\n    predictions_dict <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>score<span class=\"pl-pds\">\"</span></span>: probs,\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>label<span class=\"pl-pds\">\"</span></span>: predictions}\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">PREDICT</span>:\n        predictions_output <span class=\"pl-k\">=</span> tf.estimator.export.PredictOutput(predictions_dict)\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n                                          <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions_dict,\n                                          <span class=\"pl-v\">export_outputs</span><span class=\"pl-k\">=</span>{\n                                              tf.saved_model.signature_constants.<span class=\"pl-c1\">DEFAULT_SERVING_SIGNATURE_DEF_KEY</span>: predictions_output\n                                          })\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate loss</span>\n    onehot_labels <span class=\"pl-k\">=</span> tf.one_hot(tf.cast(labels, tf.int32), <span class=\"pl-c1\">_NUM_CLASSES</span>)\n    gamma <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1.5</span>\n    weights <span class=\"pl-k\">=</span> tf.reduce_sum(tf.multiply(onehot_labels, tf.pow(<span class=\"pl-c1\">1</span>. <span class=\"pl-k\">-</span> probs, gamma)), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    loss <span class=\"pl-k\">=</span> tf.losses.softmax_cross_entropy(onehot_labels, logits, <span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span>weights)\n    accuracy <span class=\"pl-k\">=</span> tf.metrics.accuracy(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels,\n                                   <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions)\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        lr <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.001</span>\n        optimizer <span class=\"pl-k\">=</span> tf.train.RMSPropOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>lr, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n        update_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n        <span class=\"pl-k\">with</span> tf.control_dependencies(update_ops):\n            train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_global_step())\n        tensors_to_log <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_accuracy<span class=\"pl-pds\">'</span></span>: accuracy[<span class=\"pl-c1\">1</span>],\n                          <span class=\"pl-s\"><span class=\"pl-pds\">'</span>logits<span class=\"pl-pds\">'</span></span>: logits,\n                          <span class=\"pl-s\"><span class=\"pl-pds\">'</span>label<span class=\"pl-pds\">'</span></span>: labels}\n        logging_hook <span class=\"pl-k\">=</span> tf.train.LoggingTensorHook(<span class=\"pl-v\">tensors</span><span class=\"pl-k\">=</span>tensors_to_log, <span class=\"pl-v\">every_n_iter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n                                          <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss,\n                                          <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op,\n                                          <span class=\"pl-v\">training_hooks</span><span class=\"pl-k\">=</span>[logging_hook])\n    <span class=\"pl-k\">else</span>:\n        eval_metric_ops <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>: accuracy}\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n                                          <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss,\n                                          <span class=\"pl-v\">eval_metric_ops</span><span class=\"pl-k\">=</span>eval_metric_ops)</pre></div>", "body_text": "@formigone I think you should put tf.control_dependencies() function inside your estimator TRAIN branch, for the moving average and variance are only need to update during training. Here is my example:\ndef model_fn(features, labels, mode, params):\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        is_training = True\n    else:\n        is_training = False\n    input_tensor = features[\"input\"]\n    logits = your_model_fn(input_tensor, is_training=is_training)\n    probs = tf.nn.softmax(logits, name=\"output_score\")\n    predictions = tf.argmax(probs, axis=-1, name=\"output_label\")\n    # provide a tf.estimator spec for PREDICT\n    predictions_dict = {\"score\": probs,\n                        \"label\": predictions}\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        predictions_output = tf.estimator.export.PredictOutput(predictions_dict)\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          predictions=predictions_dict,\n                                          export_outputs={\n                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output\n                                          })\n    # calculate loss\n    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), _NUM_CLASSES)\n    gamma = 1.5\n    weights = tf.reduce_sum(tf.multiply(onehot_labels, tf.pow(1. - probs, gamma)), axis=-1)\n    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits, weights=weights)\n    accuracy = tf.metrics.accuracy(labels=labels,\n                                   predictions=predictions)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        lr = 0.001\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n        tensors_to_log = {'batch_accuracy': accuracy[1],\n                          'logits': logits,\n                          'label': labels}\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          loss=loss,\n                                          train_op=train_op,\n                                          training_hooks=[logging_hook])\n    else:\n        eval_metric_ops = {\"accuracy\": accuracy}\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          loss=loss,\n                                          eval_metric_ops=eval_metric_ops)", "body": "@formigone I think you should put `tf.control_dependencies()` function inside your estimator TRAIN branch, for the moving average and variance are only need to update during training. Here is my example:\r\n```python\r\ndef model_fn(features, labels, mode, params):\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        is_training = True\r\n    else:\r\n        is_training = False\r\n    input_tensor = features[\"input\"]\r\n    logits = your_model_fn(input_tensor, is_training=is_training)\r\n    probs = tf.nn.softmax(logits, name=\"output_score\")\r\n    predictions = tf.argmax(probs, axis=-1, name=\"output_label\")\r\n    # provide a tf.estimator spec for PREDICT\r\n    predictions_dict = {\"score\": probs,\r\n                        \"label\": predictions}\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions_output = tf.estimator.export.PredictOutput(predictions_dict)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=predictions_dict,\r\n                                          export_outputs={\r\n                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output\r\n                                          })\r\n    # calculate loss\r\n    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), _NUM_CLASSES)\r\n    gamma = 1.5\r\n    weights = tf.reduce_sum(tf.multiply(onehot_labels, tf.pow(1. - probs, gamma)), axis=-1)\r\n    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits, weights=weights)\r\n    accuracy = tf.metrics.accuracy(labels=labels,\r\n                                   predictions=predictions)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        lr = 0.001\r\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(update_ops):\r\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n        tensors_to_log = {'batch_accuracy': accuracy[1],\r\n                          'logits': logits,\r\n                          'label': labels}\r\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=[logging_hook])\r\n    else:\r\n        eval_metric_ops = {\"accuracy\": accuracy}\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=eval_metric_ops)\r\n```"}