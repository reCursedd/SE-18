{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361247033", "html_url": "https://github.com/tensorflow/tensorflow/issues/16455#issuecomment-361247033", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16455", "id": 361247033, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTI0NzAzMw==", "user": {"login": "CasiaFan", "id": 10608984, "node_id": "MDQ6VXNlcjEwNjA4OTg0", "avatar_url": "https://avatars3.githubusercontent.com/u/10608984?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CasiaFan", "html_url": "https://github.com/CasiaFan", "followers_url": "https://api.github.com/users/CasiaFan/followers", "following_url": "https://api.github.com/users/CasiaFan/following{/other_user}", "gists_url": "https://api.github.com/users/CasiaFan/gists{/gist_id}", "starred_url": "https://api.github.com/users/CasiaFan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CasiaFan/subscriptions", "organizations_url": "https://api.github.com/users/CasiaFan/orgs", "repos_url": "https://api.github.com/users/CasiaFan/repos", "events_url": "https://api.github.com/users/CasiaFan/events{/privacy}", "received_events_url": "https://api.github.com/users/CasiaFan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-29T13:36:14Z", "updated_at": "2018-01-29T13:36:14Z", "author_association": "NONE", "body_html": "<p>It seems that my BN layer usage is wrong. Based on Tensorflow <code>tf.layers.batch_normalization()</code> <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\" rel=\"nofollow\">document</a>,</p>\n<blockquote>\n<p>Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_optf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op</p>\n</blockquote>\n<p>train_op should be included in with control_dependancy scope:  <br></p>\n<div class=\"highlight highlight-source-python\"><pre>update_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n<span class=\"pl-k\">with</span> tf.control_dependencies(update_ops):\n  train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss)</pre></div>\n<p>Modify upper script into following snippet and it works fine now!</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">...</span>\noptimizer <span class=\"pl-k\">=</span> tf.train.RMSPropOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>lr, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train optimizer</span>\nupdate_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n<span class=\"pl-k\">with</span> tf.control_dependencies(update_ops):\n    train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_global_step())\n<span class=\"pl-c1\">...</span></pre></div>", "body_text": "It seems that my BN layer usage is wrong. Based on Tensorflow tf.layers.batch_normalization() document,\n\nNote: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_optf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op\n\ntrain_op should be included in with control_dependancy scope:  \nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n  train_op = optimizer.minimize(loss)\nModify upper script into following snippet and it works fine now!\n...\noptimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\n# train optimizer\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n...", "body": "It seems that my BN layer usage is wrong. Based on Tensorflow `tf.layers.batch_normalization()` [document](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization), \r\n> Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_optf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op\r\n\r\ntrain_op should be included in with control_dependancy scope:  <br>\r\n```python\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n  train_op = optimizer.minimize(loss)\r\n```\r\n\r\nModify upper script into following snippet and it works fine now!\r\n```python\r\n...\r\noptimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\r\n# train optimizer\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n...\r\n```"}