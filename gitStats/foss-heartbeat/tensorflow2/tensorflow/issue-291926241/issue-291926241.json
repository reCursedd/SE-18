{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16455", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16455/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16455/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16455/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16455", "id": 291926241, "node_id": "MDU6SXNzdWUyOTE5MjYyNDE=", "number": 16455, "title": "Set training=True in BatchNormalization layer causes evaluation error in custom Estimator model", "user": {"login": "CasiaFan", "id": 10608984, "node_id": "MDQ6VXNlcjEwNjA4OTg0", "avatar_url": "https://avatars3.githubusercontent.com/u/10608984?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CasiaFan", "html_url": "https://github.com/CasiaFan", "followers_url": "https://api.github.com/users/CasiaFan/followers", "following_url": "https://api.github.com/users/CasiaFan/following{/other_user}", "gists_url": "https://api.github.com/users/CasiaFan/gists{/gist_id}", "starred_url": "https://api.github.com/users/CasiaFan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CasiaFan/subscriptions", "organizations_url": "https://api.github.com/users/CasiaFan/orgs", "repos_url": "https://api.github.com/users/CasiaFan/repos", "events_url": "https://api.github.com/users/CasiaFan/events{/privacy}", "received_events_url": "https://api.github.com/users/CasiaFan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-01-26T14:58:00Z", "updated_at": "2018-11-23T06:14:31Z", "closed_at": "2018-01-29T13:36:24Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.1</li>\n<li><strong>Python version</strong>:  3.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/6</li>\n<li><strong>GPU model and memory</strong>: GeForce 1080ti, 11G</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I define a custom estimator model for classification following <a href=\"https://www.tensorflow.org/extend/estimators\" rel=\"nofollow\">this document</a>. <strong>Cifar10</strong> dataset is used for test and network framework is <strong>xception</strong> rewritten in tensorflow. But when using <code>estimator.train_and_evaluate()</code> to train and evaluate the model repeatedly, I find evaluation accuracy dones't improve while training accuracy is normally increasing with training. Inspired by tensorflow official <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/resnet.py\">resnet estimator example</a>: <br></p>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv_layer1<span class=\"pl-pds\">'</span></span>):\n    net <span class=\"pl-k\">=</span> tf.layers.conv2d(\n        x,\n        <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n        <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">7</span>,\n        <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu)\n    net <span class=\"pl-k\">=</span> tf.layers.batch_normalization(net)   <span class=\"pl-c\"><span class=\"pl-c\">#</span> no training status, default is False</span></pre></div>\n<p>I turn off <code>training=is_training</code> in <code>tf.layers.batch_normalization()</code>, both training and evaluation do work normally. For estimator model_fn is used multiple times (see <a href=\"https://github.com/tensorflow/tensorflow/issues/13895\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/13895/hovercard\">issue 13895</a>),  so is this issue related to graph reuse in BN layer and if training status option could be set to enable BN layer to act differently during training and evaluation/predict?</p>\n<p>BTW, same issue occurs when using <code>keras</code> or <code>slim</code> instead of <code>tf.layers</code> to construct network architecture.</p>\n<h3>Source code / logs</h3>\n<p><strong>Network architecture:</strong> <br></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">tf_xception</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">input_shape</span>, <span class=\"pl-smi\">pooling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> is_training = False  # manually set False to disable training option</span>\n    x <span class=\"pl-k\">=</span> tf.layers.conv2d(features, <span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block1_conv1<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block1_conv1_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block1_conv1_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.conv2d(x, <span class=\"pl-c1\">64</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block1_conv2<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block1_conv2_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block1_conv2_act<span class=\"pl-pds\">'</span></span>)\n\n    residual <span class=\"pl-k\">=</span> tf.layers.conv2d(x, <span class=\"pl-c1\">128</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n                      <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    residual <span class=\"pl-k\">=</span> tf.layers.batch_normalization(residual, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)\n\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">128</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block2_sepconv1<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block2_sepconv1_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block2_sepconv2_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">128</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block2_sepconv2<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block2_sepconv2_bn<span class=\"pl-pds\">'</span></span>)\n\n    x <span class=\"pl-k\">=</span> tf.layers.max_pooling2d(x, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block2_pool<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.add(x, residual, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block2_add<span class=\"pl-pds\">'</span></span>)\n\n    residual <span class=\"pl-k\">=</span> tf.layers.conv2d(x, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n                      <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    residual <span class=\"pl-k\">=</span> tf.layers.batch_normalization(residual, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)\n\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block3_sepconv1_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block3_sepconv1<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block3_sepconv1_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block3_sepconv2_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">256</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block3_sepconv2<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block3_sepconv2_bn<span class=\"pl-pds\">'</span></span>)\n\n    x <span class=\"pl-k\">=</span> tf.layers.max_pooling2d(x, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block3_pool<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.add(x, residual, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>block3_add<span class=\"pl-pds\">\"</span></span>)\n\n    residual <span class=\"pl-k\">=</span> tf.layers.conv2d(x, <span class=\"pl-c1\">728</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n                      <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    residual <span class=\"pl-k\">=</span> tf.layers.batch_normalization(residual, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)\n\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block4_sepconv1_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">728</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block4_sepconv1<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block4_sepconv1_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block4_sepconv2_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">728</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block4_sepconv2<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block4_sepconv2_bn<span class=\"pl-pds\">'</span></span>)\n\n    x <span class=\"pl-k\">=</span> tf.layers.max_pooling2d(x, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block4_pool<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.add(x, residual, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>block4_add<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">8</span>):\n        residual <span class=\"pl-k\">=</span> x\n        prefix <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>block<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">5</span>)\n\n        x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv1_act<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">728</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv1<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv1_bn<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv2_act<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">728</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv2<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv2_bn<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv3_act<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">728</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv3<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_sepconv3_bn<span class=\"pl-pds\">'</span></span>)\n\n        x <span class=\"pl-k\">=</span> tf.add(x, residual, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>prefix<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_add<span class=\"pl-pds\">\"</span></span>)\n\n    residual <span class=\"pl-k\">=</span> tf.layers.conv2d(x, <span class=\"pl-c1\">1024</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n                      <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    residual <span class=\"pl-k\">=</span> tf.layers.batch_normalization(residual, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)\n\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block13_sepconv1_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">728</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block13_sepconv1<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block13_sepconv1_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block13_sepconv2_act<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">1024</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block13_sepconv2<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block13_sepconv2_bn<span class=\"pl-pds\">'</span></span>)\n\n    x <span class=\"pl-k\">=</span> tf.layers.max_pooling2d(x, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block13_pool<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.add(x, residual, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>block13_add<span class=\"pl-pds\">\"</span></span>)\n\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">1536</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block14_sepconv1<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block14_sepconv1_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block14_sepconv1_act<span class=\"pl-pds\">'</span></span>)\n\n    x <span class=\"pl-k\">=</span> tf.layers.separable_conv2d(x, <span class=\"pl-c1\">2048</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block14_sepconv2<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block14_sepconv2_bn<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>block14_sepconv2_act<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> replace conv layer with fc</span>\n    x <span class=\"pl-k\">=</span> tf.layers.average_pooling2d(x, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>global_average_pooling<span class=\"pl-pds\">\"</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.conv2d(x, <span class=\"pl-c1\">2048</span>, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>block15_conv1<span class=\"pl-pds\">\"</span></span>)\n    x <span class=\"pl-k\">=</span> tf.layers.conv2d(x, classes, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>block15_conv2<span class=\"pl-pds\">\"</span></span>)\n    x <span class=\"pl-k\">=</span> tf.squeeze(x, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>logits<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">return</span> x</pre></div>\n<p><strong>model_fn:</strong> <br></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">params</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> check if training stage</span>\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        is_training <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    <span class=\"pl-k\">else</span>:\n        is_training <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n    input_tensor <span class=\"pl-k\">=</span> features[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>]\n    logits <span class=\"pl-k\">=</span> tf_xception(input_tensor, <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training)\n    probs <span class=\"pl-k\">=</span> tf.nn.softmax(logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_score<span class=\"pl-pds\">\"</span></span>)\n    predictions <span class=\"pl-k\">=</span> tf.argmax(probs, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output_label<span class=\"pl-pds\">\"</span></span>)\n    onehot_labels <span class=\"pl-k\">=</span> tf.one_hot(tf.cast(labels, tf.int32), <span class=\"pl-c1\">10</span>)\n    predictions_dict <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>score<span class=\"pl-pds\">\"</span></span>: probs,\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>label<span class=\"pl-pds\">\"</span></span>: predictions}\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">PREDICT</span>:\n        predictions_output <span class=\"pl-k\">=</span> tf.estimator.export.PredictOutput(predictions_dict)\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n                                          <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions_dict,\n                                          <span class=\"pl-v\">export_outputs</span><span class=\"pl-k\">=</span>{\n                                              tf.saved_model.signature_constants.<span class=\"pl-c1\">DEFAULT_SERVING_SIGNATURE_DEF_KEY</span>: predictions_output\n                                          })\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate loss</span>\n    loss <span class=\"pl-k\">=</span> tf.losses.softmax_cross_entropy(onehot_labels, logits)\n    accuracy <span class=\"pl-k\">=</span> tf.metrics.accuracy(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels,\n                                   <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions)\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        lr <span class=\"pl-k\">=</span> params.learning_rate\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> train optimizer</span>\n        optimizer <span class=\"pl-k\">=</span> tf.train.RMSPropOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>lr, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n        train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_global_step())\n        tensors_to_log <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_accuracy<span class=\"pl-pds\">'</span></span>: accuracy[<span class=\"pl-c1\">1</span>]}\n        logging_hook <span class=\"pl-k\">=</span> tf.train.LoggingTensorHook(<span class=\"pl-v\">tensors</span><span class=\"pl-k\">=</span>tensors_to_log, <span class=\"pl-v\">every_n_iter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n                                          <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss,\n                                          <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op,\n                                          <span class=\"pl-v\">training_hooks</span><span class=\"pl-k\">=</span>[logging_hook])\n    <span class=\"pl-k\">else</span>:\n        eval_metric_ops <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>: accuracy}\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n                                          <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss,\n                                          <span class=\"pl-v\">eval_metric_ops</span><span class=\"pl-k\">=</span>eval_metric_ops)</pre></div>\n<p><strong>If the training status set True in training and False in evaluation</strong><br><br>\n<strong>train logs:</strong><br></p>\n<pre><code>INFO:tensorflow:Saving checkpoints for 1 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.3025837, step = 1\nINFO:tensorflow:batch_accuracy = 0.109375\nINFO:tensorflow:global_step/sec: 3.50983\nINFO:tensorflow:loss = 2.3048878, step = 101 (28.492 sec)\nINFO:tensorflow:Saving checkpoints for 185 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.3093615.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-185\nINFO:tensorflow:Saving checkpoints for 186 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.2975698, step = 186\nINFO:tensorflow:batch_accuracy = 0.09375\nINFO:tensorflow:global_step/sec: 3.47248\nINFO:tensorflow:loss = 2.3078504, step = 286 (28.798 sec)\nINFO:tensorflow:Saving checkpoints for 374 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.290754.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-374\nINFO:tensorflow:Saving checkpoints for 375 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.2987142, step = 375\nINFO:tensorflow:batch_accuracy = 0.140625\nINFO:tensorflow:global_step/sec: 3.50966\nINFO:tensorflow:loss = 2.0407405, step = 475 (28.493 sec)\nINFO:tensorflow:Saving checkpoints for 560 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.1280906.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-560\nINFO:tensorflow:Saving checkpoints for 561 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.0747793, step = 561\nINFO:tensorflow:batch_accuracy = 0.203125\nINFO:tensorflow:global_step/sec: 3.31447\nINFO:tensorflow:loss = 2.1767468, step = 661 (30.171 sec)\nINFO:tensorflow:Saving checkpoints for 740 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 1.9530052.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-740\nINFO:tensorflow:Saving checkpoints for 741 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 1.9676144, step = 741\nINFO:tensorflow:batch_accuracy = 0.296875\nINFO:tensorflow:global_step/sec: 3.50441\nINFO:tensorflow:loss = 1.8766258, step = 841 (28.536 sec)\nINFO:tensorflow:Saving checkpoints for 930 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 1.884157.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-930\nINFO:tensorflow:Saving checkpoints for 931 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 1.8624167, step = 931\nINFO:tensorflow:batch_accuracy = 0.296875\nINFO:tensorflow:global_step/sec: 3.30778\nINFO:tensorflow:loss = 1.7580669, step = 1031 (30.232 sec)\nINFO:tensorflow:Saving checkpoints for 1112 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 1.9509349.\n...\n</code></pre>\n<p><strong>eval log:</strong></p>\n<pre><code>INFO:tensorflow:Saving dict for global step 170: accuracy = 0.099306434, global_step = 170, loss = 2.3029344\nINFO:tensorflow:Saving dict for global step 348: accuracy = 0.11751261, global_step = 348, loss = 2.2920265\nINFO:tensorflow:Saving dict for global step 528: accuracy = 0.13106872, global_step = 528, loss = 2.5031097\nINFO:tensorflow:Saving dict for global step 697: accuracy = 0.085986756, global_step = 697, loss = 30.668789\nINFO:tensorflow:Saving dict for global step 871: accuracy = 0.10009458, global_step = 871, loss = 47931.96\n...\n</code></pre>\n<p><strong>accuracy round initial 0.1 and loss increase ridiculously !!!!</strong></p>\n<p><strong>If training status set False in both stage:</strong><br><br>\n<strong>train log:</strong> similar to above train log</p>\n<p><strong>eval log:</strong> <br></p>\n<pre><code>INFO:tensorflow:Saving dict for global step 185: accuracy = 0.1012768, global_step = 185, loss = 2.3037012\nINFO:tensorflow:Saving dict for global step 374: accuracy = 0.10001576, global_step = 374, loss = 2.3124988\nINFO:tensorflow:Saving dict for global step 560: accuracy = 0.20081967, global_step = 560, loss = 2.0881999\nINFO:tensorflow:Saving dict for global step 740: accuracy = 0.26134932, global_step = 740, loss = 2.0297167\nINFO:tensorflow:Saving dict for global step 930: accuracy = 0.26379257, global_step = 930, loss = 1.9529407\nINFO:tensorflow:Saving dict for global step 1112: accuracy = 0.3454445, global_step = 1112, loss = 1.832811\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu16.04\nTensorFlow installed from (source or binary): pip install\nTensorFlow version (use command below): 1.4.1\nPython version:  3.5.2\nCUDA/cuDNN version: 8.0/6\nGPU model and memory: GeForce 1080ti, 11G\n\nDescribe the problem\nI define a custom estimator model for classification following this document. Cifar10 dataset is used for test and network framework is xception rewritten in tensorflow. But when using estimator.train_and_evaluate() to train and evaluate the model repeatedly, I find evaluation accuracy dones't improve while training accuracy is normally increasing with training. Inspired by tensorflow official resnet estimator example: \n  with tf.variable_scope('conv_layer1'):\n    net = tf.layers.conv2d(\n        x,\n        filters=64,\n        kernel_size=7,\n        activation=tf.nn.relu)\n    net = tf.layers.batch_normalization(net)   # no training status, default is False\nI turn off training=is_training in tf.layers.batch_normalization(), both training and evaluation do work normally. For estimator model_fn is used multiple times (see issue 13895),  so is this issue related to graph reuse in BN layer and if training status option could be set to enable BN layer to act differently during training and evaluation/predict?\nBTW, same issue occurs when using keras or slim instead of tf.layers to construct network architecture.\nSource code / logs\nNetwork architecture: \ndef tf_xception(features, input_shape, pooling=None, classes=2, is_training=True):\n    # is_training = False  # manually set False to disable training option\n    x = tf.layers.conv2d(features, 32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv1_bn')\n    x = tf.nn.relu(x, name='block1_conv1_act')\n    x = tf.layers.conv2d(x, 64, (3, 3), use_bias=False, name='block1_conv2')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv2_bn')\n    x = tf.nn.relu(x, name='block1_conv2_act')\n\n    residual = tf.layers.conv2d(x, 128, (1, 1), strides=(2, 2),\n                      padding='same', use_bias=False)\n    residual = tf.layers.batch_normalization(residual, training=is_training)\n\n    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv1_bn')\n    x = tf.nn.relu(x, name='block2_sepconv2_act')\n    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv2_bn')\n\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block2_pool')\n    x = tf.add(x, residual, name='block2_add')\n\n    residual = tf.layers.conv2d(x, 256, (1, 1), strides=(2, 2),\n                      padding='same', use_bias=False)\n    residual = tf.layers.batch_normalization(residual, training=is_training)\n\n    x = tf.nn.relu(x, name='block3_sepconv1_act')\n    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv1_bn')\n    x = tf.nn.relu(x, name='block3_sepconv2_act')\n    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv2_bn')\n\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block3_pool')\n    x = tf.add(x, residual, name=\"block3_add\")\n\n    residual = tf.layers.conv2d(x, 728, (1, 1), strides=(2, 2),\n                      padding='same', use_bias=False)\n    residual = tf.layers.batch_normalization(residual, training=is_training)\n\n    x = tf.nn.relu(x, name='block4_sepconv1_act')\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv1_bn')\n    x = tf.nn.relu(x, name='block4_sepconv2_act')\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv2_bn')\n\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block4_pool')\n    x = tf.add(x, residual, name=\"block4_add\")\n\n    for i in range(8):\n        residual = x\n        prefix = 'block' + str(i + 5)\n\n        x = tf.nn.relu(x, name=prefix + '_sepconv1_act')\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv1_bn')\n        x = tf.nn.relu(x, name=prefix + '_sepconv2_act')\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv2_bn')\n        x = tf.nn.relu(x, name=prefix + '_sepconv3_act')\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv3_bn')\n\n        x = tf.add(x, residual, name=prefix+\"_add\")\n\n    residual = tf.layers.conv2d(x, 1024, (1, 1), strides=(2, 2),\n                      padding='same', use_bias=False)\n    residual = tf.layers.batch_normalization(residual, training=is_training)\n\n    x = tf.nn.relu(x, name='block13_sepconv1_act')\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv1_bn')\n    x = tf.nn.relu(x, name='block13_sepconv2_act')\n    x = tf.layers.separable_conv2d(x, 1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv2_bn')\n\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block13_pool')\n    x = tf.add(x, residual, name=\"block13_add\")\n\n    x = tf.layers.separable_conv2d(x, 1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv1_bn')\n    x = tf.nn.relu(x, name='block14_sepconv1_act')\n\n    x = tf.layers.separable_conv2d(x, 2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')\n    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv2_bn')\n    x = tf.nn.relu(x, name='block14_sepconv2_act')\n    # replace conv layer with fc\n    x = tf.layers.average_pooling2d(x, (3, 3), (2, 2), name=\"global_average_pooling\")\n    x = tf.layers.conv2d(x, 2048, [1, 1], activation=None, name=\"block15_conv1\")\n    x = tf.layers.conv2d(x, classes, [1, 1], activation=None, name=\"block15_conv2\")\n    x = tf.squeeze(x, axis=[1, 2], name=\"logits\")\n    return x\nmodel_fn: \ndef model_fn(features, labels, mode, params):\n    # check if training stage\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        is_training = True\n    else:\n        is_training = False\n    input_tensor = features[\"input\"]\n    logits = tf_xception(input_tensor, input_shape=(96, 96, 3), classes=10, is_training=is_training)\n    probs = tf.nn.softmax(logits, name=\"output_score\")\n    predictions = tf.argmax(probs, axis=-1, name=\"output_label\")\n    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), 10)\n    predictions_dict = {\"score\": probs,\n                        \"label\": predictions}\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        predictions_output = tf.estimator.export.PredictOutput(predictions_dict)\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          predictions=predictions_dict,\n                                          export_outputs={\n                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output\n                                          })\n    # calculate loss\n    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)\n    accuracy = tf.metrics.accuracy(labels=labels,\n                                   predictions=predictions)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        lr = params.learning_rate\n        # train optimizer\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n        tensors_to_log = {'batch_accuracy': accuracy[1]}\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          loss=loss,\n                                          train_op=train_op,\n                                          training_hooks=[logging_hook])\n    else:\n        eval_metric_ops = {\"accuracy\": accuracy}\n        return tf.estimator.EstimatorSpec(mode=mode,\n                                          loss=loss,\n                                          eval_metric_ops=eval_metric_ops)\nIf the training status set True in training and False in evaluation\ntrain logs:\nINFO:tensorflow:Saving checkpoints for 1 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.3025837, step = 1\nINFO:tensorflow:batch_accuracy = 0.109375\nINFO:tensorflow:global_step/sec: 3.50983\nINFO:tensorflow:loss = 2.3048878, step = 101 (28.492 sec)\nINFO:tensorflow:Saving checkpoints for 185 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.3093615.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-185\nINFO:tensorflow:Saving checkpoints for 186 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.2975698, step = 186\nINFO:tensorflow:batch_accuracy = 0.09375\nINFO:tensorflow:global_step/sec: 3.47248\nINFO:tensorflow:loss = 2.3078504, step = 286 (28.798 sec)\nINFO:tensorflow:Saving checkpoints for 374 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.290754.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-374\nINFO:tensorflow:Saving checkpoints for 375 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.2987142, step = 375\nINFO:tensorflow:batch_accuracy = 0.140625\nINFO:tensorflow:global_step/sec: 3.50966\nINFO:tensorflow:loss = 2.0407405, step = 475 (28.493 sec)\nINFO:tensorflow:Saving checkpoints for 560 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.1280906.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-560\nINFO:tensorflow:Saving checkpoints for 561 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 2.0747793, step = 561\nINFO:tensorflow:batch_accuracy = 0.203125\nINFO:tensorflow:global_step/sec: 3.31447\nINFO:tensorflow:loss = 2.1767468, step = 661 (30.171 sec)\nINFO:tensorflow:Saving checkpoints for 740 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 1.9530052.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-740\nINFO:tensorflow:Saving checkpoints for 741 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 1.9676144, step = 741\nINFO:tensorflow:batch_accuracy = 0.296875\nINFO:tensorflow:global_step/sec: 3.50441\nINFO:tensorflow:loss = 1.8766258, step = 841 (28.536 sec)\nINFO:tensorflow:Saving checkpoints for 930 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 1.884157.\n...\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-930\nINFO:tensorflow:Saving checkpoints for 931 into train_episode5/model.ckpt.\nINFO:tensorflow:loss = 1.8624167, step = 931\nINFO:tensorflow:batch_accuracy = 0.296875\nINFO:tensorflow:global_step/sec: 3.30778\nINFO:tensorflow:loss = 1.7580669, step = 1031 (30.232 sec)\nINFO:tensorflow:Saving checkpoints for 1112 into train_episode5/model.ckpt.\nINFO:tensorflow:Loss for final step: 1.9509349.\n...\n\neval log:\nINFO:tensorflow:Saving dict for global step 170: accuracy = 0.099306434, global_step = 170, loss = 2.3029344\nINFO:tensorflow:Saving dict for global step 348: accuracy = 0.11751261, global_step = 348, loss = 2.2920265\nINFO:tensorflow:Saving dict for global step 528: accuracy = 0.13106872, global_step = 528, loss = 2.5031097\nINFO:tensorflow:Saving dict for global step 697: accuracy = 0.085986756, global_step = 697, loss = 30.668789\nINFO:tensorflow:Saving dict for global step 871: accuracy = 0.10009458, global_step = 871, loss = 47931.96\n...\n\naccuracy round initial 0.1 and loss increase ridiculously !!!!\nIf training status set False in both stage:\ntrain log: similar to above train log\neval log: \nINFO:tensorflow:Saving dict for global step 185: accuracy = 0.1012768, global_step = 185, loss = 2.3037012\nINFO:tensorflow:Saving dict for global step 374: accuracy = 0.10001576, global_step = 374, loss = 2.3124988\nINFO:tensorflow:Saving dict for global step 560: accuracy = 0.20081967, global_step = 560, loss = 2.0881999\nINFO:tensorflow:Saving dict for global step 740: accuracy = 0.26134932, global_step = 740, loss = 2.0297167\nINFO:tensorflow:Saving dict for global step 930: accuracy = 0.26379257, global_step = 930, loss = 1.9529407\nINFO:tensorflow:Saving dict for global step 1112: accuracy = 0.3454445, global_step = 1112, loss = 1.832811", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu16.04\r\n- **TensorFlow installed from (source or binary)**: pip install \r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**:  3.5.2\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: GeForce 1080ti, 11G\r\n\r\n### Describe the problem\r\nI define a custom estimator model for classification following [this document](https://www.tensorflow.org/extend/estimators). **Cifar10** dataset is used for test and network framework is **xception** rewritten in tensorflow. But when using `estimator.train_and_evaluate()` to train and evaluate the model repeatedly, I find evaluation accuracy dones't improve while training accuracy is normally increasing with training. Inspired by tensorflow official [resnet estimator example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/resnet.py): <br>\r\n```python\r\n  with tf.variable_scope('conv_layer1'):\r\n    net = tf.layers.conv2d(\r\n        x,\r\n        filters=64,\r\n        kernel_size=7,\r\n        activation=tf.nn.relu)\r\n    net = tf.layers.batch_normalization(net)   # no training status, default is False\r\n```\r\n \r\nI turn off `training=is_training` in `tf.layers.batch_normalization()`, both training and evaluation do work normally. For estimator model_fn is used multiple times (see [issue 13895](https://github.com/tensorflow/tensorflow/issues/13895)),  so is this issue related to graph reuse in BN layer and if training status option could be set to enable BN layer to act differently during training and evaluation/predict?\r\n\r\nBTW, same issue occurs when using `keras` or `slim` instead of `tf.layers` to construct network architecture.\r\n\r\n### Source code / logs\r\n**Network architecture:** <br>\r\n```python\r\ndef tf_xception(features, input_shape, pooling=None, classes=2, is_training=True):\r\n    # is_training = False  # manually set False to disable training option\r\n    x = tf.layers.conv2d(features, 32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv1_bn')\r\n    x = tf.nn.relu(x, name='block1_conv1_act')\r\n    x = tf.layers.conv2d(x, 64, (3, 3), use_bias=False, name='block1_conv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv2_bn')\r\n    x = tf.nn.relu(x, name='block1_conv2_act')\r\n\r\n    residual = tf.layers.conv2d(x, 128, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block2_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block2_pool')\r\n    x = tf.add(x, residual, name='block2_add')\r\n\r\n    residual = tf.layers.conv2d(x, 256, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.nn.relu(x, name='block3_sepconv1_act')\r\n    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block3_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block3_pool')\r\n    x = tf.add(x, residual, name=\"block3_add\")\r\n\r\n    residual = tf.layers.conv2d(x, 728, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.nn.relu(x, name='block4_sepconv1_act')\r\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block4_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block4_pool')\r\n    x = tf.add(x, residual, name=\"block4_add\")\r\n\r\n    for i in range(8):\r\n        residual = x\r\n        prefix = 'block' + str(i + 5)\r\n\r\n        x = tf.nn.relu(x, name=prefix + '_sepconv1_act')\r\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')\r\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv1_bn')\r\n        x = tf.nn.relu(x, name=prefix + '_sepconv2_act')\r\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')\r\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv2_bn')\r\n        x = tf.nn.relu(x, name=prefix + '_sepconv3_act')\r\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')\r\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv3_bn')\r\n\r\n        x = tf.add(x, residual, name=prefix+\"_add\")\r\n\r\n    residual = tf.layers.conv2d(x, 1024, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.nn.relu(x, name='block13_sepconv1_act')\r\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block13_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block13_pool')\r\n    x = tf.add(x, residual, name=\"block13_add\")\r\n\r\n    x = tf.layers.separable_conv2d(x, 1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block14_sepconv1_act')\r\n\r\n    x = tf.layers.separable_conv2d(x, 2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv2_bn')\r\n    x = tf.nn.relu(x, name='block14_sepconv2_act')\r\n    # replace conv layer with fc\r\n    x = tf.layers.average_pooling2d(x, (3, 3), (2, 2), name=\"global_average_pooling\")\r\n    x = tf.layers.conv2d(x, 2048, [1, 1], activation=None, name=\"block15_conv1\")\r\n    x = tf.layers.conv2d(x, classes, [1, 1], activation=None, name=\"block15_conv2\")\r\n    x = tf.squeeze(x, axis=[1, 2], name=\"logits\")\r\n    return x\r\n```\r\n\r\n**model_fn:** <br>\r\n```python\r\ndef model_fn(features, labels, mode, params):\r\n    # check if training stage\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        is_training = True\r\n    else:\r\n        is_training = False\r\n    input_tensor = features[\"input\"]\r\n    logits = tf_xception(input_tensor, input_shape=(96, 96, 3), classes=10, is_training=is_training)\r\n    probs = tf.nn.softmax(logits, name=\"output_score\")\r\n    predictions = tf.argmax(probs, axis=-1, name=\"output_label\")\r\n    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), 10)\r\n    predictions_dict = {\"score\": probs,\r\n                        \"label\": predictions}\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions_output = tf.estimator.export.PredictOutput(predictions_dict)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=predictions_dict,\r\n                                          export_outputs={\r\n                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output\r\n                                          })\r\n    # calculate loss\r\n    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)\r\n    accuracy = tf.metrics.accuracy(labels=labels,\r\n                                   predictions=predictions)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        lr = params.learning_rate\r\n        # train optimizer\r\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\r\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n        tensors_to_log = {'batch_accuracy': accuracy[1]}\r\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=[logging_hook])\r\n    else:\r\n        eval_metric_ops = {\"accuracy\": accuracy}\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=eval_metric_ops)\r\n```\r\n\r\n**If the training status set True in training and False in evaluation**<br>\r\n**train logs:**<br>\r\n```\r\nINFO:tensorflow:Saving checkpoints for 1 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.3025837, step = 1\r\nINFO:tensorflow:batch_accuracy = 0.109375\r\nINFO:tensorflow:global_step/sec: 3.50983\r\nINFO:tensorflow:loss = 2.3048878, step = 101 (28.492 sec)\r\nINFO:tensorflow:Saving checkpoints for 185 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 2.3093615.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-185\r\nINFO:tensorflow:Saving checkpoints for 186 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.2975698, step = 186\r\nINFO:tensorflow:batch_accuracy = 0.09375\r\nINFO:tensorflow:global_step/sec: 3.47248\r\nINFO:tensorflow:loss = 2.3078504, step = 286 (28.798 sec)\r\nINFO:tensorflow:Saving checkpoints for 374 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 2.290754.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-374\r\nINFO:tensorflow:Saving checkpoints for 375 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.2987142, step = 375\r\nINFO:tensorflow:batch_accuracy = 0.140625\r\nINFO:tensorflow:global_step/sec: 3.50966\r\nINFO:tensorflow:loss = 2.0407405, step = 475 (28.493 sec)\r\nINFO:tensorflow:Saving checkpoints for 560 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 2.1280906.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-560\r\nINFO:tensorflow:Saving checkpoints for 561 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.0747793, step = 561\r\nINFO:tensorflow:batch_accuracy = 0.203125\r\nINFO:tensorflow:global_step/sec: 3.31447\r\nINFO:tensorflow:loss = 2.1767468, step = 661 (30.171 sec)\r\nINFO:tensorflow:Saving checkpoints for 740 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.9530052.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-740\r\nINFO:tensorflow:Saving checkpoints for 741 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 1.9676144, step = 741\r\nINFO:tensorflow:batch_accuracy = 0.296875\r\nINFO:tensorflow:global_step/sec: 3.50441\r\nINFO:tensorflow:loss = 1.8766258, step = 841 (28.536 sec)\r\nINFO:tensorflow:Saving checkpoints for 930 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.884157.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-930\r\nINFO:tensorflow:Saving checkpoints for 931 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 1.8624167, step = 931\r\nINFO:tensorflow:batch_accuracy = 0.296875\r\nINFO:tensorflow:global_step/sec: 3.30778\r\nINFO:tensorflow:loss = 1.7580669, step = 1031 (30.232 sec)\r\nINFO:tensorflow:Saving checkpoints for 1112 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.9509349.\r\n...\r\n```\r\n\r\n**eval log:**\r\n```\r\nINFO:tensorflow:Saving dict for global step 170: accuracy = 0.099306434, global_step = 170, loss = 2.3029344\r\nINFO:tensorflow:Saving dict for global step 348: accuracy = 0.11751261, global_step = 348, loss = 2.2920265\r\nINFO:tensorflow:Saving dict for global step 528: accuracy = 0.13106872, global_step = 528, loss = 2.5031097\r\nINFO:tensorflow:Saving dict for global step 697: accuracy = 0.085986756, global_step = 697, loss = 30.668789\r\nINFO:tensorflow:Saving dict for global step 871: accuracy = 0.10009458, global_step = 871, loss = 47931.96\r\n...\r\n```\r\n**accuracy round initial 0.1 and loss increase ridiculously !!!!**\r\n\r\n**If training status set False in both stage:**<br>\r\n**train log:** similar to above train log \r\n\r\n**eval log:** <br>\r\n```\r\nINFO:tensorflow:Saving dict for global step 185: accuracy = 0.1012768, global_step = 185, loss = 2.3037012\r\nINFO:tensorflow:Saving dict for global step 374: accuracy = 0.10001576, global_step = 374, loss = 2.3124988\r\nINFO:tensorflow:Saving dict for global step 560: accuracy = 0.20081967, global_step = 560, loss = 2.0881999\r\nINFO:tensorflow:Saving dict for global step 740: accuracy = 0.26134932, global_step = 740, loss = 2.0297167\r\nINFO:tensorflow:Saving dict for global step 930: accuracy = 0.26379257, global_step = 930, loss = 1.9529407\r\nINFO:tensorflow:Saving dict for global step 1112: accuracy = 0.3454445, global_step = 1112, loss = 1.832811\r\n```\r\n"}