{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/373073312", "html_url": "https://github.com/tensorflow/tensorflow/issues/16455#issuecomment-373073312", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16455", "id": 373073312, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzA3MzMxMg==", "user": {"login": "formigone", "id": 852234, "node_id": "MDQ6VXNlcjg1MjIzNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/852234?v=4", "gravatar_id": "", "url": "https://api.github.com/users/formigone", "html_url": "https://github.com/formigone", "followers_url": "https://api.github.com/users/formigone/followers", "following_url": "https://api.github.com/users/formigone/following{/other_user}", "gists_url": "https://api.github.com/users/formigone/gists{/gist_id}", "starred_url": "https://api.github.com/users/formigone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/formigone/subscriptions", "organizations_url": "https://api.github.com/users/formigone/orgs", "repos_url": "https://api.github.com/users/formigone/repos", "events_url": "https://api.github.com/users/formigone/events{/privacy}", "received_events_url": "https://api.github.com/users/formigone/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T15:56:03Z", "updated_at": "2018-03-14T15:56:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Could you post a working example of how to properly set the value for <code>training</code> in <code>tf.layers.batch_normalization()</code>?</p>\n<p>As per the docs (TensorFlow 1.4):</p>\n<blockquote>\n<p>training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). <strong>NOTE</strong>: make sure to set this parameter correctly, or else your training/inference will not work properly.</p>\n</blockquote>\n<p>This is how I'm setting it, but clearly it's not behaving as expected in <code>EVAL</code> and <code>PREDICT</code> modes:</p>\n<pre><code>def model_fn(features, labels, mode, params):\n  training = bool(mode == tf.estimator.ModeKeys.TRAIN)\n  extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n  # ...\n\n  x = tf.layers.batch_normalization(x, training=training)\n\n with tf.control_dependencies(extra_update_ops):\n     train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/852234/37413679-3069a75e-276d-11e8-8691-0cc05bf41ec1.png\"><img width=\"722\" alt=\"tf-bn\" src=\"https://user-images.githubusercontent.com/852234/37413679-3069a75e-276d-11e8-8691-0cc05bf41ec1.png\" style=\"max-width:100%;\"></a></p>\n<p>In the graph above, the red and blue losses are for a network without <code>batch_normalization</code>. Then, using the same training and eval sets and the same architecture (plus <code>batch_normalization</code>), you can see that the pink training loss drops similar to how it did without BN, but the evaluation loss is way off.</p>\n<p>Also, I'm graphing a histogram of the predicted values during training. They are always in a range of values (100, 300). Using the same training dataset to predict (<code>estimator.predict()</code>), the output is now in a range (15, 19). Way off. Looks like the <code>moving_mean</code> and <code>moving_variance</code> are not being saved correctly during training (or not used correctly during EVAL and PREDICT).</p>\n<p>Thanks.</p>", "body_text": "Could you post a working example of how to properly set the value for training in tf.layers.batch_normalization()?\nAs per the docs (TensorFlow 1.4):\n\ntraining: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly.\n\nThis is how I'm setting it, but clearly it's not behaving as expected in EVAL and PREDICT modes:\ndef model_fn(features, labels, mode, params):\n  training = bool(mode == tf.estimator.ModeKeys.TRAIN)\n  extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n  # ...\n\n  x = tf.layers.batch_normalization(x, training=training)\n\n with tf.control_dependencies(extra_update_ops):\n     train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n\n\nIn the graph above, the red and blue losses are for a network without batch_normalization. Then, using the same training and eval sets and the same architecture (plus batch_normalization), you can see that the pink training loss drops similar to how it did without BN, but the evaluation loss is way off.\nAlso, I'm graphing a histogram of the predicted values during training. They are always in a range of values (100, 300). Using the same training dataset to predict (estimator.predict()), the output is now in a range (15, 19). Way off. Looks like the moving_mean and moving_variance are not being saved correctly during training (or not used correctly during EVAL and PREDICT).\nThanks.", "body": "Could you post a working example of how to properly set the value for `training` in `tf.layers.batch_normalization()`? \r\n\r\nAs per the docs (TensorFlow 1.4):\r\n\r\n> training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). **NOTE**: make sure to set this parameter correctly, or else your training/inference will not work properly.\r\n\r\nThis is how I'm setting it, but clearly it's not behaving as expected in `EVAL` and `PREDICT` modes:\r\n\r\n    def model_fn(features, labels, mode, params):\r\n      training = bool(mode == tf.estimator.ModeKeys.TRAIN)\r\n      extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\r\n      # ...\r\n\r\n      x = tf.layers.batch_normalization(x, training=training)\r\n\r\n     with tf.control_dependencies(extra_update_ops):\r\n         train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n\r\n<img width=\"722\" alt=\"tf-bn\" src=\"https://user-images.githubusercontent.com/852234/37413679-3069a75e-276d-11e8-8691-0cc05bf41ec1.png\">\r\n\r\nIn the graph above, the red and blue losses are for a network without `batch_normalization`. Then, using the same training and eval sets and the same architecture (plus `batch_normalization`), you can see that the pink training loss drops similar to how it did without BN, but the evaluation loss is way off. \r\n\r\nAlso, I'm graphing a histogram of the predicted values during training. They are always in a range of values (100, 300). Using the same training dataset to predict (`estimator.predict()`), the output is now in a range (15, 19). Way off. Looks like the `moving_mean` and `moving_variance` are not being saved correctly during training (or not used correctly during EVAL and PREDICT).\r\n\r\nThanks."}