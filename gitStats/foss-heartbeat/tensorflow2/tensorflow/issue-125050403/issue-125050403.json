{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/699", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/699/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/699/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/699/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/699", "id": 125050403, "node_id": "MDU6SXNzdWUxMjUwNTA0MDM=", "number": 699, "title": "Change name of sigmoid_cross_entropy_with_logits to log_loss", "user": {"login": "zackchase", "id": 2390222, "node_id": "MDQ6VXNlcjIzOTAyMjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2390222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zackchase", "html_url": "https://github.com/zackchase", "followers_url": "https://api.github.com/users/zackchase/followers", "following_url": "https://api.github.com/users/zackchase/following{/other_user}", "gists_url": "https://api.github.com/users/zackchase/gists{/gist_id}", "starred_url": "https://api.github.com/users/zackchase/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zackchase/subscriptions", "organizations_url": "https://api.github.com/users/zackchase/orgs", "repos_url": "https://api.github.com/users/zackchase/repos", "events_url": "https://api.github.com/users/zackchase/events{/privacy}", "received_events_url": "https://api.github.com/users/zackchase/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-01-05T21:00:00Z", "updated_at": "2016-06-08T23:46:12Z", "closed_at": "2016-06-08T23:46:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi folks. This name sigmoid_cross_entropy_with_logits struck me as strange. In addition to being a mouthful, we're calling it cross_entropy when it really is just binary cross_entropy calculated separately on every node.</p>\n<p>The more intuitive name I would give to this function would be log_loss or mean_log_loss (if normalized). I think this would be both easier to write, and easier to understand.</p>", "body_text": "Hi folks. This name sigmoid_cross_entropy_with_logits struck me as strange. In addition to being a mouthful, we're calling it cross_entropy when it really is just binary cross_entropy calculated separately on every node.\nThe more intuitive name I would give to this function would be log_loss or mean_log_loss (if normalized). I think this would be both easier to write, and easier to understand.", "body": "Hi folks. This name sigmoid_cross_entropy_with_logits struck me as strange. In addition to being a mouthful, we're calling it cross_entropy when it really is just binary cross_entropy calculated separately on every node. \n\nThe more intuitive name I would give to this function would be log_loss or mean_log_loss (if normalized). I think this would be both easier to write, and easier to understand. \n"}