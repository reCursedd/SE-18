{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/255600005", "html_url": "https://github.com/tensorflow/tensorflow/issues/5118#issuecomment-255600005", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5118", "id": 255600005, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTYwMDAwNQ==", "user": {"login": "briangoodness", "id": 2949027, "node_id": "MDQ6VXNlcjI5NDkwMjc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2949027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/briangoodness", "html_url": "https://github.com/briangoodness", "followers_url": "https://api.github.com/users/briangoodness/followers", "following_url": "https://api.github.com/users/briangoodness/following{/other_user}", "gists_url": "https://api.github.com/users/briangoodness/gists{/gist_id}", "starred_url": "https://api.github.com/users/briangoodness/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/briangoodness/subscriptions", "organizations_url": "https://api.github.com/users/briangoodness/orgs", "repos_url": "https://api.github.com/users/briangoodness/repos", "events_url": "https://api.github.com/users/briangoodness/events{/privacy}", "received_events_url": "https://api.github.com/users/briangoodness/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-23T17:00:03Z", "updated_at": "2016-10-23T17:00:03Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a>, thank you for the help and direction! This makes sense-- I was wondering why the files in the models/rnn folder in the repo were not in sync with those in the python pip installation (e.g., I couldn't find the translate.py file in the python install).</p>\n<p>Per your suggestion, I copied the models/rnn folder from the master branch of the tensorflow repo into the python installation, and re-ran. It downloads and tokenizes the files, and then breaks at the same point-- after line 22,500,000 in the 'giga-fren.release2.fixed.en' file. See below for the precise error.</p>\n<p>I've replicated it on my laptop (8Gb RAM) and desktop (32Gb RAM)-- the code breaks on both machines, at the same location. It seems strange that it would be break at this specific point, not sure if this might be related to memory. In my example below, I toyed with tweaks at the command line (e.g., limiting the training size) to see if it affected success.</p>\n<pre><code>&gt;&gt; python3 translate.py --data_dir ~/tf-data/ --train_dir ~/tf-data/ --size=256 --num_layers=2 --steps_per_checkpoint=50 --max_train_data_size=50000\nPreparing WMT data in /home/user/tf-data/\nDownloading http://www.statmt.org/wmt10/training-giga-fren.tar to /home/user/tf-data/training-giga-fren.tar\nSuccesfully downloaded training-giga-fren.tar 2595102720 bytes\nExtracting tar file /home/user/tf-data/training-giga-fren.tar\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.fr.gz to /home/user/tf-data/giga-fren.release2.fixed.fr\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.en.gz to /home/user/tf-data/giga-fren.release2.fixed.en\nDownloading http://www.statmt.org/wmt15/dev-v2.tgz to /home/user/tf-data/dev-v2.tgz\nSuccesfully downloaded dev-v2.tgz 21393583 bytes\nExtracting tgz file /home/user/tf-data/dev-v2.tgz\nCreating vocabulary /home/user/tf-data/vocab40000.fr from data /home/user/tf-data/giga-fren.release2.fixed.fr\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n  processing line 500000\n\n....\n\n  processing line 22200000\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nCreating vocabulary /home/user/tf-data/vocab40000.en from data /home/user/tf-data/giga-fren.release2.fixed.en\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n\n...\n\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nTokenizing data in /home/user/tf-data/giga-fren.release2.fixed.fr\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in &lt;module&gt;\n    tf.app.run()\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"translate.py\", line 287, in main\n    train()\n  File \"translate.py\", line 147, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 279, in prepare_wmt_data\n    data_to_token_ids(train_path + \".fr\", fr_train_ids_path, fr_vocab_path, tokenizer)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 243, in data_to_token_ids\n    normalize_digits)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 209, in sentence_to_token_ids\n    words = basic_tokenizer(sentence)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 110, in basic_tokenizer\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\nTypeError: cannot use a bytes pattern on a string-like object\n</code></pre>", "body_text": "@asimshankar, thank you for the help and direction! This makes sense-- I was wondering why the files in the models/rnn folder in the repo were not in sync with those in the python pip installation (e.g., I couldn't find the translate.py file in the python install).\nPer your suggestion, I copied the models/rnn folder from the master branch of the tensorflow repo into the python installation, and re-ran. It downloads and tokenizes the files, and then breaks at the same point-- after line 22,500,000 in the 'giga-fren.release2.fixed.en' file. See below for the precise error.\nI've replicated it on my laptop (8Gb RAM) and desktop (32Gb RAM)-- the code breaks on both machines, at the same location. It seems strange that it would be break at this specific point, not sure if this might be related to memory. In my example below, I toyed with tweaks at the command line (e.g., limiting the training size) to see if it affected success.\n>> python3 translate.py --data_dir ~/tf-data/ --train_dir ~/tf-data/ --size=256 --num_layers=2 --steps_per_checkpoint=50 --max_train_data_size=50000\nPreparing WMT data in /home/user/tf-data/\nDownloading http://www.statmt.org/wmt10/training-giga-fren.tar to /home/user/tf-data/training-giga-fren.tar\nSuccesfully downloaded training-giga-fren.tar 2595102720 bytes\nExtracting tar file /home/user/tf-data/training-giga-fren.tar\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.fr.gz to /home/user/tf-data/giga-fren.release2.fixed.fr\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.en.gz to /home/user/tf-data/giga-fren.release2.fixed.en\nDownloading http://www.statmt.org/wmt15/dev-v2.tgz to /home/user/tf-data/dev-v2.tgz\nSuccesfully downloaded dev-v2.tgz 21393583 bytes\nExtracting tgz file /home/user/tf-data/dev-v2.tgz\nCreating vocabulary /home/user/tf-data/vocab40000.fr from data /home/user/tf-data/giga-fren.release2.fixed.fr\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n  processing line 500000\n\n....\n\n  processing line 22200000\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nCreating vocabulary /home/user/tf-data/vocab40000.en from data /home/user/tf-data/giga-fren.release2.fixed.en\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n\n...\n\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nTokenizing data in /home/user/tf-data/giga-fren.release2.fixed.fr\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"translate.py\", line 287, in main\n    train()\n  File \"translate.py\", line 147, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 279, in prepare_wmt_data\n    data_to_token_ids(train_path + \".fr\", fr_train_ids_path, fr_vocab_path, tokenizer)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 243, in data_to_token_ids\n    normalize_digits)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 209, in sentence_to_token_ids\n    words = basic_tokenizer(sentence)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 110, in basic_tokenizer\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\nTypeError: cannot use a bytes pattern on a string-like object", "body": "@asimshankar, thank you for the help and direction! This makes sense-- I was wondering why the files in the models/rnn folder in the repo were not in sync with those in the python pip installation (e.g., I couldn't find the translate.py file in the python install).\n\nPer your suggestion, I copied the models/rnn folder from the master branch of the tensorflow repo into the python installation, and re-ran. It downloads and tokenizes the files, and then breaks at the same point-- after line 22,500,000 in the 'giga-fren.release2.fixed.en' file. See below for the precise error.\n\nI've replicated it on my laptop (8Gb RAM) and desktop (32Gb RAM)-- the code breaks on both machines, at the same location. It seems strange that it would be break at this specific point, not sure if this might be related to memory. In my example below, I toyed with tweaks at the command line (e.g., limiting the training size) to see if it affected success.\n\n```\n>> python3 translate.py --data_dir ~/tf-data/ --train_dir ~/tf-data/ --size=256 --num_layers=2 --steps_per_checkpoint=50 --max_train_data_size=50000\nPreparing WMT data in /home/user/tf-data/\nDownloading http://www.statmt.org/wmt10/training-giga-fren.tar to /home/user/tf-data/training-giga-fren.tar\nSuccesfully downloaded training-giga-fren.tar 2595102720 bytes\nExtracting tar file /home/user/tf-data/training-giga-fren.tar\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.fr.gz to /home/user/tf-data/giga-fren.release2.fixed.fr\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.en.gz to /home/user/tf-data/giga-fren.release2.fixed.en\nDownloading http://www.statmt.org/wmt15/dev-v2.tgz to /home/user/tf-data/dev-v2.tgz\nSuccesfully downloaded dev-v2.tgz 21393583 bytes\nExtracting tgz file /home/user/tf-data/dev-v2.tgz\nCreating vocabulary /home/user/tf-data/vocab40000.fr from data /home/user/tf-data/giga-fren.release2.fixed.fr\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n  processing line 500000\n\n....\n\n  processing line 22200000\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nCreating vocabulary /home/user/tf-data/vocab40000.en from data /home/user/tf-data/giga-fren.release2.fixed.en\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n\n...\n\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nTokenizing data in /home/user/tf-data/giga-fren.release2.fixed.fr\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"translate.py\", line 287, in main\n    train()\n  File \"translate.py\", line 147, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 279, in prepare_wmt_data\n    data_to_token_ids(train_path + \".fr\", fr_train_ids_path, fr_vocab_path, tokenizer)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 243, in data_to_token_ids\n    normalize_digits)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 209, in sentence_to_token_ids\n    words = basic_tokenizer(sentence)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 110, in basic_tokenizer\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\nTypeError: cannot use a bytes pattern on a string-like object\n```\n"}