{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12475", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12475/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12475/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12475/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12475", "id": 251846942, "node_id": "MDU6SXNzdWUyNTE4NDY5NDI=", "number": 12475, "title": "Feature request: sparse_tensor_dense_matmul optimization on GPU", "user": {"login": "stepochkin", "id": 1241615, "node_id": "MDQ6VXNlcjEyNDE2MTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/1241615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stepochkin", "html_url": "https://github.com/stepochkin", "followers_url": "https://api.github.com/users/stepochkin/followers", "following_url": "https://api.github.com/users/stepochkin/following{/other_user}", "gists_url": "https://api.github.com/users/stepochkin/gists{/gist_id}", "starred_url": "https://api.github.com/users/stepochkin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stepochkin/subscriptions", "organizations_url": "https://api.github.com/users/stepochkin/orgs", "repos_url": "https://api.github.com/users/stepochkin/repos", "events_url": "https://api.github.com/users/stepochkin/events{/privacy}", "received_events_url": "https://api.github.com/users/stepochkin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-08-22T06:03:58Z", "updated_at": "2018-01-03T19:55:15Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Debian GNU/Linux 8 (jessie)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.3.0-rc2-0-g2784b1c', '1.3.0-rc2')</li>\n<li><strong>Python version</strong>: 2.7.9</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.3</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0 / 5.0</li>\n<li><strong>GPU model and memory</strong>: Nvidia GeForce GTX TITAN X</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>I would like to optimize sparse_tensor_dense_matmul operation on GPU to process sparse input completely on GPU. Now code like this:</p>\n<pre><code>import tensorflow as tf\n\nwith tf.device('/gpu:0'):\n    st = tf.SparseTensor(\n        tf.constant([[0, 0], [1, 1]], dtype=tf.int64),\n        tf.constant([1.2, 3.4], dtype=tf.float32),\n        tf.constant([2, 2], dtype=tf.int64)\n    ) \n    v = tf.Variable([[1.0, 0.0], [0.0, 1.0]], dtype=tf.float32)\n    st = tf.sparse_tensor_dense_matmul(st, v)\n    st = tf.reduce_min(st)\n    optimizer = tf.train.AdamOptimizer()\n    trainer = optimizer.minimize(st)\n\nwith tf.Session() as sess:\n    print(sess.run(trainer))\n</code></pre>\n<p>Fails with error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test_tf3.py\", line 18, in &lt;module&gt;\n    print(sess.run(trainer))\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n    options, run_metadata)\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1 = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2, _device=\"/device:GPU:0\"](Const, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_1, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_2)]]\n</code></pre>\n<p>It looks like it requires \"int64 strided slice\" to be executed on GPU. So maybe it just needs to enable int64 strided slice on GPU.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 8 (jessie)\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): ('v1.3.0-rc2-0-g2784b1c', '1.3.0-rc2')\nPython version: 2.7.9\nBazel version (if compiling from source): 0.5.3\nCUDA/cuDNN version: 8.0 / 5.0\nGPU model and memory: Nvidia GeForce GTX TITAN X\nExact command to reproduce:\n\nI would like to optimize sparse_tensor_dense_matmul operation on GPU to process sparse input completely on GPU. Now code like this:\nimport tensorflow as tf\n\nwith tf.device('/gpu:0'):\n    st = tf.SparseTensor(\n        tf.constant([[0, 0], [1, 1]], dtype=tf.int64),\n        tf.constant([1.2, 3.4], dtype=tf.float32),\n        tf.constant([2, 2], dtype=tf.int64)\n    ) \n    v = tf.Variable([[1.0, 0.0], [0.0, 1.0]], dtype=tf.float32)\n    st = tf.sparse_tensor_dense_matmul(st, v)\n    st = tf.reduce_min(st)\n    optimizer = tf.train.AdamOptimizer()\n    trainer = optimizer.minimize(st)\n\nwith tf.Session() as sess:\n    print(sess.run(trainer))\n\nFails with error:\nTraceback (most recent call last):\n  File \"test_tf3.py\", line 18, in <module>\n    print(sess.run(trainer))\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n    options, run_metadata)\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1 = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2, _device=\"/device:GPU:0\"](Const, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_1, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_2)]]\n\nIt looks like it requires \"int64 strided slice\" to be executed on GPU. So maybe it just needs to enable int64 strided slice on GPU.", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-0-g2784b1c', '1.3.0-rc2')\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**: 0.5.3\r\n- **CUDA/cuDNN version**: 8.0 / 5.0\r\n- **GPU model and memory**: Nvidia GeForce GTX TITAN X\r\n- **Exact command to reproduce**:\r\n\r\nI would like to optimize sparse_tensor_dense_matmul operation on GPU to process sparse input completely on GPU. Now code like this:\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    st = tf.SparseTensor(\r\n        tf.constant([[0, 0], [1, 1]], dtype=tf.int64),\r\n        tf.constant([1.2, 3.4], dtype=tf.float32),\r\n        tf.constant([2, 2], dtype=tf.int64)\r\n    ) \r\n    v = tf.Variable([[1.0, 0.0], [0.0, 1.0]], dtype=tf.float32)\r\n    st = tf.sparse_tensor_dense_matmul(st, v)\r\n    st = tf.reduce_min(st)\r\n    optimizer = tf.train.AdamOptimizer()\r\n    trainer = optimizer.minimize(st)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(trainer))\r\n```\r\nFails with error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tf3.py\", line 18, in <module>\r\n    print(sess.run(trainer))\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n     [[Node: gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1 = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2, _device=\"/device:GPU:0\"](Const, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_1, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_2)]]\r\n```\r\nIt looks like it requires \"int64 strided slice\" to be executed on GPU. So maybe it just needs to enable int64 strided slice on GPU."}