{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421225661", "html_url": "https://github.com/tensorflow/tensorflow/pull/21509#issuecomment-421225661", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21509", "id": 421225661, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTIyNTY2MQ==", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-14T04:13:40Z", "updated_at": "2018-09-14T06:04:18Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4313109\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nataliaponomareva\">@nataliaponomareva</a> Hi, softmax is good idea to me. However, I cannot find a numerical stable softmax implementation. The original <code>e^(x_i)/sum_over_j e^(x_j)</code> is easy to overflow, and I don't want to make it very complex.</p>\n<p>So I just move the assertion line into <code>if normalize</code> block, and leave others unchanged. Namely, we require feature_importance must be non-negative only if using normalization. How do you think?</p>", "body_text": "@nataliaponomareva Hi, softmax is good idea to me. However, I cannot find a numerical stable softmax implementation. The original e^(x_i)/sum_over_j e^(x_j) is easy to overflow, and I don't want to make it very complex.\nSo I just move the assertion line into if normalize block, and leave others unchanged. Namely, we require feature_importance must be non-negative only if using normalization. How do you think?", "body": "@nataliaponomareva Hi, softmax is good idea to me. However, I cannot find a numerical stable softmax implementation. The original `e^(x_i)/sum_over_j e^(x_j)` is easy to overflow, and I don't want to make it very complex.\r\n\r\nSo I just move the assertion line into `if normalize` block, and leave others unchanged. Namely, we require feature_importance must be non-negative only if using normalization. How do you think?"}