{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/211730925", "pull_request_review_id": 148204058, "id": 211730925, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMTczMDkyNQ==", "diff_hunk": "@@ -544,6 +550,615 @@ def testTrainEvaluateAndPredictWithOnlyIndicatorColumn(self):\n     self.assertEqual(1, ensemble.trees[0].nodes[0].bucketized_split.feature_id)\n     self.assertEqual(0, ensemble.trees[0].nodes[0].bucketized_split.threshold)\n \n+  def testFeatureImportancesWithTrainedEnsemble(self):\n+    input_fn = _make_train_input_fn(is_classification=True)\n+\n+    est = boosted_trees.BoostedTreesClassifier(\n+        feature_columns=self._feature_columns,\n+        n_batches_per_layer=1,\n+        n_trees=2,\n+        max_depth=5)\n+\n+    # It will stop after 5 steps because of the max depth and num trees.\n+    num_steps = 100\n+    # Train for a few steps, and validate final checkpoint.\n+    est.train(input_fn, steps=num_steps)\n+\n+    feature_names_expected = ['f_0_bucketized', 'f_2_bucketized', 'f_1_bucketized']\n+\n+    feature_names, importances = est.experimental_feature_importances(normalize=False)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([0.833933, 0.606342, 0.0], importances)\n+\n+    feature_names, importances = est.experimental_feature_importances(normalize=True)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([0.579010, 0.420990, 0.0], importances)\n+\n+  def testFeatureImportancesOnEmtpyEnsemble(self):\n+    input_fn = _make_train_input_fn(is_classification=True)\n+\n+    est = boosted_trees.BoostedTreesClassifier(\n+        feature_columns=self._feature_columns,\n+        n_batches_per_layer=1,\n+        n_trees=1,\n+        max_depth=5)\n+\n+    class BailOutWithoutTraining(session_run_hook.SessionRunHook):\n+\n+      def before_run(self, run_context):\n+        raise StopIteration('to bail out.')\n+\n+    # The step-0 checkpoint will have only an empty ensemble.\n+    est.train(input_fn,\n+              steps=100,  # must stop at 0 anyway.\n+              hooks=[BailOutWithoutTraining()])\n+\n+    with self.assertRaisesRegexp(ValueError, 'empty serialized string'):\n+      est.experimental_feature_importances(normalize=False)\n+\n+    with self.assertRaisesRegexp(ValueError, 'empty serialized string'):\n+      est.experimental_feature_importances(normalize=True)\n+\n+  def _create_fake_checkpoint_with_tree_ensemble_proto(self, est, tree_ensemble_text):\n+    with ops.Graph().as_default():\n+      with ops.name_scope('boosted_trees') as name:\n+        tree_ensemble = boosted_trees_ops.TreeEnsemble(name=name)\n+        tree_ensemble_proto = boosted_trees_pb2.TreeEnsemble()\n+        text_format.Merge(tree_ensemble_text, tree_ensemble_proto)\n+        stamp_token, _ = tree_ensemble.serialize()\n+        restore_op = tree_ensemble.deserialize(\n+            stamp_token, tree_ensemble_proto.SerializeToString())\n+\n+        with session.Session() as sess:\n+          resources.initialize_resources(resources.shared_resources()).run()\n+          restore_op.run()\n+          saver = saver_lib.Saver()\n+          save_path = os.path.join(est.model_dir, 'model.ckpt')\n+          saver.save(sess, save_path)\n+\n+  def testFeatureImportances(self):\n+    est = boosted_trees.BoostedTreesClassifier(\n+        feature_columns=self._feature_columns,\n+        n_batches_per_layer=1,\n+        n_trees=2,\n+        max_depth=5)\n+\n+    tree_ensemble_text = \"\"\"\n+        trees {\n+          nodes {\n+            bucketized_split {\n+              feature_id: 2\n+              left_id: 1\n+              right_id: 2\n+            }\n+            metadata {\n+              gain: 2.0\n+            }\n+          }\n+          nodes {\n+            bucketized_split {\n+              feature_id: 0\n+              left_id: 3\n+              right_id: 4\n+            }\n+            metadata {\n+              gain: 3.0\n+            }\n+          }\n+          nodes {\n+            bucketized_split {\n+              feature_id: 1\n+              left_id: 5\n+              right_id: 6\n+            }\n+            metadata {\n+              gain: 2.0\n+            }\n+          }\n+          nodes {\n+            bucketized_split {\n+              feature_id: 0\n+              left_id: 7\n+              right_id: 8\n+            }\n+            metadata {\n+              gain: 1.0\n+            }\n+          }\n+        }\n+        trees {\n+          nodes {\n+            bucketized_split {\n+              feature_id: 0\n+              left_id: 1\n+              right_id: 2\n+            }\n+            metadata {\n+              gain: 1.0\n+            }\n+          }\n+          nodes {\n+            bucketized_split {\n+              feature_id: 2\n+              left_id: 3\n+              right_id: 4\n+            }\n+            metadata {\n+              gain: 1.0\n+            }\n+          }\n+        }\n+        tree_weights: 1.0\n+        tree_weights: 1.0\n+        \"\"\"\n+    self._create_fake_checkpoint_with_tree_ensemble_proto(est, tree_ensemble_text)\n+\n+    feature_names_expected = ['f_0_bucketized', 'f_2_bucketized', 'f_1_bucketized']\n+    feature_names, importances = est.experimental_feature_importances(normalize=False)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([5.0, 3.0, 2.0], importances)\n+\n+    feature_names, importances = est.experimental_feature_importances(normalize=True)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([0.5, 0.3, 0.2], importances)\n+\n+  def testFeatureImportancesWithTreeWeights(self):\n+    est = boosted_trees.BoostedTreesClassifier(\n+        feature_columns=self._feature_columns,\n+        n_batches_per_layer=1,\n+        n_trees=2,\n+        max_depth=5)\n+\n+    tree_ensemble_text = \"\"\"\n+        trees {\n+          nodes {\n+            bucketized_split {\n+              feature_id: 0\n+              left_id: 1\n+              right_id: 2\n+            }\n+            metadata {\n+              gain: 12.5\n+            }\n+          }\n+          nodes {\n+            bucketized_split {\n+              feature_id: 1\n+              left_id: 3\n+              right_id: 4\n+            }\n+            metadata {\n+              gain: 5.0\n+            }\n+          }\n+        }\n+        trees {\n+          nodes {\n+            bucketized_split {\n+              feature_id: 2\n+              left_id: 1\n+              right_id: 2\n+            }\n+            metadata {\n+              gain: 5.0\n+            }\n+          }\n+        }\n+        tree_weights: 0.4\n+        tree_weights: 0.6\n+        \"\"\"\n+    self._create_fake_checkpoint_with_tree_ensemble_proto(est, tree_ensemble_text)\n+\n+    feature_names_expected = ['f_0_bucketized', 'f_2_bucketized', 'f_1_bucketized']\n+    feature_names, importances = est.experimental_feature_importances(normalize=False)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([5.0, 3.0, 2.0], importances)\n+\n+    feature_names, importances = est.experimental_feature_importances(normalize=True)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([0.5, 0.3, 0.2], importances)\n+\n+  def testFeatureImportancesWithEmptyTree(self):\n+    est = boosted_trees.BoostedTreesClassifier(\n+        feature_columns=self._feature_columns,\n+        n_batches_per_layer=1,\n+        n_trees=2,\n+        max_depth=5)\n+\n+    tree_ensemble_text = \"\"\"\n+        trees {\n+          nodes {\n+            bucketized_split {\n+              feature_id: 2\n+              left_id: 1\n+              right_id: 2\n+            }\n+            metadata {\n+              gain: 3.0\n+            }\n+          }\n+          nodes {\n+            bucketized_split {\n+              feature_id: 0\n+              left_id: 3\n+              right_id: 4\n+            }\n+            metadata {\n+              gain: 1.0\n+            }\n+          }\n+        }\n+        trees {\n+          nodes {\n+            leaf {\n+              scalar: 0.0\n+            }\n+          }\n+        }\n+        tree_weights: 1.0\n+        tree_weights: 1.0\n+        \"\"\"\n+    self._create_fake_checkpoint_with_tree_ensemble_proto(est, tree_ensemble_text)\n+\n+    feature_names_expected = ['f_2_bucketized', 'f_0_bucketized', 'f_1_bucketized']\n+    feature_names, importances = est.experimental_feature_importances(normalize=False)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([3.0, 1.0, 0.0], importances)\n+\n+    feature_names, importances = est.experimental_feature_importances(normalize=True)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([0.75, 0.25, 0.0], importances)\n+\n+  def testFeatureImportancesWithAllEmptyTree(self):\n+    est = boosted_trees.BoostedTreesClassifier(\n+        feature_columns=self._feature_columns,\n+        n_batches_per_layer=1,\n+        n_trees=2,\n+        max_depth=5)\n+\n+    tree_ensemble_text = \"\"\"\n+        trees {\n+          nodes {\n+            leaf {\n+              scalar: 0.0\n+            }\n+          }\n+        }\n+        trees {\n+          nodes {\n+            leaf {\n+              scalar: 0.0\n+            }\n+          }\n+        }\n+        tree_weights: 1.0\n+        tree_weights: 1.0\n+        \"\"\"\n+    self._create_fake_checkpoint_with_tree_ensemble_proto(est, tree_ensemble_text)\n+\n+    # Reverse order because feature importances are sorted by np.argsort(f)[::-1]\n+    feature_names_expected = ['f_2_bucketized', 'f_1_bucketized', 'f_0_bucketized']\n+    feature_names, importances = est.experimental_feature_importances(normalize=False)\n+    self.assertAllEqual(feature_names_expected, feature_names)\n+    self.assertAllClose([0.0, 0.0, 0.0], importances)\n+\n+    with self.assertRaisesRegexp(AssertionError, 'empty or contains'):", "path": "tensorflow/python/estimator/canned/boosted_trees_test.py", "position": null, "original_position": 323, "commit_id": "046c74c8e7c68aaa726977dd6e8a2523f854f9cc", "original_commit_id": "73c8cbb413029cf3e540e99b883ae89f4b08fc11", "user": {"login": "nataliaponomareva", "id": 4313109, "node_id": "MDQ6VXNlcjQzMTMxMDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4313109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nataliaponomareva", "html_url": "https://github.com/nataliaponomareva", "followers_url": "https://api.github.com/users/nataliaponomareva/followers", "following_url": "https://api.github.com/users/nataliaponomareva/following{/other_user}", "gists_url": "https://api.github.com/users/nataliaponomareva/gists{/gist_id}", "starred_url": "https://api.github.com/users/nataliaponomareva/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nataliaponomareva/subscriptions", "organizations_url": "https://api.github.com/users/nataliaponomareva/orgs", "repos_url": "https://api.github.com/users/nataliaponomareva/repos", "events_url": "https://api.github.com/users/nataliaponomareva/events{/privacy}", "received_events_url": "https://api.github.com/users/nataliaponomareva/received_events", "type": "User", "site_admin": false}, "body": "may be full error so it is clear why it chokes", "created_at": "2018-08-21T19:31:06Z", "updated_at": "2018-09-19T22:45:29Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21509#discussion_r211730925", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21509", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/211730925"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21509#discussion_r211730925"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21509"}}, "body_html": "<p>may be full error so it is clear why it chokes</p>", "body_text": "may be full error so it is clear why it chokes"}