{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/399354000", "html_url": "https://github.com/tensorflow/tensorflow/issues/20191#issuecomment-399354000", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20191", "id": 399354000, "node_id": "MDEyOklzc3VlQ29tbWVudDM5OTM1NDAwMA==", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-22T07:47:58Z", "updated_at": "2018-06-22T08:33:29Z", "author_association": "MEMBER", "body_html": "<p>Thanks for ping me, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>.</p>\n<p>The restriction is designed for safety at first, since it means lost data if <code>stride &gt; batch_size</code>. I think if people really need it, they should filter their data explicitly and / or apply <code>batch</code>, instead .</p>\n<p>Take the case of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3270063\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/danielwatson6\">@danielwatson6</a> , I think there are at least two solutions:</p>\n<ol>\n<li>use filter:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>dataset.apply(sliding_window_batch(<span class=\"pl-v\">window_size</span><span class=\"pl-k\">=</span>batch_size, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>))\n        .filter()   <span class=\"pl-c\"><span class=\"pl-c\">#</span> drop the data you don't need</span>\n        .batch(<span class=\"pl-c1\">3</span>)</pre></div>\n<p>or</p>\n<div class=\"highlight highlight-source-python\"><pre>dataset.apply(sliding_window_batch(<span class=\"pl-v\">window_size</span><span class=\"pl-k\">=</span>batch_size, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>))\n        .batch(batch_size)\n        .map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x[<span class=\"pl-c1\">0</span>:<span class=\"pl-c1\">3</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> drop the data you don't need</span></pre></div>\n<ol start=\"2\">\n<li>don't use filter</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>dataset.apply(sliding_window_batch(<span class=\"pl-v\">window_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>))\n        .batch(batch_size)\n        .map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: tf.transpose(x))</pre></div>\n<p>In my experience, <code>batch</code> is really what I need in most cases (<code>stride &gt; batch_size</code>) .</p>", "body_text": "Thanks for ping me, @mrry.\nThe restriction is designed for safety at first, since it means lost data if stride > batch_size. I think if people really need it, they should filter their data explicitly and / or apply batch, instead .\nTake the case of @danielwatson6 , I think there are at least two solutions:\n\nuse filter:\n\ndataset.apply(sliding_window_batch(window_size=batch_size, stride=1))\n        .filter()   # drop the data you don't need\n        .batch(3)\nor\ndataset.apply(sliding_window_batch(window_size=batch_size, stride=1))\n        .batch(batch_size)\n        .map(lambda x: x[0:3])  # drop the data you don't need\n\ndon't use filter\n\ndataset.apply(sliding_window_batch(window_size=3, stride=1))\n        .batch(batch_size)\n        .map(lambda x: tf.transpose(x))\nIn my experience, batch is really what I need in most cases (stride > batch_size) .", "body": "Thanks for ping me, @mrry.\r\n\r\nThe restriction is designed for safety at first, since it means lost data if `stride > batch_size`. I think if people really need it, they should filter their data explicitly and / or apply `batch`, instead . \r\n\r\nTake the case of @danielwatson6 , I think there are at least two solutions:\r\n\r\n1. use filter:\r\n```python\r\ndataset.apply(sliding_window_batch(window_size=batch_size, stride=1))\r\n        .filter()   # drop the data you don't need\r\n        .batch(3)\r\n```\r\n\r\nor\r\n\r\n```python\r\ndataset.apply(sliding_window_batch(window_size=batch_size, stride=1))\r\n        .batch(batch_size)\r\n        .map(lambda x: x[0:3])  # drop the data you don't need\r\n```\r\n\r\n\r\n2. don't use filter\r\n```python\r\ndataset.apply(sliding_window_batch(window_size=3, stride=1))\r\n        .batch(batch_size)\r\n        .map(lambda x: tf.transpose(x))\r\n```\r\n\r\nIn my experience, `batch` is really what I need in most cases (`stride > batch_size`) . \r\n"}