{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22014", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22014/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22014/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22014/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22014", "id": 356315884, "node_id": "MDU6SXNzdWUzNTYzMTU4ODQ=", "number": 22014, "title": "tf.assign - inputs not compatible with expected types - misleading error message", "user": {"login": "mpekalski", "id": 2975068, "node_id": "MDQ6VXNlcjI5NzUwNjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2975068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpekalski", "html_url": "https://github.com/mpekalski", "followers_url": "https://api.github.com/users/mpekalski/followers", "following_url": "https://api.github.com/users/mpekalski/following{/other_user}", "gists_url": "https://api.github.com/users/mpekalski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpekalski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpekalski/subscriptions", "organizations_url": "https://api.github.com/users/mpekalski/orgs", "repos_url": "https://api.github.com/users/mpekalski/repos", "events_url": "https://api.github.com/users/mpekalski/events{/privacy}", "received_events_url": "https://api.github.com/users/mpekalski/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-02T21:36:11Z", "updated_at": "2018-09-28T20:58:54Z", "closed_at": "2018-09-28T20:58:54Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n</ul>\n<p>TF checkpoint I have built</p>\n<pre><code>/tmp/tensorflow# git log   \ncommit 09792df012c22622324f085f46edde33006c7355\nAuthor: A. Unique TensorFlower &lt;gardener@tensorflow.org&gt;\nDate:   Sun Aug 26 02:07:11 2018 -0700\n\n    compat: Update forward compatibility horizon to 2018-08-26\n    \n    PiperOrigin-RevId: 210266798\n</code></pre>\n<pre><code>== cat /etc/issue ===============================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nYes\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.5)\nprotobuf (3.6.1)\ntensorflow (1.10.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.10.0\ntf.GIT_VERSION = b'unknown'\ntf.COMPILER_VERSION = b'unknown'\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nWed Aug 29 19:57:14 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\n\n</code></pre>\n<p><strong>Bazel</strong> version</p>\n<pre><code>$ bazel version\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\nBuild label: 0.16.0\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\nBuild timestamp: 1533056484\nBuild timestamp as int: 1533056484\n</code></pre>\n<p><strong>CUDNN</strong> version:</p>\n<pre><code>$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2018 NVIDIA Corporation\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\nCuda compilation tools, release 9.2, V9.2.148\n</code></pre>\n<p><strong>GPU</strong>: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS</p>\n<ul>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\n                               , trainable=False, use_resource=True)             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\n                                   , trainable=False)#, use_resource=True)         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            print(y)\n            print(inputs)\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\n            with tf.control_dependencies([assign_two]):\n                #with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\n                return y.read_value(), label\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                    .map(scope_2)\n                    .batch(1)\n                    .repeat(1)        \n                    )\n    return dataset\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1()\n\nwith tf.variable_scope(\"iterator\"):\n    # Define iterator from_string_handle. In general it is useful to have\n    # this kind of iterator if one wants to switch between train and validation\n    # within the training loop.        \n    iterator_t = dataset_fn.make_initializable_iterator()\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                iterator_t.output_types,\n                                                iterator_t.output_shapes)\n    \n    def get_next_item():\n        next_elem = iterator.get_next(name=\"next_element\")\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n        return x, y    \nwith tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\n\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    handle_t = sess.run(iterator_t.string_handle())\n    # Run data iterator initialisation\n    sess.run(iterator_t.initializer)\n    print(sess.graph.get_operations()) \n    while True:\n        try:\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n        except tf.errors.OutOfRangeError:\n                        print(\"End of training dataset.\")\n                        break        \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n</code></pre>\n<h3>Describe the problem</h3>\n<p>When using <code>tf.get_variable</code> inside function called in <code>dataset.map()</code> if it is without <code>use_resource=True</code>, which the original variable has, the assign statement gives an error</p>\n<pre><code>TypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\n</code></pre>\n<p>although the variables used in the assign statement have expected types</p>\n<pre><code>&lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;\n&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref&gt;\n</code></pre>\n<p>When passing <code>use_resource=True</code> to <code>tf.get_variable</code> within <code>def scope_2</code> they become</p>\n<pre><code>&lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;\n&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;\n</code></pre>\n<p>and the whole code works as expected.</p>\n<p>I think the error message in the first case is misleading, because variables have expected types and the whole thing fails. Maybe the input types and expected types in the error msg should be swapped?</p>\n<p>Also, as far as I understand <code>trainable</code> and <code>use_resource</code> flags should be inferred from the original variable when using <code>tf.get_variable</code>, so different behavior dependent on the use_resource flag makes me even more confused.</p>\n<h3>Source code / logs</h3>\n<p>RUN 1 - without <code>use_resource=True</code></p>\n<pre><code>DS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;\ninitial scope: \nDS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref&gt;\n=============\n&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref&gt;\nTensor(\"arg0:0\", shape=(), dtype=int32)\nTraceback (most recent call last):\n  File \"bug.py\", line 51, in &lt;module&gt;\n    dataset_fn = scope_1()\n  File \"bug.py\", line 43, in scope_1\n    .map(scope_2)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\n    return MapDataset(self, map_func)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\n    map_func, \"Dataset.map()\", input_dataset)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\n    self._function.add_to_graph(ops.get_default_graph())\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\n    self._create_definition_if_needed()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\n    self._create_definition_if_needed_impl()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\n    self._capture_by_value, self._caller_device)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\n    outputs = func(*func_graph.inputs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\n    ret = func(*nested_args)\n  File \"bug.py\", line 34, in scope_2\n    assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 221, in assign\n    validate_shape=validate_shape)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 61, in assign\n    use_locking=use_locking, name=name)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 738, in create_op\n    **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3254, in create_op\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1728, in __init__\n    input_types))\nTypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\n</code></pre>\n<p>RUN 2 - with <code>use_resource=True</code></p>\n<pre><code>DS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f61af7de390&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;\ninitial scope: \nDS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f61af7de390&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;\n=============\n&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;\nTensor(\"arg0:0\", shape=(), dtype=int32)\n2018-09-02 21:07:18.255844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-02 21:07:18.257387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 10.23GiB\n2018-09-02 21:07:18.258168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-09-02 21:07:18.258274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-02 21:07:18.258631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-09-02 21:07:18.258993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-09-02 21:07:18.272483: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n[&lt;tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/Const' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0/tensors/component_0' type=Const&gt;, &lt;tf.Operation 'scope_0/tensors/component_1' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0/batch_size' type=Const&gt;, &lt;tf.Operation 'scope_0/drop_remainder' type=Const&gt;, &lt;tf.Operation 'scope_0/count' type=Const&gt;, &lt;tf.Operation 'iterator/IteratorV2' type=IteratorV2&gt;, &lt;tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset&gt;, &lt;tf.Operation 'iterator/MapDataset' type=MapDataset&gt;, &lt;tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2&gt;, &lt;tf.Operation 'iterator/RepeatDataset' type=RepeatDataset&gt;, &lt;tf.Operation 'iterator/MakeIterator' type=MakeIterator&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'iterator/iterator_handle' type=Placeholder&gt;, &lt;tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'init' type=NoOp&gt;, &lt;tf.Operation 'init_1' type=NoOp&gt;]\n(array([2.], dtype=float32), array([-1], dtype=int32))\n(array([3.], dtype=float32), array([-2], dtype=int32))\n(array([4.], dtype=float32), array([-3], dtype=int32))\n(array([5.], dtype=float32), array([-4], dtype=int32))\n(array([6.], dtype=float32), array([-5], dtype=int32))\nEnd of training dataset.\n\nglobal vars: [&lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;, &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;]\nlocal vars: []\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n\nTF checkpoint I have built\n/tmp/tensorflow# git log   \ncommit 09792df012c22622324f085f46edde33006c7355\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\nDate:   Sun Aug 26 02:07:11 2018 -0700\n\n    compat: Update forward compatibility horizon to 2018-08-26\n    \n    PiperOrigin-RevId: 210266798\n\n== cat /etc/issue ===============================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nYes\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.5)\nprotobuf (3.6.1)\ntensorflow (1.10.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.10.0\ntf.GIT_VERSION = b'unknown'\ntf.COMPILER_VERSION = b'unknown'\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nWed Aug 29 19:57:14 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\n\n\nBazel version\n$ bazel version\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\nBuild label: 0.16.0\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\nBuild timestamp: 1533056484\nBuild timestamp as int: 1533056484\n\nCUDNN version:\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2018 NVIDIA Corporation\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\nCuda compilation tools, release 9.2, V9.2.148\n\nGPU: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\n\nExact command to reproduce:\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\n                               , trainable=False, use_resource=True)             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\n                                   , trainable=False)#, use_resource=True)         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            print(y)\n            print(inputs)\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\n            with tf.control_dependencies([assign_two]):\n                #with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\n                return y.read_value(), label\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                    .map(scope_2)\n                    .batch(1)\n                    .repeat(1)        \n                    )\n    return dataset\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1()\n\nwith tf.variable_scope(\"iterator\"):\n    # Define iterator from_string_handle. In general it is useful to have\n    # this kind of iterator if one wants to switch between train and validation\n    # within the training loop.        \n    iterator_t = dataset_fn.make_initializable_iterator()\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                iterator_t.output_types,\n                                                iterator_t.output_shapes)\n    \n    def get_next_item():\n        next_elem = iterator.get_next(name=\"next_element\")\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n        return x, y    \nwith tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\n\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    handle_t = sess.run(iterator_t.string_handle())\n    # Run data iterator initialisation\n    sess.run(iterator_t.initializer)\n    print(sess.graph.get_operations()) \n    while True:\n        try:\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n        except tf.errors.OutOfRangeError:\n                        print(\"End of training dataset.\")\n                        break        \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n\nDescribe the problem\nWhen using tf.get_variable inside function called in dataset.map() if it is without use_resource=True, which the original variable has, the assign statement gives an error\nTypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\n\nalthough the variables used in the assign statement have expected types\n<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\n\nWhen passing use_resource=True to tf.get_variable within def scope_2 they become\n<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n\nand the whole code works as expected.\nI think the error message in the first case is misleading, because variables have expected types and the whole thing fails. Maybe the input types and expected types in the error msg should be swapped?\nAlso, as far as I understand trainable and use_resource flags should be inferred from the original variable when using tf.get_variable, so different behavior dependent on the use_resource flag makes me even more confused.\nSource code / logs\nRUN 1 - without use_resource=True\nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\ninitial scope: \nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\n=============\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\nTensor(\"arg0:0\", shape=(), dtype=int32)\nTraceback (most recent call last):\n  File \"bug.py\", line 51, in <module>\n    dataset_fn = scope_1()\n  File \"bug.py\", line 43, in scope_1\n    .map(scope_2)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\n    return MapDataset(self, map_func)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\n    map_func, \"Dataset.map()\", input_dataset)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\n    self._function.add_to_graph(ops.get_default_graph())\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\n    self._create_definition_if_needed()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\n    self._create_definition_if_needed_impl()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\n    self._capture_by_value, self._caller_device)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\n    outputs = func(*func_graph.inputs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\n    ret = func(*nested_args)\n  File \"bug.py\", line 34, in scope_2\n    assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 221, in assign\n    validate_shape=validate_shape)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 61, in assign\n    use_locking=use_locking, name=name)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 738, in create_op\n    **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3254, in create_op\n    op_def=op_def)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1728, in __init__\n    input_types))\nTypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\n\nRUN 2 - with use_resource=True\nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\ninitial scope: \nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n=============\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\nTensor(\"arg0:0\", shape=(), dtype=int32)\n2018-09-02 21:07:18.255844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-02 21:07:18.257387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 10.23GiB\n2018-09-02 21:07:18.258168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-09-02 21:07:18.258274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-02 21:07:18.258631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-09-02 21:07:18.258993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-09-02 21:07:18.272483: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n[<tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/scope_1/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/tensors/component_0' type=Const>, <tf.Operation 'scope_0/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/batch_size' type=Const>, <tf.Operation 'scope_0/drop_remainder' type=Const>, <tf.Operation 'scope_0/count' type=Const>, <tf.Operation 'iterator/IteratorV2' type=IteratorV2>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\n(array([2.], dtype=float32), array([-1], dtype=int32))\n(array([3.], dtype=float32), array([-2], dtype=int32))\n(array([4.], dtype=float32), array([-3], dtype=int32))\n(array([5.], dtype=float32), array([-4], dtype=int32))\n(array([6.], dtype=float32), array([-5], dtype=int32))\nEnd of training dataset.\n\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>]\nlocal vars: []", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n\r\nTF checkpoint I have built\r\n```\r\n/tmp/tensorflow# git log   \r\ncommit 09792df012c22622324f085f46edde33006c7355\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Sun Aug 26 02:07:11 2018 -0700\r\n\r\n    compat: Update forward compatibility horizon to 2018-08-26\r\n    \r\n    PiperOrigin-RevId: 210266798\r\n```\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.5)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Aug 29 19:57:14 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n```\r\n**Bazel** version\r\n```\r\n$ bazel version\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.16.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\r\nBuild timestamp: 1533056484\r\nBuild timestamp as int: 1533056484\r\n```\r\n\r\n**CUDNN** version:\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.148\r\n```\r\n\r\n**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\r\n                               , trainable=False, use_resource=True)             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\r\n                                   , trainable=False)#, use_resource=True)         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            print(y)\r\n            print(inputs)\r\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\r\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\r\n            with tf.control_dependencies([assign_two]):\r\n                #with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\r\n                return y.read_value(), label\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\r\n                    .map(scope_2)\r\n                    .batch(1)\r\n                    .repeat(1)        \r\n                    )\r\n    return dataset\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1()\r\n\r\nwith tf.variable_scope(\"iterator\"):\r\n    # Define iterator from_string_handle. In general it is useful to have\r\n    # this kind of iterator if one wants to switch between train and validation\r\n    # within the training loop.        \r\n    iterator_t = dataset_fn.make_initializable_iterator()\r\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\r\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \r\n                                                iterator_t.output_types,\r\n                                                iterator_t.output_shapes)\r\n    \r\n    def get_next_item():\r\n        next_elem = iterator.get_next(name=\"next_element\")\r\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\r\n        return x, y    \r\nwith tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})) as sess:\r\n\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    handle_t = sess.run(iterator_t.string_handle())\r\n    # Run data iterator initialisation\r\n    sess.run(iterator_t.initializer)\r\n    print(sess.graph.get_operations()) \r\n    while True:\r\n        try:\r\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\r\n        except tf.errors.OutOfRangeError:\r\n                        print(\"End of training dataset.\")\r\n                        break        \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n```\r\n### Describe the problem\r\nWhen using `tf.get_variable` inside function called in `dataset.map()` if it is without `use_resource=True`, which the original variable has, the assign statement gives an error\r\n```\r\nTypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\r\n```\r\nalthough the variables used in the assign statement have expected types\r\n```\r\n<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\r\n```\r\nWhen passing `use_resource=True` to `tf.get_variable` within `def scope_2` they become \r\n```\r\n<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n```\r\nand the whole code works as expected. \r\n\r\nI think the error message in the first case is misleading, because variables have expected types and the whole thing fails. Maybe the input types and expected types in the error msg should be swapped?\r\n\r\nAlso, as far as I understand `trainable` and `use_resource` flags should be inferred from the original variable when using `tf.get_variable`, so different behavior dependent on the use_resource flag makes me even more confused.\r\n\r\n### Source code / logs\r\nRUN 1 - without `use_resource=True`\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7ffb011d4390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32_ref>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 51, in <module>\r\n    dataset_fn = scope_1()\r\n  File \"bug.py\", line 43, in scope_1\r\n    .map(scope_2)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\r\n    return MapDataset(self, map_func)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\r\n    map_func, \"Dataset.map()\", input_dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\r\n    self._capture_by_value, self._caller_device)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\r\n    ret = func(*nested_args)\r\n  File \"bug.py\", line 34, in scope_2\r\n    assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0, name=\"first_assign\")))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 221, in assign\r\n    validate_shape=validate_shape)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 61, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 738, in create_op\r\n    **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3254, in create_op\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1728, in __init__\r\n    input_types))\r\nTypeError: In op 'scope_1/Assign', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\r\n```\r\n\r\nRUN 2 - with `use_resource=True`\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f61af7de390>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n2018-09-02 21:07:18.255844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-02 21:07:18.257387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.23GiB\r\n2018-09-02 21:07:18.258168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-09-02 21:07:18.258274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-02 21:07:18.258631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-09-02 21:07:18.258993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-09-02 21:07:18.272483: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n[<tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/scope_1/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/tensors/component_0' type=Const>, <tf.Operation 'scope_0/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Const>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/batch_size' type=Const>, <tf.Operation 'scope_0/drop_remainder' type=Const>, <tf.Operation 'scope_0/count' type=Const>, <tf.Operation 'iterator/IteratorV2' type=IteratorV2>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\r\n(array([2.], dtype=float32), array([-1], dtype=int32))\r\n(array([3.], dtype=float32), array([-2], dtype=int32))\r\n(array([4.], dtype=float32), array([-3], dtype=int32))\r\n(array([5.], dtype=float32), array([-4], dtype=int32))\r\n(array([6.], dtype=float32), array([-5], dtype=int32))\r\nEnd of training dataset.\r\n\r\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>]\r\nlocal vars: []\r\n```"}