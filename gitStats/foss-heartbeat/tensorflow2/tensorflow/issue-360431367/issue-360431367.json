{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22285", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22285/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22285/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22285/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22285", "id": 360431367, "node_id": "MDU6SXNzdWUzNjA0MzEzNjc=", "number": 22285, "title": "Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)", "user": {"login": "tychota", "id": 13785185, "node_id": "MDQ6VXNlcjEzNzg1MTg1", "avatar_url": "https://avatars2.githubusercontent.com/u/13785185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tychota", "html_url": "https://github.com/tychota", "followers_url": "https://api.github.com/users/tychota/followers", "following_url": "https://api.github.com/users/tychota/following{/other_user}", "gists_url": "https://api.github.com/users/tychota/gists{/gist_id}", "starred_url": "https://api.github.com/users/tychota/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tychota/subscriptions", "organizations_url": "https://api.github.com/users/tychota/orgs", "repos_url": "https://api.github.com/users/tychota/repos", "events_url": "https://api.github.com/users/tychota/events{/privacy}", "received_events_url": "https://api.github.com/users/tychota/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-09-14T19:45:27Z", "updated_at": "2018-11-20T07:56:06Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Note: I'm noob to tensorflow. I can provide more info if needed.</p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No. I trained <a href=\"https://github.com/lightvector/GoNN\">https://github.com/lightvector/GoNN</a> on my own data (<code>data.h5</code>) without code modification</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: gentoo (kernel 4.17.14-gentoo)  uptodate</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: no mobile device</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source (from gentoo repository)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 'unknown' 1.10.0</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.16.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 7.3.0-r3 p1.4</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda 9.2.88 / cudnn 7.1.4</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1070 Ti 7.93GiB</li>\n<li><strong>Exact command to reproduce</strong>: <code>./train.py -traindir models -fast-factor 50 -gamesh5 data.h5</code></li>\n</ul>\n<p><strong>Extra:</strong></p>\n<ul>\n<li>ebuild from tensorflow I used: <a href=\"https://gitweb.gentoo.org/repo/gentoo.git/tree/sci-libs/tensorflow/tensorflow-1.10.0.ebuild\" rel=\"nofollow\">https://gitweb.gentoo.org/repo/gentoo.git/tree/sci-libs/tensorflow/tensorflow-1.10.0.ebuild</a></li>\n<li>use flag :<br>\n<code>sci-libs/tensorflow cuda -jemalloc -system-libs -mpi PYTHON_TARGETS: -* python3_6</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When I run tensorflow on GPU, it crashed.</p>\n<p>When I run on CPU, it works:</p>\n<pre><code>CUDA_VISIBLE_DEVICES=\"\" ./train.py  -traindir models -fast-factor 50 -gamesh5 data.h5\n</code></pre>\n<h3>Source code / logs</h3>\n<p>Code:</p>\n<p><a href=\"https://github.com/lightvector/GoNN/tree/2d74b1300fef6700e75af7f2344d852798b713fb\">https://github.com/lightvector/GoNN/tree/2d74b1300fef6700e75af7f2344d852798b713fb</a></p>\n<p>Logs:</p>\n<pre><code> ~/c/GoNN: ./train.py -traindir models -fast-factor 50 -gamesh5 data.h5                                                                                                                                      1058ms  ven. 14 sept. 2018 20:43:51 CEST\nBuilding model\nWARNING:tensorflow:From /home/tychota/code/GoNN/model.py:912: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\nAdjusting gradient for rconv1/w1:0 by Tensor(\"PadV2:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv1/w2:0 by Tensor(\"PadV2_1:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv2/w1a:0 by Tensor(\"PadV2_2:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv2/w1b:0 by Tensor(\"PadV2_3:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv2/w2:0 by Tensor(\"PadV2_4:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv3/w1a:0 by Tensor(\"PadV2_5:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv3/w1b:0 by Tensor(\"PadV2_6:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv3/w2:0 by Tensor(\"PadV2_7:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv4/w1a:0 by Tensor(\"PadV2_8:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv4/w1b:0 by Tensor(\"PadV2_9:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv4/w2:0 by Tensor(\"PadV2_10:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv5/w1a:0 by Tensor(\"PadV2_11:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv5/w1b:0 by Tensor(\"PadV2_12:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv5/w2:0 by Tensor(\"PadV2_13:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv7/w1a:0 by Tensor(\"PadV2_14:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv7/w1b:0 by Tensor(\"PadV2_15:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv7/w2:0 by Tensor(\"PadV2_16:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv8/w1a:0 by Tensor(\"PadV2_17:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv8/w1b:0 by Tensor(\"PadV2_18:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv8/w2:0 by Tensor(\"PadV2_19:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv9/w1a:0 by Tensor(\"PadV2_20:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv9/w1b:0 by Tensor(\"PadV2_21:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv9/w2:0 by Tensor(\"PadV2_22:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv10/w1a:0 by Tensor(\"PadV2_23:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv10/w1b:0 by Tensor(\"PadV2_24:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv10/w2:0 by Tensor(\"PadV2_25:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv11/w1a:0 by Tensor(\"PadV2_26:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv11/w1b:0 by Tensor(\"PadV2_27:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv11/w2:0 by Tensor(\"PadV2_28:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv13/w1a:0 by Tensor(\"PadV2_29:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv13/w1b:0 by Tensor(\"PadV2_30:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv13/w2:0 by Tensor(\"PadV2_31:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv14/w1a:0 by Tensor(\"PadV2_32:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv14/w1b:0 by Tensor(\"PadV2_33:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv14/w2:0 by Tensor(\"PadV2_34:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for p1/norm/beta:0 by 0.25\nAdjusting gradient for p2/w:0 by 0.25\nModel variable conv1/wcenter:0, 4256 parameters\nModel variable conv1/w:0, 106400 parameters\nModel variable rconv1/norm1/beta:0, 224 parameters\nModel variable rconv1/w1:0, 451584 parameters\nModel variable rconv1/norm2/beta:0, 224 parameters\nModel variable rconv1/w2:0, 451584 parameters\nModel variable rconv2/norm1/beta:0, 224 parameters\nModel variable rconv2/w1a:0, 322560 parameters\nModel variable rconv2/w1b:0, 129024 parameters\nModel variable rconv2/norm2/beta:0, 224 parameters\nModel variable rconv2/w2:0, 451584 parameters\nModel variable rconv3/norm1/beta:0, 224 parameters\nModel variable rconv3/w1a:0, 322560 parameters\nModel variable rconv3/w1b:0, 129024 parameters\nModel variable rconv3/norm2/beta:0, 224 parameters\nModel variable rconv3/w2:0, 451584 parameters\nModel variable rconv4/norm1/beta:0, 224 parameters\nModel variable rconv4/w1a:0, 322560 parameters\nModel variable rconv4/w1b:0, 129024 parameters\nModel variable rconv4/norm2/beta:0, 224 parameters\nModel variable rconv4/w2:0, 451584 parameters\nModel variable rconv5/norm1/beta:0, 224 parameters\nModel variable rconv5/w1a:0, 322560 parameters\nModel variable rconv5/w1b:0, 129024 parameters\nModel variable rconv5/norm2/beta:0, 224 parameters\nModel variable rconv5/w2:0, 451584 parameters\nModel variable rconv7/norm1/beta:0, 224 parameters\nModel variable rconv7/w1a:0, 322560 parameters\nModel variable rconv7/w1b:0, 129024 parameters\nModel variable rconv7/norm1b/beta:0, 64 parameters\nModel variable rconv7/w1r:0, 20480 parameters\nModel variable rconv7/norm2/beta:0, 160 parameters\nModel variable rconv7/w2:0, 322560 parameters\nModel variable rconv8/norm1/beta:0, 224 parameters\nModel variable rconv8/w1a:0, 322560 parameters\nModel variable rconv8/w1b:0, 129024 parameters\nModel variable rconv8/norm2/beta:0, 224 parameters\nModel variable rconv8/w2:0, 451584 parameters\nModel variable rconv9/norm1/beta:0, 224 parameters\nModel variable rconv9/w1a:0, 322560 parameters\nModel variable rconv9/w1b:0, 129024 parameters\nModel variable rconv9/norm2/beta:0, 224 parameters\nModel variable rconv9/w2:0, 451584 parameters\nModel variable rconv10/norm1/beta:0, 224 parameters\nModel variable rconv10/w1a:0, 322560 parameters\nModel variable rconv10/w1b:0, 129024 parameters\nModel variable rconv10/norm2/beta:0, 224 parameters\nModel variable rconv10/w2:0, 451584 parameters\nModel variable rconv11/norm1/beta:0, 224 parameters\nModel variable rconv11/w1a:0, 322560 parameters\nModel variable rconv11/w1b:0, 129024 parameters\nModel variable rconv11/norm1b/beta:0, 64 parameters\nModel variable rconv11/w1r:0, 20480 parameters\nModel variable rconv11/norm2/beta:0, 160 parameters\nModel variable rconv11/w2:0, 322560 parameters\nModel variable rconv13/norm1/beta:0, 224 parameters\nModel variable rconv13/w1a:0, 322560 parameters\nModel variable rconv13/w1b:0, 129024 parameters\nModel variable rconv13/norm2/beta:0, 224 parameters\nModel variable rconv13/w2:0, 451584 parameters\nModel variable rconv14/norm1/beta:0, 224 parameters\nModel variable rconv14/w1a:0, 322560 parameters\nModel variable rconv14/w1b:0, 129024 parameters\nModel variable rconv14/norm2/beta:0, 224 parameters\nModel variable rconv14/w2:0, 451584 parameters\nModel variable trunk/norm/beta:0, 224 parameters\nModel variable p1/intermediate_conv/w:0, 96768 parameters\nModel variable g1/w:0, 64512 parameters\nModel variable g1/norm/beta:0, 32 parameters\nModel variable matmulg2w:0, 3072 parameters\nModel variable p1/norm/beta:0, 48 parameters\nModel variable p2/w:0, 48 parameters\nBuilt model, 10901664 total parameters\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg_1\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg_1\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg_1\nAdditional update op on train step: trunk/norm/AssignMovingAvg\nAdditional update op on train step: trunk/norm/AssignMovingAvg_1\nAdditional update op on train step: g1/norm/AssignMovingAvg\nAdditional update op on train step: g1/norm/AssignMovingAvg_1\nAdditional update op on train step: p1/norm/AssignMovingAvg\nAdditional update op on train step: p1/norm/AssignMovingAvg_1\nOpening H5 file: data.h5\nAdjusting H5 cache settings to: [0, 521, 134217728, 0.75]\nTraining\n2018-09-14 20:44:04.209964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-14 20:44:04.210288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\npciBusID: 0000:01:00.0\ntotalMemory: 7.93GiB freeMemory: 7.34GiB\n2018-09-14 20:44:04.210299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-09-14 20:44:04.581780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-14 20:44:04.581800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-09-14 20:44:04.581804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-09-14 20:44:04.581914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7077 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\nBegan session\nTraining on 13234987 rows, validating on 688542/688542 rows\nEpoch size = 20000\nh5_chunk_size = 6000\nBatch size = 200\nL2 coeff value = 3e-05\nuse_ranks = False\npredict_pass = False\n2018-09-14 20:44:05.383592: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\n\t.  Registered:  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='GPU'; T in [DT_INT64]\n  device='GPU'; T in [DT_INT32]\n\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\n    return fn(*args)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\n    options, feed_dict, fetch_list, target_list, run_metadata)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\n\t.  Registered:  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='GPU'; T in [DT_INT64]\n  device='GPU'; T in [DT_INT32]\n\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"./train.py\", line 455, in &lt;module&gt;\n    vmetrics_evaled = merge_dicts(run_validation_in_batches(vmetrics), np.sum)\n  File \"./train.py\", line 325, in run_validation_in_batches\n    result = run(fetches, rows, symmetries=[False,False,False], training=False)\n  File \"./train.py\", line 309, in run\n    model.is_training: training\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\n    run_metadata_ptr)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\n    run_metadata)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\n\t.  Registered:  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='GPU'; T in [DT_INT64]\n  device='GPU'; T in [DT_INT32]\n\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\n\n</code></pre>", "body_text": "Note: I'm noob to tensorflow. I can provide more info if needed.\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No. I trained https://github.com/lightvector/GoNN on my own data (data.h5) without code modification\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): gentoo (kernel 4.17.14-gentoo)  uptodate\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no mobile device\nTensorFlow installed from (source or binary): source (from gentoo repository)\nTensorFlow version (use command below): 'unknown' 1.10.0\nPython version: 3.6.5\nBazel version (if compiling from source): 0.16.0\nGCC/Compiler version (if compiling from source): 7.3.0-r3 p1.4\nCUDA/cuDNN version: cuda 9.2.88 / cudnn 7.1.4\nGPU model and memory: GeForce GTX 1070 Ti 7.93GiB\nExact command to reproduce: ./train.py -traindir models -fast-factor 50 -gamesh5 data.h5\n\nExtra:\n\nebuild from tensorflow I used: https://gitweb.gentoo.org/repo/gentoo.git/tree/sci-libs/tensorflow/tensorflow-1.10.0.ebuild\nuse flag :\nsci-libs/tensorflow cuda -jemalloc -system-libs -mpi PYTHON_TARGETS: -* python3_6\n\nDescribe the problem\nWhen I run tensorflow on GPU, it crashed.\nWhen I run on CPU, it works:\nCUDA_VISIBLE_DEVICES=\"\" ./train.py  -traindir models -fast-factor 50 -gamesh5 data.h5\n\nSource code / logs\nCode:\nhttps://github.com/lightvector/GoNN/tree/2d74b1300fef6700e75af7f2344d852798b713fb\nLogs:\n ~/c/GoNN: ./train.py -traindir models -fast-factor 50 -gamesh5 data.h5                                                                                                                                      1058ms  ven. 14 sept. 2018 20:43:51 CEST\nBuilding model\nWARNING:tensorflow:From /home/tychota/code/GoNN/model.py:912: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\nAdjusting gradient for rconv1/w1:0 by Tensor(\"PadV2:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv1/w2:0 by Tensor(\"PadV2_1:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv2/w1a:0 by Tensor(\"PadV2_2:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv2/w1b:0 by Tensor(\"PadV2_3:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv2/w2:0 by Tensor(\"PadV2_4:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv3/w1a:0 by Tensor(\"PadV2_5:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv3/w1b:0 by Tensor(\"PadV2_6:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv3/w2:0 by Tensor(\"PadV2_7:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv4/w1a:0 by Tensor(\"PadV2_8:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv4/w1b:0 by Tensor(\"PadV2_9:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv4/w2:0 by Tensor(\"PadV2_10:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv5/w1a:0 by Tensor(\"PadV2_11:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv5/w1b:0 by Tensor(\"PadV2_12:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv5/w2:0 by Tensor(\"PadV2_13:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv7/w1a:0 by Tensor(\"PadV2_14:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv7/w1b:0 by Tensor(\"PadV2_15:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv7/w2:0 by Tensor(\"PadV2_16:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv8/w1a:0 by Tensor(\"PadV2_17:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv8/w1b:0 by Tensor(\"PadV2_18:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv8/w2:0 by Tensor(\"PadV2_19:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv9/w1a:0 by Tensor(\"PadV2_20:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv9/w1b:0 by Tensor(\"PadV2_21:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv9/w2:0 by Tensor(\"PadV2_22:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv10/w1a:0 by Tensor(\"PadV2_23:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv10/w1b:0 by Tensor(\"PadV2_24:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv10/w2:0 by Tensor(\"PadV2_25:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv11/w1a:0 by Tensor(\"PadV2_26:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv11/w1b:0 by Tensor(\"PadV2_27:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv11/w2:0 by Tensor(\"PadV2_28:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv13/w1a:0 by Tensor(\"PadV2_29:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv13/w1b:0 by Tensor(\"PadV2_30:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv13/w2:0 by Tensor(\"PadV2_31:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv14/w1a:0 by Tensor(\"PadV2_32:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv14/w1b:0 by Tensor(\"PadV2_33:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for rconv14/w2:0 by Tensor(\"PadV2_34:0\", shape=(3, 3, 1, 1), dtype=float32)\nAdjusting gradient for p1/norm/beta:0 by 0.25\nAdjusting gradient for p2/w:0 by 0.25\nModel variable conv1/wcenter:0, 4256 parameters\nModel variable conv1/w:0, 106400 parameters\nModel variable rconv1/norm1/beta:0, 224 parameters\nModel variable rconv1/w1:0, 451584 parameters\nModel variable rconv1/norm2/beta:0, 224 parameters\nModel variable rconv1/w2:0, 451584 parameters\nModel variable rconv2/norm1/beta:0, 224 parameters\nModel variable rconv2/w1a:0, 322560 parameters\nModel variable rconv2/w1b:0, 129024 parameters\nModel variable rconv2/norm2/beta:0, 224 parameters\nModel variable rconv2/w2:0, 451584 parameters\nModel variable rconv3/norm1/beta:0, 224 parameters\nModel variable rconv3/w1a:0, 322560 parameters\nModel variable rconv3/w1b:0, 129024 parameters\nModel variable rconv3/norm2/beta:0, 224 parameters\nModel variable rconv3/w2:0, 451584 parameters\nModel variable rconv4/norm1/beta:0, 224 parameters\nModel variable rconv4/w1a:0, 322560 parameters\nModel variable rconv4/w1b:0, 129024 parameters\nModel variable rconv4/norm2/beta:0, 224 parameters\nModel variable rconv4/w2:0, 451584 parameters\nModel variable rconv5/norm1/beta:0, 224 parameters\nModel variable rconv5/w1a:0, 322560 parameters\nModel variable rconv5/w1b:0, 129024 parameters\nModel variable rconv5/norm2/beta:0, 224 parameters\nModel variable rconv5/w2:0, 451584 parameters\nModel variable rconv7/norm1/beta:0, 224 parameters\nModel variable rconv7/w1a:0, 322560 parameters\nModel variable rconv7/w1b:0, 129024 parameters\nModel variable rconv7/norm1b/beta:0, 64 parameters\nModel variable rconv7/w1r:0, 20480 parameters\nModel variable rconv7/norm2/beta:0, 160 parameters\nModel variable rconv7/w2:0, 322560 parameters\nModel variable rconv8/norm1/beta:0, 224 parameters\nModel variable rconv8/w1a:0, 322560 parameters\nModel variable rconv8/w1b:0, 129024 parameters\nModel variable rconv8/norm2/beta:0, 224 parameters\nModel variable rconv8/w2:0, 451584 parameters\nModel variable rconv9/norm1/beta:0, 224 parameters\nModel variable rconv9/w1a:0, 322560 parameters\nModel variable rconv9/w1b:0, 129024 parameters\nModel variable rconv9/norm2/beta:0, 224 parameters\nModel variable rconv9/w2:0, 451584 parameters\nModel variable rconv10/norm1/beta:0, 224 parameters\nModel variable rconv10/w1a:0, 322560 parameters\nModel variable rconv10/w1b:0, 129024 parameters\nModel variable rconv10/norm2/beta:0, 224 parameters\nModel variable rconv10/w2:0, 451584 parameters\nModel variable rconv11/norm1/beta:0, 224 parameters\nModel variable rconv11/w1a:0, 322560 parameters\nModel variable rconv11/w1b:0, 129024 parameters\nModel variable rconv11/norm1b/beta:0, 64 parameters\nModel variable rconv11/w1r:0, 20480 parameters\nModel variable rconv11/norm2/beta:0, 160 parameters\nModel variable rconv11/w2:0, 322560 parameters\nModel variable rconv13/norm1/beta:0, 224 parameters\nModel variable rconv13/w1a:0, 322560 parameters\nModel variable rconv13/w1b:0, 129024 parameters\nModel variable rconv13/norm2/beta:0, 224 parameters\nModel variable rconv13/w2:0, 451584 parameters\nModel variable rconv14/norm1/beta:0, 224 parameters\nModel variable rconv14/w1a:0, 322560 parameters\nModel variable rconv14/w1b:0, 129024 parameters\nModel variable rconv14/norm2/beta:0, 224 parameters\nModel variable rconv14/w2:0, 451584 parameters\nModel variable trunk/norm/beta:0, 224 parameters\nModel variable p1/intermediate_conv/w:0, 96768 parameters\nModel variable g1/w:0, 64512 parameters\nModel variable g1/norm/beta:0, 32 parameters\nModel variable matmulg2w:0, 3072 parameters\nModel variable p1/norm/beta:0, 48 parameters\nModel variable p2/w:0, 48 parameters\nBuilt model, 10901664 total parameters\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg_1\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg_1\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg_1\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg_1\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg_1\nAdditional update op on train step: trunk/norm/AssignMovingAvg\nAdditional update op on train step: trunk/norm/AssignMovingAvg_1\nAdditional update op on train step: g1/norm/AssignMovingAvg\nAdditional update op on train step: g1/norm/AssignMovingAvg_1\nAdditional update op on train step: p1/norm/AssignMovingAvg\nAdditional update op on train step: p1/norm/AssignMovingAvg_1\nOpening H5 file: data.h5\nAdjusting H5 cache settings to: [0, 521, 134217728, 0.75]\nTraining\n2018-09-14 20:44:04.209964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-14 20:44:04.210288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\npciBusID: 0000:01:00.0\ntotalMemory: 7.93GiB freeMemory: 7.34GiB\n2018-09-14 20:44:04.210299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-09-14 20:44:04.581780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-14 20:44:04.581800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-09-14 20:44:04.581804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-09-14 20:44:04.581914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7077 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\nBegan session\nTraining on 13234987 rows, validating on 688542/688542 rows\nEpoch size = 20000\nh5_chunk_size = 6000\nBatch size = 200\nL2 coeff value = 3e-05\nuse_ranks = False\npredict_pass = False\n2018-09-14 20:44:05.383592: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\n\t.  Registered:  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='GPU'; T in [DT_INT64]\n  device='GPU'; T in [DT_INT32]\n\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\n    return fn(*args)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\n    options, feed_dict, fetch_list, target_list, run_metadata)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\n\t.  Registered:  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='GPU'; T in [DT_INT64]\n  device='GPU'; T in [DT_INT32]\n\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"./train.py\", line 455, in <module>\n    vmetrics_evaled = merge_dicts(run_validation_in_batches(vmetrics), np.sum)\n  File \"./train.py\", line 325, in run_validation_in_batches\n    result = run(fetches, rows, symmetries=[False,False,False], training=False)\n  File \"./train.py\", line 309, in run\n    model.is_training: training\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\n    run_metadata_ptr)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\n    run_metadata)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\n\t.  Registered:  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='GPU'; T in [DT_INT64]\n  device='GPU'; T in [DT_INT32]\n\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]", "body": "Note: I'm noob to tensorflow. I can provide more info if needed.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. I trained https://github.com/lightvector/GoNN on my own data (`data.h5`) without code modification\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: gentoo (kernel 4.17.14-gentoo)  uptodate\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no mobile device\r\n- **TensorFlow installed from (source or binary)**: source (from gentoo repository)\r\n- **TensorFlow version (use command below)**: 'unknown' 1.10.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0-r3 p1.4\r\n- **CUDA/cuDNN version**: cuda 9.2.88 / cudnn 7.1.4\r\n- **GPU model and memory**: GeForce GTX 1070 Ti 7.93GiB\r\n- **Exact command to reproduce**: `./train.py -traindir models -fast-factor 50 -gamesh5 data.h5`\r\n\r\n**Extra:** \r\n- ebuild from tensorflow I used: https://gitweb.gentoo.org/repo/gentoo.git/tree/sci-libs/tensorflow/tensorflow-1.10.0.ebuild\r\n- use flag : \r\n`sci-libs/tensorflow cuda -jemalloc -system-libs -mpi PYTHON_TARGETS: -* python3_6`\r\n\r\n### Describe the problem\r\n\r\nWhen I run tensorflow on GPU, it crashed.\r\n\r\nWhen I run on CPU, it works:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=\"\" ./train.py  -traindir models -fast-factor 50 -gamesh5 data.h5\r\n```\r\n\r\n### Source code / logs\r\n\r\nCode:\r\n\r\nhttps://github.com/lightvector/GoNN/tree/2d74b1300fef6700e75af7f2344d852798b713fb\r\n\r\nLogs:\r\n\r\n```\r\n ~/c/GoNN: ./train.py -traindir models -fast-factor 50 -gamesh5 data.h5                                                                                                                                      1058ms  ven. 14 sept. 2018 20:43:51 CEST\r\nBuilding model\r\nWARNING:tensorflow:From /home/tychota/code/GoNN/model.py:912: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\r\n\r\nAdjusting gradient for rconv1/w1:0 by Tensor(\"PadV2:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv1/w2:0 by Tensor(\"PadV2_1:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv2/w1a:0 by Tensor(\"PadV2_2:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv2/w1b:0 by Tensor(\"PadV2_3:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv2/w2:0 by Tensor(\"PadV2_4:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv3/w1a:0 by Tensor(\"PadV2_5:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv3/w1b:0 by Tensor(\"PadV2_6:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv3/w2:0 by Tensor(\"PadV2_7:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv4/w1a:0 by Tensor(\"PadV2_8:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv4/w1b:0 by Tensor(\"PadV2_9:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv4/w2:0 by Tensor(\"PadV2_10:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv5/w1a:0 by Tensor(\"PadV2_11:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv5/w1b:0 by Tensor(\"PadV2_12:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv5/w2:0 by Tensor(\"PadV2_13:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv7/w1a:0 by Tensor(\"PadV2_14:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv7/w1b:0 by Tensor(\"PadV2_15:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv7/w2:0 by Tensor(\"PadV2_16:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv8/w1a:0 by Tensor(\"PadV2_17:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv8/w1b:0 by Tensor(\"PadV2_18:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv8/w2:0 by Tensor(\"PadV2_19:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv9/w1a:0 by Tensor(\"PadV2_20:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv9/w1b:0 by Tensor(\"PadV2_21:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv9/w2:0 by Tensor(\"PadV2_22:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv10/w1a:0 by Tensor(\"PadV2_23:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv10/w1b:0 by Tensor(\"PadV2_24:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv10/w2:0 by Tensor(\"PadV2_25:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv11/w1a:0 by Tensor(\"PadV2_26:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv11/w1b:0 by Tensor(\"PadV2_27:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv11/w2:0 by Tensor(\"PadV2_28:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv13/w1a:0 by Tensor(\"PadV2_29:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv13/w1b:0 by Tensor(\"PadV2_30:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv13/w2:0 by Tensor(\"PadV2_31:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv14/w1a:0 by Tensor(\"PadV2_32:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv14/w1b:0 by Tensor(\"PadV2_33:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv14/w2:0 by Tensor(\"PadV2_34:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for p1/norm/beta:0 by 0.25\r\nAdjusting gradient for p2/w:0 by 0.25\r\nModel variable conv1/wcenter:0, 4256 parameters\r\nModel variable conv1/w:0, 106400 parameters\r\nModel variable rconv1/norm1/beta:0, 224 parameters\r\nModel variable rconv1/w1:0, 451584 parameters\r\nModel variable rconv1/norm2/beta:0, 224 parameters\r\nModel variable rconv1/w2:0, 451584 parameters\r\nModel variable rconv2/norm1/beta:0, 224 parameters\r\nModel variable rconv2/w1a:0, 322560 parameters\r\nModel variable rconv2/w1b:0, 129024 parameters\r\nModel variable rconv2/norm2/beta:0, 224 parameters\r\nModel variable rconv2/w2:0, 451584 parameters\r\nModel variable rconv3/norm1/beta:0, 224 parameters\r\nModel variable rconv3/w1a:0, 322560 parameters\r\nModel variable rconv3/w1b:0, 129024 parameters\r\nModel variable rconv3/norm2/beta:0, 224 parameters\r\nModel variable rconv3/w2:0, 451584 parameters\r\nModel variable rconv4/norm1/beta:0, 224 parameters\r\nModel variable rconv4/w1a:0, 322560 parameters\r\nModel variable rconv4/w1b:0, 129024 parameters\r\nModel variable rconv4/norm2/beta:0, 224 parameters\r\nModel variable rconv4/w2:0, 451584 parameters\r\nModel variable rconv5/norm1/beta:0, 224 parameters\r\nModel variable rconv5/w1a:0, 322560 parameters\r\nModel variable rconv5/w1b:0, 129024 parameters\r\nModel variable rconv5/norm2/beta:0, 224 parameters\r\nModel variable rconv5/w2:0, 451584 parameters\r\nModel variable rconv7/norm1/beta:0, 224 parameters\r\nModel variable rconv7/w1a:0, 322560 parameters\r\nModel variable rconv7/w1b:0, 129024 parameters\r\nModel variable rconv7/norm1b/beta:0, 64 parameters\r\nModel variable rconv7/w1r:0, 20480 parameters\r\nModel variable rconv7/norm2/beta:0, 160 parameters\r\nModel variable rconv7/w2:0, 322560 parameters\r\nModel variable rconv8/norm1/beta:0, 224 parameters\r\nModel variable rconv8/w1a:0, 322560 parameters\r\nModel variable rconv8/w1b:0, 129024 parameters\r\nModel variable rconv8/norm2/beta:0, 224 parameters\r\nModel variable rconv8/w2:0, 451584 parameters\r\nModel variable rconv9/norm1/beta:0, 224 parameters\r\nModel variable rconv9/w1a:0, 322560 parameters\r\nModel variable rconv9/w1b:0, 129024 parameters\r\nModel variable rconv9/norm2/beta:0, 224 parameters\r\nModel variable rconv9/w2:0, 451584 parameters\r\nModel variable rconv10/norm1/beta:0, 224 parameters\r\nModel variable rconv10/w1a:0, 322560 parameters\r\nModel variable rconv10/w1b:0, 129024 parameters\r\nModel variable rconv10/norm2/beta:0, 224 parameters\r\nModel variable rconv10/w2:0, 451584 parameters\r\nModel variable rconv11/norm1/beta:0, 224 parameters\r\nModel variable rconv11/w1a:0, 322560 parameters\r\nModel variable rconv11/w1b:0, 129024 parameters\r\nModel variable rconv11/norm1b/beta:0, 64 parameters\r\nModel variable rconv11/w1r:0, 20480 parameters\r\nModel variable rconv11/norm2/beta:0, 160 parameters\r\nModel variable rconv11/w2:0, 322560 parameters\r\nModel variable rconv13/norm1/beta:0, 224 parameters\r\nModel variable rconv13/w1a:0, 322560 parameters\r\nModel variable rconv13/w1b:0, 129024 parameters\r\nModel variable rconv13/norm2/beta:0, 224 parameters\r\nModel variable rconv13/w2:0, 451584 parameters\r\nModel variable rconv14/norm1/beta:0, 224 parameters\r\nModel variable rconv14/w1a:0, 322560 parameters\r\nModel variable rconv14/w1b:0, 129024 parameters\r\nModel variable rconv14/norm2/beta:0, 224 parameters\r\nModel variable rconv14/w2:0, 451584 parameters\r\nModel variable trunk/norm/beta:0, 224 parameters\r\nModel variable p1/intermediate_conv/w:0, 96768 parameters\r\nModel variable g1/w:0, 64512 parameters\r\nModel variable g1/norm/beta:0, 32 parameters\r\nModel variable matmulg2w:0, 3072 parameters\r\nModel variable p1/norm/beta:0, 48 parameters\r\nModel variable p2/w:0, 48 parameters\r\nBuilt model, 10901664 total parameters\r\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg\r\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg_1\r\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg\r\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg_1\r\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: trunk/norm/AssignMovingAvg\r\nAdditional update op on train step: trunk/norm/AssignMovingAvg_1\r\nAdditional update op on train step: g1/norm/AssignMovingAvg\r\nAdditional update op on train step: g1/norm/AssignMovingAvg_1\r\nAdditional update op on train step: p1/norm/AssignMovingAvg\r\nAdditional update op on train step: p1/norm/AssignMovingAvg_1\r\nOpening H5 file: data.h5\r\nAdjusting H5 cache settings to: [0, 521, 134217728, 0.75]\r\nTraining\r\n2018-09-14 20:44:04.209964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-14 20:44:04.210288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.34GiB\r\n2018-09-14 20:44:04.210299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-14 20:44:04.581780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-14 20:44:04.581800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-14 20:44:04.581804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-14 20:44:04.581914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7077 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nBegan session\r\nTraining on 13234987 rows, validating on 688542/688542 rows\r\nEpoch size = 20000\r\nh5_chunk_size = 6000\r\nBatch size = 200\r\nL2 coeff value = 3e-05\r\nuse_ranks = False\r\npredict_pass = False\r\n2018-09-14 20:44:05.383592: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\r\n\t.  Registered:  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n\r\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\r\n\t.  Registered:  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n\r\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./train.py\", line 455, in <module>\r\n    vmetrics_evaled = merge_dicts(run_validation_in_batches(vmetrics), np.sum)\r\n  File \"./train.py\", line 325, in run_validation_in_batches\r\n    result = run(fetches, rows, symmetries=[False,False,False], training=False)\r\n  File \"./train.py\", line 309, in run\r\n    model.is_training: training\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\r\n\t.  Registered:  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n\r\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\r\n\r\n```\r\n"}