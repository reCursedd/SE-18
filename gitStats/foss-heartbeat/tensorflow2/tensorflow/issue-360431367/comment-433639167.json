{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433639167", "html_url": "https://github.com/tensorflow/tensorflow/issues/22285#issuecomment-433639167", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22285", "id": 433639167, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzYzOTE2Nw==", "user": {"login": "lightvector", "id": 11942395, "node_id": "MDQ6VXNlcjExOTQyMzk1", "avatar_url": "https://avatars1.githubusercontent.com/u/11942395?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lightvector", "html_url": "https://github.com/lightvector", "followers_url": "https://api.github.com/users/lightvector/followers", "following_url": "https://api.github.com/users/lightvector/following{/other_user}", "gists_url": "https://api.github.com/users/lightvector/gists{/gist_id}", "starred_url": "https://api.github.com/users/lightvector/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lightvector/subscriptions", "organizations_url": "https://api.github.com/users/lightvector/orgs", "repos_url": "https://api.github.com/users/lightvector/repos", "events_url": "https://api.github.com/users/lightvector/events{/privacy}", "received_events_url": "https://api.github.com/users/lightvector/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-27T17:22:45Z", "updated_at": "2018-10-27T18:14:52Z", "author_association": "NONE", "body_html": "<p>Ran into this myself after upgrading tensorflow versions. The below seems to cause the bug.<br>\nEdit: Looks like tweaking the implementation of apply_symmetry to use tf.cond directly to switch on whether to apply tf.reverse or not rather than to build a rev_axes tensor works around the issue. I'm guessing there's some bug in the layout optimizer where tensorflow is not expecting this funny way of nesting operations?</p>\n<pre><code>#!/usr/bin/python3\nimport math\nimport tensorflow as tf\nimport numpy as np\n\ndef apply_symmetry(tensor,symmetries,inverse):\n  ud = symmetries[0]\n  lr = symmetries[1]\n  transp = symmetries[2]\n\n  rev_axes = tf.concat([\n    tf.cond(ud, lambda: tf.constant([1]), lambda: tf.constant([],dtype='int32')),\n    tf.cond(lr, lambda: tf.constant([2]), lambda: tf.constant([],dtype='int32')),\n  ], axis=0)\n\n  if not inverse:\n    tensor = tf.reverse(tensor, rev_axes)\n\n  tensor = tf.cond(\n    transp,\n    lambda: tf.transpose(tensor, [0,2,1,3]),\n    lambda: tensor)\n\n  if inverse:\n    tensor = tf.reverse(tensor, rev_axes)\n\n  return tensor\n\ndef conv_weight_variable(diam1, diam2, in_channels, out_channels):\n  weights = tf.Variable(tf.zeros([diam1,diam2,in_channels,out_channels]))\n  return weights\ndef conv_layer(in_layer, diam, in_channels, out_channels):\n  weights = conv_weight_variable(diam, diam, in_channels, out_channels)\n  out_layer = tf.nn.conv2d(in_layer, weights, strides=[1,1,1,1], padding='SAME')\n  return out_layer\n\n\ninputs = tf.placeholder(tf.float32, [None, 8, 8, 4]) # NHWC\nsymmetries = tf.placeholder(tf.bool, [3])\nlayer = apply_symmetry(inputs,symmetries,inverse=False)\nlayer = conv_layer(layer, diam=3, in_channels=4, out_channels=1)\nlayer = apply_symmetry(layer,symmetries,inverse=True)\nlayer = tf.reshape(layer,[-1,64]) # Convert [N, 8, 8, 1] into [N,64]\n\nwith tf.Session() as session:\n  session.run(tf.global_variables_initializer())\n  fetches = layer\n  result = session.run(fetches, feed_dict={\n    inputs: np.zeros([1,8,8,4]),\n    symmetries: [False,False,False]\n  })\n  print(result)\n\n</code></pre>", "body_text": "Ran into this myself after upgrading tensorflow versions. The below seems to cause the bug.\nEdit: Looks like tweaking the implementation of apply_symmetry to use tf.cond directly to switch on whether to apply tf.reverse or not rather than to build a rev_axes tensor works around the issue. I'm guessing there's some bug in the layout optimizer where tensorflow is not expecting this funny way of nesting operations?\n#!/usr/bin/python3\nimport math\nimport tensorflow as tf\nimport numpy as np\n\ndef apply_symmetry(tensor,symmetries,inverse):\n  ud = symmetries[0]\n  lr = symmetries[1]\n  transp = symmetries[2]\n\n  rev_axes = tf.concat([\n    tf.cond(ud, lambda: tf.constant([1]), lambda: tf.constant([],dtype='int32')),\n    tf.cond(lr, lambda: tf.constant([2]), lambda: tf.constant([],dtype='int32')),\n  ], axis=0)\n\n  if not inverse:\n    tensor = tf.reverse(tensor, rev_axes)\n\n  tensor = tf.cond(\n    transp,\n    lambda: tf.transpose(tensor, [0,2,1,3]),\n    lambda: tensor)\n\n  if inverse:\n    tensor = tf.reverse(tensor, rev_axes)\n\n  return tensor\n\ndef conv_weight_variable(diam1, diam2, in_channels, out_channels):\n  weights = tf.Variable(tf.zeros([diam1,diam2,in_channels,out_channels]))\n  return weights\ndef conv_layer(in_layer, diam, in_channels, out_channels):\n  weights = conv_weight_variable(diam, diam, in_channels, out_channels)\n  out_layer = tf.nn.conv2d(in_layer, weights, strides=[1,1,1,1], padding='SAME')\n  return out_layer\n\n\ninputs = tf.placeholder(tf.float32, [None, 8, 8, 4]) # NHWC\nsymmetries = tf.placeholder(tf.bool, [3])\nlayer = apply_symmetry(inputs,symmetries,inverse=False)\nlayer = conv_layer(layer, diam=3, in_channels=4, out_channels=1)\nlayer = apply_symmetry(layer,symmetries,inverse=True)\nlayer = tf.reshape(layer,[-1,64]) # Convert [N, 8, 8, 1] into [N,64]\n\nwith tf.Session() as session:\n  session.run(tf.global_variables_initializer())\n  fetches = layer\n  result = session.run(fetches, feed_dict={\n    inputs: np.zeros([1,8,8,4]),\n    symmetries: [False,False,False]\n  })\n  print(result)", "body": "Ran into this myself after upgrading tensorflow versions. The below seems to cause the bug.\r\nEdit: Looks like tweaking the implementation of apply_symmetry to use tf.cond directly to switch on whether to apply tf.reverse or not rather than to build a rev_axes tensor works around the issue. I'm guessing there's some bug in the layout optimizer where tensorflow is not expecting this funny way of nesting operations?\r\n\r\n```\r\n#!/usr/bin/python3\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef apply_symmetry(tensor,symmetries,inverse):\r\n  ud = symmetries[0]\r\n  lr = symmetries[1]\r\n  transp = symmetries[2]\r\n\r\n  rev_axes = tf.concat([\r\n    tf.cond(ud, lambda: tf.constant([1]), lambda: tf.constant([],dtype='int32')),\r\n    tf.cond(lr, lambda: tf.constant([2]), lambda: tf.constant([],dtype='int32')),\r\n  ], axis=0)\r\n\r\n  if not inverse:\r\n    tensor = tf.reverse(tensor, rev_axes)\r\n\r\n  tensor = tf.cond(\r\n    transp,\r\n    lambda: tf.transpose(tensor, [0,2,1,3]),\r\n    lambda: tensor)\r\n\r\n  if inverse:\r\n    tensor = tf.reverse(tensor, rev_axes)\r\n\r\n  return tensor\r\n\r\ndef conv_weight_variable(diam1, diam2, in_channels, out_channels):\r\n  weights = tf.Variable(tf.zeros([diam1,diam2,in_channels,out_channels]))\r\n  return weights\r\ndef conv_layer(in_layer, diam, in_channels, out_channels):\r\n  weights = conv_weight_variable(diam, diam, in_channels, out_channels)\r\n  out_layer = tf.nn.conv2d(in_layer, weights, strides=[1,1,1,1], padding='SAME')\r\n  return out_layer\r\n\r\n\r\ninputs = tf.placeholder(tf.float32, [None, 8, 8, 4]) # NHWC\r\nsymmetries = tf.placeholder(tf.bool, [3])\r\nlayer = apply_symmetry(inputs,symmetries,inverse=False)\r\nlayer = conv_layer(layer, diam=3, in_channels=4, out_channels=1)\r\nlayer = apply_symmetry(layer,symmetries,inverse=True)\r\nlayer = tf.reshape(layer,[-1,64]) # Convert [N, 8, 8, 1] into [N,64]\r\n\r\nwith tf.Session() as session:\r\n  session.run(tf.global_variables_initializer())\r\n  fetches = layer\r\n  result = session.run(fetches, feed_dict={\r\n    inputs: np.zeros([1,8,8,4]),\r\n    symmetries: [False,False,False]\r\n  })\r\n  print(result)\r\n\r\n```\r\n\r\n"}