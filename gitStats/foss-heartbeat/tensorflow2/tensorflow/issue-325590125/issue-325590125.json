{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19491", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19491/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19491/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19491/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19491", "id": 325590125, "node_id": "MDU6SXNzdWUzMjU1OTAxMjU=", "number": 19491, "title": "Performance problem with TensorFlow training", "user": {"login": "fcacarminati", "id": 4223137, "node_id": "MDQ6VXNlcjQyMjMxMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/4223137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fcacarminati", "html_url": "https://github.com/fcacarminati", "followers_url": "https://api.github.com/users/fcacarminati/followers", "following_url": "https://api.github.com/users/fcacarminati/following{/other_user}", "gists_url": "https://api.github.com/users/fcacarminati/gists{/gist_id}", "starred_url": "https://api.github.com/users/fcacarminati/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fcacarminati/subscriptions", "organizations_url": "https://api.github.com/users/fcacarminati/orgs", "repos_url": "https://api.github.com/users/fcacarminati/repos", "events_url": "https://api.github.com/users/fcacarminati/events{/privacy}", "received_events_url": "https://api.github.com/users/fcacarminati/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 969831806, "node_id": "MDU6TGFiZWw5Njk4MzE4MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/WIP", "name": "WIP", "color": "f4c77f", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, {"login": "ezhulenev", "id": 1174378, "node_id": "MDQ6VXNlcjExNzQzNzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1174378?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezhulenev", "html_url": "https://github.com/ezhulenev", "followers_url": "https://api.github.com/users/ezhulenev/followers", "following_url": "https://api.github.com/users/ezhulenev/following{/other_user}", "gists_url": "https://api.github.com/users/ezhulenev/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezhulenev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezhulenev/subscriptions", "organizations_url": "https://api.github.com/users/ezhulenev/orgs", "repos_url": "https://api.github.com/users/ezhulenev/repos", "events_url": "https://api.github.com/users/ezhulenev/events{/privacy}", "received_events_url": "https://api.github.com/users/ezhulenev/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 24, "created_at": "2018-05-23T07:56:21Z", "updated_at": "2018-11-19T23:25:17Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hello,<br>\nWe are running a not-very-complex 3D convolution problem had we have extremely poor performance. Here is the summary of our problem</p>\n<p>Now the technical part.</p>\n<p>I am running on an Haswell CPU in a Mac OS running High Sierra.</p>\n<p>Model Name:\tMacBook Pro<br>\nModel Identifier:\tMacBookPro11,5<br>\nProcessor Name:\tIntel Core i7<br>\nProcessor Speed:\t2.5 GHz<br>\nNumber of Processors:\t1<br>\nTotal Number of Cores:\t4<br>\nL2 Cache (per Core):\t256 KB<br>\nL3 Cache:\t6 MB<br>\nMemory:\t16 GB</p>\n<h2>Tensorflow performance</h2>\n<p>1.) Memory allocation</p>\n<p>Memory allocation seems highly unoptimized. I see an allocation of ~80GB (78M allocations) out of which we are left with 37GB persistent (corresponding to 575k permanent allocations). The memory churn is enormous and this may affect very seriously performance. Most of those are very small allocation / deallocation which happen here</p>\n<p>Eigen::NonBlockingThreadPoolTempltensorflow::thread::EigenEnvironment::Schedule(std::__1::function&lt;void ()&gt;)</p>\n<p>I have tried to run with tcmalloc from google hoping to improve memory allocation and handling. tcmalloc complains that there are the following \u201clarge allocation\u201d from TF even before starting the epochs: (23+23+2+18+4+9+2) = 81GB of allocation even before starting the epoch\u2019s. After that my disk is full of swap files and my machine dies.</p>\n<p>Then I went back running with the Mac allocator, which surprisingly seems to be more robust.</p>\n<p>2.) Code performance</p>\n<p>A careful VTune analysis performed by Sofia has identified Eigen as the major source of CPU consumption.  All the time is wasted simply in repacking (gemm_pack_rhs).</p>\n<p>To look at the code I attempted to compile with -g, however the default compilation is with -g0 and I could not find yet a way to replace this default on bazel. I added -g3 that, according to the manual (and to a small test I have made) should override -g0. However the Mac Instrument (a poor relation of VTune on Mac) could not find out the source. The library should be _pywrap_tensorflow_internal.so. Then I went looking for the source and I found that gemm_pack_rhs::operator() is defined in the following files</p>\n<p>./bazel-tensorflow/external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h<br>\n./bazel-tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h<br>\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h</p>\n<p>The last two are identical. Putting good old printf\u2019s I discovered we are calling the GeneralBlockPanelKernel.h version. The operator works with packets of size 8, which is fine for AVX2 (256) and float32 as we are using. However I am not sure that the compiler manages to vectorize this procedure. Indeed, most of the time is spent in line 559 of eigen_volume_patch.h.</p>\n<p>for (int i = 0; i &lt; PacketSize; ++i) {<br>\nvalues[i] = coeff(index + i);<br>\n}</p>\n<p>The packet structure of the code is meant to have each packet treated as a unit. This for loop simply destroys all possibility of optimisation. There is a lot of room for optimisation in tensorflow before we get really serious about performance with a problem like ours. But who is going to pick up the tab?</p>\n<p>3.) MKL or not MKL.</p>\n<p>When bringing up this problem, I have been answered that tensorflow in Mac does not support the usage of MKL, and therefore, till then, my findings were not entirely relevant. MKL for Mac exists, however clang does not support OMP (or rather the default version of clang distributed with Mac does not have OMP support enabled). So the only way to compile tensorflow on the Mac with MKL was to change compiler.</p>\n<p>Unfortunately changing compiler with bazel on the Mac seems a very ambitious proposition. After posting to and perusing stackoverflow, bazel forum and tensorflow forum, I came to the following recipe</p>\n<p>export BAZEL_USE_CPP_ONLY_TOOLCHAIN=1<br>\nexport CC=/path/to/compiler<br>\nbazel build [\u2026]</p>\n<p>does indeed force Bazel to use a new compiler, however controlling the compiler switches is much more complicated. The two compiler flags -Wthread-safety and -Wself-assign, as well as the linker flag \u201c-no-as-needed\u201d and \u201c-z\u201d are incompatible with g++ linker. The CROSSTOOL.tpl are automatically generated during configuration. The only occurrences of (for instance) -Wself-assign in the TF code are in</p>\n<p>third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl<br>\nthird_party/toolchains/clang6/CROSSTOOL.tpl<br>\nthird_party/toolchains/cpus/arm/CROSSTOOL.tpl</p>\n<p>but even if I comment the lines:</p>\n<ul>\n<li>compiler_flag: \"-Wthread-safety\"</li>\n<li>compiler_flag: \"-Wself-assign\"<br>\n+#  compiler_flag: \"-Wthread-safety\"<br>\n+#  compiler_flag: \"-Wself-assign\u201d</li>\n</ul>\n<p>in all three of them \u201csomething\u201d creates a CROSSTOOL.tpl with these flags in. The hack I am trying now is to configure and then edit the file</p>\n<p>./bazel-tensorflow/external/local_config_cc/cc_wrapper.sh</p>\n<p>adding the following line</p>\n<p>/sw/bin/gcc-fsf-7 <code>echo \"$@\" | sed -e 's/-Wself-assign//' | sed -e 's/-Wthread-safety//' | sed -e 's/-Wl,-no-as-needed//' | sed -e 's/-Wl,-z,relro,-z,now//\u2018</code></p>\n<p>which is a very poor hack.</p>\n<p>With this I could build a version of tensorflow using the Mac MKL, but to no avail. Performance is still abysmal with the same bottleneck.</p>\n<p>Thanks for reading up to here...</p>", "body_text": "Hello,\nWe are running a not-very-complex 3D convolution problem had we have extremely poor performance. Here is the summary of our problem\nNow the technical part.\nI am running on an Haswell CPU in a Mac OS running High Sierra.\nModel Name:\tMacBook Pro\nModel Identifier:\tMacBookPro11,5\nProcessor Name:\tIntel Core i7\nProcessor Speed:\t2.5 GHz\nNumber of Processors:\t1\nTotal Number of Cores:\t4\nL2 Cache (per Core):\t256 KB\nL3 Cache:\t6 MB\nMemory:\t16 GB\nTensorflow performance\n1.) Memory allocation\nMemory allocation seems highly unoptimized. I see an allocation of ~80GB (78M allocations) out of which we are left with 37GB persistent (corresponding to 575k permanent allocations). The memory churn is enormous and this may affect very seriously performance. Most of those are very small allocation / deallocation which happen here\nEigen::NonBlockingThreadPoolTempltensorflow::thread::EigenEnvironment::Schedule(std::__1::function<void ()>)\nI have tried to run with tcmalloc from google hoping to improve memory allocation and handling. tcmalloc complains that there are the following \u201clarge allocation\u201d from TF even before starting the epochs: (23+23+2+18+4+9+2) = 81GB of allocation even before starting the epoch\u2019s. After that my disk is full of swap files and my machine dies.\nThen I went back running with the Mac allocator, which surprisingly seems to be more robust.\n2.) Code performance\nA careful VTune analysis performed by Sofia has identified Eigen as the major source of CPU consumption.  All the time is wasted simply in repacking (gemm_pack_rhs).\nTo look at the code I attempted to compile with -g, however the default compilation is with -g0 and I could not find yet a way to replace this default on bazel. I added -g3 that, according to the manual (and to a small test I have made) should override -g0. However the Mac Instrument (a poor relation of VTune on Mac) could not find out the source. The library should be _pywrap_tensorflow_internal.so. Then I went looking for the source and I found that gemm_pack_rhs::operator() is defined in the following files\n./bazel-tensorflow/external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h\n./bazel-tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h\nThe last two are identical. Putting good old printf\u2019s I discovered we are calling the GeneralBlockPanelKernel.h version. The operator works with packets of size 8, which is fine for AVX2 (256) and float32 as we are using. However I am not sure that the compiler manages to vectorize this procedure. Indeed, most of the time is spent in line 559 of eigen_volume_patch.h.\nfor (int i = 0; i < PacketSize; ++i) {\nvalues[i] = coeff(index + i);\n}\nThe packet structure of the code is meant to have each packet treated as a unit. This for loop simply destroys all possibility of optimisation. There is a lot of room for optimisation in tensorflow before we get really serious about performance with a problem like ours. But who is going to pick up the tab?\n3.) MKL or not MKL.\nWhen bringing up this problem, I have been answered that tensorflow in Mac does not support the usage of MKL, and therefore, till then, my findings were not entirely relevant. MKL for Mac exists, however clang does not support OMP (or rather the default version of clang distributed with Mac does not have OMP support enabled). So the only way to compile tensorflow on the Mac with MKL was to change compiler.\nUnfortunately changing compiler with bazel on the Mac seems a very ambitious proposition. After posting to and perusing stackoverflow, bazel forum and tensorflow forum, I came to the following recipe\nexport BAZEL_USE_CPP_ONLY_TOOLCHAIN=1\nexport CC=/path/to/compiler\nbazel build [\u2026]\ndoes indeed force Bazel to use a new compiler, however controlling the compiler switches is much more complicated. The two compiler flags -Wthread-safety and -Wself-assign, as well as the linker flag \u201c-no-as-needed\u201d and \u201c-z\u201d are incompatible with g++ linker. The CROSSTOOL.tpl are automatically generated during configuration. The only occurrences of (for instance) -Wself-assign in the TF code are in\nthird_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\nthird_party/toolchains/clang6/CROSSTOOL.tpl\nthird_party/toolchains/cpus/arm/CROSSTOOL.tpl\nbut even if I comment the lines:\n\ncompiler_flag: \"-Wthread-safety\"\ncompiler_flag: \"-Wself-assign\"\n+#  compiler_flag: \"-Wthread-safety\"\n+#  compiler_flag: \"-Wself-assign\u201d\n\nin all three of them \u201csomething\u201d creates a CROSSTOOL.tpl with these flags in. The hack I am trying now is to configure and then edit the file\n./bazel-tensorflow/external/local_config_cc/cc_wrapper.sh\nadding the following line\n/sw/bin/gcc-fsf-7 echo \"$@\" | sed -e 's/-Wself-assign//' | sed -e 's/-Wthread-safety//' | sed -e 's/-Wl,-no-as-needed//' | sed -e 's/-Wl,-z,relro,-z,now//\u2018\nwhich is a very poor hack.\nWith this I could build a version of tensorflow using the Mac MKL, but to no avail. Performance is still abysmal with the same bottleneck.\nThanks for reading up to here...", "body": "Hello,\r\n    We are running a not-very-complex 3D convolution problem had we have extremely poor performance. Here is the summary of our problem\r\n\r\nNow the technical part.\r\n\r\nI am running on an Haswell CPU in a Mac OS running High Sierra. \r\n\r\nModel Name:\tMacBook Pro\r\nModel Identifier:\tMacBookPro11,5\r\nProcessor Name:\tIntel Core i7\r\nProcessor Speed:\t2.5 GHz\r\nNumber of Processors:\t1\r\nTotal Number of Cores:\t4\r\nL2 Cache (per Core):\t256 KB\r\nL3 Cache:\t6 MB\r\nMemory:\t16 GB\r\n\r\n\r\nTensorflow performance\r\n---------------------------------\r\n\r\n1.) Memory allocation\r\n\r\nMemory allocation seems highly unoptimized. I see an allocation of ~80GB (78M allocations) out of which we are left with 37GB persistent (corresponding to 575k permanent allocations). The memory churn is enormous and this may affect very seriously performance. Most of those are very small allocation / deallocation which happen here\r\n\r\nEigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::Schedule(std::__1::function<void ()>)\r\n\r\nI have tried to run with tcmalloc from google hoping to improve memory allocation and handling. tcmalloc complains that there are the following \u201clarge allocation\u201d from TF even before starting the epochs: (23+23+2+18+4+9+2) = 81GB of allocation even before starting the epoch\u2019s. After that my disk is full of swap files and my machine dies.\r\n\r\nThen I went back running with the Mac allocator, which surprisingly seems to be more robust. \r\n\r\n2.) Code performance\r\n\r\nA careful VTune analysis performed by Sofia has identified Eigen as the major source of CPU consumption.  All the time is wasted simply in repacking (gemm_pack_rhs).\r\n\r\nTo look at the code I attempted to compile with -g, however the default compilation is with -g0 and I could not find yet a way to replace this default on bazel. I added -g3 that, according to the manual (and to a small test I have made) should override -g0. However the Mac Instrument (a poor relation of VTune on Mac) could not find out the source. The library should be _pywrap_tensorflow_internal.so. Then I went looking for the source and I found that gemm_pack_rhs::operator() is defined in the following files\r\n\r\n./bazel-tensorflow/external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h\r\n./bazel-tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h\r\n\r\nThe last two are identical. Putting good old printf\u2019s I discovered we are calling the GeneralBlockPanelKernel.h version. The operator works with packets of size 8, which is fine for AVX2 (256) and float32 as we are using. However I am not sure that the compiler manages to vectorize this procedure. Indeed, most of the time is spent in line 559 of eigen_volume_patch.h. \r\n\r\n   for (int i = 0; i < PacketSize; ++i) {\r\n      values[i] = coeff(index + i);\r\n    }\r\n\r\nThe packet structure of the code is meant to have each packet treated as a unit. This for loop simply destroys all possibility of optimisation. There is a lot of room for optimisation in tensorflow before we get really serious about performance with a problem like ours. But who is going to pick up the tab? \r\n\r\n3.) MKL or not MKL. \r\n\r\nWhen bringing up this problem, I have been answered that tensorflow in Mac does not support the usage of MKL, and therefore, till then, my findings were not entirely relevant. MKL for Mac exists, however clang does not support OMP (or rather the default version of clang distributed with Mac does not have OMP support enabled). So the only way to compile tensorflow on the Mac with MKL was to change compiler. \r\n\r\nUnfortunately changing compiler with bazel on the Mac seems a very ambitious proposition. After posting to and perusing stackoverflow, bazel forum and tensorflow forum, I came to the following recipe\r\n\r\nexport BAZEL_USE_CPP_ONLY_TOOLCHAIN=1 \r\nexport CC=/path/to/compiler\r\nbazel build [\u2026]\r\n\r\ndoes indeed force Bazel to use a new compiler, however controlling the compiler switches is much more complicated. The two compiler flags -Wthread-safety and -Wself-assign, as well as the linker flag \u201c-no-as-needed\u201d and \u201c-z\u201d are incompatible with g++ linker. The CROSSTOOL.tpl are automatically generated during configuration. The only occurrences of (for instance) -Wself-assign in the TF code are in \r\n\r\nthird_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\nthird_party/toolchains/clang6/CROSSTOOL.tpl\r\nthird_party/toolchains/cpus/arm/CROSSTOOL.tpl\r\n\r\nbut even if I comment the lines:\r\n\r\n-  compiler_flag: \"-Wthread-safety\"\r\n-  compiler_flag: \"-Wself-assign\"\r\n+#  compiler_flag: \"-Wthread-safety\"\r\n+#  compiler_flag: \"-Wself-assign\u201d\r\n\r\nin all three of them \u201csomething\u201d creates a CROSSTOOL.tpl with these flags in. The hack I am trying now is to configure and then edit the file\r\n\r\n./bazel-tensorflow/external/local_config_cc/cc_wrapper.sh\r\n\r\nadding the following line\r\n\r\n/sw/bin/gcc-fsf-7 `echo \"$@\" | sed -e 's/-Wself-assign//' | sed -e 's/-Wthread-safety//' | sed -e 's/-Wl,-no-as-needed//' | sed -e 's/-Wl,-z,relro,-z,now//\u2018`\r\n\r\nwhich is a very poor hack. \r\n\r\nWith this I could build a version of tensorflow using the Mac MKL, but to no avail. Performance is still abysmal with the same bottleneck. \r\n\r\nThanks for reading up to here..."}