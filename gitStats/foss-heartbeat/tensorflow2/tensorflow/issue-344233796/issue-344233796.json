{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21108", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21108/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21108/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21108/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21108", "id": 344233796, "node_id": "MDU6SXNzdWUzNDQyMzM3OTY=", "number": 21108, "title": "Higher iterations_per_loop values in TPU training lead to NaN gradients!", "user": {"login": "mrezak", "id": 4903456, "node_id": "MDQ6VXNlcjQ5MDM0NTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/4903456?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrezak", "html_url": "https://github.com/mrezak", "followers_url": "https://api.github.com/users/mrezak/followers", "following_url": "https://api.github.com/users/mrezak/following{/other_user}", "gists_url": "https://api.github.com/users/mrezak/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrezak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrezak/subscriptions", "organizations_url": "https://api.github.com/users/mrezak/orgs", "repos_url": "https://api.github.com/users/mrezak/repos", "events_url": "https://api.github.com/users/mrezak/events{/privacy}", "received_events_url": "https://api.github.com/users/mrezak/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-24T23:02:37Z", "updated_at": "2018-08-01T20:54:28Z", "closed_at": "2018-08-01T20:54:13Z", "author_association": "NONE", "body_html": "<p>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes<br>\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform (Linux Debian)<br>\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:<br>\nTensorFlow installed from (source or binary):Binary<br>\nTensorFlow version (use command below):1.9<br>\nPython version: 2.7<br>\nBazel version (if compiling from source):<br>\nGCC/Compiler version (if compiling from source):<br>\nCUDA/cuDNN version: NA (TPU)<br>\nGPU model and memory: NA (TPU)<br>\nExact command to reproduce:</p>\n<h3>Describe the problem</h3>\n<p>I am using TPU and my model trains properly with <code>tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=20)</code> but when I increase <code>iterations_per_loop</code> to say 50 or 100, the gradients becomes NaN during the training (see the error below). I consistently see this behavior with higher values of <code>iterations_per_loop</code> while lower values (e.g &lt;20) never lead to NaN gradients.</p>\n<p>I tried to set <code>num_shards=1</code> but the problem is the same.</p>\n<h3>Source code / logs</h3>\n<pre><code>InvalidArgumentError (see above for traceback): Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN : Tensor had NaN values\n\t [[Node: CheckNumerics_11 = CheckNumerics[T=DT_FLOAT, message=\"Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN\", _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](Read_13/ReadVariableOp)]]\n</code></pre>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform (Linux Debian)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):Binary\nTensorFlow version (use command below):1.9\nPython version: 2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: NA (TPU)\nGPU model and memory: NA (TPU)\nExact command to reproduce:\nDescribe the problem\nI am using TPU and my model trains properly with tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=20) but when I increase iterations_per_loop to say 50 or 100, the gradients becomes NaN during the training (see the error below). I consistently see this behavior with higher values of iterations_per_loop while lower values (e.g <20) never lead to NaN gradients.\nI tried to set num_shards=1 but the problem is the same.\nSource code / logs\nInvalidArgumentError (see above for traceback): Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN : Tensor had NaN values\n\t [[Node: CheckNumerics_11 = CheckNumerics[T=DT_FLOAT, message=\"Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN\", _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](Read_13/ReadVariableOp)]]", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform (Linux Debian)\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):Binary\r\nTensorFlow version (use command below):1.9\r\nPython version: 2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: NA (TPU)\r\nGPU model and memory: NA (TPU)\r\nExact command to reproduce:\r\n\r\n### Describe the problem\r\nI am using TPU and my model trains properly with `tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=20)` but when I increase `iterations_per_loop` to say 50 or 100, the gradients becomes NaN during the training (see the error below). I consistently see this behavior with higher values of `iterations_per_loop` while lower values (e.g <20) never lead to NaN gradients. \r\n\r\nI tried to set `num_shards=1` but the problem is the same.\r\n\r\n \r\n### Source code / logs\r\n```\r\nInvalidArgumentError (see above for traceback): Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN : Tensor had NaN values\r\n\t [[Node: CheckNumerics_11 = CheckNumerics[T=DT_FLOAT, message=\"Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN\", _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](Read_13/ReadVariableOp)]]\r\n```\r\n"}