{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271086206", "html_url": "https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-271086206", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5147", "id": 271086206, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTA4NjIwNg==", "user": {"login": "standy66", "id": 1818586, "node_id": "MDQ6VXNlcjE4MTg1ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1818586?v=4", "gravatar_id": "", "url": "https://api.github.com/users/standy66", "html_url": "https://github.com/standy66", "followers_url": "https://api.github.com/users/standy66/followers", "following_url": "https://api.github.com/users/standy66/following{/other_user}", "gists_url": "https://api.github.com/users/standy66/gists{/gist_id}", "starred_url": "https://api.github.com/users/standy66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/standy66/subscriptions", "organizations_url": "https://api.github.com/users/standy66/orgs", "repos_url": "https://api.github.com/users/standy66/repos", "events_url": "https://api.github.com/users/standy66/events{/privacy}", "received_events_url": "https://api.github.com/users/standy66/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-07T14:16:19Z", "updated_at": "2017-01-08T08:31:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4667922\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cbockman\">@cbockman</a>, I use the following workaround:</p>\n<pre><code># Imagine inputs is a list or tuple of tensors representing single training example.\n# In my case, inputs is a tuple (features, label) obtained by reading TFRecords.\n\ndtypes = list(map(lambda x: x.dtype, inputs))\nshapes = list(map(lambda x: x.get_shape(), inputs))\nqueue = tf.RandomShuffleQueue(CAPACITY, MIN_AFTER_DEQUEUE, dtypes)\nenqueue_op = queue.enqueue(inputs)\nqr = tf.train.QueueRunner(queue, [enqueue_op] * NUM_THREADS)\ntf.add_to_collection(tf.GraphKeys.QUEUE_RUNNERS, qr)\ninputs = queue.dequeue()\nfor tensor, shape in zip(inputs, shapes):\n    tensor.set_shape(shape)\n\n# Now you can use tf.train.batch with dynamic_pad=True, and the order in which\n# it enqueues elements will be permuted because of RandomShuffleQueue.\ninputs_batch = tf.train.batch(inputs, batch_size, capacity=capacity,\n                              dynamic_pad=True, name=name)\n</code></pre>\n<p>Don't forget to start queue runners. If you use SparseTensors there is some extra work of packing and unpacking them into a list of plain tensors.</p>", "body_text": "@cbockman, I use the following workaround:\n# Imagine inputs is a list or tuple of tensors representing single training example.\n# In my case, inputs is a tuple (features, label) obtained by reading TFRecords.\n\ndtypes = list(map(lambda x: x.dtype, inputs))\nshapes = list(map(lambda x: x.get_shape(), inputs))\nqueue = tf.RandomShuffleQueue(CAPACITY, MIN_AFTER_DEQUEUE, dtypes)\nenqueue_op = queue.enqueue(inputs)\nqr = tf.train.QueueRunner(queue, [enqueue_op] * NUM_THREADS)\ntf.add_to_collection(tf.GraphKeys.QUEUE_RUNNERS, qr)\ninputs = queue.dequeue()\nfor tensor, shape in zip(inputs, shapes):\n    tensor.set_shape(shape)\n\n# Now you can use tf.train.batch with dynamic_pad=True, and the order in which\n# it enqueues elements will be permuted because of RandomShuffleQueue.\ninputs_batch = tf.train.batch(inputs, batch_size, capacity=capacity,\n                              dynamic_pad=True, name=name)\n\nDon't forget to start queue runners. If you use SparseTensors there is some extra work of packing and unpacking them into a list of plain tensors.", "body": "@cbockman, I use the following workaround:\r\n\r\n```\r\n# Imagine inputs is a list or tuple of tensors representing single training example.\r\n# In my case, inputs is a tuple (features, label) obtained by reading TFRecords.\r\n\r\ndtypes = list(map(lambda x: x.dtype, inputs))\r\nshapes = list(map(lambda x: x.get_shape(), inputs))\r\nqueue = tf.RandomShuffleQueue(CAPACITY, MIN_AFTER_DEQUEUE, dtypes)\r\nenqueue_op = queue.enqueue(inputs)\r\nqr = tf.train.QueueRunner(queue, [enqueue_op] * NUM_THREADS)\r\ntf.add_to_collection(tf.GraphKeys.QUEUE_RUNNERS, qr)\r\ninputs = queue.dequeue()\r\nfor tensor, shape in zip(inputs, shapes):\r\n    tensor.set_shape(shape)\r\n\r\n# Now you can use tf.train.batch with dynamic_pad=True, and the order in which\r\n# it enqueues elements will be permuted because of RandomShuffleQueue.\r\ninputs_batch = tf.train.batch(inputs, batch_size, capacity=capacity,\r\n                              dynamic_pad=True, name=name)\r\n```\r\n\r\nDon't forget to start queue runners. If you use SparseTensors there is some extra work of packing and unpacking them into a list of plain tensors."}