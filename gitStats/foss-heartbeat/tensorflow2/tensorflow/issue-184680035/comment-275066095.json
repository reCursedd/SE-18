{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275066095", "html_url": "https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-275066095", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5147", "id": 275066095, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTA2NjA5NQ==", "user": {"login": "eiennohito", "id": 1021694, "node_id": "MDQ6VXNlcjEwMjE2OTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1021694?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eiennohito", "html_url": "https://github.com/eiennohito", "followers_url": "https://api.github.com/users/eiennohito/followers", "following_url": "https://api.github.com/users/eiennohito/following{/other_user}", "gists_url": "https://api.github.com/users/eiennohito/gists{/gist_id}", "starred_url": "https://api.github.com/users/eiennohito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eiennohito/subscriptions", "organizations_url": "https://api.github.com/users/eiennohito/orgs", "repos_url": "https://api.github.com/users/eiennohito/repos", "events_url": "https://api.github.com/users/eiennohito/events{/privacy}", "received_events_url": "https://api.github.com/users/eiennohito/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-25T09:58:56Z", "updated_at": "2017-01-25T09:58:56Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1818586\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/standy66\">@standy66</a> in my case, a workaround like that had a bottleneck between <code>ShuffleFIFOQueue</code> and <code>batch</code> operations. One thread over there could not keep up with the training itself.</p>\n<p>However, increasing the number of threads of <code>batch</code> caused contention on queueing and decreased throughput compared to one thread for ~20% for even two threads.</p>\n<p>Sequence processing with tensorflow is not still as good as it can be.</p>", "body_text": "@standy66 in my case, a workaround like that had a bottleneck between ShuffleFIFOQueue and batch operations. One thread over there could not keep up with the training itself.\nHowever, increasing the number of threads of batch caused contention on queueing and decreased throughput compared to one thread for ~20% for even two threads.\nSequence processing with tensorflow is not still as good as it can be.", "body": "@standy66 in my case, a workaround like that had a bottleneck between `ShuffleFIFOQueue` and `batch` operations. One thread over there could not keep up with the training itself.\r\n\r\nHowever, increasing the number of threads of `batch` caused contention on queueing and decreased throughput compared to one thread for ~20% for even two threads. \r\n\r\nSequence processing with tensorflow is not still as good as it can be.\r\n"}