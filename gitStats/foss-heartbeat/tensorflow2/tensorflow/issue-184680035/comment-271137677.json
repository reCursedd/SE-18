{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271137677", "html_url": "https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-271137677", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5147", "id": 271137677, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTEzNzY3Nw==", "user": {"login": "standy66", "id": 1818586, "node_id": "MDQ6VXNlcjE4MTg1ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1818586?v=4", "gravatar_id": "", "url": "https://api.github.com/users/standy66", "html_url": "https://github.com/standy66", "followers_url": "https://api.github.com/users/standy66/followers", "following_url": "https://api.github.com/users/standy66/following{/other_user}", "gists_url": "https://api.github.com/users/standy66/gists{/gist_id}", "starred_url": "https://api.github.com/users/standy66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/standy66/subscriptions", "organizations_url": "https://api.github.com/users/standy66/orgs", "repos_url": "https://api.github.com/users/standy66/repos", "events_url": "https://api.github.com/users/standy66/events{/privacy}", "received_events_url": "https://api.github.com/users/standy66/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-08T08:25:16Z", "updated_at": "2017-01-08T08:32:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4667922\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cbockman\">@cbockman</a>, no, we are not loading everything into memory. I use this solution myself for a research problem employing large datasets (100GB+ of raw data) serialized in TFRecords format.</p>\n<p>I should clarify: <em>inputs</em> is a list or tuple of tensors, representing a single training example, (in my case they are obtained by reading a TFRecord file format with TFRecorReader). Maybe this can help:</p>\n<pre><code>reader = TFRecordReader()\nkey, value = reader.read(filename_queue)\n\n# deserialize is a custom function that deserializes string\n# representation of a single tf.train.Example into tensors\n# (features, label) representing single training example\nfeatures, label = deserialize(value)\n\ninputs = [features, label]\n...\n</code></pre>\n<p>I updated the solution above.</p>", "body_text": "@cbockman, no, we are not loading everything into memory. I use this solution myself for a research problem employing large datasets (100GB+ of raw data) serialized in TFRecords format.\nI should clarify: inputs is a list or tuple of tensors, representing a single training example, (in my case they are obtained by reading a TFRecord file format with TFRecorReader). Maybe this can help:\nreader = TFRecordReader()\nkey, value = reader.read(filename_queue)\n\n# deserialize is a custom function that deserializes string\n# representation of a single tf.train.Example into tensors\n# (features, label) representing single training example\nfeatures, label = deserialize(value)\n\ninputs = [features, label]\n...\n\nI updated the solution above.", "body": "@cbockman, no, we are not loading everything into memory. I use this solution myself for a research problem employing large datasets (100GB+ of raw data) serialized in TFRecords format.\r\n\r\nI should clarify: _inputs_ is a list or tuple of tensors, representing a single training example, (in my case they are obtained by reading a TFRecord file format with TFRecorReader). Maybe this can help:\r\n\r\n```\r\nreader = TFRecordReader()\r\nkey, value = reader.read(filename_queue)\r\n\r\n# deserialize is a custom function that deserializes string\r\n# representation of a single tf.train.Example into tensors\r\n# (features, label) representing single training example\r\nfeatures, label = deserialize(value)\r\n\r\ninputs = [features, label]\r\n...\r\n```\r\n\r\nI updated the solution above."}