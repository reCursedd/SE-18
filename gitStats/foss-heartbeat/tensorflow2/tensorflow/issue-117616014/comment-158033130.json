{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/158033130", "html_url": "https://github.com/tensorflow/tensorflow/issues/272#issuecomment-158033130", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/272", "id": 158033130, "node_id": "MDEyOklzc3VlQ29tbWVudDE1ODAzMzEzMA==", "user": {"login": "mjwillson", "id": 4502, "node_id": "MDQ6VXNlcjQ1MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4502?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjwillson", "html_url": "https://github.com/mjwillson", "followers_url": "https://api.github.com/users/mjwillson/followers", "following_url": "https://api.github.com/users/mjwillson/following{/other_user}", "gists_url": "https://api.github.com/users/mjwillson/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjwillson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjwillson/subscriptions", "organizations_url": "https://api.github.com/users/mjwillson/orgs", "repos_url": "https://api.github.com/users/mjwillson/repos", "events_url": "https://api.github.com/users/mjwillson/events{/privacy}", "received_events_url": "https://api.github.com/users/mjwillson/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-19T11:48:52Z", "updated_at": "2015-11-19T12:15:03Z", "author_association": "NONE", "body_html": "<p>Cool, thanks!</p>\n<p>Slightly confused by the logits thing -- in the docs that refers to the first argument (the input to the softmax) not the 'labels'/one-hot-vectors argument, no? Even then, the inputs to the softmax are unnormalized log-probabilities, but are they logits? I thought logits were log(p / (1-p)), rather than log p + const.</p>\n<p>Edit: I'm guessing it was called 'logits' by (dodgy?) analogy to the corresponding argument in <code>sigmoid_cross_entropy_with_logits</code> ?</p>", "body_text": "Cool, thanks!\nSlightly confused by the logits thing -- in the docs that refers to the first argument (the input to the softmax) not the 'labels'/one-hot-vectors argument, no? Even then, the inputs to the softmax are unnormalized log-probabilities, but are they logits? I thought logits were log(p / (1-p)), rather than log p + const.\nEdit: I'm guessing it was called 'logits' by (dodgy?) analogy to the corresponding argument in sigmoid_cross_entropy_with_logits ?", "body": "Cool, thanks!\n\nSlightly confused by the logits thing -- in the docs that refers to the first argument (the input to the softmax) not the 'labels'/one-hot-vectors argument, no? Even then, the inputs to the softmax are unnormalized log-probabilities, but are they logits? I thought logits were log(p / (1-p)), rather than log p + const.\n\nEdit: I'm guessing it was called 'logits' by (dodgy?) analogy to the corresponding argument in `sigmoid_cross_entropy_with_logits` ?\n"}