{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10815", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10815/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10815/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10815/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10815", "id": 236800362, "node_id": "MDU6SXNzdWUyMzY4MDAzNjI=", "number": 10815, "title": "Feature request: Add a subclass of seq2seq.Decoder to support regression", "user": {"login": "rickyhan", "id": 1768528, "node_id": "MDQ6VXNlcjE3Njg1Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1768528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rickyhan", "html_url": "https://github.com/rickyhan", "followers_url": "https://api.github.com/users/rickyhan/followers", "following_url": "https://api.github.com/users/rickyhan/following{/other_user}", "gists_url": "https://api.github.com/users/rickyhan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rickyhan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rickyhan/subscriptions", "organizations_url": "https://api.github.com/users/rickyhan/orgs", "repos_url": "https://api.github.com/users/rickyhan/repos", "events_url": "https://api.github.com/users/rickyhan/events{/privacy}", "received_events_url": "https://api.github.com/users/rickyhan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 20, "created_at": "2017-06-19T07:20:01Z", "updated_at": "2018-01-25T02:38:08Z", "closed_at": "2018-01-25T02:38:08Z", "author_association": "NONE", "body_html": "<p>Currently, seq2seq decoder class only supports classification which uses 1D softmax with embedding. The library is very good for this particular task. However, seq2seq is also extremely useful in regression tasks by replacing embedding with a dense layer.</p>\n<p>As of 1.2, the current architecture only allows 1D sequence data by supplying a Dense layer as _embedding_fn to TrainingHelper. Currently, training is working(loss decreases) but I have yet to find a way to decode.</p>\n<p>I have tried to modify these following pieces to support 2D regression:<br>\n<code>TrainingHelper</code><br>\n<code>BasicDecoder</code><br>\n<code>dynamic_decode</code><br>\n<code>GreedyEmbeddingHelper</code>(used during decoding, not working)</p>\n<p>I had to change a lot of seq2seq internals, but here is more or less my code:</p>\n<div class=\"highlight highlight-source-python\"><pre>            <span class=\"pl-c1\">self</span>.decoder_cell, <span class=\"pl-c1\">self</span>.decoder_initial_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.build_decoder_cell()\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Input projection layer to feed embedded inputs to the cell</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> ** Essential when use_residual=True to match input/output dims</span>\n            input_layer <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">self</span>.hidden_units, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dtype, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_projection<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Output projection layer to convert cell_outputs to actual values</span>\n            output_layer <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">self</span>.dimension, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>output_projection<span class=\"pl-pds\">'</span></span>)\n</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> decoder_inputs_train :: [batch_size , max_time_steps + 1]</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Decoder inputs having gone through input projection layer</span>\n    <span class=\"pl-c1\">self</span>.decoder_inputs_proj <span class=\"pl-k\">=</span> input_layer(<span class=\"pl-c1\">self</span>.decoder_inputs_train)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Helper to feed inputs for training: read inputs from dense ground truth vectors</span>\n    training_helper <span class=\"pl-k\">=</span> seq2seq.TrainingHelper(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.decoder_inputs_proj,\n                                       <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.decoder_inputs_length_train,\n                                       <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                       <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>training_helper<span class=\"pl-pds\">'</span></span>)\n\n    training_decoder <span class=\"pl-k\">=</span> seq2seq.BasicDecoder(<span class=\"pl-v\">cell</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.decoder_cell,\n                                       <span class=\"pl-v\">helper</span><span class=\"pl-k\">=</span>training_helper,\n                                       <span class=\"pl-v\">initial_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.decoder_initial_state,\n                                       <span class=\"pl-v\">output_layer</span><span class=\"pl-k\">=</span>output_layer)\n                                       <span class=\"pl-c\"><span class=\"pl-c\">#</span>output_layer=None)</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Maximum decoder time_steps in current batch</span>\n    max_decoder_length <span class=\"pl-k\">=</span> tf.reduce_max(<span class=\"pl-c1\">self</span>.decoder_inputs_length_train)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> decoder_outputs_train: BasicDecoderOutput</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>                        namedtuple(rnn_outputs, sample_id)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, dimension] if output_time_major=False</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>                                   [max_time_step + 1, batch_size, dimension] if output_time_major=True</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> decoder_outputs_train.sample_id: [batch_size], tf.int32</span>\n    (<span class=\"pl-c1\">self</span>.decoder_outputs_train, <span class=\"pl-c1\">self</span>.decoder_last_state_train,\n     <span class=\"pl-c1\">self</span>.decoder_outputs_length_train) <span class=\"pl-k\">=</span> (seq2seq.dynamic_decode(\n        <span class=\"pl-v\">decoder</span><span class=\"pl-k\">=</span>training_decoder,\n        <span class=\"pl-v\">output_time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        <span class=\"pl-v\">impute_finished</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n        <span class=\"pl-v\">maximum_iterations</span><span class=\"pl-k\">=</span>max_decoder_length))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> More efficient to do the projection on the batch-time-concatenated tensor</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> logits_train: [batch_size, max_time_step + 1, dimension]</span>\n    <span class=\"pl-c1\">self</span>.decoder_logits_train <span class=\"pl-k\">=</span> tf.identity(<span class=\"pl-c1\">self</span>.decoder_outputs_train.rnn_output)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> RMSE</span>\n    <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.sqrt(\n          tf.abs(tf.subtract(<span class=\"pl-c1\">self</span>.decoder_logits_train, <span class=\"pl-c1\">self</span>.decoder_targets_train))\n        ), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n    <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_sum(<span class=\"pl-c1\">self</span>.loss)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Contruct graphs for minimizing loss</span>\n    <span class=\"pl-c1\">self</span>.init_optimizer()</pre></div>\n<p>And here is the code for <code>decoding</code> and this is where I get all sorts of errors:</p>\n<pre><code>\nelif self.mode == 'decode':\n\n    # Start_tokens: [batch_size,] `int32` vector\n    start_tokens = tf.ones([self.batch_size, self.dimension], tf.float32) * 0.1337\n    end_token = 0.1337\n\n    def project_inputs(inputs):\n        print \"INPUT SHAPE\", inputs.shape\n        return input_layer(inputs)\n\n    if not self.use_beamsearch_decode:\n        # Helper to feed inputs for greedy decoding: uses the argmax of the output\n        decoding_helper = seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n                                                        end_token=end_token,\n                                                        embedding=project_inputs)\n        # Basic decoder performs greedy decoding at each time step\n        print(\"building greedy decoder..\")\n        inference_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n                                                 helper=decoding_helper,\n                                                 initial_state=self.decoder_initial_state,\n                                                 output_layer=output_layer)\n    # For GreedyDecoder, return\n    # decoder_outputs_decode: BasicDecoderOutput instance\n    #                         namedtuple(rnn_outputs, sample_id)\n    # decoder_outputs_decode.rnn_output: [batch_size, max_time_step, num_decoder_symbols]   if output_time_major=False\n    #                                    [max_time_step, batch_size, num_decoder_symbols]   if output_time_major=True\n    # decoder_outputs_decode.sample_id: [batch_size, max_time_step], tf.int32       if output_time_major=False\n    #                                   [max_time_step, batch_size], tf.int32               if output_time_major=True\n\n    (self.decoder_outputs_decode, self.decoder_last_state_decode,\n     self.decoder_outputs_length_decode) = (seq2seq.dynamic_decode(\n        decoder=inference_decoder,\n        output_time_major=False,\n        #impute_finished=True,  # error occurs\n        maximum_iterations=self.max_decode_step))\n\n    if not self.use_beamsearch_decode:\n        # decoder_outputs_decode.sample_id: [batch_size, max_time_step]\n        # Or use argmax to find decoder symbols to emit:\n        # self.decoder_pred_decode = tf.argmax(self.decoder_outputs_decode.rnn_output,\n        #                                      axis=-1, name='decoder_pred_decode')\n\n        # Here, we use expand_dims to be compatible with the result of the beamsearch decoder\n        # decoder_pred_decode: [batch_size, max_time_step, 1] (output_major=False)\n        self.decoder_pred_decode = tf.expand_dims(self.decoder_outputs_decode.sample_id, -1)\n</code></pre>\n<p>It would be great if someone could sort it out and streamline the process.</p>", "body_text": "Currently, seq2seq decoder class only supports classification which uses 1D softmax with embedding. The library is very good for this particular task. However, seq2seq is also extremely useful in regression tasks by replacing embedding with a dense layer.\nAs of 1.2, the current architecture only allows 1D sequence data by supplying a Dense layer as _embedding_fn to TrainingHelper. Currently, training is working(loss decreases) but I have yet to find a way to decode.\nI have tried to modify these following pieces to support 2D regression:\nTrainingHelper\nBasicDecoder\ndynamic_decode\nGreedyEmbeddingHelper(used during decoding, not working)\nI had to change a lot of seq2seq internals, but here is more or less my code:\n            self.decoder_cell, self.decoder_initial_state = self.build_decoder_cell()\n\n            # Input projection layer to feed embedded inputs to the cell\n            # ** Essential when use_residual=True to match input/output dims\n            input_layer = Dense(self.hidden_units, dtype=self.dtype, name='input_projection')\n\n            # Output projection layer to convert cell_outputs to actual values\n            output_layer = Dense(self.dimension, name='output_projection')\n\nif self.mode == 'train':\n    # decoder_inputs_train :: [batch_size , max_time_steps + 1]\n    # Decoder inputs having gone through input projection layer\n    self.decoder_inputs_proj = input_layer(self.decoder_inputs_train)\n\n    # Helper to feed inputs for training: read inputs from dense ground truth vectors\n    training_helper = seq2seq.TrainingHelper(inputs=self.decoder_inputs_proj,\n                                       sequence_length=self.decoder_inputs_length_train,\n                                       time_major=False,\n                                       name='training_helper')\n\n    training_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n                                       helper=training_helper,\n                                       initial_state=self.decoder_initial_state,\n                                       output_layer=output_layer)\n                                       #output_layer=None)\n\n    # Maximum decoder time_steps in current batch\n    max_decoder_length = tf.reduce_max(self.decoder_inputs_length_train)\n\n    # decoder_outputs_train: BasicDecoderOutput\n    #                        namedtuple(rnn_outputs, sample_id)\n    # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, dimension] if output_time_major=False\n    #                                   [max_time_step + 1, batch_size, dimension] if output_time_major=True\n    # decoder_outputs_train.sample_id: [batch_size], tf.int32\n    (self.decoder_outputs_train, self.decoder_last_state_train,\n     self.decoder_outputs_length_train) = (seq2seq.dynamic_decode(\n        decoder=training_decoder,\n        output_time_major=False,\n        impute_finished=True,\n        maximum_iterations=max_decoder_length))\n\n    # More efficient to do the projection on the batch-time-concatenated tensor\n    # logits_train: [batch_size, max_time_step + 1, dimension]\n    self.decoder_logits_train = tf.identity(self.decoder_outputs_train.rnn_output)\n\n    # RMSE\n    self.loss = tf.reduce_mean(tf.sqrt(\n          tf.abs(tf.subtract(self.decoder_logits_train, self.decoder_targets_train))\n        ), axis=2)\n    self.loss = tf.reduce_sum(self.loss)\n\n    # Contruct graphs for minimizing loss\n    self.init_optimizer()\nAnd here is the code for decoding and this is where I get all sorts of errors:\n\nelif self.mode == 'decode':\n\n    # Start_tokens: [batch_size,] `int32` vector\n    start_tokens = tf.ones([self.batch_size, self.dimension], tf.float32) * 0.1337\n    end_token = 0.1337\n\n    def project_inputs(inputs):\n        print \"INPUT SHAPE\", inputs.shape\n        return input_layer(inputs)\n\n    if not self.use_beamsearch_decode:\n        # Helper to feed inputs for greedy decoding: uses the argmax of the output\n        decoding_helper = seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n                                                        end_token=end_token,\n                                                        embedding=project_inputs)\n        # Basic decoder performs greedy decoding at each time step\n        print(\"building greedy decoder..\")\n        inference_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n                                                 helper=decoding_helper,\n                                                 initial_state=self.decoder_initial_state,\n                                                 output_layer=output_layer)\n    # For GreedyDecoder, return\n    # decoder_outputs_decode: BasicDecoderOutput instance\n    #                         namedtuple(rnn_outputs, sample_id)\n    # decoder_outputs_decode.rnn_output: [batch_size, max_time_step, num_decoder_symbols]   if output_time_major=False\n    #                                    [max_time_step, batch_size, num_decoder_symbols]   if output_time_major=True\n    # decoder_outputs_decode.sample_id: [batch_size, max_time_step], tf.int32       if output_time_major=False\n    #                                   [max_time_step, batch_size], tf.int32               if output_time_major=True\n\n    (self.decoder_outputs_decode, self.decoder_last_state_decode,\n     self.decoder_outputs_length_decode) = (seq2seq.dynamic_decode(\n        decoder=inference_decoder,\n        output_time_major=False,\n        #impute_finished=True,  # error occurs\n        maximum_iterations=self.max_decode_step))\n\n    if not self.use_beamsearch_decode:\n        # decoder_outputs_decode.sample_id: [batch_size, max_time_step]\n        # Or use argmax to find decoder symbols to emit:\n        # self.decoder_pred_decode = tf.argmax(self.decoder_outputs_decode.rnn_output,\n        #                                      axis=-1, name='decoder_pred_decode')\n\n        # Here, we use expand_dims to be compatible with the result of the beamsearch decoder\n        # decoder_pred_decode: [batch_size, max_time_step, 1] (output_major=False)\n        self.decoder_pred_decode = tf.expand_dims(self.decoder_outputs_decode.sample_id, -1)\n\nIt would be great if someone could sort it out and streamline the process.", "body": "Currently, seq2seq decoder class only supports classification which uses 1D softmax with embedding. The library is very good for this particular task. However, seq2seq is also extremely useful in regression tasks by replacing embedding with a dense layer.\r\n\r\nAs of 1.2, the current architecture only allows 1D sequence data by supplying a Dense layer as _embedding_fn to TrainingHelper. Currently, training is working(loss decreases) but I have yet to find a way to decode.\r\n\r\nI have tried to modify these following pieces to support 2D regression:\r\n`TrainingHelper`\r\n`BasicDecoder`\r\n`dynamic_decode`\r\n`GreedyEmbeddingHelper`(used during decoding, not working)\r\n\r\nI had to change a lot of seq2seq internals, but here is more or less my code:\r\n```python\r\n            self.decoder_cell, self.decoder_initial_state = self.build_decoder_cell()\r\n\r\n            # Input projection layer to feed embedded inputs to the cell\r\n            # ** Essential when use_residual=True to match input/output dims\r\n            input_layer = Dense(self.hidden_units, dtype=self.dtype, name='input_projection')\r\n\r\n            # Output projection layer to convert cell_outputs to actual values\r\n            output_layer = Dense(self.dimension, name='output_projection')\r\n\r\n```\r\n```python\r\n\r\nif self.mode == 'train':\r\n    # decoder_inputs_train :: [batch_size , max_time_steps + 1]\r\n    # Decoder inputs having gone through input projection layer\r\n    self.decoder_inputs_proj = input_layer(self.decoder_inputs_train)\r\n\r\n    # Helper to feed inputs for training: read inputs from dense ground truth vectors\r\n    training_helper = seq2seq.TrainingHelper(inputs=self.decoder_inputs_proj,\r\n                                       sequence_length=self.decoder_inputs_length_train,\r\n                                       time_major=False,\r\n                                       name='training_helper')\r\n\r\n    training_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\r\n                                       helper=training_helper,\r\n                                       initial_state=self.decoder_initial_state,\r\n                                       output_layer=output_layer)\r\n                                       #output_layer=None)\r\n\r\n    # Maximum decoder time_steps in current batch\r\n    max_decoder_length = tf.reduce_max(self.decoder_inputs_length_train)\r\n\r\n    # decoder_outputs_train: BasicDecoderOutput\r\n    #                        namedtuple(rnn_outputs, sample_id)\r\n    # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, dimension] if output_time_major=False\r\n    #                                   [max_time_step + 1, batch_size, dimension] if output_time_major=True\r\n    # decoder_outputs_train.sample_id: [batch_size], tf.int32\r\n    (self.decoder_outputs_train, self.decoder_last_state_train,\r\n     self.decoder_outputs_length_train) = (seq2seq.dynamic_decode(\r\n        decoder=training_decoder,\r\n        output_time_major=False,\r\n        impute_finished=True,\r\n        maximum_iterations=max_decoder_length))\r\n\r\n    # More efficient to do the projection on the batch-time-concatenated tensor\r\n    # logits_train: [batch_size, max_time_step + 1, dimension]\r\n    self.decoder_logits_train = tf.identity(self.decoder_outputs_train.rnn_output)\r\n\r\n    # RMSE\r\n    self.loss = tf.reduce_mean(tf.sqrt(\r\n          tf.abs(tf.subtract(self.decoder_logits_train, self.decoder_targets_train))\r\n        ), axis=2)\r\n    self.loss = tf.reduce_sum(self.loss)\r\n\r\n    # Contruct graphs for minimizing loss\r\n    self.init_optimizer()\r\n```\r\nAnd here is the code for `decoding` and this is where I get all sorts of errors:\r\n\r\n```\r\n\r\nelif self.mode == 'decode':\r\n\r\n    # Start_tokens: [batch_size,] `int32` vector\r\n    start_tokens = tf.ones([self.batch_size, self.dimension], tf.float32) * 0.1337\r\n    end_token = 0.1337\r\n\r\n    def project_inputs(inputs):\r\n        print \"INPUT SHAPE\", inputs.shape\r\n        return input_layer(inputs)\r\n\r\n    if not self.use_beamsearch_decode:\r\n        # Helper to feed inputs for greedy decoding: uses the argmax of the output\r\n        decoding_helper = seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\r\n                                                        end_token=end_token,\r\n                                                        embedding=project_inputs)\r\n        # Basic decoder performs greedy decoding at each time step\r\n        print(\"building greedy decoder..\")\r\n        inference_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\r\n                                                 helper=decoding_helper,\r\n                                                 initial_state=self.decoder_initial_state,\r\n                                                 output_layer=output_layer)\r\n    # For GreedyDecoder, return\r\n    # decoder_outputs_decode: BasicDecoderOutput instance\r\n    #                         namedtuple(rnn_outputs, sample_id)\r\n    # decoder_outputs_decode.rnn_output: [batch_size, max_time_step, num_decoder_symbols]   if output_time_major=False\r\n    #                                    [max_time_step, batch_size, num_decoder_symbols]   if output_time_major=True\r\n    # decoder_outputs_decode.sample_id: [batch_size, max_time_step], tf.int32       if output_time_major=False\r\n    #                                   [max_time_step, batch_size], tf.int32               if output_time_major=True\r\n\r\n    (self.decoder_outputs_decode, self.decoder_last_state_decode,\r\n     self.decoder_outputs_length_decode) = (seq2seq.dynamic_decode(\r\n        decoder=inference_decoder,\r\n        output_time_major=False,\r\n        #impute_finished=True,  # error occurs\r\n        maximum_iterations=self.max_decode_step))\r\n\r\n    if not self.use_beamsearch_decode:\r\n        # decoder_outputs_decode.sample_id: [batch_size, max_time_step]\r\n        # Or use argmax to find decoder symbols to emit:\r\n        # self.decoder_pred_decode = tf.argmax(self.decoder_outputs_decode.rnn_output,\r\n        #                                      axis=-1, name='decoder_pred_decode')\r\n\r\n        # Here, we use expand_dims to be compatible with the result of the beamsearch decoder\r\n        # decoder_pred_decode: [batch_size, max_time_step, 1] (output_major=False)\r\n        self.decoder_pred_decode = tf.expand_dims(self.decoder_outputs_decode.sample_id, -1)\r\n```\r\n\r\nIt would be great if someone could sort it out and streamline the process."}