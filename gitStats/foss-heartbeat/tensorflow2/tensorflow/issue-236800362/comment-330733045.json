{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/330733045", "html_url": "https://github.com/tensorflow/tensorflow/issues/10815#issuecomment-330733045", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10815", "id": 330733045, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDczMzA0NQ==", "user": {"login": "wangshang19911011", "id": 21189342, "node_id": "MDQ6VXNlcjIxMTg5MzQy", "avatar_url": "https://avatars1.githubusercontent.com/u/21189342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangshang19911011", "html_url": "https://github.com/wangshang19911011", "followers_url": "https://api.github.com/users/wangshang19911011/followers", "following_url": "https://api.github.com/users/wangshang19911011/following{/other_user}", "gists_url": "https://api.github.com/users/wangshang19911011/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangshang19911011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangshang19911011/subscriptions", "organizations_url": "https://api.github.com/users/wangshang19911011/orgs", "repos_url": "https://api.github.com/users/wangshang19911011/repos", "events_url": "https://api.github.com/users/wangshang19911011/events{/privacy}", "received_events_url": "https://api.github.com/users/wangshang19911011/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-20T03:11:10Z", "updated_at": "2017-09-21T09:16:18Z", "author_association": "NONE", "body_html": "<p>Hi, I'm doing similar decoder, but the decoder inputs is conditioned by the encoder final hidden states (context vector), <a href=\"https://arxiv.org/pdf/1702.05538.pdf\" rel=\"nofollow\">Related Paper</a> see picture a in page 3. The decoder is trying to fully inference during training with feeding previous outputs and context vector as inputs at each step.</p>\n<p>`class RNNEncoder_Decoder(object):</p>\n<pre><code>def __init__(self,input_dim,\n             context_dim,output_dim,hidden_dim,\n             layers_stacked_count,learning_rate):\n    \n    self.graph = tf.get_default_graph()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.context_dim = context_dim\n    self.hidden_dim = hidden_dim\n    self.layers_stacked_count = layers_stacked_count\n    self.learning_rate = learning_rate\n    self.sampling_probability = tf.constant(dtype=tf.float32,value=1.0)\n    \n    # [batch_size,sequence_length,input_dimension]\n    self.enc_inp = tf.placeholder(tf.float32, [None,None,self.input_dim], name='encoder_inputs')\n    self.expected_out = tf.placeholder(tf.float32, [None,None,self.output_dim], name='expected_outs')\n    # fullly inference during trianing\n    self.dec_inp = tf.zeros_like(self.expected_out,dtype=tf.float32,name='decoder_inputs')\n            \n    seq_length = tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(self.enc_inp), 2)), 1)\n    self.seq_length = tf.cast(seq_length, tf.int32)\n    \n    with tf.variable_scope('RNNEncoderDecoder'):\n        with tf.variable_scope(\"Enocder\") as encoder_varscope:\n            # create encoder LSTM cell\n            encoder_cells = []\n            for i in range(self.layers_stacked_count):\n                with tf.variable_scope('EncoderCell_{}'.format(i)):\n                    encoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\n                                                         use_peepholes=True))\n            self.encoder_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cells)\n\n            # ruuning dynamic rnn encoder                \n            _, enc_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\n                                             initial_state=None,\n                                             dtype=tf.float32,\n                                             inputs = self.enc_inp,\n                                             sequence_length = self.seq_length\n                                            )\n\n            # extract top layer hidden state as feature representation\n            self.context_vector = enc_state[-1].h\n            \n            cell_state0 = tf.zeros_like(enc_state[0].c,dtype=tf.float32)\n            hidden_state0 = tf.zeros_like(enc_state[0].h,dtype=tf.float32)\n\n            dec_init_state = (enc_state[1], # pass the top layer state of enocder to the bottom layer of decoder\n                              tf.nn.rnn_cell.LSTMStateTuple(cell_state0, hidden_state0))\n            \n            # condition extracted features on decoder inputs\n            # with a shape that matches decoder inputs in all but (potentially) the final dimension. \n            # tile context vector from [batch_size,context_dim] to [batch_size,decoder_sequence_length,context_dim]\n            context_vector_shape = tf.shape(self.context_vector)\n            context_vector_reshaped = tf.reshape(self.context_vector, \n                                                 [context_vector_shape[0], 1, context_vector_shape[1]]\n                                                )\n            enc_inp_shape = tf.shape(self.enc_inp)\n            self.auxiliary_inputs = tf.tile(context_vector_reshaped,\n                                       multiples=[1,enc_inp_shape[1],1]\n                                      )\n            \n        with tf.variable_scope(\"Deocder\") as decoder_varscope:\n            # create decoder LSTM cell\n            decoder_cells = []\n            for i in range(self.layers_stacked_count):\n                with tf.variable_scope('DecoderCell_{}'.format(i)):\n                    decoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\n                                                         use_peepholes=True))\n            self.decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_cells)\n\n            dec_out_dense = Dense(units = self.output_dim,\n                                  activation = None,\n                                  use_bias = False,\n                                  kernel_initializer = tf.truncated_normal_initializer(\n                                      dtype=tf.float32,\n                                      stddev = 1.0 / math.sqrt(float(self.hidden_dim))\n                                  ),\n                                  name = 'dec_outp_linear_projection'\n                                 )\n            \n            training_helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(\n                inputs = self.dec_inp,\n                sequence_length = self.seq_length,\n                auxiliary_inputs = self.auxiliary_inputs, # condtional on inputs\n                sampling_probability = 1.0, # for fullly inference\n                name = 'feeding_conditional_input'\n            )\n            \n            decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell = self.decoder_cell,\n                helper = training_helper,\n                initial_state = dec_init_state,\n                output_layer = dec_out_dense\n            )\n            \n            outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,\n                                                                               impute_finished = True\n                                                                              )\n        self.outputs = outputs\n        \n### optimize loss part\n\ndef get_decoder_prediction(self,X,session):\n    feed_dict = {\n        self.enc_inp:X\n    }\n    feed_dict.update({self.expected_out:X})\n    run = [self.outputs]\n    return session.run(run,feed_dict=feed_dict)\n</code></pre>\n<p>RNN_test = RNNEncoder_Decoder(input_dim=1,context_dim=32,output_dim=1,hidden_dim=32,layers_stacked_count=2,learning_rate=0.01)`</p>\n<p>Without \"auxiliary_inputs = self.auxiliary_inputs\", it running successfully,<br>\nbut with auxiliary_inputs = self.auxiliary_inputs<br>\nAnd I got following:</p>\n<p>`---------------------------------------------------------------------------<br>\nValueError                                Traceback (most recent call last)<br>\n in ()<br>\n9                           hidden_dim=hidden_dim,<br>\n10                           layers_stacked_count=layers_stacked_count,<br>\n---&gt; 11                           learning_rate=learning_rate<br>\n12                          )</p>\n<p> in <strong>init</strong>(self, input_dim, context_dim, output_dim, hidden_dim, layers_stacked_count, learning_rate)<br>\n98<br>\n99                 outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,<br>\n--&gt; 100                                                                                    impute_finished = True<br>\n101                                                                                   )<br>\n102             self.outputs = outputs</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)<br>\n284         ],<br>\n285         parallel_iterations=parallel_iterations,<br>\n--&gt; 286         swap_memory=swap_memory)<br>\n287<br>\n288     final_outputs_ta = res[1]</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)<br>\n2773     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)<br>\n2774     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)<br>\n-&gt; 2775     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)<br>\n2776     return result<br>\n2777</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)<br>\n2602       self.Enter()<br>\n2603       original_body_result, exit_vars = self._BuildLoop(<br>\n-&gt; 2604           pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n2605     finally:<br>\n2606       self.Exit()</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n2552         structure=original_loop_vars,<br>\n2553         flat_sequence=vars_for_body_with_tensor_arrays)<br>\n-&gt; 2554     body_result = body(*packed_vars_for_body)<br>\n2555     if not nest.is_sequence(body_result):<br>\n2556       body_result = [body_result]</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)<br>\n232       \"\"\"<br>\n233       (next_outputs, decoder_state, next_inputs,<br>\n--&gt; 234        decoder_finished) = decoder.step(time, inputs, state)<br>\n235       next_finished = math_ops.logical_or(decoder_finished, finished)<br>\n236       if maximum_iterations is not None:</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)<br>\n137     \"\"\"<br>\n138     with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):<br>\n--&gt; 139       cell_outputs, cell_state = self._cell(inputs, state)<br>\n140       if self._output_layer is not None:<br>\n141         cell_outputs = self._output_layer(cell_outputs)</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in <strong>call</strong>(self, inputs, state, scope)<br>\n178       with vs.variable_scope(vs.get_variable_scope(),<br>\n179                              custom_getter=self._rnn_get_variable):<br>\n--&gt; 180         return super(RNNCell, self).<strong>call</strong>(inputs, state)<br>\n181<br>\n182   def _rnn_get_variable(self, getter, *args, **kwargs):</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in <strong>call</strong>(self, inputs, *args, **kwargs)<br>\n448         # Check input assumptions set after layer building, e.g. input shape.<br>\n449         self._assert_input_compatibility(inputs)<br>\n--&gt; 450         outputs = self.call(inputs, *args, **kwargs)<br>\n451<br>\n452         # Apply activity regularization.</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)<br>\n936                                       [-1, cell.state_size])<br>\n937           cur_state_pos += cell.state_size<br>\n--&gt; 938         cur_inp, new_state = cell(cur_inp, cur_state)<br>\n939         new_states.append(new_state)<br>\n940</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in <strong>call</strong>(self, inputs, state, scope)<br>\n178       with vs.variable_scope(vs.get_variable_scope(),<br>\n179                              custom_getter=self._rnn_get_variable):<br>\n--&gt; 180         return super(RNNCell, self).<strong>call</strong>(inputs, state)<br>\n181<br>\n182   def _rnn_get_variable(self, getter, *args, **kwargs):</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in <strong>call</strong>(self, inputs, *args, **kwargs)<br>\n448         # Check input assumptions set after layer building, e.g. input shape.<br>\n449         self._assert_input_compatibility(inputs)<br>\n--&gt; 450         outputs = self.call(inputs, *args, **kwargs)<br>\n451<br>\n452         # Apply activity regularization.</p>\n<p>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)<br>\n554     input_size = inputs.get_shape().with_rank(2)[1]<br>\n555     if input_size.value is None:<br>\n--&gt; 556       raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")<br>\n557     scope = vs.get_variable_scope()<br>\n558     with vs.variable_scope(scope, initializer=self._initializer) as unit_scope:</p>\n<p>ValueError: Could not infer input size from inputs.get_shape()[-1]`</p>\n<p>Could anyone help me with:<br>\nIs this a correct way to condition the last hidden state of encoder on the inputs of decoder?<br>\nand why the inputs of decoder become None after I feed the auxiliary_inputs as the error?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a></p>", "body_text": "Hi, I'm doing similar decoder, but the decoder inputs is conditioned by the encoder final hidden states (context vector), Related Paper see picture a in page 3. The decoder is trying to fully inference during training with feeding previous outputs and context vector as inputs at each step.\n`class RNNEncoder_Decoder(object):\ndef __init__(self,input_dim,\n             context_dim,output_dim,hidden_dim,\n             layers_stacked_count,learning_rate):\n    \n    self.graph = tf.get_default_graph()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.context_dim = context_dim\n    self.hidden_dim = hidden_dim\n    self.layers_stacked_count = layers_stacked_count\n    self.learning_rate = learning_rate\n    self.sampling_probability = tf.constant(dtype=tf.float32,value=1.0)\n    \n    # [batch_size,sequence_length,input_dimension]\n    self.enc_inp = tf.placeholder(tf.float32, [None,None,self.input_dim], name='encoder_inputs')\n    self.expected_out = tf.placeholder(tf.float32, [None,None,self.output_dim], name='expected_outs')\n    # fullly inference during trianing\n    self.dec_inp = tf.zeros_like(self.expected_out,dtype=tf.float32,name='decoder_inputs')\n            \n    seq_length = tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(self.enc_inp), 2)), 1)\n    self.seq_length = tf.cast(seq_length, tf.int32)\n    \n    with tf.variable_scope('RNNEncoderDecoder'):\n        with tf.variable_scope(\"Enocder\") as encoder_varscope:\n            # create encoder LSTM cell\n            encoder_cells = []\n            for i in range(self.layers_stacked_count):\n                with tf.variable_scope('EncoderCell_{}'.format(i)):\n                    encoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\n                                                         use_peepholes=True))\n            self.encoder_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cells)\n\n            # ruuning dynamic rnn encoder                \n            _, enc_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\n                                             initial_state=None,\n                                             dtype=tf.float32,\n                                             inputs = self.enc_inp,\n                                             sequence_length = self.seq_length\n                                            )\n\n            # extract top layer hidden state as feature representation\n            self.context_vector = enc_state[-1].h\n            \n            cell_state0 = tf.zeros_like(enc_state[0].c,dtype=tf.float32)\n            hidden_state0 = tf.zeros_like(enc_state[0].h,dtype=tf.float32)\n\n            dec_init_state = (enc_state[1], # pass the top layer state of enocder to the bottom layer of decoder\n                              tf.nn.rnn_cell.LSTMStateTuple(cell_state0, hidden_state0))\n            \n            # condition extracted features on decoder inputs\n            # with a shape that matches decoder inputs in all but (potentially) the final dimension. \n            # tile context vector from [batch_size,context_dim] to [batch_size,decoder_sequence_length,context_dim]\n            context_vector_shape = tf.shape(self.context_vector)\n            context_vector_reshaped = tf.reshape(self.context_vector, \n                                                 [context_vector_shape[0], 1, context_vector_shape[1]]\n                                                )\n            enc_inp_shape = tf.shape(self.enc_inp)\n            self.auxiliary_inputs = tf.tile(context_vector_reshaped,\n                                       multiples=[1,enc_inp_shape[1],1]\n                                      )\n            \n        with tf.variable_scope(\"Deocder\") as decoder_varscope:\n            # create decoder LSTM cell\n            decoder_cells = []\n            for i in range(self.layers_stacked_count):\n                with tf.variable_scope('DecoderCell_{}'.format(i)):\n                    decoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\n                                                         use_peepholes=True))\n            self.decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_cells)\n\n            dec_out_dense = Dense(units = self.output_dim,\n                                  activation = None,\n                                  use_bias = False,\n                                  kernel_initializer = tf.truncated_normal_initializer(\n                                      dtype=tf.float32,\n                                      stddev = 1.0 / math.sqrt(float(self.hidden_dim))\n                                  ),\n                                  name = 'dec_outp_linear_projection'\n                                 )\n            \n            training_helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(\n                inputs = self.dec_inp,\n                sequence_length = self.seq_length,\n                auxiliary_inputs = self.auxiliary_inputs, # condtional on inputs\n                sampling_probability = 1.0, # for fullly inference\n                name = 'feeding_conditional_input'\n            )\n            \n            decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell = self.decoder_cell,\n                helper = training_helper,\n                initial_state = dec_init_state,\n                output_layer = dec_out_dense\n            )\n            \n            outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,\n                                                                               impute_finished = True\n                                                                              )\n        self.outputs = outputs\n        \n### optimize loss part\n\ndef get_decoder_prediction(self,X,session):\n    feed_dict = {\n        self.enc_inp:X\n    }\n    feed_dict.update({self.expected_out:X})\n    run = [self.outputs]\n    return session.run(run,feed_dict=feed_dict)\n\nRNN_test = RNNEncoder_Decoder(input_dim=1,context_dim=32,output_dim=1,hidden_dim=32,layers_stacked_count=2,learning_rate=0.01)`\nWithout \"auxiliary_inputs = self.auxiliary_inputs\", it running successfully,\nbut with auxiliary_inputs = self.auxiliary_inputs\nAnd I got following:\n`---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n in ()\n9                           hidden_dim=hidden_dim,\n10                           layers_stacked_count=layers_stacked_count,\n---> 11                           learning_rate=learning_rate\n12                          )\n in init(self, input_dim, context_dim, output_dim, hidden_dim, layers_stacked_count, learning_rate)\n98\n99                 outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,\n--> 100                                                                                    impute_finished = True\n101                                                                                   )\n102             self.outputs = outputs\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\n284         ],\n285         parallel_iterations=parallel_iterations,\n--> 286         swap_memory=swap_memory)\n287\n288     final_outputs_ta = res[1]\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\n2773     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\n2774     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\n-> 2775     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n2776     return result\n2777\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n2602       self.Enter()\n2603       original_body_result, exit_vars = self._BuildLoop(\n-> 2604           pred, body, original_loop_vars, loop_vars, shape_invariants)\n2605     finally:\n2606       self.Exit()\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n2552         structure=original_loop_vars,\n2553         flat_sequence=vars_for_body_with_tensor_arrays)\n-> 2554     body_result = body(*packed_vars_for_body)\n2555     if not nest.is_sequence(body_result):\n2556       body_result = [body_result]\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\n232       \"\"\"\n233       (next_outputs, decoder_state, next_inputs,\n--> 234        decoder_finished) = decoder.step(time, inputs, state)\n235       next_finished = math_ops.logical_or(decoder_finished, finished)\n236       if maximum_iterations is not None:\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)\n137     \"\"\"\n138     with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\n--> 139       cell_outputs, cell_state = self._cell(inputs, state)\n140       if self._output_layer is not None:\n141         cell_outputs = self._output_layer(cell_outputs)\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state, scope)\n178       with vs.variable_scope(vs.get_variable_scope(),\n179                              custom_getter=self._rnn_get_variable):\n--> 180         return super(RNNCell, self).call(inputs, state)\n181\n182   def _rnn_get_variable(self, getter, *args, **kwargs):\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in call(self, inputs, *args, **kwargs)\n448         # Check input assumptions set after layer building, e.g. input shape.\n449         self._assert_input_compatibility(inputs)\n--> 450         outputs = self.call(inputs, *args, **kwargs)\n451\n452         # Apply activity regularization.\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\n936                                       [-1, cell.state_size])\n937           cur_state_pos += cell.state_size\n--> 938         cur_inp, new_state = cell(cur_inp, cur_state)\n939         new_states.append(new_state)\n940\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state, scope)\n178       with vs.variable_scope(vs.get_variable_scope(),\n179                              custom_getter=self._rnn_get_variable):\n--> 180         return super(RNNCell, self).call(inputs, state)\n181\n182   def _rnn_get_variable(self, getter, *args, **kwargs):\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in call(self, inputs, *args, **kwargs)\n448         # Check input assumptions set after layer building, e.g. input shape.\n449         self._assert_input_compatibility(inputs)\n--> 450         outputs = self.call(inputs, *args, **kwargs)\n451\n452         # Apply activity regularization.\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\n554     input_size = inputs.get_shape().with_rank(2)[1]\n555     if input_size.value is None:\n--> 556       raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\n557     scope = vs.get_variable_scope()\n558     with vs.variable_scope(scope, initializer=self._initializer) as unit_scope:\nValueError: Could not infer input size from inputs.get_shape()[-1]`\nCould anyone help me with:\nIs this a correct way to condition the last hidden state of encoder on the inputs of decoder?\nand why the inputs of decoder become None after I feed the auxiliary_inputs as the error?\n@ebrevdo", "body": "Hi, I'm doing similar decoder, but the decoder inputs is conditioned by the encoder final hidden states (context vector), [Related Paper](https://arxiv.org/pdf/1702.05538.pdf) see picture a in page 3. The decoder is trying to fully inference during training with feeding previous outputs and context vector as inputs at each step.\r\n\r\n`class RNNEncoder_Decoder(object):\r\n\r\n    def __init__(self,input_dim,\r\n                 context_dim,output_dim,hidden_dim,\r\n                 layers_stacked_count,learning_rate):\r\n        \r\n        self.graph = tf.get_default_graph()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.context_dim = context_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.layers_stacked_count = layers_stacked_count\r\n        self.learning_rate = learning_rate\r\n        self.sampling_probability = tf.constant(dtype=tf.float32,value=1.0)\r\n        \r\n        # [batch_size,sequence_length,input_dimension]\r\n        self.enc_inp = tf.placeholder(tf.float32, [None,None,self.input_dim], name='encoder_inputs')\r\n        self.expected_out = tf.placeholder(tf.float32, [None,None,self.output_dim], name='expected_outs')\r\n        # fullly inference during trianing\r\n        self.dec_inp = tf.zeros_like(self.expected_out,dtype=tf.float32,name='decoder_inputs')\r\n                \r\n        seq_length = tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(self.enc_inp), 2)), 1)\r\n        self.seq_length = tf.cast(seq_length, tf.int32)\r\n        \r\n        with tf.variable_scope('RNNEncoderDecoder'):\r\n            with tf.variable_scope(\"Enocder\") as encoder_varscope:\r\n                # create encoder LSTM cell\r\n                encoder_cells = []\r\n                for i in range(self.layers_stacked_count):\r\n                    with tf.variable_scope('EncoderCell_{}'.format(i)):\r\n                        encoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\r\n                                                             use_peepholes=True))\r\n                self.encoder_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cells)\r\n\r\n                # ruuning dynamic rnn encoder                \r\n                _, enc_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\r\n                                                 initial_state=None,\r\n                                                 dtype=tf.float32,\r\n                                                 inputs = self.enc_inp,\r\n                                                 sequence_length = self.seq_length\r\n                                                )\r\n \r\n                # extract top layer hidden state as feature representation\r\n                self.context_vector = enc_state[-1].h\r\n                \r\n                cell_state0 = tf.zeros_like(enc_state[0].c,dtype=tf.float32)\r\n                hidden_state0 = tf.zeros_like(enc_state[0].h,dtype=tf.float32)\r\n\r\n                dec_init_state = (enc_state[1], # pass the top layer state of enocder to the bottom layer of decoder\r\n                                  tf.nn.rnn_cell.LSTMStateTuple(cell_state0, hidden_state0))\r\n                \r\n                # condition extracted features on decoder inputs\r\n                # with a shape that matches decoder inputs in all but (potentially) the final dimension. \r\n                # tile context vector from [batch_size,context_dim] to [batch_size,decoder_sequence_length,context_dim]\r\n                context_vector_shape = tf.shape(self.context_vector)\r\n                context_vector_reshaped = tf.reshape(self.context_vector, \r\n                                                     [context_vector_shape[0], 1, context_vector_shape[1]]\r\n                                                    )\r\n                enc_inp_shape = tf.shape(self.enc_inp)\r\n                self.auxiliary_inputs = tf.tile(context_vector_reshaped,\r\n                                           multiples=[1,enc_inp_shape[1],1]\r\n                                          )\r\n                \r\n            with tf.variable_scope(\"Deocder\") as decoder_varscope:\r\n                # create decoder LSTM cell\r\n                decoder_cells = []\r\n                for i in range(self.layers_stacked_count):\r\n                    with tf.variable_scope('DecoderCell_{}'.format(i)):\r\n                        decoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\r\n                                                             use_peepholes=True))\r\n                self.decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_cells)\r\n\r\n                dec_out_dense = Dense(units = self.output_dim,\r\n                                      activation = None,\r\n                                      use_bias = False,\r\n                                      kernel_initializer = tf.truncated_normal_initializer(\r\n                                          dtype=tf.float32,\r\n                                          stddev = 1.0 / math.sqrt(float(self.hidden_dim))\r\n                                      ),\r\n                                      name = 'dec_outp_linear_projection'\r\n                                     )\r\n                \r\n                training_helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(\r\n                    inputs = self.dec_inp,\r\n                    sequence_length = self.seq_length,\r\n                    auxiliary_inputs = self.auxiliary_inputs, # condtional on inputs\r\n                    sampling_probability = 1.0, # for fullly inference\r\n                    name = 'feeding_conditional_input'\r\n                )\r\n                \r\n                decoder = tf.contrib.seq2seq.BasicDecoder(\r\n                    cell = self.decoder_cell,\r\n                    helper = training_helper,\r\n                    initial_state = dec_init_state,\r\n                    output_layer = dec_out_dense\r\n                )\r\n                \r\n                outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,\r\n                                                                                   impute_finished = True\r\n                                                                                  )\r\n            self.outputs = outputs\r\n            \r\n    ### optimize loss part\r\n    \r\n    def get_decoder_prediction(self,X,session):\r\n        feed_dict = {\r\n            self.enc_inp:X\r\n        }\r\n        feed_dict.update({self.expected_out:X})\r\n        run = [self.outputs]\r\n        return session.run(run,feed_dict=feed_dict)\r\nRNN_test = RNNEncoder_Decoder(input_dim=1,context_dim=32,output_dim=1,hidden_dim=32,layers_stacked_count=2,learning_rate=0.01)`\r\n\r\nWithout \"auxiliary_inputs = self.auxiliary_inputs\", it running successfully,\r\nbut with auxiliary_inputs = self.auxiliary_inputs\r\nAnd I got following:\r\n\r\n`---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-02522a01f0d8> in <module>()\r\n      9                           hidden_dim=hidden_dim,\r\n     10                           layers_stacked_count=layers_stacked_count,\r\n---> 11                           learning_rate=learning_rate\r\n     12                          )\r\n\r\n<ipython-input-2-86494b8d99fa> in __init__(self, input_dim, context_dim, output_dim, hidden_dim, layers_stacked_count, learning_rate)\r\n     98 \r\n     99                 outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,\r\n--> 100                                                                                    impute_finished = True\r\n    101                                                                                   )\r\n    102             self.outputs = outputs\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\r\n    284         ],\r\n    285         parallel_iterations=parallel_iterations,\r\n--> 286         swap_memory=swap_memory)\r\n    287 \r\n    288     final_outputs_ta = res[1]\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2773     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\r\n   2774     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\r\n-> 2775     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2776     return result\r\n   2777 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2602       self.Enter()\r\n   2603       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2604           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2605     finally:\r\n   2606       self.Exit()\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2552         structure=original_loop_vars,\r\n   2553         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2554     body_result = body(*packed_vars_for_body)\r\n   2555     if not nest.is_sequence(body_result):\r\n   2556       body_result = [body_result]\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\r\n    232       \"\"\"\r\n    233       (next_outputs, decoder_state, next_inputs,\r\n--> 234        decoder_finished) = decoder.step(time, inputs, state)\r\n    235       next_finished = math_ops.logical_or(decoder_finished, finished)\r\n    236       if maximum_iterations is not None:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)\r\n    137     \"\"\"\r\n    138     with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\r\n--> 139       cell_outputs, cell_state = self._cell(inputs, state)\r\n    140       if self._output_layer is not None:\r\n    141         cell_outputs = self._output_layer(cell_outputs)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    448         # Check input assumptions set after layer building, e.g. input shape.\r\n    449         self._assert_input_compatibility(inputs)\r\n--> 450         outputs = self.call(inputs, *args, **kwargs)\r\n    451 \r\n    452         # Apply activity regularization.\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\r\n    936                                       [-1, cell.state_size])\r\n    937           cur_state_pos += cell.state_size\r\n--> 938         cur_inp, new_state = cell(cur_inp, cur_state)\r\n    939         new_states.append(new_state)\r\n    940 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    448         # Check input assumptions set after layer building, e.g. input shape.\r\n    449         self._assert_input_compatibility(inputs)\r\n--> 450         outputs = self.call(inputs, *args, **kwargs)\r\n    451 \r\n    452         # Apply activity regularization.\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\r\n    554     input_size = inputs.get_shape().with_rank(2)[1]\r\n    555     if input_size.value is None:\r\n--> 556       raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\r\n    557     scope = vs.get_variable_scope()\r\n    558     with vs.variable_scope(scope, initializer=self._initializer) as unit_scope:\r\n\r\nValueError: Could not infer input size from inputs.get_shape()[-1]`\r\n\r\nCould anyone help me with:\r\nIs this a correct way to condition the last hidden state of encoder on the inputs of decoder?\r\nand why the inputs of decoder become None after I feed the auxiliary_inputs as the error?\r\n@ebrevdo "}