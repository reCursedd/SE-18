{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/326241575", "html_url": "https://github.com/tensorflow/tensorflow/issues/10815#issuecomment-326241575", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10815", "id": 326241575, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjI0MTU3NQ==", "user": {"login": "chococig", "id": 31504573, "node_id": "MDQ6VXNlcjMxNTA0NTcz", "avatar_url": "https://avatars1.githubusercontent.com/u/31504573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chococig", "html_url": "https://github.com/chococig", "followers_url": "https://api.github.com/users/chococig/followers", "following_url": "https://api.github.com/users/chococig/following{/other_user}", "gists_url": "https://api.github.com/users/chococig/gists{/gist_id}", "starred_url": "https://api.github.com/users/chococig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chococig/subscriptions", "organizations_url": "https://api.github.com/users/chococig/orgs", "repos_url": "https://api.github.com/users/chococig/repos", "events_url": "https://api.github.com/users/chococig/events{/privacy}", "received_events_url": "https://api.github.com/users/chococig/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T09:26:51Z", "updated_at": "2017-09-01T02:22:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a><br>\nhello, I also have a question about ScheduledOutputTrainingHelper. I'm currently doing regression using ScheduledOutputTrainingHelper. But, in the inference stage, is it right to set the sampling probability to 1 and use an arbitrary input to target? For the classification case, GreedyEmbeddingHelper can be used, but for regression there is no Greedy'Output'Helper.</p>\n<p>sampling_probability: the probability of sampling from the outputs instead of reading directly from the inputs.</p>\n<p>my decoder_input is like Start_value(zeros) + y_target[:-1]</p>\n<pre><code>x_1 = tf.placeholder(dtype=tf.float32, shape=[None, n_step, n_1])\nx_2 = tf.placeholder(dtype=tf.float32, shape=[None, n_step, n_2])\ny_target = tf.placeholder(dtype=tf.float32, shape=[None, n_step, 1])\n\nphase = tf.placeholder(tf.bool) # if True: training / if False: inference\nlength = tf.placeholder(dtype=tf.int32, shape=[None,])\ntf_batch_size = tf.placeholder(dtype=tf.int32, shape = [])\n\nencoder_input = tf.concat([x_1,x_2], axis=2)\n\n# like Start_token + target[:-1]\ndecoder_input = tf.concat([tf.fill([tf_batch_size,1,1], 0.0), # Start by Zeros\n                           y_target[:,:-1,:]], #targets[:-1]\n                           axis=1) \n</code></pre>\n<p>then my decoder is like this</p>\n<pre><code>def decoder(decoder_input, encoder_output, decoder_initial_state, decoder_dimension, phase):    \n    cell = LayerNormBasicLSTMCell(decoder_dimension)\n    \n    cell = AttentionWrapper(cell, \n                            attention_mechanism=BahdanauAttention(decoder_dimension, encoder_output), \n                            initial_cell_state = (LSTMStateTuple(decoder_initial_state, decoder_initial_state)),\n                            alignment_history = True) \n\n    output_cell = OutputProjectionWrapper(cell, output_size=1)\n    \n    sampling_prob = tf.cond(phase,\n                            \n                            #training\n                            lambda :tf.constant(1.0) - tf.train.inverse_time_decay(learning_rate=1.0,\n                                                                                   global_step=global_step,\n                                                                                   decay_steps=1000,\n                                                                                   decay_rate=0.9),\n                            \n                            #inference\n                            lambda : tf.constant(1.0))\n\n    helper = ScheduledOutputTrainingHelper(decoder_input, \n                                           sequence_length=length, \n                                           sampling_probability=sampling_prob)    \n    \n    my_decoder = BasicDecoder(cell=output_cell, \n                              helper=helper, \n                              initial_state=output_cell.zero_state(batch_size=tf_batch_size, dtype=tf.float32))\n    \n    final_outputs, final_state, final_sequence_length = dynamic_decode(my_decoder, maximum_iterations=24)\n        \n    return final_outputs.rnn_output\n</code></pre>\n<p>In the inference stage, if i feed zero values(or any arbitrary value) to y_target and set sampling_probabilty to 1.0, i think result has to be same as the case when I feed ground truth values to y_target, because sampling_prob=1 means model always sample from the previous outputs, not ground truth value. So what I'm saying is that if the sampling_probability is set to 1.0, it should not matter what value I feed to y_target in the inference stage</p>\n<p>However, result is slightly different like below(but almost same) / phase: False means inference<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/31504573/29915809-eab1f3ee-8e77-11e7-8a94-006ec10222ae.png\"><img src=\"https://user-images.githubusercontent.com/31504573/29915809-eab1f3ee-8e77-11e7-8a94-006ec10222ae.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>is this a precision(float32) issue of python? or problem of ScheduledOutputTrainingHelper?</p>", "body_text": "@ebrevdo\nhello, I also have a question about ScheduledOutputTrainingHelper. I'm currently doing regression using ScheduledOutputTrainingHelper. But, in the inference stage, is it right to set the sampling probability to 1 and use an arbitrary input to target? For the classification case, GreedyEmbeddingHelper can be used, but for regression there is no Greedy'Output'Helper.\nsampling_probability: the probability of sampling from the outputs instead of reading directly from the inputs.\nmy decoder_input is like Start_value(zeros) + y_target[:-1]\nx_1 = tf.placeholder(dtype=tf.float32, shape=[None, n_step, n_1])\nx_2 = tf.placeholder(dtype=tf.float32, shape=[None, n_step, n_2])\ny_target = tf.placeholder(dtype=tf.float32, shape=[None, n_step, 1])\n\nphase = tf.placeholder(tf.bool) # if True: training / if False: inference\nlength = tf.placeholder(dtype=tf.int32, shape=[None,])\ntf_batch_size = tf.placeholder(dtype=tf.int32, shape = [])\n\nencoder_input = tf.concat([x_1,x_2], axis=2)\n\n# like Start_token + target[:-1]\ndecoder_input = tf.concat([tf.fill([tf_batch_size,1,1], 0.0), # Start by Zeros\n                           y_target[:,:-1,:]], #targets[:-1]\n                           axis=1) \n\nthen my decoder is like this\ndef decoder(decoder_input, encoder_output, decoder_initial_state, decoder_dimension, phase):    \n    cell = LayerNormBasicLSTMCell(decoder_dimension)\n    \n    cell = AttentionWrapper(cell, \n                            attention_mechanism=BahdanauAttention(decoder_dimension, encoder_output), \n                            initial_cell_state = (LSTMStateTuple(decoder_initial_state, decoder_initial_state)),\n                            alignment_history = True) \n\n    output_cell = OutputProjectionWrapper(cell, output_size=1)\n    \n    sampling_prob = tf.cond(phase,\n                            \n                            #training\n                            lambda :tf.constant(1.0) - tf.train.inverse_time_decay(learning_rate=1.0,\n                                                                                   global_step=global_step,\n                                                                                   decay_steps=1000,\n                                                                                   decay_rate=0.9),\n                            \n                            #inference\n                            lambda : tf.constant(1.0))\n\n    helper = ScheduledOutputTrainingHelper(decoder_input, \n                                           sequence_length=length, \n                                           sampling_probability=sampling_prob)    \n    \n    my_decoder = BasicDecoder(cell=output_cell, \n                              helper=helper, \n                              initial_state=output_cell.zero_state(batch_size=tf_batch_size, dtype=tf.float32))\n    \n    final_outputs, final_state, final_sequence_length = dynamic_decode(my_decoder, maximum_iterations=24)\n        \n    return final_outputs.rnn_output\n\nIn the inference stage, if i feed zero values(or any arbitrary value) to y_target and set sampling_probabilty to 1.0, i think result has to be same as the case when I feed ground truth values to y_target, because sampling_prob=1 means model always sample from the previous outputs, not ground truth value. So what I'm saying is that if the sampling_probability is set to 1.0, it should not matter what value I feed to y_target in the inference stage\nHowever, result is slightly different like below(but almost same) / phase: False means inference\n\nis this a precision(float32) issue of python? or problem of ScheduledOutputTrainingHelper?", "body": "@ebrevdo \r\nhello, I also have a question about ScheduledOutputTrainingHelper. I'm currently doing regression using ScheduledOutputTrainingHelper. But, in the inference stage, is it right to set the sampling probability to 1 and use an arbitrary input to target? For the classification case, GreedyEmbeddingHelper can be used, but for regression there is no Greedy'Output'Helper.\r\n\r\nsampling_probability: the probability of sampling from the outputs instead of reading directly from the inputs.\r\n\r\nmy decoder_input is like Start_value(zeros) + y_target[:-1]\r\n\r\n```\r\nx_1 = tf.placeholder(dtype=tf.float32, shape=[None, n_step, n_1])\r\nx_2 = tf.placeholder(dtype=tf.float32, shape=[None, n_step, n_2])\r\ny_target = tf.placeholder(dtype=tf.float32, shape=[None, n_step, 1])\r\n\r\nphase = tf.placeholder(tf.bool) # if True: training / if False: inference\r\nlength = tf.placeholder(dtype=tf.int32, shape=[None,])\r\ntf_batch_size = tf.placeholder(dtype=tf.int32, shape = [])\r\n\r\nencoder_input = tf.concat([x_1,x_2], axis=2)\r\n\r\n# like Start_token + target[:-1]\r\ndecoder_input = tf.concat([tf.fill([tf_batch_size,1,1], 0.0), # Start by Zeros\r\n                           y_target[:,:-1,:]], #targets[:-1]\r\n                           axis=1) \r\n```\r\n\r\nthen my decoder is like this\r\n\r\n```\r\ndef decoder(decoder_input, encoder_output, decoder_initial_state, decoder_dimension, phase):    \r\n    cell = LayerNormBasicLSTMCell(decoder_dimension)\r\n    \r\n    cell = AttentionWrapper(cell, \r\n                            attention_mechanism=BahdanauAttention(decoder_dimension, encoder_output), \r\n                            initial_cell_state = (LSTMStateTuple(decoder_initial_state, decoder_initial_state)),\r\n                            alignment_history = True) \r\n\r\n    output_cell = OutputProjectionWrapper(cell, output_size=1)\r\n    \r\n    sampling_prob = tf.cond(phase,\r\n                            \r\n                            #training\r\n                            lambda :tf.constant(1.0) - tf.train.inverse_time_decay(learning_rate=1.0,\r\n                                                                                   global_step=global_step,\r\n                                                                                   decay_steps=1000,\r\n                                                                                   decay_rate=0.9),\r\n                            \r\n                            #inference\r\n                            lambda : tf.constant(1.0))\r\n\r\n    helper = ScheduledOutputTrainingHelper(decoder_input, \r\n                                           sequence_length=length, \r\n                                           sampling_probability=sampling_prob)    \r\n    \r\n    my_decoder = BasicDecoder(cell=output_cell, \r\n                              helper=helper, \r\n                              initial_state=output_cell.zero_state(batch_size=tf_batch_size, dtype=tf.float32))\r\n    \r\n    final_outputs, final_state, final_sequence_length = dynamic_decode(my_decoder, maximum_iterations=24)\r\n        \r\n    return final_outputs.rnn_output\r\n```\r\n\r\nIn the inference stage, if i feed zero values(or any arbitrary value) to y_target and set sampling_probabilty to 1.0, i think result has to be same as the case when I feed ground truth values to y_target, because sampling_prob=1 means model always sample from the previous outputs, not ground truth value. So what I'm saying is that if the sampling_probability is set to 1.0, it should not matter what value I feed to y_target in the inference stage\r\n\r\nHowever, result is slightly different like below(but almost same) / phase: False means inference\r\n![image](https://user-images.githubusercontent.com/31504573/29915809-eab1f3ee-8e77-11e7-8a94-006ec10222ae.png)\r\n\r\nis this a precision(float32) issue of python? or problem of ScheduledOutputTrainingHelper?"}