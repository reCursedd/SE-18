{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309932137", "html_url": "https://github.com/tensorflow/tensorflow/issues/10815#issuecomment-309932137", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10815", "id": 309932137, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTkzMjEzNw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-21T00:55:52Z", "updated_at": "2017-06-21T00:55:52Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Ricky, is your question about tf.contrib.seq2seq?  Looks like you've rolled\nyour own</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Jun 20, 2017 11:01 AM, \"Ricky Han\" ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; here is my toy example for decoder.\n It's still WIP:\n\n\n     enc_inp = tf.placeholder(tf.float32, shape=(batch_size, seq_length_in, input_dim))\n\n     dec_target = tf.placeholder(tf.float32,\n         shape=(batch_size, seq_length_out, output_dim))\n\n     with tf.variable_scope(\"ENCODE\"):\n         # enc_cells = []\n         # for i in range(0, encoder_depth):\n         #     with tf.variable_scope('enc_RNN_{}'.format(i)):\n         #         cell = tf.contrib.rnn.GRUCell(hidden_dim)  # Or LSTMCell(hidden_dim)\n         #         cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\n         #         enc_cells.append(cell)\n         # enc_cell = tf.contrib.rnn.MultiRNNCell(enc_cells)\n         enc_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n\n         enc_inp_len = np.array([seq_length_in for _ in range(batch_size)])\n\n         ((encoder_fw_outputs,\n           encoder_bw_outputs),\n          (encoder_fw_final_state,\n           encoder_bw_final_state)) = (\n             tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\n                                             cell_bw=enc_cell,\n                                             inputs=enc_inp,\n                                             sequence_length=enc_inp_len,\n                                             dtype=tf.float32)\n             )\n         encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n\n         encoder_final_state_c = tf.concat(\n             (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n\n         encoder_final_state_h = tf.concat(\n             (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n\n         encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n             c=encoder_final_state_c,\n             h=encoder_final_state_h\n         )\n\n     W = tf.Variable(tf.random_uniform([hidden_dim, output_dim], -1, 1), dtype=tf.float32)\n     b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32)\n     decoder_lengths = seq_length_out + 10\n     dec_inp_initial = tf.zeros([batch_size, hidden_dim], dtype=np.float32, name=\"GO\") # BUG:\n\n\n     def loop_fn_initial():\n         initial_elements_finished = (0 &gt;= decoder_lengths)  # all False at the initial step\n         initial_input = dec_inp_initial\n         initial_cell_state = encoder_final_state\n         initial_cell_output = None\n         initial_loop_state = None  # we don't need to pass any additional information\n         return (initial_elements_finished,\n                 initial_input,\n                 initial_cell_state,\n                 initial_cell_output,\n                 initial_loop_state)\n\n     def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n         def get_next_input():\n             output_logits = tf.add(tf.matmul(previous_output, W), b) # time x output_dim\n             prediction = tf.argmax(output_logits, axis=1)\n             next_input = tf.nn.embedding_lookup(embeddings, prediction)\n             return next_input\n         elements_finished = (time &gt;= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n                                                       # defining if corresponding sequence has ended\n         finished = tf.reduce_all(elements_finished) # -&gt; boolean scalar\n         input = tf.cond(finished, lambda: dec_inp_initial, lambda: dec_inp_initial)\n         state = previous_state\n         output = previous_output\n         loop_state = None\n         return (elements_finished,\n                 input,\n                 state,\n                 output,\n                 loop_state)\n     def loop_fn(time, previous_output, previous_state, previous_loop_state):\n         if previous_state is None:    # time == 0\n             assert previous_output is None and previous_state is None\n             return loop_fn_initial()\n         else:\n             return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n\n\n     with tf.variable_scope(\"DECODE\"):\n         # dec_cells = []\n         # # name=\"dec_cell_{}\".format(i)\n         # for i in range(0, decoder_depth):\n         #     with tf.variable_scope('dec_RNN_{}'.format(i)):\n         #         cell = tf.contrib.rnn.GRUCell(hidden_dim)\n         #         cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\n         #         dec_cells.append(cell)\n         # dec_cell = tf.contrib.rnn.MultiRNNCell(dec_cells)\n         decoder_cell = tf.contrib.rnn.LSTMCell(hidden_dim*2) # BUG:\n\n         decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n         decoder_outputs = decoder_outputs_ta.stack()\n\n\n\n\n     with tf.Session() as sess:\n         init = tf.global_variables_initializer()\n         sess.run(init)\n\n         print sess.run(decoder_outputs, feed_dict={\n             enc_inp: sample_x,\n             dec_target: sample_y\n         })\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"236800362\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/10815\" href=\"https://github.com/tensorflow/tensorflow/issues/10815#issuecomment-309839091\">#10815 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim9F4U0Yl1JYEoDlzR_N1hYC00yf_ks5sGAjigaJpZM4N911G\">https://github.com/notifications/unsubscribe-auth/ABtim9F4U0Yl1JYEoDlzR_N1hYC00yf_ks5sGAjigaJpZM4N911G</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Ricky, is your question about tf.contrib.seq2seq?  Looks like you've rolled\nyour own\n\u2026\nOn Jun 20, 2017 11:01 AM, \"Ricky Han\" ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> here is my toy example for decoder.\n It's still WIP:\n\n\n     enc_inp = tf.placeholder(tf.float32, shape=(batch_size, seq_length_in, input_dim))\n\n     dec_target = tf.placeholder(tf.float32,\n         shape=(batch_size, seq_length_out, output_dim))\n\n     with tf.variable_scope(\"ENCODE\"):\n         # enc_cells = []\n         # for i in range(0, encoder_depth):\n         #     with tf.variable_scope('enc_RNN_{}'.format(i)):\n         #         cell = tf.contrib.rnn.GRUCell(hidden_dim)  # Or LSTMCell(hidden_dim)\n         #         cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\n         #         enc_cells.append(cell)\n         # enc_cell = tf.contrib.rnn.MultiRNNCell(enc_cells)\n         enc_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n\n         enc_inp_len = np.array([seq_length_in for _ in range(batch_size)])\n\n         ((encoder_fw_outputs,\n           encoder_bw_outputs),\n          (encoder_fw_final_state,\n           encoder_bw_final_state)) = (\n             tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\n                                             cell_bw=enc_cell,\n                                             inputs=enc_inp,\n                                             sequence_length=enc_inp_len,\n                                             dtype=tf.float32)\n             )\n         encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n\n         encoder_final_state_c = tf.concat(\n             (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n\n         encoder_final_state_h = tf.concat(\n             (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n\n         encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n             c=encoder_final_state_c,\n             h=encoder_final_state_h\n         )\n\n     W = tf.Variable(tf.random_uniform([hidden_dim, output_dim], -1, 1), dtype=tf.float32)\n     b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32)\n     decoder_lengths = seq_length_out + 10\n     dec_inp_initial = tf.zeros([batch_size, hidden_dim], dtype=np.float32, name=\"GO\") # BUG:\n\n\n     def loop_fn_initial():\n         initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n         initial_input = dec_inp_initial\n         initial_cell_state = encoder_final_state\n         initial_cell_output = None\n         initial_loop_state = None  # we don't need to pass any additional information\n         return (initial_elements_finished,\n                 initial_input,\n                 initial_cell_state,\n                 initial_cell_output,\n                 initial_loop_state)\n\n     def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n         def get_next_input():\n             output_logits = tf.add(tf.matmul(previous_output, W), b) # time x output_dim\n             prediction = tf.argmax(output_logits, axis=1)\n             next_input = tf.nn.embedding_lookup(embeddings, prediction)\n             return next_input\n         elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n                                                       # defining if corresponding sequence has ended\n         finished = tf.reduce_all(elements_finished) # -> boolean scalar\n         input = tf.cond(finished, lambda: dec_inp_initial, lambda: dec_inp_initial)\n         state = previous_state\n         output = previous_output\n         loop_state = None\n         return (elements_finished,\n                 input,\n                 state,\n                 output,\n                 loop_state)\n     def loop_fn(time, previous_output, previous_state, previous_loop_state):\n         if previous_state is None:    # time == 0\n             assert previous_output is None and previous_state is None\n             return loop_fn_initial()\n         else:\n             return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n\n\n     with tf.variable_scope(\"DECODE\"):\n         # dec_cells = []\n         # # name=\"dec_cell_{}\".format(i)\n         # for i in range(0, decoder_depth):\n         #     with tf.variable_scope('dec_RNN_{}'.format(i)):\n         #         cell = tf.contrib.rnn.GRUCell(hidden_dim)\n         #         cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\n         #         dec_cells.append(cell)\n         # dec_cell = tf.contrib.rnn.MultiRNNCell(dec_cells)\n         decoder_cell = tf.contrib.rnn.LSTMCell(hidden_dim*2) # BUG:\n\n         decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n         decoder_outputs = decoder_outputs_ta.stack()\n\n\n\n\n     with tf.Session() as sess:\n         init = tf.global_variables_initializer()\n         sess.run(init)\n\n         print sess.run(decoder_outputs, feed_dict={\n             enc_inp: sample_x,\n             dec_target: sample_y\n         })\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#10815 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim9F4U0Yl1JYEoDlzR_N1hYC00yf_ks5sGAjigaJpZM4N911G>\n .", "body": "Ricky, is your question about tf.contrib.seq2seq?  Looks like you've rolled\nyour own\n\nOn Jun 20, 2017 11:01 AM, \"Ricky Han\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> here is my toy example for decoder.\n> It's still WIP:\n>\n>\n>     enc_inp = tf.placeholder(tf.float32, shape=(batch_size, seq_length_in, input_dim))\n>\n>     dec_target = tf.placeholder(tf.float32,\n>         shape=(batch_size, seq_length_out, output_dim))\n>\n>     with tf.variable_scope(\"ENCODE\"):\n>         # enc_cells = []\n>         # for i in range(0, encoder_depth):\n>         #     with tf.variable_scope('enc_RNN_{}'.format(i)):\n>         #         cell = tf.contrib.rnn.GRUCell(hidden_dim)  # Or LSTMCell(hidden_dim)\n>         #         cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\n>         #         enc_cells.append(cell)\n>         # enc_cell = tf.contrib.rnn.MultiRNNCell(enc_cells)\n>         enc_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n>\n>         enc_inp_len = np.array([seq_length_in for _ in range(batch_size)])\n>\n>         ((encoder_fw_outputs,\n>           encoder_bw_outputs),\n>          (encoder_fw_final_state,\n>           encoder_bw_final_state)) = (\n>             tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\n>                                             cell_bw=enc_cell,\n>                                             inputs=enc_inp,\n>                                             sequence_length=enc_inp_len,\n>                                             dtype=tf.float32)\n>             )\n>         encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n>\n>         encoder_final_state_c = tf.concat(\n>             (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n>\n>         encoder_final_state_h = tf.concat(\n>             (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n>\n>         encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n>             c=encoder_final_state_c,\n>             h=encoder_final_state_h\n>         )\n>\n>     W = tf.Variable(tf.random_uniform([hidden_dim, output_dim], -1, 1), dtype=tf.float32)\n>     b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32)\n>     decoder_lengths = seq_length_out + 10\n>     dec_inp_initial = tf.zeros([batch_size, hidden_dim], dtype=np.float32, name=\"GO\") # BUG:\n>\n>\n>     def loop_fn_initial():\n>         initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n>         initial_input = dec_inp_initial\n>         initial_cell_state = encoder_final_state\n>         initial_cell_output = None\n>         initial_loop_state = None  # we don't need to pass any additional information\n>         return (initial_elements_finished,\n>                 initial_input,\n>                 initial_cell_state,\n>                 initial_cell_output,\n>                 initial_loop_state)\n>\n>     def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n>         def get_next_input():\n>             output_logits = tf.add(tf.matmul(previous_output, W), b) # time x output_dim\n>             prediction = tf.argmax(output_logits, axis=1)\n>             next_input = tf.nn.embedding_lookup(embeddings, prediction)\n>             return next_input\n>         elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n>                                                       # defining if corresponding sequence has ended\n>         finished = tf.reduce_all(elements_finished) # -> boolean scalar\n>         input = tf.cond(finished, lambda: dec_inp_initial, lambda: dec_inp_initial)\n>         state = previous_state\n>         output = previous_output\n>         loop_state = None\n>         return (elements_finished,\n>                 input,\n>                 state,\n>                 output,\n>                 loop_state)\n>     def loop_fn(time, previous_output, previous_state, previous_loop_state):\n>         if previous_state is None:    # time == 0\n>             assert previous_output is None and previous_state is None\n>             return loop_fn_initial()\n>         else:\n>             return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n>\n>\n>     with tf.variable_scope(\"DECODE\"):\n>         # dec_cells = []\n>         # # name=\"dec_cell_{}\".format(i)\n>         # for i in range(0, decoder_depth):\n>         #     with tf.variable_scope('dec_RNN_{}'.format(i)):\n>         #         cell = tf.contrib.rnn.GRUCell(hidden_dim)\n>         #         cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\n>         #         dec_cells.append(cell)\n>         # dec_cell = tf.contrib.rnn.MultiRNNCell(dec_cells)\n>         decoder_cell = tf.contrib.rnn.LSTMCell(hidden_dim*2) # BUG:\n>\n>         decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n>         decoder_outputs = decoder_outputs_ta.stack()\n>\n>\n>\n>\n>     with tf.Session() as sess:\n>         init = tf.global_variables_initializer()\n>         sess.run(init)\n>\n>         print sess.run(decoder_outputs, feed_dict={\n>             enc_inp: sample_x,\n>             dec_target: sample_y\n>         })\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10815#issuecomment-309839091>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9F4U0Yl1JYEoDlzR_N1hYC00yf_ks5sGAjigaJpZM4N911G>\n> .\n>\n"}