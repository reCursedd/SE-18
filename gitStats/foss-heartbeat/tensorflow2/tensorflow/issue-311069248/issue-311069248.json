{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18226", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18226/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18226/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18226/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18226", "id": 311069248, "node_id": "MDU6SXNzdWUzMTEwNjkyNDg=", "number": 18226, "title": "After applying dataset.batch(batchsize), could the batchsize show in items' dimension?", "user": {"login": "panfengli", "id": 18660165, "node_id": "MDQ6VXNlcjE4NjYwMTY1", "avatar_url": "https://avatars2.githubusercontent.com/u/18660165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panfengli", "html_url": "https://github.com/panfengli", "followers_url": "https://api.github.com/users/panfengli/followers", "following_url": "https://api.github.com/users/panfengli/following{/other_user}", "gists_url": "https://api.github.com/users/panfengli/gists{/gist_id}", "starred_url": "https://api.github.com/users/panfengli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panfengli/subscriptions", "organizations_url": "https://api.github.com/users/panfengli/orgs", "repos_url": "https://api.github.com/users/panfengli/repos", "events_url": "https://api.github.com/users/panfengli/events{/privacy}", "received_events_url": "https://api.github.com/users/panfengli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-04-04T03:04:27Z", "updated_at": "2018-04-06T00:42:44Z", "closed_at": "2018-04-06T00:11:20Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (<a href=\"https://github.com/umich-vl/px2graph/blob/master/util/loader.py\">https://github.com/umich-vl/px2graph/blob/master/util/loader.py</a>)</strong>:</li>\n<li><strong>OS Platform and Distribution (CentOS Linux release 7.4.1708 (Core))</strong>:</li>\n<li><strong>TensorFlow installed from (binary)</strong>:</li>\n<li><strong>TensorFlow version (1.4)</strong>:</li>\n<li><strong>Python version (3.6)</strong>:</li>\n<li><strong>Bazel version (N/A)</strong>:</li>\n<li><strong>CUDA/cuDNN version (Cuda 8, Cudnn v6)</strong>:</li>\n<li><strong>GPU model and memory (Titan X, 12 G)</strong>:</li>\n<li><strong>Exact command to reproduce: The original code is from <a href=\"https://github.com/umich-vl/px2graph\">https://github.com/umich-vl/px2graph</a>, the author use a queue-based input data method, and I want to change it to use tf.data API. Basically only the util/loader.py need to be changed, and the main parts have been shown as follows.</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I would like to change the original queue based data load mechanism to tf.data API.</p>\n<p>The original code is:</p>\n<pre><code>        # Index queue\n        self.input_idxs = tf.placeholder(tf.int64, shape=[None, 2])\n        idx_queue = tf.FIFOQueue(1e8, tf.int64)\n        self.enq_idxs = idx_queue.enqueue_many(self.input_idxs)\n        get_idx = idx_queue.dequeue()\n\n        # Image loading queue\n        img_queue = tf.FIFOQueue(opt.max_queue_size, task.proc_arg_dtype)\n        load_data = tf.py_func(task.load_sample_data, [get_idx], task.proc_arg_dtype)\n        enq_img = img_queue.enqueue(load_data)\n        init_sample = img_queue.dequeue()\n\n        # Preprocessing queue\n        # (for any preprocessing that can be done with TF operations)\n        data_queue = tf.FIFOQueue(opt.max_queue_size, task.data_arg_dtype,\n                                  shapes=task.data_shape)\n        enq_data = data_queue.enqueue(task.preprocess(init_sample, train_flag))\n        self.get_sample = data_queue.dequeue_many(opt.batchsize)\n</code></pre>\n<p>After the change, it is:</p>\n<pre><code>        # Dataset\n        self.input_idxs = tf.placeholder(tf.int64, shape=[None, 2])\n        dataset = tf.data.Dataset.from_tensor_slices(self.input_idxs)\n\n        def load_sample(idx):\n            sample = task.load_sample_data(idx)\n            sample = task.preprocess(sample, train_flag)\n            return sample\n\n        dataset = dataset.map(lambda idx: tf.py_func(load_sample, [idx], task.proc_arg_dtype), num_parallel_calls=self.num_threads)\n\n        def gen(dataset):\n            yield dataset.make_one_shot_iterator().get_next()\n\n        dataset = tf.data.Dataset.from_generator(gen, tuple(task.proc_arg_dtype), tuple(task.data_shape)) # set task.data_shape\n        dataset = dataset.batch(opt.batchsize)\n        self.iterator = dataset.make_initializable_iterator()\n        self.get_sample = self.iterator.get_next()\n</code></pre>\n<p>where <code>task.proc_arg_dtype</code> and <code>task.data_shape</code> are:</p>\n<pre><code>    proc_arg_dtype = [tf.float32, tf.float32, tf.int32, tf.int32, tf.int32, tf.float32, tf.int32, tf.int32, tf.int32]\n    data_shape = [\n        [opt.input_res, opt.input_res, 3],\n        [opt.output_res, opt.output_res, opt.det_inputs],\n        [2, opt.max_nodes, 2],\n        [4],\n        [opt.max_nodes, opt.obj_slots + opt.rel_slots],\n        [opt.max_nodes, opt.obj_slots, 5],\n        [opt.max_nodes, opt.rel_slots, 2],\n        [opt.max_nodes, 7],\n        [1]\n    ]\n</code></pre>\n<p>Since I find <code>tf.py_func</code> doesn't have <code>data_shape</code> argument so that I add additional <code>gen</code> function and use the <code>tf.data.Dataset.from_generator</code> to specify the <code>data_shape</code> of items in a dataset.</p>\n<p>My problem is the shape of item in <code>dataset.batch(opt.batchsize)</code> is</p>\n<pre><code>[&lt;tf.Tensor 'IteratorGetNext:0' shape=(?, 512, 512, 3) dtype=float32&gt;, &lt;tf.Tensor 'IteratorGetNext:1' shape=(?, 64, 64, 300) dtype=float32&gt;, &lt;tf.Tensor 'IteratorGetNext:2' shape=(?, 2, 200, 2) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:3' shape=(?, 4) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:4' shape=(?, 200, 9) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:5' shape=(?, 200, 3, 5) dtype=float32&gt;, &lt;tf.Tensor 'IteratorGetNext:6' shape=(?, 200, 6, 2) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:7' shape=(?, 200, 7) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:8' shape=(?, 1) dtype=int32&gt;]\n</code></pre>\n<p>But I need</p>\n<pre><code>[&lt;tf.Tensor 'IteratorGetNext:0' shape=(8, 512, 512, 3) dtype=float32&gt;, &lt;tf.Tensor 'IteratorGetNext:1' shape=(8, 64, 64, 300) dtype=float32&gt;, &lt;tf.Tensor 'IteratorGetNext:2' shape=(8, 2, 200, 2) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:3' shape=(8, 4) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:4' shape=(8, 200, 9) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:5' shape=(8, 200, 3, 5) dtype=float32&gt;, &lt;tf.Tensor 'IteratorGetNext:6' shape=(8, 200, 6, 2) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:7' shape=(8, 200, 7) dtype=int32&gt;, &lt;tf.Tensor 'IteratorGetNext:8' shape=(8, 1) dtype=int32&gt;]\n</code></pre>\n<p>where <code>8</code> is the batchsize.</p>\n<p>I tried the following code to solve it:</p>\n<pre><code>        def add_batch_shape(data):\n            for d in data:\n                shapes = [int(s) for s in d.shape[1:]]\n                shapes.insert(0, opt.batchsize)\n                d.set_shape(shapes)\n\n        def wrapped_add(data):\n            return tf.py_func(add_batch_shape, [data], task.proc_arg_dtype)\n\n        dataset = dataset.map(wrapped_add, num_parallel_calls=self.num_threads) # (?, 512, 512, 3) to (batchsize, 512, 512, 3)\n</code></pre>\n<p>But it will give out an error:</p>\n<p><code>TypeError: wrapped_add() takes 1 positional argument but 9 were given</code></p>\n<p>The <code>[data]</code> has a <code>data_shape</code> as above.</p>\n<p>I am wondering after applying <code>dataset.batch(batchsize)</code>, could the first dimension of items be the batchsize but not <code>None</code>. (and whether <code>tf.py_func</code> could add a new argument which specify the data's shape)</p>", "body_text": "System information\n\nHave I written custom code (https://github.com/umich-vl/px2graph/blob/master/util/loader.py):\nOS Platform and Distribution (CentOS Linux release 7.4.1708 (Core)):\nTensorFlow installed from (binary):\nTensorFlow version (1.4):\nPython version (3.6):\nBazel version (N/A):\nCUDA/cuDNN version (Cuda 8, Cudnn v6):\nGPU model and memory (Titan X, 12 G):\nExact command to reproduce: The original code is from https://github.com/umich-vl/px2graph, the author use a queue-based input data method, and I want to change it to use tf.data API. Basically only the util/loader.py need to be changed, and the main parts have been shown as follows.:\n\nDescribe the problem\nI would like to change the original queue based data load mechanism to tf.data API.\nThe original code is:\n        # Index queue\n        self.input_idxs = tf.placeholder(tf.int64, shape=[None, 2])\n        idx_queue = tf.FIFOQueue(1e8, tf.int64)\n        self.enq_idxs = idx_queue.enqueue_many(self.input_idxs)\n        get_idx = idx_queue.dequeue()\n\n        # Image loading queue\n        img_queue = tf.FIFOQueue(opt.max_queue_size, task.proc_arg_dtype)\n        load_data = tf.py_func(task.load_sample_data, [get_idx], task.proc_arg_dtype)\n        enq_img = img_queue.enqueue(load_data)\n        init_sample = img_queue.dequeue()\n\n        # Preprocessing queue\n        # (for any preprocessing that can be done with TF operations)\n        data_queue = tf.FIFOQueue(opt.max_queue_size, task.data_arg_dtype,\n                                  shapes=task.data_shape)\n        enq_data = data_queue.enqueue(task.preprocess(init_sample, train_flag))\n        self.get_sample = data_queue.dequeue_many(opt.batchsize)\n\nAfter the change, it is:\n        # Dataset\n        self.input_idxs = tf.placeholder(tf.int64, shape=[None, 2])\n        dataset = tf.data.Dataset.from_tensor_slices(self.input_idxs)\n\n        def load_sample(idx):\n            sample = task.load_sample_data(idx)\n            sample = task.preprocess(sample, train_flag)\n            return sample\n\n        dataset = dataset.map(lambda idx: tf.py_func(load_sample, [idx], task.proc_arg_dtype), num_parallel_calls=self.num_threads)\n\n        def gen(dataset):\n            yield dataset.make_one_shot_iterator().get_next()\n\n        dataset = tf.data.Dataset.from_generator(gen, tuple(task.proc_arg_dtype), tuple(task.data_shape)) # set task.data_shape\n        dataset = dataset.batch(opt.batchsize)\n        self.iterator = dataset.make_initializable_iterator()\n        self.get_sample = self.iterator.get_next()\n\nwhere task.proc_arg_dtype and task.data_shape are:\n    proc_arg_dtype = [tf.float32, tf.float32, tf.int32, tf.int32, tf.int32, tf.float32, tf.int32, tf.int32, tf.int32]\n    data_shape = [\n        [opt.input_res, opt.input_res, 3],\n        [opt.output_res, opt.output_res, opt.det_inputs],\n        [2, opt.max_nodes, 2],\n        [4],\n        [opt.max_nodes, opt.obj_slots + opt.rel_slots],\n        [opt.max_nodes, opt.obj_slots, 5],\n        [opt.max_nodes, opt.rel_slots, 2],\n        [opt.max_nodes, 7],\n        [1]\n    ]\n\nSince I find tf.py_func doesn't have data_shape argument so that I add additional gen function and use the tf.data.Dataset.from_generator to specify the data_shape of items in a dataset.\nMy problem is the shape of item in dataset.batch(opt.batchsize) is\n[<tf.Tensor 'IteratorGetNext:0' shape=(?, 512, 512, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(?, 64, 64, 300) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(?, 2, 200, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:3' shape=(?, 4) dtype=int32>, <tf.Tensor 'IteratorGetNext:4' shape=(?, 200, 9) dtype=int32>, <tf.Tensor 'IteratorGetNext:5' shape=(?, 200, 3, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(?, 200, 6, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:7' shape=(?, 200, 7) dtype=int32>, <tf.Tensor 'IteratorGetNext:8' shape=(?, 1) dtype=int32>]\n\nBut I need\n[<tf.Tensor 'IteratorGetNext:0' shape=(8, 512, 512, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(8, 64, 64, 300) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(8, 2, 200, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:3' shape=(8, 4) dtype=int32>, <tf.Tensor 'IteratorGetNext:4' shape=(8, 200, 9) dtype=int32>, <tf.Tensor 'IteratorGetNext:5' shape=(8, 200, 3, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(8, 200, 6, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:7' shape=(8, 200, 7) dtype=int32>, <tf.Tensor 'IteratorGetNext:8' shape=(8, 1) dtype=int32>]\n\nwhere 8 is the batchsize.\nI tried the following code to solve it:\n        def add_batch_shape(data):\n            for d in data:\n                shapes = [int(s) for s in d.shape[1:]]\n                shapes.insert(0, opt.batchsize)\n                d.set_shape(shapes)\n\n        def wrapped_add(data):\n            return tf.py_func(add_batch_shape, [data], task.proc_arg_dtype)\n\n        dataset = dataset.map(wrapped_add, num_parallel_calls=self.num_threads) # (?, 512, 512, 3) to (batchsize, 512, 512, 3)\n\nBut it will give out an error:\nTypeError: wrapped_add() takes 1 positional argument but 9 were given\nThe [data] has a data_shape as above.\nI am wondering after applying dataset.batch(batchsize), could the first dimension of items be the batchsize but not None. (and whether tf.py_func could add a new argument which specify the data's shape)", "body": "### System information\r\n- **Have I written custom code (https://github.com/umich-vl/px2graph/blob/master/util/loader.py)**:\r\n- **OS Platform and Distribution (CentOS Linux release 7.4.1708 (Core))**:\r\n- **TensorFlow installed from (binary)**:\r\n- **TensorFlow version (1.4)**:\r\n- **Python version (3.6)**: \r\n- **Bazel version (N/A)**:\r\n- **CUDA/cuDNN version (Cuda 8, Cudnn v6)**:\r\n- **GPU model and memory (Titan X, 12 G)**:\r\n- **Exact command to reproduce: The original code is from https://github.com/umich-vl/px2graph, the author use a queue-based input data method, and I want to change it to use tf.data API. Basically only the util/loader.py need to be changed, and the main parts have been shown as follows.**:\r\n\r\n### Describe the problem\r\nI would like to change the original queue based data load mechanism to tf.data API.\r\n\r\nThe original code is:\r\n```\r\n        # Index queue\r\n        self.input_idxs = tf.placeholder(tf.int64, shape=[None, 2])\r\n        idx_queue = tf.FIFOQueue(1e8, tf.int64)\r\n        self.enq_idxs = idx_queue.enqueue_many(self.input_idxs)\r\n        get_idx = idx_queue.dequeue()\r\n\r\n        # Image loading queue\r\n        img_queue = tf.FIFOQueue(opt.max_queue_size, task.proc_arg_dtype)\r\n        load_data = tf.py_func(task.load_sample_data, [get_idx], task.proc_arg_dtype)\r\n        enq_img = img_queue.enqueue(load_data)\r\n        init_sample = img_queue.dequeue()\r\n\r\n        # Preprocessing queue\r\n        # (for any preprocessing that can be done with TF operations)\r\n        data_queue = tf.FIFOQueue(opt.max_queue_size, task.data_arg_dtype,\r\n                                  shapes=task.data_shape)\r\n        enq_data = data_queue.enqueue(task.preprocess(init_sample, train_flag))\r\n        self.get_sample = data_queue.dequeue_many(opt.batchsize)\r\n```\r\n\r\nAfter the change, it is:\r\n```\r\n        # Dataset\r\n        self.input_idxs = tf.placeholder(tf.int64, shape=[None, 2])\r\n        dataset = tf.data.Dataset.from_tensor_slices(self.input_idxs)\r\n\r\n        def load_sample(idx):\r\n            sample = task.load_sample_data(idx)\r\n            sample = task.preprocess(sample, train_flag)\r\n            return sample\r\n\r\n        dataset = dataset.map(lambda idx: tf.py_func(load_sample, [idx], task.proc_arg_dtype), num_parallel_calls=self.num_threads)\r\n\r\n        def gen(dataset):\r\n            yield dataset.make_one_shot_iterator().get_next()\r\n\r\n        dataset = tf.data.Dataset.from_generator(gen, tuple(task.proc_arg_dtype), tuple(task.data_shape)) # set task.data_shape\r\n        dataset = dataset.batch(opt.batchsize)\r\n        self.iterator = dataset.make_initializable_iterator()\r\n        self.get_sample = self.iterator.get_next()\r\n```\r\n\r\nwhere `task.proc_arg_dtype` and `task.data_shape` are:\r\n```\r\n    proc_arg_dtype = [tf.float32, tf.float32, tf.int32, tf.int32, tf.int32, tf.float32, tf.int32, tf.int32, tf.int32]\r\n    data_shape = [\r\n        [opt.input_res, opt.input_res, 3],\r\n        [opt.output_res, opt.output_res, opt.det_inputs],\r\n        [2, opt.max_nodes, 2],\r\n        [4],\r\n        [opt.max_nodes, opt.obj_slots + opt.rel_slots],\r\n        [opt.max_nodes, opt.obj_slots, 5],\r\n        [opt.max_nodes, opt.rel_slots, 2],\r\n        [opt.max_nodes, 7],\r\n        [1]\r\n    ]\r\n```\r\nSince I find `tf.py_func` doesn't have `data_shape` argument so that I add additional `gen` function and use the `tf.data.Dataset.from_generator` to specify the `data_shape` of items in a dataset. \r\n\r\nMy problem is the shape of item in `dataset.batch(opt.batchsize)` is \r\n```\r\n[<tf.Tensor 'IteratorGetNext:0' shape=(?, 512, 512, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(?, 64, 64, 300) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(?, 2, 200, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:3' shape=(?, 4) dtype=int32>, <tf.Tensor 'IteratorGetNext:4' shape=(?, 200, 9) dtype=int32>, <tf.Tensor 'IteratorGetNext:5' shape=(?, 200, 3, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(?, 200, 6, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:7' shape=(?, 200, 7) dtype=int32>, <tf.Tensor 'IteratorGetNext:8' shape=(?, 1) dtype=int32>]\r\n```\r\nBut I need \r\n```\r\n[<tf.Tensor 'IteratorGetNext:0' shape=(8, 512, 512, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(8, 64, 64, 300) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(8, 2, 200, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:3' shape=(8, 4) dtype=int32>, <tf.Tensor 'IteratorGetNext:4' shape=(8, 200, 9) dtype=int32>, <tf.Tensor 'IteratorGetNext:5' shape=(8, 200, 3, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(8, 200, 6, 2) dtype=int32>, <tf.Tensor 'IteratorGetNext:7' shape=(8, 200, 7) dtype=int32>, <tf.Tensor 'IteratorGetNext:8' shape=(8, 1) dtype=int32>]\r\n```\r\nwhere `8` is the batchsize.\r\n\r\nI tried the following code to solve it:\r\n```\r\n        def add_batch_shape(data):\r\n            for d in data:\r\n                shapes = [int(s) for s in d.shape[1:]]\r\n                shapes.insert(0, opt.batchsize)\r\n                d.set_shape(shapes)\r\n\r\n        def wrapped_add(data):\r\n            return tf.py_func(add_batch_shape, [data], task.proc_arg_dtype)\r\n\r\n        dataset = dataset.map(wrapped_add, num_parallel_calls=self.num_threads) # (?, 512, 512, 3) to (batchsize, 512, 512, 3)\r\n```\r\nBut it will give out an error:\r\n\r\n`TypeError: wrapped_add() takes 1 positional argument but 9 were given`\r\n\r\nThe `[data]` has a `data_shape` as above.\r\n\r\nI am wondering after applying `dataset.batch(batchsize)`, could the first dimension of items be the batchsize but not `None`. (and whether `tf.py_func` could add a new argument which specify the data's shape)\r\n"}