{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19506", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19506/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19506/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19506/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/19506", "id": 325819674, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkwMDYzMDY4", "number": 19506, "title": "Numa device prototype", "user": {"login": "shengfuintel", "id": 3935335, "node_id": "MDQ6VXNlcjM5MzUzMzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3935335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shengfuintel", "html_url": "https://github.com/shengfuintel", "followers_url": "https://api.github.com/users/shengfuintel/followers", "following_url": "https://api.github.com/users/shengfuintel/following{/other_user}", "gists_url": "https://api.github.com/users/shengfuintel/gists{/gist_id}", "starred_url": "https://api.github.com/users/shengfuintel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shengfuintel/subscriptions", "organizations_url": "https://api.github.com/users/shengfuintel/orgs", "repos_url": "https://api.github.com/users/shengfuintel/repos", "events_url": "https://api.github.com/users/shengfuintel/events{/privacy}", "received_events_url": "https://api.github.com/users/shengfuintel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "case540", "id": 1299636, "node_id": "MDQ6VXNlcjEyOTk2MzY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1299636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/case540", "html_url": "https://github.com/case540", "followers_url": "https://api.github.com/users/case540/followers", "following_url": "https://api.github.com/users/case540/following{/other_user}", "gists_url": "https://api.github.com/users/case540/gists{/gist_id}", "starred_url": "https://api.github.com/users/case540/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/case540/subscriptions", "organizations_url": "https://api.github.com/users/case540/orgs", "repos_url": "https://api.github.com/users/case540/repos", "events_url": "https://api.github.com/users/case540/events{/privacy}", "received_events_url": "https://api.github.com/users/case540/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "case540", "id": 1299636, "node_id": "MDQ6VXNlcjEyOTk2MzY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1299636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/case540", "html_url": "https://github.com/case540", "followers_url": "https://api.github.com/users/case540/followers", "following_url": "https://api.github.com/users/case540/following{/other_user}", "gists_url": "https://api.github.com/users/case540/gists{/gist_id}", "starred_url": "https://api.github.com/users/case540/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/case540/subscriptions", "organizations_url": "https://api.github.com/users/case540/orgs", "repos_url": "https://api.github.com/users/case540/repos", "events_url": "https://api.github.com/users/case540/events{/privacy}", "received_events_url": "https://api.github.com/users/case540/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-05-23T18:18:17Z", "updated_at": "2018-08-31T16:25:43Z", "closed_at": "2018-08-31T16:25:43Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19506", "html_url": "https://github.com/tensorflow/tensorflow/pull/19506", "diff_url": "https://github.com/tensorflow/tensorflow/pull/19506.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/19506.patch"}, "body_html": "<p>This pull request is to get the comments for the prototype to support NUMA aware CPU device in TensorFlow.</p>\n<p>From the performance profiling, we found TensorFlow does not scale to multi sockets, the inter-core memory traffic is high.</p>\n<p>This pull request has a prototype to support NUMA aware CPU device. A new device, called NumaDevice, is a collection of CPU cores that belongs to the same NUMA node.  The graph running on the NumaDevice will only use the memory close to that Numa node.  Our experiment showed this prototype dramatically reduced the inter-core memory traffic, and improved the training and inference performance.</p>\n<p>Since this is only a prototype, there are some hacked code to just make it run.</p>\n<p>To make it compile, copy /usr/lib64/libnuma.so to bazel cache directory/external/mkl_linux/lib, and copy NonBlockingThreadPool.h to bazel cache directory/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool.</p>\n<p>The TensorFlow benchmark is modified a little bit to make it run. The branch numa-device-prototype under <a href=\"https://github.com/Intel-tensorflow/benchmarks.git\">https://github.com/Intel-tensorflow/benchmarks.git</a> has the modified script. There is an example in scripts/tf_cnn_benchmarks/run.sh to run ResNet50 with two NUMA devices.</p>\n<p>Feel free to contact me @ <a href=\"mailto:sheng.fu@intel.com\">sheng.fu@intel.com</a> if you have any questions. Looking forward to your comments.</p>", "body_text": "This pull request is to get the comments for the prototype to support NUMA aware CPU device in TensorFlow.\nFrom the performance profiling, we found TensorFlow does not scale to multi sockets, the inter-core memory traffic is high.\nThis pull request has a prototype to support NUMA aware CPU device. A new device, called NumaDevice, is a collection of CPU cores that belongs to the same NUMA node.  The graph running on the NumaDevice will only use the memory close to that Numa node.  Our experiment showed this prototype dramatically reduced the inter-core memory traffic, and improved the training and inference performance.\nSince this is only a prototype, there are some hacked code to just make it run.\nTo make it compile, copy /usr/lib64/libnuma.so to bazel cache directory/external/mkl_linux/lib, and copy NonBlockingThreadPool.h to bazel cache directory/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool.\nThe TensorFlow benchmark is modified a little bit to make it run. The branch numa-device-prototype under https://github.com/Intel-tensorflow/benchmarks.git has the modified script. There is an example in scripts/tf_cnn_benchmarks/run.sh to run ResNet50 with two NUMA devices.\nFeel free to contact me @ sheng.fu@intel.com if you have any questions. Looking forward to your comments.", "body": "This pull request is to get the comments for the prototype to support NUMA aware CPU device in TensorFlow.\r\n \r\nFrom the performance profiling, we found TensorFlow does not scale to multi sockets, the inter-core memory traffic is high.\r\n\r\nThis pull request has a prototype to support NUMA aware CPU device. A new device, called NumaDevice, is a collection of CPU cores that belongs to the same NUMA node.  The graph running on the NumaDevice will only use the memory close to that Numa node.  Our experiment showed this prototype dramatically reduced the inter-core memory traffic, and improved the training and inference performance.  \r\n\r\nSince this is only a prototype, there are some hacked code to just make it run. \r\n\r\nTo make it compile, copy /usr/lib64/libnuma.so to bazel cache directory/external/mkl_linux/lib, and copy NonBlockingThreadPool.h to bazel cache directory/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool. \r\n\r\nThe TensorFlow benchmark is modified a little bit to make it run. The branch numa-device-prototype under https://github.com/Intel-tensorflow/benchmarks.git has the modified script. There is an example in scripts/tf_cnn_benchmarks/run.sh to run ResNet50 with two NUMA devices. \r\n\r\nFeel free to contact me @ sheng.fu@intel.com if you have any questions. Looking forward to your comments. \r\n"}