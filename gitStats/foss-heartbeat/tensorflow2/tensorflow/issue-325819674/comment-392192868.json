{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/392192868", "html_url": "https://github.com/tensorflow/tensorflow/pull/19506#issuecomment-392192868", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19506", "id": 392192868, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjE5Mjg2OA==", "user": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-25T21:32:25Z", "updated_at": "2018-05-25T21:32:38Z", "author_association": "MEMBER", "body_html": "<p>It seems a friend from Intel shared an experimental performance enhancement.</p>\n<p>I've read other teams at Google got 10-20% gains by modifying scheduling to be NUMA-aware. What were your benchmark numbers here? (I ask because your App Engine demo appears down.)</p>\n<p>I see you added <code>pthread_setaffinity</code> to the Eigen thread pools. Here's your diff: <a href=\"https://gist.github.com/jart/84b96f56b92f21eb9ed2ffc4a372dedf\">https://gist.github.com/jart/84b96f56b92f21eb9ed2ffc4a372dedf</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16907534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rmlarsen\">@rmlarsen</a> might be able to help you get those changes upstreamed. We can't vendor sources. We'll also want portability before merging.</p>\n<p>I can't say for certain if the TensorFlow team would consider adopting OpenMP language extensions. GCC and Clang don't support these out of the box.</p>\n<p>Lastly, would it be possible for Intel to distribute MKL-DNN to include <code>.a</code> files built with <code>-ffunction-sections</code> and <code>-fdata-sections</code>? I'm not sure if we can copy 180MB of .so files into our pip wheels. These libraries are also still BSD-3, correct? I heard Intel announced a new license last month.</p>\n<p>cc: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15679194\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rajatmonga\">@rajatmonga</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10168793\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/m3bm3b\">@m3bm3b</a></p>", "body_text": "It seems a friend from Intel shared an experimental performance enhancement.\nI've read other teams at Google got 10-20% gains by modifying scheduling to be NUMA-aware. What were your benchmark numbers here? (I ask because your App Engine demo appears down.)\nI see you added pthread_setaffinity to the Eigen thread pools. Here's your diff: https://gist.github.com/jart/84b96f56b92f21eb9ed2ffc4a372dedf @rmlarsen might be able to help you get those changes upstreamed. We can't vendor sources. We'll also want portability before merging.\nI can't say for certain if the TensorFlow team would consider adopting OpenMP language extensions. GCC and Clang don't support these out of the box.\nLastly, would it be possible for Intel to distribute MKL-DNN to include .a files built with -ffunction-sections and -fdata-sections? I'm not sure if we can copy 180MB of .so files into our pip wheels. These libraries are also still BSD-3, correct? I heard Intel announced a new license last month.\ncc: @rajatmonga @m3bm3b", "body": "It seems a friend from Intel shared an experimental performance enhancement.\r\n\r\nI've read other teams at Google got 10-20% gains by modifying scheduling to be NUMA-aware. What were your benchmark numbers here? (I ask because your App Engine demo appears down.)\r\n\r\nI see you added `pthread_setaffinity` to the Eigen thread pools. Here's your diff: https://gist.github.com/jart/84b96f56b92f21eb9ed2ffc4a372dedf @rmlarsen might be able to help you get those changes upstreamed. We can't vendor sources. We'll also want portability before merging.\r\n\r\nI can't say for certain if the TensorFlow team would consider adopting OpenMP language extensions. GCC and Clang don't support these out of the box.\r\n\r\nLastly, would it be possible for Intel to distribute MKL-DNN to include `.a` files built with `-ffunction-sections` and `-fdata-sections`? I'm not sure if we can copy 180MB of .so files into our pip wheels. These libraries are also still BSD-3, correct? I heard Intel announced a new license last month.\r\n\r\ncc: @rajatmonga @m3bm3b "}