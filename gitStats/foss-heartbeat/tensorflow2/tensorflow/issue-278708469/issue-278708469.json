{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15060", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15060/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15060/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15060/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15060", "id": 278708469, "node_id": "MDU6SXNzdWUyNzg3MDg0Njk=", "number": 15060, "title": "AttentionWrapperZeroState: Input to reshape is a tensor with 32768 values, but the requested shape has 65536", "user": {"login": "tusharag171", "id": 15711945, "node_id": "MDQ6VXNlcjE1NzExOTQ1", "avatar_url": "https://avatars0.githubusercontent.com/u/15711945?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tusharag171", "html_url": "https://github.com/tusharag171", "followers_url": "https://api.github.com/users/tusharag171/followers", "following_url": "https://api.github.com/users/tusharag171/following{/other_user}", "gists_url": "https://api.github.com/users/tusharag171/gists{/gist_id}", "starred_url": "https://api.github.com/users/tusharag171/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tusharag171/subscriptions", "organizations_url": "https://api.github.com/users/tusharag171/orgs", "repos_url": "https://api.github.com/users/tusharag171/repos", "events_url": "https://api.github.com/users/tusharag171/events{/privacy}", "received_events_url": "https://api.github.com/users/tusharag171/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-12-02T19:01:51Z", "updated_at": "2018-04-24T00:17:42Z", "closed_at": "2017-12-20T03:01:52Z", "author_association": "NONE", "body_html": "<p>I am building an encoder-decoder model with attention and BeamSearchDecoder using tensor flow documentation. I am getting following error:</p>\n<pre><code>---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n&lt;ipython-input-160-ac947b28f4dd&gt; in &lt;module&gt;()\n     29                                           summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)],\n     30                                           text_length: [len(text)]*batch_size,\n---&gt; 31                                           keep_prob: 1.0})[0] \n     32         # Remove the padding from the summaries\n     33         pad = vocab_to_int[\"&lt;PAD&gt;\"]\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    893     try:\n    894       result = self._run(None, fetches, feed_dict, options_ptr,\n--&gt; 895                          run_metadata_ptr)\n    896       if run_metadata:\n    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\n   1123       results = self._do_run(handle, final_targets, final_fetches,\n-&gt; 1124                              feed_dict_tensor, options, run_metadata)\n   1125     else:\n   1126       results = []\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n   1319     if handle is None:\n   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n-&gt; 1321                            options, run_metadata)\n   1322     else:\n   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n   1338         except KeyError:\n   1339           pass\n-&gt; 1340       raise type(e)(node_def, op, message)\n   1341 \n   1342   def _extend_graph(self):\n\nInvalidArgumentError: Input to reshape is a tensor with 32768 values, but the requested shape has 65536\n     [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\n\nCaused by op u'decode_1/Reshape_2', defined at:\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in &lt;module&gt;\n    app.launch_new_instance()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-160-ac947b28f4dd&gt;\", line 18, in &lt;module&gt;\n    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\n    **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\n    op_def=op_def)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 32768 values, but the requested shape has 65536\n     [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\n</code></pre>\n<p>The error occurs when trying to Make predictions(Test the Model Section) .Training runs fine. The error occurs when I set beam_size &gt;=2. (The error shown is for beam_size=2). However it runs fine for beam_size = 1. I am not able to figure out what is going wrong. I know the code is long. Any help will be highly appreciated as I am unable to debug it and stuck on it for days. My code is:</p>\n<p>My code is here:</p>\n<h1>Model Inputs</h1>\n<pre><code>def model_inputs():\n    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n    lr = tf.placeholder(tf.float32, name='learning_rate')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n\n    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\n\ndef process_encoding_input(target_data, vocab_to_int, batch_size):  \n    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['&lt;GO&gt;']), ending], 1)\n\n    return dec_input\n</code></pre>\n<h1>Encoding layer</h1>\n<pre><code>def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n    for layer in range(num_layers):\n        with tf.variable_scope('encoder_{}'.format(layer)):\n            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n                                                    input_keep_prob = keep_prob)\n\n            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n                                                    input_keep_prob = keep_prob)\n\n            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n                                                                    cell_bw, \n                                                                    rnn_inputs,\n                                                                    sequence_length,\n                                                                    dtype=tf.float32)\n            enc_output = tf.concat(enc_output,2)\n            # original code is missing this line below, that is how we connect layers \n            # by feeding the current layer's output to next layer's input\n            #rnn_inputs = enc_output\n    return enc_output, enc_state\n</code></pre>\n<h1>Training Decoding layer</h1>\n<pre><code>def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n                            vocab_size, max_summary_length,batch_size,enc_state):\n    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n                                                        sequence_length=summary_length,\n                                                        time_major=False)\n\n    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n                                                       helper=training_helper,\n                                        initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(cell_state=enc_state),\n                                                       output_layer = output_layer)\n\n    training_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n                                                            impute_finished = True,\n                                                           maximum_iterations=max_summary_length)\n    return training_logits\n</code></pre>\n<h1>Inference Decoding layer</h1>\n<pre><code>def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n                             max_summary_length, batch_size,lengths,enc_state,beam_width):\n    '''Create the inference logits'''\n\n    start_tokens = tf.ones_like(lengths) * start_token\n\n\n\n    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                    cell          = dec_cell,\n                    embedding     = embeddings,\n                    start_tokens  = start_tokens,\n                    end_token     = end_token,\n                    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size * beam_width*2).clone(cell_state=enc_state) ,\n                    beam_width    = beam_width,\n                    output_layer  = output_layer)\n\n\n    inference_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n                                                             impute_finished = False,\n                                                            maximum_iterations=max_summary_length)\n\n    return inference_logits\n\ndef lstm_cell(lstm_size, keep_prob):\n    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n</code></pre>\n<h1>Decoding layer</h1>\n<pre><code>def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n    '''Create the decoding cell and attention for the training and inference decoding layers'''\n    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n\n    with tf.variable_scope(\"decode\"):\n        dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n        attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n                                                     enc_output,\n                                                     text_length,\n                                                     normalize=False,\n                                                     )\n        dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\n                                                       attention_mechanism = attn_mech, \n                                                       attention_layer_size=rnn_size)\n\n        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n                                                  output_layer,\n                                                  vocab_size,\n                                                  max_summary_length,\n                                                  batch_size,enc_state)\n\n\n    beam_width = 2\n    enc_output = tf.contrib.seq2seq.tile_batch(enc_output, multiplier=beam_width)\n    lengths = tf.contrib.seq2seq.tile_batch(text_length, multiplier=beam_width)\n    enc_state = tf.contrib.seq2seq.tile_batch(enc_state, multiplier=beam_width)\n\n    with tf.variable_scope(\"decode\", reuse=True):\n        dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n        attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n                                                     enc_output,\n                                                     lengths,\n                                                     normalize=False,\n                                                     )\n        dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\n                                                       attention_mechanism = attn_mech, \n                                                       attention_layer_size=rnn_size)\n\n\n        inference_logits = inference_decoding_layer(embeddings,\n                                                    vocab_to_int['&lt;GO&gt;'],\n                                                    vocab_to_int['&lt;EOS&gt;'],\n                                                    dec_cell,\n                                                    output_layer,\n                                                    max_summary_length,\n                                                    batch_size,lengths,enc_state,beam_width)\n    return training_logits, inference_logits\n\n\ndef seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n    '''Use the previous functions to create the training and inference logits'''\n\n    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n    #embeddings = word_embedding_matrix\n    embeddings_s = tf.get_variable(\"word_embeddings\",[vocab_size, 300])\n    embeddings_t = tf.get_variable(\"word_embeddings2\",[vocab_size, 300])\n    enc_embed_input = tf.nn.embedding_lookup(embeddings_s, input_data)\n    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of&lt;GO&gt;\n    dec_embed_input = tf.nn.embedding_lookup(embeddings_t, dec_input)\n    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n                                                        embeddings_t,\n                                                        enc_output,\n                                                        enc_state, \n                                                        vocab_size, \n                                                        text_length, \n                                                        summary_length, \n                                                        max_summary_length,\n                                                        rnn_size, \n                                                        vocab_to_int, \n                                                        keep_prob, \n                                                        batch_size,\n                                                        num_layers)\n    return training_logits, inference_logits\n</code></pre>\n<h1>Set the Hyperparameters</h1>\n<pre><code>epochs = 100\nbatch_size = 64\nrnn_size = 256\nnum_layers = 2\nlearning_rate = 0.005\nkeep_probability = 0.95\n</code></pre>\n<h1>Build the graph</h1>\n<pre><code>train_graph = tf.Graph()\n# Set the graph to default to ensure that it is ready for training\nwith train_graph.as_default():\n\n    # Load the model inputs    \n    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n\n    # Create the training and inference logits\n    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n                                                      targets, \n                                                      keep_prob,   \n                                                      text_length,\n                                                      summary_length,\n                                                      max_summary_length,\n                                                      len(vocab_to_int)+1,\n                                                      rnn_size, \n                                                      num_layers, \n                                                      vocab_to_int,\n                                                      batch_size)\n\n    # Create tensors for the training logits and inference logits\n    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n    #inference_logits = inference_logits.predicted_ids[:,:,0]\n    inference_logits = tf.identity(inference_logits.predicted_ids, name='predictions')\n\n    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n\n    with tf.name_scope(\"optimization\"):\n        # Loss function\n        cost = tf.contrib.seq2seq.sequence_loss(\n            training_logits,\n            targets,\n            masks)\n\n        # Optimizer\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n\n        # Gradient Clipping\n        gradients = optimizer.compute_gradients(cost)\n        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n        train_op = optimizer.apply_gradients(capped_gradients)\nprint(\"Graph is built.\")\ngraph_location = \"./graph\"\nprint(graph_location)\ntrain_writer = tf.summary.FileWriter(graph_location)\ntrain_writer.add_graph(train_graph)\n</code></pre>\n<h1>Train the Model</h1>\n<pre><code>learning_rate_decay = 0.95\nmin_learning_rate = 0.0005\ndisplay_step = 20 # Check training loss after every 20 batches\nstop_early = 0 \nstop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\nper_epoch = 3 # Make 3 update checks per epoch\n#update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\nupdate_check = 2\n\nupdate_loss = 0 \nbatch_loss = 0\nsummary_update_loss = [] # Record the update losses for saving improvements in the model\n\ncheckpoint = \"./best_model.ckpt\" \nwith tf.Session(graph=train_graph) as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # If we want to continue training a previous session\n    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n    #loader.restore(sess, checkpoint)\n\n    for epoch_i in range(1, epochs+1):\n        update_loss = 0\n        batch_loss = 0\n        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n            start_time = time.time()\n            _, loss = sess.run(\n                [train_op, cost],\n                {input_data: texts_batch,\n                 targets: summaries_batch,\n                 lr: learning_rate,\n                 summary_length: summaries_lengths,\n                 text_length: texts_lengths,\n                 keep_prob: keep_probability})\n\n            batch_loss += loss\n            update_loss += loss\n            end_time = time.time()\n            batch_time = end_time - start_time\n\n            if batch_i % display_step == 0 and batch_i &gt; 0:\n                print('Epoch {:&gt;3}/{} Batch {:&gt;4}/{} - Loss: {:&gt;6.3f}, Seconds: {:&gt;4.2f}'\n                      .format(epoch_i,\n                              epochs, \n                              batch_i, \n                              len(sorted_texts_short) // batch_size, \n                              batch_loss / display_step, \n                              batch_time*display_step))\n                batch_loss = 0\n\n            if batch_i % update_check == 0 and batch_i &gt; 0:\n                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n                summary_update_loss.append(update_loss)\n\n                # If the update loss is at a new minimum, save the model\n                if update_loss &lt;= min(summary_update_loss):\n                    print('New Record!') \n                    stop_early = 0\n                    saver = tf.train.Saver() \n                    saver.save(sess, checkpoint)\n\n                else:\n                    print(\"No Improvement.\")\n                    stop_early += 1\n                    if stop_early == stop:\n                        break\n                update_loss = 0\n\n\n        # Reduce learning rate, but not below its minimum value\n        learning_rate *= learning_rate_decay\n        if learning_rate &lt; min_learning_rate:\n            learning_rate = min_learning_rate\n\n        if stop_early == stop:\n            print(\"Stopping Training.\")\n            break\n\n\ndef text_to_seq(text):\n    '''Prepare the text for the model'''\n\n    text = clean_text(text)\n    return [vocab_to_int.get(word, vocab_to_int['&lt;UNK&gt;']) for word in text.split()]\n</code></pre>\n<h1>Test The Model</h1>\n<pre><code>input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\n               \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\ngeneragte_summary_length =  [3,2]\n\n# input_sentences = test_text[0:10]\n# generagte_summary_length = summary_l[0:10]\ntexts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\ncheckpoint = \"./best_model.ckpt\"\nif type(generagte_summary_length) is list:\n    if len(input_sentences)!=len(generagte_summary_length):\n        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n    generagte_summary_length_list = generagte_summary_length\nelse:\n    generagte_summary_length_list = [generagte_summary_length] * len(texts)\nloaded_graph = tf.Graph()\nwith tf.Session(graph=loaded_graph) as sess:\n    # Load saved model\n    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n    loader.restore(sess, checkpoint)\n    input_data = loaded_graph.get_tensor_by_name('input:0')\n    logits = loaded_graph.get_tensor_by_name('predictions:0')\n    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n    #Multiply by batch_size to match the model's input parameters\n    for i, text in enumerate(texts):\n        generagte_summary_length = generagte_summary_length_list[i]\n        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n                                          text_length: [len(text)]*batch_size,\n                                          keep_prob: 1.0})[0] \n        # Remove the padding from the summaries\n        pad = vocab_to_int[\"&lt;PAD&gt;\"] \n        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i[0]] for i in answer_logits if i[0] != pad])))\n</code></pre>\n<p>tensorflow</p>", "body_text": "I am building an encoder-decoder model with attention and BeamSearchDecoder using tensor flow documentation. I am getting following error:\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n<ipython-input-160-ac947b28f4dd> in <module>()\n     29                                           summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)],\n     30                                           text_length: [len(text)]*batch_size,\n---> 31                                           keep_prob: 1.0})[0] \n     32         # Remove the padding from the summaries\n     33         pad = vocab_to_int[\"<PAD>\"]\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    893     try:\n    894       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 895                          run_metadata_ptr)\n    896       if run_metadata:\n    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\n   1123       results = self._do_run(handle, final_targets, final_fetches,\n-> 1124                              feed_dict_tensor, options, run_metadata)\n   1125     else:\n   1126       results = []\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n   1319     if handle is None:\n   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n-> 1321                            options, run_metadata)\n   1322     else:\n   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\n\n/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n   1338         except KeyError:\n   1339           pass\n-> 1340       raise type(e)(node_def, op, message)\n   1341 \n   1342   def _extend_graph(self):\n\nInvalidArgumentError: Input to reshape is a tensor with 32768 values, but the requested shape has 65536\n     [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\n\nCaused by op u'decode_1/Reshape_2', defined at:\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-160-ac947b28f4dd>\", line 18, in <module>\n    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\n    **kwargs)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\n    op_def=op_def)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 32768 values, but the requested shape has 65536\n     [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\n\nThe error occurs when trying to Make predictions(Test the Model Section) .Training runs fine. The error occurs when I set beam_size >=2. (The error shown is for beam_size=2). However it runs fine for beam_size = 1. I am not able to figure out what is going wrong. I know the code is long. Any help will be highly appreciated as I am unable to debug it and stuck on it for days. My code is:\nMy code is here:\nModel Inputs\ndef model_inputs():\n    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n    lr = tf.placeholder(tf.float32, name='learning_rate')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n\n    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\n\ndef process_encoding_input(target_data, vocab_to_int, batch_size):  \n    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n\n    return dec_input\n\nEncoding layer\ndef encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n    for layer in range(num_layers):\n        with tf.variable_scope('encoder_{}'.format(layer)):\n            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n                                                    input_keep_prob = keep_prob)\n\n            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n                                                    input_keep_prob = keep_prob)\n\n            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n                                                                    cell_bw, \n                                                                    rnn_inputs,\n                                                                    sequence_length,\n                                                                    dtype=tf.float32)\n            enc_output = tf.concat(enc_output,2)\n            # original code is missing this line below, that is how we connect layers \n            # by feeding the current layer's output to next layer's input\n            #rnn_inputs = enc_output\n    return enc_output, enc_state\n\nTraining Decoding layer\ndef training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n                            vocab_size, max_summary_length,batch_size,enc_state):\n    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n                                                        sequence_length=summary_length,\n                                                        time_major=False)\n\n    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n                                                       helper=training_helper,\n                                        initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(cell_state=enc_state),\n                                                       output_layer = output_layer)\n\n    training_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n                                                            impute_finished = True,\n                                                           maximum_iterations=max_summary_length)\n    return training_logits\n\nInference Decoding layer\ndef inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n                             max_summary_length, batch_size,lengths,enc_state,beam_width):\n    '''Create the inference logits'''\n\n    start_tokens = tf.ones_like(lengths) * start_token\n\n\n\n    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                    cell          = dec_cell,\n                    embedding     = embeddings,\n                    start_tokens  = start_tokens,\n                    end_token     = end_token,\n                    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size * beam_width*2).clone(cell_state=enc_state) ,\n                    beam_width    = beam_width,\n                    output_layer  = output_layer)\n\n\n    inference_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n                                                             impute_finished = False,\n                                                            maximum_iterations=max_summary_length)\n\n    return inference_logits\n\ndef lstm_cell(lstm_size, keep_prob):\n    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n\nDecoding layer\ndef decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n    '''Create the decoding cell and attention for the training and inference decoding layers'''\n    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n\n    with tf.variable_scope(\"decode\"):\n        dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n        attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n                                                     enc_output,\n                                                     text_length,\n                                                     normalize=False,\n                                                     )\n        dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\n                                                       attention_mechanism = attn_mech, \n                                                       attention_layer_size=rnn_size)\n\n        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n                                                  output_layer,\n                                                  vocab_size,\n                                                  max_summary_length,\n                                                  batch_size,enc_state)\n\n\n    beam_width = 2\n    enc_output = tf.contrib.seq2seq.tile_batch(enc_output, multiplier=beam_width)\n    lengths = tf.contrib.seq2seq.tile_batch(text_length, multiplier=beam_width)\n    enc_state = tf.contrib.seq2seq.tile_batch(enc_state, multiplier=beam_width)\n\n    with tf.variable_scope(\"decode\", reuse=True):\n        dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n        attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n                                                     enc_output,\n                                                     lengths,\n                                                     normalize=False,\n                                                     )\n        dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\n                                                       attention_mechanism = attn_mech, \n                                                       attention_layer_size=rnn_size)\n\n\n        inference_logits = inference_decoding_layer(embeddings,\n                                                    vocab_to_int['<GO>'],\n                                                    vocab_to_int['<EOS>'],\n                                                    dec_cell,\n                                                    output_layer,\n                                                    max_summary_length,\n                                                    batch_size,lengths,enc_state,beam_width)\n    return training_logits, inference_logits\n\n\ndef seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n    '''Use the previous functions to create the training and inference logits'''\n\n    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n    #embeddings = word_embedding_matrix\n    embeddings_s = tf.get_variable(\"word_embeddings\",[vocab_size, 300])\n    embeddings_t = tf.get_variable(\"word_embeddings2\",[vocab_size, 300])\n    enc_embed_input = tf.nn.embedding_lookup(embeddings_s, input_data)\n    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n    dec_embed_input = tf.nn.embedding_lookup(embeddings_t, dec_input)\n    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n                                                        embeddings_t,\n                                                        enc_output,\n                                                        enc_state, \n                                                        vocab_size, \n                                                        text_length, \n                                                        summary_length, \n                                                        max_summary_length,\n                                                        rnn_size, \n                                                        vocab_to_int, \n                                                        keep_prob, \n                                                        batch_size,\n                                                        num_layers)\n    return training_logits, inference_logits\n\nSet the Hyperparameters\nepochs = 100\nbatch_size = 64\nrnn_size = 256\nnum_layers = 2\nlearning_rate = 0.005\nkeep_probability = 0.95\n\nBuild the graph\ntrain_graph = tf.Graph()\n# Set the graph to default to ensure that it is ready for training\nwith train_graph.as_default():\n\n    # Load the model inputs    \n    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n\n    # Create the training and inference logits\n    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n                                                      targets, \n                                                      keep_prob,   \n                                                      text_length,\n                                                      summary_length,\n                                                      max_summary_length,\n                                                      len(vocab_to_int)+1,\n                                                      rnn_size, \n                                                      num_layers, \n                                                      vocab_to_int,\n                                                      batch_size)\n\n    # Create tensors for the training logits and inference logits\n    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n    #inference_logits = inference_logits.predicted_ids[:,:,0]\n    inference_logits = tf.identity(inference_logits.predicted_ids, name='predictions')\n\n    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n\n    with tf.name_scope(\"optimization\"):\n        # Loss function\n        cost = tf.contrib.seq2seq.sequence_loss(\n            training_logits,\n            targets,\n            masks)\n\n        # Optimizer\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n\n        # Gradient Clipping\n        gradients = optimizer.compute_gradients(cost)\n        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n        train_op = optimizer.apply_gradients(capped_gradients)\nprint(\"Graph is built.\")\ngraph_location = \"./graph\"\nprint(graph_location)\ntrain_writer = tf.summary.FileWriter(graph_location)\ntrain_writer.add_graph(train_graph)\n\nTrain the Model\nlearning_rate_decay = 0.95\nmin_learning_rate = 0.0005\ndisplay_step = 20 # Check training loss after every 20 batches\nstop_early = 0 \nstop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\nper_epoch = 3 # Make 3 update checks per epoch\n#update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\nupdate_check = 2\n\nupdate_loss = 0 \nbatch_loss = 0\nsummary_update_loss = [] # Record the update losses for saving improvements in the model\n\ncheckpoint = \"./best_model.ckpt\" \nwith tf.Session(graph=train_graph) as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # If we want to continue training a previous session\n    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n    #loader.restore(sess, checkpoint)\n\n    for epoch_i in range(1, epochs+1):\n        update_loss = 0\n        batch_loss = 0\n        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n            start_time = time.time()\n            _, loss = sess.run(\n                [train_op, cost],\n                {input_data: texts_batch,\n                 targets: summaries_batch,\n                 lr: learning_rate,\n                 summary_length: summaries_lengths,\n                 text_length: texts_lengths,\n                 keep_prob: keep_probability})\n\n            batch_loss += loss\n            update_loss += loss\n            end_time = time.time()\n            batch_time = end_time - start_time\n\n            if batch_i % display_step == 0 and batch_i > 0:\n                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n                      .format(epoch_i,\n                              epochs, \n                              batch_i, \n                              len(sorted_texts_short) // batch_size, \n                              batch_loss / display_step, \n                              batch_time*display_step))\n                batch_loss = 0\n\n            if batch_i % update_check == 0 and batch_i > 0:\n                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n                summary_update_loss.append(update_loss)\n\n                # If the update loss is at a new minimum, save the model\n                if update_loss <= min(summary_update_loss):\n                    print('New Record!') \n                    stop_early = 0\n                    saver = tf.train.Saver() \n                    saver.save(sess, checkpoint)\n\n                else:\n                    print(\"No Improvement.\")\n                    stop_early += 1\n                    if stop_early == stop:\n                        break\n                update_loss = 0\n\n\n        # Reduce learning rate, but not below its minimum value\n        learning_rate *= learning_rate_decay\n        if learning_rate < min_learning_rate:\n            learning_rate = min_learning_rate\n\n        if stop_early == stop:\n            print(\"Stopping Training.\")\n            break\n\n\ndef text_to_seq(text):\n    '''Prepare the text for the model'''\n\n    text = clean_text(text)\n    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n\nTest The Model\ninput_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\n               \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\ngeneragte_summary_length =  [3,2]\n\n# input_sentences = test_text[0:10]\n# generagte_summary_length = summary_l[0:10]\ntexts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\ncheckpoint = \"./best_model.ckpt\"\nif type(generagte_summary_length) is list:\n    if len(input_sentences)!=len(generagte_summary_length):\n        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n    generagte_summary_length_list = generagte_summary_length\nelse:\n    generagte_summary_length_list = [generagte_summary_length] * len(texts)\nloaded_graph = tf.Graph()\nwith tf.Session(graph=loaded_graph) as sess:\n    # Load saved model\n    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n    loader.restore(sess, checkpoint)\n    input_data = loaded_graph.get_tensor_by_name('input:0')\n    logits = loaded_graph.get_tensor_by_name('predictions:0')\n    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n    #Multiply by batch_size to match the model's input parameters\n    for i, text in enumerate(texts):\n        generagte_summary_length = generagte_summary_length_list[i]\n        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n                                          text_length: [len(text)]*batch_size,\n                                          keep_prob: 1.0})[0] \n        # Remove the padding from the summaries\n        pad = vocab_to_int[\"<PAD>\"] \n        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i[0]] for i in answer_logits if i[0] != pad])))\n\ntensorflow", "body": "I am building an encoder-decoder model with attention and BeamSearchDecoder using tensor flow documentation. I am getting following error:\r\n\r\n    ---------------------------------------------------------------------------\r\n    InvalidArgumentError                      Traceback (most recent call last)\r\n    <ipython-input-160-ac947b28f4dd> in <module>()\r\n         29                                           summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)],\r\n         30                                           text_length: [len(text)]*batch_size,\r\n    ---> 31                                           keep_prob: 1.0})[0] \r\n         32         # Remove the padding from the summaries\r\n         33         pad = vocab_to_int[\"<PAD>\"]\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n        893     try:\r\n        894       result = self._run(None, fetches, feed_dict, options_ptr,\r\n    --> 895                          run_metadata_ptr)\r\n        896       if run_metadata:\r\n        897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n       1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n       1123       results = self._do_run(handle, final_targets, final_fetches,\r\n    -> 1124                              feed_dict_tensor, options, run_metadata)\r\n       1125     else:\r\n       1126       results = []\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n       1319     if handle is None:\r\n       1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n    -> 1321                            options, run_metadata)\r\n       1322     else:\r\n       1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n       1338         except KeyError:\r\n       1339           pass\r\n    -> 1340       raise type(e)(node_def, op, message)\r\n       1341 \r\n       1342   def _extend_graph(self):\r\n\r\n    InvalidArgumentError: Input to reshape is a tensor with 32768 values, but the requested shape has 65536\r\n         [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\r\n\r\n    Caused by op u'decode_1/Reshape_2', defined at:\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n        \"__main__\", fname, loader, pkg_name)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\r\n        exec code in run_globals\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n        app.launch_new_instance()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n        app.start()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n        ioloop.IOLoop.instance().start()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n        super(ZMQIOLoop, self).start()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\r\n        handler_func(fd_obj, events)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n        return fn(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n        self._handle_recv()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n        self._run_callback(callback, msg)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n        callback(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n        return fn(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n        return self.dispatch_shell(stream, msg)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n        handler(stream, idents, msg)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n        user_expressions, allow_stdin)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n        res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n        interactivity=interactivity, compiler=compiler, result=result)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n        if self.run_code(code, result):\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n        exec(code_obj, self.user_global_ns, self.user_ns)\r\n      File \"<ipython-input-160-ac947b28f4dd>\", line 18, in <module>\r\n        loader = tf.train.import_meta_graph(checkpoint + '.meta')\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\r\n        **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\r\n        producer_op_list=producer_op_list)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n        op_def=op_def)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n        original_op=self._default_original_op, op_def=op_def)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n    InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 32768 values, but the requested shape has 65536\r\n         [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\r\n\r\nThe error occurs when trying to Make predictions(Test the Model Section) .Training runs fine. The error occurs when I set beam_size >=2. (The error shown is for beam_size=2). However it runs fine for beam_size = 1. I am not able to figure out what is going wrong. I know the code is long. Any help will be highly appreciated as I am unable to debug it and stuck on it for days. My code is:\r\n\r\nMy code is here:\r\n# Model Inputs\r\n\r\n    def model_inputs():\r\n        input_data = tf.placeholder(tf.int32, [None, None], name='input')\r\n        targets = tf.placeholder(tf.int32, [None, None], name='targets')\r\n        lr = tf.placeholder(tf.float32, name='learning_rate')\r\n        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\r\n        summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\r\n        max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\r\n        text_length = tf.placeholder(tf.int32, (None,), name='text_length')\r\n\r\n        return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\r\n\r\n    def process_encoding_input(target_data, vocab_to_int, batch_size):  \r\n        ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\r\n        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\r\n\r\n        return dec_input\r\n# Encoding layer\r\n\r\n    def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\r\n        for layer in range(num_layers):\r\n            with tf.variable_scope('encoder_{}'.format(layer)):\r\n                cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\r\n                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n                cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \r\n                                                        input_keep_prob = keep_prob)\r\n\r\n                cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\r\n                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n                cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \r\n                                                        input_keep_prob = keep_prob)\r\n\r\n                enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \r\n                                                                        cell_bw, \r\n                                                                        rnn_inputs,\r\n                                                                        sequence_length,\r\n                                                                        dtype=tf.float32)\r\n                enc_output = tf.concat(enc_output,2)\r\n                # original code is missing this line below, that is how we connect layers \r\n                # by feeding the current layer's output to next layer's input\r\n                #rnn_inputs = enc_output\r\n        return enc_output, enc_state\r\n\r\n# Training Decoding layer  \r\n\r\n    def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\r\n                                vocab_size, max_summary_length,batch_size,enc_state):\r\n        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\r\n                                                            sequence_length=summary_length,\r\n                                                            time_major=False)\r\n\r\n        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\r\n                                                           helper=training_helper,\r\n                                            initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(cell_state=enc_state),\r\n                                                           output_layer = output_layer)\r\n\r\n        training_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\r\n                                                                impute_finished = True,\r\n                                                               maximum_iterations=max_summary_length)\r\n        return training_logits\r\n\r\n# Inference Decoding layer  \r\n\r\n    def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\r\n                                 max_summary_length, batch_size,lengths,enc_state,beam_width):\r\n        '''Create the inference logits'''\r\n\r\n        start_tokens = tf.ones_like(lengths) * start_token\r\n\r\n\r\n\r\n        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n                        cell          = dec_cell,\r\n                        embedding     = embeddings,\r\n                        start_tokens  = start_tokens,\r\n                        end_token     = end_token,\r\n                        initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size * beam_width*2).clone(cell_state=enc_state) ,\r\n                        beam_width    = beam_width,\r\n                        output_layer  = output_layer)\r\n\r\n\r\n        inference_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\r\n                                                                 impute_finished = False,\r\n                                                                maximum_iterations=max_summary_length)\r\n\r\n        return inference_logits\r\n\r\n    def lstm_cell(lstm_size, keep_prob):\r\n        cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n        return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\r\n\r\n# Decoding layer \r\n\r\n    def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\r\n                       max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\r\n        '''Create the decoding cell and attention for the training and inference decoding layers'''\r\n        output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\r\n\r\n        with tf.variable_scope(\"decode\"):\r\n            dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\r\n            attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\r\n                                                         enc_output,\r\n                                                         text_length,\r\n                                                         normalize=False,\r\n                                                         )\r\n            dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\r\n                                                           attention_mechanism = attn_mech, \r\n                                                           attention_layer_size=rnn_size)\r\n\r\n            training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\r\n                                                      output_layer,\r\n                                                      vocab_size,\r\n                                                      max_summary_length,\r\n                                                      batch_size,enc_state)\r\n\r\n\r\n        beam_width = 2\r\n        enc_output = tf.contrib.seq2seq.tile_batch(enc_output, multiplier=beam_width)\r\n        lengths = tf.contrib.seq2seq.tile_batch(text_length, multiplier=beam_width)\r\n        enc_state = tf.contrib.seq2seq.tile_batch(enc_state, multiplier=beam_width)\r\n\r\n        with tf.variable_scope(\"decode\", reuse=True):\r\n            dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\r\n            attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\r\n                                                         enc_output,\r\n                                                         lengths,\r\n                                                         normalize=False,\r\n                                                         )\r\n            dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\r\n                                                           attention_mechanism = attn_mech, \r\n                                                           attention_layer_size=rnn_size)\r\n\r\n\r\n            inference_logits = inference_decoding_layer(embeddings,\r\n                                                        vocab_to_int['<GO>'],\r\n                                                        vocab_to_int['<EOS>'],\r\n                                                        dec_cell,\r\n                                                        output_layer,\r\n                                                        max_summary_length,\r\n                                                        batch_size,lengths,enc_state,beam_width)\r\n        return training_logits, inference_logits\r\n\r\n\r\n    def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \r\n                      vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\r\n        '''Use the previous functions to create the training and inference logits'''\r\n\r\n        # Use Numberbatch's embeddings and the newly created ones as our embeddings\r\n        #embeddings = word_embedding_matrix\r\n        embeddings_s = tf.get_variable(\"word_embeddings\",[vocab_size, 300])\r\n        embeddings_t = tf.get_variable(\"word_embeddings2\",[vocab_size, 300])\r\n        enc_embed_input = tf.nn.embedding_lookup(embeddings_s, input_data)\r\n        enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\r\n        dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\r\n        dec_embed_input = tf.nn.embedding_lookup(embeddings_t, dec_input)\r\n        training_logits, inference_logits  = decoding_layer(dec_embed_input, \r\n                                                            embeddings_t,\r\n                                                            enc_output,\r\n                                                            enc_state, \r\n                                                            vocab_size, \r\n                                                            text_length, \r\n                                                            summary_length, \r\n                                                            max_summary_length,\r\n                                                            rnn_size, \r\n                                                            vocab_to_int, \r\n                                                            keep_prob, \r\n                                                            batch_size,\r\n                                                            num_layers)\r\n        return training_logits, inference_logits\r\n\r\n# Set the Hyperparameters\r\n    epochs = 100\r\n    batch_size = 64\r\n    rnn_size = 256\r\n    num_layers = 2\r\n    learning_rate = 0.005\r\n    keep_probability = 0.95\r\n\r\n\r\n# Build the graph\r\n    train_graph = tf.Graph()\r\n    # Set the graph to default to ensure that it is ready for training\r\n    with train_graph.as_default():\r\n\r\n        # Load the model inputs    \r\n        input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\r\n\r\n        # Create the training and inference logits\r\n        training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\r\n                                                          targets, \r\n                                                          keep_prob,   \r\n                                                          text_length,\r\n                                                          summary_length,\r\n                                                          max_summary_length,\r\n                                                          len(vocab_to_int)+1,\r\n                                                          rnn_size, \r\n                                                          num_layers, \r\n                                                          vocab_to_int,\r\n                                                          batch_size)\r\n\r\n        # Create tensors for the training logits and inference logits\r\n        training_logits = tf.identity(training_logits.rnn_output, 'logits')\r\n        #inference_logits = inference_logits.predicted_ids[:,:,0]\r\n        inference_logits = tf.identity(inference_logits.predicted_ids, name='predictions')\r\n\r\n        # Create the weights for sequence_loss, the sould be all True across since each batch is padded\r\n        masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\r\n\r\n        with tf.name_scope(\"optimization\"):\r\n            # Loss function\r\n            cost = tf.contrib.seq2seq.sequence_loss(\r\n                training_logits,\r\n                targets,\r\n                masks)\r\n\r\n            # Optimizer\r\n            optimizer = tf.train.AdamOptimizer(learning_rate)\r\n\r\n            # Gradient Clipping\r\n            gradients = optimizer.compute_gradients(cost)\r\n            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\r\n            train_op = optimizer.apply_gradients(capped_gradients)\r\n    print(\"Graph is built.\")\r\n    graph_location = \"./graph\"\r\n    print(graph_location)\r\n    train_writer = tf.summary.FileWriter(graph_location)\r\n    train_writer.add_graph(train_graph)\r\n\r\n\r\n# Train the Model\r\n    learning_rate_decay = 0.95\r\n    min_learning_rate = 0.0005\r\n    display_step = 20 # Check training loss after every 20 batches\r\n    stop_early = 0 \r\n    stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\r\n    per_epoch = 3 # Make 3 update checks per epoch\r\n    #update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\r\n    update_check = 2\r\n\r\n    update_loss = 0 \r\n    batch_loss = 0\r\n    summary_update_loss = [] # Record the update losses for saving improvements in the model\r\n\r\n    checkpoint = \"./best_model.ckpt\" \r\n    with tf.Session(graph=train_graph) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        # If we want to continue training a previous session\r\n        #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\r\n        #loader.restore(sess, checkpoint)\r\n\r\n        for epoch_i in range(1, epochs+1):\r\n            update_loss = 0\r\n            batch_loss = 0\r\n            for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\r\n                    get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\r\n                start_time = time.time()\r\n                _, loss = sess.run(\r\n                    [train_op, cost],\r\n                    {input_data: texts_batch,\r\n                     targets: summaries_batch,\r\n                     lr: learning_rate,\r\n                     summary_length: summaries_lengths,\r\n                     text_length: texts_lengths,\r\n                     keep_prob: keep_probability})\r\n\r\n                batch_loss += loss\r\n                update_loss += loss\r\n                end_time = time.time()\r\n                batch_time = end_time - start_time\r\n\r\n                if batch_i % display_step == 0 and batch_i > 0:\r\n                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\r\n                          .format(epoch_i,\r\n                                  epochs, \r\n                                  batch_i, \r\n                                  len(sorted_texts_short) // batch_size, \r\n                                  batch_loss / display_step, \r\n                                  batch_time*display_step))\r\n                    batch_loss = 0\r\n\r\n                if batch_i % update_check == 0 and batch_i > 0:\r\n                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\r\n                    summary_update_loss.append(update_loss)\r\n\r\n                    # If the update loss is at a new minimum, save the model\r\n                    if update_loss <= min(summary_update_loss):\r\n                        print('New Record!') \r\n                        stop_early = 0\r\n                        saver = tf.train.Saver() \r\n                        saver.save(sess, checkpoint)\r\n\r\n                    else:\r\n                        print(\"No Improvement.\")\r\n                        stop_early += 1\r\n                        if stop_early == stop:\r\n                            break\r\n                    update_loss = 0\r\n\r\n\r\n            # Reduce learning rate, but not below its minimum value\r\n            learning_rate *= learning_rate_decay\r\n            if learning_rate < min_learning_rate:\r\n                learning_rate = min_learning_rate\r\n\r\n            if stop_early == stop:\r\n                print(\"Stopping Training.\")\r\n                break\r\n\r\n\r\n    def text_to_seq(text):\r\n        '''Prepare the text for the model'''\r\n\r\n        text = clean_text(text)\r\n        return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\r\n# Test The Model   \r\n\r\n    input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\r\n                   \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\r\n    generagte_summary_length =  [3,2]\r\n\r\n    # input_sentences = test_text[0:10]\r\n    # generagte_summary_length = summary_l[0:10]\r\n    texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\r\n    checkpoint = \"./best_model.ckpt\"\r\n    if type(generagte_summary_length) is list:\r\n        if len(input_sentences)!=len(generagte_summary_length):\r\n            raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\r\n        generagte_summary_length_list = generagte_summary_length\r\n    else:\r\n        generagte_summary_length_list = [generagte_summary_length] * len(texts)\r\n    loaded_graph = tf.Graph()\r\n    with tf.Session(graph=loaded_graph) as sess:\r\n        # Load saved model\r\n        loader = tf.train.import_meta_graph(checkpoint + '.meta')\r\n        loader.restore(sess, checkpoint)\r\n        input_data = loaded_graph.get_tensor_by_name('input:0')\r\n        logits = loaded_graph.get_tensor_by_name('predictions:0')\r\n        text_length = loaded_graph.get_tensor_by_name('text_length:0')\r\n        summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\r\n        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\r\n        #Multiply by batch_size to match the model's input parameters\r\n        for i, text in enumerate(texts):\r\n            generagte_summary_length = generagte_summary_length_list[i]\r\n            answer_logits = sess.run(logits, {input_data: [text]*batch_size, \r\n                                              summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \r\n                                              text_length: [len(text)]*batch_size,\r\n                                              keep_prob: 1.0})[0] \r\n            # Remove the padding from the summaries\r\n            pad = vocab_to_int[\"<PAD>\"] \r\n            print('- Review:\\n\\r {}'.format(input_sentences[i]))\r\n            print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i[0]] for i in answer_logits if i[0] != pad])))\r\ntensorflow\r\n"}