{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/236572459", "html_url": "https://github.com/tensorflow/tensorflow/issues/23#issuecomment-236572459", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23", "id": 236572459, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNjU3MjQ1OQ==", "user": {"login": "JinXinDeep", "id": 16409463, "node_id": "MDQ6VXNlcjE2NDA5NDYz", "avatar_url": "https://avatars3.githubusercontent.com/u/16409463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JinXinDeep", "html_url": "https://github.com/JinXinDeep", "followers_url": "https://api.github.com/users/JinXinDeep/followers", "following_url": "https://api.github.com/users/JinXinDeep/following{/other_user}", "gists_url": "https://api.github.com/users/JinXinDeep/gists{/gist_id}", "starred_url": "https://api.github.com/users/JinXinDeep/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JinXinDeep/subscriptions", "organizations_url": "https://api.github.com/users/JinXinDeep/orgs", "repos_url": "https://api.github.com/users/JinXinDeep/repos", "events_url": "https://api.github.com/users/JinXinDeep/events{/privacy}", "received_events_url": "https://api.github.com/users/JinXinDeep/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-01T12:54:50Z", "updated_at": "2016-08-01T12:54:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>  Thanks for the distributed version of TensorFlow since v0.8. In my opinion, TensorFlow is very flexible, it can do model parallel, data parallel, or mixed parallel, although the examples are for data parallel.</p>\n<p>For example, for model parallel, typically there is a cluster consists of a number of distributed nodes (e.g. machines) for model training. If we use the <strong>In-graph</strong> mode, a main program can be used to define the tasks for all the nodes. To reduce communication overhead and to do model parallel efficiently, each node includes a parameter server (ps) task contains the sub-model parameters and a worker task that corresponding to the computations of the sub-model; each node contains a different sub-model, which is assigned by the main program, and these sub-models collectively form the whole model.<br>\nEach ps task from each node will receive the computation results from at least one node\u2019s worker in the cluster. In model parallel, for example, for each node, its worker task typically do the compution corresponding to its ps task\u2019s sub-model; the ps task in each node will update its sub-model according to the training results it received.</p>\n<p>Is that right? Thanks!</p>", "body_text": "@mrry  Thanks for the distributed version of TensorFlow since v0.8. In my opinion, TensorFlow is very flexible, it can do model parallel, data parallel, or mixed parallel, although the examples are for data parallel.\nFor example, for model parallel, typically there is a cluster consists of a number of distributed nodes (e.g. machines) for model training. If we use the In-graph mode, a main program can be used to define the tasks for all the nodes. To reduce communication overhead and to do model parallel efficiently, each node includes a parameter server (ps) task contains the sub-model parameters and a worker task that corresponding to the computations of the sub-model; each node contains a different sub-model, which is assigned by the main program, and these sub-models collectively form the whole model.\nEach ps task from each node will receive the computation results from at least one node\u2019s worker in the cluster. In model parallel, for example, for each node, its worker task typically do the compution corresponding to its ps task\u2019s sub-model; the ps task in each node will update its sub-model according to the training results it received.\nIs that right? Thanks!", "body": "@mrry  Thanks for the distributed version of TensorFlow since v0.8. In my opinion, TensorFlow is very flexible, it can do model parallel, data parallel, or mixed parallel, although the examples are for data parallel. \n\nFor example, for model parallel, typically there is a cluster consists of a number of distributed nodes (e.g. machines) for model training. If we use the **In-graph** mode, a main program can be used to define the tasks for all the nodes. To reduce communication overhead and to do model parallel efficiently, each node includes a parameter server (ps) task contains the sub-model parameters and a worker task that corresponding to the computations of the sub-model; each node contains a different sub-model, which is assigned by the main program, and these sub-models collectively form the whole model. \nEach ps task from each node will receive the computation results from at least one node\u2019s worker in the cluster. In model parallel, for example, for each node, its worker task typically do the compution corresponding to its ps task\u2019s sub-model; the ps task in each node will update its sub-model according to the training results it received.\n\nIs that right? Thanks!\n"}