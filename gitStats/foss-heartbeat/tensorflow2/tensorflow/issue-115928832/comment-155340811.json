{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155340811", "html_url": "https://github.com/tensorflow/tensorflow/issues/23#issuecomment-155340811", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23", "id": 155340811, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTM0MDgxMQ==", "user": {"login": "edwardyoon", "id": 189909, "node_id": "MDQ6VXNlcjE4OTkwOQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/189909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edwardyoon", "html_url": "https://github.com/edwardyoon", "followers_url": "https://api.github.com/users/edwardyoon/followers", "following_url": "https://api.github.com/users/edwardyoon/following{/other_user}", "gists_url": "https://api.github.com/users/edwardyoon/gists{/gist_id}", "starred_url": "https://api.github.com/users/edwardyoon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edwardyoon/subscriptions", "organizations_url": "https://api.github.com/users/edwardyoon/orgs", "repos_url": "https://api.github.com/users/edwardyoon/repos", "events_url": "https://api.github.com/users/edwardyoon/events{/privacy}", "received_events_url": "https://api.github.com/users/edwardyoon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-10T07:19:26Z", "updated_at": "2015-11-10T07:19:26Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>After reading these plans and ideas, I'm somewhat surprised. According to <a href=\"http://static.googleusercontent.com/media/research.google.com/en//people/jeff/BayLearn2015.pdf\" rel=\"nofollow\">http://static.googleusercontent.com/media/research.google.com/en//people/jeff/BayLearn2015.pdf</a>, both data and model parallel are needed to train large and powerful models quickly. BTW, GPUs transferring data takes time as described in <a href=\"http://tensorflow.org/tutorials/deep_cnn/index.md\" rel=\"nofollow\">http://tensorflow.org/tutorials/deep_cnn/index.md</a>. Then, how it's possible to efficiently support both model parallelism and heterogeneous multi-devices (of a single node) on distributed cluster? Could you please roughly explain how different it from DistBelief?</p>\n<p>Thanks!</p>", "body_text": "Hello,\nAfter reading these plans and ideas, I'm somewhat surprised. According to http://static.googleusercontent.com/media/research.google.com/en//people/jeff/BayLearn2015.pdf, both data and model parallel are needed to train large and powerful models quickly. BTW, GPUs transferring data takes time as described in http://tensorflow.org/tutorials/deep_cnn/index.md. Then, how it's possible to efficiently support both model parallelism and heterogeneous multi-devices (of a single node) on distributed cluster? Could you please roughly explain how different it from DistBelief?\nThanks!", "body": "Hello,\n\nAfter reading these plans and ideas, I'm somewhat surprised. According to http://static.googleusercontent.com/media/research.google.com/en//people/jeff/BayLearn2015.pdf, both data and model parallel are needed to train large and powerful models quickly. BTW, GPUs transferring data takes time as described in http://tensorflow.org/tutorials/deep_cnn/index.md. Then, how it's possible to efficiently support both model parallelism and heterogeneous multi-devices (of a single node) on distributed cluster? Could you please roughly explain how different it from DistBelief?\n\nThanks!\n"}