{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/440018695", "html_url": "https://github.com/tensorflow/tensorflow/issues/15805#issuecomment-440018695", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15805", "id": 440018695, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDAxODY5NQ==", "user": {"login": "lauriebyrum", "id": 4791172, "node_id": "MDQ6VXNlcjQ3OTExNzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/4791172?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lauriebyrum", "html_url": "https://github.com/lauriebyrum", "followers_url": "https://api.github.com/users/lauriebyrum/followers", "following_url": "https://api.github.com/users/lauriebyrum/following{/other_user}", "gists_url": "https://api.github.com/users/lauriebyrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/lauriebyrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lauriebyrum/subscriptions", "organizations_url": "https://api.github.com/users/lauriebyrum/orgs", "repos_url": "https://api.github.com/users/lauriebyrum/repos", "events_url": "https://api.github.com/users/lauriebyrum/events{/privacy}", "received_events_url": "https://api.github.com/users/lauriebyrum/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-19T19:46:04Z", "updated_at": "2018-11-19T19:46:04Z", "author_association": "NONE", "body_html": "<p>By switching from using dynamic-rnn to using an unrolled rnn using LSTMs (I know LSTM isn't formally supported, but it does exist), I was able to produce and run a tflite model on android and ios. However, the model is extremely huge (&gt;400MB on disk) and extremely slow (&gt;5s). If I remove the rnn portion of my model (it is a custom model containing more than just the rnn portion), the model size drops to 13MB and runs in 2s. So this substitution with an unrolled rnn is not a viable answer. The same model is 26MB/0.8s when I run with a dynamic-rnn in tensorflow mobile on the same devices. My point is this: +1 to guidance and samples on how we can use RNNs with TFLite</p>", "body_text": "By switching from using dynamic-rnn to using an unrolled rnn using LSTMs (I know LSTM isn't formally supported, but it does exist), I was able to produce and run a tflite model on android and ios. However, the model is extremely huge (>400MB on disk) and extremely slow (>5s). If I remove the rnn portion of my model (it is a custom model containing more than just the rnn portion), the model size drops to 13MB and runs in 2s. So this substitution with an unrolled rnn is not a viable answer. The same model is 26MB/0.8s when I run with a dynamic-rnn in tensorflow mobile on the same devices. My point is this: +1 to guidance and samples on how we can use RNNs with TFLite", "body": "By switching from using dynamic-rnn to using an unrolled rnn using LSTMs (I know LSTM isn't formally supported, but it does exist), I was able to produce and run a tflite model on android and ios. However, the model is extremely huge (>400MB on disk) and extremely slow (>5s). If I remove the rnn portion of my model (it is a custom model containing more than just the rnn portion), the model size drops to 13MB and runs in 2s. So this substitution with an unrolled rnn is not a viable answer. The same model is 26MB/0.8s when I run with a dynamic-rnn in tensorflow mobile on the same devices. My point is this: +1 to guidance and samples on how we can use RNNs with TFLite"}