{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15805", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15805/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15805/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15805/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15805", "id": 285595483, "node_id": "MDU6SXNzdWUyODU1OTU0ODM=", "number": 15805, "title": "Unable to convert LSTM model to .tflite model", "user": {"login": "SullyChen", "id": 11732807, "node_id": "MDQ6VXNlcjExNzMyODA3", "avatar_url": "https://avatars1.githubusercontent.com/u/11732807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SullyChen", "html_url": "https://github.com/SullyChen", "followers_url": "https://api.github.com/users/SullyChen/followers", "following_url": "https://api.github.com/users/SullyChen/following{/other_user}", "gists_url": "https://api.github.com/users/SullyChen/gists{/gist_id}", "starred_url": "https://api.github.com/users/SullyChen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SullyChen/subscriptions", "organizations_url": "https://api.github.com/users/SullyChen/orgs", "repos_url": "https://api.github.com/users/SullyChen/repos", "events_url": "https://api.github.com/users/SullyChen/events{/privacy}", "received_events_url": "https://api.github.com/users/SullyChen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 36, "created_at": "2018-01-03T04:08:53Z", "updated_at": "2018-11-19T19:46:04Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS High Sierra 10.13.2</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.3.0-rc2-20-g0787eee', '1.3.0')</li>\n<li><strong>Python version</strong>: 2.7.13</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A, using CPU only</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\\n  --input_file=\"$(pwd)/lstm-model.pb\" \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT \\\n  --input_type=FLOAT --input_arrays=input \\\n  --output_arrays=output --input_shapes=28,28\n</code></pre>\n<h3>The Issue</h3>\n<p>When trying to convert an LSTM from a frozen graph (.pb) file to (.tflite) using the tensorflow toco script, I get unsupported operations error.</p>\n<h3>Source code / logs</h3>\n<p>This is the source code for the mode:</p>\n<pre><code>'''\nEdited code from https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/\n'''\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n#import mnist dataset\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\n\n#define constants\n#unrolled through 28 time steps\ntime_steps=28\n#hidden LSTM units\nnum_units=128\n#rows of 28 pixels\nn_input=28\n#learning rate for adam\nlearning_rate=0.001\n#mnist is meant to be classified in 10 classes(0-9).\nn_classes=10\n#size of batch\nbatch_size=128\n\n#weights and biases of appropriate shape to accomplish above task\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\nout_bias=tf.Variable(tf.random_normal([n_classes]))\n\n#defining placeholders\n#input image placeholder\nx=tf.placeholder(\"float\",[None,time_steps,n_input],name=\"input\")\n#input label placeholder\ny=tf.placeholder(\"float\",[None,n_classes])\n\n#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\ninput=tf.unstack(x ,time_steps,1)\n\n#defining the network\n#cell = rnn.BasicLSTMCell(num_units,forget_bias=0)\nlstm_layer = tf.nn.rnn_cell.MultiRNNCell([rnn.BasicLSTMCell(num_units) for _ in range(3)])\noutputs, _ = rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n\n#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\nprediction=tf.matmul(outputs[-1],out_weights,name=\"output\")+out_bias\n\n#loss_function\nloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n#optimization\nopt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n#model evaluation\ncorrect_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n#initialize variables\ninit=tf.global_variables_initializer()\n\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    iter=1\n    while iter&lt;800:\n        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n\n        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n\n        if iter %10==0:\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n            print(\"For iter \",iter)\n            print(\"Accuracy \",acc)\n            print(\"Loss \",los)\n            print(\"__________________\")\n\n        filename = saver.save(sess, \"model/model.ckpt\")\n\n        iter=iter+1\n\n#calculating test accuracy\ntest_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))\ntest_label = mnist.test.labels[:128]\nprint(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n</code></pre>\n<p>This is code I used for freezing the graph:</p>\n<pre><code>'''\nCode from https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\n'''\n\nimport os, argparse\n\nimport tensorflow as tf\n\n# The original freeze_graph function\n# from tensorflow.python.tools.freeze_graph import freeze_graph\n\ndir = os.path.dirname(os.path.realpath(__file__))\n\ndef freeze_graph(model_dir, output_node_names):\n    \"\"\"Extract the sub graph defined by the output nodes and convert\n    all its variables into constant\n    Args:\n        model_dir: the root folder containing the checkpoint state file\n        output_node_names: a string, containing all the output node's names,\n                            comma separated\n    \"\"\"\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            \"Export directory doesn't exists. Please specify an export \"\n            \"directory: %s\" % model_dir)\n\n    if not output_node_names:\n        print(\"You need to supply the name of a node to --output_node_names.\")\n        return -1\n\n    # We retrieve our checkpoint fullpath\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    # We precise the file fullname of our freezed graph\n    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n\n    # We clear devices to allow TensorFlow to control on which device it will load operations\n    clear_devices = True\n\n    # We start a session using a temporary fresh Graph\n    with tf.Session(graph=tf.Graph()) as sess:\n        # We import the meta graph in the current default Graph\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n\n        # We restore the weights\n        saver.restore(sess, input_checkpoint)\n\n        # We use a built-in TF helper to export variables to constants\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess, # The session is used to retrieve the weights\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes\n            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n        )\n\n        # Finally we serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\n            f.write(output_graph_def.SerializeToString())\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n\n    return output_graph_def\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_dir\", type=str, default=\"\", help=\"Model folder to export\")\n    parser.add_argument(\"--output_node_names\", type=str, default=\"\", help=\"The name of the output nodes, comma separated.\")\n    args = parser.parse_args()\n\n    freeze_graph(args.model_dir, args.output_node_names)\n</code></pre>\n<p>This is the output of the toco command:</p>\n<pre><code>2018-01-02 20:05:24.912921: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\n2018-01-02 20:05:24.973744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: Unpack\n2018-01-02 20:05:24.974315: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: StridedSlice\n2018-01-02 20:05:25.041459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1209 operators, 1775 arrays (0 quantized)\n2018-01-02 20:05:25.118862: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1114 operators, 1672 arrays (0 quantized)\n2018-01-02 20:05:25.176555: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1114 operators, 1672 arrays (0 quantized)\n2018-01-02 20:05:25.208552: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\n2018-01-02 20:05:25.234811: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Fill, SPLIT, StridedSlice, TensorFlowShape, Unpack.\npbtotflite.sh: line 8:  8277 Abort trap: 6           ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=\"$(pwd)/lstm-model.pb\" --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=28,28\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.2\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): ('v1.3.0-rc2-20-g0787eee', '1.3.0')\nPython version: 2.7.13\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A, using CPU only\nGPU model and memory: N/A\nExact command to reproduce:\n\n~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\\n  --input_file=\"$(pwd)/lstm-model.pb\" \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT \\\n  --input_type=FLOAT --input_arrays=input \\\n  --output_arrays=output --input_shapes=28,28\n\nThe Issue\nWhen trying to convert an LSTM from a frozen graph (.pb) file to (.tflite) using the tensorflow toco script, I get unsupported operations error.\nSource code / logs\nThis is the source code for the mode:\n'''\nEdited code from https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/\n'''\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n#import mnist dataset\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\n\n#define constants\n#unrolled through 28 time steps\ntime_steps=28\n#hidden LSTM units\nnum_units=128\n#rows of 28 pixels\nn_input=28\n#learning rate for adam\nlearning_rate=0.001\n#mnist is meant to be classified in 10 classes(0-9).\nn_classes=10\n#size of batch\nbatch_size=128\n\n#weights and biases of appropriate shape to accomplish above task\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\nout_bias=tf.Variable(tf.random_normal([n_classes]))\n\n#defining placeholders\n#input image placeholder\nx=tf.placeholder(\"float\",[None,time_steps,n_input],name=\"input\")\n#input label placeholder\ny=tf.placeholder(\"float\",[None,n_classes])\n\n#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\ninput=tf.unstack(x ,time_steps,1)\n\n#defining the network\n#cell = rnn.BasicLSTMCell(num_units,forget_bias=0)\nlstm_layer = tf.nn.rnn_cell.MultiRNNCell([rnn.BasicLSTMCell(num_units) for _ in range(3)])\noutputs, _ = rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n\n#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\nprediction=tf.matmul(outputs[-1],out_weights,name=\"output\")+out_bias\n\n#loss_function\nloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n#optimization\nopt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n#model evaluation\ncorrect_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n#initialize variables\ninit=tf.global_variables_initializer()\n\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    iter=1\n    while iter<800:\n        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n\n        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n\n        if iter %10==0:\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n            print(\"For iter \",iter)\n            print(\"Accuracy \",acc)\n            print(\"Loss \",los)\n            print(\"__________________\")\n\n        filename = saver.save(sess, \"model/model.ckpt\")\n\n        iter=iter+1\n\n#calculating test accuracy\ntest_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))\ntest_label = mnist.test.labels[:128]\nprint(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n\nThis is code I used for freezing the graph:\n'''\nCode from https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\n'''\n\nimport os, argparse\n\nimport tensorflow as tf\n\n# The original freeze_graph function\n# from tensorflow.python.tools.freeze_graph import freeze_graph\n\ndir = os.path.dirname(os.path.realpath(__file__))\n\ndef freeze_graph(model_dir, output_node_names):\n    \"\"\"Extract the sub graph defined by the output nodes and convert\n    all its variables into constant\n    Args:\n        model_dir: the root folder containing the checkpoint state file\n        output_node_names: a string, containing all the output node's names,\n                            comma separated\n    \"\"\"\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            \"Export directory doesn't exists. Please specify an export \"\n            \"directory: %s\" % model_dir)\n\n    if not output_node_names:\n        print(\"You need to supply the name of a node to --output_node_names.\")\n        return -1\n\n    # We retrieve our checkpoint fullpath\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    # We precise the file fullname of our freezed graph\n    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n\n    # We clear devices to allow TensorFlow to control on which device it will load operations\n    clear_devices = True\n\n    # We start a session using a temporary fresh Graph\n    with tf.Session(graph=tf.Graph()) as sess:\n        # We import the meta graph in the current default Graph\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n\n        # We restore the weights\n        saver.restore(sess, input_checkpoint)\n\n        # We use a built-in TF helper to export variables to constants\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess, # The session is used to retrieve the weights\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes\n            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n        )\n\n        # Finally we serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\n            f.write(output_graph_def.SerializeToString())\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n\n    return output_graph_def\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_dir\", type=str, default=\"\", help=\"Model folder to export\")\n    parser.add_argument(\"--output_node_names\", type=str, default=\"\", help=\"The name of the output nodes, comma separated.\")\n    args = parser.parse_args()\n\n    freeze_graph(args.model_dir, args.output_node_names)\n\nThis is the output of the toco command:\n2018-01-02 20:05:24.912921: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\n2018-01-02 20:05:24.973744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: Unpack\n2018-01-02 20:05:24.974315: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: StridedSlice\n2018-01-02 20:05:25.041459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1209 operators, 1775 arrays (0 quantized)\n2018-01-02 20:05:25.118862: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1114 operators, 1672 arrays (0 quantized)\n2018-01-02 20:05:25.176555: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1114 operators, 1672 arrays (0 quantized)\n2018-01-02 20:05:25.208552: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\n2018-01-02 20:05:25.234811: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Fill, SPLIT, StridedSlice, TensorFlowShape, Unpack.\npbtotflite.sh: line 8:  8277 Abort trap: 6           ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=\"$(pwd)/lstm-model.pb\" --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=28,28", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.2\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A, using CPU only\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n```\r\n~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=\"$(pwd)/lstm-model.pb\" \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --output_arrays=output --input_shapes=28,28\r\n```\r\n### The Issue\r\nWhen trying to convert an LSTM from a frozen graph (.pb) file to (.tflite) using the tensorflow toco script, I get unsupported operations error.\r\n\r\n### Source code / logs\r\nThis is the source code for the mode:\r\n```\r\n'''\r\nEdited code from https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/\r\n'''\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import rnn\r\n\r\n#import mnist dataset\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\r\n\r\n#define constants\r\n#unrolled through 28 time steps\r\ntime_steps=28\r\n#hidden LSTM units\r\nnum_units=128\r\n#rows of 28 pixels\r\nn_input=28\r\n#learning rate for adam\r\nlearning_rate=0.001\r\n#mnist is meant to be classified in 10 classes(0-9).\r\nn_classes=10\r\n#size of batch\r\nbatch_size=128\r\n\r\n#weights and biases of appropriate shape to accomplish above task\r\nout_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\r\nout_bias=tf.Variable(tf.random_normal([n_classes]))\r\n\r\n#defining placeholders\r\n#input image placeholder\r\nx=tf.placeholder(\"float\",[None,time_steps,n_input],name=\"input\")\r\n#input label placeholder\r\ny=tf.placeholder(\"float\",[None,n_classes])\r\n\r\n#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\r\ninput=tf.unstack(x ,time_steps,1)\r\n\r\n#defining the network\r\n#cell = rnn.BasicLSTMCell(num_units,forget_bias=0)\r\nlstm_layer = tf.nn.rnn_cell.MultiRNNCell([rnn.BasicLSTMCell(num_units) for _ in range(3)])\r\noutputs, _ = rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\r\n\r\n#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\r\nprediction=tf.matmul(outputs[-1],out_weights,name=\"output\")+out_bias\r\n\r\n#loss_function\r\nloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\r\n#optimization\r\nopt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n\r\n#model evaluation\r\ncorrect_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n#initialize variables\r\ninit=tf.global_variables_initializer()\r\n\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    iter=1\r\n    while iter<800:\r\n        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\r\n\r\n        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\r\n\r\n        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\r\n\r\n        if iter %10==0:\r\n            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\r\n            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\r\n            print(\"For iter \",iter)\r\n            print(\"Accuracy \",acc)\r\n            print(\"Loss \",los)\r\n            print(\"__________________\")\r\n\r\n        filename = saver.save(sess, \"model/model.ckpt\")\r\n\r\n        iter=iter+1\r\n\r\n#calculating test accuracy\r\ntest_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))\r\ntest_label = mnist.test.labels[:128]\r\nprint(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\r\n```\r\nThis is code I used for freezing the graph:\r\n```\r\n'''\r\nCode from https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\r\n'''\r\n\r\nimport os, argparse\r\n\r\nimport tensorflow as tf\r\n\r\n# The original freeze_graph function\r\n# from tensorflow.python.tools.freeze_graph import freeze_graph\r\n\r\ndir = os.path.dirname(os.path.realpath(__file__))\r\n\r\ndef freeze_graph(model_dir, output_node_names):\r\n    \"\"\"Extract the sub graph defined by the output nodes and convert\r\n    all its variables into constant\r\n    Args:\r\n        model_dir: the root folder containing the checkpoint state file\r\n        output_node_names: a string, containing all the output node's names,\r\n                            comma separated\r\n    \"\"\"\r\n    if not tf.gfile.Exists(model_dir):\r\n        raise AssertionError(\r\n            \"Export directory doesn't exists. Please specify an export \"\r\n            \"directory: %s\" % model_dir)\r\n\r\n    if not output_node_names:\r\n        print(\"You need to supply the name of a node to --output_node_names.\")\r\n        return -1\r\n\r\n    # We retrieve our checkpoint fullpath\r\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\r\n    input_checkpoint = checkpoint.model_checkpoint_path\r\n\r\n    # We precise the file fullname of our freezed graph\r\n    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\r\n    output_graph = absolute_model_dir + \"/frozen_model.pb\"\r\n\r\n    # We clear devices to allow TensorFlow to control on which device it will load operations\r\n    clear_devices = True\r\n\r\n    # We start a session using a temporary fresh Graph\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        # We import the meta graph in the current default Graph\r\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n        # We restore the weights\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        # We use a built-in TF helper to export variables to constants\r\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, # The session is used to retrieve the weights\r\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes\r\n            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n        )\r\n\r\n        # Finally we serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n    return output_graph_def\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_dir\", type=str, default=\"\", help=\"Model folder to export\")\r\n    parser.add_argument(\"--output_node_names\", type=str, default=\"\", help=\"The name of the output nodes, comma separated.\")\r\n    args = parser.parse_args()\r\n\r\n    freeze_graph(args.model_dir, args.output_node_names)\r\n```\r\n\r\nThis is the output of the toco command:\r\n```\r\n2018-01-02 20:05:24.912921: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\r\n2018-01-02 20:05:24.973744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: Unpack\r\n2018-01-02 20:05:24.974315: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: StridedSlice\r\n2018-01-02 20:05:25.041459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1209 operators, 1775 arrays (0 quantized)\r\n2018-01-02 20:05:25.118862: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1114 operators, 1672 arrays (0 quantized)\r\n2018-01-02 20:05:25.176555: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1114 operators, 1672 arrays (0 quantized)\r\n2018-01-02 20:05:25.208552: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2018-01-02 20:05:25.234811: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Fill, SPLIT, StridedSlice, TensorFlowShape, Unpack.\r\npbtotflite.sh: line 8:  8277 Abort trap: 6           ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=\"$(pwd)/lstm-model.pb\" --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=\"$(pwd)/lstm-model.tflite\" --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=28,28\r\n```"}