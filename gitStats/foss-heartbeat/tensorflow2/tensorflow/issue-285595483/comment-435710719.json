{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435710719", "html_url": "https://github.com/tensorflow/tensorflow/issues/15805#issuecomment-435710719", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15805", "id": 435710719, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTcxMDcxOQ==", "user": {"login": "hhxxttxsh", "id": 19598790, "node_id": "MDQ6VXNlcjE5NTk4Nzkw", "avatar_url": "https://avatars2.githubusercontent.com/u/19598790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hhxxttxsh", "html_url": "https://github.com/hhxxttxsh", "followers_url": "https://api.github.com/users/hhxxttxsh/followers", "following_url": "https://api.github.com/users/hhxxttxsh/following{/other_user}", "gists_url": "https://api.github.com/users/hhxxttxsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/hhxxttxsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hhxxttxsh/subscriptions", "organizations_url": "https://api.github.com/users/hhxxttxsh/orgs", "repos_url": "https://api.github.com/users/hhxxttxsh/repos", "events_url": "https://api.github.com/users/hhxxttxsh/events{/privacy}", "received_events_url": "https://api.github.com/users/hhxxttxsh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-04T21:44:13Z", "updated_at": "2018-11-04T21:45:25Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>We're planning to write some tutorials on RNN conversion. This approach does still include an unsupported unstack. I think you'd probably want to make a function that generates either a training graph that uses a dynamic rnn and a inference graph that does a 1 time step static_rnn. Then your tflite Invoke() call feeds one sequence item at a time.</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a></p>\n<ol>\n<li>Do we have the RNN conversion tutorial anywhere now?</li>\n<li>Your answer means that we need to get rid of control flow from the inference graph in order to make it work in TFLite, right? The efficiency might drop a lot when we use Invoke() in Python layer for handling variable length of the input, right?</li>\n<li>Is it possible to quantize the static_rnn graph to 8-bit in TFLite?</li>\n</ol>", "body_text": "We're planning to write some tutorials on RNN conversion. This approach does still include an unsupported unstack. I think you'd probably want to make a function that generates either a training graph that uses a dynamic rnn and a inference graph that does a 1 time step static_rnn. Then your tflite Invoke() call feeds one sequence item at a time.\n\n@aselle\n\nDo we have the RNN conversion tutorial anywhere now?\nYour answer means that we need to get rid of control flow from the inference graph in order to make it work in TFLite, right? The efficiency might drop a lot when we use Invoke() in Python layer for handling variable length of the input, right?\nIs it possible to quantize the static_rnn graph to 8-bit in TFLite?", "body": "> We're planning to write some tutorials on RNN conversion. This approach does still include an unsupported unstack. I think you'd probably want to make a function that generates either a training graph that uses a dynamic rnn and a inference graph that does a 1 time step static_rnn. Then your tflite Invoke() call feeds one sequence item at a time.\r\n\r\n@aselle \r\n1. Do we have the RNN conversion tutorial anywhere now? \r\n2. Your answer means that we need to get rid of control flow from the inference graph in order to make it work in TFLite, right? The efficiency might drop a lot when we use Invoke() in Python layer for handling variable length of the input, right?\r\n3. Is it possible to quantize the static_rnn graph to 8-bit in TFLite?\r\n"}