{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/334247305", "html_url": "https://github.com/tensorflow/tensorflow/issues/13101#issuecomment-334247305", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13101", "id": 334247305, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDI0NzMwNQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-04T18:26:59Z", "updated_at": "2017-10-04T18:26:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi folks, thanks for the feedback. I started with a pretty conservative interface and implementation for <code>Dataset.from_generator()</code>, so we should be able to evolve it towards a better state. Responding to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=966348\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tillahoffmann\">@tillahoffmann</a>'s and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1365079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Utumno\">@Utumno</a>'s  individual points:</p>\n<blockquote>\n<p>In addition to having generator be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable?</p>\n</blockquote>\n<p>Yes, we could do this, but I wasn't able to figure out a sensible set of type checks to make this work smoothly. There are also some annoying wrinkles: e.g. a generator expression is a Python iterator so you can only iterate over it once, unless you wrap it in a callable. Of course, this only matters if you try to <code>.repeat()</code> the resulting dataset! I think the workaround (explicit wrapping) is better than the confusion that could arise here, but I'm open to better suggestions... perhaps these would involve better static information about whether it's possible to repeat a dataset?</p>\n<blockquote>\n<p><code>from_generator</code> does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have.</p>\n</blockquote>\n<p>I suppose we could extend the <code>nest</code> library so that it would delve into NumPy arrays <a href=\"http://google3/third_party/tensorflow/python/data/ops/dataset_ops.py?l=309&amp;rcl=170909071\" rel=\"nofollow\">when it does <code>nest.flatten_up_to(output_types, values)</code></a> on the elements yielded from the generator. It doesn't seem incompatible with how we already use it, but I'm not sure what it would take. Let's call that \"contributions welcome\". :)</p>\n<blockquote>\n<p>I've played around with a <em>very</em> naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using <code>feed_dicts</code>.</p>\n</blockquote>\n<p>This is a bit surprising, and we'll need to do some more investigation. It's possible that there's some overhead from acquiring and releasing the GIL in <code>tf.py_func()</code> and I don't think it's been heavily optimized. One quick question though: is the steady-state performance 7x worse, or does the difference in mean shrink if you exclude the first step when the iterator is constructed? (That's not an excuse of course, but it would help to focus our efforts!)</p>\n<blockquote>\n<p>I have been trying to use the Dataset.from_generator API (thanks) to return an iterator to another dataset, constructed in the generator - but I can't really make it work without passing the session in[.]</p>\n</blockquote>\n<p>This is not really the expected use for <code>Dataset.from_generator()</code> and as <a href=\"https://stackoverflow.com/a/46557087/3574081\" rel=\"nofollow\">we discussed on Stack Overflow</a>, you can use <code>Dataset.map()</code> if you need to capture a different iterator. However, I'm not quite sure what the use case is here... why don't you want to use the original iterator directly?</p>\n<blockquote>\n<p>I erroneously wrote <code>Dataset.zip((Dataset.from_tensors(...), tf.range(count))</code> and I got the rather cryptic <code>errors_impl.UnknownError: AttributeError: 'Tensor' object has no attribute 'output_types'</code>.</p>\n</blockquote>\n<p>I coincidentally fixed this in <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/3c00952c6680d77ee2f10def35fbc7cbd138aea3/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/3c00952c6680d77ee2f10def35fbc7cbd138aea3\"><tt>3c00952</tt></a>.</p>\n<p>Thanks again for using the new API, and let me know if you have more comments or thoughts on these answers!</p>", "body_text": "Hi folks, thanks for the feedback. I started with a pretty conservative interface and implementation for Dataset.from_generator(), so we should be able to evolve it towards a better state. Responding to @tillahoffmann's and @Utumno's  individual points:\n\nIn addition to having generator be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable?\n\nYes, we could do this, but I wasn't able to figure out a sensible set of type checks to make this work smoothly. There are also some annoying wrinkles: e.g. a generator expression is a Python iterator so you can only iterate over it once, unless you wrap it in a callable. Of course, this only matters if you try to .repeat() the resulting dataset! I think the workaround (explicit wrapping) is better than the confusion that could arise here, but I'm open to better suggestions... perhaps these would involve better static information about whether it's possible to repeat a dataset?\n\nfrom_generator does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have.\n\nI suppose we could extend the nest library so that it would delve into NumPy arrays when it does nest.flatten_up_to(output_types, values) on the elements yielded from the generator. It doesn't seem incompatible with how we already use it, but I'm not sure what it would take. Let's call that \"contributions welcome\". :)\n\nI've played around with a very naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using feed_dicts.\n\nThis is a bit surprising, and we'll need to do some more investigation. It's possible that there's some overhead from acquiring and releasing the GIL in tf.py_func() and I don't think it's been heavily optimized. One quick question though: is the steady-state performance 7x worse, or does the difference in mean shrink if you exclude the first step when the iterator is constructed? (That's not an excuse of course, but it would help to focus our efforts!)\n\nI have been trying to use the Dataset.from_generator API (thanks) to return an iterator to another dataset, constructed in the generator - but I can't really make it work without passing the session in[.]\n\nThis is not really the expected use for Dataset.from_generator() and as we discussed on Stack Overflow, you can use Dataset.map() if you need to capture a different iterator. However, I'm not quite sure what the use case is here... why don't you want to use the original iterator directly?\n\nI erroneously wrote Dataset.zip((Dataset.from_tensors(...), tf.range(count)) and I got the rather cryptic errors_impl.UnknownError: AttributeError: 'Tensor' object has no attribute 'output_types'.\n\nI coincidentally fixed this in 3c00952.\nThanks again for using the new API, and let me know if you have more comments or thoughts on these answers!", "body": "Hi folks, thanks for the feedback. I started with a pretty conservative interface and implementation for `Dataset.from_generator()`, so we should be able to evolve it towards a better state. Responding to @tillahoffmann's and @Utumno's  individual points:\r\n\r\n> In addition to having generator be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable?\r\n\r\nYes, we could do this, but I wasn't able to figure out a sensible set of type checks to make this work smoothly. There are also some annoying wrinkles: e.g. a generator expression is a Python iterator so you can only iterate over it once, unless you wrap it in a callable. Of course, this only matters if you try to `.repeat()` the resulting dataset! I think the workaround (explicit wrapping) is better than the confusion that could arise here, but I'm open to better suggestions... perhaps these would involve better static information about whether it's possible to repeat a dataset?\r\n\r\n> `from_generator` does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have.\r\n\r\nI suppose we could extend the `nest` library so that it would delve into NumPy arrays [when it does `nest.flatten_up_to(output_types, values)`](http://google3/third_party/tensorflow/python/data/ops/dataset_ops.py?l=309&rcl=170909071) on the elements yielded from the generator. It doesn't seem incompatible with how we already use it, but I'm not sure what it would take. Let's call that \"contributions welcome\". :)\r\n\r\n> I've played around with a *very* naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using `feed_dicts`.\r\n\r\nThis is a bit surprising, and we'll need to do some more investigation. It's possible that there's some overhead from acquiring and releasing the GIL in `tf.py_func()` and I don't think it's been heavily optimized. One quick question though: is the steady-state performance 7x worse, or does the difference in mean shrink if you exclude the first step when the iterator is constructed? (That's not an excuse of course, but it would help to focus our efforts!) \r\n\r\n> I have been trying to use the Dataset.from_generator API (thanks) to return an iterator to another dataset, constructed in the generator - but I can't really make it work without passing the session in[.]\r\n\r\nThis is not really the expected use for `Dataset.from_generator()` and as [we discussed on Stack Overflow](https://stackoverflow.com/a/46557087/3574081), you can use `Dataset.map()` if you need to capture a different iterator. However, I'm not quite sure what the use case is here... why don't you want to use the original iterator directly?\r\n\r\n> I erroneously wrote `Dataset.zip((Dataset.from_tensors(...), tf.range(count))` and I got the rather cryptic `errors_impl.UnknownError: AttributeError: 'Tensor' object has no attribute 'output_types'`.\r\n\r\nI coincidentally fixed this in 3c00952c6680d77ee2f10def35fbc7cbd138aea3.\r\n\r\nThanks again for using the new API, and let me know if you have more comments or thoughts on these answers!"}