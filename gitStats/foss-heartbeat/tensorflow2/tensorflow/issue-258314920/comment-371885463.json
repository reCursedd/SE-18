{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/371885463", "html_url": "https://github.com/tensorflow/tensorflow/issues/13101#issuecomment-371885463", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13101", "id": 371885463, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTg4NTQ2Mw==", "user": {"login": "davidparks21", "id": 964997, "node_id": "MDQ6VXNlcjk2NDk5Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/964997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidparks21", "html_url": "https://github.com/davidparks21", "followers_url": "https://api.github.com/users/davidparks21/followers", "following_url": "https://api.github.com/users/davidparks21/following{/other_user}", "gists_url": "https://api.github.com/users/davidparks21/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidparks21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidparks21/subscriptions", "organizations_url": "https://api.github.com/users/davidparks21/orgs", "repos_url": "https://api.github.com/users/davidparks21/repos", "events_url": "https://api.github.com/users/davidparks21/events{/privacy}", "received_events_url": "https://api.github.com/users/davidparks21/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-09T17:38:04Z", "updated_at": "2018-03-09T17:38:04Z", "author_association": "NONE", "body_html": "<p>I had opened another issue (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"291049165\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/16343\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/16343/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/16343\">#16343</a>) which is also related to feedback of the generator process. I think it's beneficial to move those comments here and close that issue in order to consolidate the related discussions to one issue.</p>\n<hr>\n<p>We're given hundreds of data files, each containing many gigabytes worth of sample data in a custom format. As far as I can tell there are only two approaches to extract samples from this using <code>Dataset</code>:</p>\n<ol>\n<li><code>tf.data.Dataset.from_generator(generator=my_custom_reader, ...)</code></li>\n</ol>\n<p>Create a generator which produces samples. This approach is not ideal because this method must be the first dataset in the chain. The generator cannot accept a tensor. Therefore you can't batch and shuffle your list of 100's of filenames (or anything more complex). You also can't make use of <code>interleave(...)</code> because the generator can't accept a tensor, and this use case is begging to use <code>interleave(...)</code>.</p>\n<p>A solution here might be to provide a method for a generator to accept a tensor, as <code>tf.py_func(...)</code> does for functions.</p>\n<ol start=\"2\">\n<li><code>tf.data.Dataset.map(map_func=tf.py_func(my_custom_reader, ...), ...)</code></li>\n</ol>\n<p>The map function does allow us to shuffle and parallelize the filenames using all of the functionality of the Dataset pipeline, however, with <code>map</code>, the files must be read into memory completely, and these files are large. Reading numerous files into memory is infeasible.</p>\n<p>A solution here might be to extend the <code>map</code> function to support generators.</p>\n<p>Unless there's an alternative approach, which I didn't glean from the docs or stackoverflow, then this seems to be an inherent limitation and a seemingly reasonable use case on which to base a feature request.</p>\n<p>I've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.</p>\n<p><a href=\"https://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\" rel=\"nofollow\">https://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464</a></p>\n<h2>Update to this post:</h2>\n<p>The solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an <code>unsupported object type numpy.ndarray</code> error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef mypymap(data_numpy):\n  result = np.array([\n    np.array([100,101,102]),\n    np.array([200,201,202,203])\n  ])\n  return result\n\ndata_input = ['fileA','fileB','fileC']\n\nds = tf.data.Dataset.from_tensor_slices(data_input)\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\n\nelement = ds.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n  for _ in range(3):\n    print(sess.run(element))\n</code></pre>\n<p>So that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the <code>Dataset</code> pipeline.</p>", "body_text": "I had opened another issue (#16343) which is also related to feedback of the generator process. I think it's beneficial to move those comments here and close that issue in order to consolidate the related discussions to one issue.\n\nWe're given hundreds of data files, each containing many gigabytes worth of sample data in a custom format. As far as I can tell there are only two approaches to extract samples from this using Dataset:\n\ntf.data.Dataset.from_generator(generator=my_custom_reader, ...)\n\nCreate a generator which produces samples. This approach is not ideal because this method must be the first dataset in the chain. The generator cannot accept a tensor. Therefore you can't batch and shuffle your list of 100's of filenames (or anything more complex). You also can't make use of interleave(...) because the generator can't accept a tensor, and this use case is begging to use interleave(...).\nA solution here might be to provide a method for a generator to accept a tensor, as tf.py_func(...) does for functions.\n\ntf.data.Dataset.map(map_func=tf.py_func(my_custom_reader, ...), ...)\n\nThe map function does allow us to shuffle and parallelize the filenames using all of the functionality of the Dataset pipeline, however, with map, the files must be read into memory completely, and these files are large. Reading numerous files into memory is infeasible.\nA solution here might be to extend the map function to support generators.\nUnless there's an alternative approach, which I didn't glean from the docs or stackoverflow, then this seems to be an inherent limitation and a seemingly reasonable use case on which to base a feature request.\nI've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.\nhttps://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\nUpdate to this post:\nThe solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an unsupported object type numpy.ndarray error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file.\nimport tensorflow as tf\nimport numpy as np\n\ndef mypymap(data_numpy):\n  result = np.array([\n    np.array([100,101,102]),\n    np.array([200,201,202,203])\n  ])\n  return result\n\ndata_input = ['fileA','fileB','fileC']\n\nds = tf.data.Dataset.from_tensor_slices(data_input)\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\n\nelement = ds.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n  for _ in range(3):\n    print(sess.run(element))\n\nSo that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the Dataset pipeline.", "body": "I had opened another issue (https://github.com/tensorflow/tensorflow/issues/16343) which is also related to feedback of the generator process. I think it's beneficial to move those comments here and close that issue in order to consolidate the related discussions to one issue.\r\n\r\n---\r\n\r\nWe're given hundreds of data files, each containing many gigabytes worth of sample data in a custom format. As far as I can tell there are only two approaches to extract samples from this using `Dataset`:\r\n\r\n1) `tf.data.Dataset.from_generator(generator=my_custom_reader, ...)`\r\n\r\nCreate a generator which produces samples. This approach is not ideal because this method must be the first dataset in the chain. The generator cannot accept a tensor. Therefore you can't batch and shuffle your list of 100's of filenames (or anything more complex). You also can't make use of `interleave(...)` because the generator can't accept a tensor, and this use case is begging to use `interleave(...)`.\r\n\r\nA solution here might be to provide a method for a generator to accept a tensor, as `tf.py_func(...)` does for functions.\r\n\r\n2) `tf.data.Dataset.map(map_func=tf.py_func(my_custom_reader, ...), ...)`\r\n\r\nThe map function does allow us to shuffle and parallelize the filenames using all of the functionality of the Dataset pipeline, however, with `map`, the files must be read into memory completely, and these files are large. Reading numerous files into memory is infeasible.\r\n\r\nA solution here might be to extend the `map` function to support generators.\r\n\r\nUnless there's an alternative approach, which I didn't glean from the docs or stackoverflow, then this seems to be an inherent limitation and a seemingly reasonable use case on which to base a feature request.\r\n\r\nI've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.\r\n\r\nhttps://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\r\n\r\nUpdate to this post:\r\n---------------\r\nThe solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an `unsupported object type numpy.ndarray` error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef mypymap(data_numpy):\r\n  result = np.array([\r\n    np.array([100,101,102]),\r\n    np.array([200,201,202,203])\r\n  ])\r\n  return result\r\n\r\ndata_input = ['fileA','fileB','fileC']\r\n\r\nds = tf.data.Dataset.from_tensor_slices(data_input)\r\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\r\n\r\nelement = ds.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n  for _ in range(3):\r\n    print(sess.run(element))\r\n```\r\nSo that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the `Dataset` pipeline."}