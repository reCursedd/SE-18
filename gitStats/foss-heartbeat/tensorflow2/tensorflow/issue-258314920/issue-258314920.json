{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13101", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13101/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13101/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13101/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13101", "id": 258314920, "node_id": "MDU6SXNzdWUyNTgzMTQ5MjA=", "number": 13101, "title": "from_generator feedback and questions", "user": {"login": "tillahoffmann", "id": 966348, "node_id": "MDQ6VXNlcjk2NjM0OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/966348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tillahoffmann", "html_url": "https://github.com/tillahoffmann", "followers_url": "https://api.github.com/users/tillahoffmann/followers", "following_url": "https://api.github.com/users/tillahoffmann/following{/other_user}", "gists_url": "https://api.github.com/users/tillahoffmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/tillahoffmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tillahoffmann/subscriptions", "organizations_url": "https://api.github.com/users/tillahoffmann/orgs", "repos_url": "https://api.github.com/users/tillahoffmann/repos", "events_url": "https://api.github.com/users/tillahoffmann/events{/privacy}", "received_events_url": "https://api.github.com/users/tillahoffmann/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 47, "created_at": "2017-09-17T16:25:38Z", "updated_at": "2018-11-20T07:51:39Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>, thank you for implementing the <code>from_generator</code> method in <code>tf.data</code>. I just wanted to provide some feedback and ask a few more questions.</p>\n<h1>Interface</h1>\n<p>In addition to having <code>generator</code> be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable? E.g. instead of</p>\n<div class=\"highlight highlight-source-python\"><pre>pool <span class=\"pl-k\">=</span> multiprocessing.Pool()\ndataset <span class=\"pl-k\">=</span> tf.contrib.data.Dataset.from_generator(\n    <span class=\"pl-k\">lambda</span>: pool.imap(some_function, some_data), dtypes, shapes\n)</pre></div>\n<p>also support</p>\n<div class=\"highlight highlight-source-python\"><pre>pool <span class=\"pl-k\">=</span> multiprocessing.Pool()\ndataset <span class=\"pl-k\">=</span> tf.contrib.data.Dataset.from_generator(\n    pool.imap(some_function, some_data), dtypes, shapes\n)</pre></div>\n<h1>Return types</h1>\n<p><code>from_generator</code> does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have. E.g. this fails</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">generator</span>():\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        <span class=\"pl-k\">yield</span> np.zeros(<span class=\"pl-c1\">2</span>, np.float32)\n        \ndataset <span class=\"pl-k\">=</span> tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\nx, y <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator().get_next()\nsession <span class=\"pl-k\">=</span> tf.Session()\nsession.run([x, y])</pre></div>\n<p>but this runs smoothly</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">generator</span>():\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        <span class=\"pl-k\">yield</span> <span class=\"pl-c1\">tuple</span>(np.zeros(<span class=\"pl-c1\">2</span>, np.float32))\n        \ndataset <span class=\"pl-k\">=</span> tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\nx, y <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator().get_next()\nsession <span class=\"pl-k\">=</span> tf.Session()\nsession.run([x, y])</pre></div>\n<h1>Performance</h1>\n<p>I've played around with a <em>very</em> naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using <code>feed_dicts</code>. The setup is as follows</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib <span class=\"pl-k\">import</span> data <span class=\"pl-k\">as</span> tfdata\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> time <span class=\"pl-k\">import</span> time\n\nnum_batches <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Generator</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>.times <span class=\"pl-k\">=</span> []\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__iter__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            x <span class=\"pl-k\">=</span> np.random.normal()\n            y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">5</span> <span class=\"pl-k\">*</span> x\n            x, y <span class=\"pl-k\">=</span> np.asarray([x, y], np.float32)\n            <span class=\"pl-c1\">self</span>.times.append(time())\n            <span class=\"pl-k\">yield</span> x, y\n\ngenerator_state1 <span class=\"pl-k\">=</span> Generator()\n\ndataset <span class=\"pl-k\">=</span> tfdata.Dataset.from_generator(\n    <span class=\"pl-k\">lambda</span>: generator_state1, \n    (tf.float32, tf.float32),\n    (tf.TensorShape([]), tf.TensorShape([]))\n)\nprefetched <span class=\"pl-k\">=</span> dataset.prefetch(<span class=\"pl-c1\">3</span> <span class=\"pl-k\">*</span> batch_size)\nbatches <span class=\"pl-k\">=</span> prefetched.batch(batch_size)\niterator <span class=\"pl-k\">=</span> batches.make_one_shot_iterator()\n\nx, y <span class=\"pl-k\">=</span> iterator.get_next()\n\nw <span class=\"pl-k\">=</span> tf.Variable([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nprediction <span class=\"pl-k\">=</span> w[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span> w[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> x\nloss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(y, prediction)\noptimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.1</span>)\ntrain_op <span class=\"pl-k\">=</span> optimizer.minimize(loss)\ninit_op <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\nsession <span class=\"pl-k\">=</span> tf.Session()\nsession.run(init_op)</pre></div>\n<p>Running the optimisation gives me</p>\n<div class=\"highlight highlight-source-python\"><pre>losses <span class=\"pl-k\">=</span> []\n\nstart <span class=\"pl-k\">=</span> time()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_batches):\n    _, _loss <span class=\"pl-k\">=</span> session.run([train_op, loss])\n    losses.append(_loss)\ntime() <span class=\"pl-k\">-</span> start  <span class=\"pl-c\"><span class=\"pl-c\">#</span> about seven seconds</span></pre></div>\n<p>Doing the same using feed_dicts gives</p>\n<div class=\"highlight highlight-source-python\"><pre>losses <span class=\"pl-k\">=</span> []\n\ngenerator_state2 <span class=\"pl-k\">=</span> Generator()\niterator <span class=\"pl-k\">=</span> <span class=\"pl-c1\">iter</span>(generator_state2)\n\nstart <span class=\"pl-k\">=</span> time()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_batches):\n    _x, _y <span class=\"pl-k\">=</span> np.transpose([<span class=\"pl-c1\">next</span>(iterator) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_size)])\n    _, _loss <span class=\"pl-k\">=</span> session.run([train_op, loss], {x: _x, y: _y})\n    losses.append(_loss)\ntime() <span class=\"pl-k\">-</span> start  <span class=\"pl-c\"><span class=\"pl-c\">#</span> about one second</span></pre></div>\n<p>It seems that the dataset created using the <code>from_generator</code> method isn't fetching from the generator fast enough:</p>\n<div class=\"highlight highlight-source-python\"><pre>np.mean(np.diff(generator_state1.times))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 7.1533812683949508e-05</span>\nnp.mean(np.diff(generator_state2.times))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1.0633696558370612e-05</span></pre></div>\n<p>Thanks again for this, and looking forward to hearing your thoughts.</p>", "body_text": "@mrry, thank you for implementing the from_generator method in tf.data. I just wanted to provide some feedback and ask a few more questions.\nInterface\nIn addition to having generator be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable? E.g. instead of\npool = multiprocessing.Pool()\ndataset = tf.contrib.data.Dataset.from_generator(\n    lambda: pool.imap(some_function, some_data), dtypes, shapes\n)\nalso support\npool = multiprocessing.Pool()\ndataset = tf.contrib.data.Dataset.from_generator(\n    pool.imap(some_function, some_data), dtypes, shapes\n)\nReturn types\nfrom_generator does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have. E.g. this fails\ndef generator():\n    while True:\n        yield np.zeros(2, np.float32)\n        \ndataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\nx, y = dataset.make_one_shot_iterator().get_next()\nsession = tf.Session()\nsession.run([x, y])\nbut this runs smoothly\ndef generator():\n    while True:\n        yield tuple(np.zeros(2, np.float32))\n        \ndataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\nx, y = dataset.make_one_shot_iterator().get_next()\nsession = tf.Session()\nsession.run([x, y])\nPerformance\nI've played around with a very naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using feed_dicts. The setup is as follows\nimport tensorflow as tf\nfrom tensorflow.contrib import data as tfdata\nimport numpy as np\nfrom time import time\n\nnum_batches = 1000\nbatch_size = 100\n\nclass Generator:\n    def __init__(self):\n        self.times = []\n    \n    def __iter__(self):\n        while True:\n            x = np.random.normal()\n            y = 3 + 5 * x\n            x, y = np.asarray([x, y], np.float32)\n            self.times.append(time())\n            yield x, y\n\ngenerator_state1 = Generator()\n\ndataset = tfdata.Dataset.from_generator(\n    lambda: generator_state1, \n    (tf.float32, tf.float32),\n    (tf.TensorShape([]), tf.TensorShape([]))\n)\nprefetched = dataset.prefetch(3 * batch_size)\nbatches = prefetched.batch(batch_size)\niterator = batches.make_one_shot_iterator()\n\nx, y = iterator.get_next()\n\nw = tf.Variable([0, 0], dtype=tf.float32)\nprediction = w[0] + w[1] * x\nloss = tf.losses.mean_squared_error(y, prediction)\noptimizer = tf.train.AdamOptimizer(0.1)\ntrain_op = optimizer.minimize(loss)\ninit_op = tf.global_variables_initializer()\n\nsession = tf.Session()\nsession.run(init_op)\nRunning the optimisation gives me\nlosses = []\n\nstart = time()\nfor _ in range(num_batches):\n    _, _loss = session.run([train_op, loss])\n    losses.append(_loss)\ntime() - start  # about seven seconds\nDoing the same using feed_dicts gives\nlosses = []\n\ngenerator_state2 = Generator()\niterator = iter(generator_state2)\n\nstart = time()\nfor _ in range(num_batches):\n    _x, _y = np.transpose([next(iterator) for _ in range(batch_size)])\n    _, _loss = session.run([train_op, loss], {x: _x, y: _y})\n    losses.append(_loss)\ntime() - start  # about one second\nIt seems that the dataset created using the from_generator method isn't fetching from the generator fast enough:\nnp.mean(np.diff(generator_state1.times))  # 7.1533812683949508e-05\nnp.mean(np.diff(generator_state2.times))  # 1.0633696558370612e-05\nThanks again for this, and looking forward to hearing your thoughts.", "body": "@mrry, thank you for implementing the `from_generator` method in `tf.data`. I just wanted to provide some feedback and ask a few more questions.\r\n\r\n# Interface\r\n\r\nIn addition to having `generator` be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable? E.g. instead of \r\n\r\n```python\r\npool = multiprocessing.Pool()\r\ndataset = tf.contrib.data.Dataset.from_generator(\r\n    lambda: pool.imap(some_function, some_data), dtypes, shapes\r\n)\r\n```\r\n\r\nalso support\r\n\r\n```python\r\npool = multiprocessing.Pool()\r\ndataset = tf.contrib.data.Dataset.from_generator(\r\n    pool.imap(some_function, some_data), dtypes, shapes\r\n)\r\n```\r\n\r\n# Return types\r\n\r\n`from_generator` does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have. E.g. this fails\r\n\r\n```python\r\ndef generator():\r\n    while True:\r\n        yield np.zeros(2, np.float32)\r\n        \r\ndataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\r\nx, y = dataset.make_one_shot_iterator().get_next()\r\nsession = tf.Session()\r\nsession.run([x, y])\r\n```\r\n\r\nbut this runs smoothly\r\n\r\n```python\r\ndef generator():\r\n    while True:\r\n        yield tuple(np.zeros(2, np.float32))\r\n        \r\ndataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\r\nx, y = dataset.make_one_shot_iterator().get_next()\r\nsession = tf.Session()\r\nsession.run([x, y])\r\n```\r\n\r\n# Performance\r\n\r\nI've played around with a *very* naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using `feed_dicts`. The setup is as follows\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import data as tfdata\r\nimport numpy as np\r\nfrom time import time\r\n\r\nnum_batches = 1000\r\nbatch_size = 100\r\n\r\nclass Generator:\r\n    def __init__(self):\r\n        self.times = []\r\n    \r\n    def __iter__(self):\r\n        while True:\r\n            x = np.random.normal()\r\n            y = 3 + 5 * x\r\n            x, y = np.asarray([x, y], np.float32)\r\n            self.times.append(time())\r\n            yield x, y\r\n\r\ngenerator_state1 = Generator()\r\n\r\ndataset = tfdata.Dataset.from_generator(\r\n    lambda: generator_state1, \r\n    (tf.float32, tf.float32),\r\n    (tf.TensorShape([]), tf.TensorShape([]))\r\n)\r\nprefetched = dataset.prefetch(3 * batch_size)\r\nbatches = prefetched.batch(batch_size)\r\niterator = batches.make_one_shot_iterator()\r\n\r\nx, y = iterator.get_next()\r\n\r\nw = tf.Variable([0, 0], dtype=tf.float32)\r\nprediction = w[0] + w[1] * x\r\nloss = tf.losses.mean_squared_error(y, prediction)\r\noptimizer = tf.train.AdamOptimizer(0.1)\r\ntrain_op = optimizer.minimize(loss)\r\ninit_op = tf.global_variables_initializer()\r\n\r\nsession = tf.Session()\r\nsession.run(init_op)\r\n```\r\n\r\nRunning the optimisation gives me\r\n\r\n```python\r\nlosses = []\r\n\r\nstart = time()\r\nfor _ in range(num_batches):\r\n    _, _loss = session.run([train_op, loss])\r\n    losses.append(_loss)\r\ntime() - start  # about seven seconds\r\n```\r\n\r\nDoing the same using feed_dicts gives\r\n\r\n```python\r\nlosses = []\r\n\r\ngenerator_state2 = Generator()\r\niterator = iter(generator_state2)\r\n\r\nstart = time()\r\nfor _ in range(num_batches):\r\n    _x, _y = np.transpose([next(iterator) for _ in range(batch_size)])\r\n    _, _loss = session.run([train_op, loss], {x: _x, y: _y})\r\n    losses.append(_loss)\r\ntime() - start  # about one second\r\n```\r\n\r\nIt seems that the dataset created using the `from_generator` method isn't fetching from the generator fast enough:\r\n\r\n```python\r\nnp.mean(np.diff(generator_state1.times))  # 7.1533812683949508e-05\r\nnp.mean(np.diff(generator_state2.times))  # 1.0633696558370612e-05\r\n```\r\n\r\nThanks again for this, and looking forward to hearing your thoughts."}