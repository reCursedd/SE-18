{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/342946599", "html_url": "https://github.com/tensorflow/tensorflow/issues/13101#issuecomment-342946599", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13101", "id": 342946599, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mjk0NjU5OQ==", "user": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-08T20:19:40Z", "updated_at": "2017-11-08T20:24:10Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=966348\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tillahoffmann\">@tillahoffmann</a> I digged into this a bit more and my conclusion is that the slowdown is due to the (constant) overhead of <code>tf.data</code> runtime. In particular, I  have run the following experiment to evaluate my hypothesis:</p>\n<pre><code>import time\nimport numpy as np\nimport tensorflow as tf\n\nfrom tqdm import tqdm\nfrom tensorflow.contrib import data as tfdata\n\nBATCH_SIZE = 128\nFEATURE_SHAPE = (16, 16)\nNUM_BATCHES = 1024*2\nPREFETCH_FACTOR = 4\n\nweights = np.random.normal(0, 1, FEATURE_SHAPE)\n\ndef data_generator():\n  for _ in range(NUM_BATCHES * BATCH_SIZE):\n    feature = np.random.normal(0, 1, FEATURE_SHAPE)\n    value = np.sum(feature * weights) + np.random.normal(0, 1)\n    yield feature.astype(np.float32), value.astype(np.float32)\n\n\ndef data_generator_batch():\n  for _ in range(NUM_BATCHES):\n    features, values = np.empty((BATCH_SIZE, 16, 16)), np.empty((BATCH_SIZE))\n    for i in range(BATCH_SIZE):\n      features[i] = np.random.normal(0, 1, FEATURE_SHAPE)\n      values[i] = np.sum(features[i] * weights) + np.random.normal(0, 1)\n    yield features, values\n\n\ndef build_graph(batchX, batchY):\n  weights_tensor = tf.get_variable('weights', FEATURE_SHAPE, tf.float32)\n  predictions = tf.reduce_sum(weights_tensor * batchX, axis=(1, 2))\n  loss_tensor = tf.losses.mean_squared_error(batchY, predictions)\n\n  optimizer = tf.train.AdamOptimizer()\n  train_op = optimizer.minimize(loss_tensor)\n  return loss_tensor, train_op\n\n\ndef train(sess, train_op, loss_tensor):\n  times = []\n  losses = []\n  for i in tqdm(range(NUM_BATCHES)):\n    start = time.time()\n    _, loss = sess.run([train_op, loss_tensor])\n    times.append(time.time() - start)\n    losses.append(loss)\n  return np.asarray(times), np.asarray(losses)\n\ntimes = {}\nlosses = {}\n\n# Dataset without batch training\nprint(\"Dataset with batching generator\")\nwith tf.Graph().as_default():\n  dataset = tfdata.Dataset.from_generator(\n      data_generator_batch, (tf.float32, tf.float32), ((BATCH_SIZE, 16, 16),\n                                                       (BATCH_SIZE)))\n  batches = dataset.prefetch(PREFETCH_FACTOR)\n  iterator = batches.make_one_shot_iterator()\n  batch_X, batch_y = iterator.get_next()\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\ntimes['datasets_batch'], losses['datasets_batch'] = train(\n    sess, train_op, loss_tensor)\n\n# Using generator\nprint(\"Feed with batching generator\")\nwith tf.Graph().as_default():\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  tf.train.start_queue_runners(sess)\n\ndata = data_generator_batch()\n_times = []\n_losses = []\n\nfor i in tqdm(range(NUM_BATCHES)):\n  X, y = next(data)\n  start = time.time()\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\n  _times.append(time.time() - start)\n  _losses.append(loss)\ntimes['generator_batch'] = np.asarray(_times)\nlosses['generator_batch'] = np.asarray(_losses)\n\n# Dataset with batch training\nprint(\"Dataset without batching generator\")\nwith tf.Graph().as_default():\n  dataset = tfdata.Dataset.from_generator(\n      data_generator, (tf.float32, tf.float32), (FEATURE_SHAPE, ()))\n  batches = dataset.batch(BATCH_SIZE).prefetch(PREFETCH_FACTOR)\n  iterator = batches.make_one_shot_iterator()\n  batch_X, batch_y = iterator.get_next()\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\ntimes['datasets_no_batch'], losses['datasets_no_batch'] = train(\n    sess, train_op, loss_tensor)\n\n# Using generator\nprint(\"Feed without batching generator\")\nwith tf.Graph().as_default():\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  tf.train.start_queue_runners(sess)\n\ndata = data_generator()\n_times = []\n_losses = []\n\nfor i in tqdm(range(NUM_BATCHES)):\n  X = []\n  y = []\n\n  for i in range(BATCH_SIZE):\n      _X, _y = next(data)\n      X.append(_X)\n      y.append(_y)\n\n  start = time.time()\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\n  _times.append(time.time() - start)\n  _losses.append(loss)\ntimes['generator_no_batch'] = np.asarray(_times)\nlosses['generator_no_batch'] = np.asarray(_losses)\n</code></pre>\n<p>which results in:</p>\n<pre><code>Dataset with batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:09&lt;00:00, 214.56it/s]\nFeed with batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:07&lt;00:00, 291.40it/s]\nDataset without batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [01:05&lt;00:00, 31.11it/s]\nFeed without batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:08&lt;00:00, 227.84it/s]\n</code></pre>\n<p>In particular, if a generator that produces whole batches is used (as opposed to a generator that produces single elements batched by <code>tf.data</code>), the throughput of <code>tf.data</code>-based execution is comparable to the throughput of feed-based execution.</p>\n<p>Normally, I would expect the cost of fetching a single element from a pipeline to include a possibly expensive <code>tf.data.map</code> operation, over which the constant overhead of executing the <code>tf.data</code> pipeline (e.g. scheduling TF ops) would be amortized. For pipelines that do very little computation (such as the one used in your example), the overhead of the general purpose pipeline execution engine can be non-negligible and pushing the batching logic into the generator is one possible solution to avoid the slowdown.</p>", "body_text": "@tillahoffmann I digged into this a bit more and my conclusion is that the slowdown is due to the (constant) overhead of tf.data runtime. In particular, I  have run the following experiment to evaluate my hypothesis:\nimport time\nimport numpy as np\nimport tensorflow as tf\n\nfrom tqdm import tqdm\nfrom tensorflow.contrib import data as tfdata\n\nBATCH_SIZE = 128\nFEATURE_SHAPE = (16, 16)\nNUM_BATCHES = 1024*2\nPREFETCH_FACTOR = 4\n\nweights = np.random.normal(0, 1, FEATURE_SHAPE)\n\ndef data_generator():\n  for _ in range(NUM_BATCHES * BATCH_SIZE):\n    feature = np.random.normal(0, 1, FEATURE_SHAPE)\n    value = np.sum(feature * weights) + np.random.normal(0, 1)\n    yield feature.astype(np.float32), value.astype(np.float32)\n\n\ndef data_generator_batch():\n  for _ in range(NUM_BATCHES):\n    features, values = np.empty((BATCH_SIZE, 16, 16)), np.empty((BATCH_SIZE))\n    for i in range(BATCH_SIZE):\n      features[i] = np.random.normal(0, 1, FEATURE_SHAPE)\n      values[i] = np.sum(features[i] * weights) + np.random.normal(0, 1)\n    yield features, values\n\n\ndef build_graph(batchX, batchY):\n  weights_tensor = tf.get_variable('weights', FEATURE_SHAPE, tf.float32)\n  predictions = tf.reduce_sum(weights_tensor * batchX, axis=(1, 2))\n  loss_tensor = tf.losses.mean_squared_error(batchY, predictions)\n\n  optimizer = tf.train.AdamOptimizer()\n  train_op = optimizer.minimize(loss_tensor)\n  return loss_tensor, train_op\n\n\ndef train(sess, train_op, loss_tensor):\n  times = []\n  losses = []\n  for i in tqdm(range(NUM_BATCHES)):\n    start = time.time()\n    _, loss = sess.run([train_op, loss_tensor])\n    times.append(time.time() - start)\n    losses.append(loss)\n  return np.asarray(times), np.asarray(losses)\n\ntimes = {}\nlosses = {}\n\n# Dataset without batch training\nprint(\"Dataset with batching generator\")\nwith tf.Graph().as_default():\n  dataset = tfdata.Dataset.from_generator(\n      data_generator_batch, (tf.float32, tf.float32), ((BATCH_SIZE, 16, 16),\n                                                       (BATCH_SIZE)))\n  batches = dataset.prefetch(PREFETCH_FACTOR)\n  iterator = batches.make_one_shot_iterator()\n  batch_X, batch_y = iterator.get_next()\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\ntimes['datasets_batch'], losses['datasets_batch'] = train(\n    sess, train_op, loss_tensor)\n\n# Using generator\nprint(\"Feed with batching generator\")\nwith tf.Graph().as_default():\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  tf.train.start_queue_runners(sess)\n\ndata = data_generator_batch()\n_times = []\n_losses = []\n\nfor i in tqdm(range(NUM_BATCHES)):\n  X, y = next(data)\n  start = time.time()\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\n  _times.append(time.time() - start)\n  _losses.append(loss)\ntimes['generator_batch'] = np.asarray(_times)\nlosses['generator_batch'] = np.asarray(_losses)\n\n# Dataset with batch training\nprint(\"Dataset without batching generator\")\nwith tf.Graph().as_default():\n  dataset = tfdata.Dataset.from_generator(\n      data_generator, (tf.float32, tf.float32), (FEATURE_SHAPE, ()))\n  batches = dataset.batch(BATCH_SIZE).prefetch(PREFETCH_FACTOR)\n  iterator = batches.make_one_shot_iterator()\n  batch_X, batch_y = iterator.get_next()\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\ntimes['datasets_no_batch'], losses['datasets_no_batch'] = train(\n    sess, train_op, loss_tensor)\n\n# Using generator\nprint(\"Feed without batching generator\")\nwith tf.Graph().as_default():\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\n\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  tf.train.start_queue_runners(sess)\n\ndata = data_generator()\n_times = []\n_losses = []\n\nfor i in tqdm(range(NUM_BATCHES)):\n  X = []\n  y = []\n\n  for i in range(BATCH_SIZE):\n      _X, _y = next(data)\n      X.append(_X)\n      y.append(_y)\n\n  start = time.time()\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\n  _times.append(time.time() - start)\n  _losses.append(loss)\ntimes['generator_no_batch'] = np.asarray(_times)\nlosses['generator_no_batch'] = np.asarray(_losses)\n\nwhich results in:\nDataset with batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:09<00:00, 214.56it/s]\nFeed with batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:07<00:00, 291.40it/s]\nDataset without batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [01:05<00:00, 31.11it/s]\nFeed without batching generator\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:08<00:00, 227.84it/s]\n\nIn particular, if a generator that produces whole batches is used (as opposed to a generator that produces single elements batched by tf.data), the throughput of tf.data-based execution is comparable to the throughput of feed-based execution.\nNormally, I would expect the cost of fetching a single element from a pipeline to include a possibly expensive tf.data.map operation, over which the constant overhead of executing the tf.data pipeline (e.g. scheduling TF ops) would be amortized. For pipelines that do very little computation (such as the one used in your example), the overhead of the general purpose pipeline execution engine can be non-negligible and pushing the batching logic into the generator is one possible solution to avoid the slowdown.", "body": "@tillahoffmann I digged into this a bit more and my conclusion is that the slowdown is due to the (constant) overhead of `tf.data` runtime. In particular, I  have run the following experiment to evaluate my hypothesis:\r\n\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tqdm import tqdm\r\nfrom tensorflow.contrib import data as tfdata\r\n\r\nBATCH_SIZE = 128\r\nFEATURE_SHAPE = (16, 16)\r\nNUM_BATCHES = 1024*2\r\nPREFETCH_FACTOR = 4\r\n\r\nweights = np.random.normal(0, 1, FEATURE_SHAPE)\r\n\r\ndef data_generator():\r\n  for _ in range(NUM_BATCHES * BATCH_SIZE):\r\n    feature = np.random.normal(0, 1, FEATURE_SHAPE)\r\n    value = np.sum(feature * weights) + np.random.normal(0, 1)\r\n    yield feature.astype(np.float32), value.astype(np.float32)\r\n\r\n\r\ndef data_generator_batch():\r\n  for _ in range(NUM_BATCHES):\r\n    features, values = np.empty((BATCH_SIZE, 16, 16)), np.empty((BATCH_SIZE))\r\n    for i in range(BATCH_SIZE):\r\n      features[i] = np.random.normal(0, 1, FEATURE_SHAPE)\r\n      values[i] = np.sum(features[i] * weights) + np.random.normal(0, 1)\r\n    yield features, values\r\n\r\n\r\ndef build_graph(batchX, batchY):\r\n  weights_tensor = tf.get_variable('weights', FEATURE_SHAPE, tf.float32)\r\n  predictions = tf.reduce_sum(weights_tensor * batchX, axis=(1, 2))\r\n  loss_tensor = tf.losses.mean_squared_error(batchY, predictions)\r\n\r\n  optimizer = tf.train.AdamOptimizer()\r\n  train_op = optimizer.minimize(loss_tensor)\r\n  return loss_tensor, train_op\r\n\r\n\r\ndef train(sess, train_op, loss_tensor):\r\n  times = []\r\n  losses = []\r\n  for i in tqdm(range(NUM_BATCHES)):\r\n    start = time.time()\r\n    _, loss = sess.run([train_op, loss_tensor])\r\n    times.append(time.time() - start)\r\n    losses.append(loss)\r\n  return np.asarray(times), np.asarray(losses)\r\n\r\ntimes = {}\r\nlosses = {}\r\n\r\n# Dataset without batch training\r\nprint(\"Dataset with batching generator\")\r\nwith tf.Graph().as_default():\r\n  dataset = tfdata.Dataset.from_generator(\r\n      data_generator_batch, (tf.float32, tf.float32), ((BATCH_SIZE, 16, 16),\r\n                                                       (BATCH_SIZE)))\r\n  batches = dataset.prefetch(PREFETCH_FACTOR)\r\n  iterator = batches.make_one_shot_iterator()\r\n  batch_X, batch_y = iterator.get_next()\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\ntimes['datasets_batch'], losses['datasets_batch'] = train(\r\n    sess, train_op, loss_tensor)\r\n\r\n# Using generator\r\nprint(\"Feed with batching generator\")\r\nwith tf.Graph().as_default():\r\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\r\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\n  tf.train.start_queue_runners(sess)\r\n\r\ndata = data_generator_batch()\r\n_times = []\r\n_losses = []\r\n\r\nfor i in tqdm(range(NUM_BATCHES)):\r\n  X, y = next(data)\r\n  start = time.time()\r\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\r\n  _times.append(time.time() - start)\r\n  _losses.append(loss)\r\ntimes['generator_batch'] = np.asarray(_times)\r\nlosses['generator_batch'] = np.asarray(_losses)\r\n\r\n# Dataset with batch training\r\nprint(\"Dataset without batching generator\")\r\nwith tf.Graph().as_default():\r\n  dataset = tfdata.Dataset.from_generator(\r\n      data_generator, (tf.float32, tf.float32), (FEATURE_SHAPE, ()))\r\n  batches = dataset.batch(BATCH_SIZE).prefetch(PREFETCH_FACTOR)\r\n  iterator = batches.make_one_shot_iterator()\r\n  batch_X, batch_y = iterator.get_next()\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\ntimes['datasets_no_batch'], losses['datasets_no_batch'] = train(\r\n    sess, train_op, loss_tensor)\r\n\r\n# Using generator\r\nprint(\"Feed without batching generator\")\r\nwith tf.Graph().as_default():\r\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\r\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\n  tf.train.start_queue_runners(sess)\r\n\r\ndata = data_generator()\r\n_times = []\r\n_losses = []\r\n\r\nfor i in tqdm(range(NUM_BATCHES)):\r\n  X = []\r\n  y = []\r\n\r\n  for i in range(BATCH_SIZE):\r\n      _X, _y = next(data)\r\n      X.append(_X)\r\n      y.append(_y)\r\n\r\n  start = time.time()\r\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\r\n  _times.append(time.time() - start)\r\n  _losses.append(loss)\r\ntimes['generator_no_batch'] = np.asarray(_times)\r\nlosses['generator_no_batch'] = np.asarray(_losses)\r\n```\r\n\r\nwhich results in:\r\n\r\n```\r\nDataset with batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:09<00:00, 214.56it/s]\r\nFeed with batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:07<00:00, 291.40it/s]\r\nDataset without batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [01:05<00:00, 31.11it/s]\r\nFeed without batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:08<00:00, 227.84it/s]\r\n```\r\n\r\nIn particular, if a generator that produces whole batches is used (as opposed to a generator that produces single elements batched by `tf.data`), the throughput of `tf.data`-based execution is comparable to the throughput of feed-based execution.\r\n\r\nNormally, I would expect the cost of fetching a single element from a pipeline to include a possibly expensive `tf.data.map` operation, over which the constant overhead of executing the `tf.data` pipeline (e.g. scheduling TF ops) would be amortized. For pipelines that do very little computation (such as the one used in your example), the overhead of the general purpose pipeline execution engine can be non-negligible and pushing the batching logic into the generator is one possible solution to avoid the slowdown."}