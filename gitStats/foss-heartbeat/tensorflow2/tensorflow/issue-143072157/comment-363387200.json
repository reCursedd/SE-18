{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/363387200", "html_url": "https://github.com/tensorflow/tensorflow/issues/1606#issuecomment-363387200", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1606", "id": 363387200, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MzM4NzIwMA==", "user": {"login": "syedk01", "id": 3174993, "node_id": "MDQ6VXNlcjMxNzQ5OTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/3174993?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syedk01", "html_url": "https://github.com/syedk01", "followers_url": "https://api.github.com/users/syedk01/followers", "following_url": "https://api.github.com/users/syedk01/following{/other_user}", "gists_url": "https://api.github.com/users/syedk01/gists{/gist_id}", "starred_url": "https://api.github.com/users/syedk01/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syedk01/subscriptions", "organizations_url": "https://api.github.com/users/syedk01/orgs", "repos_url": "https://api.github.com/users/syedk01/repos", "events_url": "https://api.github.com/users/syedk01/events{/privacy}", "received_events_url": "https://api.github.com/users/syedk01/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-06T10:57:13Z", "updated_at": "2018-02-06T10:57:13Z", "author_association": "NONE", "body_html": "<p>I am hoping to print the confusion matrix in my code<br>\nIn the line where it says<br>\nTgtc=11<br>\nHow do i add something like<br>\nTgtc=[2,5,7,8,11] where i want to do a confusion matrix for the 5 variables</p>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<p>import sys</p>\n<p>if sys.version_info.major != 3:<br>\nraise Exception('Please run with python3')</p>\n<p>import os<br>\nimport time<br>\nimport gc<br>\nimport numpy as np</p>\n<p>import afib_common</p>\n<p>from afib_common import (<br>\nbuild_model, get_batch, train_test_split, get_batch2)<br>\nfrom afib_data import reload_settings</p>\n<h1>Load constants defined in afib_data</h1>\n<p>reload_settings()<br>\nif not os.path.isdir('modelsave'):<br>\nos.path.mkdir('modelsave')<br>\nmodel = build_model()</p>\n<p>def excld(N, i):<br>\n''' Generates a list of numbers 0..N-1 without the number i'''<br>\nreturn tuple(_ for _ in range(N) if _ != i)</p>\n<h1>Target class for performance metric</h1>\n<p>tgtC = 11</p>\n<h1>(normal = \"N\", is the 11th item in afib_common.symbols_keep)</h1>\n<p>def calc_confusion_matrix(test_y, test_yhat):<br>\n''' Generates a confusion matrix from actual classes and predicted classes.</p>\n<pre><code>Columns are the predicted class, while rows are the actual.\n'''\nN = test_y.shape[1]\ncm = np.zeros((N, N))\ncls_y = test_y.argmax(1)\ncls_yhat = test_yhat.argmax(1)\nfor actual, predicted in zip(cls_y, cls_yhat):\n    cm[actual, predicted] += 1\nreturn cm / cm.sum()\n</code></pre>\n<p>def estimate_f1_score():<br>\n''' Estimate the F1 score using many samples.</p>\n<pre><code>Numbrer of samples is controlled by the Nbatch constant here.\nThe F1 score is evaluated for a specific class. (See tgtC variable defined\nearlier in this file.)\n\nF1 definition: https://en.wikipedia.org/wiki/F1_score\n\nPS: This function uses the class prevalence as returned by `get_batch`.\n(I.e. all classes are sampled with probability 1/NC where NC = number of\nclasses)\n'''\nNbatch = 1000\ntest_batch = get_batch(test_records, Nbatch)\ntest_yhat = model.predict(test_batch.X)\ntrue_positive = (\n    (test_batch.y.argmax(1) == tgtC) *\n    (test_yhat.argmax(1) == tgtC)).sum()\nfalse_positive = (\n    (test_batch.y.argmax(1) != tgtC) *\n    (test_yhat.argmax(1) == tgtC)).sum()\nfalse_negative = (\n    (test_batch.y.argmax(1) == tgtC) *\n    (test_yhat.argmax(1) != tgtC)).sum()\n# true_negative = (\n#     (test_batch.y.argmax(1) != tgtC) *\n#     (test_yhat.argmax(1) != tgtC)).sum()\nf1 = 2 * true_positive / (\n    2 * true_positive + false_positive + false_negative)\nconf_matrix = calc_confusion_matrix(test_batch.y, test_yhat)\nnormal_acc = calc_normal_accuracy(conf_matrix)\nreturn f1, normal_acc, conf_matrix\n</code></pre>\n<h1>NC = Number of classes</h1>\n<p>NC = len(afib_common.symbols_keep)</p>\n<p>def generate_proper_test_train_split():<br>\n'''Prepare a train-test split such that the split is across patients.</p>\n<pre><code>This prevents that a data leak occurs where the model learns to identify\npatients instead of heartbeat classes.\n\nAlthough, this makes it more difficult to find a split where the amount\nof beats in the training set vs testing set is not close to 80%:20% that\nwe would otherwise easily get with a naive split.\n\nThe approach here is to randomly repeat the split until the beats are\nsplit close enough to 80:20.\n(For some classes this is impossible, and we settle for anything between\n40% to 95% in the training set)\n'''\nproblems = 1\nretries = -1\nwhile problems != 0:\n    retries += 1\n    print(\n        'Generating properly balanced test-train split. Try#', retries + 1)\n    train_records, test_records = train_test_split()\n    problems = 0\n    prob_fracs = []\n    for k in train_records:\n        len_train = len(train_records[k])\n        len_test = len(test_records[k])\n        frac = len_train / (len_train + len_test)\n        if frac &lt; 0.4 or frac &gt; 0.95:  # want 0.8 mostly...\n            problems += 1\n            prob_fracs.append(frac)\n        # print(retries, problems, prob_fracs)\nVIEW_SPLIT_STATS = 0\nif VIEW_SPLIT_STATS:\n    for k in train_records:\n        len_train = len(train_records[k])\n        len_test = len(test_records[k])\n        frac = len_train / (len_train + len_test)\n        print(k, '   ', '%3f' % frac, '   ', len_train + len_test)\nreturn train_records, test_records\n</code></pre>\n<p>def conf_matrix_for_class(conf_matrix, tgtC):<br>\n''' Get the 2x2 confidence matrix for a specific class given the confidence<br>\nmatrix for all the classes.<br>\n'''<br>\ntp = conf_matrix[tgtC, tgtC]<br>\ntn = conf_matrix[np.ix_(excld(NC,tgtC),excld(NC,tgtC))].sum()<br>\nfp = conf_matrix[np.ix_(excld(NC, tgtC),[tgtC])].sum()<br>\nfn = conf_matrix[np.ix_([tgtC], excld(NC, tgtC))].sum()<br>\nreturn np.array([[tp, fn], [fp, tn]])</p>\n<p>def adjust_prevalence(conf_matrix_2x2, prevl_new):<br>\n''' Takes a 2x2 confidence matrix and adjusts the rows so that the<br>\nprevalnce of the positive class becomes <code>prevl_new</code></p>\n<pre><code>    Prevalence = (TP + FN) / (TP + FN + FP + TN)\n'''\ntmp = conf_matrix_2x2\ntmp = tmp / tmp.sum(1, keepdims=1)\ntmp = tmp * [[prevl_new], [1 - prevl_new]]\ntmp = tmp / tmp.sum()\nreturn tmp\n</code></pre>\n<p>def conf_matrix_metrics(conf_mat_2x2):<br>\n''' Takes a 2x2 confidence matrix and returns the various binary metrics<br>\nassociated with it as a dictionary.</p>\n<pre><code>definitions: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n'''\n(tp, fn), (fp, tn) = conf_mat_2x2\nacc = tp + tn\nsens = tp / (tp + fn)\nprec = tp / (tp + fp)\nprev = tp + fn\nf1 = 2 * tp / (2 * tp + fn + fp)\nreturn {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n        'accuracy': acc, 'sensitivity': sens, 'precision': prec,\n        'prevalence': prev, 'f1': f1}\n</code></pre>\n<p>def calc_normal_accuracy(conf_matrix):<br>\n''' Takes a confidence matrix, converts it to the 2x2 specific confidence<br>\nmatrix of class <code>tgtC</code>(normal) and rescales the prevalence according<br>\nto the prevalence of the normal class in the databases.<br>\n(At the time of writing 96%)<br>\n'''<br>\nnormal_prevalence = 0.96  # Prevalence of \"normal\" beats in the datasets<br>\ntmp0 = conf_matrix_for_class(conf_matrix, tgtC)<br>\ntmp1 = adjust_prevalence(tmp0, normal_prevalence)<br>\nreturn conf_matrix_metrics(tmp1)['accuracy']</p>\n<p>train_records, test_records = generate_proper_test_train_split()</p>\n<p>score_hist = []<br>\nscore_hist2f1 = []  # predictive f1<br>\nscore_hist2acc = []  # predictive accuracy<br>\nIT1 = 20000  # Number of batches to train on before exiting</p>\n<p>class_acc_hist = []  # Per-class accuracies<br>\ndt_hist = []  # timing performance history (for debugging)</p>\n<p>print('\\nStarting training...')<br>\nk = 0<br>\nwhile len(score_hist) &lt; IT1:<br>\nt1 = time.time()<br>\nbatch = get_batch2(train_records, 16)<br>\nbatch_y_soft = batch.y * 0.99 + 0.01 / batch.y.shape[1]  # soften labels<br>\nscore = model.train_on_batch(batch.X, batch_y_soft)<br>\nscore_hist.append(score[0])<br>\nt2 = time.time()<br>\ndt_hist.append(t2 - t1)<br>\nif len(score_hist) % 50 == 1:<br>\ngc.collect()  # Helps - although it shouldn't :(<br>\nf1_pred, acc_pred, conf_matrix = estimate_f1_score()<br>\nclass_acc_hist.append(<br>\nconf_matrix.diagonal() / conf_matrix.sum(1))  # sensitivity<br>\nprint('Batches seen:', len(score_hist))<br>\nprint('    f1, acc = %r' % ((f1_pred, acc_pred),))<br>\nscore_hist2f1.append(f1_pred)<br>\nscore_hist2acc.append(acc_pred)</p>\n<p>print('Saving trained model...')<br>\nmodel.save('modelsave/model.data')<br>\nprint('Done!')</p>", "body_text": "I am hoping to print the confusion matrix in my code\nIn the line where it says\nTgtc=11\nHow do i add something like\nTgtc=[2,5,7,8,11] where i want to do a confusion matrix for the 5 variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys\nif sys.version_info.major != 3:\nraise Exception('Please run with python3')\nimport os\nimport time\nimport gc\nimport numpy as np\nimport afib_common\nfrom afib_common import (\nbuild_model, get_batch, train_test_split, get_batch2)\nfrom afib_data import reload_settings\nLoad constants defined in afib_data\nreload_settings()\nif not os.path.isdir('modelsave'):\nos.path.mkdir('modelsave')\nmodel = build_model()\ndef excld(N, i):\n''' Generates a list of numbers 0..N-1 without the number i'''\nreturn tuple(_ for _ in range(N) if _ != i)\nTarget class for performance metric\ntgtC = 11\n(normal = \"N\", is the 11th item in afib_common.symbols_keep)\ndef calc_confusion_matrix(test_y, test_yhat):\n''' Generates a confusion matrix from actual classes and predicted classes.\nColumns are the predicted class, while rows are the actual.\n'''\nN = test_y.shape[1]\ncm = np.zeros((N, N))\ncls_y = test_y.argmax(1)\ncls_yhat = test_yhat.argmax(1)\nfor actual, predicted in zip(cls_y, cls_yhat):\n    cm[actual, predicted] += 1\nreturn cm / cm.sum()\n\ndef estimate_f1_score():\n''' Estimate the F1 score using many samples.\nNumbrer of samples is controlled by the Nbatch constant here.\nThe F1 score is evaluated for a specific class. (See tgtC variable defined\nearlier in this file.)\n\nF1 definition: https://en.wikipedia.org/wiki/F1_score\n\nPS: This function uses the class prevalence as returned by `get_batch`.\n(I.e. all classes are sampled with probability 1/NC where NC = number of\nclasses)\n'''\nNbatch = 1000\ntest_batch = get_batch(test_records, Nbatch)\ntest_yhat = model.predict(test_batch.X)\ntrue_positive = (\n    (test_batch.y.argmax(1) == tgtC) *\n    (test_yhat.argmax(1) == tgtC)).sum()\nfalse_positive = (\n    (test_batch.y.argmax(1) != tgtC) *\n    (test_yhat.argmax(1) == tgtC)).sum()\nfalse_negative = (\n    (test_batch.y.argmax(1) == tgtC) *\n    (test_yhat.argmax(1) != tgtC)).sum()\n# true_negative = (\n#     (test_batch.y.argmax(1) != tgtC) *\n#     (test_yhat.argmax(1) != tgtC)).sum()\nf1 = 2 * true_positive / (\n    2 * true_positive + false_positive + false_negative)\nconf_matrix = calc_confusion_matrix(test_batch.y, test_yhat)\nnormal_acc = calc_normal_accuracy(conf_matrix)\nreturn f1, normal_acc, conf_matrix\n\nNC = Number of classes\nNC = len(afib_common.symbols_keep)\ndef generate_proper_test_train_split():\n'''Prepare a train-test split such that the split is across patients.\nThis prevents that a data leak occurs where the model learns to identify\npatients instead of heartbeat classes.\n\nAlthough, this makes it more difficult to find a split where the amount\nof beats in the training set vs testing set is not close to 80%:20% that\nwe would otherwise easily get with a naive split.\n\nThe approach here is to randomly repeat the split until the beats are\nsplit close enough to 80:20.\n(For some classes this is impossible, and we settle for anything between\n40% to 95% in the training set)\n'''\nproblems = 1\nretries = -1\nwhile problems != 0:\n    retries += 1\n    print(\n        'Generating properly balanced test-train split. Try#', retries + 1)\n    train_records, test_records = train_test_split()\n    problems = 0\n    prob_fracs = []\n    for k in train_records:\n        len_train = len(train_records[k])\n        len_test = len(test_records[k])\n        frac = len_train / (len_train + len_test)\n        if frac < 0.4 or frac > 0.95:  # want 0.8 mostly...\n            problems += 1\n            prob_fracs.append(frac)\n        # print(retries, problems, prob_fracs)\nVIEW_SPLIT_STATS = 0\nif VIEW_SPLIT_STATS:\n    for k in train_records:\n        len_train = len(train_records[k])\n        len_test = len(test_records[k])\n        frac = len_train / (len_train + len_test)\n        print(k, '   ', '%3f' % frac, '   ', len_train + len_test)\nreturn train_records, test_records\n\ndef conf_matrix_for_class(conf_matrix, tgtC):\n''' Get the 2x2 confidence matrix for a specific class given the confidence\nmatrix for all the classes.\n'''\ntp = conf_matrix[tgtC, tgtC]\ntn = conf_matrix[np.ix_(excld(NC,tgtC),excld(NC,tgtC))].sum()\nfp = conf_matrix[np.ix_(excld(NC, tgtC),[tgtC])].sum()\nfn = conf_matrix[np.ix_([tgtC], excld(NC, tgtC))].sum()\nreturn np.array([[tp, fn], [fp, tn]])\ndef adjust_prevalence(conf_matrix_2x2, prevl_new):\n''' Takes a 2x2 confidence matrix and adjusts the rows so that the\nprevalnce of the positive class becomes prevl_new\n    Prevalence = (TP + FN) / (TP + FN + FP + TN)\n'''\ntmp = conf_matrix_2x2\ntmp = tmp / tmp.sum(1, keepdims=1)\ntmp = tmp * [[prevl_new], [1 - prevl_new]]\ntmp = tmp / tmp.sum()\nreturn tmp\n\ndef conf_matrix_metrics(conf_mat_2x2):\n''' Takes a 2x2 confidence matrix and returns the various binary metrics\nassociated with it as a dictionary.\ndefinitions: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n'''\n(tp, fn), (fp, tn) = conf_mat_2x2\nacc = tp + tn\nsens = tp / (tp + fn)\nprec = tp / (tp + fp)\nprev = tp + fn\nf1 = 2 * tp / (2 * tp + fn + fp)\nreturn {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n        'accuracy': acc, 'sensitivity': sens, 'precision': prec,\n        'prevalence': prev, 'f1': f1}\n\ndef calc_normal_accuracy(conf_matrix):\n''' Takes a confidence matrix, converts it to the 2x2 specific confidence\nmatrix of class tgtC(normal) and rescales the prevalence according\nto the prevalence of the normal class in the databases.\n(At the time of writing 96%)\n'''\nnormal_prevalence = 0.96  # Prevalence of \"normal\" beats in the datasets\ntmp0 = conf_matrix_for_class(conf_matrix, tgtC)\ntmp1 = adjust_prevalence(tmp0, normal_prevalence)\nreturn conf_matrix_metrics(tmp1)['accuracy']\ntrain_records, test_records = generate_proper_test_train_split()\nscore_hist = []\nscore_hist2f1 = []  # predictive f1\nscore_hist2acc = []  # predictive accuracy\nIT1 = 20000  # Number of batches to train on before exiting\nclass_acc_hist = []  # Per-class accuracies\ndt_hist = []  # timing performance history (for debugging)\nprint('\\nStarting training...')\nk = 0\nwhile len(score_hist) < IT1:\nt1 = time.time()\nbatch = get_batch2(train_records, 16)\nbatch_y_soft = batch.y * 0.99 + 0.01 / batch.y.shape[1]  # soften labels\nscore = model.train_on_batch(batch.X, batch_y_soft)\nscore_hist.append(score[0])\nt2 = time.time()\ndt_hist.append(t2 - t1)\nif len(score_hist) % 50 == 1:\ngc.collect()  # Helps - although it shouldn't :(\nf1_pred, acc_pred, conf_matrix = estimate_f1_score()\nclass_acc_hist.append(\nconf_matrix.diagonal() / conf_matrix.sum(1))  # sensitivity\nprint('Batches seen:', len(score_hist))\nprint('    f1, acc = %r' % ((f1_pred, acc_pred),))\nscore_hist2f1.append(f1_pred)\nscore_hist2acc.append(acc_pred)\nprint('Saving trained model...')\nmodel.save('modelsave/model.data')\nprint('Done!')", "body": "I am hoping to print the confusion matrix in my code\r\nIn the line where it says \r\nTgtc=11\r\nHow do i add something like \r\nTgtc=[2,5,7,8,11] where i want to do a confusion matrix for the 5 variables\r\n\r\n>>>>>>>>>>>>\r\nimport sys\r\n\r\nif sys.version_info.major != 3:\r\n    raise Exception('Please run with python3')\r\n\r\nimport os\r\nimport time\r\nimport gc\r\nimport numpy as np\r\n\r\nimport afib_common\r\n\r\nfrom afib_common import (\r\n    build_model, get_batch, train_test_split, get_batch2)\r\nfrom afib_data import reload_settings\r\n\r\n# Load constants defined in afib_data\r\nreload_settings()\r\nif not os.path.isdir('modelsave'):\r\n    os.path.mkdir('modelsave')\r\nmodel = build_model()\r\n\r\ndef excld(N, i):\r\n    ''' Generates a list of numbers 0..N-1 without the number i'''\r\n    return tuple(_ for _ in range(N) if _ != i)\r\n\r\n# Target class for performance metric\r\ntgtC = 11\r\n# (normal = \"N\", is the 11th item in afib_common.symbols_keep)\r\n\r\ndef calc_confusion_matrix(test_y, test_yhat):\r\n    ''' Generates a confusion matrix from actual classes and predicted classes.\r\n\r\n    Columns are the predicted class, while rows are the actual.\r\n    '''\r\n    N = test_y.shape[1]\r\n    cm = np.zeros((N, N))\r\n    cls_y = test_y.argmax(1)\r\n    cls_yhat = test_yhat.argmax(1)\r\n    for actual, predicted in zip(cls_y, cls_yhat):\r\n        cm[actual, predicted] += 1\r\n    return cm / cm.sum()\r\n\r\ndef estimate_f1_score():\r\n    ''' Estimate the F1 score using many samples.\r\n\r\n    Numbrer of samples is controlled by the Nbatch constant here.\r\n    The F1 score is evaluated for a specific class. (See tgtC variable defined\r\n    earlier in this file.)\r\n\r\n    F1 definition: https://en.wikipedia.org/wiki/F1_score\r\n\r\n    PS: This function uses the class prevalence as returned by `get_batch`.\r\n    (I.e. all classes are sampled with probability 1/NC where NC = number of\r\n    classes)\r\n    '''\r\n    Nbatch = 1000\r\n    test_batch = get_batch(test_records, Nbatch)\r\n    test_yhat = model.predict(test_batch.X)\r\n    true_positive = (\r\n        (test_batch.y.argmax(1) == tgtC) *\r\n        (test_yhat.argmax(1) == tgtC)).sum()\r\n    false_positive = (\r\n        (test_batch.y.argmax(1) != tgtC) *\r\n        (test_yhat.argmax(1) == tgtC)).sum()\r\n    false_negative = (\r\n        (test_batch.y.argmax(1) == tgtC) *\r\n        (test_yhat.argmax(1) != tgtC)).sum()\r\n    # true_negative = (\r\n    #     (test_batch.y.argmax(1) != tgtC) *\r\n    #     (test_yhat.argmax(1) != tgtC)).sum()\r\n    f1 = 2 * true_positive / (\r\n        2 * true_positive + false_positive + false_negative)\r\n    conf_matrix = calc_confusion_matrix(test_batch.y, test_yhat)\r\n    normal_acc = calc_normal_accuracy(conf_matrix)\r\n    return f1, normal_acc, conf_matrix\r\n\r\n# NC = Number of classes\r\nNC = len(afib_common.symbols_keep)\r\n\r\ndef generate_proper_test_train_split():\r\n    '''Prepare a train-test split such that the split is across patients.\r\n\r\n    This prevents that a data leak occurs where the model learns to identify\r\n    patients instead of heartbeat classes.\r\n\r\n    Although, this makes it more difficult to find a split where the amount\r\n    of beats in the training set vs testing set is not close to 80%:20% that\r\n    we would otherwise easily get with a naive split.\r\n\r\n    The approach here is to randomly repeat the split until the beats are\r\n    split close enough to 80:20.\r\n    (For some classes this is impossible, and we settle for anything between\r\n    40% to 95% in the training set)\r\n    '''\r\n    problems = 1\r\n    retries = -1\r\n    while problems != 0:\r\n        retries += 1\r\n        print(\r\n            'Generating properly balanced test-train split. Try#', retries + 1)\r\n        train_records, test_records = train_test_split()\r\n        problems = 0\r\n        prob_fracs = []\r\n        for k in train_records:\r\n            len_train = len(train_records[k])\r\n            len_test = len(test_records[k])\r\n            frac = len_train / (len_train + len_test)\r\n            if frac < 0.4 or frac > 0.95:  # want 0.8 mostly...\r\n                problems += 1\r\n                prob_fracs.append(frac)\r\n            # print(retries, problems, prob_fracs)\r\n    VIEW_SPLIT_STATS = 0\r\n    if VIEW_SPLIT_STATS:\r\n        for k in train_records:\r\n            len_train = len(train_records[k])\r\n            len_test = len(test_records[k])\r\n            frac = len_train / (len_train + len_test)\r\n            print(k, '   ', '%3f' % frac, '   ', len_train + len_test)\r\n    return train_records, test_records\r\n\r\ndef conf_matrix_for_class(conf_matrix, tgtC):\r\n    ''' Get the 2x2 confidence matrix for a specific class given the confidence\r\n        matrix for all the classes.\r\n    '''\r\n    tp = conf_matrix[tgtC, tgtC]\r\n    tn = conf_matrix[np.ix_(excld(NC,tgtC),excld(NC,tgtC))].sum()\r\n    fp = conf_matrix[np.ix_(excld(NC, tgtC),[tgtC])].sum()\r\n    fn = conf_matrix[np.ix_([tgtC], excld(NC, tgtC))].sum()\r\n    return np.array([[tp, fn], [fp, tn]])\r\n\r\ndef adjust_prevalence(conf_matrix_2x2, prevl_new):\r\n    ''' Takes a 2x2 confidence matrix and adjusts the rows so that the\r\n        prevalnce of the positive class becomes `prevl_new`\r\n\r\n        Prevalence = (TP + FN) / (TP + FN + FP + TN)\r\n    '''\r\n    tmp = conf_matrix_2x2\r\n    tmp = tmp / tmp.sum(1, keepdims=1)\r\n    tmp = tmp * [[prevl_new], [1 - prevl_new]]\r\n    tmp = tmp / tmp.sum()\r\n    return tmp\r\n\r\ndef conf_matrix_metrics(conf_mat_2x2):\r\n    ''' Takes a 2x2 confidence matrix and returns the various binary metrics\r\n        associated with it as a dictionary.\r\n\r\n    definitions: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\r\n    '''\r\n    (tp, fn), (fp, tn) = conf_mat_2x2\r\n    acc = tp + tn\r\n    sens = tp / (tp + fn)\r\n    prec = tp / (tp + fp)\r\n    prev = tp + fn\r\n    f1 = 2 * tp / (2 * tp + fn + fp)\r\n    return {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\r\n            'accuracy': acc, 'sensitivity': sens, 'precision': prec,\r\n            'prevalence': prev, 'f1': f1}\r\n\r\ndef calc_normal_accuracy(conf_matrix):\r\n    ''' Takes a confidence matrix, converts it to the 2x2 specific confidence\r\n        matrix of class `tgtC`(normal) and rescales the prevalence according\r\n        to the prevalence of the normal class in the databases.\r\n        (At the time of writing 96%)\r\n    '''\r\n    normal_prevalence = 0.96  # Prevalence of \"normal\" beats in the datasets\r\n    tmp0 = conf_matrix_for_class(conf_matrix, tgtC)\r\n    tmp1 = adjust_prevalence(tmp0, normal_prevalence)\r\n    return conf_matrix_metrics(tmp1)['accuracy']\r\n\r\ntrain_records, test_records = generate_proper_test_train_split()\r\n\r\nscore_hist = []\r\nscore_hist2f1 = []  # predictive f1\r\nscore_hist2acc = []  # predictive accuracy\r\nIT1 = 20000  # Number of batches to train on before exiting\r\n\r\nclass_acc_hist = []  # Per-class accuracies\r\ndt_hist = []  # timing performance history (for debugging)\r\n\r\nprint('\\nStarting training...')\r\nk = 0\r\nwhile len(score_hist) < IT1:\r\n    t1 = time.time()\r\n    batch = get_batch2(train_records, 16)\r\n    batch_y_soft = batch.y * 0.99 + 0.01 / batch.y.shape[1]  # soften labels\r\n    score = model.train_on_batch(batch.X, batch_y_soft)\r\n    score_hist.append(score[0])\r\n    t2 = time.time()\r\n    dt_hist.append(t2 - t1)\r\n    if len(score_hist) % 50 == 1:\r\n        gc.collect()  # Helps - although it shouldn't :(\r\n        f1_pred, acc_pred, conf_matrix = estimate_f1_score()\r\n        class_acc_hist.append(\r\n            conf_matrix.diagonal() / conf_matrix.sum(1))  # sensitivity\r\n        print('Batches seen:', len(score_hist))\r\n        print('    f1, acc = %r' % ((f1_pred, acc_pred),))\r\n        score_hist2f1.append(f1_pred)\r\n        score_hist2acc.append(acc_pred)\r\n\r\nprint('Saving trained model...')\r\nmodel.save('modelsave/model.data')\r\nprint('Done!')"}