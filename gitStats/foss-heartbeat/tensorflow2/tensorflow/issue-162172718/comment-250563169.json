{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/250563169", "html_url": "https://github.com/tensorflow/tensorflow/issues/3028#issuecomment-250563169", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3028", "id": 250563169, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDU2MzE2OQ==", "user": {"login": "garion9013", "id": 5905027, "node_id": "MDQ6VXNlcjU5MDUwMjc=", "avatar_url": "https://avatars0.githubusercontent.com/u/5905027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/garion9013", "html_url": "https://github.com/garion9013", "followers_url": "https://api.github.com/users/garion9013/followers", "following_url": "https://api.github.com/users/garion9013/following{/other_user}", "gists_url": "https://api.github.com/users/garion9013/gists{/gist_id}", "starred_url": "https://api.github.com/users/garion9013/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/garion9013/subscriptions", "organizations_url": "https://api.github.com/users/garion9013/orgs", "repos_url": "https://api.github.com/users/garion9013/repos", "events_url": "https://api.github.com/users/garion9013/events{/privacy}", "received_events_url": "https://api.github.com/users/garion9013/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-29T19:13:06Z", "updated_at": "2016-09-29T19:14:20Z", "author_association": "NONE", "body_html": "<p>Yes, baselines are from dense matrix multiplication without iterative pruning. Also, you're right. <em>sparse_tensor_dense_matmul</em> would be more appropriate just for <a href=\"https://github.com/garion9013/impl-pruning-TF\">this example</a>, especially for CPU.<br>\nBut I think bigger models like AlexNet will have more benefits from <em>embedding_lookup</em> than <em>sparse_tensor_dense_matmul</em>. Because it truly reduces app's memory requirements, while the latter doesn't.</p>", "body_text": "Yes, baselines are from dense matrix multiplication without iterative pruning. Also, you're right. sparse_tensor_dense_matmul would be more appropriate just for this example, especially for CPU.\nBut I think bigger models like AlexNet will have more benefits from embedding_lookup than sparse_tensor_dense_matmul. Because it truly reduces app's memory requirements, while the latter doesn't.", "body": "Yes, baselines are from dense matrix multiplication without iterative pruning. Also, you're right. _sparse_tensor_dense_matmul_ would be more appropriate just for [this example](https://github.com/garion9013/impl-pruning-TF), especially for CPU.\nBut I think bigger models like AlexNet will have more benefits from _embedding_lookup_ than _sparse_tensor_dense_matmul_. Because it truly reduces app's memory requirements, while the latter doesn't.\n"}