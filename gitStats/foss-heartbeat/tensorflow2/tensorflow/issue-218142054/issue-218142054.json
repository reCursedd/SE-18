{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8833", "id": 218142054, "node_id": "MDU6SXNzdWUyMTgxNDIwNTQ=", "number": 8833, "title": "DynamicAttentionWrapper expects own state on the 0-th step", "user": {"login": "RobRomijnders", "id": 16174021, "node_id": "MDQ6VXNlcjE2MTc0MDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/16174021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RobRomijnders", "html_url": "https://github.com/RobRomijnders", "followers_url": "https://api.github.com/users/RobRomijnders/followers", "following_url": "https://api.github.com/users/RobRomijnders/following{/other_user}", "gists_url": "https://api.github.com/users/RobRomijnders/gists{/gist_id}", "starred_url": "https://api.github.com/users/RobRomijnders/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RobRomijnders/subscriptions", "organizations_url": "https://api.github.com/users/RobRomijnders/orgs", "repos_url": "https://api.github.com/users/RobRomijnders/repos", "events_url": "https://api.github.com/users/RobRomijnders/events{/privacy}", "received_events_url": "https://api.github.com/users/RobRomijnders/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 41, "created_at": "2017-03-30T09:54:57Z", "updated_at": "2017-06-23T00:10:13Z", "closed_at": "2017-04-01T00:17:48Z", "author_association": "NONE", "body_html": "<p>Using Tensorflow 1.1rc0<br>\nAFAIK the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\">DynamicAttentionWrapper</a> wraps an attention mechanism around the RNNCell. To do so, it passes around its own DynamicAttentionWrapperState in the <strong>call</strong>()</p>\n<p>However, I think that on the 0-th step, the network cannot pass such state as an argument, because the function gets called by a general decoder. (which doesnt know what cell it will encounter)</p>\n<p>I got the following error</p>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/trainer_class.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">54</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">self</span>.model <span class=\"pl-k\">=</span> Model(<span class=\"pl-c1\">self</span>.tok_chr.dim)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/models/rnn_seq2seq_tf.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">164</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    outputs, _ <span class=\"pl-k\">=</span> dynamic_decode(decoder, <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>, <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> max_sl)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">278</span>, <span class=\"pl-k\">in</span> dynamic_decode\n    swap_memory<span class=\"pl-k\">=</span>swap_memory)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2623</span>, <span class=\"pl-k\">in</span> while_loop\n    result <span class=\"pl-k\">=</span> context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2456</span>, <span class=\"pl-k\">in</span> BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2406</span>, <span class=\"pl-k\">in</span> _BuildLoop\n    body_result <span class=\"pl-k\">=</span> body(<span class=\"pl-k\">*</span>packed_vars_for_body)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">231</span>, <span class=\"pl-k\">in</span> body\n    decoder_finished) <span class=\"pl-k\">=</span> decoder.step(time, inputs, state)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">140</span>, <span class=\"pl-k\">in</span> step\n    cell_outputs, cell_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._cell(inputs, state)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">530</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>\n    cell_inputs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._cell_input_fn(inputs, state.attention)\n<span class=\"pl-c1\">AttributeError</span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tuple<span class=\"pl-pds\">'</span></span> <span class=\"pl-c1\">object</span> has no attribute <span class=\"pl-s\"><span class=\"pl-pds\">'</span>attention<span class=\"pl-pds\">'</span></span></pre></div>\n<p>The error indicates that the current state has no attribute attention. But that's the final state of the encoder, which doesn't use this syntax</p>\n<p>The following snippet shows the use of the wrapper. This was coded after <a href=\"https://www.tensorflow.org/versions/r1.1/api_guides/python/contrib.seq2seq#Dynamic_Decoding\" rel=\"nofollow\">this</a> explanation</p>\n<div class=\"highlight highlight-source-python\"><pre>encoder_outputs, encoder_state <span class=\"pl-k\">=</span> core_rnn.static_rnn(\n                encoder_cell, encoder_inputs, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">SL_enc</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Some other code</span>\n\nattention_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nattn_obj <span class=\"pl-k\">=</span> BahdanauAttention(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>attention_size,\n                                             <span class=\"pl-v\">memory</span><span class=\"pl-k\">=</span>attention_states,\n                                             <span class=\"pl-v\">memory_sequence_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">SL_enc</span>,\n                                             <span class=\"pl-v\">normalize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                             <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>BahdanauAttentionObject<span class=\"pl-pds\">'</span></span>)\n\nwrapped_cell <span class=\"pl-k\">=</span> DynamicAttentionWrapper(cell_dec_fw,\n                                                       attn_obj,\n                                                       D,\n                                                       <span class=\"pl-v\">output_attention</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                                       <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>DynAttnWrap<span class=\"pl-pds\">'</span></span>)\n\nsampler <span class=\"pl-k\">=</span> ScheduledEmbeddingTrainingHelper(decoder_inputs,\n                                                        <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">SL_dec</span>,\n                                                        <span class=\"pl-v\">embedding</span><span class=\"pl-k\">=</span>embedding_in,\n                                                        <span class=\"pl-v\">sampling_probability</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.samp_prob)\ndecoder <span class=\"pl-k\">=</span> BasicDecoder(wrapped_cell,\n                                       sampler,\n                                       encoder_state)\noutputs, _ <span class=\"pl-k\">=</span> dynamic_decode(decoder)</pre></div>", "body_text": "Using Tensorflow 1.1rc0\nAFAIK the DynamicAttentionWrapper wraps an attention mechanism around the RNNCell. To do so, it passes around its own DynamicAttentionWrapperState in the call()\nHowever, I think that on the 0-th step, the network cannot pass such state as an argument, because the function gets called by a general decoder. (which doesnt know what cell it will encounter)\nI got the following error\nTraceback (most recent call last):\n  File \"/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/trainer_class.py\", line 54, in __init__\n    self.model = Model(self.tok_chr.dim)\n  File \"/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/models/rnn_seq2seq_tf.py\", line 164, in __init__\n    outputs, _ = dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_sl)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py\", line 530, in __call__\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\nAttributeError: 'tuple' object has no attribute 'attention'\nThe error indicates that the current state has no attribute attention. But that's the final state of the encoder, which doesn't use this syntax\nThe following snippet shows the use of the wrapper. This was coded after this explanation\nencoder_outputs, encoder_state = core_rnn.static_rnn(\n                encoder_cell, encoder_inputs, dtype=dtype, sequence_length=self.SL_enc)\n\n#Some other code\n\nattention_size = 10\nattn_obj = BahdanauAttention(num_units=attention_size,\n                                             memory=attention_states,\n                                             memory_sequence_length=self.SL_enc,\n                                             normalize=True,\n                                             name='BahdanauAttentionObject')\n\nwrapped_cell = DynamicAttentionWrapper(cell_dec_fw,\n                                                       attn_obj,\n                                                       D,\n                                                       output_attention=False,\n                                                       name='DynAttnWrap')\n\nsampler = ScheduledEmbeddingTrainingHelper(decoder_inputs,\n                                                        sequence_length=self.SL_dec,\n                                                        embedding=embedding_in,\n                                                        sampling_probability=self.samp_prob)\ndecoder = BasicDecoder(wrapped_cell,\n                                       sampler,\n                                       encoder_state)\noutputs, _ = dynamic_decode(decoder)", "body": "Using Tensorflow 1.1rc0\r\nAFAIK the [DynamicAttentionWrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py) wraps an attention mechanism around the RNNCell. To do so, it passes around its own DynamicAttentionWrapperState in the __call__()\r\n\r\nHowever, I think that on the 0-th step, the network cannot pass such state as an argument, because the function gets called by a general decoder. (which doesnt know what cell it will encounter)\r\n\r\nI got the following error\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/trainer_class.py\", line 54, in __init__\r\n    self.model = Model(self.tok_chr.dim)\r\n  File \"/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/models/rnn_seq2seq_tf.py\", line 164, in __init__\r\n    outputs, _ = dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_sl)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\r\n    decoder_finished) = decoder.step(time, inputs, state)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\r\n    cell_outputs, cell_state = self._cell(inputs, state)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py\", line 530, in __call__\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\nAttributeError: 'tuple' object has no attribute 'attention'\r\n```\r\nThe error indicates that the current state has no attribute attention. But that's the final state of the encoder, which doesn't use this syntax\r\n\r\nThe following snippet shows the use of the wrapper. This was coded after [this](https://www.tensorflow.org/versions/r1.1/api_guides/python/contrib.seq2seq#Dynamic_Decoding) explanation\r\n\r\n```python\r\nencoder_outputs, encoder_state = core_rnn.static_rnn(\r\n                encoder_cell, encoder_inputs, dtype=dtype, sequence_length=self.SL_enc)\r\n\r\n#Some other code\r\n\r\nattention_size = 10\r\nattn_obj = BahdanauAttention(num_units=attention_size,\r\n                                             memory=attention_states,\r\n                                             memory_sequence_length=self.SL_enc,\r\n                                             normalize=True,\r\n                                             name='BahdanauAttentionObject')\r\n\r\nwrapped_cell = DynamicAttentionWrapper(cell_dec_fw,\r\n                                                       attn_obj,\r\n                                                       D,\r\n                                                       output_attention=False,\r\n                                                       name='DynAttnWrap')\r\n\r\nsampler = ScheduledEmbeddingTrainingHelper(decoder_inputs,\r\n                                                        sequence_length=self.SL_dec,\r\n                                                        embedding=embedding_in,\r\n                                                        sampling_probability=self.samp_prob)\r\ndecoder = BasicDecoder(wrapped_cell,\r\n                                       sampler,\r\n                                       encoder_state)\r\noutputs, _ = dynamic_decode(decoder)\r\n```"}