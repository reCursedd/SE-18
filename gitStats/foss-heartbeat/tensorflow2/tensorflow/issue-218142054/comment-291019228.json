{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/291019228", "html_url": "https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-291019228", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833", "id": 291019228, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MTAxOTIyOA==", "user": {"login": "andrecianflone", "id": 19253511, "node_id": "MDQ6VXNlcjE5MjUzNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/19253511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrecianflone", "html_url": "https://github.com/andrecianflone", "followers_url": "https://api.github.com/users/andrecianflone/followers", "following_url": "https://api.github.com/users/andrecianflone/following{/other_user}", "gists_url": "https://api.github.com/users/andrecianflone/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrecianflone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrecianflone/subscriptions", "organizations_url": "https://api.github.com/users/andrecianflone/orgs", "repos_url": "https://api.github.com/users/andrecianflone/repos", "events_url": "https://api.github.com/users/andrecianflone/events{/privacy}", "received_events_url": "https://api.github.com/users/andrecianflone/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-02T22:18:06Z", "updated_at": "2017-04-02T22:18:06Z", "author_association": "NONE", "body_html": "<p>There's something I'm not getting, and it may be more theoretical. If the decoder's cell is an AttentionWrapper, which in turn attends to the output of the encoder RNN, why must the decoder's initial_state be an AttentionWrapperState?</p>\n<p>Also I checked for the <code>.clone(...)</code> in <code>attention_wrapper.py</code> on master but couldn't find it. I'm probably blind though.</p>", "body_text": "There's something I'm not getting, and it may be more theoretical. If the decoder's cell is an AttentionWrapper, which in turn attends to the output of the encoder RNN, why must the decoder's initial_state be an AttentionWrapperState?\nAlso I checked for the .clone(...) in attention_wrapper.py on master but couldn't find it. I'm probably blind though.", "body": "There's something I'm not getting, and it may be more theoretical. If the decoder's cell is an AttentionWrapper, which in turn attends to the output of the encoder RNN, why must the decoder's initial_state be an AttentionWrapperState?\r\n\r\nAlso I checked for the `.clone(...)` in `attention_wrapper.py` on master but couldn't find it. I'm probably blind though.  "}