{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/291036045", "html_url": "https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-291036045", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833", "id": 291036045, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MTAzNjA0NQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-03T02:28:18Z", "updated_at": "2017-04-03T02:28:18Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">You want:\n\ninit_state = attn_zero.clone(cell_state=encoder_final_state)</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Apr 2, 2017 4:11 PM, \"Andre Cianflone\" ***@***.***&gt; wrote:\n Ok, nope. I tried to set the initial cell state and attention, based on\n the other attention's zero_state method, but no go.\n\n Here's what I tried\n\n     # Attention Mechanisms. Bahdanau is additive style attention\n     attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n         num_units = mem_units, # depth of query mechanism\n         memory = attention_states, # hidden states to attend (output of RNN)\n         memory_sequence_length=seq_len_enc, # masks false memories\n         normalize=False, # normalize energy term\n         name='BahdanauAttention')\n\n     # Attention Wrapper: adds the attention mechanism to the cell\n     attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n         cell = cell,# Instance of RNNCell\n         attention_mechanism = attn_mech, # Instance of AttentionMechanism\n         attention_size = attn_units, # Int, depth of attention (output) tensor\n         attention_history=False, # whether to store history in final output\n         name=\"attention_wrapper\")\n\n     # TrainingHelper does no sampling, only uses inputs\n     helper = tf.contrib.seq2seq.TrainingHelper(\n         inputs = x, # decoder inputs\n         sequence_length = seq_len_dec, # decoder input length\n         name = \"decoder_training_helper\")\n\n     # Decoder setup\n     batch_size = tf.shape(x)[0]\n     attn_zero = attn_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n     init_state = tf.contrib.seq2seq.AttentionWrapperState(\\\n                 cell_state=encoder_state,\n                 attention=attn_zero,\n                 time=0,\n                 attention_history=())\n     decoder = tf.contrib.seq2seq.BasicDecoder(\n               cell = attn_cell,\n               helper = helper, # A Helper instance\n               initial_state = init_state, # initial state of decoder\n               output_layer = None) # instance of tf.layers.Layer, like Dense\n\n     # Perform dynamic decoding with decoder object\n     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n\n But got an error on the dynamic_decode (below). Maybe you have\n instructions on how to properly set this up?\n\n   File \"/home/andre/projects/seq2seq_drr/enc_dec.py\", line 232, in decoder_train_attn\n     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\n     swap_memory=swap_memory)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\n     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\n     pred, body, original_loop_vars, loop_vars, shape_invariants)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\n     body_result = body(*packed_vars_for_body)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\n     decoder_finished) = decoder.step(time, inputs, state)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\n     cell_outputs, cell_state = self._cell(inputs, state)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 532, in __call__\n     cell_inputs = self._cell_input_fn(inputs, state.attention)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 443, in &lt;lambda&gt;\n     lambda inputs, attention: array_ops.concat([inputs, attention], -1))\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1036, in concat\n     name=name)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n     name=name)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 464, in apply_op\n     raise TypeError(\"%s that don't all match.\" % prefix)\n TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, &lt;NOT CONVERTIBLE TO TENSOR&gt;] that don't all match.\n\n \u2014\n You are receiving this because you modified the open/close state.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"218142054\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8833\" href=\"https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-291021918\">#8833 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim59Wb4MPNiKHa_gNxHRQyMIlwWW8ks5rsCsWgaJpZM4MuICf\">https://github.com/notifications/unsubscribe-auth/ABtim59Wb4MPNiKHa_gNxHRQyMIlwWW8ks5rsCsWgaJpZM4MuICf</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "You want:\n\ninit_state = attn_zero.clone(cell_state=encoder_final_state)\n\u2026\nOn Apr 2, 2017 4:11 PM, \"Andre Cianflone\" ***@***.***> wrote:\n Ok, nope. I tried to set the initial cell state and attention, based on\n the other attention's zero_state method, but no go.\n\n Here's what I tried\n\n     # Attention Mechanisms. Bahdanau is additive style attention\n     attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n         num_units = mem_units, # depth of query mechanism\n         memory = attention_states, # hidden states to attend (output of RNN)\n         memory_sequence_length=seq_len_enc, # masks false memories\n         normalize=False, # normalize energy term\n         name='BahdanauAttention')\n\n     # Attention Wrapper: adds the attention mechanism to the cell\n     attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n         cell = cell,# Instance of RNNCell\n         attention_mechanism = attn_mech, # Instance of AttentionMechanism\n         attention_size = attn_units, # Int, depth of attention (output) tensor\n         attention_history=False, # whether to store history in final output\n         name=\"attention_wrapper\")\n\n     # TrainingHelper does no sampling, only uses inputs\n     helper = tf.contrib.seq2seq.TrainingHelper(\n         inputs = x, # decoder inputs\n         sequence_length = seq_len_dec, # decoder input length\n         name = \"decoder_training_helper\")\n\n     # Decoder setup\n     batch_size = tf.shape(x)[0]\n     attn_zero = attn_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n     init_state = tf.contrib.seq2seq.AttentionWrapperState(\\\n                 cell_state=encoder_state,\n                 attention=attn_zero,\n                 time=0,\n                 attention_history=())\n     decoder = tf.contrib.seq2seq.BasicDecoder(\n               cell = attn_cell,\n               helper = helper, # A Helper instance\n               initial_state = init_state, # initial state of decoder\n               output_layer = None) # instance of tf.layers.Layer, like Dense\n\n     # Perform dynamic decoding with decoder object\n     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n\n But got an error on the dynamic_decode (below). Maybe you have\n instructions on how to properly set this up?\n\n   File \"/home/andre/projects/seq2seq_drr/enc_dec.py\", line 232, in decoder_train_attn\n     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\n     swap_memory=swap_memory)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\n     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\n     pred, body, original_loop_vars, loop_vars, shape_invariants)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\n     body_result = body(*packed_vars_for_body)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\n     decoder_finished) = decoder.step(time, inputs, state)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\n     cell_outputs, cell_state = self._cell(inputs, state)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 532, in __call__\n     cell_inputs = self._cell_input_fn(inputs, state.attention)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 443, in <lambda>\n     lambda inputs, attention: array_ops.concat([inputs, attention], -1))\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1036, in concat\n     name=name)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n     name=name)\n   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 464, in apply_op\n     raise TypeError(\"%s that don't all match.\" % prefix)\n TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\n\n \u2014\n You are receiving this because you modified the open/close state.\n Reply to this email directly, view it on GitHub\n <#8833 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim59Wb4MPNiKHa_gNxHRQyMIlwWW8ks5rsCsWgaJpZM4MuICf>\n .", "body": "You want:\n\ninit_state = attn_zero.clone(cell_state=encoder_final_state)\n\n\n\nOn Apr 2, 2017 4:11 PM, \"Andre Cianflone\" <notifications@github.com> wrote:\n\n> Ok, nope. I tried to set the initial cell state and attention, based on\n> the other attention's zero_state method, but no go.\n>\n> Here's what I tried\n>\n>     # Attention Mechanisms. Bahdanau is additive style attention\n>     attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n>         num_units = mem_units, # depth of query mechanism\n>         memory = attention_states, # hidden states to attend (output of RNN)\n>         memory_sequence_length=seq_len_enc, # masks false memories\n>         normalize=False, # normalize energy term\n>         name='BahdanauAttention')\n>\n>     # Attention Wrapper: adds the attention mechanism to the cell\n>     attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n>         cell = cell,# Instance of RNNCell\n>         attention_mechanism = attn_mech, # Instance of AttentionMechanism\n>         attention_size = attn_units, # Int, depth of attention (output) tensor\n>         attention_history=False, # whether to store history in final output\n>         name=\"attention_wrapper\")\n>\n>     # TrainingHelper does no sampling, only uses inputs\n>     helper = tf.contrib.seq2seq.TrainingHelper(\n>         inputs = x, # decoder inputs\n>         sequence_length = seq_len_dec, # decoder input length\n>         name = \"decoder_training_helper\")\n>\n>     # Decoder setup\n>     batch_size = tf.shape(x)[0]\n>     attn_zero = attn_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n>     init_state = tf.contrib.seq2seq.AttentionWrapperState(\\\n>                 cell_state=encoder_state,\n>                 attention=attn_zero,\n>                 time=0,\n>                 attention_history=())\n>     decoder = tf.contrib.seq2seq.BasicDecoder(\n>               cell = attn_cell,\n>               helper = helper, # A Helper instance\n>               initial_state = init_state, # initial state of decoder\n>               output_layer = None) # instance of tf.layers.Layer, like Dense\n>\n>     # Perform dynamic decoding with decoder object\n>     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n>\n> But got an error on the dynamic_decode (below). Maybe you have\n> instructions on how to properly set this up?\n>\n>   File \"/home/andre/projects/seq2seq_drr/enc_dec.py\", line 232, in decoder_train_attn\n>     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\n>     swap_memory=swap_memory)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\n>     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\n>     pred, body, original_loop_vars, loop_vars, shape_invariants)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\n>     body_result = body(*packed_vars_for_body)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\n>     decoder_finished) = decoder.step(time, inputs, state)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\n>     cell_outputs, cell_state = self._cell(inputs, state)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 532, in __call__\n>     cell_inputs = self._cell_input_fn(inputs, state.attention)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 443, in <lambda>\n>     lambda inputs, attention: array_ops.concat([inputs, attention], -1))\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1036, in concat\n>     name=name)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n>     name=name)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 464, in apply_op\n>     raise TypeError(\"%s that don't all match.\" % prefix)\n> TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-291021918>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim59Wb4MPNiKHa_gNxHRQyMIlwWW8ks5rsCsWgaJpZM4MuICf>\n> .\n>\n"}