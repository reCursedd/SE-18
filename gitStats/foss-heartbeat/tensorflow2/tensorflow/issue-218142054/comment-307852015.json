{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307852015", "html_url": "https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-307852015", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833", "id": 307852015, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzg1MjAxNQ==", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-12T17:02:22Z", "updated_at": "2017-06-12T17:02:22Z", "author_association": "NONE", "body_html": "<p>For anyone else coming along, I think I found the problem*. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> 's comments are based on TensorFlow 1.2, not TensorFlow 1.1, so if you want to solve this problem, you'll need to switch. Then, you'll want to replace DynamicAttentionWrapper with AttentionWrapper.</p>\n<p>*if I'm wrong, anyone can feel free to correct me.</p>", "body_text": "For anyone else coming along, I think I found the problem*. @ebrevdo 's comments are based on TensorFlow 1.2, not TensorFlow 1.1, so if you want to solve this problem, you'll need to switch. Then, you'll want to replace DynamicAttentionWrapper with AttentionWrapper.\n*if I'm wrong, anyone can feel free to correct me.", "body": "For anyone else coming along, I think I found the problem*. @ebrevdo 's comments are based on TensorFlow 1.2, not TensorFlow 1.1, so if you want to solve this problem, you'll need to switch. Then, you'll want to replace DynamicAttentionWrapper with AttentionWrapper.\r\n\r\n*if I'm wrong, anyone can feel free to correct me."}