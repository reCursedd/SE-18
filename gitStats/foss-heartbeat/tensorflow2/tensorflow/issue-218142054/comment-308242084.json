{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308242084", "html_url": "https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-308242084", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833", "id": 308242084, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODI0MjA4NA==", "user": {"login": "oahziur", "id": 4604464, "node_id": "MDQ6VXNlcjQ2MDQ0NjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4604464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oahziur", "html_url": "https://github.com/oahziur", "followers_url": "https://api.github.com/users/oahziur/followers", "following_url": "https://api.github.com/users/oahziur/following{/other_user}", "gists_url": "https://api.github.com/users/oahziur/gists{/gist_id}", "starred_url": "https://api.github.com/users/oahziur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oahziur/subscriptions", "organizations_url": "https://api.github.com/users/oahziur/orgs", "repos_url": "https://api.github.com/users/oahziur/repos", "events_url": "https://api.github.com/users/oahziur/events{/privacy}", "received_events_url": "https://api.github.com/users/oahziur/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-13T20:41:22Z", "updated_at": "2017-06-13T20:41:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8942987\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/RylanSchaeffer\">@RylanSchaeffer</a> , if your LSTM_SIZE=64, the final state should be (2, ?, 256) tensor if you set <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\" rel=\"nofollow\"><code>state_is_tuple=False</code> in BasicLSTM</a>, since each state is 64, so a single state should have 128.</p>\n<p>I think you shouldn't do <code>final_states = tf.concat(final_states, axis=-1)</code> if <code>state_is_tuple=True</code> (the default behavior) since the return value of <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn\" rel=\"nofollow\">bidirectional_dynamic_rnn</a> will be a tuple of LSTMStateTuple instead of a tuple of Tensor. Concatenate c state and h state separately and then create a  <code>tf.contrib.rnn.LSTMStateTuple</code> to pass to the decoder should fix your problem.</p>", "body_text": "@RylanSchaeffer , if your LSTM_SIZE=64, the final state should be (2, ?, 256) tensor if you set state_is_tuple=False in BasicLSTM, since each state is 64, so a single state should have 128.\nI think you shouldn't do final_states = tf.concat(final_states, axis=-1) if state_is_tuple=True (the default behavior) since the return value of bidirectional_dynamic_rnn will be a tuple of LSTMStateTuple instead of a tuple of Tensor. Concatenate c state and h state separately and then create a  tf.contrib.rnn.LSTMStateTuple to pass to the decoder should fix your problem.", "body": "@RylanSchaeffer , if your LSTM_SIZE=64, the final state should be (2, ?, 256) tensor if you set [`state_is_tuple=False` in BasicLSTM](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell), since each state is 64, so a single state should have 128.\r\n\r\nI think you shouldn't do `final_states = tf.concat(final_states, axis=-1)` if `state_is_tuple=True` (the default behavior) since the return value of [bidirectional_dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) will be a tuple of LSTMStateTuple instead of a tuple of Tensor. Concatenate c state and h state separately and then create a  `tf.contrib.rnn.LSTMStateTuple` to pass to the decoder should fix your problem.\r\n"}