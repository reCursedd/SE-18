{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310225673", "html_url": "https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-310225673", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833", "id": 310225673, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDIyNTY3Mw==", "user": {"login": "andrecianflone", "id": 19253511, "node_id": "MDQ6VXNlcjE5MjUzNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/19253511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrecianflone", "html_url": "https://github.com/andrecianflone", "followers_url": "https://api.github.com/users/andrecianflone/followers", "following_url": "https://api.github.com/users/andrecianflone/following{/other_user}", "gists_url": "https://api.github.com/users/andrecianflone/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrecianflone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrecianflone/subscriptions", "organizations_url": "https://api.github.com/users/andrecianflone/orgs", "repos_url": "https://api.github.com/users/andrecianflone/repos", "events_url": "https://api.github.com/users/andrecianflone/events{/privacy}", "received_events_url": "https://api.github.com/users/andrecianflone/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-21T22:42:16Z", "updated_at": "2017-06-21T22:42:16Z", "author_association": "NONE", "body_html": "<p>Funny, I was playing with my code and I think I got the exact error <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8942987\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/RylanSchaeffer\">@RylanSchaeffer</a> got. Basically I was switching back and forth between GRU and LSTM which don't have the same outputs from the <code>tf.nn.bidirectional_dynamic_rnn</code> function. The problem arises from:</p>\n<pre><code>      outputs, state = tf.nn.bidirectional_dynamic_rnn(\\\n                  cell_fw=cell_fw,\n                  cell_bw=cell_bw,\n                  inputs=x,\n                  sequence_length=seq_len,\n                  initial_state_fw=init_state_fw,\n                  initial_state_bw=init_state_bw,\n                  dtype=self.float_type)\n</code></pre>\n<p>If your cells are LSTM, then <code>state</code> is a tuple of 2 LSTMStateTuple, and if your cells are GRU then <code>state</code> is a tuple of Tensor. As mentioned, for LSTMStateTuple you need to \"hack\" concat <code>state</code>. I also have to <code>set_shape</code> to the <code>state</code> Tensor otherwise you can get issues with decoding. The code below works for both GRU and LSTM:</p>\n<pre><code>      # If LSTM cell, then \"state\" is not a tuple of Tensors but an\n      # LSTMStateTuple of \"c\" and \"h\". Need to concat separately then new\n      if \"LSTMStateTuple\" in str(type(state[0])):\n        c = tf.concat([state[0][0],state[1][0]],axis=1)\n        h = tf.concat([state[0][1],state[1][1]],axis=1)\n        state = tf.contrib.rnn.LSTMStateTuple(c,h)\n      else:\n        state = tf.concat(state,1)\n        state.set_shape([None, bi_encoder_size])\n</code></pre>\n<p>To me this seems a little hackish, but works for now. I think <code>tf.nn.bidirectional_dynamic_rnn</code> should have an optional \"concat\" arg which checks in the background what type of cell was passed and concats accordingly.</p>\n<p>Cheers</p>", "body_text": "Funny, I was playing with my code and I think I got the exact error @RylanSchaeffer got. Basically I was switching back and forth between GRU and LSTM which don't have the same outputs from the tf.nn.bidirectional_dynamic_rnn function. The problem arises from:\n      outputs, state = tf.nn.bidirectional_dynamic_rnn(\\\n                  cell_fw=cell_fw,\n                  cell_bw=cell_bw,\n                  inputs=x,\n                  sequence_length=seq_len,\n                  initial_state_fw=init_state_fw,\n                  initial_state_bw=init_state_bw,\n                  dtype=self.float_type)\n\nIf your cells are LSTM, then state is a tuple of 2 LSTMStateTuple, and if your cells are GRU then state is a tuple of Tensor. As mentioned, for LSTMStateTuple you need to \"hack\" concat state. I also have to set_shape to the state Tensor otherwise you can get issues with decoding. The code below works for both GRU and LSTM:\n      # If LSTM cell, then \"state\" is not a tuple of Tensors but an\n      # LSTMStateTuple of \"c\" and \"h\". Need to concat separately then new\n      if \"LSTMStateTuple\" in str(type(state[0])):\n        c = tf.concat([state[0][0],state[1][0]],axis=1)\n        h = tf.concat([state[0][1],state[1][1]],axis=1)\n        state = tf.contrib.rnn.LSTMStateTuple(c,h)\n      else:\n        state = tf.concat(state,1)\n        state.set_shape([None, bi_encoder_size])\n\nTo me this seems a little hackish, but works for now. I think tf.nn.bidirectional_dynamic_rnn should have an optional \"concat\" arg which checks in the background what type of cell was passed and concats accordingly.\nCheers", "body": "Funny, I was playing with my code and I think I got the exact error @RylanSchaeffer got. Basically I was switching back and forth between GRU and LSTM which don't have the same outputs from the `tf.nn.bidirectional_dynamic_rnn` function. The problem arises from:\r\n\r\n```\r\n      outputs, state = tf.nn.bidirectional_dynamic_rnn(\\\r\n                  cell_fw=cell_fw,\r\n                  cell_bw=cell_bw,\r\n                  inputs=x,\r\n                  sequence_length=seq_len,\r\n                  initial_state_fw=init_state_fw,\r\n                  initial_state_bw=init_state_bw,\r\n                  dtype=self.float_type)\r\n```\r\nIf your cells are LSTM, then `state` is a tuple of 2 LSTMStateTuple, and if your cells are GRU then `state` is a tuple of Tensor. As mentioned, for LSTMStateTuple you need to \"hack\" concat `state`. I also have to `set_shape` to the `state` Tensor otherwise you can get issues with decoding. The code below works for both GRU and LSTM:\r\n\r\n```\r\n      # If LSTM cell, then \"state\" is not a tuple of Tensors but an\r\n      # LSTMStateTuple of \"c\" and \"h\". Need to concat separately then new\r\n      if \"LSTMStateTuple\" in str(type(state[0])):\r\n        c = tf.concat([state[0][0],state[1][0]],axis=1)\r\n        h = tf.concat([state[0][1],state[1][1]],axis=1)\r\n        state = tf.contrib.rnn.LSTMStateTuple(c,h)\r\n      else:\r\n        state = tf.concat(state,1)\r\n        state.set_shape([None, bi_encoder_size])\r\n```\r\nTo me this seems a little hackish, but works for now. I think `tf.nn.bidirectional_dynamic_rnn` should have an optional \"concat\" arg which checks in the background what type of cell was passed and concats accordingly.\r\n\r\nCheers"}