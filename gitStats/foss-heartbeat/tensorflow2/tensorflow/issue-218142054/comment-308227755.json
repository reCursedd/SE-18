{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308227755", "html_url": "https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-308227755", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8833", "id": 308227755, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODIyNzc1NQ==", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-13T19:47:48Z", "updated_at": "2017-06-13T19:58:06Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4604464\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oahziur\">@oahziur</a> , thank you! My decoder is not a MultiRNNCell - it's just one BasicLSTMCell with an AttentionWrapper. Here is my method I use to create an LSTM cell:</p>\n<pre><code>@staticmethod  \ndef _create_lstm_cell():  \n    return BasicLSTMCell(LSTM_SIZE)\n</code></pre>\n<p>Here is my function call to wrap the cell with a Bahdanau Attention mechanism:</p>\n<pre><code>attention_cell = AttentionWrapper(cell=self._create_lstm_cell(),\n                           attention_mechanism=bahdanau_attention)\n</code></pre>\n<p>And finally here is how I pass the attention cell <code>BasicDecoder</code>:</p>\n<pre><code>decoder = BasicDecoder(cell=attention_cell,\n                  helper=training_helper,\n                  initial_state=init_state)\n</code></pre>\n<p>My <code>encoder_final_states</code> has type <code>&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;</code> and <code>shape=(2, ?, 128)</code>; this makes sense as I'm using a Bidirectional RNN and my <code>LSTM_SIZE = 64</code>. <code>encoder_final_states</code> one of the two values returned by the following method:</p>\n<pre><code>def _define_encoder(self):\n    with tf.name_scope('define_encoder'):\n        outputs, final_states = bidirectional_dynamic_rnn(cell_fw=self._create_lstm_cell(),\n                                              cell_bw=self._create_lstm_cell(),\n                                              inputs=self.x,\n                                              dtype=tf.float32)\n        outputs = tf.concat(outputs, axis=-1)\n        final_states = tf.concat(final_states, axis=-1)\n\n    return outputs, final_states\n</code></pre>\n<p>Sorry about the formatting. I tried to clean it up best I could.</p>", "body_text": "@oahziur , thank you! My decoder is not a MultiRNNCell - it's just one BasicLSTMCell with an AttentionWrapper. Here is my method I use to create an LSTM cell:\n@staticmethod  \ndef _create_lstm_cell():  \n    return BasicLSTMCell(LSTM_SIZE)\n\nHere is my function call to wrap the cell with a Bahdanau Attention mechanism:\nattention_cell = AttentionWrapper(cell=self._create_lstm_cell(),\n                           attention_mechanism=bahdanau_attention)\n\nAnd finally here is how I pass the attention cell BasicDecoder:\ndecoder = BasicDecoder(cell=attention_cell,\n                  helper=training_helper,\n                  initial_state=init_state)\n\nMy encoder_final_states has type <class 'tensorflow.python.framework.ops.Tensor'> and shape=(2, ?, 128); this makes sense as I'm using a Bidirectional RNN and my LSTM_SIZE = 64. encoder_final_states one of the two values returned by the following method:\ndef _define_encoder(self):\n    with tf.name_scope('define_encoder'):\n        outputs, final_states = bidirectional_dynamic_rnn(cell_fw=self._create_lstm_cell(),\n                                              cell_bw=self._create_lstm_cell(),\n                                              inputs=self.x,\n                                              dtype=tf.float32)\n        outputs = tf.concat(outputs, axis=-1)\n        final_states = tf.concat(final_states, axis=-1)\n\n    return outputs, final_states\n\nSorry about the formatting. I tried to clean it up best I could.", "body": "@oahziur , thank you! My decoder is not a MultiRNNCell - it's just one BasicLSTMCell with an AttentionWrapper. Here is my method I use to create an LSTM cell:\r\n\r\n```\r\n@staticmethod  \r\ndef _create_lstm_cell():  \r\n    return BasicLSTMCell(LSTM_SIZE)\r\n```\r\n\r\nHere is my function call to wrap the cell with a Bahdanau Attention mechanism:\r\n\r\n```\r\nattention_cell = AttentionWrapper(cell=self._create_lstm_cell(),\r\n                           attention_mechanism=bahdanau_attention)\r\n```\r\n\r\nAnd finally here is how I pass the attention cell `BasicDecoder`:\r\n\r\n```\r\ndecoder = BasicDecoder(cell=attention_cell,\r\n                  helper=training_helper,\r\n                  initial_state=init_state)\r\n```\r\n\r\n\r\nMy `encoder_final_states` has type `<class 'tensorflow.python.framework.ops.Tensor'>` and `shape=(2, ?, 128)`; this makes sense as I'm using a Bidirectional RNN and my `LSTM_SIZE = 64`. `encoder_final_states` one of the two values returned by the following method:\r\n\r\n\r\n```\r\ndef _define_encoder(self):\r\n    with tf.name_scope('define_encoder'):\r\n        outputs, final_states = bidirectional_dynamic_rnn(cell_fw=self._create_lstm_cell(),\r\n                                              cell_bw=self._create_lstm_cell(),\r\n                                              inputs=self.x,\r\n                                              dtype=tf.float32)\r\n        outputs = tf.concat(outputs, axis=-1)\r\n        final_states = tf.concat(final_states, axis=-1)\r\n\r\n    return outputs, final_states\r\n```\r\n\r\nSorry about the formatting. I tried to clean it up best I could."}