{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/92246661", "pull_request_review_id": 12758647, "id": 92246661, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDkyMjQ2NjYx", "diff_hunk": "@@ -0,0 +1,89 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Wrappers for efficient GPU layer_norm_fused operations.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.contrib.util import loader\n+from tensorflow.python.framework import ops\n+from tensorflow.python.platform import resource_loader\n+\n+import tensorflow as tf\n+\n+_layer_norm_fused_op = loader.load_op_library(\n+    resource_loader.get_path_to_datafile(\"_layer_norm_fused_op.so\"))\n+\n+@ops.RegisterGradient(\"LayerNormCustom\")\n+def _LayerNormCustomGrad(op, grad):\n+    return [_layer_norm_fused_op.layer_norm_backprop_custom(\n+        op.inputs[0], grad, op.get_attr(\"epsilon\"))]\n+\n+\n+@ops.RegisterGradient(\"LayerNormBiasAddCustom\")\n+def _LayerNormBiasAddCustomGrad(op, grad):\n+    in_back, beta_back = _layer_norm_fused_op.layer_norm_bias_add_backprop_custom(\n+        op.inputs[0], grad, op.inputs[1],\n+        op.get_attr(\"epsilon\"))\n+    return [in_back, beta_back]\n+\n+\n+@ops.RegisterGradient(\"LayerNormFusedCustom\")\n+def _LayerNormFusedCustomGrad(op, grad):\n+    in_back, gamma_back, beta_back = _layer_norm_fused_op.layer_norm_fused_backprop_custom(\n+        op.inputs[0], grad, op.inputs[1],\n+        op.get_attr(\"epsilon\"))\n+    return [in_back, gamma_back, beta_back]\n+\n+\n+def layer_norm_fused_op(input_tensor, gamma=None, beta=None,\n+                     epsilon=1e-12, name=None):\n+    \"\"\"Fast and efficient layer normalization along the last dimension\n+\n+    See layer_norm_fused_op.cc for more details.\n+\n+    Args:\n+      input_tensor: A `Tensor` which will be normalized.\n+      gamma: 1-D tensor for scaling after normalization.\n+             Must be the same size as the last dimension of input_tensor.\n+             Will be omitted if is None.\n+      beta: 1-D tensor for centering after normalization.\n+            Must be the same size as the last dimension of input_tensor.\n+             Will be omitted if is None.\n+      epsilon: small number added before variance calculation\n+              to avoid division by zero.\n+\n+    Returns:\n+      A normalized `Tensor` with same dtype and shape as the input_tensor.\n+    \"\"\"\n+    if epsilon <= 0:\n+        print(\"WARNING: epsilon <=0, may result in NaN outputs.\")", "path": "tensorflow/contrib/layers/python/ops/layer_norm_fused_op.py", "position": null, "original_position": 72, "commit_id": "c956265da134875e8e79bc12cc2223deaceea87c", "original_commit_id": "75883a49e704a93131e94a935906aa14b88cb4d0", "user": {"login": "ilblackdragon", "id": 175486, "node_id": "MDQ6VXNlcjE3NTQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/175486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilblackdragon", "html_url": "https://github.com/ilblackdragon", "followers_url": "https://api.github.com/users/ilblackdragon/followers", "following_url": "https://api.github.com/users/ilblackdragon/following{/other_user}", "gists_url": "https://api.github.com/users/ilblackdragon/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilblackdragon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilblackdragon/subscriptions", "organizations_url": "https://api.github.com/users/ilblackdragon/orgs", "repos_url": "https://api.github.com/users/ilblackdragon/repos", "events_url": "https://api.github.com/users/ilblackdragon/events{/privacy}", "received_events_url": "https://api.github.com/users/ilblackdragon/received_events", "type": "User", "site_admin": false}, "body": "use `logging.warn(...)`. also when do argument checking, please also report the value (e.g. `logging.warn(\"epsilon is %f <= ...\", epsilon)`)", "created_at": "2016-12-13T19:27:29Z", "updated_at": "2017-03-09T00:28:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r92246661", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/92246661"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r92246661"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205"}}, "body_html": "<p>use <code>logging.warn(...)</code>. also when do argument checking, please also report the value (e.g. <code>logging.warn(\"epsilon is %f &lt;= ...\", epsilon)</code>)</p>", "body_text": "use logging.warn(...). also when do argument checking, please also report the value (e.g. logging.warn(\"epsilon is %f <= ...\", epsilon))"}