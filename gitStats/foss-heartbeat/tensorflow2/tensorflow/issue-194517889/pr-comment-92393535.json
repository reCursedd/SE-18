{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/92393535", "pull_request_review_id": 12907224, "id": 92393535, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDkyMzkzNTM1", "diff_hunk": "@@ -0,0 +1,344 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#define EIGEN_USE_THREADS\n+\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"./layer_norm_fused_op.h\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename Device, typename T>\n+struct LaunchLayerNormOp;\n+\n+template <typename T>\n+struct LayerNormGPULaunch {\n+  static void Run(const GPUDevice& d, const LayerNormFusedArgs args, const T* input,\n+                   T* output);\n+};\n+template <typename T>\n+struct LaunchLayerNormOp<GPUDevice, T> {\n+  static void launch(OpKernelContext* ctx, const LayerNormFusedArgs args,\n+                     const T* input,  T* output\n+                     ) {\n+\n+    const GPUDevice& d = ctx->eigen_device<GPUDevice>();\n+    LayerNormGPULaunch<T>().Run(d, args, input, output);\n+\n+  }\n+};\n+\n+\n+template <typename Device, typename T>\n+class LayerNormOp : public OpKernel {\n+\n+ public:\n+\n+  explicit LayerNormOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon_));\n+\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+\n+    const Tensor& input = context->input(0);\n+\n+    OP_REQUIRES(context, input.dims() >= 2,\n+                errors::InvalidArgument(\"input dimensions must be larger than 2D\",\n+                                        input.shape().DebugString()));\n+\n+\n+    const int32 last_dim = input.dims()-1;\n+    const int32 depth = input.dim_size(last_dim);\n+\n+    int32 n_slices = 1;\n+    for (int i = 0; i < last_dim; ++i)\n+    {\n+        n_slices *= input.dim_size(i);\n+    }\n+\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context, context->allocate_output(0, input.shape(), &output));\n+\n+    VLOG(2) << \"LayerNormCustom: \"\n+            << \"depth:\"<<depth << \", \" <<\"n_slices:\"<<n_slices;\n+\n+    //temporarily hard-coding warp_size for CUDA kernel.\n+    const int warp_size = 32;\n+    LayerNormFusedArgs args;\n+    args.depth = depth;\n+    args.n_slices = n_slices;\n+    args.n_inputs = n_slices*depth;\n+    args.epsilon = epsilon_;\n+\n+    if (depth<=warp_size)\n+    {\n+        int tmp_depth = depth;\n+        int slice_size = 1;\n+        while(tmp_depth >>=1)slice_size*=2;\n+        args.slice_size = slice_size>=depth?slice_size:slice_size*2;\n+    }else{\n+        int slice_size = (depth/warp_size)*warp_size;\n+        args.slice_size = slice_size>=depth?slice_size:slice_size+warp_size;\n+    }\n+    auto input_ptr = input.template flat<T>().data();\n+    auto output_ptr = output->template flat<T>().data();\n+\n+    LaunchLayerNormOp<Device, T>::launch(context, args, input_ptr,output_ptr);\n+  }\n+\n+ private:\n+  float epsilon_;\n+\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(LayerNormOp);\n+};\n+\n+template <typename Device, typename T>\n+struct LaunchLayerNormBiasAddOp;\n+\n+template <typename T>\n+struct LayerNormBiasAddGPULaunch {\n+  static void Run(const GPUDevice& d, const LayerNormFusedArgs args, const T* input,\n+                   const T* beta,T* output);\n+};\n+template <typename T>\n+struct LaunchLayerNormBiasAddOp<GPUDevice, T> {\n+  static void launch(OpKernelContext* ctx, const LayerNormFusedArgs args,\n+                     const T* input,const T* beta,  T* output\n+                     ) {\n+    const GPUDevice& d = ctx->eigen_device<GPUDevice>();\n+    LayerNormBiasAddGPULaunch<T>().Run(d, args, input,beta, output);\n+  }\n+};\n+\n+\n+template <typename Device, typename T>\n+class LayerNormBiasAddOp : public OpKernel {\n+\n+ public:\n+\n+  explicit LayerNormBiasAddOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon_));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+\n+    const Tensor& input = context->input(0);\n+    const Tensor& beta = context->input(1);\n+\n+    OP_REQUIRES(context, input.dims() >= 2,\n+                errors::InvalidArgument(\"input dimensions must be larger than 2D\",\n+                                        input.shape().DebugString()));\n+    OP_REQUIRES(context, beta.dims() == 1,\n+                errors::InvalidArgument(\"beta dimension must be 1D\",\n+                                        beta.shape().DebugString()));\n+\n+    const int32 last_dim = input.dims()-1;\n+    const int32 depth = input.dim_size(last_dim);\n+\n+    OP_REQUIRES(\n+        context, depth == beta.dim_size(0),\n+        errors::InvalidArgument(\"input depth and beta must have the same size: \",\n+                                depth, \" vs \", beta.dim_size(0)));\n+\n+    int32 n_slices = 1;\n+    for (int i = 0; i < last_dim; ++i)\n+    {\n+        n_slices *= input.dim_size(i);\n+    }\n+\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context, context->allocate_output(0, input.shape(), &output));\n+\n+    VLOG(2) << \"LayerNormBiasAddCustom: \"\n+            << \"depth:\"<<depth << \", \" <<\"n_slices:\"<<n_slices;\n+\n+    //temporarily hard-coding warp_size for CUDA kernel.\n+    const int warp_size = 32;\n+    LayerNormFusedArgs args;\n+    args.depth = depth;\n+    args.n_slices = n_slices;\n+    args.n_inputs = n_slices*depth;\n+    args.epsilon = epsilon_;\n+\n+    if (depth<=warp_size)\n+    {\n+        int tmp_depth = depth;\n+        int slice_size = 1;\n+        while(tmp_depth >>=1)slice_size*=2;\n+        args.slice_size = slice_size>=depth?slice_size:slice_size*2;\n+    }else{\n+        int slice_size = (depth/warp_size)*warp_size;\n+        args.slice_size = slice_size>=depth?slice_size:slice_size+warp_size;\n+    }\n+    auto input_ptr = input.template flat<T>().data();\n+    auto beta_ptr = beta.template flat<T>().data();\n+    auto output_ptr = output->template flat<T>().data();\n+\n+    LaunchLayerNormBiasAddOp<Device, T>::launch(context, args, input_ptr,beta_ptr,\n+                                             output_ptr\n+                                             );\n+  }\n+\n+ private:\n+  float epsilon_;\n+\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(LayerNormBiasAddOp);\n+};\n+\n+template <typename Device, typename T>\n+struct LaunchLayerNormFusedOp;\n+\n+template <typename T>\n+struct LayerNormFusedGPULaunch {\n+  static void Run(const GPUDevice& d, const LayerNormFusedArgs args, const T* input,\n+                   const T* gamma,const T* beta,T* output);\n+};\n+template <typename T>\n+struct LaunchLayerNormFusedOp<GPUDevice, T> {\n+  static void launch(OpKernelContext* ctx, const LayerNormFusedArgs args,\n+                     const T* input,const T* gamma,const T* beta,  T* output\n+                     ) {\n+    const GPUDevice& d = ctx->eigen_device<GPUDevice>();\n+    LayerNormFusedGPULaunch<T>().Run(d, args, input,gamma,beta, output);\n+  }\n+};\n+\n+\n+template <typename Device, typename T>\n+class LayerNormFusedOp : public OpKernel {\n+\n+ public:\n+\n+  explicit LayerNormFusedOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon_));\n+\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+\n+    const Tensor& input = context->input(0);\n+    const Tensor& gamma = context->input(1);\n+    const Tensor& beta = context->input(2);\n+\n+    OP_REQUIRES(context, input.dims() >= 2,\n+                errors::InvalidArgument(\"input dimensions must be larger than 2D\",\n+                                        input.shape().DebugString()));\n+    OP_REQUIRES(context, gamma.dims() == 1,\n+                errors::InvalidArgument(\"gamma dimension must be 1D\",\n+                                        gamma.shape().DebugString()));\n+    OP_REQUIRES(context, beta.dims() == 1,\n+                errors::InvalidArgument(\"beta dimension must be 1D\",\n+                                        beta.shape().DebugString()));\n+\n+    const int32 last_dim = input.dims()-1;\n+    const int32 depth = input.dim_size(last_dim);\n+\n+    OP_REQUIRES(\n+        context, depth == gamma.dim_size(0),\n+        errors::InvalidArgument(\"input depth and gamma must have the same size: \",\n+                                depth, \" vs \", gamma.dim_size(0)));\n+    OP_REQUIRES(\n+        context, depth == beta.dim_size(0),\n+        errors::InvalidArgument(\"input depth and beta must have the same size: \",\n+                                depth, \" vs \", beta.dim_size(0)));\n+\n+    int32 n_slices = 1;\n+    for (int i = 0; i < last_dim; ++i)\n+    {\n+        n_slices *= input.dim_size(i);\n+    }\n+\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context, context->allocate_output(0, input.shape(), &output));\n+\n+    VLOG(2) << \"LayerNormFusedCustom: \"\n+            << \"depth:\"<<depth << \", \" <<\"n_slices:\"<<n_slices;\n+\n+    //temporarily hard-coding warp_size for CUDA kernel.\n+    const int warp_size = 32;\n+    LayerNormFusedArgs args;\n+    args.depth = depth;\n+    args.n_slices = n_slices;\n+    args.n_inputs = n_slices*depth;\n+    args.epsilon = epsilon_;\n+\n+    if (depth<=warp_size)\n+    {\n+        int tmp_depth = depth;\n+        int slice_size = 1;\n+        while(tmp_depth >>=1)slice_size*=2;\n+        args.slice_size = slice_size>=depth?slice_size:slice_size*2;\n+    }else{\n+        int slice_size = (depth/warp_size)*warp_size;\n+        args.slice_size = slice_size>=depth?slice_size:slice_size+warp_size;\n+    }\n+    auto input_ptr = input.template flat<T>().data();\n+    auto gamma_ptr = gamma.template flat<T>().data();\n+    auto beta_ptr = beta.template flat<T>().data();\n+    auto output_ptr = output->template flat<T>().data();\n+\n+    LaunchLayerNormFusedOp<Device, T>::launch(context, args, input_ptr,gamma_ptr,beta_ptr,\n+                                             output_ptr\n+                                             );\n+  }\n+\n+ private:\n+  float epsilon_;\n+\n+\n+  TF_DISALLOW_COPY_AND_ASSIGN(LayerNormFusedOp);\n+};\n+\n+\n+\n+\n+\n+REGISTER_KERNEL_BUILDER(", "path": "tensorflow/contrib/layers/ln_kernels/layer_norm_fused_op.cc", "position": null, "original_position": 317, "commit_id": "c956265da134875e8e79bc12cc2223deaceea87c", "original_commit_id": "75883a49e704a93131e94a935906aa14b88cb4d0", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "body": "Yes! I removed them so I could compile the kernel without using bazel and forgot to add them back before I send the pull request.", "created_at": "2016-12-14T13:38:44Z", "updated_at": "2017-03-09T00:28:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r92393535", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/92393535"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r92393535"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205"}}, "body_html": "<p>Yes! I removed them so I could compile the kernel without using bazel and forgot to add them back before I send the pull request.</p>", "body_text": "Yes! I removed them so I could compile the kernel without using bazel and forgot to add them back before I send the pull request.", "in_reply_to_id": 92264050}