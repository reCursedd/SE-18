{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267243820", "html_url": "https://github.com/tensorflow/tensorflow/pull/6205#issuecomment-267243820", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6205", "id": 267243820, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzI0MzgyMA==", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-15T05:40:18Z", "updated_at": "2016-12-15T05:40:18Z", "author_association": "NONE", "body_html": "<p>I have run clang-format on the c++ codes using <code>-style=Google</code>, renamed a few variables for clarity, and fixed the issues pointed out by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=175486\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ilblackdragon\">@ilblackdragon</a> that I didn't have questions about.</p>\n<p>I also tried to merge the 3 separate ops into one, but to do that, I would have to pass <code>None</code> into the ops from python (for the cases where gamma and beta are not needed). I searched through the core kernels but couldn't find examples doing this, I have thought of two ways of handling this:</p>\n<ol>\n<li>pass <code>False</code> when gamma or beta is <code>None</code> and add boolean as supported dtype in <code>REGISTER_OP</code>. Then I could just handle different cases in c++ codes.</li>\n<li>Eliminate <code>LayerNormCustom</code> and <code>LayerNormBiasAddCustom</code>, then just pass in dummy constant tensors into the op like I am currently doing when gamma is not <code>None</code> but beta is.</li>\n</ol>\n<p>option 1 is a bit hacky and will probably not reduce too much code, since most code duplication happens in <code>layer_norm_fused_op_gpu.cu.cc</code> and I am not sure how to merge the three kernels into one without performance hit.<br>\noption 2 will reduce the code by 3-fold, but will increase the run-time during backward pass by about 1.5-2x  when beta and gamma are not needed because of the unnecessary atomic adds. However, this performance hit will be a lot less significant in the context of entire model, since it is still a lot faster than most time-consuming ops like convolution and matmul.</p>\n<p>Both options have some drawbacks, and I would like to know if you guys have any better suggestions to tackle this problem.</p>", "body_text": "I have run clang-format on the c++ codes using -style=Google, renamed a few variables for clarity, and fixed the issues pointed out by @ilblackdragon that I didn't have questions about.\nI also tried to merge the 3 separate ops into one, but to do that, I would have to pass None into the ops from python (for the cases where gamma and beta are not needed). I searched through the core kernels but couldn't find examples doing this, I have thought of two ways of handling this:\n\npass False when gamma or beta is None and add boolean as supported dtype in REGISTER_OP. Then I could just handle different cases in c++ codes.\nEliminate LayerNormCustom and LayerNormBiasAddCustom, then just pass in dummy constant tensors into the op like I am currently doing when gamma is not None but beta is.\n\noption 1 is a bit hacky and will probably not reduce too much code, since most code duplication happens in layer_norm_fused_op_gpu.cu.cc and I am not sure how to merge the three kernels into one without performance hit.\noption 2 will reduce the code by 3-fold, but will increase the run-time during backward pass by about 1.5-2x  when beta and gamma are not needed because of the unnecessary atomic adds. However, this performance hit will be a lot less significant in the context of entire model, since it is still a lot faster than most time-consuming ops like convolution and matmul.\nBoth options have some drawbacks, and I would like to know if you guys have any better suggestions to tackle this problem.", "body": "I have run clang-format on the c++ codes using `-style=Google`, renamed a few variables for clarity, and fixed the issues pointed out by @ilblackdragon that I didn't have questions about.\r\n\r\nI also tried to merge the 3 separate ops into one, but to do that, I would have to pass `None` into the ops from python (for the cases where gamma and beta are not needed). I searched through the core kernels but couldn't find examples doing this, I have thought of two ways of handling this:\r\n1. pass `False` when gamma or beta is `None` and add boolean as supported dtype in `REGISTER_OP`. Then I could just handle different cases in c++ codes.\r\n2. Eliminate `LayerNormCustom` and `LayerNormBiasAddCustom`, then just pass in dummy constant tensors into the op like I am currently doing when gamma is not `None` but beta is.\r\n\r\noption 1 is a bit hacky and will probably not reduce too much code, since most code duplication happens in `layer_norm_fused_op_gpu.cu.cc` and I am not sure how to merge the three kernels into one without performance hit.\r\noption 2 will reduce the code by 3-fold, but will increase the run-time during backward pass by about 1.5-2x  when beta and gamma are not needed because of the unnecessary atomic adds. However, this performance hit will be a lot less significant in the context of entire model, since it is still a lot faster than most time-consuming ops like convolution and matmul. \r\n\r\nBoth options have some drawbacks, and I would like to know if you guys have any better suggestions to tackle this problem."}