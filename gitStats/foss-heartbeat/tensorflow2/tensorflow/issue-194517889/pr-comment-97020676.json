{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/97020676", "pull_request_review_id": 17621338, "id": 97020676, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk3MDIwNjc2", "diff_hunk": "@@ -0,0 +1,1455 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include <math.h>\n+#include <stdio.h>\n+#include <algorithm>\n+\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+\n+#include \"./layer_norm_fused_op.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+#if !defined(_MSC_VER)\n+#define UNROLL _Pragma(\"unroll\")\n+#else\n+#define UNROLL\n+#endif\n+\n+#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 600\n+#else\n+__device__ double atomicAdd(double* a, double b) { return b; }\n+#endif\n+\n+#define MAX_GRID_SIZE 480\n+inline int get_num_blocks(const int n_slices, const int slice_per_block) {\n+  const int _num_blocks = n_slices / slice_per_block;\n+  if (_num_blocks * slice_per_block == n_slices)\n+    return _num_blocks;\n+  else\n+    return _num_blocks + 1;\n+}\n+\n+inline int get_block_size(const int slice_size, int& block_size, int& mult) {\n+  const int _warp_size = 32;\n+  int _block_size = _warp_size;\n+  int _mult = slice_size / _block_size;\n+  mult = _mult * _block_size >= slice_size ? _mult : _mult + 1;\n+  while (mult > 5) {\n+    _block_size += _warp_size;\n+    _mult = slice_size / _block_size;\n+    mult = _mult * _block_size >= slice_size ? _mult : _mult + 1;\n+  }", "path": "tensorflow/contrib/layers/kernels/layer_norm_fused_op_gpu.cu.cc", "position": null, "original_position": 54, "commit_id": "c956265da134875e8e79bc12cc2223deaceea87c", "original_commit_id": "6fd112700dabe27bb85c3d35eca59c971d7ff985", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "body": "Yes... I stuck on this part for a long time, but eventually compromised on this solution, because most of my use cases had slice_size <= 1500, so the loop will not go beyond 10 steps. Also, I thought the  extra time needed for this loop would be many times shorter than the kernel itself, so I was a bit reluctant on optimizing this part.", "created_at": "2017-01-20T06:49:37Z", "updated_at": "2017-03-09T00:28:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r97020676", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/97020676"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r97020676"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205"}}, "body_html": "<p>Yes... I stuck on this part for a long time, but eventually compromised on this solution, because most of my use cases had slice_size &lt;= 1500, so the loop will not go beyond 10 steps. Also, I thought the  extra time needed for this loop would be many times shorter than the kernel itself, so I was a bit reluctant on optimizing this part.</p>", "body_text": "Yes... I stuck on this part for a long time, but eventually compromised on this solution, because most of my use cases had slice_size <= 1500, so the loop will not go beyond 10 steps. Also, I thought the  extra time needed for this loop would be many times shorter than the kernel itself, so I was a bit reluctant on optimizing this part.", "in_reply_to_id": 96956741}