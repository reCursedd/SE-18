{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/96956123", "pull_request_review_id": 17533815, "id": 96956123, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk2OTU2MTIz", "diff_hunk": "@@ -0,0 +1,336 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#define EIGEN_USE_THREADS\n+\n+#include \"./layer_norm_fused_op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+\n+// temporarily hard coding warp_size for CUDA kernels.\n+#define WARP_SIZE 32\n+namespace tensorflow {\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename Device, typename T>\n+struct LaunchLayerNormBackpropOp;\n+\n+template <typename Device, typename T>\n+struct LaunchLayerNormBiasAddBackpropOp;\n+\n+template <typename Device, typename T>\n+struct LaunchLayerNormFusedBackpropOp;\n+\n+#if GOOGLE_CUDA\n+template <typename T>\n+struct LayerNormBackpropGPULaunch {\n+  static void Run(const GPUDevice& d, const LayerNormFusedArgs args,\n+                  const T* input, const T* out_back, T* in_back);\n+};\n+template <typename T>\n+struct LaunchLayerNormBackpropOp<GPUDevice, T> {\n+  static void launch(OpKernelContext* ctx, const LayerNormFusedArgs args,\n+                     const T* input, const T* out_back, T* in_back) {\n+    const GPUDevice& d = ctx->eigen_device<GPUDevice>();\n+    LayerNormBackpropGPULaunch<T>().Run(d, args, input, out_back, in_back);\n+  }\n+};\n+\n+template <typename T>\n+struct LayerNormBiasAddBackpropGPULaunch {\n+  static void Run(const GPUDevice& d, const LayerNormFusedArgs args,\n+                  const T* input, const T* out_back, T* in_back, T* beta_back);\n+};\n+template <typename T>\n+struct LaunchLayerNormBiasAddBackpropOp<GPUDevice, T> {\n+  static void launch(OpKernelContext* ctx, const LayerNormFusedArgs args,\n+                     const T* input, const T* out_back, T* in_back,\n+                     T* beta_back) {\n+    const GPUDevice& d = ctx->eigen_device<GPUDevice>();\n+    LayerNormBiasAddBackpropGPULaunch<T>().Run(d, args, input, out_back,\n+                                               in_back, beta_back);\n+  }\n+};\n+\n+template <typename T>\n+struct LayerNormFusedBackpropGPULaunch {\n+  static void Run(const GPUDevice& d, const LayerNormFusedArgs args,\n+                  const T* input, const T* out_back, const T* gamma, T* in_back,\n+                  T* gamma_back, T* beta_back);\n+};\n+template <typename T>\n+struct LaunchLayerNormFusedBackpropOp<GPUDevice, T> {\n+  static void launch(OpKernelContext* ctx, const LayerNormFusedArgs args,\n+                     const T* input, const T* out_back, const T* gamma,\n+                     T* in_back, T* gamma_back, T* beta_back) {\n+    const GPUDevice& d = ctx->eigen_device<GPUDevice>();\n+    LayerNormFusedBackpropGPULaunch<T>().Run(d, args, input, out_back, gamma,\n+                                             in_back, gamma_back, beta_back);\n+  }\n+};\n+#endif  // GOOGLE_CUDA\n+\n+template <typename Device, typename T>\n+class LayerNormBackpropOp : public OpKernel {\n+ public:\n+  explicit LayerNormBackpropOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"epsilon\", &epsilon_));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    const Tensor& input = context->input(0);\n+    const Tensor& out_back = context->input(1);\n+\n+    OP_REQUIRES(\n+        context, input.dims() >= 2,\n+        errors::InvalidArgument(\"input dimensions must be larger than 2D\",\n+                                input.shape().DebugString()));\n+\n+    const int32 last_dim = input.dims() - 1;\n+    const int32 depth = input.dim_size(last_dim);\n+\n+    int32 n_slices = 1;\n+    for (int i = 0; i < last_dim; ++i) {\n+      n_slices *= input.dim_size(i);\n+    }\n+\n+    Tensor* in_back = nullptr;\n+    OP_REQUIRES_OK(context,\n+                   context->allocate_output(0, input.shape(), &in_back));\n+\n+    VLOG(2) << \"LayerNormBackpropCustom: \"\n+            << \"depth:\" << depth << \", \"\n+            << \"n_slices:\" << n_slices;\n+\n+    LayerNormFusedArgs args;\n+    args.depth = depth;\n+    args.n_slices = n_slices;\n+    args.n_inputs = n_slices * depth;\n+    args.epsilon = epsilon_;\n+\n+    if (depth <= WARP_SIZE) {\n+      int tmp_depth = depth;\n+      int slice_size = 1;\n+      while (tmp_depth >>= 1) slice_size *= 2;\n+      args.slice_size = slice_size >= depth ? slice_size : slice_size * 2;\n+    } else {\n+      int slice_size = (depth / WARP_SIZE) * WARP_SIZE;\n+      args.slice_size =\n+          slice_size >= depth ? slice_size : slice_size + WARP_SIZE;", "path": "tensorflow/contrib/layers/kernels/layer_norm_fused_grad_op.cc", "position": null, "original_position": 132, "commit_id": "c956265da134875e8e79bc12cc2223deaceea87c", "original_commit_id": "6fd112700dabe27bb85c3d35eca59c971d7ff985", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "body": "The typical way to write this is: \r\n\r\n`\r\nslice_size = (depth + WARP_SIZE -1 ) / WARP_SIZE * WARP_SIZE\r\n`\r\n\r\nor\r\n\r\n`\r\nslice_size = Eigen::divup(depth, WARP_SIZE) * WARP_SIZE\r\n`\r\n", "created_at": "2017-01-19T20:58:32Z", "updated_at": "2017-03-09T00:28:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r96956123", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/96956123"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r96956123"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205"}}, "body_html": "<p>The typical way to write this is:</p>\n<p><code>slice_size = (depth + WARP_SIZE -1 ) / WARP_SIZE * WARP_SIZE</code></p>\n<p>or</p>\n<p><code>slice_size = Eigen::divup(depth, WARP_SIZE) * WARP_SIZE</code></p>", "body_text": "The typical way to write this is:\nslice_size = (depth + WARP_SIZE -1 ) / WARP_SIZE * WARP_SIZE\nor\nslice_size = Eigen::divup(depth, WARP_SIZE) * WARP_SIZE"}