{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/96958343", "pull_request_review_id": 17533815, "id": 96958343, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk2OTU4MzQz", "diff_hunk": "@@ -0,0 +1,1455 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include <math.h>\n+#include <stdio.h>\n+#include <algorithm>\n+\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+\n+#include \"./layer_norm_fused_op.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+#if !defined(_MSC_VER)\n+#define UNROLL _Pragma(\"unroll\")\n+#else\n+#define UNROLL\n+#endif\n+\n+#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 600\n+#else\n+__device__ double atomicAdd(double* a, double b) { return b; }\n+#endif\n+\n+#define MAX_GRID_SIZE 480\n+inline int get_num_blocks(const int n_slices, const int slice_per_block) {\n+  const int _num_blocks = n_slices / slice_per_block;\n+  if (_num_blocks * slice_per_block == n_slices)\n+    return _num_blocks;\n+  else\n+    return _num_blocks + 1;\n+}\n+\n+inline int get_block_size(const int slice_size, int& block_size, int& mult) {\n+  const int _warp_size = 32;\n+  int _block_size = _warp_size;\n+  int _mult = slice_size / _block_size;\n+  mult = _mult * _block_size >= slice_size ? _mult : _mult + 1;\n+  while (mult > 5) {\n+    _block_size += _warp_size;\n+    _mult = slice_size / _block_size;\n+    mult = _mult * _block_size >= slice_size ? _mult : _mult + 1;\n+  }\n+  block_size = _block_size;\n+  return 0;\n+}\n+\n+template <typename T>\n+__global__ void fillZeros(T* __restrict__ output, const int n_inputs) {\n+  for (int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n+       thread_id < n_inputs; thread_id += blockDim.x * gridDim.x)\n+    output[thread_id] = static_cast<T>(0.0f);\n+}\n+\n+template <typename T>\n+__device__ __inline__ void warpSum(T& val1, T& val2) {\n+  val1 += __shfl_xor(val1, 16);\n+  val2 += __shfl_xor(val2, 16);\n+  val1 += __shfl_xor(val1, 8);\n+  val2 += __shfl_xor(val2, 8);\n+  val1 += __shfl_xor(val1, 4);\n+  val2 += __shfl_xor(val2, 4);\n+  val1 += __shfl_xor(val1, 2);\n+  val2 += __shfl_xor(val2, 2);\n+  val1 += __shfl_xor(val1, 1);\n+  val2 += __shfl_xor(val2, 1);\n+}\n+\n+template <typename T>\n+__device__ __inline__ T get_value(const T* index, const int bound_check,\n+                                  const int up_bound) {\n+  if (bound_check < up_bound)\n+    return __ldg(index);\n+  else\n+    return static_cast<T>(0.0f);\n+}\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename T, int mult>\n+__global__ void LayerNormGPUKernel(const LayerNormFusedArgs args,\n+                                   const T* __restrict__ input,\n+                                   T* __restrict__ output,\n+                                   const int num_blocks) {\n+  const int in_depth = args.depth;\n+  const int slice_size = args.slice_size;\n+  const int n_inputs = args.n_inputs;\n+  const T epsilon = args.epsilon;\n+\n+  const int thread_warp_id = threadIdx.x % warpSize;\n+  const int thread_slice_id = threadIdx.x % slice_size;\n+\n+  extern __shared__ __align__(sizeof(T)) unsigned char my_smem[];\n+  T* mean_cache = (T*)my_smem;\n+  T* std_cache = (T*)&my_smem[sizeof(T)];\n+\n+  const T i_n = static_cast<T>(1.0f) / static_cast<T>(in_depth);\n+  T inp[mult];\n+  int thread_id[mult];\n+\n+  T sum;\n+  T sqSum;\n+  T mu;\n+  T rstd;\n+\n+  for (int bId = blockIdx.x; bId < num_blocks; bId += gridDim.x) {\n+    sum = static_cast<T>(0.0f);\n+    sqSum = static_cast<T>(0.0f);\n+\n+    if (thread_slice_id == 0) {\n+      mean_cache[0] = static_cast<T>(0.0f);\n+      std_cache[0] = static_cast<T>(0.0f);\n+    }\n+    __syncthreads();\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      thread_id[m] = bId * in_depth + m * blockDim.x + thread_slice_id;\n+    }\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      inp[m] = get_value<T>(input + thread_id[m],\n+                            thread_slice_id + m * blockDim.x, in_depth);\n+    }\n+\n+    UNROLL for (int m = 0; m < mult; m++) { sum += inp[m] * i_n; }\n+    for (int mask = warpSize / 2; mask > 0; mask /= 2) {\n+      sum += __shfl_xor(sum, mask);\n+    }\n+    if (thread_warp_id == 0) {\n+      atomicAdd(&mean_cache[0], sum);\n+    }\n+    __syncthreads();\n+\n+    mu = mean_cache[0];\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      if (thread_slice_id + m * blockDim.x < in_depth)\n+        sqSum += (inp[m] - mu) * (inp[m] - mu);\n+    }\n+    for (int mask = warpSize / 2; mask > 0; mask /= 2) {\n+      sqSum += __shfl_xor(sqSum, mask);\n+    }\n+    if (thread_warp_id == 0) {\n+      atomicAdd(&std_cache[0], sqSum);\n+    }\n+    __syncthreads();\n+    if (thread_slice_id == 0) {\n+      std_cache[0] = rsqrt(std_cache[0] * i_n + epsilon);\n+    }\n+    __syncthreads();\n+    rstd = std_cache[0];\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      if (thread_slice_id + m * blockDim.x < in_depth &&\n+          thread_id[m] < n_inputs) {\n+        output[thread_id[m]] = (inp[m] - mu) * rstd;\n+      }\n+    }\n+    __syncthreads();\n+  }\n+}\n+\n+// fused small LN kernel\n+template <typename T>\n+__global__ void LayerNormSmallGPUKernel(const LayerNormFusedArgs args,\n+                                        const T* __restrict__ input,\n+                                        T* __restrict__ output,\n+                                        const int num_blocks,\n+                                        const int slice_per_block) {\n+  const int slice_size = args.slice_size;\n+  const int in_depth = args.depth;\n+  const int n_inputs = args.n_inputs;\n+  const T epsilon = args.epsilon;\n+\n+  const T i_n = static_cast<T>(1.0f) / static_cast<T>(in_depth);\n+\n+  const int slice_id = threadIdx.x / slice_size;\n+  const int thread_slice_id = threadIdx.x % slice_size;\n+\n+  T mu;\n+  T rstd;\n+\n+  for (int bId = blockIdx.x; bId < num_blocks; bId += gridDim.x) {\n+    mu = static_cast<T>(0.0f);\n+    rstd = static_cast<T>(0.0f);\n+\n+    const int thread_id =\n+        (bId * slice_per_block + slice_id) * in_depth + thread_slice_id;\n+    // const T inp = 0;\n+    const T inp = get_value<T>(input + thread_id, thread_slice_id, in_depth);\n+\n+    mu += inp * i_n;\n+\n+    for (int mask = slice_size / 2; mask > 0; mask /= 2) {\n+      mu += __shfl_xor(mu, mask);\n+    }\n+\n+    if (thread_slice_id < in_depth) rstd += (inp - mu) * (inp - mu);\n+\n+    for (int mask = slice_size / 2; mask > 0; mask /= 2) {\n+      rstd += __shfl_xor(rstd, mask);\n+    }\n+\n+    rstd = rsqrt(rstd * i_n + epsilon);\n+\n+    if (thread_slice_id < in_depth && thread_id < n_inputs)\n+      output[thread_id] = (inp - mu) * rstd;\n+  }\n+}\n+\n+template <typename T, int mult>\n+__global__ void LayerNormBiasAddGPUKernel(const LayerNormFusedArgs args,\n+                                          const T* __restrict__ input,\n+                                          const T* __restrict__ beta,\n+                                          T* __restrict__ output,\n+                                          const int num_blocks) {\n+  const int in_depth = args.depth;\n+  const int slice_size = args.slice_size;\n+  const int n_inputs = args.n_inputs;\n+  const T epsilon = args.epsilon;\n+\n+  const int thread_warp_id = threadIdx.x % warpSize;\n+\n+  const int thread_slice_id = threadIdx.x % slice_size;\n+\n+  extern __shared__ __align__(sizeof(T)) unsigned char my_smem[];\n+  T* mean_cache = (T*)my_smem;\n+  T* std_cache = (T*)&my_smem[sizeof(T)];\n+\n+  const T i_n = static_cast<T>(1.0f) / static_cast<T>(in_depth);\n+  T inp[mult];\n+  T _beta[mult];\n+  int thread_id[mult];\n+\n+  T sum;\n+  T sqSum;\n+  T mu;\n+  T rstd;\n+\n+  UNROLL for (int m = 0; m < mult; m++) {\n+    _beta[m] = get_value<T>(beta + thread_slice_id + m * blockDim.x,\n+                            thread_slice_id + m * blockDim.x, in_depth);\n+  }\n+\n+  for (int bId = blockIdx.x; bId < num_blocks; bId += gridDim.x) {\n+    sum = static_cast<T>(0.0f);\n+    sqSum = static_cast<T>(0.0f);\n+\n+    if (thread_slice_id == 0) {\n+      mean_cache[0] = static_cast<T>(0.0f);\n+      std_cache[0] = static_cast<T>(0.0f);\n+    }\n+    __syncthreads();\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      thread_id[m] = bId * in_depth + m * blockDim.x + thread_slice_id;\n+    }\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      inp[m] = get_value<T>(input + thread_id[m],\n+                            thread_slice_id + m * blockDim.x, in_depth);\n+    }\n+\n+    UNROLL for (int m = 0; m < mult; m++) { sum += inp[m] * i_n; }\n+    for (int mask = warpSize / 2; mask > 0; mask /= 2) {\n+      sum += __shfl_xor(sum, mask);\n+    }\n+    if (thread_warp_id == 0) {\n+      atomicAdd(&mean_cache[0], sum);\n+    }\n+    __syncthreads();\n+\n+    mu = mean_cache[0];\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      if (thread_slice_id + m * blockDim.x < in_depth)\n+        sqSum += (inp[m] - mu) * (inp[m] - mu);\n+    }\n+    for (int mask = warpSize / 2; mask > 0; mask /= 2) {\n+      sqSum += __shfl_xor(sqSum, mask);\n+    }\n+    if (thread_warp_id == 0) {\n+      atomicAdd(&std_cache[0], sqSum);\n+    }\n+    __syncthreads();\n+    if (thread_slice_id == 0) {\n+      std_cache[0] = rsqrt(std_cache[0] * i_n + epsilon);\n+    }\n+    __syncthreads();\n+    rstd = std_cache[0];\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      if (thread_slice_id + m * blockDim.x < in_depth &&\n+          thread_id[m] < n_inputs) {\n+        output[thread_id[m]] = (inp[m] - mu) * rstd + _beta[m];\n+      }\n+    }\n+    __syncthreads();\n+  }\n+}\n+\n+// fused small LN kernel\n+template <typename T>\n+__global__ void LayerNormBiasAddSmallGPUKernel(const LayerNormFusedArgs args,\n+                                               const T* __restrict__ input,\n+                                               const T* __restrict__ beta,\n+                                               T* __restrict__ output,\n+                                               const int num_blocks,\n+                                               const int slice_per_block) {\n+  const int slice_size = args.slice_size;\n+  const int in_depth = args.depth;\n+  const int n_inputs = args.n_inputs;\n+  const T epsilon = args.epsilon;\n+\n+  const T i_n = static_cast<T>(1.0f) / static_cast<T>(in_depth);\n+\n+  const int slice_id = threadIdx.x / slice_size;\n+  const int thread_slice_id = threadIdx.x % slice_size;\n+\n+  const T _beta =\n+      get_value<T>(beta + thread_slice_id, thread_slice_id, in_depth);\n+\n+  T mu;\n+  T rstd;\n+\n+  for (int bId = blockIdx.x; bId < num_blocks; bId += gridDim.x) {\n+    mu = static_cast<T>(0.0f);\n+    rstd = static_cast<T>(0.0f);\n+\n+    const int thread_id =\n+        (bId * slice_per_block + slice_id) * in_depth + thread_slice_id;\n+    // const T inp = 0;\n+    const T inp = get_value<T>(input + thread_id, thread_slice_id, in_depth);\n+\n+    mu += inp * i_n;\n+\n+    for (int mask = slice_size / 2; mask > 0; mask /= 2) {\n+      mu += __shfl_xor(mu, mask);\n+    }\n+\n+    if (thread_slice_id < in_depth) rstd += (inp - mu) * (inp - mu);\n+\n+    for (int mask = slice_size / 2; mask > 0; mask /= 2) {\n+      rstd += __shfl_xor(rstd, mask);\n+    }\n+\n+    rstd = rsqrt(rstd * i_n + epsilon);\n+\n+    if (thread_slice_id < in_depth && thread_id < n_inputs)\n+      output[thread_id] = (inp - mu) * rstd + _beta;\n+  }\n+}\n+\n+template <typename T, int mult>\n+__global__ void LayerNormFusedGPUKernel(const LayerNormFusedArgs args,\n+                                        const T* __restrict__ input,\n+                                        const T* __restrict__ gamma,\n+                                        const T* __restrict__ beta,\n+                                        T* __restrict__ output,\n+                                        const int num_blocks) {\n+  const int in_depth = args.depth;\n+  const int slice_size = args.slice_size;\n+  const int n_inputs = args.n_inputs;\n+  const T epsilon = args.epsilon;\n+\n+  const int thread_warp_id = threadIdx.x % warpSize;\n+\n+  const int thread_slice_id = threadIdx.x % slice_size;\n+\n+  extern __shared__ __align__(sizeof(T)) unsigned char my_smem[];\n+  T* mean_cache = (T*)my_smem;\n+  T* std_cache = (T*)&my_smem[sizeof(T)];\n+\n+  const T i_n = static_cast<T>(1.0f) / static_cast<T>(in_depth);\n+  T inp[mult];\n+  T _gamma[mult];\n+  T _beta[mult];\n+  int thread_id[mult];\n+\n+  T sum;\n+  T sqSum;\n+  T mu;\n+  T rstd;\n+\n+  UNROLL for (int m = 0; m < mult; m++) {\n+    _gamma[m] = get_value<T>(gamma + thread_slice_id + m * blockDim.x,\n+                             thread_slice_id + m * blockDim.x, in_depth);\n+    _beta[m] = get_value<T>(beta + thread_slice_id + m * blockDim.x,\n+                            thread_slice_id + m * blockDim.x, in_depth);\n+  }\n+\n+  for (int bId = blockIdx.x; bId < num_blocks; bId += gridDim.x) {\n+    sum = static_cast<T>(0.0f);\n+    sqSum = static_cast<T>(0.0f);\n+\n+    if (thread_slice_id == 0) {\n+      mean_cache[0] = static_cast<T>(0.0f);\n+      std_cache[0] = static_cast<T>(0.0f);\n+    }\n+    __syncthreads();\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      thread_id[m] = bId * in_depth + m * blockDim.x + thread_slice_id;\n+    }\n+\n+    UNROLL for (int m = 0; m < mult; m++) {\n+      // inp[m] = __ldg(input+thread_id[m]);", "path": "tensorflow/contrib/layers/kernels/layer_norm_fused_op_gpu.cu.cc", "position": null, "original_position": 420, "commit_id": "c956265da134875e8e79bc12cc2223deaceea87c", "original_commit_id": "6fd112700dabe27bb85c3d35eca59c971d7ff985", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "body": "Remove the comments for the code review. ", "created_at": "2017-01-19T21:09:32Z", "updated_at": "2017-03-09T00:28:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r96958343", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/96958343"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r96958343"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205"}}, "body_html": "<p>Remove the comments for the code review.</p>", "body_text": "Remove the comments for the code review."}