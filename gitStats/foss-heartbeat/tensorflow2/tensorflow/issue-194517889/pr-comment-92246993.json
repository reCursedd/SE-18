{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/92246993", "pull_request_review_id": 12758647, "id": 92246993, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDkyMjQ2OTkz", "diff_hunk": "@@ -0,0 +1,131 @@\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+\n+namespace tensorflow\n+{\n+using shape_inference::InferenceContext;\n+using shape_inference::ShapeHandle;\n+\n+\n+  REGISTER_OP(\"LayerNormCustom\")\n+      .Input(\"input: T\")\n+      .Output(\"output: T\")\n+      .Attr(\"T: {float, double}\")\n+      .Attr(\"epsilon: float = 0.0000001\")\n+      .SetShapeFn([](InferenceContext* c){\n+        c->set_output(0,c->input(0));        \n+        return Status::OK();\n+      })\n+      .Doc(R\"doc(\n+        Custom efficient Layer Normalization GPU kernel.\n+        Normalizes along last dimension.Uses two-pass algorithm to calculate variance.\n+\n+        epsilon: tiny number added before taking rsqrt(variance) to prevent division of zero.\n+  )doc\");\n+\n+  REGISTER_OP(\"LayerNormBackpropCustom\")\n+      .Input(\"input: T\")\n+      .Input(\"out_back: T\")\n+      .Output(\"in_back: T\")\n+      .Attr(\"T: {float, double}\")\n+      .Attr(\"epsilon: float = 0.0000001\")\n+      .SetShapeFn([](InferenceContext* c){\n+        c->set_output(0,c->input(0));        \n+        return Status::OK();\n+      })\n+      .Doc(R\"doc(\n+        Custom efficient Layer Normalization GPU kernel for back propagation.\n+        Normalizes along last dimension.Uses two-pass algorithm to calculate variance. \n+\n+        epsilon: tiny number added before taking rsqrt(variance) to prevent division of zero.\n+  )doc\");\n+\n+  REGISTER_OP(\"LayerNormBiasAddCustom\")\n+      .Input(\"input: T\")\n+      .Input(\"beta: T\")\n+      .Output(\"output: T\")\n+      .Attr(\"T: {float, double}\")\n+      .Attr(\"epsilon: float = 0.0000001\")\n+      .SetShapeFn([](InferenceContext* c){\n+        c->set_output(0,c->input(0));        \n+        return Status::OK();\n+      })\n+      .Doc(R\"doc(\n+        Custom efficient Layer Normalization fused with center(beta) op.\n+        Normalizes along last dimension.Uses two-pass algorithm to calculate variance.\n+\n+        With only CUDA kernel for GPU.\n+\n+        epsilon: tiny number added before taking rsqrt(variance) to prevent division of zero.\n+  )doc\");\n+\n+  REGISTER_OP(\"LayerNormBiasAddBackpropCustom\")\n+      .Input(\"input: T\")\n+      .Input(\"out_back: T\")\n+      .Input(\"beta: T\")\n+      .Output(\"in_back: T\")\n+      .Output(\"beta_back: T\")\n+      .Attr(\"T: {float, double}\")\n+      .Attr(\"epsilon: float = 0.0000001\")\n+      .SetShapeFn([](InferenceContext* c){\n+        c->set_output(0,c->input(0));        \n+        c->set_output(1,c->input(2));        \n+        return Status::OK();\n+      })\n+      .Doc(R\"doc(\n+        Custom efficient Layer Normalization backprop fused with center(beta) op.\n+        Normalizes along last dimension.Uses two-pass algorithm to calculate variance. \n+\n+        With only CUDA kernel for GPU.\n+\n+        TODO: We don't really need beta as input to calculate gradient, I am temporarily requesting\n+        beta to set output shape for beta_back before I find better method.\n+\n+        epsilon: tiny number added before taking rsqrt(variance) to prevent division of zero.\n+  )doc\");      \n+\n+  REGISTER_OP(\"LayerNormFusedCustom\")\n+      .Input(\"input: T\")\n+      .Input(\"gamma: T\")\n+      .Input(\"beta: T\")\n+      .Output(\"output: T\")\n+      .Attr(\"T: {float, double}\")\n+      .Attr(\"epsilon: float = 0.0000001\")\n+      .SetShapeFn([](InferenceContext* c){\n+        c->set_output(0,c->input(0));        \n+        return Status::OK();\n+      })\n+      .Doc(R\"doc(\n+        Custom efficient Layer Normalization fused with scale(gamma) and center(beta) ops.\n+        Normalizes along last dimension.Uses two-pass algorithm to calculate variance.\n+\n+        With only CUDA kernel for GPU.\n+\n+        epsilon: tiny number added before taking rsqrt(variance) to prevent division of zero.\n+  )doc\");\n+\n+  REGISTER_OP(\"LayerNormFusedBackpropCustom\")\n+      .Input(\"input: T\")\n+      .Input(\"out_back: T\")\n+      .Input(\"gamma: T\")\n+      .Output(\"in_back: T\")\n+      .Output(\"gamma_back: T\")\n+      .Output(\"beta_back: T\")\n+      .Attr(\"T: {float, double}\")\n+      .Attr(\"epsilon: float = 0.0000001\")\n+      .SetShapeFn([](InferenceContext* c){\n+        c->set_output(0,c->input(0));        \n+        c->set_output(1,c->input(2));        \n+        c->set_output(2,c->input(2));        \n+        return Status::OK();\n+      })\n+      .Doc(R\"doc(\n+        Custom efficient Layer Normalization backprop fused with scale(gamma) and center(beta) ops.\n+        Normalizes along last dimension.Uses two-pass algorithm to calculate variance. \n+\n+        With only CUDA kernel for GPU.\n+\n+        epsilon: tiny number added before taking rsqrt(variance) to prevent division of zero.\n+  )doc\");\n+\n+}// namespace tensorflow", "path": "tensorflow/contrib/layers/ops/layer_norm_fused_op.cc", "position": null, "original_position": 131, "commit_id": "c956265da134875e8e79bc12cc2223deaceea87c", "original_commit_id": "75883a49e704a93131e94a935906aa14b88cb4d0", "user": {"login": "ilblackdragon", "id": 175486, "node_id": "MDQ6VXNlcjE3NTQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/175486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilblackdragon", "html_url": "https://github.com/ilblackdragon", "followers_url": "https://api.github.com/users/ilblackdragon/followers", "following_url": "https://api.github.com/users/ilblackdragon/following{/other_user}", "gists_url": "https://api.github.com/users/ilblackdragon/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilblackdragon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilblackdragon/subscriptions", "organizations_url": "https://api.github.com/users/ilblackdragon/orgs", "repos_url": "https://api.github.com/users/ilblackdragon/repos", "events_url": "https://api.github.com/users/ilblackdragon/events{/privacy}", "received_events_url": "https://api.github.com/users/ilblackdragon/received_events", "type": "User", "site_admin": false}, "body": "new line at the end, please.", "created_at": "2016-12-13T19:28:54Z", "updated_at": "2017-03-09T00:28:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r92246993", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/92246993"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6205#discussion_r92246993"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6205"}}, "body_html": "<p>new line at the end, please.</p>", "body_text": "new line at the end, please."}