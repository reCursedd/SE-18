{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235671175", "html_url": "https://github.com/tensorflow/tensorflow/issues/3532#issuecomment-235671175", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3532", "id": 235671175, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTY3MTE3NQ==", "user": {"login": "shoyer", "id": 1217238, "node_id": "MDQ6VXNlcjEyMTcyMzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1217238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shoyer", "html_url": "https://github.com/shoyer", "followers_url": "https://api.github.com/users/shoyer/followers", "following_url": "https://api.github.com/users/shoyer/following{/other_user}", "gists_url": "https://api.github.com/users/shoyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/shoyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shoyer/subscriptions", "organizations_url": "https://api.github.com/users/shoyer/orgs", "repos_url": "https://api.github.com/users/shoyer/repos", "events_url": "https://api.github.com/users/shoyer/events{/privacy}", "received_events_url": "https://api.github.com/users/shoyer/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-27T18:10:58Z", "updated_at": "2016-07-27T18:10:58Z", "author_association": "MEMBER", "body_html": "<p>A few thoughts from looking through <a href=\"https://github.com/tensorflow/tensorflow/blob/5df4c71c86b28c2a4dd746bd67f00fc0281bd24f/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py#L507\">the code of DaskDataFeeder</a>:</p>\n<ul>\n<li>It looks like DaskDataFeeder currently does random sampling for input. This is nice in some cases, but in general could be very expensive. I would prefer something simpler that simply iterates through in rows in order, which would also fit better with the chunking design of dask.</li>\n<li>The current design calls <code>.compute()</code> twice in <code>_feed_dict_fn</code>. This could be trivially improved by calling <code>dask.compute</code> once, but the bigger issue is that the non-distributed version of dask is not really designed for incremental computation -- calling <code>compute()</code> evaluates the entire graph from scratch. For example, if you center a column by subtracting the mean, the mean would be recalculated each time you call compute! This is potentially a pretty big gotcha. So we should either integrate with dask-distributed's executor or consider extending the non-distributed engine (which uses threads) to handle incremental computation.</li>\n<li>There are also potential for integration with <code>dask.array</code>, which would make it quite straightforward to map <code>xarray.Dataset</code> objects backed by dask into a <code>learn.DataFrame</code> (with multi-dimensional arrays in each column). This could be quite useful for multi-dimensional scientific data.</li>\n</ul>", "body_text": "A few thoughts from looking through the code of DaskDataFeeder:\n\nIt looks like DaskDataFeeder currently does random sampling for input. This is nice in some cases, but in general could be very expensive. I would prefer something simpler that simply iterates through in rows in order, which would also fit better with the chunking design of dask.\nThe current design calls .compute() twice in _feed_dict_fn. This could be trivially improved by calling dask.compute once, but the bigger issue is that the non-distributed version of dask is not really designed for incremental computation -- calling compute() evaluates the entire graph from scratch. For example, if you center a column by subtracting the mean, the mean would be recalculated each time you call compute! This is potentially a pretty big gotcha. So we should either integrate with dask-distributed's executor or consider extending the non-distributed engine (which uses threads) to handle incremental computation.\nThere are also potential for integration with dask.array, which would make it quite straightforward to map xarray.Dataset objects backed by dask into a learn.DataFrame (with multi-dimensional arrays in each column). This could be quite useful for multi-dimensional scientific data.", "body": "A few thoughts from looking through [the code of DaskDataFeeder](https://github.com/tensorflow/tensorflow/blob/5df4c71c86b28c2a4dd746bd67f00fc0281bd24f/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py#L507):\n- It looks like DaskDataFeeder currently does random sampling for input. This is nice in some cases, but in general could be very expensive. I would prefer something simpler that simply iterates through in rows in order, which would also fit better with the chunking design of dask.\n- The current design calls `.compute()` twice in `_feed_dict_fn`. This could be trivially improved by calling `dask.compute` once, but the bigger issue is that the non-distributed version of dask is not really designed for incremental computation -- calling `compute()` evaluates the entire graph from scratch. For example, if you center a column by subtracting the mean, the mean would be recalculated each time you call compute! This is potentially a pretty big gotcha. So we should either integrate with dask-distributed's executor or consider extending the non-distributed engine (which uses threads) to handle incremental computation.\n- There are also potential for integration with `dask.array`, which would make it quite straightforward to map `xarray.Dataset` objects backed by dask into a `learn.DataFrame` (with multi-dimensional arrays in each column). This could be quite useful for multi-dimensional scientific data.\n"}