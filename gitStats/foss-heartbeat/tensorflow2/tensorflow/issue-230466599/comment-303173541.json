{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303173541", "html_url": "https://github.com/tensorflow/tensorflow/issues/10109#issuecomment-303173541", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10109", "id": 303173541, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzE3MzU0MQ==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-22T17:54:38Z", "updated_at": "2017-05-22T17:54:38Z", "author_association": "MEMBER", "body_html": "<p>Thanks Alex.  I will take a look and thank you.  I can definitely help and will take a look but I cannot take credit for the scripts.  I was just the person that did the commit to github  :-) .</p>\n<p>One quick note before I dig in.  My guess is it is creating multiple copies on each GPU.  Check nvidia-smi to see how many python processes are running on each GPU.  it should be 1 on each GPU so 4 total in your situation.  Sometimes if you kill the script it will not kill the processes on the GPUs and you end up doubling up.  Worth a quick check.  I also made a mistake in the documents and when you start the PS server you want to do this, which I assume you likely already know.  Even if your ps_server is set to CPU TensorFlow ill still try to take memory from the GPUs when starting the stand alone ps_server:</p>\n<div class=\"highlight highlight-source-shell\"><pre>CUDA_VISIBLE_DEVICES=<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span> python tf_cnn_benchmarks.py</pre></div>\n<p>I do not know a lot about the P40 other than it seems to be a card usually used for inference and has 24GB of memory which should be more than enough for batch 64.  I have used Batch-Size 128 on the P100 and it only has 16GB.  Not with TF 1.2, so I will give that a try to see if I can repo.</p>\n<p>Thank you for the heads up.</p>", "body_text": "Thanks Alex.  I will take a look and thank you.  I can definitely help and will take a look but I cannot take credit for the scripts.  I was just the person that did the commit to github  :-) .\nOne quick note before I dig in.  My guess is it is creating multiple copies on each GPU.  Check nvidia-smi to see how many python processes are running on each GPU.  it should be 1 on each GPU so 4 total in your situation.  Sometimes if you kill the script it will not kill the processes on the GPUs and you end up doubling up.  Worth a quick check.  I also made a mistake in the documents and when you start the PS server you want to do this, which I assume you likely already know.  Even if your ps_server is set to CPU TensorFlow ill still try to take memory from the GPUs when starting the stand alone ps_server:\nCUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py\nI do not know a lot about the P40 other than it seems to be a card usually used for inference and has 24GB of memory which should be more than enough for batch 64.  I have used Batch-Size 128 on the P100 and it only has 16GB.  Not with TF 1.2, so I will give that a try to see if I can repo.\nThank you for the heads up.", "body": "Thanks Alex.  I will take a look and thank you.  I can definitely help and will take a look but I cannot take credit for the scripts.  I was just the person that did the commit to github  :-) .\r\n\r\nOne quick note before I dig in.  My guess is it is creating multiple copies on each GPU.  Check nvidia-smi to see how many python processes are running on each GPU.  it should be 1 on each GPU so 4 total in your situation.  Sometimes if you kill the script it will not kill the processes on the GPUs and you end up doubling up.  Worth a quick check.  I also made a mistake in the documents and when you start the PS server you want to do this, which I assume you likely already know.  Even if your ps_server is set to CPU TensorFlow ill still try to take memory from the GPUs when starting the stand alone ps_server:  \r\n\r\n```bash\r\nCUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py\r\n```\r\n\r\nI do not know a lot about the P40 other than it seems to be a card usually used for inference and has 24GB of memory which should be more than enough for batch 64.  I have used Batch-Size 128 on the P100 and it only has 16GB.  Not with TF 1.2, so I will give that a try to see if I can repo.\r\n\r\nThank you for the heads up.\r\n"}