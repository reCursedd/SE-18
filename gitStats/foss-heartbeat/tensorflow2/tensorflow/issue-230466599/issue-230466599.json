{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10109", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10109/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10109/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10109/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10109", "id": 230466599, "node_id": "MDU6SXNzdWUyMzA0NjY1OTk=", "number": 10109, "title": "TensorFlow 1.2.0rc0 strange behavior with Benchmark scripts", "user": {"login": "alsrgv", "id": 16640218, "node_id": "MDQ6VXNlcjE2NjQwMjE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16640218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alsrgv", "html_url": "https://github.com/alsrgv", "followers_url": "https://api.github.com/users/alsrgv/followers", "following_url": "https://api.github.com/users/alsrgv/following{/other_user}", "gists_url": "https://api.github.com/users/alsrgv/gists{/gist_id}", "starred_url": "https://api.github.com/users/alsrgv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alsrgv/subscriptions", "organizations_url": "https://api.github.com/users/alsrgv/orgs", "repos_url": "https://api.github.com/users/alsrgv/repos", "events_url": "https://api.github.com/users/alsrgv/events{/privacy}", "received_events_url": "https://api.github.com/users/alsrgv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-05-22T17:32:33Z", "updated_at": "2017-05-22T18:35:56Z", "closed_at": "2017-05-22T18:35:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm running <a href=\"https://github.com/tensorflow/benchmarks/\">https://github.com/tensorflow/benchmarks/</a> scripts by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a> and I'm observing strange behavior and crashes.</p>\n<ol>\n<li>There's a LOT of messages like these (below).  These messages happen in both standalone and distributed runs.</li>\n</ol>\n<pre><code>2017-05-22 17:21:00.589871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.600723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.600741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.600746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n2017-05-22 17:21:00.600750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.619525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.619543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.619548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n2017-05-22 17:21:00.619552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.629770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.629786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.629807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n2017-05-22 17:21:00.629811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.649854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.649871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.649892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n</code></pre>\n<ol start=\"2\">\n<li><del>In distributed mode, TF 1.2.0 rc0 is not able to handle batch size 64 for Inception V3:</del> - This was figured out.</li>\n</ol>\n<p>Slave worker crashes with:</p>\n<pre><code>2017-05-22 17:22:25.411569: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when allocating tensor with shape[64,32,149,149]\n</code></pre>\n<p>Chief worker crashes with:</p>\n<pre><code>Exception in thread Thread-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n    self.run()\n  File \"tf_cnn_benchmarks.py\", line 226, in run\n    global_step_val, = self.sess.run([self.global_step_op])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 789, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 925, in _run\n    raise RuntimeError('Attempted to use a closed Session.')\nRuntimeError: Attempted to use a closed Session.\n</code></pre>\n<p>My command:</p>\n<pre><code>python -u tf_cnn_benchmarks.py --model inception3 --batch_size 64 --num_gpus 4 --worker_hosts {worker_hosts} --ps_hosts {ps_hosts} --task_index {task_index} --job_name {job_name} --local_parameter_device cpu\n</code></pre>\n<p>This worked perfectly fine on hash <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/cae8ed1ca54a9fd4f9cc64d08cadebce31fd4607/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/cae8ed1ca54a9fd4f9cc64d08cadebce31fd4607\"><tt>cae8ed1</tt></a> (just a little bit past TF 1.1 release).</p>", "body_text": "I'm running https://github.com/tensorflow/benchmarks/ scripts by @tfboyd and I'm observing strange behavior and crashes.\n\nThere's a LOT of messages like these (below).  These messages happen in both standalone and distributed runs.\n\n2017-05-22 17:21:00.589871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.600723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.600741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.600746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n2017-05-22 17:21:00.600750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.619525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.619543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.619548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n2017-05-22 17:21:00.619552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.629770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.629786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.629807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n2017-05-22 17:21:00.629811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\n2017-05-22 17:21:00.649854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\n2017-05-22 17:21:00.649871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\n2017-05-22 17:21:00.649892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\n\n\nIn distributed mode, TF 1.2.0 rc0 is not able to handle batch size 64 for Inception V3: - This was figured out.\n\nSlave worker crashes with:\n2017-05-22 17:22:25.411569: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when allocating tensor with shape[64,32,149,149]\n\nChief worker crashes with:\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n    self.run()\n  File \"tf_cnn_benchmarks.py\", line 226, in run\n    global_step_val, = self.sess.run([self.global_step_op])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 789, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 925, in _run\n    raise RuntimeError('Attempted to use a closed Session.')\nRuntimeError: Attempted to use a closed Session.\n\nMy command:\npython -u tf_cnn_benchmarks.py --model inception3 --batch_size 64 --num_gpus 4 --worker_hosts {worker_hosts} --ps_hosts {ps_hosts} --task_index {task_index} --job_name {job_name} --local_parameter_device cpu\n\nThis worked perfectly fine on hash cae8ed1 (just a little bit past TF 1.1 release).", "body": "I'm running https://github.com/tensorflow/benchmarks/ scripts by @tfboyd and I'm observing strange behavior and crashes.\r\n\r\n1. There's a LOT of messages like these (below).  These messages happen in both standalone and distributed runs.\r\n\r\n```\r\n2017-05-22 17:21:00.589871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\r\n2017-05-22 17:21:00.600723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\r\n2017-05-22 17:21:00.600741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\r\n2017-05-22 17:21:00.600746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\r\n2017-05-22 17:21:00.600750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\r\n2017-05-22 17:21:00.619525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\r\n2017-05-22 17:21:00.619543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\r\n2017-05-22 17:21:00.619548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\r\n2017-05-22 17:21:00.619552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\r\n2017-05-22 17:21:00.629770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\r\n2017-05-22 17:21:00.629786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\r\n2017-05-22 17:21:00.629807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\r\n2017-05-22 17:21:00.629811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)\r\n2017-05-22 17:21:00.649854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)\r\n2017-05-22 17:21:00.649871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)\r\n2017-05-22 17:21:00.649892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)\r\n```\r\n\r\n2. ~~In distributed mode, TF 1.2.0 rc0 is not able to handle batch size 64 for Inception V3:~~ - This was figured out.\r\n\r\nSlave worker crashes with:\r\n```\r\n2017-05-22 17:22:25.411569: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when allocating tensor with shape[64,32,149,149]\r\n```\r\n\r\nChief worker crashes with:\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\r\n    self.run()\r\n  File \"tf_cnn_benchmarks.py\", line 226, in run\r\n    global_step_val, = self.sess.run([self.global_step_op])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 925, in _run\r\n    raise RuntimeError('Attempted to use a closed Session.')\r\nRuntimeError: Attempted to use a closed Session.\r\n```\r\n\r\nMy command:\r\n```\r\npython -u tf_cnn_benchmarks.py --model inception3 --batch_size 64 --num_gpus 4 --worker_hosts {worker_hosts} --ps_hosts {ps_hosts} --task_index {task_index} --job_name {job_name} --local_parameter_device cpu\r\n```\r\n\r\nThis worked perfectly fine on hash cae8ed1ca54a9fd4f9cc64d08cadebce31fd4607 (just a little bit past TF 1.1 release)."}