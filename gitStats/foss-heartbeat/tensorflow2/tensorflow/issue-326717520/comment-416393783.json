{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416393783", "html_url": "https://github.com/tensorflow/tensorflow/issues/19568#issuecomment-416393783", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19568", "id": 416393783, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjM5Mzc4Mw==", "user": {"login": "sharvil", "id": 391004, "node_id": "MDQ6VXNlcjM5MTAwNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/391004?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sharvil", "html_url": "https://github.com/sharvil", "followers_url": "https://api.github.com/users/sharvil/followers", "following_url": "https://api.github.com/users/sharvil/following{/other_user}", "gists_url": "https://api.github.com/users/sharvil/gists{/gist_id}", "starred_url": "https://api.github.com/users/sharvil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sharvil/subscriptions", "organizations_url": "https://api.github.com/users/sharvil/orgs", "repos_url": "https://api.github.com/users/sharvil/repos", "events_url": "https://api.github.com/users/sharvil/events{/privacy}", "received_events_url": "https://api.github.com/users/sharvil/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-27T22:48:07Z", "updated_at": "2018-08-27T22:48:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a> you're right, that issue looks suspiciously similar and probably has the same root cause. On another note, I've come up with a workaround for the sample code I posted above.</p>\n<p>The following code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)):\n  grads_and_vars <span class=\"pl-k\">=</span> optimizer.compute_gradients(res)\n  train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(grads_and_vars, global_step)</pre></div>\n<p>can be transformed into:</p>\n<div class=\"highlight highlight-source-python\"><pre>res <span class=\"pl-k\">=</span> tf.tuple([res], <span class=\"pl-v\">control_inputs</span><span class=\"pl-k\">=</span>tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>))[<span class=\"pl-c1\">0</span>]\ngrads_and_vars <span class=\"pl-k\">=</span> optimizer.compute_gradients(res)\ntrain_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(grads_and_vars, global_step)</pre></div>\n<p>which forces the batch norm ops to run before gradients on <code>res</code> are computed, as desired.</p>", "body_text": "@ppwwyyxx you're right, that issue looks suspiciously similar and probably has the same root cause. On another note, I've come up with a workaround for the sample code I posted above.\nThe following code:\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n  grads_and_vars = optimizer.compute_gradients(res)\n  train_op = optimizer.apply_gradients(grads_and_vars, global_step)\ncan be transformed into:\nres = tf.tuple([res], control_inputs=tf.get_collection(tf.GraphKeys.UPDATE_OPS))[0]\ngrads_and_vars = optimizer.compute_gradients(res)\ntrain_op = optimizer.apply_gradients(grads_and_vars, global_step)\nwhich forces the batch norm ops to run before gradients on res are computed, as desired.", "body": "@ppwwyyxx you're right, that issue looks suspiciously similar and probably has the same root cause. On another note, I've come up with a workaround for the sample code I posted above.\r\n\r\nThe following code:\r\n\r\n```python\r\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n  grads_and_vars = optimizer.compute_gradients(res)\r\n  train_op = optimizer.apply_gradients(grads_and_vars, global_step)\r\n```\r\n\r\ncan be transformed into:\r\n\r\n```python\r\nres = tf.tuple([res], control_inputs=tf.get_collection(tf.GraphKeys.UPDATE_OPS))[0]\r\ngrads_and_vars = optimizer.compute_gradients(res)\r\ntrain_op = optimizer.apply_gradients(grads_and_vars, global_step)\r\n```\r\n\r\nwhich forces the batch norm ops to run before gradients on `res` are computed, as desired."}