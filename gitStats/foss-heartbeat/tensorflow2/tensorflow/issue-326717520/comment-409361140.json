{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/409361140", "html_url": "https://github.com/tensorflow/tensorflow/issues/19568#issuecomment-409361140", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19568", "id": 409361140, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTM2MTE0MA==", "user": {"login": "sharvil", "id": 391004, "node_id": "MDQ6VXNlcjM5MTAwNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/391004?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sharvil", "html_url": "https://github.com/sharvil", "followers_url": "https://api.github.com/users/sharvil/followers", "following_url": "https://api.github.com/users/sharvil/following{/other_user}", "gists_url": "https://api.github.com/users/sharvil/gists{/gist_id}", "starred_url": "https://api.github.com/users/sharvil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sharvil/subscriptions", "organizations_url": "https://api.github.com/users/sharvil/orgs", "repos_url": "https://api.github.com/users/sharvil/repos", "events_url": "https://api.github.com/users/sharvil/events{/privacy}", "received_events_url": "https://api.github.com/users/sharvil/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T20:43:19Z", "updated_at": "2018-07-31T20:43:19Z", "author_association": "NONE", "body_html": "<p>I'm seeing this issue as well. Here's my configuration:</p>\n<p>Ubuntu 16.04.5<br>\nTF 1.9.0 (v1.9.0-0-g25c197e) from source<br>\nBazel 0.15.2<br>\nCUDA Version 9.1.85<br>\ncuDNN 7.1.3<br>\nGeForce GTX 1080 Ti 11GB</p>\n<p>The following sample code demonstrates the issue:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">TIME_STEPS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> no lock up with TIME_STEPS &lt; 32</span>\n\nmemory <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>])\n\ncell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n  tf.contrib.rnn.LSTMCell(<span class=\"pl-c1\">3</span>),\n  tf.contrib.seq2seq.BahdanauAttention(<span class=\"pl-c1\">3</span>, memory),\n)\n\ninputs <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">TIME_STEPS</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\nhelper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(inputs, [<span class=\"pl-c1\">TIME_STEPS</span>])\ninitial_state <span class=\"pl-k\">=</span> cell.zero_state(<span class=\"pl-c1\">BATCH_SIZE</span>, tf.float32)\ndecoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(cell, helper, initial_state)\n\nfinal_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(decoder)\n\nres <span class=\"pl-k\">=</span> final_output.rnn_output\nres <span class=\"pl-k\">=</span> tf.layers.batch_normalization(res, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nglobal_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\noptimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer()\n\n<span class=\"pl-k\">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)):\n  grads_and_vars <span class=\"pl-k\">=</span> optimizer.compute_gradients(res)\n  train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(grads_and_vars, global_step)\n\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(train_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{memory: [[[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>]]]})</pre></div>\n<p>There seems to be a connection between the number of decoder steps and batch normalization. The issue is reproducible with <code>TIME_STEPS &gt;= 32</code> but not with <code>TIME_STEPS &lt; 32</code>.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>, could you please help find someone to triage this issue? Thanks!</p>", "body_text": "I'm seeing this issue as well. Here's my configuration:\nUbuntu 16.04.5\nTF 1.9.0 (v1.9.0-0-g25c197e) from source\nBazel 0.15.2\nCUDA Version 9.1.85\ncuDNN 7.1.3\nGeForce GTX 1080 Ti 11GB\nThe following sample code demonstrates the issue:\nimport tensorflow as tf\nimport numpy as np\n\nBATCH_SIZE = 1\nTIME_STEPS = 32  # no lock up with TIME_STEPS < 32\n\nmemory = tf.placeholder(tf.float32, [BATCH_SIZE, 1, 3])\n\ncell = tf.contrib.seq2seq.AttentionWrapper(\n  tf.contrib.rnn.LSTMCell(3),\n  tf.contrib.seq2seq.BahdanauAttention(3, memory),\n)\n\ninputs = np.zeros((BATCH_SIZE, TIME_STEPS, 1), dtype=np.float32)\nhelper = tf.contrib.seq2seq.TrainingHelper(inputs, [TIME_STEPS])\ninitial_state = cell.zero_state(BATCH_SIZE, tf.float32)\ndecoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, initial_state)\n\nfinal_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n\nres = final_output.rnn_output\nres = tf.layers.batch_normalization(res, training=True)\n\nglobal_step = tf.train.get_or_create_global_step()\noptimizer = tf.train.AdamOptimizer()\n\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n  grads_and_vars = optimizer.compute_gradients(res)\n  train_op = optimizer.apply_gradients(grads_and_vars, global_step)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(train_op, feed_dict={memory: [[[1, 2, 3]]]})\nThere seems to be a connection between the number of decoder steps and batch normalization. The issue is reproducible with TIME_STEPS >= 32 but not with TIME_STEPS < 32.\n@mrry, could you please help find someone to triage this issue? Thanks!", "body": "I'm seeing this issue as well. Here's my configuration:\r\n\r\nUbuntu 16.04.5\r\nTF 1.9.0 (v1.9.0-0-g25c197e) from source\r\nBazel 0.15.2\r\nCUDA Version 9.1.85\r\ncuDNN 7.1.3\r\nGeForce GTX 1080 Ti 11GB\r\n\r\nThe following sample code demonstrates the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nBATCH_SIZE = 1\r\nTIME_STEPS = 32  # no lock up with TIME_STEPS < 32\r\n\r\nmemory = tf.placeholder(tf.float32, [BATCH_SIZE, 1, 3])\r\n\r\ncell = tf.contrib.seq2seq.AttentionWrapper(\r\n  tf.contrib.rnn.LSTMCell(3),\r\n  tf.contrib.seq2seq.BahdanauAttention(3, memory),\r\n)\r\n\r\ninputs = np.zeros((BATCH_SIZE, TIME_STEPS, 1), dtype=np.float32)\r\nhelper = tf.contrib.seq2seq.TrainingHelper(inputs, [TIME_STEPS])\r\ninitial_state = cell.zero_state(BATCH_SIZE, tf.float32)\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, initial_state)\r\n\r\nfinal_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\r\n\r\nres = final_output.rnn_output\r\nres = tf.layers.batch_normalization(res, training=True)\r\n\r\nglobal_step = tf.train.get_or_create_global_step()\r\noptimizer = tf.train.AdamOptimizer()\r\n\r\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n  grads_and_vars = optimizer.compute_gradients(res)\r\n  train_op = optimizer.apply_gradients(grads_and_vars, global_step)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(train_op, feed_dict={memory: [[[1, 2, 3]]]})\r\n```\r\n\r\nThere seems to be a connection between the number of decoder steps and batch normalization. The issue is reproducible with `TIME_STEPS >= 32` but not with `TIME_STEPS < 32`.\r\n\r\n@mrry, could you please help find someone to triage this issue? Thanks!"}