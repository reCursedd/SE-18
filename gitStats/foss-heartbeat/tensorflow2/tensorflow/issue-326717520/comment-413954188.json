{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413954188", "html_url": "https://github.com/tensorflow/tensorflow/issues/19568#issuecomment-413954188", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19568", "id": 413954188, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzk1NDE4OA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T18:38:43Z", "updated_at": "2018-08-17T18:38:43Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=391004\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sharvil\">@sharvil</a> : Apologies for the late response. Nope, no update yet, haven't gotten around to debugging this yet.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7523982\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/chsigg\">@chsigg</a> - would either of you have some cycles to look into this?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5118881\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/qlzh727\">@qlzh727</a> have been looking at the LSTMCell implementations recently and may also have some insight.</p>\n<p>For the record, if I understand correctly, the following reproduces the problem (picked up from the <a href=\"https://stackoverflow.com/questions/47047124/tf-layers-batch-normalization-freezes-during-sess-run-1-5-0-dev20171031\" rel=\"nofollow\">StackOverflow question</a>, haven't tried it myself yet):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nstarter_learning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.001</span>\ndecay_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\ndecay_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.96</span>\nnum_RNN_layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\n<span class=\"pl-c1\">LSTM_CELL_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nkeep_prob <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.95</span>\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Inputs<span class=\"pl-pds\">'</span></span>):\n    x <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">200</span>])\n    y <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">200</span>])\n    length <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>])\n    Flg_training <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.bool, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[])\n\n    x_1 <span class=\"pl-k\">=</span> tf.expand_dims(x, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>BiLSTM<span class=\"pl-pds\">'</span></span>):\n    dropcells <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> iiLyr <span class=\"pl-k\">in</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(num_RNN_layers)):\n        cell_iiLyr <span class=\"pl-k\">=</span> tf.nn.rnn_cell.LSTMCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LSTM_CELL_SIZE</span>, <span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        dropcells.append(tf.nn.rnn_cell.DropoutWrapper(<span class=\"pl-v\">cell</span><span class=\"pl-k\">=</span>cell_iiLyr, <span class=\"pl-v\">output_keep_prob</span><span class=\"pl-k\">=</span>keep_prob))  <span class=\"pl-c\"><span class=\"pl-c\">#</span>,, input_keep_prob=self.keep_prob input_keep_prob=1.0, seed=None</span>\n\n    MultiLyr_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.MultiRNNCell(<span class=\"pl-v\">cells</span><span class=\"pl-k\">=</span>dropcells, <span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    outputs, states  <span class=\"pl-k\">=</span> tf.nn.bidirectional_dynamic_rnn(\n        <span class=\"pl-v\">cell_fw</span><span class=\"pl-k\">=</span>MultiLyr_cell, \n        <span class=\"pl-v\">cell_bw</span><span class=\"pl-k\">=</span>MultiLyr_cell, \n        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n        <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>length, <span class=\"pl-c\"><span class=\"pl-c\">#</span>tf_b_lens </span>\n        <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>x_1, <span class=\"pl-c\"><span class=\"pl-c\">#</span>stacked_RefPts_desc, #tf_b_VCCs_AMs_BN1</span>\n        <span class=\"pl-v\">scope</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BiLSTM<span class=\"pl-pds\">\"</span></span>\n        )\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>output_fw, output_bw = outputs</span>\n    states_fw, states_bw <span class=\"pl-k\">=</span> states\n\n    c_fw_lstLyr, h_fw_lstLyr <span class=\"pl-k\">=</span> states_fw[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n    c_bw_lstLyr, h_bw_lstLyr <span class=\"pl-k\">=</span> states_bw[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n    states_concat1 <span class=\"pl-k\">=</span> tf.concat([h_fw_lstLyr, h_bw_lstLyr], <span class=\"pl-v\">axis</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>states_concat<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cs_BN1<span class=\"pl-pds\">\"</span></span>):\n    x_BN <span class=\"pl-k\">=</span> tf.layers.batch_normalization(\n        states_concat1,\n        <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span> axis that should be normalized (typically the features axis, in this case the concated states or hidden vectors)</span>\n        <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.99</span>,\n        <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-10</span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span>0.001,</span>\n        <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span>True,</span>\n        <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span>True,</span>\n        <span class=\"pl-v\">beta_initializer</span><span class=\"pl-k\">=</span>tf.zeros_initializer(),\n        <span class=\"pl-v\">gamma_initializer</span><span class=\"pl-k\">=</span>tf.ones_initializer(),\n        <span class=\"pl-v\">moving_mean_initializer</span><span class=\"pl-k\">=</span>tf.zeros_initializer(),\n        <span class=\"pl-v\">moving_variance_initializer</span><span class=\"pl-k\">=</span>tf.ones_initializer(),\n        <span class=\"pl-v\">beta_regularizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n        <span class=\"pl-v\">gamma_regularizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n        <span class=\"pl-v\">beta_constraint</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n        <span class=\"pl-v\">gamma_constraint</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n        <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>Flg_training, <span class=\"pl-c\"><span class=\"pl-c\">#</span>False,</span>\n        <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n        <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test_BN<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c\"><span class=\"pl-c\">#</span>None,</span>\n        <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n        <span class=\"pl-v\">renorm</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        <span class=\"pl-v\">renorm_clipping</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n        <span class=\"pl-v\">renorm_momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.99</span>,\n        <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        <span class=\"pl-v\">virtual_batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n        <span class=\"pl-v\">adjustment</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>\n        )\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Regression<span class=\"pl-pds\">\"</span></span>):\n    a <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>a<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">1.0</span>))\n    b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>b<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>))\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Prediction<span class=\"pl-pds\">\"</span></span>):\n    y_pred <span class=\"pl-k\">=</span> tf.multiply(x_BN, a) <span class=\"pl-k\">+</span> b\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loss<span class=\"pl-pds\">'</span></span>):\n    losses <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(y, y_pred, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span>tf.losses.Reduction.<span class=\"pl-c1\">NONE</span>)\n    mean_loss <span class=\"pl-k\">=</span> tf.reduce_mean(losses)\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Training<span class=\"pl-pds\">'</span></span>):\n    global_step <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    learning_rate <span class=\"pl-k\">=</span> tf.train.exponential_decay(starter_learning_rate, global_step,\n                                               decay_steps, decay_rate, <span class=\"pl-v\">staircase</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) \n\n    update_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n\n    <span class=\"pl-k\">with</span> tf.control_dependencies(update_ops):\n        train_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>learning_rate).minimize(losses, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>x_mean = tf.reduce_mean(x_BN, axis=0)</span>\n\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\ntrain_writer <span class=\"pl-k\">=</span> tf.summary.FileWriter(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>G:<span class=\"pl-cce\">\\\\</span>Surface_Ozone<span class=\"pl-cce\">\\\\</span>Temp<span class=\"pl-cce\">\\\\</span><span class=\"pl-pds\">\"</span></span>, sess.graph)   \nsess.run(tf.global_variables_initializer())\n\n<span class=\"pl-k\">for</span> ii <span class=\"pl-k\">in</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2000</span>)):\n    x_in <span class=\"pl-k\">=</span> (np.random.rand(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">200</span>))\n    y_in <span class=\"pl-k\">=</span> x_in <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1.5</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">3.0</span>\n    length_in <span class=\"pl-k\">=</span> np.full([<span class=\"pl-c1\">20</span>], <span class=\"pl-c1\">200</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32)\n\n    _, mean_loss_val, a_val, b_val <span class=\"pl-k\">=</span> sess.run([train_step, mean_loss, a, b], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n        x: x_in, \n        Flg_training: <span class=\"pl-c1\">True</span>, \n        y: y_in,\n        length: length_in\n        })\n\n    <span class=\"pl-k\">if</span> (ii <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">50</span>):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>step <span class=\"pl-c1\">{}</span>: <span class=\"pl-c1\">{}</span> | a: <span class=\"pl-c1\">{}</span> | b: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(ii, mean_loss_val, a_val, b_val))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">if</span> (ii <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>):\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>step <span class=\"pl-c1\">{}</span>: <span class=\"pl-c1\">{}</span> | a: <span class=\"pl-c1\">{}</span> | b: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(ii, mean_loss_val, a_val, b_val))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Normal End.<span class=\"pl-pds\">\"</span></span>)</pre></div>", "body_text": "@sharvil : Apologies for the late response. Nope, no update yet, haven't gotten around to debugging this yet.\n@zheng-xq @chsigg - would either of you have some cycles to look into this?\n@drpngx @qlzh727 have been looking at the LSTMCell implementations recently and may also have some insight.\nFor the record, if I understand correctly, the following reproduces the problem (picked up from the StackOverflow question, haven't tried it myself yet):\nimport tensorflow as tf\nimport numpy as np\n\nstarter_learning_rate = 0.001\ndecay_steps = 100\ndecay_rate = 0.96\nnum_RNN_layers = 3\nLSTM_CELL_SIZE = 100\nkeep_prob = 0.95\n\nwith tf.name_scope('Inputs'):\n    x = tf.placeholder(dtype=tf.float32, shape=[None, 200])\n    y = tf.placeholder(dtype=tf.float32, shape=[None, 200])\n    length = tf.placeholder(dtype=tf.int32, shape=[None])\n    Flg_training = tf.placeholder(dtype=tf.bool, shape=[])\n\n    x_1 = tf.expand_dims(x, -1)\n\nwith tf.name_scope('BiLSTM'):\n    dropcells = []\n    for iiLyr in list(range(num_RNN_layers)):\n        cell_iiLyr = tf.nn.rnn_cell.LSTMCell(num_units=LSTM_CELL_SIZE, state_is_tuple=True)\n        dropcells.append(tf.nn.rnn_cell.DropoutWrapper(cell=cell_iiLyr, output_keep_prob=keep_prob))  #,, input_keep_prob=self.keep_prob input_keep_prob=1.0, seed=None\n\n    MultiLyr_cell = tf.nn.rnn_cell.MultiRNNCell(cells=dropcells, state_is_tuple=True)\n\n    outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n        cell_fw=MultiLyr_cell, \n        cell_bw=MultiLyr_cell, \n        dtype=tf.float32,\n        sequence_length=length, #tf_b_lens \n        inputs=x_1, #stacked_RefPts_desc, #tf_b_VCCs_AMs_BN1\n        scope = \"BiLSTM\"\n        )\n\n    #output_fw, output_bw = outputs\n    states_fw, states_bw = states\n\n    c_fw_lstLyr, h_fw_lstLyr = states_fw[-1]\n    c_bw_lstLyr, h_bw_lstLyr = states_bw[-1]\n\n    states_concat1 = tf.concat([h_fw_lstLyr, h_bw_lstLyr], axis = 1, name = 'states_concat')\n\nwith tf.name_scope(\"cs_BN1\"):\n    x_BN = tf.layers.batch_normalization(\n        states_concat1,\n        axis=-1, # axis that should be normalized (typically the features axis, in this case the concated states or hidden vectors)\n        momentum=0.99,\n        epsilon=1e-10, #0.001,\n        center=False, #True,\n        scale=False, #True,\n        beta_initializer=tf.zeros_initializer(),\n        gamma_initializer=tf.ones_initializer(),\n        moving_mean_initializer=tf.zeros_initializer(),\n        moving_variance_initializer=tf.ones_initializer(),\n        beta_regularizer=None,\n        gamma_regularizer=None,\n        beta_constraint=None,\n        gamma_constraint=None,\n        training=Flg_training, #False,\n        trainable=True,\n        name=\"test_BN\", #None,\n        reuse=None,\n        renorm=False,\n        renorm_clipping=None,\n        renorm_momentum=0.99,\n        fused=False,\n        virtual_batch_size=None,\n        adjustment=None\n        )\n\nwith tf.name_scope(\"Regression\"):\n    a = tf.get_variable(\"a\", shape=[1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n    b = tf.get_variable(\"b\", shape=[1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n\nwith tf.name_scope(\"Prediction\"):\n    y_pred = tf.multiply(x_BN, a) + b\n\nwith tf.name_scope('Loss'):\n    losses = tf.losses.mean_squared_error(y, y_pred, reduction=tf.losses.Reduction.NONE)\n    mean_loss = tf.reduce_mean(losses)\n\nwith tf.name_scope('Training'):\n    global_step = tf.Variable(0, trainable=False)\n    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n                                               decay_steps, decay_rate, staircase=True) \n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n    with tf.control_dependencies(update_ops):\n        train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(losses, global_step=global_step)\n\n\n#x_mean = tf.reduce_mean(x_BN, axis=0)\n\nsess = tf.InteractiveSession()\ntrain_writer = tf.summary.FileWriter(\"G:\\\\Surface_Ozone\\\\Temp\\\\\", sess.graph)   \nsess.run(tf.global_variables_initializer())\n\nfor ii in list(range(2000)):\n    x_in = (np.random.rand(20, 200))\n    y_in = x_in * 1.5 + 3.0\n    length_in = np.full([20], 200, dtype=np.int32)\n\n    _, mean_loss_val, a_val, b_val = sess.run([train_step, mean_loss, a, b], feed_dict={\n        x: x_in, \n        Flg_training: True, \n        y: y_in,\n        length: length_in\n        })\n\n    if (ii < 50):\n        print(\"step {}: {} | a: {} | b: {}\".format(ii, mean_loss_val, a_val, b_val))\n    else:\n        if (ii % 100 == 0):\n            print(\"step {}: {} | a: {} | b: {}\".format(ii, mean_loss_val, a_val, b_val))\n\nprint(\"Normal End.\")", "body": "@sharvil : Apologies for the late response. Nope, no update yet, haven't gotten around to debugging this yet. \r\n\r\n@zheng-xq @chsigg - would either of you have some cycles to look into this?\r\n@drpngx @qlzh727 have been looking at the LSTMCell implementations recently and may also have some insight.\r\n\r\nFor the record, if I understand correctly, the following reproduces the problem (picked up from the [StackOverflow question](https://stackoverflow.com/questions/47047124/tf-layers-batch-normalization-freezes-during-sess-run-1-5-0-dev20171031), haven't tried it myself yet):\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nstarter_learning_rate = 0.001\r\ndecay_steps = 100\r\ndecay_rate = 0.96\r\nnum_RNN_layers = 3\r\nLSTM_CELL_SIZE = 100\r\nkeep_prob = 0.95\r\n\r\nwith tf.name_scope('Inputs'):\r\n    x = tf.placeholder(dtype=tf.float32, shape=[None, 200])\r\n    y = tf.placeholder(dtype=tf.float32, shape=[None, 200])\r\n    length = tf.placeholder(dtype=tf.int32, shape=[None])\r\n    Flg_training = tf.placeholder(dtype=tf.bool, shape=[])\r\n\r\n    x_1 = tf.expand_dims(x, -1)\r\n\r\nwith tf.name_scope('BiLSTM'):\r\n    dropcells = []\r\n    for iiLyr in list(range(num_RNN_layers)):\r\n        cell_iiLyr = tf.nn.rnn_cell.LSTMCell(num_units=LSTM_CELL_SIZE, state_is_tuple=True)\r\n        dropcells.append(tf.nn.rnn_cell.DropoutWrapper(cell=cell_iiLyr, output_keep_prob=keep_prob))  #,, input_keep_prob=self.keep_prob input_keep_prob=1.0, seed=None\r\n\r\n    MultiLyr_cell = tf.nn.rnn_cell.MultiRNNCell(cells=dropcells, state_is_tuple=True)\r\n\r\n    outputs, states  = tf.nn.bidirectional_dynamic_rnn(\r\n        cell_fw=MultiLyr_cell, \r\n        cell_bw=MultiLyr_cell, \r\n        dtype=tf.float32,\r\n        sequence_length=length, #tf_b_lens \r\n        inputs=x_1, #stacked_RefPts_desc, #tf_b_VCCs_AMs_BN1\r\n        scope = \"BiLSTM\"\r\n        )\r\n\r\n    #output_fw, output_bw = outputs\r\n    states_fw, states_bw = states\r\n\r\n    c_fw_lstLyr, h_fw_lstLyr = states_fw[-1]\r\n    c_bw_lstLyr, h_bw_lstLyr = states_bw[-1]\r\n\r\n    states_concat1 = tf.concat([h_fw_lstLyr, h_bw_lstLyr], axis = 1, name = 'states_concat')\r\n\r\nwith tf.name_scope(\"cs_BN1\"):\r\n    x_BN = tf.layers.batch_normalization(\r\n        states_concat1,\r\n        axis=-1, # axis that should be normalized (typically the features axis, in this case the concated states or hidden vectors)\r\n        momentum=0.99,\r\n        epsilon=1e-10, #0.001,\r\n        center=False, #True,\r\n        scale=False, #True,\r\n        beta_initializer=tf.zeros_initializer(),\r\n        gamma_initializer=tf.ones_initializer(),\r\n        moving_mean_initializer=tf.zeros_initializer(),\r\n        moving_variance_initializer=tf.ones_initializer(),\r\n        beta_regularizer=None,\r\n        gamma_regularizer=None,\r\n        beta_constraint=None,\r\n        gamma_constraint=None,\r\n        training=Flg_training, #False,\r\n        trainable=True,\r\n        name=\"test_BN\", #None,\r\n        reuse=None,\r\n        renorm=False,\r\n        renorm_clipping=None,\r\n        renorm_momentum=0.99,\r\n        fused=False,\r\n        virtual_batch_size=None,\r\n        adjustment=None\r\n        )\r\n\r\nwith tf.name_scope(\"Regression\"):\r\n    a = tf.get_variable(\"a\", shape=[1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\r\n    b = tf.get_variable(\"b\", shape=[1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\r\n\r\nwith tf.name_scope(\"Prediction\"):\r\n    y_pred = tf.multiply(x_BN, a) + b\r\n\r\nwith tf.name_scope('Loss'):\r\n    losses = tf.losses.mean_squared_error(y, y_pred, reduction=tf.losses.Reduction.NONE)\r\n    mean_loss = tf.reduce_mean(losses)\r\n\r\nwith tf.name_scope('Training'):\r\n    global_step = tf.Variable(0, trainable=False)\r\n    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\r\n                                               decay_steps, decay_rate, staircase=True) \r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\r\n    with tf.control_dependencies(update_ops):\r\n        train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(losses, global_step=global_step)\r\n\r\n\r\n#x_mean = tf.reduce_mean(x_BN, axis=0)\r\n\r\nsess = tf.InteractiveSession()\r\ntrain_writer = tf.summary.FileWriter(\"G:\\\\Surface_Ozone\\\\Temp\\\\\", sess.graph)   \r\nsess.run(tf.global_variables_initializer())\r\n\r\nfor ii in list(range(2000)):\r\n    x_in = (np.random.rand(20, 200))\r\n    y_in = x_in * 1.5 + 3.0\r\n    length_in = np.full([20], 200, dtype=np.int32)\r\n\r\n    _, mean_loss_val, a_val, b_val = sess.run([train_step, mean_loss, a, b], feed_dict={\r\n        x: x_in, \r\n        Flg_training: True, \r\n        y: y_in,\r\n        length: length_in\r\n        })\r\n\r\n    if (ii < 50):\r\n        print(\"step {}: {} | a: {} | b: {}\".format(ii, mean_loss_val, a_val, b_val))\r\n    else:\r\n        if (ii % 100 == 0):\r\n            print(\"step {}: {} | a: {} | b: {}\".format(ii, mean_loss_val, a_val, b_val))\r\n\r\nprint(\"Normal End.\")\r\n```"}