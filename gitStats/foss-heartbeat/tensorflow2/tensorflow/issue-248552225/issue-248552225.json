{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12090", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12090/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12090/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12090/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12090", "id": 248552225, "node_id": "MDU6SXNzdWUyNDg1NTIyMjU=", "number": 12090, "title": "imagenet_distributed_train using inception v3 stuck on saving check points forever.", "user": {"login": "YuxinxinChen", "id": 24256365, "node_id": "MDQ6VXNlcjI0MjU2MzY1", "avatar_url": "https://avatars0.githubusercontent.com/u/24256365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YuxinxinChen", "html_url": "https://github.com/YuxinxinChen", "followers_url": "https://api.github.com/users/YuxinxinChen/followers", "following_url": "https://api.github.com/users/YuxinxinChen/following{/other_user}", "gists_url": "https://api.github.com/users/YuxinxinChen/gists{/gist_id}", "starred_url": "https://api.github.com/users/YuxinxinChen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YuxinxinChen/subscriptions", "organizations_url": "https://api.github.com/users/YuxinxinChen/orgs", "repos_url": "https://api.github.com/users/YuxinxinChen/repos", "events_url": "https://api.github.com/users/YuxinxinChen/events{/privacy}", "received_events_url": "https://api.github.com/users/YuxinxinChen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-08-07T22:46:08Z", "updated_at": "2018-02-16T19:08:00Z", "closed_at": "2018-02-16T19:08:00Z", "author_association": "NONE", "body_html": "<p>System information</p>\n<div class=\"highlight highlight-source-shell\"><pre>== cat /etc/issue ===============================================\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 <span class=\"pl-c\"><span class=\"pl-c\">#</span>1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span>\nVERSION=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>7 (Core)<span class=\"pl-pds\">\"</span></span>\nVERSION_ID=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>7<span class=\"pl-pds\">\"</span></span>\nCENTOS_MANTISBT_PROJECT_VERSION=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>7<span class=\"pl-pds\">\"</span></span>\nREDHAT_SUPPORT_PRODUCT_VERSION=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>7<span class=\"pl-pds\">\"</span></span>\n\n== are we <span class=\"pl-k\">in</span> docker =============================================\nNo\n\n== compiler =====================================================\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software<span class=\"pl-k\">;</span> see the <span class=\"pl-c1\">source</span> <span class=\"pl-k\">for</span> copying conditions.  There is NO\nwarranty<span class=\"pl-k\">;</span> not even <span class=\"pl-k\">for</span> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 <span class=\"pl-c\"><span class=\"pl-c\">#</span>1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span>\n\n== check pips ===================================================\nnumpy (1.13.0)\nprotobuf (3.3.0)\ntensorflow-gpu (1.2.0)\n\n== check <span class=\"pl-k\">for</span> virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.2.0\ntf.GIT_VERSION = v1.2.0-rc2-21-g12f033d\ntf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH is <span class=\"pl-c1\">unset</span>\nDYLD_LIBRARY_PATH is <span class=\"pl-c1\">unset</span>\n\n== nvidia-smi ===================================================\nMon Aug  7 21:00:40 2017\n+-----------------------------------------------------------------------------+\n<span class=\"pl-k\">|</span> NVIDIA-SMI 375.51                 Driver Version: 375.51                    <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span> GPU  Name        Persistence-M<span class=\"pl-k\">|</span> Bus-Id        Disp.A <span class=\"pl-k\">|</span> Volatile Uncorr. ECC <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> Fan  Temp  Perf  Pwr:Usage/Cap<span class=\"pl-k\">|</span>         Memory-Usage <span class=\"pl-k\">|</span> GPU-Util  Compute M. <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>===============================+======================+======================<span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>   0  Tesla K80           On   <span class=\"pl-k\">|</span> 0000:00:1E.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   51C    P0    72W / 149W <span class=\"pl-k\">|</span>  10944MiB / 11439MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n<span class=\"pl-k\">|</span> Processes:                                                       GPU Memory <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>  GPU       PID  Type  Process name                               Usage      <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>=============================================================================<span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>    0      2886    C   /bin/python                                  10938MiB <span class=\"pl-k\">|</span>\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a</pre></div>\n<p>I am running the imagenet_distributed_train.py of inception: <a href=\"https://github.com/tensorflow/models/tree/master/inception\">https://github.com/tensorflow/models/tree/master/inception</a>, with 16 AWS p2x2 machines. I didn't change any code of inception and follow the guidance to run imagenet_distributed_train using parallel-ssh.</p>\n<p>The script I use to run parallel-ssh:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> pssh.pssh_client <span class=\"pl-k\">import</span> ParallelSSHClient\n<span class=\"pl-k\">import</span> datetime\n<span class=\"pl-k\">from</span> pprint <span class=\"pl-k\">import</span> pprint\n<span class=\"pl-k\">from</span> pssh.utils <span class=\"pl-k\">import</span> load_private_key\n\noutput_ps <span class=\"pl-k\">=</span> []\noutput_worker <span class=\"pl-k\">=</span> []\nsome host ip here\nps <span class=\"pl-k\">=</span> [host1,host2,host3]\nworker <span class=\"pl-k\">=</span> [host0,host1,host2,host3,host4,host5,host6,host7,host8,host9,host10,host11,host12,host13,host14,host15]\nclient_ps <span class=\"pl-k\">=</span> ParallelSSHClient(ps, <span class=\"pl-v\">user</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>centos<span class=\"pl-pds\">'</span></span>)\nclient_worker <span class=\"pl-k\">=</span> ParallelSSHClient(worker, <span class=\"pl-v\">user</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>centos<span class=\"pl-pds\">'</span></span>)\n\noutput_ps <span class=\"pl-k\">=</span> client_ps.run_command(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">host_args</span><span class=\"pl-k\">=</span>(\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_ps.sh --job_name ps --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_ps.sh --job_name ps --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_ps.sh --job_name ps --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    ))\n\noutput_worker <span class=\"pl-k\">=</span> client_worker.run_command( <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">host_args</span><span class=\"pl-k\">=</span>(\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 3 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 4 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 5 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 6 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 7 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 8 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 9 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 10 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 11 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 12 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 13 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 14 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n    (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/imagenet/run_worker.sh --job_name worker --task_id 15 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******<span class=\"pl-pds\">'</span></span>),\n       ))\n\nclient_ps.join(output_ps)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>client_worker.join(output_worker)</span>\npprint(output_ps.values()[<span class=\"pl-c1\">0</span>].exit_code)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>pprint(output_worker.values()[0].exit_code)</span>\n\n<span class=\"pl-k\">for</span> host, host_output <span class=\"pl-k\">in</span> output_ps.items():\n    <span class=\"pl-k\">for</span> line <span class=\"pl-k\">in</span> host_output.stdout:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Host [<span class=\"pl-c1\">%s</span>] - <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (host, line))</pre></div>\n<p>I think this script worked fine because I logged in every machine and checked with ps command and ensured the program was running with correct parameters. Then the program just worked fine but to some point, it started to save checkpoints forever(here is the output of worker0):</p>\n<div class=\"highlight highlight-source-shell\"><pre>INFO:tensorflow:Worker 0: 2017-08-04 06:46:08.510727: step 2340, loss = 11.22(2.0 examples/sec<span class=\"pl-k\">;</span> 15.788  sec/batch)\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2017-08-04 06:53:55.553703: step 2370, loss = 10.30(2.1 examples/sec<span class=\"pl-k\">;</span> 15.573  sec/batch)\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2017-08-04 07:01:44.226068: step 2400, loss = 10.84(2.1 examples/sec<span class=\"pl-k\">;</span> 15.421  sec/batch)\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\n(same saving checkpoint output forever)</pre></div>\n<p>I ran nvidia-smi and found the GPU wasn't working and same with other nodes. The output of worker 1-15 just stucked on step 2400 and didn't do any progress. I tried this several time on new set of 16 machines but it all stucked on saving checkpoint forever problem at some time. I guess it might be a bug in tensorflow? Or does this caused network failure? but it didn't retrun any network failure error.</p>", "body_text": "System information\n== cat /etc/issue ===============================================\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"7 (Core)\"\nVERSION_ID=\"7\"\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.13.0)\nprotobuf (3.3.0)\ntensorflow-gpu (1.2.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.2.0\ntf.GIT_VERSION = v1.2.0-rc2-21-g12f033d\ntf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nMon Aug  7 21:00:40 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |\n| N/A   51C    P0    72W / 149W |  10944MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      2886    C   /bin/python                                  10938MiB |\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\nI am running the imagenet_distributed_train.py of inception: https://github.com/tensorflow/models/tree/master/inception, with 16 AWS p2x2 machines. I didn't change any code of inception and follow the guidance to run imagenet_distributed_train using parallel-ssh.\nThe script I use to run parallel-ssh:\nfrom pssh.pssh_client import ParallelSSHClient\nimport datetime\nfrom pprint import pprint\nfrom pssh.utils import load_private_key\n\noutput_ps = []\noutput_worker = []\nsome host ip here\nps = [host1,host2,host3]\nworker = [host0,host1,host2,host3,host4,host5,host6,host7,host8,host9,host10,host11,host12,host13,host14,host15]\nclient_ps = ParallelSSHClient(ps, user='centos')\nclient_worker = ParallelSSHClient(worker, user='centos')\n\noutput_ps = client_ps.run_command('%s', host_args=(\n    ('/imagenet/run_ps.sh --job_name ps --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_ps.sh --job_name ps --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_ps.sh --job_name ps --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ))\n\noutput_worker = client_worker.run_command( '%s', host_args=(\n    ('/imagenet/run_worker.sh --job_name worker --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 3 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 4 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 5 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 6 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 7 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 8 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 9 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 10 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 11 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 12 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 13 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 14 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n    ('/imagenet/run_worker.sh --job_name worker --task_id 15 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\n       ))\n\nclient_ps.join(output_ps)\n#client_worker.join(output_worker)\npprint(output_ps.values()[0].exit_code)\n#pprint(output_worker.values()[0].exit_code)\n\nfor host, host_output in output_ps.items():\n    for line in host_output.stdout:\n        print(\"Host [%s] - %s\" % (host, line))\nI think this script worked fine because I logged in every machine and checked with ps command and ensured the program was running with correct parameters. Then the program just worked fine but to some point, it started to save checkpoints forever(here is the output of worker0):\nINFO:tensorflow:Worker 0: 2017-08-04 06:46:08.510727: step 2340, loss = 11.22(2.0 examples/sec; 15.788  sec/batch)\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2017-08-04 06:53:55.553703: step 2370, loss = 10.30(2.1 examples/sec; 15.573  sec/batch)\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2017-08-04 07:01:44.226068: step 2400, loss = 10.84(2.1 examples/sec; 15.421  sec/batch)\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\n(same saving checkpoint output forever)\nI ran nvidia-smi and found the GPU wasn't working and same with other nodes. The output of worker 1-15 just stucked on step 2400 and didn't do any progress. I tried this several time on new set of 16 machines but it all stucked on saving checkpoint forever problem at some time. I guess it might be a bug in tensorflow? Or does this caused network failure? but it didn't retrun any network failure error.", "body": "System information\r\n```bash\r\n== cat /etc/issue ===============================================\r\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ip-172-30-4-87 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow-gpu (1.2.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.0\r\ntf.GIT_VERSION = v1.2.0-rc2-21-g12f033d\r\ntf.COMPILER_VERSION = v1.2.0-rc2-21-g12f033d\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon Aug  7 21:00:40 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |\r\n| N/A   51C    P0    72W / 149W |  10944MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      2886    C   /bin/python                                  10938MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n```\r\nI am running the imagenet_distributed_train.py of inception: https://github.com/tensorflow/models/tree/master/inception, with 16 AWS p2x2 machines. I didn't change any code of inception and follow the guidance to run imagenet_distributed_train using parallel-ssh.\r\n\r\nThe script I use to run parallel-ssh:\r\n```python\r\nfrom pssh.pssh_client import ParallelSSHClient\r\nimport datetime\r\nfrom pprint import pprint\r\nfrom pssh.utils import load_private_key\r\n\r\noutput_ps = []\r\noutput_worker = []\r\nsome host ip here\r\nps = [host1,host2,host3]\r\nworker = [host0,host1,host2,host3,host4,host5,host6,host7,host8,host9,host10,host11,host12,host13,host14,host15]\r\nclient_ps = ParallelSSHClient(ps, user='centos')\r\nclient_worker = ParallelSSHClient(worker, user='centos')\r\n\r\noutput_ps = client_ps.run_command('%s', host_args=(\r\n    ('/imagenet/run_ps.sh --job_name ps --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_ps.sh --job_name ps --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_ps.sh --job_name ps --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ))\r\n\r\noutput_worker = client_worker.run_command( '%s', host_args=(\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 0 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 1 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 2 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 3 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 4 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 5 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 6 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 7 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 8 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 9 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 10 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 11 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 12 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 13 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 14 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n    ('/imagenet/run_worker.sh --job_name worker --task_id 15 --batch_size 32 --num_ps 3 --num_workers 16 --ps_hosts ******* --worker_hosts *******'),\r\n       ))\r\n\r\nclient_ps.join(output_ps)\r\n#client_worker.join(output_worker)\r\npprint(output_ps.values()[0].exit_code)\r\n#pprint(output_worker.values()[0].exit_code)\r\n\r\nfor host, host_output in output_ps.items():\r\n    for line in host_output.stdout:\r\n        print(\"Host [%s] - %s\" % (host, line))\r\n```\r\nI think this script worked fine because I logged in every machine and checked with ps command and ensured the program was running with correct parameters. Then the program just worked fine but to some point, it started to save checkpoints forever(here is the output of worker0):\r\n\r\n```bash\r\nINFO:tensorflow:Worker 0: 2017-08-04 06:46:08.510727: step 2340, loss = 11.22(2.0 examples/sec; 15.788  sec/batch)\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Worker 0: 2017-08-04 06:53:55.553703: step 2370, loss = 10.30(2.1 examples/sec; 15.573  sec/batch)\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Worker 0: 2017-08-04 07:01:44.226068: step 2400, loss = 10.84(2.1 examples/sec; 15.421  sec/batch)\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Running Summary operation on the chief.\r\nINFO:tensorflow:Finished running Summary operation.\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path /home/centos/experiment_16W_2P_32BS_2017-08-03_IMAGENET_W0/model.ckpt\r\n(same saving checkpoint output forever)\r\n```\r\nI ran nvidia-smi and found the GPU wasn't working and same with other nodes. The output of worker 1-15 just stucked on step 2400 and didn't do any progress. I tried this several time on new set of 16 machines but it all stucked on saving checkpoint forever problem at some time. I guess it might be a bug in tensorflow? Or does this caused network failure? but it didn't retrun any network failure error."}