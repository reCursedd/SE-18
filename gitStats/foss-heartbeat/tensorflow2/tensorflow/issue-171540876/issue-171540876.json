{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3867", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3867/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3867/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3867/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3867", "id": 171540876, "node_id": "MDU6SXNzdWUxNzE1NDA4NzY=", "number": 3867, "title": "parallel_iterations option behaves differently if per_process_gpu_memory_fraction is set", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2016-08-16T23:23:06Z", "updated_at": "2017-01-26T17:58:57Z", "closed_at": "2017-01-24T22:06:18Z", "author_association": "NONE", "body_html": "<p>I'm seeing inconsistent behavior with the <code>parallel_iterations</code> option in dynamic RNNs. For large RNNs, if I set <code>parallel_iterations</code> to too high a number, I run out of memory. However, setting <code>parallel_iterations</code> to 1 or 2 solves the problem. This is only true however if <code>per_process_gpu_memory_fraction</code> is not set. If it is set, even to 1.0, then no matter what value I set <code>parallel_iterations</code> to, I always run out of memory (for this large RNN).</p>\n<p>I am not sure if this is related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"158007947\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2610\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2610/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2610\">#2610</a> or not.</p>", "body_text": "I'm seeing inconsistent behavior with the parallel_iterations option in dynamic RNNs. For large RNNs, if I set parallel_iterations to too high a number, I run out of memory. However, setting parallel_iterations to 1 or 2 solves the problem. This is only true however if per_process_gpu_memory_fraction is not set. If it is set, even to 1.0, then no matter what value I set parallel_iterations to, I always run out of memory (for this large RNN).\nI am not sure if this is related to #2610 or not.", "body": "I'm seeing inconsistent behavior with the `parallel_iterations` option in dynamic RNNs. For large RNNs, if I set `parallel_iterations` to too high a number, I run out of memory. However, setting `parallel_iterations` to 1 or 2 solves the problem. This is only true however if `per_process_gpu_memory_fraction` is not set. If it is set, even to 1.0, then no matter what value I set `parallel_iterations` to, I always run out of memory (for this large RNN).\n\nI am not sure if this is related to #2610 or not.\n"}