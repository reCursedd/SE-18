{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/391270398", "html_url": "https://github.com/tensorflow/tensorflow/issues/8517#issuecomment-391270398", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8517", "id": 391270398, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MTI3MDM5OA==", "user": {"login": "Vargeel", "id": 7975891, "node_id": "MDQ6VXNlcjc5NzU4OTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7975891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Vargeel", "html_url": "https://github.com/Vargeel", "followers_url": "https://api.github.com/users/Vargeel/followers", "following_url": "https://api.github.com/users/Vargeel/following{/other_user}", "gists_url": "https://api.github.com/users/Vargeel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Vargeel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Vargeel/subscriptions", "organizations_url": "https://api.github.com/users/Vargeel/orgs", "repos_url": "https://api.github.com/users/Vargeel/repos", "events_url": "https://api.github.com/users/Vargeel/events{/privacy}", "received_events_url": "https://api.github.com/users/Vargeel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-23T08:47:47Z", "updated_at": "2018-05-23T08:50:19Z", "author_association": "NONE", "body_html": "<p>Hi Oliver and Adam,<br>\nI ended up managing to avoid this issue during its previous occurence. (sorry I went silent on the thread, since this is an issue that arised on a work project it severly limited my ability to share code)<br>\nThe core of my problem was that I had some custom CUDA code running for some operators and they internally initialised a device number without receiving it from tensorflow. In the end the only way I managed to run my code efficiently was to have the custom op run on its own GPU while the rest of the graph was parallelized. Definitely hurt perfomance but it did manage to artificially increase my batch size.</p>", "body_text": "Hi Oliver and Adam,\nI ended up managing to avoid this issue during its previous occurence. (sorry I went silent on the thread, since this is an issue that arised on a work project it severly limited my ability to share code)\nThe core of my problem was that I had some custom CUDA code running for some operators and they internally initialised a device number without receiving it from tensorflow. In the end the only way I managed to run my code efficiently was to have the custom op run on its own GPU while the rest of the graph was parallelized. Definitely hurt perfomance but it did manage to artificially increase my batch size.", "body": "Hi Oliver and Adam, \r\nI ended up managing to avoid this issue during its previous occurence. (sorry I went silent on the thread, since this is an issue that arised on a work project it severly limited my ability to share code)\r\nThe core of my problem was that I had some custom CUDA code running for some operators and they internally initialised a device number without receiving it from tensorflow. In the end the only way I managed to run my code efficiently was to have the custom op run on its own GPU while the rest of the graph was parallelized. Definitely hurt perfomance but it did manage to artificially increase my batch size."}