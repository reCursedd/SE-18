{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/292381423", "html_url": "https://github.com/tensorflow/tensorflow/issues/8574#issuecomment-292381423", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8574", "id": 292381423, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MjM4MTQyMw==", "user": {"login": "gallybaba", "id": 6888151, "node_id": "MDQ6VXNlcjY4ODgxNTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6888151?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gallybaba", "html_url": "https://github.com/gallybaba", "followers_url": "https://api.github.com/users/gallybaba/followers", "following_url": "https://api.github.com/users/gallybaba/following{/other_user}", "gists_url": "https://api.github.com/users/gallybaba/gists{/gist_id}", "starred_url": "https://api.github.com/users/gallybaba/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gallybaba/subscriptions", "organizations_url": "https://api.github.com/users/gallybaba/orgs", "repos_url": "https://api.github.com/users/gallybaba/repos", "events_url": "https://api.github.com/users/gallybaba/events{/privacy}", "received_events_url": "https://api.github.com/users/gallybaba/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-07T00:52:15Z", "updated_at": "2017-04-07T00:52:15Z", "author_association": "NONE", "body_html": "<p>I am having similar issue.<br>\nI am saving checkpoint in the same session and trying to read / restore it later in the same session. I get this error<br>\nNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt<br>\n[[Node: save/RestoreV2_24 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_24/tensor_names, save/RestoreV2_24/shape_and_slices)]]</p>\n<p>I know for sure the model is saved. For some reason, most of the checkpoint files are removed.</p>\n<p>session creation<br>\nwith tf.Graph().as_default():<br>\n# global step<br>\nbatch_start_index = tf.placeholder(tf.int32, shape=(1), name=\"BatchStartIndex\")<br>\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)<br>\nprint(\"Global Step\", global_step)</p>\n<p>Saver<br>\nsaver = tf.train.Saver()<br>\nprint(\"RNN Saver: \", saver)<br>\nsess = tf.Session()<br>\nprint(\"RNN Session: \", sess)<br>\nsw = tf.summary.FileWriter(base_checkpoint_path, sess.graph)<br>\nprint(\"Summary Writer: \", sw)<br>\nsess.run(init)<br>\nprint(\"sesson init run complete\")</p>\n<p>Saving session<br>\nfile_name = \"modelrnnopt_\" + str(step) + \".ckpt\"<br>\ncheckpoint_file = os.path.join(base_checkpoint_path, file_name)<br>\nsaver.save(sess, checkpoint_file, global_step=step)<br>\ncheckpoint_files[checkpoint_file] = 0<br>\n--------------------&gt;&gt;&gt;&gt; I see this file getting saved<br>\nRestoring session<br>\nprint(\"Running checkpoint: \", cp)<br>\ntest_feed_dict = build_rnn_feed(HYPER_BATCH_SIZE, std_test_data, std_test_label,128)<br>\nsaver.restore(sess, cp) -------------------------------------&gt;&gt;&gt;&gt;&gt;&gt;&gt; This is where it fails</p>\n<h4>cp =&gt; D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt</h4>\n<pre><code>    test_prediction = sess.run(accuracy, feed_dict = test_feed_dict)\n</code></pre>\n<p>Why are the checkpoints removed?<br>\nbase_checkpoint_path = \"D:\\temp\\rnnopt\"</p>", "body_text": "I am having similar issue.\nI am saving checkpoint in the same session and trying to read / restore it later in the same session. I get this error\nNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt\n[[Node: save/RestoreV2_24 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_24/tensor_names, save/RestoreV2_24/shape_and_slices)]]\nI know for sure the model is saved. For some reason, most of the checkpoint files are removed.\nsession creation\nwith tf.Graph().as_default():\n# global step\nbatch_start_index = tf.placeholder(tf.int32, shape=(1), name=\"BatchStartIndex\")\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\nprint(\"Global Step\", global_step)\nSaver\nsaver = tf.train.Saver()\nprint(\"RNN Saver: \", saver)\nsess = tf.Session()\nprint(\"RNN Session: \", sess)\nsw = tf.summary.FileWriter(base_checkpoint_path, sess.graph)\nprint(\"Summary Writer: \", sw)\nsess.run(init)\nprint(\"sesson init run complete\")\nSaving session\nfile_name = \"modelrnnopt_\" + str(step) + \".ckpt\"\ncheckpoint_file = os.path.join(base_checkpoint_path, file_name)\nsaver.save(sess, checkpoint_file, global_step=step)\ncheckpoint_files[checkpoint_file] = 0\n-------------------->>>> I see this file getting saved\nRestoring session\nprint(\"Running checkpoint: \", cp)\ntest_feed_dict = build_rnn_feed(HYPER_BATCH_SIZE, std_test_data, std_test_label,128)\nsaver.restore(sess, cp) ------------------------------------->>>>>>> This is where it fails\ncp => D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt\n    test_prediction = sess.run(accuracy, feed_dict = test_feed_dict)\n\nWhy are the checkpoints removed?\nbase_checkpoint_path = \"D:\\temp\\rnnopt\"", "body": "I am having similar issue.\r\nI am saving checkpoint in the same session and trying to read / restore it later in the same session. I get this error \r\nNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt\r\n\t [[Node: save/RestoreV2_24 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_24/tensor_names, save/RestoreV2_24/shape_and_slices)]]\r\n\r\nI know for sure the model is saved. For some reason, most of the checkpoint files are removed.\r\n\r\nsession creation\r\nwith tf.Graph().as_default(): \r\n    # global step\r\n    batch_start_index = tf.placeholder(tf.int32, shape=(1), name=\"BatchStartIndex\")\r\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n    print(\"Global Step\", global_step)\r\n\r\nSaver\r\n    saver = tf.train.Saver()\r\n    print(\"RNN Saver: \", saver)\r\n    sess = tf.Session()\r\n    print(\"RNN Session: \", sess)\r\n    sw = tf.summary.FileWriter(base_checkpoint_path, sess.graph)\r\n    print(\"Summary Writer: \", sw)\r\n    sess.run(init)\r\n    print(\"sesson init run complete\")\r\n\r\nSaving session\r\n            file_name = \"modelrnnopt_\" + str(step) + \".ckpt\"\r\n            checkpoint_file = os.path.join(base_checkpoint_path, file_name)\r\n            saver.save(sess, checkpoint_file, global_step=step)\r\n            checkpoint_files[checkpoint_file] = 0    \r\n-------------------->>>> I see this file getting saved\r\nRestoring session\r\n        print(\"Running checkpoint: \", cp)\r\n        test_feed_dict = build_rnn_feed(HYPER_BATCH_SIZE, std_test_data, std_test_label,128)\r\n        saver.restore(sess, cp) ------------------------------------->>>>>>> This is where it fails\r\n#### cp => D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt\r\n        test_prediction = sess.run(accuracy, feed_dict = test_feed_dict)\r\n\r\nWhy are the checkpoints removed?\r\nbase_checkpoint_path = \"D:\\\\temp\\\\rnnopt\"\r\n"}