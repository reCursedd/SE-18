{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/993", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/993/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/993/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/993/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/993", "id": 131439685, "node_id": "MDU6SXNzdWUxMzE0Mzk2ODU=", "number": 993, "title": "How to run nvidia-docker with TensorFlow GPU docker", "user": {"login": "bcordo", "id": 550797, "node_id": "MDQ6VXNlcjU1MDc5Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/550797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bcordo", "html_url": "https://github.com/bcordo", "followers_url": "https://api.github.com/users/bcordo/followers", "following_url": "https://api.github.com/users/bcordo/following{/other_user}", "gists_url": "https://api.github.com/users/bcordo/gists{/gist_id}", "starred_url": "https://api.github.com/users/bcordo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bcordo/subscriptions", "organizations_url": "https://api.github.com/users/bcordo/orgs", "repos_url": "https://api.github.com/users/bcordo/repos", "events_url": "https://api.github.com/users/bcordo/events{/privacy}", "received_events_url": "https://api.github.com/users/bcordo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-02-04T18:44:16Z", "updated_at": "2016-02-06T22:02:31Z", "closed_at": "2016-02-06T22:02:31Z", "author_association": "NONE", "body_html": "<p>Thanks for looking at my issue, really appreciate it.</p>\n<h3>What I've Done</h3>\n<p>I have setup an equivalent of a Nvidia DIGITS machine (running Ubuntu 14.04 server), and am attempting to run everything in docker containers.</p>\n<ol>\n<li>I have docker installed, and have run <code>nvidia-docker run nvidia/cuda nvidia-smi</code> described <a href=\"https://github.com/NVIDIA/nvidia-docker\">here</a>, and I see my 4 TitanX graphic cards.</li>\n<li>I have also run the nvidia-docker-plugin described <a href=\"https://github.com/NVIDIA/nvidia-docker/wiki/Using-nvidia-docker-plugin\">here</a> as <code>sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker</code> and I get the output:</li>\n</ol>\n<pre><code>nvidia-docker-plugin | 2016/02/04 12:54:02 Loading NVIDIA management library\nnvidia-docker-plugin | 2016/02/04 12:54:04 Loading NVIDIA unified memory\nnvidia-docker-plugin | 2016/02/04 12:54:04 Discovering GPU devices\nnvidia-docker-plugin | 2016/02/04 12:54:05 Provisioning volumes at /var/lib/nvidia-docker/volumes\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving plugin API at /var/lib/nvidia-docker\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving remote API at localhost:3476\n</code></pre>\n<p>which signifies to me that it's working.</p>\n<ol>\n<li>I ran the tests <a href=\"https://github.com/NVIDIA/nvidia-docker/wiki/Testing-the-samples\">here</a> and they all passed.</li>\n</ol>\n<h3>My Problem</h3>\n<ol>\n<li>When I try to run the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker\">TensorFlow GPU docker image</a> using nvidia-docker</li>\n</ol>\n<p>I first run <code>sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker</code> in a tmux session.</p>\n<p>Then I run <code>nvidia-docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow-devel-gpu</code> it downloads everything and runs the docker container. Next I run ipython and try to import tensorflow but I get the following errors:</p>\n<pre><code>In [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:92] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 16b84b6e71f9\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1054] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1055] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\n</code></pre>\n<p>**I think I just have a lack in understanding about how I should run the TensorFlow container, or maybe I have to build the container using nvidia-docker.</p>\n<p>Any ideas about how to do this, or general advice about what I'm doing wrong would be amazing. **</p>\n<p>Thanks so much.</p>\n<p>Brad</p>", "body_text": "Thanks for looking at my issue, really appreciate it.\nWhat I've Done\nI have setup an equivalent of a Nvidia DIGITS machine (running Ubuntu 14.04 server), and am attempting to run everything in docker containers.\n\nI have docker installed, and have run nvidia-docker run nvidia/cuda nvidia-smi described here, and I see my 4 TitanX graphic cards.\nI have also run the nvidia-docker-plugin described here as sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker and I get the output:\n\nnvidia-docker-plugin | 2016/02/04 12:54:02 Loading NVIDIA management library\nnvidia-docker-plugin | 2016/02/04 12:54:04 Loading NVIDIA unified memory\nnvidia-docker-plugin | 2016/02/04 12:54:04 Discovering GPU devices\nnvidia-docker-plugin | 2016/02/04 12:54:05 Provisioning volumes at /var/lib/nvidia-docker/volumes\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving plugin API at /var/lib/nvidia-docker\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving remote API at localhost:3476\n\nwhich signifies to me that it's working.\n\nI ran the tests here and they all passed.\n\nMy Problem\n\nWhen I try to run the TensorFlow GPU docker image using nvidia-docker\n\nI first run sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker in a tmux session.\nThen I run nvidia-docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow-devel-gpu it downloads everything and runs the docker container. Next I run ipython and try to import tensorflow but I get the following errors:\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:92] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 16b84b6e71f9\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1054] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1055] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\n\n**I think I just have a lack in understanding about how I should run the TensorFlow container, or maybe I have to build the container using nvidia-docker.\nAny ideas about how to do this, or general advice about what I'm doing wrong would be amazing. **\nThanks so much.\nBrad", "body": "Thanks for looking at my issue, really appreciate it. \n### What I've Done\n\nI have setup an equivalent of a Nvidia DIGITS machine (running Ubuntu 14.04 server), and am attempting to run everything in docker containers.\n1. I have docker installed, and have run `nvidia-docker run nvidia/cuda nvidia-smi` described [here](https://github.com/NVIDIA/nvidia-docker), and I see my 4 TitanX graphic cards. \n2. I have also run the nvidia-docker-plugin described [here](https://github.com/NVIDIA/nvidia-docker/wiki/Using-nvidia-docker-plugin) as `sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker` and I get the output:\n\n```\nnvidia-docker-plugin | 2016/02/04 12:54:02 Loading NVIDIA management library\nnvidia-docker-plugin | 2016/02/04 12:54:04 Loading NVIDIA unified memory\nnvidia-docker-plugin | 2016/02/04 12:54:04 Discovering GPU devices\nnvidia-docker-plugin | 2016/02/04 12:54:05 Provisioning volumes at /var/lib/nvidia-docker/volumes\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving plugin API at /var/lib/nvidia-docker\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving remote API at localhost:3476\n```\n\nwhich signifies to me that it's working.\n1. I ran the tests [here](https://github.com/NVIDIA/nvidia-docker/wiki/Testing-the-samples) and they all passed. \n### My Problem\n1. When I try to run the [TensorFlow GPU docker image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) using nvidia-docker\n\nI first run `sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker` in a tmux session. \n\nThen I run `nvidia-docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow-devel-gpu` it downloads everything and runs the docker container. Next I run ipython and try to import tensorflow but I get the following errors:\n\n```\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:92] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 16b84b6e71f9\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1054] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1055] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\n```\n\n**I think I just have a lack in understanding about how I should run the TensorFlow container, or maybe I have to build the container using nvidia-docker. \n\nAny ideas about how to do this, or general advice about what I'm doing wrong would be amazing. **\n\nThanks so much.\n\nBrad\n"}