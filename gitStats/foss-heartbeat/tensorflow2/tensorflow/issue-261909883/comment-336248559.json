{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336248559", "html_url": "https://github.com/tensorflow/tensorflow/issues/13429#issuecomment-336248559", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13429", "id": 336248559, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjI0ODU1OQ==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-12T19:57:41Z", "updated_at": "2017-10-12T19:57:41Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a></p>\n<p>The re-opening of name-scope when re-entering a variable scope is entirely on puropse. In this example:</p>\n<pre><code>with tf.variable_scope('a') as scope:\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\nwith tf.variable_scope(scope):\n    w = v * 2\nprint(w.name)    # expected: a/mul:0\n#result:  'a_1/mul:0'\n</code></pre>\n<p>the current result, <code>a_1/mul:0</code>, is entirely expected. Imagine you're sharing a large subgraph, a large LSTM cell or a whole net. When you re-enter the scope of that graph, you'd rather add a <code>_1</code> to the root scope than add it to every single op, right? (Note that every op in the graph must have a unique name, so the <code>_1</code> will appear somewhere if you're re-creating a subgraph.)</p>\n<p>The problem you note appears in a situation where you re-enter the scope not to re-create a subgraph but to continue some construction. This used to be more rare than reusing before, but with layers it's becoming more and more common, am I understanding this right? In any case, I think there is a simple way that was designed to do what you want, namely jump back to the original name scope, it's through <code>scope.original_name_scope</code>. So you can just rewrite your above example like this:</p>\n<pre><code>with tf.variable_scope('a') as scope:\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\nwith tf.variable_scope(scope):\n   with tf.name_scope(scope.original_name_scope):\n     w = v * 2\nprint(w.name)\n</code></pre>\n<p>Does that work? I thought we were doing that everywhere in Layer class where it makes sense, but now I'm less sure. Can you check? I think this should make it possible to just make changes in Layer and not touch variable_scope, but maybe there's a bug somewhere?</p>", "body_text": "Hi @reedwm\nThe re-opening of name-scope when re-entering a variable scope is entirely on puropse. In this example:\nwith tf.variable_scope('a') as scope:\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\nwith tf.variable_scope(scope):\n    w = v * 2\nprint(w.name)    # expected: a/mul:0\n#result:  'a_1/mul:0'\n\nthe current result, a_1/mul:0, is entirely expected. Imagine you're sharing a large subgraph, a large LSTM cell or a whole net. When you re-enter the scope of that graph, you'd rather add a _1 to the root scope than add it to every single op, right? (Note that every op in the graph must have a unique name, so the _1 will appear somewhere if you're re-creating a subgraph.)\nThe problem you note appears in a situation where you re-enter the scope not to re-create a subgraph but to continue some construction. This used to be more rare than reusing before, but with layers it's becoming more and more common, am I understanding this right? In any case, I think there is a simple way that was designed to do what you want, namely jump back to the original name scope, it's through scope.original_name_scope. So you can just rewrite your above example like this:\nwith tf.variable_scope('a') as scope:\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\nwith tf.variable_scope(scope):\n   with tf.name_scope(scope.original_name_scope):\n     w = v * 2\nprint(w.name)\n\nDoes that work? I thought we were doing that everywhere in Layer class where it makes sense, but now I'm less sure. Can you check? I think this should make it possible to just make changes in Layer and not touch variable_scope, but maybe there's a bug somewhere?", "body": "Hi @reedwm \r\n\r\nThe re-opening of name-scope when re-entering a variable scope is entirely on puropse. In this example:\r\n\r\n```\r\nwith tf.variable_scope('a') as scope:\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\nwith tf.variable_scope(scope):\r\n    w = v * 2\r\nprint(w.name)    # expected: a/mul:0\r\n#result:  'a_1/mul:0'\r\n```\r\n\r\nthe current result, `a_1/mul:0`, is entirely expected. Imagine you're sharing a large subgraph, a large LSTM cell or a whole net. When you re-enter the scope of that graph, you'd rather add a `_1` to the root scope than add it to every single op, right? (Note that every op in the graph must have a unique name, so the `_1` will appear somewhere if you're re-creating a subgraph.)\r\n\r\nThe problem you note appears in a situation where you re-enter the scope not to re-create a subgraph but to continue some construction. This used to be more rare than reusing before, but with layers it's becoming more and more common, am I understanding this right? In any case, I think there is a simple way that was designed to do what you want, namely jump back to the original name scope, it's through `scope.original_name_scope`. So you can just rewrite your above example like this:\r\n\r\n```\r\nwith tf.variable_scope('a') as scope:\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\nwith tf.variable_scope(scope):\r\n   with tf.name_scope(scope.original_name_scope):\r\n     w = v * 2\r\nprint(w.name)\r\n```\r\n\r\nDoes that work? I thought we were doing that everywhere in Layer class where it makes sense, but now I'm less sure. Can you check? I think this should make it possible to just make changes in Layer and not touch variable_scope, but maybe there's a bug somewhere?"}