{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175451043", "pull_request_review_id": 104980632, "id": 175451043, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTQ1MTA0Mw==", "diff_hunk": "@@ -47,6 +48,38 @@ Status SoftmaxGrad(const Scope& scope, const Operation& op,\n }\n REGISTER_GRADIENT_OP(\"Softmax\", SoftmaxGrad);\n \n+Status SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\n+                                          const Operation& op,\n+                                          const std::vector<Output>&\n+                                          grad_inputs,\n+                                          std::vector<Output>* grad_outputs) {\n+  // Softmax gradient with cross entropy logits function\n+  // We multiply the backprop for cost with the gradients - op.output[1]\n+  // There is no gradient for labels\n+\n+  auto softmaxGrad = op.output(1);\n+  auto gradLoss = grad_inputs[0];\n+  auto gradGrad = grad_inputs[1];\n+\n+  auto tempGrad = Mul(scope, gradLoss, softmaxGrad);\n+\n+  // TODO: Check if this sufficient, need a check for null ?\n+  if (gradGrad.op().output_type(0) != 0) {\n+\t  auto logits = op.input(0);\n+\t  auto softmax = ops::Softmax(scope, logits);\n+\t  auto prod = ops::MatMul(scope, gradGrad, softmax);\n+\t  auto squeezeProd = ops::Squeeze(scope, prod);\n+\t  auto fProd = Sub(scope, gradGrad, squeezeProd);\n+\t  auto grad = Add(scope, tempGrad, fProd);\n+\t  grad_outputs->push_back(grad);\n+\t  grad_outputs->push_back(NoGradient());", "path": "tensorflow/cc/gradients/nn_grad.cc", "position": null, "original_position": 36, "commit_id": "4d98d2b592b7990968be33e10064d06ca35f40c9", "original_commit_id": "ace35ecca311c15724584b80b2cf0a35cbfd7eb6", "user": {"login": "nietras", "id": 10798831, "node_id": "MDQ6VXNlcjEwNzk4ODMx", "avatar_url": "https://avatars1.githubusercontent.com/u/10798831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nietras", "html_url": "https://github.com/nietras", "followers_url": "https://api.github.com/users/nietras/followers", "following_url": "https://api.github.com/users/nietras/following{/other_user}", "gists_url": "https://api.github.com/users/nietras/gists{/gist_id}", "starred_url": "https://api.github.com/users/nietras/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nietras/subscriptions", "organizations_url": "https://api.github.com/users/nietras/orgs", "repos_url": "https://api.github.com/users/nietras/repos", "events_url": "https://api.github.com/users/nietras/events{/privacy}", "received_events_url": "https://api.github.com/users/nietras/received_events", "type": "User", "site_admin": false}, "body": "@pbanavara one I could find was the softmax op test:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/softmax_op_test.py\r\n\r\nThere seems to be some kind of `SoftmaxGrad` in `nn_grad.cc` to backup the python impl (hadn't seen this before):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/21c774cc1fc2aec8fd469c4f622fe7478b761e51/tensorflow/core/ops/nn_grad.cc#L25\r\n\r\nFor this I could find the following test:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/838a8f54f92452a15e3bb62a23ad5cd67e86933f/tensorflow/cc/gradients/nn_grad_test.cc#L95\r\n\r\nWhich I have no idea how actually does a proper test of this... seems pretty small to me.", "created_at": "2018-03-19T14:21:19Z", "updated_at": "2018-07-12T20:46:26Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14727#discussion_r175451043", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14727", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175451043"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14727#discussion_r175451043"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14727"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=600054\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pbanavara\">@pbanavara</a> one I could find was the softmax op test:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/softmax_op_test.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/softmax_op_test.py</a></p>\n<p>There seems to be some kind of <code>SoftmaxGrad</code> in <code>nn_grad.cc</code> to backup the python impl (hadn't seen this before):</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/21c774cc1fc2aec8fd469c4f622fe7478b761e51/tensorflow/core/ops/nn_grad.cc#L25\">tensorflow/tensorflow/core/ops/nn_grad.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 25\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/21c774cc1fc2aec8fd469c4f622fe7478b761e51\">21c774c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L25\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"25\"></td>\n          <td id=\"LC25\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> Status <span class=\"pl-en\">SoftmaxGrad</span>(<span class=\"pl-k\">const</span> AttrSlice&amp; attrs, FunctionDef* g) { </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>For this I could find the following test:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/838a8f54f92452a15e3bb62a23ad5cd67e86933f/tensorflow/cc/gradients/nn_grad_test.cc#L95\">tensorflow/tensorflow/cc/gradients/nn_grad_test.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 95\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/838a8f54f92452a15e3bb62a23ad5cd67e86933f\">838a8f5</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L95\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"95\"></td>\n          <td id=\"LC95\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-en\">TEST_F</span>(NNGradTest, SoftmaxGrad) { </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>Which I have no idea how actually does a proper test of this... seems pretty small to me.</p>", "body_text": "@pbanavara one I could find was the softmax op test:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/softmax_op_test.py\nThere seems to be some kind of SoftmaxGrad in nn_grad.cc to backup the python impl (hadn't seen this before):\n\n  \n    \n      tensorflow/tensorflow/core/ops/nn_grad.cc\n    \n    \n         Line 25\n      in\n      21c774c\n    \n    \n    \n    \n\n        \n          \n           Status SoftmaxGrad(const AttrSlice& attrs, FunctionDef* g) { \n        \n    \n  \n\n\nFor this I could find the following test:\n\n  \n    \n      tensorflow/tensorflow/cc/gradients/nn_grad_test.cc\n    \n    \n         Line 95\n      in\n      838a8f5\n    \n    \n    \n    \n\n        \n          \n           TEST_F(NNGradTest, SoftmaxGrad) { \n        \n    \n  \n\n\nWhich I have no idea how actually does a proper test of this... seems pretty small to me.", "in_reply_to_id": 170222650}