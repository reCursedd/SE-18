{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/170221735", "pull_request_review_id": 98873982, "id": 170221735, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDIyMTczNQ==", "diff_hunk": "@@ -47,6 +48,38 @@ Status SoftmaxGrad(const Scope& scope, const Operation& op,\n }\n REGISTER_GRADIENT_OP(\"Softmax\", SoftmaxGrad);\n \n+Status SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\n+                                          const Operation& op,\n+                                          const std::vector<Output>&\n+                                          grad_inputs,\n+                                          std::vector<Output>* grad_outputs) {\n+  // Softmax gradient with cross entropy logits function\n+  // We multiply the backprop for cost with the gradients - op.output[1]\n+  // There is no gradient for labels\n+\n+  auto softmaxGrad = op.output(1);\n+  auto gradLoss = grad_inputs[0];\n+  auto gradGrad = grad_inputs[1];\n+\n+  auto tempGrad = Mul(scope, gradLoss, softmaxGrad);\n+\n+  // TODO: Check if this sufficient, need a check for null ?\n+  if (gradGrad.op().output_type(0) != 0) {\n+\t  auto logits = op.input(0);\n+\t  auto softmax = ops::Softmax(scope, logits);\n+\t  auto prod = ops::MatMul(scope, gradGrad, softmax);", "path": "tensorflow/cc/gradients/nn_grad.cc", "position": null, "original_position": 31, "commit_id": "4d98d2b592b7990968be33e10064d06ca35f40c9", "original_commit_id": "ace35ecca311c15724584b80b2cf0a35cbfd7eb6", "user": {"login": "nietras", "id": 10798831, "node_id": "MDQ6VXNlcjEwNzk4ODMx", "avatar_url": "https://avatars1.githubusercontent.com/u/10798831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nietras", "html_url": "https://github.com/nietras", "followers_url": "https://api.github.com/users/nietras/followers", "following_url": "https://api.github.com/users/nietras/following{/other_user}", "gists_url": "https://api.github.com/users/nietras/gists{/gist_id}", "starred_url": "https://api.github.com/users/nietras/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nietras/subscriptions", "organizations_url": "https://api.github.com/users/nietras/orgs", "repos_url": "https://api.github.com/users/nietras/repos", "events_url": "https://api.github.com/users/nietras/events{/privacy}", "received_events_url": "https://api.github.com/users/nietras/received_events", "type": "User", "site_admin": false}, "body": "For ref the python code is:\r\n```python\r\nlogits = op.inputs[0]\r\n    softmax = nn_ops.softmax(logits)\r\n\r\n    grad += ((grad_grad - array_ops.squeeze(\r\n        math_ops.matmul(grad_grad[:, None, :],\r\n                        softmax[:, :, None]), axis=1)) * softmax)\r\n```", "created_at": "2018-02-23T10:50:40Z", "updated_at": "2018-07-12T20:46:25Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14727#discussion_r170221735", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14727", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/170221735"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14727#discussion_r170221735"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14727"}}, "body_html": "<p>For ref the python code is:</p>\n<div class=\"highlight highlight-source-python\"><pre>logits <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n    softmax <span class=\"pl-k\">=</span> nn_ops.softmax(logits)\n\n    grad <span class=\"pl-k\">+=</span> ((grad_grad <span class=\"pl-k\">-</span> array_ops.squeeze(\n        math_ops.matmul(grad_grad[:, <span class=\"pl-c1\">None</span>, :],\n                        softmax[:, :, <span class=\"pl-c1\">None</span>]), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)) <span class=\"pl-k\">*</span> softmax)</pre></div>", "body_text": "For ref the python code is:\nlogits = op.inputs[0]\n    softmax = nn_ops.softmax(logits)\n\n    grad += ((grad_grad - array_ops.squeeze(\n        math_ops.matmul(grad_grad[:, None, :],\n                        softmax[:, :, None]), axis=1)) * softmax)", "in_reply_to_id": 170221530}