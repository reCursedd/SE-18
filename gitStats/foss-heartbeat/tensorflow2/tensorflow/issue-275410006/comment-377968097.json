{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377968097", "html_url": "https://github.com/tensorflow/tensorflow/pull/14727#issuecomment-377968097", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14727", "id": 377968097, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Nzk2ODA5Nw==", "user": {"login": "rajha-korithrien", "id": 8145476, "node_id": "MDQ6VXNlcjgxNDU0NzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8145476?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rajha-korithrien", "html_url": "https://github.com/rajha-korithrien", "followers_url": "https://api.github.com/users/rajha-korithrien/followers", "following_url": "https://api.github.com/users/rajha-korithrien/following{/other_user}", "gists_url": "https://api.github.com/users/rajha-korithrien/gists{/gist_id}", "starred_url": "https://api.github.com/users/rajha-korithrien/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rajha-korithrien/subscriptions", "organizations_url": "https://api.github.com/users/rajha-korithrien/orgs", "repos_url": "https://api.github.com/users/rajha-korithrien/repos", "events_url": "https://api.github.com/users/rajha-korithrien/events{/privacy}", "received_events_url": "https://api.github.com/users/rajha-korithrien/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-02T16:19:21Z", "updated_at": "2018-04-02T16:37:17Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=600054\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pbanavara\">@pbanavara</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10798831\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nietras\">@nietras</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1450614\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suharshs\">@suharshs</a>,</p>\n<p>I apologize for the long comment.</p>\n<p>I have made some (I think significant) progress on figuring out why <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=600054\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pbanavara\">@pbanavara</a> test has shape failures. Which has lead to what I think is a passing test for SoftmaxCrossEntropyGrad. There are some steps in the process, so bear with me as we go through it.</p>\n<ul>\n<li>_BroadcastMul in the original python is a function call to a function in <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/f16b50e388543d2c05698926aa1ef265090dda6b/tensorflow/python/ops/nn_grad.py#L431\">tensorflow/tensorflow/python/ops/nn_grad.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 431\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/f16b50e388543d2c05698926aa1ef265090dda6b\">f16b50e</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L431\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"431\"></td>\n          <td id=\"LC431\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">def</span> <span class=\"pl-en\">_BroadcastMul</span>(<span class=\"pl-smi\">vec</span>, <span class=\"pl-smi\">mat</span>): </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n which resizes the input vector into a matrix, so we need a C++ implementation of this.</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre>Output <span class=\"pl-en\">_BroadcastMul</span>(<span class=\"pl-k\">const</span> Scope&amp; scope, Output vec, Output mat){\n  <span class=\"pl-c\"><span class=\"pl-c\">/*</span> Multiply after broadcasting vec to match dimensions of mat.</span>\n<span class=\"pl-c\">     Args:</span>\n<span class=\"pl-c\">       vec: A 1-D tensor of dimension [D0]</span>\n<span class=\"pl-c\">       mat: A 2-D tensor of dimesnion [D0, D1]</span>\n<span class=\"pl-c\"></span>\n<span class=\"pl-c\">    Returns:</span>\n<span class=\"pl-c\">      A tensor of dimension [D0, D1], the result fo vec * mat</span>\n<span class=\"pl-c\">      we use an element for element multiply here.</span>\n<span class=\"pl-c\">  <span class=\"pl-c\">*/</span></span>\n  <span class=\"pl-k\">auto</span> reshaped = <span class=\"pl-c1\">ExpandDims</span>(scope, vec, -<span class=\"pl-c1\">1</span>);\n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">Multiply</span>(scope, reshaped, mat);\n}</pre></div>\n<ul>\n<li>The python code that <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10798831\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nietras\">@nietras</a> asked about \"math_ops.matmul(grad_grad[:, None, :], softmax[:, :,None]), axis=1)) * softmax)\" is inserting dimensions of size 1 into the respective tensors where the \"None\" is. We can use the C++ ExpandDims operator to accomplish the same thing.</li>\n<li>Once we use our C++ _BroadcastMul, we find that the MatMul C++ operator only handles 2D tensors, however we have 3D so we need to use BatchMatMul.</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre>  <span class=\"pl-k\">auto</span> grad = _BroadcastMul(scope, grad_loss, softmax_grad);\n  <span class=\"pl-k\">if</span>(!_isZero(scope, grad_grad)){\n    std::vector&lt;<span class=\"pl-k\">int</span>&gt; axis;\n    <span class=\"pl-k\">auto</span> logitsSoftmax = <span class=\"pl-c1\">Softmax</span>(scope, logits);\n\n    <span class=\"pl-k\">auto</span> grad_gradExpand = <span class=\"pl-c1\">ExpandDims</span>(scope, grad_grad, <span class=\"pl-c1\">1</span>);\n    <span class=\"pl-k\">auto</span> logitsSoftMaxExpand = <span class=\"pl-c1\">ExpandDims</span>(scope, logitsSoftmax, <span class=\"pl-c1\">2</span>);\n    <span class=\"pl-k\">auto</span> matMulResult = <span class=\"pl-c1\">BatchMatMul</span>(scope, grad_gradExpand, logitsSoftMaxExpand);\n    axis.<span class=\"pl-c1\">push_back</span>(<span class=\"pl-c1\">1</span>);\n    <span class=\"pl-k\">auto</span> squeezeResult = <span class=\"pl-c1\">Squeeze</span>(scope, matMulResult, <span class=\"pl-c1\">Squeeze::Axis</span>(axis));\n    <span class=\"pl-k\">auto</span> subtractionResult = <span class=\"pl-c1\">Subtract</span>(scope, grad_grad, squeezeResult);\n    <span class=\"pl-k\">auto</span> multiplyResult = <span class=\"pl-c1\">Multiply</span>(scope, subtractionResult, logitsSoftmax);\n    grad = <span class=\"pl-c1\">Add</span>(scope, grad, multiplyResult);\n  }</pre></div>\n<ul>\n<li>Doing the isZero check in C++ is a little clunky and can't really be done as nicely, but we can at least deal with the edge cases of Zero and Zero like:</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">bool</span> <span class=\"pl-en\">_isZero</span>(<span class=\"pl-k\">const</span> Scope&amp; scope, Output grad){\n  std::array&lt;std::string, <span class=\"pl-c1\">2</span>&gt; zeroOpTypeNames {{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ZerosLike<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Zeros<span class=\"pl-pds\">\"</span></span>}};\n  string opTypeName = grad.<span class=\"pl-c1\">op</span>().<span class=\"pl-c1\">node</span>()-&gt;<span class=\"pl-c1\">type_string</span>();\n  <span class=\"pl-k\">for</span>(<span class=\"pl-k\">auto</span>&amp; zeroOpTypeName : zeroOpTypeNames){\n    <span class=\"pl-k\">if</span>(opTypeName == zeroOpTypeName){\n      <span class=\"pl-k\">return</span> <span class=\"pl-c1\">true</span>;\n    }\n  }\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>the Operation we were provided is not named something obvious</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>we need to actually look at its contents.</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>the original python code did this by calling a utility function called</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>tensor_util.constant_value. When you dig into tensor_tuil.constant_value</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>it is a large number of 'if' statements that measure certain edge cases</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>where it is possible to get the value of the tensor without actually</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>evaluating it. There are many kinds of tensors that can not have this</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>done.</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>There is no C++ equivalent to tensor_util.constant_value so we do nothing</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span>for the moment.</span>\n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">false</span>;\n}</pre></div>\n<ul>\n<li>Please see my attached file <a href=\"https://github.com/tensorflow/tensorflow/files/1868452/nn_grad_snip.cc.txt\">nn_grad_snip.cc.txt</a><br>\nfor full working code of the SoftmaxCrossEntropyWithLogitsGrad function and support functions.</li>\n<li>Making the above changes causes the test that <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=600054\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pbanavara\">@pbanavara</a> wrote to no longer complain about shape mismatches. Here is my version of the same test code:</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-en\">TEST_F</span>(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad){\n  TensorShape <span class=\"pl-smi\">logitsShape</span>({<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">3</span>}); <span class=\"pl-c\"><span class=\"pl-c\">//</span>batch size of 5, 3 possible labels (classes),</span>\n                                  <span class=\"pl-c\"><span class=\"pl-c\">//</span>logits is what is produced by a network</span>\n                                  <span class=\"pl-c\"><span class=\"pl-c\">//</span>they are compared to labels which are the truth</span>\n  TensorShape <span class=\"pl-smi\">lossShape</span>({<span class=\"pl-c1\">5</span>}); <span class=\"pl-c\"><span class=\"pl-c\">//</span>batch size of 5, 1 value for each entry in the batch</span>\n                              <span class=\"pl-c\"><span class=\"pl-c\">//</span>loss is the difference between logits and labels</span>\n\n  TensorShape <span class=\"pl-smi\">backPropShape</span>({<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">3</span>}); <span class=\"pl-c\"><span class=\"pl-c\">//</span>the docmentation says the backprop output</span>\n                                    <span class=\"pl-c\"><span class=\"pl-c\">//</span>will have batch size x num classes as its shape</span>\n\n  <span class=\"pl-k\">auto</span> logits = <span class=\"pl-c1\">Placeholder</span>(scope_, DT_FLOAT, <span class=\"pl-c1\">Placeholder::Shape</span>(logitsShape)); <span class=\"pl-c\"><span class=\"pl-c\">//</span>estimation</span>\n  <span class=\"pl-k\">auto</span> labels = <span class=\"pl-c1\">Placeholder</span>(scope_, DT_FLOAT, <span class=\"pl-c1\">Placeholder::Shape</span>(logitsShape)); <span class=\"pl-c\"><span class=\"pl-c\">//</span>truth</span>\n\n  <span class=\"pl-k\">auto</span> y = <span class=\"pl-c1\">SoftmaxCrossEntropyWithLogits</span>(scope_, logits, labels);\n\n  <span class=\"pl-c1\">RunTest</span>({logits, labels}, {logitsShape, logitsShape}, {y.<span class=\"pl-smi\">loss</span>, y.<span class=\"pl-smi\">backprop</span>}, {lossShape, backPropShape});\n}</pre></div>\n<ul>\n<li>This is where my lack of knowledge of how Tensorflow is testing the gradient functions throws a monkey wrench into things. The test now fails with the error:</li>\n</ul>\n<pre><code>[ RUN      ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad\n2018-04-02 15:41:01.235423: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\ntensorflow/cc/gradients/nn_grad_test.cc:72: Failure\nExpected: (max_error) &lt; (1e-3), actual: 1 vs 0.001\n[  FAILED  ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad (90 ms)\n</code></pre>\n<ul>\n<li>Tracking this error down took me a while and I am still not certain my solution is correct. The next step was to isolate both the Python and C++ implementations of SoftmaxCrossEntropyWithLogitsGrad and run them as \"standalone\" programs providing them the same known inputs and ensuring that the outputs were identical. The file<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1868506/SoftmaxCrossEntropyWithLogitsGradTest.py.txt\">SoftmaxCrossEntropyWithLogitsGradTest.py.txt</a> is the isolation of the Python gradient code and the file<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1868531/softmaxcrossentropy-test.cc.txt\">softmaxcrossentropy-test.cc.txt</a> is the isolated version of the C++.\n<ul>\n<li>The isolated python code outputs</li>\n</ul>\n</li>\n</ul>\n<pre><code>0\n[ 1.00194287  0.91191393]\n1\n[[-0.63283461  0.33222499  0.30060962]\n [ 0.29762709 -0.59824544  0.30061829]]\n2\n[ 0.02  0.03]\n3\n[[ 0.04  0.05  0.06]\n [ 0.07  0.08  0.09]]\n4\n[[-0.01608398  0.00686562  0.00921836]\n [ 0.00594364 -0.01795938  0.01201574]]\n5\n[[ 0.02003886  0.02203886  0.02403886]\n [ 0.03635742  0.02735742  0.03605742]]\n</code></pre>\n<ul>\n<li>Note that the output order of the tensors is the same between the Python and C++\n<ul>\n<li>The isolated C++ code outputs</li>\n</ul>\n</li>\n</ul>\n<pre><code>y.loss: Tensor&lt;type: float shape: [2] values: 1.00194287 0.911913931&gt;\n 1.00194\n0.911914\ny.backprop: Tensor&lt;type: float shape: [2,3] values: [-0.632834613 0.332225 0.300609618]...&gt;\n-0.632835  0.332225   0.30061\n 0.297627 -0.598245  0.300618\ngrad_loss: Tensor&lt;type: double shape: [2] values: 0.02 0.03&gt;\n0.02\n0.03\ngrad_grad: Tensor&lt;type: double shape: [2,3] values: [0.04 0.05 0.06]...&gt;\n0.04 0.05 0.06\n0.07 0.08 0.09\ngrad_outputs[0]: Tensor&lt;type: float shape: [2,3] values: [-0.0160839763 0.00686561596 0.00921836123]...&gt;\n -0.016084 0.00686562 0.00921836\n0.00594364 -0.0179594  0.0120157\ngrad_outputs[1]: Tensor&lt;type: float shape: [2,3] values: [0.0200388562 0.0220388565 0.0240388587]...&gt;\n0.0200389 0.0220389 0.0240389\n0.0363574 0.0273574 0.0360574\n</code></pre>\n<ul>\n<li>At this point I am certain that the C++ gradient code is a faithful port of the python code. I have changed the inputs several times and the C++ produces what the python produces. This leads me to think that the C++ test code is somehow wrong. If we look at the generated documentation for <a href=\"https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/softmax-cross-entropy-with-logits\" rel=\"nofollow\">tensorflow::ops::SoftmaxCrossEntropyWithLogits</a> we see that while the Outputs are in the order loss, backprop the auto generated public attributes are in the <em>opposite order</em>. Because of this I changed the C++ test.</li>\n<li>From</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-en\">RunTest</span>({logits, labels}, {logitsShape, logitsShape}, {y.<span class=\"pl-smi\">loss</span>, y.<span class=\"pl-smi\">backprop</span>}, {lossShape, backPropShape});</pre></div>\n<ul>\n<li>To</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-en\">RunTest</span>({logits, labels}, {logitsShape, logitsShape}, {y.<span class=\"pl-smi\">backprop</span>, y.<span class=\"pl-smi\">loss</span>}, {backPropShape, lossShape});</pre></div>\n<ul>\n<li>Which seems to work the test will now pass... however I would like someone who knows what they are doing to verify my reasoning.</li>\n</ul>\n<pre><code>bazel test -c dbg //tensorflow/cc:gradients_nn_grad_test\n...\nINFO: Analysed target //tensorflow/cc:gradients_nn_grad_test (1 packages loaded).\nINFO: Found 1 test target...\nTarget //tensorflow/cc:gradients_nn_grad_test up-to-date:\n  bazel-bin/tensorflow/cc/gradients_nn_grad_test\nINFO: Elapsed time: 385.282s, Critical Path: 384.53s\nINFO: Build completed successfully, 4 total actions\n//tensorflow/cc:gradients_nn_grad_test                                   PASSED in 0.9s\n\nExecuted 1 out of 1 test: 1 test passes.\n</code></pre>\n<p>I hope this helps you and that someone with more knowledge can ensure this is correct because I too would like to be able to have C++ gradients for this operator (mostly so I can use it from Java).</p>", "body_text": "@pbanavara @nietras and @suharshs,\nI apologize for the long comment.\nI have made some (I think significant) progress on figuring out why @pbanavara test has shape failures. Which has lead to what I think is a passing test for SoftmaxCrossEntropyGrad. There are some steps in the process, so bear with me as we go through it.\n\n_BroadcastMul in the original python is a function call to a function in \n  \n    \n      tensorflow/tensorflow/python/ops/nn_grad.py\n    \n    \n         Line 431\n      in\n      f16b50e\n    \n    \n    \n    \n\n        \n          \n           def _BroadcastMul(vec, mat): \n        \n    \n  \n\n which resizes the input vector into a matrix, so we need a C++ implementation of this.\n\nOutput _BroadcastMul(const Scope& scope, Output vec, Output mat){\n  /* Multiply after broadcasting vec to match dimensions of mat.\n     Args:\n       vec: A 1-D tensor of dimension [D0]\n       mat: A 2-D tensor of dimesnion [D0, D1]\n\n    Returns:\n      A tensor of dimension [D0, D1], the result fo vec * mat\n      we use an element for element multiply here.\n  */\n  auto reshaped = ExpandDims(scope, vec, -1);\n  return Multiply(scope, reshaped, mat);\n}\n\nThe python code that @nietras asked about \"math_ops.matmul(grad_grad[:, None, :], softmax[:, :,None]), axis=1)) * softmax)\" is inserting dimensions of size 1 into the respective tensors where the \"None\" is. We can use the C++ ExpandDims operator to accomplish the same thing.\nOnce we use our C++ _BroadcastMul, we find that the MatMul C++ operator only handles 2D tensors, however we have 3D so we need to use BatchMatMul.\n\n  auto grad = _BroadcastMul(scope, grad_loss, softmax_grad);\n  if(!_isZero(scope, grad_grad)){\n    std::vector<int> axis;\n    auto logitsSoftmax = Softmax(scope, logits);\n\n    auto grad_gradExpand = ExpandDims(scope, grad_grad, 1);\n    auto logitsSoftMaxExpand = ExpandDims(scope, logitsSoftmax, 2);\n    auto matMulResult = BatchMatMul(scope, grad_gradExpand, logitsSoftMaxExpand);\n    axis.push_back(1);\n    auto squeezeResult = Squeeze(scope, matMulResult, Squeeze::Axis(axis));\n    auto subtractionResult = Subtract(scope, grad_grad, squeezeResult);\n    auto multiplyResult = Multiply(scope, subtractionResult, logitsSoftmax);\n    grad = Add(scope, grad, multiplyResult);\n  }\n\nDoing the isZero check in C++ is a little clunky and can't really be done as nicely, but we can at least deal with the edge cases of Zero and Zero like:\n\nbool _isZero(const Scope& scope, Output grad){\n  std::array<std::string, 2> zeroOpTypeNames {{\"ZerosLike\", \"Zeros\"}};\n  string opTypeName = grad.op().node()->type_string();\n  for(auto& zeroOpTypeName : zeroOpTypeNames){\n    if(opTypeName == zeroOpTypeName){\n      return true;\n    }\n  }\n  //the Operation we were provided is not named something obvious\n  //we need to actually look at its contents.\n  //the original python code did this by calling a utility function called\n  //tensor_util.constant_value. When you dig into tensor_tuil.constant_value\n  //it is a large number of 'if' statements that measure certain edge cases\n  //where it is possible to get the value of the tensor without actually\n  //evaluating it. There are many kinds of tensors that can not have this\n  //done.\n  //There is no C++ equivalent to tensor_util.constant_value so we do nothing\n  //for the moment.\n  return false;\n}\n\nPlease see my attached file nn_grad_snip.cc.txt\nfor full working code of the SoftmaxCrossEntropyWithLogitsGrad function and support functions.\nMaking the above changes causes the test that @pbanavara wrote to no longer complain about shape mismatches. Here is my version of the same test code:\n\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad){\n  TensorShape logitsShape({5,3}); //batch size of 5, 3 possible labels (classes),\n                                  //logits is what is produced by a network\n                                  //they are compared to labels which are the truth\n  TensorShape lossShape({5}); //batch size of 5, 1 value for each entry in the batch\n                              //loss is the difference between logits and labels\n\n  TensorShape backPropShape({5,3}); //the docmentation says the backprop output\n                                    //will have batch size x num classes as its shape\n\n  auto logits = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(logitsShape)); //estimation\n  auto labels = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(logitsShape)); //truth\n\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, logits, labels);\n\n  RunTest({logits, labels}, {logitsShape, logitsShape}, {y.loss, y.backprop}, {lossShape, backPropShape});\n}\n\nThis is where my lack of knowledge of how Tensorflow is testing the gradient functions throws a monkey wrench into things. The test now fails with the error:\n\n[ RUN      ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad\n2018-04-02 15:41:01.235423: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\ntensorflow/cc/gradients/nn_grad_test.cc:72: Failure\nExpected: (max_error) < (1e-3), actual: 1 vs 0.001\n[  FAILED  ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad (90 ms)\n\n\nTracking this error down took me a while and I am still not certain my solution is correct. The next step was to isolate both the Python and C++ implementations of SoftmaxCrossEntropyWithLogitsGrad and run them as \"standalone\" programs providing them the same known inputs and ensuring that the outputs were identical. The file\nSoftmaxCrossEntropyWithLogitsGradTest.py.txt is the isolation of the Python gradient code and the file\nsoftmaxcrossentropy-test.cc.txt is the isolated version of the C++.\n\nThe isolated python code outputs\n\n\n\n0\n[ 1.00194287  0.91191393]\n1\n[[-0.63283461  0.33222499  0.30060962]\n [ 0.29762709 -0.59824544  0.30061829]]\n2\n[ 0.02  0.03]\n3\n[[ 0.04  0.05  0.06]\n [ 0.07  0.08  0.09]]\n4\n[[-0.01608398  0.00686562  0.00921836]\n [ 0.00594364 -0.01795938  0.01201574]]\n5\n[[ 0.02003886  0.02203886  0.02403886]\n [ 0.03635742  0.02735742  0.03605742]]\n\n\nNote that the output order of the tensors is the same between the Python and C++\n\nThe isolated C++ code outputs\n\n\n\ny.loss: Tensor<type: float shape: [2] values: 1.00194287 0.911913931>\n 1.00194\n0.911914\ny.backprop: Tensor<type: float shape: [2,3] values: [-0.632834613 0.332225 0.300609618]...>\n-0.632835  0.332225   0.30061\n 0.297627 -0.598245  0.300618\ngrad_loss: Tensor<type: double shape: [2] values: 0.02 0.03>\n0.02\n0.03\ngrad_grad: Tensor<type: double shape: [2,3] values: [0.04 0.05 0.06]...>\n0.04 0.05 0.06\n0.07 0.08 0.09\ngrad_outputs[0]: Tensor<type: float shape: [2,3] values: [-0.0160839763 0.00686561596 0.00921836123]...>\n -0.016084 0.00686562 0.00921836\n0.00594364 -0.0179594  0.0120157\ngrad_outputs[1]: Tensor<type: float shape: [2,3] values: [0.0200388562 0.0220388565 0.0240388587]...>\n0.0200389 0.0220389 0.0240389\n0.0363574 0.0273574 0.0360574\n\n\nAt this point I am certain that the C++ gradient code is a faithful port of the python code. I have changed the inputs several times and the C++ produces what the python produces. This leads me to think that the C++ test code is somehow wrong. If we look at the generated documentation for tensorflow::ops::SoftmaxCrossEntropyWithLogits we see that while the Outputs are in the order loss, backprop the auto generated public attributes are in the opposite order. Because of this I changed the C++ test.\nFrom\n\nRunTest({logits, labels}, {logitsShape, logitsShape}, {y.loss, y.backprop}, {lossShape, backPropShape});\n\nTo\n\nRunTest({logits, labels}, {logitsShape, logitsShape}, {y.backprop, y.loss}, {backPropShape, lossShape});\n\nWhich seems to work the test will now pass... however I would like someone who knows what they are doing to verify my reasoning.\n\nbazel test -c dbg //tensorflow/cc:gradients_nn_grad_test\n...\nINFO: Analysed target //tensorflow/cc:gradients_nn_grad_test (1 packages loaded).\nINFO: Found 1 test target...\nTarget //tensorflow/cc:gradients_nn_grad_test up-to-date:\n  bazel-bin/tensorflow/cc/gradients_nn_grad_test\nINFO: Elapsed time: 385.282s, Critical Path: 384.53s\nINFO: Build completed successfully, 4 total actions\n//tensorflow/cc:gradients_nn_grad_test                                   PASSED in 0.9s\n\nExecuted 1 out of 1 test: 1 test passes.\n\nI hope this helps you and that someone with more knowledge can ensure this is correct because I too would like to be able to have C++ gradients for this operator (mostly so I can use it from Java).", "body": "@pbanavara @nietras and @suharshs,\r\n\r\nI apologize for the long comment.\r\n\r\nI have made some (I think significant) progress on figuring out why @pbanavara test has shape failures. Which has lead to what I think is a passing test for SoftmaxCrossEntropyGrad. There are some steps in the process, so bear with me as we go through it.\r\n\r\n- _BroadcastMul in the original python is a function call to a function in https://github.com/tensorflow/tensorflow/blob/f16b50e388543d2c05698926aa1ef265090dda6b/tensorflow/python/ops/nn_grad.py#L431 which resizes the input vector into a matrix, so we need a C++ implementation of this.\r\n```c++\r\nOutput _BroadcastMul(const Scope& scope, Output vec, Output mat){\r\n  /* Multiply after broadcasting vec to match dimensions of mat.\r\n     Args:\r\n       vec: A 1-D tensor of dimension [D0]\r\n       mat: A 2-D tensor of dimesnion [D0, D1]\r\n\r\n    Returns:\r\n      A tensor of dimension [D0, D1], the result fo vec * mat\r\n      we use an element for element multiply here.\r\n  */\r\n  auto reshaped = ExpandDims(scope, vec, -1);\r\n  return Multiply(scope, reshaped, mat);\r\n}\r\n```\r\n- The python code that @nietras asked about \"math_ops.matmul(grad_grad[:, None, :], softmax[:, :,None]), axis=1)) * softmax)\" is inserting dimensions of size 1 into the respective tensors where the \"None\" is. We can use the C++ ExpandDims operator to accomplish the same thing.\r\n- Once we use our C++ _BroadcastMul, we find that the MatMul C++ operator only handles 2D tensors, however we have 3D so we need to use BatchMatMul.\r\n```c++\r\n  auto grad = _BroadcastMul(scope, grad_loss, softmax_grad);\r\n  if(!_isZero(scope, grad_grad)){\r\n    std::vector<int> axis;\r\n    auto logitsSoftmax = Softmax(scope, logits);\r\n\r\n    auto grad_gradExpand = ExpandDims(scope, grad_grad, 1);\r\n    auto logitsSoftMaxExpand = ExpandDims(scope, logitsSoftmax, 2);\r\n    auto matMulResult = BatchMatMul(scope, grad_gradExpand, logitsSoftMaxExpand);\r\n    axis.push_back(1);\r\n    auto squeezeResult = Squeeze(scope, matMulResult, Squeeze::Axis(axis));\r\n    auto subtractionResult = Subtract(scope, grad_grad, squeezeResult);\r\n    auto multiplyResult = Multiply(scope, subtractionResult, logitsSoftmax);\r\n    grad = Add(scope, grad, multiplyResult);\r\n  }\r\n```\r\n- Doing the isZero check in C++ is a little clunky and can't really be done as nicely, but we can at least deal with the edge cases of Zero and Zero like:\r\n```c++\r\nbool _isZero(const Scope& scope, Output grad){\r\n  std::array<std::string, 2> zeroOpTypeNames {{\"ZerosLike\", \"Zeros\"}};\r\n  string opTypeName = grad.op().node()->type_string();\r\n  for(auto& zeroOpTypeName : zeroOpTypeNames){\r\n    if(opTypeName == zeroOpTypeName){\r\n      return true;\r\n    }\r\n  }\r\n  //the Operation we were provided is not named something obvious\r\n  //we need to actually look at its contents.\r\n  //the original python code did this by calling a utility function called\r\n  //tensor_util.constant_value. When you dig into tensor_tuil.constant_value\r\n  //it is a large number of 'if' statements that measure certain edge cases\r\n  //where it is possible to get the value of the tensor without actually\r\n  //evaluating it. There are many kinds of tensors that can not have this\r\n  //done.\r\n  //There is no C++ equivalent to tensor_util.constant_value so we do nothing\r\n  //for the moment.\r\n  return false;\r\n}\r\n```\r\n- Please see my attached file [nn_grad_snip.cc.txt](https://github.com/tensorflow/tensorflow/files/1868452/nn_grad_snip.cc.txt)\r\n for full working code of the SoftmaxCrossEntropyWithLogitsGrad function and support functions.\r\n- Making the above changes causes the test that @pbanavara wrote to no longer complain about shape mismatches. Here is my version of the same test code:\r\n```c++\r\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad){\r\n  TensorShape logitsShape({5,3}); //batch size of 5, 3 possible labels (classes),\r\n                                  //logits is what is produced by a network\r\n                                  //they are compared to labels which are the truth\r\n  TensorShape lossShape({5}); //batch size of 5, 1 value for each entry in the batch\r\n                              //loss is the difference between logits and labels\r\n\r\n  TensorShape backPropShape({5,3}); //the docmentation says the backprop output\r\n                                    //will have batch size x num classes as its shape\r\n\r\n  auto logits = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(logitsShape)); //estimation\r\n  auto labels = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(logitsShape)); //truth\r\n\r\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, logits, labels);\r\n\r\n  RunTest({logits, labels}, {logitsShape, logitsShape}, {y.loss, y.backprop}, {lossShape, backPropShape});\r\n}\r\n```\r\n- This is where my lack of knowledge of how Tensorflow is testing the gradient functions throws a monkey wrench into things. The test now fails with the error:\r\n```\r\n[ RUN      ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad\r\n2018-04-02 15:41:01.235423: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\ntensorflow/cc/gradients/nn_grad_test.cc:72: Failure\r\nExpected: (max_error) < (1e-3), actual: 1 vs 0.001\r\n[  FAILED  ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad (90 ms)\r\n```\r\n- Tracking this error down took me a while and I am still not certain my solution is correct. The next step was to isolate both the Python and C++ implementations of SoftmaxCrossEntropyWithLogitsGrad and run them as \"standalone\" programs providing them the same known inputs and ensuring that the outputs were identical. The file \r\n[SoftmaxCrossEntropyWithLogitsGradTest.py.txt](https://github.com/tensorflow/tensorflow/files/1868506/SoftmaxCrossEntropyWithLogitsGradTest.py.txt) is the isolation of the Python gradient code and the file \r\n[softmaxcrossentropy-test.cc.txt](https://github.com/tensorflow/tensorflow/files/1868531/softmaxcrossentropy-test.cc.txt) is the isolated version of the C++.\r\n  * The isolated python code outputs \r\n```\r\n0\r\n[ 1.00194287  0.91191393]\r\n1\r\n[[-0.63283461  0.33222499  0.30060962]\r\n [ 0.29762709 -0.59824544  0.30061829]]\r\n2\r\n[ 0.02  0.03]\r\n3\r\n[[ 0.04  0.05  0.06]\r\n [ 0.07  0.08  0.09]]\r\n4\r\n[[-0.01608398  0.00686562  0.00921836]\r\n [ 0.00594364 -0.01795938  0.01201574]]\r\n5\r\n[[ 0.02003886  0.02203886  0.02403886]\r\n [ 0.03635742  0.02735742  0.03605742]]\r\n```\r\n- Note that the output order of the tensors is the same between the Python and C++\r\n  * The isolated C++ code outputs\r\n```\r\ny.loss: Tensor<type: float shape: [2] values: 1.00194287 0.911913931>\r\n 1.00194\r\n0.911914\r\ny.backprop: Tensor<type: float shape: [2,3] values: [-0.632834613 0.332225 0.300609618]...>\r\n-0.632835  0.332225   0.30061\r\n 0.297627 -0.598245  0.300618\r\ngrad_loss: Tensor<type: double shape: [2] values: 0.02 0.03>\r\n0.02\r\n0.03\r\ngrad_grad: Tensor<type: double shape: [2,3] values: [0.04 0.05 0.06]...>\r\n0.04 0.05 0.06\r\n0.07 0.08 0.09\r\ngrad_outputs[0]: Tensor<type: float shape: [2,3] values: [-0.0160839763 0.00686561596 0.00921836123]...>\r\n -0.016084 0.00686562 0.00921836\r\n0.00594364 -0.0179594  0.0120157\r\ngrad_outputs[1]: Tensor<type: float shape: [2,3] values: [0.0200388562 0.0220388565 0.0240388587]...>\r\n0.0200389 0.0220389 0.0240389\r\n0.0363574 0.0273574 0.0360574\r\n```\r\n - At this point I am certain that the C++ gradient code is a faithful port of the python code. I have changed the inputs several times and the C++ produces what the python produces. This leads me to think that the C++ test code is somehow wrong. If we look at the generated documentation for [tensorflow::ops::SoftmaxCrossEntropyWithLogits](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/softmax-cross-entropy-with-logits) we see that while the Outputs are in the order loss, backprop the auto generated public attributes are in the _opposite order_. Because of this I changed the C++ test.\r\n- From\r\n```c++\r\nRunTest({logits, labels}, {logitsShape, logitsShape}, {y.loss, y.backprop}, {lossShape, backPropShape});\r\n```\r\n- To\r\n```c++\r\nRunTest({logits, labels}, {logitsShape, logitsShape}, {y.backprop, y.loss}, {backPropShape, lossShape});\r\n```\r\n- Which seems to work the test will now pass... however I would like someone who knows what they are doing to verify my reasoning.\r\n```\r\nbazel test -c dbg //tensorflow/cc:gradients_nn_grad_test\r\n...\r\nINFO: Analysed target //tensorflow/cc:gradients_nn_grad_test (1 packages loaded).\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/cc:gradients_nn_grad_test up-to-date:\r\n  bazel-bin/tensorflow/cc/gradients_nn_grad_test\r\nINFO: Elapsed time: 385.282s, Critical Path: 384.53s\r\nINFO: Build completed successfully, 4 total actions\r\n//tensorflow/cc:gradients_nn_grad_test                                   PASSED in 0.9s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```\r\nI hope this helps you and that someone with more knowledge can ensure this is correct because I too would like to be able to have C++ gradients for this operator (mostly so I can use it from Java)."}