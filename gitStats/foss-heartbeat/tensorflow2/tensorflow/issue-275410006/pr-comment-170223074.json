{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/170223074", "pull_request_review_id": 98873982, "id": 170223074, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDIyMzA3NA==", "diff_hunk": "@@ -47,6 +48,38 @@ Status SoftmaxGrad(const Scope& scope, const Operation& op,\n }\n REGISTER_GRADIENT_OP(\"Softmax\", SoftmaxGrad);\n \n+Status SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\n+                                          const Operation& op,\n+                                          const std::vector<Output>&\n+                                          grad_inputs,\n+                                          std::vector<Output>* grad_outputs) {\n+  // Softmax gradient with cross entropy logits function\n+  // We multiply the backprop for cost with the gradients - op.output[1]\n+  // There is no gradient for labels\n+\n+  auto softmaxGrad = op.output(1);\n+  auto gradLoss = grad_inputs[0];\n+  auto gradGrad = grad_inputs[1];\n+\n+  auto tempGrad = Mul(scope, gradLoss, softmaxGrad);\n+\n+  // TODO: Check if this sufficient, need a check for null ?\n+  if (gradGrad.op().output_type(0) != 0) {\n+\t  auto logits = op.input(0);\n+\t  auto softmax = ops::Softmax(scope, logits);\n+\t  auto prod = ops::MatMul(scope, gradGrad, softmax);\n+\t  auto squeezeProd = ops::Squeeze(scope, prod);\n+\t  auto fProd = Sub(scope, gradGrad, squeezeProd);\n+\t  auto grad = Add(scope, tempGrad, fProd);\n+\t  grad_outputs->push_back(grad);\n+\t  grad_outputs->push_back(NoGradient());", "path": "tensorflow/cc/gradients/nn_grad.cc", "position": null, "original_position": 36, "commit_id": "4d98d2b592b7990968be33e10064d06ca35f40c9", "original_commit_id": "ace35ecca311c15724584b80b2cf0a35cbfd7eb6", "user": {"login": "nietras", "id": 10798831, "node_id": "MDQ6VXNlcjEwNzk4ODMx", "avatar_url": "https://avatars1.githubusercontent.com/u/10798831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nietras", "html_url": "https://github.com/nietras", "followers_url": "https://api.github.com/users/nietras/followers", "following_url": "https://api.github.com/users/nietras/following{/other_user}", "gists_url": "https://api.github.com/users/nietras/gists{/gist_id}", "starred_url": "https://api.github.com/users/nietras/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nietras/subscriptions", "organizations_url": "https://api.github.com/users/nietras/orgs", "repos_url": "https://api.github.com/users/nietras/repos", "events_url": "https://api.github.com/users/nietras/events{/privacy}", "received_events_url": "https://api.github.com/users/nietras/received_events", "type": "User", "site_admin": false}, "body": "Wait the implementation has changed in python, see:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L409\r\n```python\r\n logits = op.inputs[0]\r\n  if grad_grad is not None and not IsZero(grad_grad):\r\n    softmax = nn_ops.softmax(logits)\r\n\r\n    grad += ((grad_grad - array_ops.squeeze(\r\n        math_ops.matmul(grad_grad[:, None, :], softmax[:, :, None]), axis=1)) *\r\n             softmax)\r\n\r\n  return grad, _BroadcastMul(grad_loss, -nn_ops.log_softmax(logits))\r\n```\r\nnote that the second order gradient is now computed too", "created_at": "2018-02-23T10:57:33Z", "updated_at": "2018-07-12T20:46:26Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14727#discussion_r170223074", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14727", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/170223074"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14727#discussion_r170223074"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14727"}}, "body_html": "<p>Wait the implementation has changed in python, see:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L409\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L409</a></p>\n<div class=\"highlight highlight-source-python\"><pre> logits <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n  <span class=\"pl-k\">if</span> grad_grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> IsZero(grad_grad):\n    softmax <span class=\"pl-k\">=</span> nn_ops.softmax(logits)\n\n    grad <span class=\"pl-k\">+=</span> ((grad_grad <span class=\"pl-k\">-</span> array_ops.squeeze(\n        math_ops.matmul(grad_grad[:, <span class=\"pl-c1\">None</span>, :], softmax[:, :, <span class=\"pl-c1\">None</span>]), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)) <span class=\"pl-k\">*</span>\n             softmax)\n\n  <span class=\"pl-k\">return</span> grad, _BroadcastMul(grad_loss, <span class=\"pl-k\">-</span>nn_ops.log_softmax(logits))</pre></div>\n<p>note that the second order gradient is now computed too</p>", "body_text": "Wait the implementation has changed in python, see:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L409\n logits = op.inputs[0]\n  if grad_grad is not None and not IsZero(grad_grad):\n    softmax = nn_ops.softmax(logits)\n\n    grad += ((grad_grad - array_ops.squeeze(\n        math_ops.matmul(grad_grad[:, None, :], softmax[:, :, None]), axis=1)) *\n             softmax)\n\n  return grad, _BroadcastMul(grad_loss, -nn_ops.log_softmax(logits))\nnote that the second order gradient is now computed too", "in_reply_to_id": 170222650}