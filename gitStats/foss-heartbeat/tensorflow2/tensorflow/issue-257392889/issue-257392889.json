{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13016", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13016/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13016/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13016/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13016", "id": 257392889, "node_id": "MDU6SXNzdWUyNTczOTI4ODk=", "number": 13016, "title": "Distributed variable initialization never reaches some nodes (affects MonitoredTrainingSession too)", "user": {"login": "leandro-gracia-gil", "id": 8785797, "node_id": "MDQ6VXNlcjg3ODU3OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8785797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leandro-gracia-gil", "html_url": "https://github.com/leandro-gracia-gil", "followers_url": "https://api.github.com/users/leandro-gracia-gil/followers", "following_url": "https://api.github.com/users/leandro-gracia-gil/following{/other_user}", "gists_url": "https://api.github.com/users/leandro-gracia-gil/gists{/gist_id}", "starred_url": "https://api.github.com/users/leandro-gracia-gil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leandro-gracia-gil/subscriptions", "organizations_url": "https://api.github.com/users/leandro-gracia-gil/orgs", "repos_url": "https://api.github.com/users/leandro-gracia-gil/repos", "events_url": "https://api.github.com/users/leandro-gracia-gil/events{/privacy}", "received_events_url": "https://api.github.com/users/leandro-gracia-gil/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-09-13T14:03:09Z", "updated_at": "2018-04-04T01:44:16Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Multiple affected in different ways, including Linux Ubuntu 16.04.03, Mac OS X 10.12.6, Windows 10 and Bash on Windows 10 running Ubuntu 16.04.03.</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary, followed the pip install instructions</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.3.0-rc2-20-g0787eee 1.3.0</li>\n<li><strong>Python version</strong>: 3.5.2, 3.6.0</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: See instructions below.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>In a distributed environment with 2 nodes (a chief 'foo', a non-chief 'bar') variable initialization performed by foo never reaches bar in some particular distributed combinations. If using MonitoredTrainingSession, this leads to the session in bar never starting (it keeps trying forever every 30 seconds). Further research showed that no matter what I do (including restarting sessions, delays and anything I could think of) the session in 'bar' is unable to see the initialization of the variables that 'foo' confirms as initialized.</p>\n<p>The source code below can be used to reproduce the problem depending on the hosts used for the 'foo' and 'bar' jobs. In particular, the following has been observed and reproduced multiple times:</p>\n<ul>\n<li>When foo and bar are in the same host and OS, the problem never happens.</li>\n<li>When ran foo in a Linux host and bar in a Mac OS X host the problem <em>always</em> happened.</li>\n<li>However, if the roles are reversed (Linux runs bar, Mac OS X runs foo) the problem never happens.</li>\n<li>Making foo_variables below an empty list also avoids the problem entirely.</li>\n</ul>\n<p>Additionally I also tested this in another couple of platforms in the same host.</p>\n<ul>\n<li>When ran foo in Windows 10 and bar in bash on Windows 10 running Ubuntu Linux 16.04, the problem <em>always</em> happened.</li>\n<li>When reversing it and making Linux run foo and Windows 10 run bar the problem disappeared.</li>\n<li>In this case both OS run in the same host and communicate through localhost sockets. Other tests suggest there's no problem in this socket communication.</li>\n</ul>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>foo<span class=\"pl-pds\">'</span></span>), tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>job:foo/task:0<span class=\"pl-pds\">'</span></span>):\n  foo_variables <span class=\"pl-k\">=</span> [tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>W<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)]\n  foo_init_vars <span class=\"pl-k\">=</span> tf.variables_initializer(foo_variables)\n  foo_pending_vars <span class=\"pl-k\">=</span> tf.report_uninitialized_variables(foo_variables)\n  foo_pending_vars.mark_used()\n\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bar<span class=\"pl-pds\">'</span></span>), tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>job:bar/task:0<span class=\"pl-pds\">'</span></span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Expect more stuff and ops in bar in a real use case. This is just an example.</span>\n  bar_pending_vars <span class=\"pl-k\">=</span> tf.report_uninitialized_variables(foo_variables)\n  bar_pending_vars.mark_used()\n\n\ncluster_spec <span class=\"pl-k\">=</span> tf.train.ClusterSpec({\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;insert_ip_here&gt;:55700<span class=\"pl-pds\">\"</span></span>],\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bar<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;insert_ip_here&gt;:55701<span class=\"pl-pds\">\"</span></span>],\n})\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">foo_job</span>():\n  server <span class=\"pl-k\">=</span> tf.train.Server(cluster_spec, <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>foo<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n  <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(server.target, <span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-k\">as</span> session:\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> This always runs fine.</span>\n    session.run(foo_init_vars)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Foo -- Variables not initialized: <span class=\"pl-pds\">\"</span></span>, session.run(foo_pending_vars))\n    server.join()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bar_job</span>():\n  server <span class=\"pl-k\">=</span> tf.train.Server(cluster_spec, <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bar<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n  <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(server.target, <span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>) <span class=\"pl-k\">as</span> session:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> On failure, this never gets executed...</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>**** Session started! ****<span class=\"pl-pds\">\"</span></span>)\n\n    vars_left <span class=\"pl-k\">=</span> session.run(bar_pending_vars)\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(vars_left) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Bar -- Variables initialized!<span class=\"pl-pds\">\"</span></span>)\n      <span class=\"pl-k\">return</span>\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Bar -- Variables not initialized: <span class=\"pl-pds\">\"</span></span>, vars_left)\n    server.join()\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(sys.argv) <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">2</span>:\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Usage: <span class=\"pl-c1\">%s</span> {foo|bar}<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> sys.argv[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-c1\">exit</span>(<span class=\"pl-c1\">1</span>)\n\n  <span class=\"pl-k\">if</span> sys.argv[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>foo<span class=\"pl-pds\">'</span></span>:\n    foo_job()\n  <span class=\"pl-k\">else</span>:\n    bar_job()</pre></div>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>, this looks like something you might have some intuition about. Any ideas of what might be going on?</p>\n<p>This is affecting my distributed system pretty badly. I'd be happy to do more experiments to help diagnosing the problem.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Multiple affected in different ways, including Linux Ubuntu 16.04.03, Mac OS X 10.12.6, Windows 10 and Bash on Windows 10 running Ubuntu 16.04.03.\nTensorFlow installed from (source or binary): binary, followed the pip install instructions\nTensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0\nPython version: 3.5.2, 3.6.0\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See instructions below.\n\nDescribe the problem\nIn a distributed environment with 2 nodes (a chief 'foo', a non-chief 'bar') variable initialization performed by foo never reaches bar in some particular distributed combinations. If using MonitoredTrainingSession, this leads to the session in bar never starting (it keeps trying forever every 30 seconds). Further research showed that no matter what I do (including restarting sessions, delays and anything I could think of) the session in 'bar' is unable to see the initialization of the variables that 'foo' confirms as initialized.\nThe source code below can be used to reproduce the problem depending on the hosts used for the 'foo' and 'bar' jobs. In particular, the following has been observed and reproduced multiple times:\n\nWhen foo and bar are in the same host and OS, the problem never happens.\nWhen ran foo in a Linux host and bar in a Mac OS X host the problem always happened.\nHowever, if the roles are reversed (Linux runs bar, Mac OS X runs foo) the problem never happens.\nMaking foo_variables below an empty list also avoids the problem entirely.\n\nAdditionally I also tested this in another couple of platforms in the same host.\n\nWhen ran foo in Windows 10 and bar in bash on Windows 10 running Ubuntu Linux 16.04, the problem always happened.\nWhen reversing it and making Linux run foo and Windows 10 run bar the problem disappeared.\nIn this case both OS run in the same host and communicate through localhost sockets. Other tests suggest there's no problem in this socket communication.\n\nSource code / logs\n#!/usr/bin/env python\n\nimport sys\nimport tensorflow as tf\n\nwith tf.variable_scope('foo'), tf.device('job:foo/task:0'):\n  foo_variables = [tf.get_variable('W', shape=(10, 5), dtype=tf.float32)]\n  foo_init_vars = tf.variables_initializer(foo_variables)\n  foo_pending_vars = tf.report_uninitialized_variables(foo_variables)\n  foo_pending_vars.mark_used()\n\n\nwith tf.variable_scope('bar'), tf.device('job:bar/task:0'):\n  # Expect more stuff and ops in bar in a real use case. This is just an example.\n  bar_pending_vars = tf.report_uninitialized_variables(foo_variables)\n  bar_pending_vars.mark_used()\n\n\ncluster_spec = tf.train.ClusterSpec({\n  \"foo\": [\"<insert_ip_here>:55700\"],\n  \"bar\": [\"<insert_ip_here>:55701\"],\n})\n\n\ndef foo_job():\n  server = tf.train.Server(cluster_spec, job_name='foo', task_index=0)\n  with tf.train.MonitoredTrainingSession(server.target, is_chief=True) as session:\n\n    # This always runs fine.\n    session.run(foo_init_vars)\n    print(\"Foo -- Variables not initialized: \", session.run(foo_pending_vars))\n    server.join()\n\n\ndef bar_job():\n  server = tf.train.Server(cluster_spec, job_name='bar', task_index=0)\n  with tf.train.MonitoredTrainingSession(server.target, is_chief=False) as session:\n    # On failure, this never gets executed...\n    print(\"**** Session started! ****\")\n\n    vars_left = session.run(bar_pending_vars)\n    if len(vars_left) == 0:\n      print(\"Bar -- Variables initialized!\")\n      return\n\n    print(\"Bar -- Variables not initialized: \", vars_left)\n    server.join()\n\n\nif __name__ == '__main__':\n  if len(sys.argv) < 2:\n    print(\"Usage: %s {foo|bar}\" % sys.argv[0])\n    exit(1)\n\n  if sys.argv[1] == 'foo':\n    foo_job()\n  else:\n    bar_job()\n@mrry, this looks like something you might have some intuition about. Any ideas of what might be going on?\nThis is affecting my distributed system pretty badly. I'd be happy to do more experiments to help diagnosing the problem.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Multiple affected in different ways, including Linux Ubuntu 16.04.03, Mac OS X 10.12.6, Windows 10 and Bash on Windows 10 running Ubuntu 16.04.03.\r\n- **TensorFlow installed from (source or binary)**: binary, followed the pip install instructions\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.5.2, 3.6.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See instructions below.\r\n\r\n### Describe the problem\r\nIn a distributed environment with 2 nodes (a chief 'foo', a non-chief 'bar') variable initialization performed by foo never reaches bar in some particular distributed combinations. If using MonitoredTrainingSession, this leads to the session in bar never starting (it keeps trying forever every 30 seconds). Further research showed that no matter what I do (including restarting sessions, delays and anything I could think of) the session in 'bar' is unable to see the initialization of the variables that 'foo' confirms as initialized.\r\n\r\nThe source code below can be used to reproduce the problem depending on the hosts used for the 'foo' and 'bar' jobs. In particular, the following has been observed and reproduced multiple times:\r\n\r\n- When foo and bar are in the same host and OS, the problem never happens.\r\n- When ran foo in a Linux host and bar in a Mac OS X host the problem *always* happened.\r\n- However, if the roles are reversed (Linux runs bar, Mac OS X runs foo) the problem never happens.\r\n- Making foo_variables below an empty list also avoids the problem entirely.\r\n\r\nAdditionally I also tested this in another couple of platforms in the same host.\r\n- When ran foo in Windows 10 and bar in bash on Windows 10 running Ubuntu Linux 16.04, the problem *always* happened.\r\n- When reversing it and making Linux run foo and Windows 10 run bar the problem disappeared.\r\n- In this case both OS run in the same host and communicate through localhost sockets. Other tests suggest there's no problem in this socket communication.\r\n\r\n### Source code / logs\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope('foo'), tf.device('job:foo/task:0'):\r\n  foo_variables = [tf.get_variable('W', shape=(10, 5), dtype=tf.float32)]\r\n  foo_init_vars = tf.variables_initializer(foo_variables)\r\n  foo_pending_vars = tf.report_uninitialized_variables(foo_variables)\r\n  foo_pending_vars.mark_used()\r\n\r\n\r\nwith tf.variable_scope('bar'), tf.device('job:bar/task:0'):\r\n  # Expect more stuff and ops in bar in a real use case. This is just an example.\r\n  bar_pending_vars = tf.report_uninitialized_variables(foo_variables)\r\n  bar_pending_vars.mark_used()\r\n\r\n\r\ncluster_spec = tf.train.ClusterSpec({\r\n  \"foo\": [\"<insert_ip_here>:55700\"],\r\n  \"bar\": [\"<insert_ip_here>:55701\"],\r\n})\r\n\r\n\r\ndef foo_job():\r\n  server = tf.train.Server(cluster_spec, job_name='foo', task_index=0)\r\n  with tf.train.MonitoredTrainingSession(server.target, is_chief=True) as session:\r\n\r\n    # This always runs fine.\r\n    session.run(foo_init_vars)\r\n    print(\"Foo -- Variables not initialized: \", session.run(foo_pending_vars))\r\n    server.join()\r\n\r\n\r\ndef bar_job():\r\n  server = tf.train.Server(cluster_spec, job_name='bar', task_index=0)\r\n  with tf.train.MonitoredTrainingSession(server.target, is_chief=False) as session:\r\n    # On failure, this never gets executed...\r\n    print(\"**** Session started! ****\")\r\n\r\n    vars_left = session.run(bar_pending_vars)\r\n    if len(vars_left) == 0:\r\n      print(\"Bar -- Variables initialized!\")\r\n      return\r\n\r\n    print(\"Bar -- Variables not initialized: \", vars_left)\r\n    server.join()\r\n\r\n\r\nif __name__ == '__main__':\r\n  if len(sys.argv) < 2:\r\n    print(\"Usage: %s {foo|bar}\" % sys.argv[0])\r\n    exit(1)\r\n\r\n  if sys.argv[1] == 'foo':\r\n    foo_job()\r\n  else:\r\n    bar_job()\r\n```\r\n\r\n@mrry, this looks like something you might have some intuition about. Any ideas of what might be going on?\r\n\r\nThis is affecting my distributed system pretty badly. I'd be happy to do more experiments to help diagnosing the problem."}