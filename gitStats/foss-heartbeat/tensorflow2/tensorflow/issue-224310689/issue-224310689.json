{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9450", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9450/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9450/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9450/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9450", "id": 224310689, "node_id": "MDU6SXNzdWUyMjQzMTA2ODk=", "number": 9450, "title": "Compute gradient inside tf.while_loop using TensorArray", "user": {"login": "mingdachen", "id": 18625540, "node_id": "MDQ6VXNlcjE4NjI1NTQw", "avatar_url": "https://avatars2.githubusercontent.com/u/18625540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingdachen", "html_url": "https://github.com/mingdachen", "followers_url": "https://api.github.com/users/mingdachen/followers", "following_url": "https://api.github.com/users/mingdachen/following{/other_user}", "gists_url": "https://api.github.com/users/mingdachen/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingdachen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingdachen/subscriptions", "organizations_url": "https://api.github.com/users/mingdachen/orgs", "repos_url": "https://api.github.com/users/mingdachen/repos", "events_url": "https://api.github.com/users/mingdachen/events{/privacy}", "received_events_url": "https://api.github.com/users/mingdachen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-04-26T00:35:42Z", "updated_at": "2017-06-16T21:12:31Z", "closed_at": "2017-06-16T21:12:31Z", "author_association": "NONE", "body_html": "<p>I was trying to call opt.compute_gradients() inside the while_loop, but it failed with the error message:</p>\n<pre><code>AttributeError: 'WhileContext' object has no attribute 'pred'\n</code></pre>\n<p>I found a similiar problem in <a href=\"http://stackoverflow.com/questions/42313788/how-to-do-opt-compute-gradients-multiple-times-in-single-sess-run\" rel=\"nofollow\">stackoverflow</a></p>\n<p>test code:</p>\n<div class=\"highlight highlight-source-python\"><pre>batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\ninputs <span class=\"pl-k\">=</span> tf.ones((batch_size, <span class=\"pl-c1\">10</span>))\nlabels <span class=\"pl-k\">=</span> tf.zeros((batch_size, <span class=\"pl-c1\">1</span>))\noutputs <span class=\"pl-k\">=</span> tf.layers.dense(inputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\nloss <span class=\"pl-k\">=</span> outputs <span class=\"pl-k\">-</span> labels\nloss_ta <span class=\"pl-k\">=</span> tf.TensorArray(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>batch_size)\nloss_ta <span class=\"pl-k\">=</span> loss_ta.unstack(loss)\n\nopt <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.1</span>)\ninit_grad <span class=\"pl-k\">=</span> []\nvars_list <span class=\"pl-k\">=</span> tf.trainable_variables()\n<span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> vars_list:\n    init_grad.append(tf.zeros_like(var))\n\ni <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">condition</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n    <span class=\"pl-k\">return</span> tf.less(i, batch_size)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loop_fn</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">gradients</span>, <span class=\"pl-smi\">all_loss</span>):\n    loss_ <span class=\"pl-k\">=</span> all_loss.read(i)\n    grads <span class=\"pl-k\">=</span> opt.compute_gradients(loss_, vars_list)\n    <span class=\"pl-k\">for</span> idx, (grad, var) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(grads):\n        gradients[idx] <span class=\"pl-k\">+=</span> grad\n    <span class=\"pl-k\">return</span> i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, gradients, all_loss\n_, final_grad, _ <span class=\"pl-k\">=</span> tf.while_loop(condition, loop_fn, [i, init_grad, loss_ta])\n\ntrain_op <span class=\"pl-k\">=</span> opt.apply_gradients(<span class=\"pl-c1\">zip</span>(final_grad, vars_list))</pre></div>\n<p>Seems like the problem is in the TensorArray, if I do not read loss from the TensorArray, it will be fine. Besides, I am using version1.0.1 on CPU</p>", "body_text": "I was trying to call opt.compute_gradients() inside the while_loop, but it failed with the error message:\nAttributeError: 'WhileContext' object has no attribute 'pred'\n\nI found a similiar problem in stackoverflow\ntest code:\nbatch_size = 2\ninputs = tf.ones((batch_size, 10))\nlabels = tf.zeros((batch_size, 1))\noutputs = tf.layers.dense(inputs, units=1)\nloss = outputs - labels\nloss_ta = tf.TensorArray(dtype=tf.float32, size=batch_size)\nloss_ta = loss_ta.unstack(loss)\n\nopt = tf.train.AdamOptimizer(0.1)\ninit_grad = []\nvars_list = tf.trainable_variables()\nfor var in vars_list:\n    init_grad.append(tf.zeros_like(var))\n\ni = tf.constant(0, dtype=tf.int32)\ndef condition(i, *args):\n    return tf.less(i, batch_size)\ndef loop_fn(i, gradients, all_loss):\n    loss_ = all_loss.read(i)\n    grads = opt.compute_gradients(loss_, vars_list)\n    for idx, (grad, var) in enumerate(grads):\n        gradients[idx] += grad\n    return i + 1, gradients, all_loss\n_, final_grad, _ = tf.while_loop(condition, loop_fn, [i, init_grad, loss_ta])\n\ntrain_op = opt.apply_gradients(zip(final_grad, vars_list))\nSeems like the problem is in the TensorArray, if I do not read loss from the TensorArray, it will be fine. Besides, I am using version1.0.1 on CPU", "body": "I was trying to call opt.compute_gradients() inside the while_loop, but it failed with the error message:\r\n```\r\nAttributeError: 'WhileContext' object has no attribute 'pred'\r\n```\r\nI found a similiar problem in [stackoverflow](http://stackoverflow.com/questions/42313788/how-to-do-opt-compute-gradients-multiple-times-in-single-sess-run)\r\n\r\ntest code:\r\n```python\r\nbatch_size = 2\r\ninputs = tf.ones((batch_size, 10))\r\nlabels = tf.zeros((batch_size, 1))\r\noutputs = tf.layers.dense(inputs, units=1)\r\nloss = outputs - labels\r\nloss_ta = tf.TensorArray(dtype=tf.float32, size=batch_size)\r\nloss_ta = loss_ta.unstack(loss)\r\n\r\nopt = tf.train.AdamOptimizer(0.1)\r\ninit_grad = []\r\nvars_list = tf.trainable_variables()\r\nfor var in vars_list:\r\n    init_grad.append(tf.zeros_like(var))\r\n\r\ni = tf.constant(0, dtype=tf.int32)\r\ndef condition(i, *args):\r\n    return tf.less(i, batch_size)\r\ndef loop_fn(i, gradients, all_loss):\r\n    loss_ = all_loss.read(i)\r\n    grads = opt.compute_gradients(loss_, vars_list)\r\n    for idx, (grad, var) in enumerate(grads):\r\n        gradients[idx] += grad\r\n    return i + 1, gradients, all_loss\r\n_, final_grad, _ = tf.while_loop(condition, loop_fn, [i, init_grad, loss_ta])\r\n\r\ntrain_op = opt.apply_gradients(zip(final_grad, vars_list))\r\n```\r\nSeems like the problem is in the TensorArray, if I do not read loss from the TensorArray, it will be fine. Besides, I am using version1.0.1 on CPU"}