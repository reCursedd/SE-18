{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/352086600", "html_url": "https://github.com/tensorflow/tensorflow/issues/14675#issuecomment-352086600", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14675", "id": 352086600, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjA4NjYwMA==", "user": {"login": "alanhdu", "id": 1914111, "node_id": "MDQ6VXNlcjE5MTQxMTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1914111?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanhdu", "html_url": "https://github.com/alanhdu", "followers_url": "https://api.github.com/users/alanhdu/followers", "following_url": "https://api.github.com/users/alanhdu/following{/other_user}", "gists_url": "https://api.github.com/users/alanhdu/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanhdu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanhdu/subscriptions", "organizations_url": "https://api.github.com/users/alanhdu/orgs", "repos_url": "https://api.github.com/users/alanhdu/repos", "events_url": "https://api.github.com/users/alanhdu/events{/privacy}", "received_events_url": "https://api.github.com/users/alanhdu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-15T19:06:46Z", "updated_at": "2017-12-15T19:06:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>FWIW, we were bitten by this issue -- we wanted to make the learning rate of our optimizer a placeholder (to experiment with some more exotic learning rate decay schemes) and were very confused why changing the learning rate from a constant to a placeholder fed the same value learned a different network.</p>", "body_text": "FWIW, we were bitten by this issue -- we wanted to make the learning rate of our optimizer a placeholder (to experiment with some more exotic learning rate decay schemes) and were very confused why changing the learning rate from a constant to a placeholder fed the same value learned a different network.", "body": "FWIW, we were bitten by this issue -- we wanted to make the learning rate of our optimizer a placeholder (to experiment with some more exotic learning rate decay schemes) and were very confused why changing the learning rate from a constant to a placeholder fed the same value learned a different network."}