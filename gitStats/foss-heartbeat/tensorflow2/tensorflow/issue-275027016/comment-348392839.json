{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/348392839", "html_url": "https://github.com/tensorflow/tensorflow/issues/14675#issuecomment-348392839", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14675", "id": 348392839, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODM5MjgzOQ==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-01T03:37:26Z", "updated_at": "2017-12-01T03:37:26Z", "author_association": "NONE", "body_html": "<p>I still feel that there's a feature/bug to discuss here. Namely, I think TensorFlow should strive to make reproducibility as easy as possible for the user, and that that's not the case here.</p>\n<p>Can you think of any cases where the current default op seed is more intuitive than what I've suggested (default op seed is a function of graph seed and tensor name)?</p>\n<p>I don't think I've ever seen a TF program that sets an op seed on every variable initializer (and other random ops). Given this, results from TF are generally not reproducible under the slightest of tweaks.</p>\n<p>Use case I'd like to support:<br>\nUser has a function that takes in a seed, builds a graph, trains a model. User saves the output of the training (learning curves, model weights, etc) along with the git commit of the code and the random seed used. User then wants to try a tiny change (for instance, changing the implementation of mean). User checks out commit, makes small change, runs again with the same seed, and sees nearly identical numerical results.</p>\n<p>Thoughts on implementation:<br>\nWe definitely can't break reproducibility of existing serialized graphs. We probably shouldn't break reproducibility of existing graph building programs. I think this means that the default op seed scheme cannot be changed. Two options I think might be possible:</p>\n<ul>\n<li>a <code>tf.set_op_seed_default</code> function that changes the method in which op seeds are generated</li>\n<li>an additional <code>tf.variable_scope</code> kwarg that controls op seeds within the scope.</li>\n</ul>", "body_text": "I still feel that there's a feature/bug to discuss here. Namely, I think TensorFlow should strive to make reproducibility as easy as possible for the user, and that that's not the case here.\nCan you think of any cases where the current default op seed is more intuitive than what I've suggested (default op seed is a function of graph seed and tensor name)?\nI don't think I've ever seen a TF program that sets an op seed on every variable initializer (and other random ops). Given this, results from TF are generally not reproducible under the slightest of tweaks.\nUse case I'd like to support:\nUser has a function that takes in a seed, builds a graph, trains a model. User saves the output of the training (learning curves, model weights, etc) along with the git commit of the code and the random seed used. User then wants to try a tiny change (for instance, changing the implementation of mean). User checks out commit, makes small change, runs again with the same seed, and sees nearly identical numerical results.\nThoughts on implementation:\nWe definitely can't break reproducibility of existing serialized graphs. We probably shouldn't break reproducibility of existing graph building programs. I think this means that the default op seed scheme cannot be changed. Two options I think might be possible:\n\na tf.set_op_seed_default function that changes the method in which op seeds are generated\nan additional tf.variable_scope kwarg that controls op seeds within the scope.", "body": "I still feel that there's a feature/bug to discuss here. Namely, I think TensorFlow should strive to make reproducibility as easy as possible for the user, and that that's not the case here.\r\n\r\nCan you think of any cases where the current default op seed is more intuitive than what I've suggested (default op seed is a function of graph seed and tensor name)?\r\n\r\nI don't think I've ever seen a TF program that sets an op seed on every variable initializer (and other random ops). Given this, results from TF are generally not reproducible under the slightest of tweaks.\r\n\r\nUse case I'd like to support:\r\nUser has a function that takes in a seed, builds a graph, trains a model. User saves the output of the training (learning curves, model weights, etc) along with the git commit of the code and the random seed used. User then wants to try a tiny change (for instance, changing the implementation of mean). User checks out commit, makes small change, runs again with the same seed, and sees nearly identical numerical results.\r\n\r\nThoughts on implementation:\r\nWe definitely can't break reproducibility of existing serialized graphs. We probably shouldn't break reproducibility of existing graph building programs. I think this means that the default op seed scheme cannot be changed. Two options I think might be possible: \r\n* a ``tf.set_op_seed_default`` function that changes the method in which op seeds are generated\r\n* an additional ``tf.variable_scope`` kwarg that controls op seeds within the scope."}