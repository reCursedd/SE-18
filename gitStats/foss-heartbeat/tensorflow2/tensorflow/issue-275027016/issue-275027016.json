{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14675", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14675/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14675/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14675/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14675", "id": 275027016, "node_id": "MDU6SXNzdWUyNzUwMjcwMTY=", "number": 14675, "title": "Small change to graph changes initial values of variables", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2017-11-17T23:31:21Z", "updated_at": "2018-11-20T13:26:20Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm running TensorFlow 1.2 with CUDA.</p>\n<p>Error case:<br>\nI call <code>tf.set_random_seed(2017)</code> and then build a graph <code>g1</code> that includes trainable variables and an optimizer. I create a session, run the <code>tf.global_variables_initializer()</code> op, and then immediately fetch the value of a scalar variable (without running any train steps, so this is the initial value of the variable). As expected, this value stays the same if I launch this process multiple times.</p>\n<p>I then build a graph <code>g2</code> that is identical to <code>g1</code> except for a slight change. <code>g1</code> contained <code>mu = tf.reduce_sum(x) / tf.size(x)[0]</code>, and <code>g2</code> contains <code>mu = tf.reduce_mean(x)</code>. <code>g2</code> is seeded the same way as <code>g1</code> and has the same variable names and shapes as <code>g1</code>. The only differently named tensors are those relating to the modification mentioned above. When I fetch the initial value of the same scalar variable from <code>g2</code>, there is a completely different value from when fetched from <code>g1</code>.</p>\n<p>I've tried to isolate this into a small test case but have not been successful yet. I will continue to work on this. Apologies for bug report without test case.<br>\nMy current intention is to workaround this with a Numpy based initialization scheme.</p>\n<p>Questions:<br>\n(1) Is this expected behavior? Ideally the variables would be initialized in the same way to help make results more reproducible. In my case, the different variable initialization makes it more difficult to test that <code>g1</code> and <code>g2</code> produce the same values. If variables were initialized the same way, would be easy to see that refactoring the mean computation in the graph did not break anything.</p>\n<p>(2) Any idea why this occurs? Perhaps relevantly, <code>tf.make_template</code> is used within this graph. My current (evidence-free) hunch is the small change in graph causes a variable to move from CPU resident to GPU resident and caused a different PRNG kernel (provided with the same seed) to be used.</p>", "body_text": "I'm running TensorFlow 1.2 with CUDA.\nError case:\nI call tf.set_random_seed(2017) and then build a graph g1 that includes trainable variables and an optimizer. I create a session, run the tf.global_variables_initializer() op, and then immediately fetch the value of a scalar variable (without running any train steps, so this is the initial value of the variable). As expected, this value stays the same if I launch this process multiple times.\nI then build a graph g2 that is identical to g1 except for a slight change. g1 contained mu = tf.reduce_sum(x) / tf.size(x)[0], and g2 contains mu = tf.reduce_mean(x). g2 is seeded the same way as g1 and has the same variable names and shapes as g1. The only differently named tensors are those relating to the modification mentioned above. When I fetch the initial value of the same scalar variable from g2, there is a completely different value from when fetched from g1.\nI've tried to isolate this into a small test case but have not been successful yet. I will continue to work on this. Apologies for bug report without test case.\nMy current intention is to workaround this with a Numpy based initialization scheme.\nQuestions:\n(1) Is this expected behavior? Ideally the variables would be initialized in the same way to help make results more reproducible. In my case, the different variable initialization makes it more difficult to test that g1 and g2 produce the same values. If variables were initialized the same way, would be easy to see that refactoring the mean computation in the graph did not break anything.\n(2) Any idea why this occurs? Perhaps relevantly, tf.make_template is used within this graph. My current (evidence-free) hunch is the small change in graph causes a variable to move from CPU resident to GPU resident and caused a different PRNG kernel (provided with the same seed) to be used.", "body": "I'm running TensorFlow 1.2 with CUDA.\r\n\r\nError case:\r\nI call ``tf.set_random_seed(2017)`` and then build a graph ``g1`` that includes trainable variables and an optimizer. I create a session, run the ``tf.global_variables_initializer()`` op, and then immediately fetch the value of a scalar variable (without running any train steps, so this is the initial value of the variable). As expected, this value stays the same if I launch this process multiple times.\r\n\r\nI then build a graph ``g2`` that is identical to ``g1`` except for a slight change. ``g1`` contained ``mu = tf.reduce_sum(x) / tf.size(x)[0]``, and ``g2`` contains ``mu = tf.reduce_mean(x)``. ``g2`` is seeded the same way as ``g1`` and has the same variable names and shapes as ``g1``. The only differently named tensors are those relating to the modification mentioned above. When I fetch the initial value of the same scalar variable from ``g2``, there is a completely different value from when fetched from ``g1``.\r\n\r\nI've tried to isolate this into a small test case but have not been successful yet. I will continue to work on this. Apologies for bug report without test case.\r\nMy current intention is to workaround this with a Numpy based initialization scheme.\r\n\r\nQuestions:\r\n(1) Is this expected behavior? Ideally the variables would be initialized in the same way to help make results more reproducible. In my case, the different variable initialization makes it more difficult to test that ``g1`` and ``g2`` produce the same values. If variables were initialized the same way, would be easy to see that refactoring the mean computation in the graph did not break anything.\r\n\r\n(2) Any idea why this occurs? Perhaps relevantly, ``tf.make_template`` is used within this graph. My current (evidence-free) hunch is the small change in graph causes a variable to move from CPU resident to GPU resident and caused a different PRNG kernel (provided with the same seed) to be used."}