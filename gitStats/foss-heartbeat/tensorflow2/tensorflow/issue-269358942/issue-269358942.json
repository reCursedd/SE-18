{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14062", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14062/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14062/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14062/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14062", "id": 269358942, "node_id": "MDU6SXNzdWUyNjkzNTg5NDI=", "number": 14062, "title": "Possible Memory Leak with Pet-variant Detection model on Android", "user": {"login": "Cpruce", "id": 2712171, "node_id": "MDQ6VXNlcjI3MTIxNzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2712171?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cpruce", "html_url": "https://github.com/Cpruce", "followers_url": "https://api.github.com/users/Cpruce/followers", "following_url": "https://api.github.com/users/Cpruce/following{/other_user}", "gists_url": "https://api.github.com/users/Cpruce/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cpruce/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cpruce/subscriptions", "organizations_url": "https://api.github.com/users/Cpruce/orgs", "repos_url": "https://api.github.com/users/Cpruce/repos", "events_url": "https://api.github.com/users/Cpruce/events{/privacy}", "received_events_url": "https://api.github.com/users/Cpruce/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-28T23:45:34Z", "updated_at": "2017-11-04T01:42:30Z", "closed_at": "2017-10-31T07:07:54Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS Sierra+Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0</li>\n<li><strong>Python version</strong>: Python 3.6.1 :: Anaconda custom (64-bit)</li>\n<li><strong>Bazel version (if compiling from source)</strong>: Build label: 0.5.4-homebrew</li>\n<li><strong>CUDA/cuDNN version</strong>: Not used</li>\n<li><strong>GPU model and memory</strong>: Not used</li>\n<li><strong>Exact command to reproduce</strong>: X</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When loading a trained-from-scratch sdd_mobilenet_v1 (frozen_inference_graph.pb) in place of \"file:///android_asset/ssd_mobilenet_v1_android_export.pb\" for the TF Detect app, the screen goes blank white and crashes, without an error logged. I have been following the pet example and have gotten passed the GraphDef Invalid error(s). In my case, the problem occurs after:</p>\n<p><code>I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference</code><br>\n<code>I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)</code></p>\n<p>When loading my custom ssd mobilenet model (5621 labels), I assume the model fails to load because it hangs on the white screen before crashing and I dont see:</p>\n<p><code>I/TensorFlowInferenceInterface: Model load took 502ms, TensorFlow version: 1.4.0-rc1</code><br>\n<code>I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/ssd_mobilenet_v1_android_export.pb'</code></p>\n<p>One notable difference is my model file is 432M while the example is 28M<br>\n<code>432M Oct 26 22:34 frozen_inference_graph.pb</code><br>\n<code>28M Oct 20 23:04 ssd_mobilenet_v1_android_export.pb</code></p>\n<p>When loading my model (I've enabled large heap), the Android profiler shows the memory used increases to around 2GB until the crash. I have tried using the transform_graph util, though any produced pb file gives GraphDef invalid or doesn't fix the issue.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2712171/32139524-06a3137a-bc00-11e7-9635-d1762bbdb32c.png\"><img width=\"1264\" alt=\"screen shot 2017-10-28 at 4 48 20 pm\" src=\"https://user-images.githubusercontent.com/2712171/32139524-06a3137a-bc00-11e7-9635-d1762bbdb32c.png\" style=\"max-width:100%;\"></a></p>\n<p>Here is a summary of the pb file:</p>\n<p><code>bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=frozen_inference_graph.pb</code><br>\n<code>Found 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3]) No variables spotted. Found 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity) Found 87826925 (87.83M) const parameters, 0 (0) variable parameters, and 90093 control_edges Op types used: 85323 Const, 33735 Gather, 28107 Minimum, 22484 Maximum, 16964 Reshape, 11281 Cast, 11265 Sub, 11248 Greater, 11242 Split, 11242 Where, 5696 Slice, 5683 ConcatV2, 5682 Mul, 5675 StridedSlice, 5659 Pack, 5658 Shape, 5654 Add, 5628 Squeeze, 5624 Unpack, 5621 ZerosLike, 5621 NonMaxSuppression, 229 Identity, 48 Fill, 45 ExpandDims, 37 Tile, 35 Relu6, 35 FusedBatchNorm, 34 Conv2D, 28 RealDiv, 28 Range, 28 Switch, 23 Enter, 13 Merge, 13 DepthwiseConv2dNative, 12 BiasAdd, 9 TensorArrayV3, 7 NextIteration, 6 Sqrt, 5 TensorArrayWriteV3, 5 TensorArrayGatherV3, 5 Exit, 5 TensorArraySizeV3, 5 Assert, 4 TensorArrayScatterV3, 4 Equal, 4 TensorArrayReadV3, 3 Rank, 3 Transpose, 2 All, 2 Exp,2 GreaterEqual, 2 LoopCond, 2 Less, 1 LogicalAnd, 1 TopKV2, 1 Size, 1 ResizeBilinear, 1 Placeholder, 1 Sigmoid</code><br>\n<code>To use with tensorflow/tools/benchmark:benchmark_model try these arguments: bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=-1,-1,-1,3 --output_layer=detection_boxes,detection_scores,detection_classes,num_detections</code></p>\n<p>Is this a memory leak? I don't think the app should be in the GB's</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p><code>tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java</code><br>\n<code>tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java</code></p>\n<p>(from models)<br>\n<code>models/research/object_detection/create_pet_tf_record.py</code><br>\n<code>models/research/object_detection/export_inference_graph.py</code><br>\n<code>ssd_mobilenet_v1.config</code></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Sierra+Ubuntu 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0\nPython version: Python 3.6.1 :: Anaconda custom (64-bit)\nBazel version (if compiling from source): Build label: 0.5.4-homebrew\nCUDA/cuDNN version: Not used\nGPU model and memory: Not used\nExact command to reproduce: X\n\nDescribe the problem\nWhen loading a trained-from-scratch sdd_mobilenet_v1 (frozen_inference_graph.pb) in place of \"file:///android_asset/ssd_mobilenet_v1_android_export.pb\" for the TF Detect app, the screen goes blank white and crashes, without an error logged. I have been following the pet example and have gotten passed the GraphDef Invalid error(s). In my case, the problem occurs after:\nI/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\nI/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\nWhen loading my custom ssd mobilenet model (5621 labels), I assume the model fails to load because it hangs on the white screen before crashing and I dont see:\nI/TensorFlowInferenceInterface: Model load took 502ms, TensorFlow version: 1.4.0-rc1\nI/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/ssd_mobilenet_v1_android_export.pb'\nOne notable difference is my model file is 432M while the example is 28M\n432M Oct 26 22:34 frozen_inference_graph.pb\n28M Oct 20 23:04 ssd_mobilenet_v1_android_export.pb\nWhen loading my model (I've enabled large heap), the Android profiler shows the memory used increases to around 2GB until the crash. I have tried using the transform_graph util, though any produced pb file gives GraphDef invalid or doesn't fix the issue.\n\nHere is a summary of the pb file:\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=frozen_inference_graph.pb\nFound 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3]) No variables spotted. Found 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity) Found 87826925 (87.83M) const parameters, 0 (0) variable parameters, and 90093 control_edges Op types used: 85323 Const, 33735 Gather, 28107 Minimum, 22484 Maximum, 16964 Reshape, 11281 Cast, 11265 Sub, 11248 Greater, 11242 Split, 11242 Where, 5696 Slice, 5683 ConcatV2, 5682 Mul, 5675 StridedSlice, 5659 Pack, 5658 Shape, 5654 Add, 5628 Squeeze, 5624 Unpack, 5621 ZerosLike, 5621 NonMaxSuppression, 229 Identity, 48 Fill, 45 ExpandDims, 37 Tile, 35 Relu6, 35 FusedBatchNorm, 34 Conv2D, 28 RealDiv, 28 Range, 28 Switch, 23 Enter, 13 Merge, 13 DepthwiseConv2dNative, 12 BiasAdd, 9 TensorArrayV3, 7 NextIteration, 6 Sqrt, 5 TensorArrayWriteV3, 5 TensorArrayGatherV3, 5 Exit, 5 TensorArraySizeV3, 5 Assert, 4 TensorArrayScatterV3, 4 Equal, 4 TensorArrayReadV3, 3 Rank, 3 Transpose, 2 All, 2 Exp,2 GreaterEqual, 2 LoopCond, 2 Less, 1 LogicalAnd, 1 TopKV2, 1 Size, 1 ResizeBilinear, 1 Placeholder, 1 Sigmoid\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments: bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=-1,-1,-1,3 --output_layer=detection_boxes,detection_scores,detection_classes,num_detections\nIs this a memory leak? I don't think the app should be in the GB's\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java\ntensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java\n(from models)\nmodels/research/object_detection/create_pet_tf_record.py\nmodels/research/object_detection/export_inference_graph.py\nssd_mobilenet_v1.config", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra+Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: Python 3.6.1 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**: Build label: 0.5.4-homebrew\r\n- **CUDA/cuDNN version**: Not used\r\n- **GPU model and memory**: Not used\r\n- **Exact command to reproduce**: X\r\n\r\n### Describe the problem\r\n\r\nWhen loading a trained-from-scratch sdd_mobilenet_v1 (frozen_inference_graph.pb) in place of \"file:///android_asset/ssd_mobilenet_v1_android_export.pb\" for the TF Detect app, the screen goes blank white and crashes, without an error logged. I have been following the pet example and have gotten passed the GraphDef Invalid error(s). In my case, the problem occurs after:\r\n\r\n`I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference`\r\n`I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)`\r\n\r\nWhen loading my custom ssd mobilenet model (5621 labels), I assume the model fails to load because it hangs on the white screen before crashing and I dont see:\r\n\r\n`I/TensorFlowInferenceInterface: Model load took 502ms, TensorFlow version: 1.4.0-rc1`\r\n`I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/ssd_mobilenet_v1_android_export.pb'`\r\n\r\nOne notable difference is my model file is 432M while the example is 28M\r\n`432M Oct 26 22:34 frozen_inference_graph.pb`\r\n`28M Oct 20 23:04 ssd_mobilenet_v1_android_export.pb`\r\n\r\nWhen loading my model (I've enabled large heap), the Android profiler shows the memory used increases to around 2GB until the crash. I have tried using the transform_graph util, though any produced pb file gives GraphDef invalid or doesn't fix the issue.\r\n\r\n<img width=\"1264\" alt=\"screen shot 2017-10-28 at 4 48 20 pm\" src=\"https://user-images.githubusercontent.com/2712171/32139524-06a3137a-bc00-11e7-9635-d1762bbdb32c.png\">\r\n\r\nHere is a summary of the pb file:\r\n\r\n`bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=frozen_inference_graph.pb`\r\n`Found 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3])\r\nNo variables spotted.\r\nFound 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity)\r\nFound 87826925 (87.83M) const parameters, 0 (0) variable parameters, and 90093 control_edges\r\nOp types used: 85323 Const, 33735 Gather, 28107 Minimum, 22484 Maximum, 16964 Reshape, 11281 Cast, 11265 Sub, 11248 Greater, 11242 Split, 11242 Where, 5696 Slice, 5683 ConcatV2, 5682 Mul, 5675 StridedSlice, 5659 Pack, 5658 Shape, 5654 Add, 5628 Squeeze, 5624 Unpack, 5621 ZerosLike, 5621 NonMaxSuppression, 229 Identity, 48 Fill, 45 ExpandDims, 37 Tile, 35 Relu6, 35 FusedBatchNorm, 34 Conv2D, 28 RealDiv, 28 Range, 28 Switch, 23 Enter, 13 Merge, 13 DepthwiseConv2dNative, 12 BiasAdd, 9 TensorArrayV3, 7 NextIteration, 6 Sqrt, 5 TensorArrayWriteV3, 5 TensorArrayGatherV3, 5 Exit, 5 TensorArraySizeV3, 5 Assert, 4 TensorArrayScatterV3, 4 Equal, 4 TensorArrayReadV3, 3 Rank, 3 Transpose, 2 All, 2 Exp,2 GreaterEqual, 2 LoopCond, 2 Less, 1 LogicalAnd, 1 TopKV2, 1 Size, 1 ResizeBilinear, 1 Placeholder, 1 Sigmoid`\r\n`To use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=-1,-1,-1,3 --output_layer=detection_boxes,detection_scores,detection_classes,num_detections`\r\n\r\nIs this a memory leak? I don't think the app should be in the GB's\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n`tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowObjectDetectionAPIModel.java`\r\n`tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java`\r\n\r\n(from models)\r\n`models/research/object_detection/create_pet_tf_record.py`\r\n`models/research/object_detection/export_inference_graph.py`\r\n`ssd_mobilenet_v1.config`\r\n\r\n\r\n"}