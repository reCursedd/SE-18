{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286013757", "html_url": "https://github.com/tensorflow/tensorflow/issues/4742#issuecomment-286013757", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4742", "id": 286013757, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjAxMzc1Nw==", "user": {"login": "ahundt", "id": 55744, "node_id": "MDQ6VXNlcjU1NzQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/55744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahundt", "html_url": "https://github.com/ahundt", "followers_url": "https://api.github.com/users/ahundt/followers", "following_url": "https://api.github.com/users/ahundt/following{/other_user}", "gists_url": "https://api.github.com/users/ahundt/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahundt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahundt/subscriptions", "organizations_url": "https://api.github.com/users/ahundt/orgs", "repos_url": "https://api.github.com/users/ahundt/repos", "events_url": "https://api.github.com/users/ahundt/events{/privacy}", "received_events_url": "https://api.github.com/users/ahundt/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-13T04:34:39Z", "updated_at": "2017-03-13T04:35:55Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4211946\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jbms\">@jbms</a> Thanks for your comment. Does the current code in  <a href=\"https://github.com/tensorflow/tensorflow/blob/57688fab6ad584034b1e6949e3101adc2c08ca10/tensorflow/python/ops/nn_ops.py#L143\">with_space_to_batch</a> or <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"207975417\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7545\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/7545/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/7545\">#7545</a> have a mode where the output tensor has the same dimensions as the input tensor?</p>\n<p>This is the case for <a href=\"https://github.com/tensorflow/models/blob/master/slim/nets/resnet_utils.py#L77\">conv2d_same</a> in <code>tensorflow/models</code>:</p>\n<pre><code>def conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  \"\"\"[snip...]\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n  [snip...]\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  \"\"\"\n</code></pre>\n<p>I think it would be very productive to add a note in <a href=\"https://github.com/tensorflow/tensorflow/blob/57688fab6ad584034b1e6949e3101adc2c08ca10/tensorflow/python/ops/nn_ops.py#L143\">with_space_to_batch</a> explaining what the output dimensions would be relative to given input dimensions in as they vary by configuration.</p>\n<p>Regarding your comment on atrous vs dilated convolutions, I quoted the following from a footnote in the <a href=\"https://arxiv.org/abs/1511.07122\" rel=\"nofollow\">Multi-Scale Context Aggregation by Dilated Convolutions</a>:</p>\n<blockquote>\n<p>Some recent work mistakenly referred to the dilated convolution operator itself as the <code>algorithme a trous</code>. This is incorrect. The algorithme a trous applies a filter at multiple scales to produce a signal decomposition. The algorithm uses dilated convolutions, but is not equivalent to the dilated convolution operator itself.</p>\n</blockquote>\n<p>Perhaps this is a bit pedantic but if the paper is stating this correctly, wouldn't it mean TensorFlow is mistaken in its use of atrous and dilation as synonyms? This seems to imply that what is described as the atrous algorithm only dilated filter size, while the dilated version can be configured so the output is the same size as the input.</p>", "body_text": "@jbms Thanks for your comment. Does the current code in  with_space_to_batch or #7545 have a mode where the output tensor has the same dimensions as the input tensor?\nThis is the case for conv2d_same in tensorflow/models:\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  \"\"\"[snip...]\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n  [snip...]\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  \"\"\"\n\nI think it would be very productive to add a note in with_space_to_batch explaining what the output dimensions would be relative to given input dimensions in as they vary by configuration.\nRegarding your comment on atrous vs dilated convolutions, I quoted the following from a footnote in the Multi-Scale Context Aggregation by Dilated Convolutions:\n\nSome recent work mistakenly referred to the dilated convolution operator itself as the algorithme a trous. This is incorrect. The algorithme a trous applies a filter at multiple scales to produce a signal decomposition. The algorithm uses dilated convolutions, but is not equivalent to the dilated convolution operator itself.\n\nPerhaps this is a bit pedantic but if the paper is stating this correctly, wouldn't it mean TensorFlow is mistaken in its use of atrous and dilation as synonyms? This seems to imply that what is described as the atrous algorithm only dilated filter size, while the dilated version can be configured so the output is the same size as the input.", "body": "@jbms Thanks for your comment. Does the current code in  [with_space_to_batch](https://github.com/tensorflow/tensorflow/blob/57688fab6ad584034b1e6949e3101adc2c08ca10/tensorflow/python/ops/nn_ops.py#L143) or #7545 have a mode where the output tensor has the same dimensions as the input tensor?\r\n\r\nThis is the case for [conv2d_same](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_utils.py#L77) in `tensorflow/models`:\r\n```\r\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\r\n  \"\"\"[snip...]\r\n  Args:\r\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\r\n  [snip...]\r\n  Returns:\r\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\r\n      the convolution output.\r\n  \"\"\"\r\n```\r\n\r\nI think it would be very productive to add a note in [with_space_to_batch](https://github.com/tensorflow/tensorflow/blob/57688fab6ad584034b1e6949e3101adc2c08ca10/tensorflow/python/ops/nn_ops.py#L143) explaining what the output dimensions would be relative to given input dimensions in as they vary by configuration. \r\n\r\nRegarding your comment on atrous vs dilated convolutions, I quoted the following from a footnote in the [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122):\r\n\r\n> Some recent work mistakenly referred to the dilated convolution operator itself as the `algorithme a trous`. This is incorrect. The algorithme a trous applies a filter at multiple scales to produce a signal decomposition. The algorithm uses dilated convolutions, but is not equivalent to the dilated convolution operator itself.\r\n\r\nPerhaps this is a bit pedantic but if the paper is stating this correctly, wouldn't it mean TensorFlow is mistaken in its use of atrous and dilation as synonyms? This seems to imply that what is described as the atrous algorithm only dilated filter size, while the dilated version can be configured so the output is the same size as the input.\r\n\r\n"}