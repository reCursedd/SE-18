{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286152032", "html_url": "https://github.com/tensorflow/tensorflow/issues/4742#issuecomment-286152032", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4742", "id": 286152032, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjE1MjAzMg==", "user": {"login": "warmspringwinds", "id": 2501383, "node_id": "MDQ6VXNlcjI1MDEzODM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2501383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/warmspringwinds", "html_url": "https://github.com/warmspringwinds", "followers_url": "https://api.github.com/users/warmspringwinds/followers", "following_url": "https://api.github.com/users/warmspringwinds/following{/other_user}", "gists_url": "https://api.github.com/users/warmspringwinds/gists{/gist_id}", "starred_url": "https://api.github.com/users/warmspringwinds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/warmspringwinds/subscriptions", "organizations_url": "https://api.github.com/users/warmspringwinds/orgs", "repos_url": "https://api.github.com/users/warmspringwinds/repos", "events_url": "https://api.github.com/users/warmspringwinds/events{/privacy}", "received_events_url": "https://api.github.com/users/warmspringwinds/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-13T15:57:46Z", "updated_at": "2017-03-13T15:57:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=55744\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ahundt\">@ahundt</a> , in TF atrous and dilated convolution mean the same thing. One of the parameters that they accept is <code>rate</code> which specifies the dilation rate. Definition of rate is consistent in the Deep Lab paper and the paper that you have cited. I think the confusion with the naming is similar to the case with <code>deconvolution</code> which a lot of people use to mean that they perform <code>fractionally strided convolution</code>, while <code>deconvolution</code> at the same time refers to a completely different operation in Signal Processing field.</p>\n<p>There are different ways to implement dilated convolution. TF has it implemented by sampling the input feature map which is described in the Deep Lab paper. The piece of code that you refer to actually uses this implementation under the hood.</p>\n<p>At the same time, dilated convolution is itself an ordinary convolution -- meaning that if you apply it with the <code>same</code> padding, it should produce the output with the same spatial dimensions.</p>\n<p>In case of Image Segmentation, dilated convolution is used to make it possible to use weights<br>\nfrom Image Classification networks after reducing their in-network downsampling (by means of removing layers responsible for downsampling or setting their stride to <code>1</code>). All of this allows to acquire the prediction map that is downsampled by a smaller factor (most Image Classification models have a downsampling factor of <code>32</code> --  for example, it can be reduced to <code>8</code> by following the approach described in these papers). After that, you can use bilinear upsampling or learn the upsampling kernel yourself during training to get the prediction map of the same size as the input image. You can find an example of adopting Resnet-101 for Image Segmentation by employing the aforementioned approach <a href=\"https://github.com/warmspringwinds/tf-image-segmentation/blob/master/tf_image_segmentation/models/resnet_v1_101_8s.py#L108\">here</a>.</p>", "body_text": "@ahundt , in TF atrous and dilated convolution mean the same thing. One of the parameters that they accept is rate which specifies the dilation rate. Definition of rate is consistent in the Deep Lab paper and the paper that you have cited. I think the confusion with the naming is similar to the case with deconvolution which a lot of people use to mean that they perform fractionally strided convolution, while deconvolution at the same time refers to a completely different operation in Signal Processing field.\nThere are different ways to implement dilated convolution. TF has it implemented by sampling the input feature map which is described in the Deep Lab paper. The piece of code that you refer to actually uses this implementation under the hood.\nAt the same time, dilated convolution is itself an ordinary convolution -- meaning that if you apply it with the same padding, it should produce the output with the same spatial dimensions.\nIn case of Image Segmentation, dilated convolution is used to make it possible to use weights\nfrom Image Classification networks after reducing their in-network downsampling (by means of removing layers responsible for downsampling or setting their stride to 1). All of this allows to acquire the prediction map that is downsampled by a smaller factor (most Image Classification models have a downsampling factor of 32 --  for example, it can be reduced to 8 by following the approach described in these papers). After that, you can use bilinear upsampling or learn the upsampling kernel yourself during training to get the prediction map of the same size as the input image. You can find an example of adopting Resnet-101 for Image Segmentation by employing the aforementioned approach here.", "body": "@ahundt , in TF atrous and dilated convolution mean the same thing. One of the parameters that they accept is ```rate``` which specifies the dilation rate. Definition of rate is consistent in the Deep Lab paper and the paper that you have cited. I think the confusion with the naming is similar to the case with ```deconvolution``` which a lot of people use to mean that they perform ```fractionally strided convolution```, while ```deconvolution``` at the same time refers to a completely different operation in Signal Processing field.\r\n\r\nThere are different ways to implement dilated convolution. TF has it implemented by sampling the input feature map which is described in the Deep Lab paper. The piece of code that you refer to actually uses this implementation under the hood.\r\n\r\nAt the same time, dilated convolution is itself an ordinary convolution -- meaning that if you apply it with the ```same``` padding, it should produce the output with the same spatial dimensions.\r\n\r\nIn case of Image Segmentation, dilated convolution is used to make it possible to use weights\r\nfrom Image Classification networks after reducing their in-network downsampling (by means of removing layers responsible for downsampling or setting their stride to ```1```). All of this allows to acquire the prediction map that is downsampled by a smaller factor (most Image Classification models have a downsampling factor of ```32``` --  for example, it can be reduced to ```8``` by following the approach described in these papers). After that, you can use bilinear upsampling or learn the upsampling kernel yourself during training to get the prediction map of the same size as the input image. You can find an example of adopting Resnet-101 for Image Segmentation by employing the aforementioned approach [here](https://github.com/warmspringwinds/tf-image-segmentation/blob/master/tf_image_segmentation/models/resnet_v1_101_8s.py#L108)."}