{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14133", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14133/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14133/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14133/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14133", "id": 270037765, "node_id": "MDU6SXNzdWUyNzAwMzc3NjU=", "number": 14133, "title": "Eager: Automatic Device Placement", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 736653459, "node_id": "MDU6TGFiZWw3MzY2NTM0NTk=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:eager", "name": "comp:eager", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2017-10-31T16:57:46Z", "updated_at": "2018-04-06T17:59:16Z", "closed_at": "2018-04-06T17:59:16Z", "author_association": "MEMBER", "body_html": "<p>When a GPU is available, TensorFlow automatically copies tensors between CPU and GPU memory (see <a href=\"https://www.tensorflow.org/tutorials/using_gpu\" rel=\"nofollow\">Using GPUs</a>).</p>\n<p>When eager execution is enabled, automatic copying between devices is disabled by default. When executing imperatively, copying data between CPU and GPU is more likely to become a performance bottleneck. Avoiding automatic copying makes it easier to identify such bottlenecks. For example, consider the program:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(\u201c<span class=\"pl-k\">/</span>cpu:<span class=\"pl-c1\">0</span>\u201d):\n  x <span class=\"pl-k\">=</span> tf.ones([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>])\n<span class=\"pl-k\">with</span> tf.device(\u201c<span class=\"pl-k\">/</span>gpu:<span class=\"pl-c1\">0</span>\u201d):\n  y <span class=\"pl-k\">=</span> tf.matmul(x, x)</pre></div>\n<p>This will fail with an error like:</p>\n<pre><code>Tensors on conflicting devices: cannot compute MatMul as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 \u2026\n</code></pre>\n<p>indicating that the <code>matmul</code> operation cannot be conducted on the GPU as its inputs were host memory.</p>\n<p>If you run into this situation, your options are:</p>\n<ul>\n<li>Accept the potential performance hit by explicitly enabling automatic copying between devices:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre>tfe.enable_eager_execution(<span class=\"pl-v\">device_policy</span><span class=\"pl-k\">=</span>tfe.<span class=\"pl-c1\">DEVICE_PLACEMENT_SILENT</span>)</pre></div>\n<ul>\n<li>Explicitly copy the tensor yourself using <code>tf.identity</code> or <code>Tensor.gpu()</code>:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(\u201c<span class=\"pl-k\">/</span>gpu:<span class=\"pl-c1\">0</span>\u201d):\n  x <span class=\"pl-k\">=</span> tf.identity(x)\n  y <span class=\"pl-k\">=</span> tf.matmul(x, x)</pre></div>", "body_text": "When a GPU is available, TensorFlow automatically copies tensors between CPU and GPU memory (see Using GPUs).\nWhen eager execution is enabled, automatic copying between devices is disabled by default. When executing imperatively, copying data between CPU and GPU is more likely to become a performance bottleneck. Avoiding automatic copying makes it easier to identify such bottlenecks. For example, consider the program:\nwith tf.device(\u201c/cpu:0\u201d):\n  x = tf.ones([2, 2])\nwith tf.device(\u201c/gpu:0\u201d):\n  y = tf.matmul(x, x)\nThis will fail with an error like:\nTensors on conflicting devices: cannot compute MatMul as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 \u2026\n\nindicating that the matmul operation cannot be conducted on the GPU as its inputs were host memory.\nIf you run into this situation, your options are:\n\nAccept the potential performance hit by explicitly enabling automatic copying between devices:\n\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n\nExplicitly copy the tensor yourself using tf.identity or Tensor.gpu():\n\nwith tf.device(\u201c/gpu:0\u201d):\n  x = tf.identity(x)\n  y = tf.matmul(x, x)", "body": "When a GPU is available, TensorFlow automatically copies tensors between CPU and GPU memory (see [Using GPUs](https://www.tensorflow.org/tutorials/using_gpu)).\r\n\r\nWhen eager execution is enabled, automatic copying between devices is disabled by default. When executing imperatively, copying data between CPU and GPU is more likely to become a performance bottleneck. Avoiding automatic copying makes it easier to identify such bottlenecks. For example, consider the program:\r\n\r\n\r\n```python\r\nwith tf.device(\u201c/cpu:0\u201d):\r\n  x = tf.ones([2, 2])\r\nwith tf.device(\u201c/gpu:0\u201d):\r\n  y = tf.matmul(x, x)\r\n```\r\n\r\nThis will fail with an error like:\r\n\r\n```\r\nTensors on conflicting devices: cannot compute MatMul as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 \u2026\r\n```\r\n\r\n\r\nindicating that the `matmul` operation cannot be conducted on the GPU as its inputs were host memory.\r\n\r\nIf you run into this situation, your options are:\r\n\r\n- Accept the potential performance hit by explicitly enabling automatic copying between devices:\r\n\r\n\r\n```python\r\ntfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\r\n```\r\n\r\n-  Explicitly copy the tensor yourself using `tf.identity` or `Tensor.gpu()`:\r\n\r\n```python\r\nwith tf.device(\u201c/gpu:0\u201d):\r\n  x = tf.identity(x)\r\n  y = tf.matmul(x, x)\r\n```"}