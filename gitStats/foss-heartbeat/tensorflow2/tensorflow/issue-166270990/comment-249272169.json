{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/249272169", "html_url": "https://github.com/tensorflow/tensorflow/issues/3385#issuecomment-249272169", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3385", "id": 249272169, "node_id": "MDEyOklzc3VlQ29tbWVudDI0OTI3MjE2OQ==", "user": {"login": "jrabary", "id": 1025387, "node_id": "MDQ6VXNlcjEwMjUzODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1025387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrabary", "html_url": "https://github.com/jrabary", "followers_url": "https://api.github.com/users/jrabary/followers", "following_url": "https://api.github.com/users/jrabary/following{/other_user}", "gists_url": "https://api.github.com/users/jrabary/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrabary/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrabary/subscriptions", "organizations_url": "https://api.github.com/users/jrabary/orgs", "repos_url": "https://api.github.com/users/jrabary/repos", "events_url": "https://api.github.com/users/jrabary/events{/privacy}", "received_events_url": "https://api.github.com/users/jrabary/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-23T18:45:57Z", "updated_at": "2016-09-23T18:45:57Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6020988\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/joel-shor\">@joel-shor</a>  Here is what exactly I'm trying to do : I have about 2000 classes that are heavily unbalanced. There can be a factor of 30 between the least and the most populated classes. I need to create a batch with a given number of classes and a given number of data per class e.g 10 classes with 20 samples per class for a batch of 200 samples. In this case after each run, I begin by sampling 10 class labels and update the target probability accordingly. In the following you have a snippets of my codes :</p>\n<pre><code># sample labels and update target prob\n# target_labels : number of label to select\n# num_labels : total number of labels\np = [float(target_labels)/float(num_labels)]\nlabel_sampler = tf.contrib.distributions.Bernoulli(p=p, dtype=tf.float32)\nlabel_samples = tf.squeeze(label_sampler.sample_n(num_labels))\nlabel_samples = tf.while_loop(lambda l: tf.less(tf.reduce_sum(l), 3),\n                                      lambda l: l,\n                                      [label_samples])\ntarget_label_prob = tf.div(label_samples, tf.reduce_sum(label_samples))\n\n\ninit_prob = np.random.binomial(1, target_labels/float(num_labels), num_labels)\ninit_prob = init_prob/np.sum(init_prob)\n\n# HACK : put the target prob in a non trainable variable to make update possible\ntarget_prob = tf.get_variable('target_prob',\n                                      trainable=False, shape=[num_labels],\n                                      initializer=tf.constant_initializer(init_prob, dtype=tf.float32))\nupdate_target_prob = tf.assign(target_prob, target_label_prob)\nupdate_target_prob = tf.group(update_target_prob,\n                                      tf.Print(target_prob, [target_prob, tf.reduce_sum(label_samples)],\n                                               message=\"Update probabilty \", first_n=50))\ntf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_target_prob)\n\n# data_prob is computed from the data set\n[imgs], lbls = tf.contrib.training.stratified_sample([images], labels, target_prob, batch_size,\n                                                                  init_probs=data_prob, enqueue_many=True,\n                                                                  threads_per_queue=num_threads)\n</code></pre>", "body_text": "@joel-shor  Here is what exactly I'm trying to do : I have about 2000 classes that are heavily unbalanced. There can be a factor of 30 between the least and the most populated classes. I need to create a batch with a given number of classes and a given number of data per class e.g 10 classes with 20 samples per class for a batch of 200 samples. In this case after each run, I begin by sampling 10 class labels and update the target probability accordingly. In the following you have a snippets of my codes :\n# sample labels and update target prob\n# target_labels : number of label to select\n# num_labels : total number of labels\np = [float(target_labels)/float(num_labels)]\nlabel_sampler = tf.contrib.distributions.Bernoulli(p=p, dtype=tf.float32)\nlabel_samples = tf.squeeze(label_sampler.sample_n(num_labels))\nlabel_samples = tf.while_loop(lambda l: tf.less(tf.reduce_sum(l), 3),\n                                      lambda l: l,\n                                      [label_samples])\ntarget_label_prob = tf.div(label_samples, tf.reduce_sum(label_samples))\n\n\ninit_prob = np.random.binomial(1, target_labels/float(num_labels), num_labels)\ninit_prob = init_prob/np.sum(init_prob)\n\n# HACK : put the target prob in a non trainable variable to make update possible\ntarget_prob = tf.get_variable('target_prob',\n                                      trainable=False, shape=[num_labels],\n                                      initializer=tf.constant_initializer(init_prob, dtype=tf.float32))\nupdate_target_prob = tf.assign(target_prob, target_label_prob)\nupdate_target_prob = tf.group(update_target_prob,\n                                      tf.Print(target_prob, [target_prob, tf.reduce_sum(label_samples)],\n                                               message=\"Update probabilty \", first_n=50))\ntf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_target_prob)\n\n# data_prob is computed from the data set\n[imgs], lbls = tf.contrib.training.stratified_sample([images], labels, target_prob, batch_size,\n                                                                  init_probs=data_prob, enqueue_many=True,\n                                                                  threads_per_queue=num_threads)", "body": "@joel-shor  Here is what exactly I'm trying to do : I have about 2000 classes that are heavily unbalanced. There can be a factor of 30 between the least and the most populated classes. I need to create a batch with a given number of classes and a given number of data per class e.g 10 classes with 20 samples per class for a batch of 200 samples. In this case after each run, I begin by sampling 10 class labels and update the target probability accordingly. In the following you have a snippets of my codes : \n\n```\n# sample labels and update target prob\n# target_labels : number of label to select\n# num_labels : total number of labels\np = [float(target_labels)/float(num_labels)]\nlabel_sampler = tf.contrib.distributions.Bernoulli(p=p, dtype=tf.float32)\nlabel_samples = tf.squeeze(label_sampler.sample_n(num_labels))\nlabel_samples = tf.while_loop(lambda l: tf.less(tf.reduce_sum(l), 3),\n                                      lambda l: l,\n                                      [label_samples])\ntarget_label_prob = tf.div(label_samples, tf.reduce_sum(label_samples))\n\n\ninit_prob = np.random.binomial(1, target_labels/float(num_labels), num_labels)\ninit_prob = init_prob/np.sum(init_prob)\n\n# HACK : put the target prob in a non trainable variable to make update possible\ntarget_prob = tf.get_variable('target_prob',\n                                      trainable=False, shape=[num_labels],\n                                      initializer=tf.constant_initializer(init_prob, dtype=tf.float32))\nupdate_target_prob = tf.assign(target_prob, target_label_prob)\nupdate_target_prob = tf.group(update_target_prob,\n                                      tf.Print(target_prob, [target_prob, tf.reduce_sum(label_samples)],\n                                               message=\"Update probabilty \", first_n=50))\ntf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_target_prob)\n\n# data_prob is computed from the data set\n[imgs], lbls = tf.contrib.training.stratified_sample([images], labels, target_prob, batch_size,\n                                                                  init_probs=data_prob, enqueue_many=True,\n                                                                  threads_per_queue=num_threads)\n```\n"}