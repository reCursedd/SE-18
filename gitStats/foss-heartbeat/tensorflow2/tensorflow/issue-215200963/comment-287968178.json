{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287968178", "html_url": "https://github.com/tensorflow/tensorflow/pull/8522#issuecomment-287968178", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8522", "id": 287968178, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Nzk2ODE3OA==", "user": {"login": "snnn", "id": 856316, "node_id": "MDQ6VXNlcjg1NjMxNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/856316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snnn", "html_url": "https://github.com/snnn", "followers_url": "https://api.github.com/users/snnn/followers", "following_url": "https://api.github.com/users/snnn/following{/other_user}", "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snnn/subscriptions", "organizations_url": "https://api.github.com/users/snnn/orgs", "repos_url": "https://api.github.com/users/snnn/repos", "events_url": "https://api.github.com/users/snnn/events{/privacy}", "received_events_url": "https://api.github.com/users/snnn/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-21T03:40:42Z", "updated_at": "2017-03-21T03:40:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22941064\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/guschmue\">@guschmue</a></p>\n<ol>\n<li>\n<p>FILE_FLAG_RANDOM_ACCESS flag is a hint for Cache Manager to keep mapped views(read blocks) of the file in memory as long as possible, until Memory Manager doesn\u2019t signal low memory condition.  So, if we run tensorflow standalone, without sharing resources with other programs, the performance is almost the same. However, if we run it on yarn, the situation is quite different.  And it's hard to say how much it faster/slower because it number varies, and there is no way to control tensorflow's memory usage.</p>\n</li>\n<li>\n<p>At the same time, this flag instructs Cache Manager to disable prefetching of file data, which could reduce unnecessary disk io and increase read latency.  However, we already have IoBuffer as the user mode cache. For sequential reads, I can't see any difference.</p>\n</li>\n</ol>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>  It would be great if there is a new file type like \"SeqAccessFile\". Actually, FILE_FLAG_RANDOM_ACCESS  is good for random accesses(e.g. KV DBs, sparse model serving). It is just not good for sequential reads.</p>", "body_text": "Hi @guschmue\n\n\nFILE_FLAG_RANDOM_ACCESS flag is a hint for Cache Manager to keep mapped views(read blocks) of the file in memory as long as possible, until Memory Manager doesn\u2019t signal low memory condition.  So, if we run tensorflow standalone, without sharing resources with other programs, the performance is almost the same. However, if we run it on yarn, the situation is quite different.  And it's hard to say how much it faster/slower because it number varies, and there is no way to control tensorflow's memory usage.\n\n\nAt the same time, this flag instructs Cache Manager to disable prefetching of file data, which could reduce unnecessary disk io and increase read latency.  However, we already have IoBuffer as the user mode cache. For sequential reads, I can't see any difference.\n\n\n@mrry  It would be great if there is a new file type like \"SeqAccessFile\". Actually, FILE_FLAG_RANDOM_ACCESS  is good for random accesses(e.g. KV DBs, sparse model serving). It is just not good for sequential reads.", "body": "Hi @guschmue  \r\n\r\n1. FILE_FLAG_RANDOM_ACCESS flag is a hint for Cache Manager to keep mapped views(read blocks) of the file in memory as long as possible, until Memory Manager doesn\u2019t signal low memory condition.  So, if we run tensorflow standalone, without sharing resources with other programs, the performance is almost the same. However, if we run it on yarn, the situation is quite different.  And it's hard to say how much it faster/slower because it number varies, and there is no way to control tensorflow's memory usage.\r\n\r\n2. At the same time, this flag instructs Cache Manager to disable prefetching of file data, which could reduce unnecessary disk io and increase read latency.  However, we already have IoBuffer as the user mode cache. For sequential reads, I can't see any difference.\r\n\r\n@mrry  It would be great if there is a new file type like \"SeqAccessFile\". Actually, FILE_FLAG_RANDOM_ACCESS  is good for random accesses(e.g. KV DBs, sparse model serving). It is just not good for sequential reads."}