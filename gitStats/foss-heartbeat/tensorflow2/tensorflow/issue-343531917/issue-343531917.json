{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21048", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21048/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21048/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21048/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21048", "id": 343531917, "node_id": "MDU6SXNzdWUzNDM1MzE5MTc=", "number": 21048, "title": "High loss of accuracy after coverting \".pb\" to \".lite\" on Android", "user": {"login": "seayqrain", "id": 22736331, "node_id": "MDQ6VXNlcjIyNzM2MzMx", "avatar_url": "https://avatars2.githubusercontent.com/u/22736331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seayqrain", "html_url": "https://github.com/seayqrain", "followers_url": "https://api.github.com/users/seayqrain/followers", "following_url": "https://api.github.com/users/seayqrain/following{/other_user}", "gists_url": "https://api.github.com/users/seayqrain/gists{/gist_id}", "starred_url": "https://api.github.com/users/seayqrain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seayqrain/subscriptions", "organizations_url": "https://api.github.com/users/seayqrain/orgs", "repos_url": "https://api.github.com/users/seayqrain/repos", "events_url": "https://api.github.com/users/seayqrain/events{/privacy}", "received_events_url": "https://api.github.com/users/seayqrain/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "miaout17", "id": 22063, "node_id": "MDQ6VXNlcjIyMDYz", "avatar_url": "https://avatars0.githubusercontent.com/u/22063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miaout17", "html_url": "https://github.com/miaout17", "followers_url": "https://api.github.com/users/miaout17/followers", "following_url": "https://api.github.com/users/miaout17/following{/other_user}", "gists_url": "https://api.github.com/users/miaout17/gists{/gist_id}", "starred_url": "https://api.github.com/users/miaout17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miaout17/subscriptions", "organizations_url": "https://api.github.com/users/miaout17/orgs", "repos_url": "https://api.github.com/users/miaout17/repos", "events_url": "https://api.github.com/users/miaout17/events{/privacy}", "received_events_url": "https://api.github.com/users/miaout17/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "miaout17", "id": 22063, "node_id": "MDQ6VXNlcjIyMDYz", "avatar_url": "https://avatars0.githubusercontent.com/u/22063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miaout17", "html_url": "https://github.com/miaout17", "followers_url": "https://api.github.com/users/miaout17/followers", "following_url": "https://api.github.com/users/miaout17/following{/other_user}", "gists_url": "https://api.github.com/users/miaout17/gists{/gist_id}", "starred_url": "https://api.github.com/users/miaout17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miaout17/subscriptions", "organizations_url": "https://api.github.com/users/miaout17/orgs", "repos_url": "https://api.github.com/users/miaout17/repos", "events_url": "https://api.github.com/users/miaout17/events{/privacy}", "received_events_url": "https://api.github.com/users/miaout17/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-07-23T08:16:20Z", "updated_at": "2018-07-24T20:46:46Z", "closed_at": "2018-07-24T20:46:45Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nI only modified the ImageClassifier.java to make it compatible with float and quant.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 14.04.3 LTS</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<br>\nLG G4</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.7.1</li>\n<li><strong>Python version</strong>:2.7.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hi,<br>\nI retrained a module refer to <a href=\"https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0\" rel=\"nofollow\">tensorflow-for-poets</a>, and its retrain.py was replaced by <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py\">this one</a>.</p>\n<p>I did two experiments:</p>\n<p><strong>1. Choose the module: mobilenet_1.0_224_quant</strong><br>\nI retrained the module with the command below:</p>\n<blockquote>\n<p>python -m scripts.retrain <br>\n  --architecture=mobilenet_1.0_224_quant <br>\n  --bottleneck_dir=tf_files/bottlenecks <br>\n  --how_many_training_steps=500 <br>\n  --model_dir=tf_files/models/ <br>\n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224_quant <br>\n  --output_graph=tf_files/retrained_graph.pb <br>\n  --output_labels=tf_files/retrained_labels.txt <br>\n  --image_dir=tf_files/flower_photos</p>\n</blockquote>\n<p>Then I converted \".pb\" to \".lite\" with this command:</p>\n<blockquote>\n<p>toco <br>\n  --input_file=tf_files/retrained_graph.pb <br>\n  --output_file=tf_files/optimized_graph_quant.lite <br>\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE <br>\n  --inference_type=QUANTIZED_UINT8 <br>\n  --input_shape=\"1,224,224,3\" <br>\n  --input_array=input <br>\n  --output_array=final_result <br>\n  --std_value=128 --mean_value=128</p>\n</blockquote>\n<p>I tested <em>optimized_graph_quant.lite</em> with the <em>TfLiteCameraDemo</em> which provided by tensorflow-for-poets.<br>\nAnd only once inference, I found that the accuracy is very good:</p>\n<blockquote>\n<p>01-01 01:12:37.942 12117 12217 D TfLiteCameraDemo: Timecost to run model inference: 354<br>\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: textToShow = 354ms<br>\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: roses: 1.00<br>\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: sunflowers: 0.00<br>\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: tulips: 0.00</p>\n</blockquote>\n<p><strong>2. Choose the module: mobilenet_1.0_224</strong><br>\nI retrained the module with the command below:</p>\n<blockquote>\n<p>python -m scripts.retrain <br>\n  --architecture=mobilenet_1.0_224 <br>\n  --bottleneck_dir=tf_files/bottlenecks <br>\n  --how_many_training_steps=500 <br>\n  --model_dir=tf_files/models/ <br>\n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224 <br>\n  --output_graph=tf_files/retrained_graph.pb <br>\n  --output_labels=tf_files/retrained_labels.txt <br>\n  --image_dir=tf_files/flower_photos</p>\n</blockquote>\n<p>Then I converted \".pb\" to \".lite\" with this command:</p>\n<blockquote>\n<p>toco <br>\n  --input_file=tf_files/retrained_graph.pb <br>\n  --output_file=tf_files/optimized_graph.lite <br>\n  --input_format=TENSORFLOW_GRAPHDEF <br>\n  --output_format=TFLITE <br>\n  --input_shape=\"1,224,224,3\" <br>\n  --input_array=input <br>\n  --output_array=final_result <br>\n  --inference_type=FLOAT <br>\n  --input_data_type=FLOAT</p>\n</blockquote>\n<p>I also tested optimized_graph.lite with the <em>TfLiteCameraDemo</em> which provided by tensorflow-for-poets.<br>\nAnd after multiple inferences, the accuracy increased to an acceptable value :</p>\n<blockquote>\n<p>01-01 01:29:23.221 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 510<br>\n01-01 01:29:23.226 13207 13224 D TfLiteCameraDemo: roses: 0.06<br>\n01-01 01:29:23.483 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 238<br>\n01-01 01:29:23.488 13207 13224 D TfLiteCameraDemo: roses: 0.18<br>\n01-01 01:29:23.741 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 221<br>\n01-01 01:29:23.746 13207 13224 D TfLiteCameraDemo: roses: 0.31<br>\n01-01 01:29:24.162 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 398<br>\n01-01 01:29:24.167 13207 13224 D TfLiteCameraDemo: roses: 0.45<br>\n01-01 01:29:24.527 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 336<br>\n01-01 01:29:24.533 13207 13224 D TfLiteCameraDemo: roses: 0.58<br>\n01-01 01:29:24.898 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 339<br>\n01-01 01:29:24.907 13207 13224 D TfLiteCameraDemo: roses: 0.68<br>\n01-01 01:29:25.274 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 326<br>\n01-01 01:29:25.280 13207 13224 D TfLiteCameraDemo: roses: 0.76<br>\n01-01 01:29:25.646 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 340<br>\n01-01 01:29:25.652 13207 13224 D TfLiteCameraDemo: roses: 0.83<br>\n01-01 01:29:26.032 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 354<br>\n01-01 01:29:26.041 13207 13224 D TfLiteCameraDemo: roses: 0.87<br>\n01-01 01:29:26.433 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365<br>\n01-01 01:29:26.438 13207 13224 D TfLiteCameraDemo: roses: 0.91<br>\n01-01 01:29:26.831 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365<br>\n01-01 01:29:26.838 13207 13224 D TfLiteCameraDemo: roses: 0.93<br>\n01-01 01:29:27.288 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 391<br>\n01-01 01:29:27.296 13207 13224 D TfLiteCameraDemo: roses: 0.95<br>\n01-01 01:29:27.658 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 333<br>\n01-01 01:29:27.663 13207 13224 D TfLiteCameraDemo: roses: 0.97<br>\n01-01 01:29:28.005 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 313<br>\n01-01 01:29:28.010 13207 13224 D TfLiteCameraDemo: roses: 0.97<br>\n01-01 01:29:28.381 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 341<br>\n01-01 01:29:28.385 13207 13224 D TfLiteCameraDemo: roses: 0.98</p>\n</blockquote>\n<p><strong>Please help to analyze that why it needs multiple inferences to get an good accuracy here? Thanks!</strong></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI only modified the ImageClassifier.java to make it compatible with float and quant.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 14.04.3 LTS\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nLG G4\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):1.7.1\nPython version:2.7.6\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce:\n\nDescribe the problem\nHi,\nI retrained a module refer to tensorflow-for-poets, and its retrain.py was replaced by this one.\nI did two experiments:\n1. Choose the module: mobilenet_1.0_224_quant\nI retrained the module with the command below:\n\npython -m scripts.retrain \n  --architecture=mobilenet_1.0_224_quant \n  --bottleneck_dir=tf_files/bottlenecks \n  --how_many_training_steps=500 \n  --model_dir=tf_files/models/ \n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224_quant \n  --output_graph=tf_files/retrained_graph.pb \n  --output_labels=tf_files/retrained_labels.txt \n  --image_dir=tf_files/flower_photos\n\nThen I converted \".pb\" to \".lite\" with this command:\n\ntoco \n  --input_file=tf_files/retrained_graph.pb \n  --output_file=tf_files/optimized_graph_quant.lite \n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \n  --inference_type=QUANTIZED_UINT8 \n  --input_shape=\"1,224,224,3\" \n  --input_array=input \n  --output_array=final_result \n  --std_value=128 --mean_value=128\n\nI tested optimized_graph_quant.lite with the TfLiteCameraDemo which provided by tensorflow-for-poets.\nAnd only once inference, I found that the accuracy is very good:\n\n01-01 01:12:37.942 12117 12217 D TfLiteCameraDemo: Timecost to run model inference: 354\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: textToShow = 354ms\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: roses: 1.00\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: sunflowers: 0.00\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: tulips: 0.00\n\n2. Choose the module: mobilenet_1.0_224\nI retrained the module with the command below:\n\npython -m scripts.retrain \n  --architecture=mobilenet_1.0_224 \n  --bottleneck_dir=tf_files/bottlenecks \n  --how_many_training_steps=500 \n  --model_dir=tf_files/models/ \n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224 \n  --output_graph=tf_files/retrained_graph.pb \n  --output_labels=tf_files/retrained_labels.txt \n  --image_dir=tf_files/flower_photos\n\nThen I converted \".pb\" to \".lite\" with this command:\n\ntoco \n  --input_file=tf_files/retrained_graph.pb \n  --output_file=tf_files/optimized_graph.lite \n  --input_format=TENSORFLOW_GRAPHDEF \n  --output_format=TFLITE \n  --input_shape=\"1,224,224,3\" \n  --input_array=input \n  --output_array=final_result \n  --inference_type=FLOAT \n  --input_data_type=FLOAT\n\nI also tested optimized_graph.lite with the TfLiteCameraDemo which provided by tensorflow-for-poets.\nAnd after multiple inferences, the accuracy increased to an acceptable value :\n\n01-01 01:29:23.221 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 510\n01-01 01:29:23.226 13207 13224 D TfLiteCameraDemo: roses: 0.06\n01-01 01:29:23.483 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 238\n01-01 01:29:23.488 13207 13224 D TfLiteCameraDemo: roses: 0.18\n01-01 01:29:23.741 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 221\n01-01 01:29:23.746 13207 13224 D TfLiteCameraDemo: roses: 0.31\n01-01 01:29:24.162 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 398\n01-01 01:29:24.167 13207 13224 D TfLiteCameraDemo: roses: 0.45\n01-01 01:29:24.527 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 336\n01-01 01:29:24.533 13207 13224 D TfLiteCameraDemo: roses: 0.58\n01-01 01:29:24.898 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 339\n01-01 01:29:24.907 13207 13224 D TfLiteCameraDemo: roses: 0.68\n01-01 01:29:25.274 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 326\n01-01 01:29:25.280 13207 13224 D TfLiteCameraDemo: roses: 0.76\n01-01 01:29:25.646 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 340\n01-01 01:29:25.652 13207 13224 D TfLiteCameraDemo: roses: 0.83\n01-01 01:29:26.032 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 354\n01-01 01:29:26.041 13207 13224 D TfLiteCameraDemo: roses: 0.87\n01-01 01:29:26.433 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365\n01-01 01:29:26.438 13207 13224 D TfLiteCameraDemo: roses: 0.91\n01-01 01:29:26.831 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365\n01-01 01:29:26.838 13207 13224 D TfLiteCameraDemo: roses: 0.93\n01-01 01:29:27.288 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 391\n01-01 01:29:27.296 13207 13224 D TfLiteCameraDemo: roses: 0.95\n01-01 01:29:27.658 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 333\n01-01 01:29:27.663 13207 13224 D TfLiteCameraDemo: roses: 0.97\n01-01 01:29:28.005 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 313\n01-01 01:29:28.010 13207 13224 D TfLiteCameraDemo: roses: 0.97\n01-01 01:29:28.381 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 341\n01-01 01:29:28.385 13207 13224 D TfLiteCameraDemo: roses: 0.98\n\nPlease help to analyze that why it needs multiple inferences to get an good accuracy here? Thanks!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI only modified the ImageClassifier.java to make it compatible with float and quant.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04.3 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nLG G4\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.7.1\r\n- **Python version**:2.7.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nHi,\r\nI retrained a module refer to [tensorflow-for-poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0), and its retrain.py was replaced by [this one](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py).\r\n\r\nI did two experiments:\r\n\r\n**1. Choose the module: mobilenet_1.0_224_quant**\r\nI retrained the module with the command below:\r\n\r\n> python -m scripts.retrain \\\r\n  --architecture=mobilenet_1.0_224_quant \\\r\n  --bottleneck_dir=tf_files/bottlenecks \\\r\n  --how_many_training_steps=500 \\\r\n  --model_dir=tf_files/models/ \\\r\n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224_quant \\\r\n  --output_graph=tf_files/retrained_graph.pb \\\r\n  --output_labels=tf_files/retrained_labels.txt \\\r\n  --image_dir=tf_files/flower_photos\r\n\r\nThen I converted \".pb\" to \".lite\" with this command:\r\n\r\n> toco \\\r\n  --input_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/optimized_graph_quant.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=\"1,224,224,3\" \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --std_value=128 --mean_value=128\r\n\r\nI tested _optimized_graph_quant.lite_ with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.\r\nAnd only once inference, I found that the accuracy is very good:\r\n\r\n> 01-01 01:12:37.942 12117 12217 D TfLiteCameraDemo: Timecost to run model inference: 354\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: textToShow = 354ms\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: roses: 1.00\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: sunflowers: 0.00\r\n01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: tulips: 0.00\r\n\r\n\r\n**2. Choose the module: mobilenet_1.0_224**\r\nI retrained the module with the command below:\r\n\r\n> python -m scripts.retrain \\\r\n  --architecture=mobilenet_1.0_224 \\\r\n  --bottleneck_dir=tf_files/bottlenecks \\\r\n  --how_many_training_steps=500 \\\r\n  --model_dir=tf_files/models/ \\\r\n  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224 \\\r\n  --output_graph=tf_files/retrained_graph.pb \\\r\n  --output_labels=tf_files/retrained_labels.txt \\\r\n  --image_dir=tf_files/flower_photos\r\n\r\nThen I converted \".pb\" to \".lite\" with this command:\r\n\r\n> toco \\\r\n  --input_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/optimized_graph.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=\"1,224,224,3\" \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --inference_type=FLOAT \\\r\n  --input_data_type=FLOAT\r\n\r\nI also tested optimized_graph.lite with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.\r\nAnd after multiple inferences, the accuracy increased to an acceptable value :\r\n\r\n> 01-01 01:29:23.221 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 510\r\n01-01 01:29:23.226 13207 13224 D TfLiteCameraDemo: roses: 0.06\r\n01-01 01:29:23.483 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 238\r\n01-01 01:29:23.488 13207 13224 D TfLiteCameraDemo: roses: 0.18\r\n01-01 01:29:23.741 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 221\r\n01-01 01:29:23.746 13207 13224 D TfLiteCameraDemo: roses: 0.31\r\n01-01 01:29:24.162 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 398\r\n01-01 01:29:24.167 13207 13224 D TfLiteCameraDemo: roses: 0.45\r\n01-01 01:29:24.527 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 336\r\n01-01 01:29:24.533 13207 13224 D TfLiteCameraDemo: roses: 0.58\r\n01-01 01:29:24.898 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 339\r\n01-01 01:29:24.907 13207 13224 D TfLiteCameraDemo: roses: 0.68\r\n01-01 01:29:25.274 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 326\r\n01-01 01:29:25.280 13207 13224 D TfLiteCameraDemo: roses: 0.76\r\n01-01 01:29:25.646 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 340\r\n01-01 01:29:25.652 13207 13224 D TfLiteCameraDemo: roses: 0.83\r\n01-01 01:29:26.032 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 354\r\n01-01 01:29:26.041 13207 13224 D TfLiteCameraDemo: roses: 0.87\r\n01-01 01:29:26.433 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365\r\n01-01 01:29:26.438 13207 13224 D TfLiteCameraDemo: roses: 0.91\r\n01-01 01:29:26.831 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365\r\n01-01 01:29:26.838 13207 13224 D TfLiteCameraDemo: roses: 0.93\r\n01-01 01:29:27.288 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 391\r\n01-01 01:29:27.296 13207 13224 D TfLiteCameraDemo: roses: 0.95\r\n01-01 01:29:27.658 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 333\r\n01-01 01:29:27.663 13207 13224 D TfLiteCameraDemo: roses: 0.97\r\n01-01 01:29:28.005 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 313\r\n01-01 01:29:28.010 13207 13224 D TfLiteCameraDemo: roses: 0.97\r\n01-01 01:29:28.381 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 341\r\n01-01 01:29:28.385 13207 13224 D TfLiteCameraDemo: roses: 0.98\r\n\r\n**Please help to analyze that why it needs multiple inferences to get an good accuracy here? Thanks!**\r\n"}