{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19305", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19305/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19305/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19305/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19305", "id": 323384918, "node_id": "MDU6SXNzdWUzMjMzODQ5MTg=", "number": 19305, "title": "Inconsistent behaviour when `Dataset.repeats()` and `max_steps` differ", "user": {"login": "elgehelge", "id": 665484, "node_id": "MDQ6VXNlcjY2NTQ4NA==", "avatar_url": "https://avatars0.githubusercontent.com/u/665484?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elgehelge", "html_url": "https://github.com/elgehelge", "followers_url": "https://api.github.com/users/elgehelge/followers", "following_url": "https://api.github.com/users/elgehelge/following{/other_user}", "gists_url": "https://api.github.com/users/elgehelge/gists{/gist_id}", "starred_url": "https://api.github.com/users/elgehelge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elgehelge/subscriptions", "organizations_url": "https://api.github.com/users/elgehelge/orgs", "repos_url": "https://api.github.com/users/elgehelge/repos", "events_url": "https://api.github.com/users/elgehelge/events{/privacy}", "received_events_url": "https://api.github.com/users/elgehelge/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "xiejw", "id": 1184671, "node_id": "MDQ6VXNlcjExODQ2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1184671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiejw", "html_url": "https://github.com/xiejw", "followers_url": "https://api.github.com/users/xiejw/followers", "following_url": "https://api.github.com/users/xiejw/following{/other_user}", "gists_url": "https://api.github.com/users/xiejw/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiejw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiejw/subscriptions", "organizations_url": "https://api.github.com/users/xiejw/orgs", "repos_url": "https://api.github.com/users/xiejw/repos", "events_url": "https://api.github.com/users/xiejw/events{/privacy}", "received_events_url": "https://api.github.com/users/xiejw/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "xiejw", "id": 1184671, "node_id": "MDQ6VXNlcjExODQ2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1184671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiejw", "html_url": "https://github.com/xiejw", "followers_url": "https://api.github.com/users/xiejw/followers", "following_url": "https://api.github.com/users/xiejw/following{/other_user}", "gists_url": "https://api.github.com/users/xiejw/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiejw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiejw/subscriptions", "organizations_url": "https://api.github.com/users/xiejw/orgs", "repos_url": "https://api.github.com/users/xiejw/repos", "events_url": "https://api.github.com/users/xiejw/events{/privacy}", "received_events_url": "https://api.github.com/users/xiejw/received_events", "type": "User", "site_admin": false}, {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-05-15T21:11:15Z", "updated_at": "2018-06-25T16:55:43Z", "closed_at": "2018-06-25T16:55:42Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>(This issue is about an inconsistent behaviour between running TF with and without GPU. I will update the system information when it is clear to me which system that is not behaving.</p>\n<h3>Describe the problem</h3>\n<p>I get inconsistent behaviour when I try to train a model with/without GPU. The problem can be summarized with these few lines of code:</p>\n<pre><code>    train_spec = tf.estimator.TrainSpec(\n        input_fn=lambda: make_dataset_iter(\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\n</code></pre>\n<p>With a <code>make_dataset_iter</code> function that produces 20 samples (<code>N_SAMPLES=20</code>) I will get a model that is trained for 60 global steps (data repeated 3 times) when I run locally without GPU, and 40 global steps (data repeated 2 times) when I run it in a Docker container on a GPU cluster. Not yet sure whether it is a GPU thing, or something else related to the way I run it in the cloud.</p>\n<p>Note that this also is a problem when either <code>max_steps</code> or <code>Dataset.repeats()</code> are set to the default value <code>None</code> and the other is set to something specific. This is probably the use-case where most people will experience this inconsistency.</p>\n<p>Which of the two are the expected behaviours?</p>\n<p>Full code and logs can be found below.</p>\n<h3>Source code / logs</h3>\n<p>Minimal code reproducing the issue:</p>\n<pre><code>import tensorflow as tf\n\n\nN_SAMPLES = 20\n\n\ndef lr_model_fn(features, labels, mode):\n    \"\"\"Simple logistic regression model\"\"\"\n    prediction = tf.layers.dense(features, units=1)\n    loss = tf.losses.mean_squared_error(prediction, labels)\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=prediction,\n        loss=loss,\n        train_op=tf.train.AdamOptimizer().minimize(\n            loss=loss,\n            global_step=tf.train.get_global_step(),\n        )\n    )\n\n\ndef make_dataset_iter(n_repeats=1):\n    \"\"\"Turns an instance of a generator into a Dataset\"\"\"\n    dataset = tf.data.Dataset.from_generator(\n        generator=lambda: zip(range(N_SAMPLES), range(N_SAMPLES)),\n        output_types=(tf.float32))\n    dataset = dataset.repeat(n_repeats)\n    dataset = dataset.make_one_shot_iterator()\n    dataset = dataset.get_next()\n    features, labels = dataset[0], dataset[1]\n    return tf.reshape(features, [-1, 1]), tf.reshape(labels, [-1, 1])\n\n\ndef run_experiment():\n    # Create the estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=lr_model_fn,\n    )\n    # Build train/eval specs\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=lambda: make_dataset_iter(\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=make_dataset_iter)\n    # Run training\n    tf.estimator.train_and_evaluate(\n        estimator,\n        train_spec,\n        eval_spec\n    )\n\n\nif __name__ == \"__main__\":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    run_experiment()\n\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2006677/with_gpu_log.txt\">Logs from the model running on GPU cluster</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/2006678/without_gpu_log.txt\">Logs from my local machine</a></p>", "body_text": "System information\n(This issue is about an inconsistent behaviour between running TF with and without GPU. I will update the system information when it is clear to me which system that is not behaving.\nDescribe the problem\nI get inconsistent behaviour when I try to train a model with/without GPU. The problem can be summarized with these few lines of code:\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=lambda: make_dataset_iter(\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\n\nWith a make_dataset_iter function that produces 20 samples (N_SAMPLES=20) I will get a model that is trained for 60 global steps (data repeated 3 times) when I run locally without GPU, and 40 global steps (data repeated 2 times) when I run it in a Docker container on a GPU cluster. Not yet sure whether it is a GPU thing, or something else related to the way I run it in the cloud.\nNote that this also is a problem when either max_steps or Dataset.repeats() are set to the default value None and the other is set to something specific. This is probably the use-case where most people will experience this inconsistency.\nWhich of the two are the expected behaviours?\nFull code and logs can be found below.\nSource code / logs\nMinimal code reproducing the issue:\nimport tensorflow as tf\n\n\nN_SAMPLES = 20\n\n\ndef lr_model_fn(features, labels, mode):\n    \"\"\"Simple logistic regression model\"\"\"\n    prediction = tf.layers.dense(features, units=1)\n    loss = tf.losses.mean_squared_error(prediction, labels)\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=prediction,\n        loss=loss,\n        train_op=tf.train.AdamOptimizer().minimize(\n            loss=loss,\n            global_step=tf.train.get_global_step(),\n        )\n    )\n\n\ndef make_dataset_iter(n_repeats=1):\n    \"\"\"Turns an instance of a generator into a Dataset\"\"\"\n    dataset = tf.data.Dataset.from_generator(\n        generator=lambda: zip(range(N_SAMPLES), range(N_SAMPLES)),\n        output_types=(tf.float32))\n    dataset = dataset.repeat(n_repeats)\n    dataset = dataset.make_one_shot_iterator()\n    dataset = dataset.get_next()\n    features, labels = dataset[0], dataset[1]\n    return tf.reshape(features, [-1, 1]), tf.reshape(labels, [-1, 1])\n\n\ndef run_experiment():\n    # Create the estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=lr_model_fn,\n    )\n    # Build train/eval specs\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=lambda: make_dataset_iter(\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=make_dataset_iter)\n    # Run training\n    tf.estimator.train_and_evaluate(\n        estimator,\n        train_spec,\n        eval_spec\n    )\n\n\nif __name__ == \"__main__\":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    run_experiment()\n\n\nLogs from the model running on GPU cluster\nLogs from my local machine", "body": "### System information\r\n(This issue is about an inconsistent behaviour between running TF with and without GPU. I will update the system information when it is clear to me which system that is not behaving. \r\n\r\n### Describe the problem\r\nI get inconsistent behaviour when I try to train a model with/without GPU. The problem can be summarized with these few lines of code:\r\n```\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: make_dataset_iter(\r\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\r\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\r\n```\r\nWith a `make_dataset_iter` function that produces 20 samples (`N_SAMPLES=20`) I will get a model that is trained for 60 global steps (data repeated 3 times) when I run locally without GPU, and 40 global steps (data repeated 2 times) when I run it in a Docker container on a GPU cluster. Not yet sure whether it is a GPU thing, or something else related to the way I run it in the cloud.\r\n\r\nNote that this also is a problem when either `max_steps` or `Dataset.repeats()` are set to the default value `None` and the other is set to something specific. This is probably the use-case where most people will experience this inconsistency.\r\n\r\nWhich of the two are the expected behaviours?\r\n\r\nFull code and logs can be found below.\r\n\r\n### Source code / logs\r\nMinimal code reproducing the issue:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nN_SAMPLES = 20\r\n\r\n\r\ndef lr_model_fn(features, labels, mode):\r\n    \"\"\"Simple logistic regression model\"\"\"\r\n    prediction = tf.layers.dense(features, units=1)\r\n    loss = tf.losses.mean_squared_error(prediction, labels)\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        predictions=prediction,\r\n        loss=loss,\r\n        train_op=tf.train.AdamOptimizer().minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step(),\r\n        )\r\n    )\r\n\r\n\r\ndef make_dataset_iter(n_repeats=1):\r\n    \"\"\"Turns an instance of a generator into a Dataset\"\"\"\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator=lambda: zip(range(N_SAMPLES), range(N_SAMPLES)),\r\n        output_types=(tf.float32))\r\n    dataset = dataset.repeat(n_repeats)\r\n    dataset = dataset.make_one_shot_iterator()\r\n    dataset = dataset.get_next()\r\n    features, labels = dataset[0], dataset[1]\r\n    return tf.reshape(features, [-1, 1]), tf.reshape(labels, [-1, 1])\r\n\r\n\r\ndef run_experiment():\r\n    # Create the estimator\r\n    estimator = tf.estimator.Estimator(\r\n        model_fn=lr_model_fn,\r\n    )\r\n    # Build train/eval specs\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: make_dataset_iter(\r\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\r\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=make_dataset_iter)\r\n    # Run training\r\n    tf.estimator.train_and_evaluate(\r\n        estimator,\r\n        train_spec,\r\n        eval_spec\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    run_experiment()\r\n\r\n```\r\n[Logs from the model running on GPU cluster](https://github.com/tensorflow/tensorflow/files/2006677/with_gpu_log.txt)\r\n[Logs from my local machine](https://github.com/tensorflow/tensorflow/files/2006678/without_gpu_log.txt)\r\n\r\n"}