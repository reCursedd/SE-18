{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/395916374", "html_url": "https://github.com/tensorflow/tensorflow/issues/19305#issuecomment-395916374", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19305", "id": 395916374, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTkxNjM3NA==", "user": {"login": "xiejw", "id": 1184671, "node_id": "MDQ6VXNlcjExODQ2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1184671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiejw", "html_url": "https://github.com/xiejw", "followers_url": "https://api.github.com/users/xiejw/followers", "following_url": "https://api.github.com/users/xiejw/following{/other_user}", "gists_url": "https://api.github.com/users/xiejw/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiejw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiejw/subscriptions", "organizations_url": "https://api.github.com/users/xiejw/orgs", "repos_url": "https://api.github.com/users/xiejw/repos", "events_url": "https://api.github.com/users/xiejw/events{/privacy}", "received_events_url": "https://api.github.com/users/xiejw/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-08T23:11:05Z", "updated_at": "2018-06-08T23:11:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry, I do not understand the problem here.  What's the inconsistent behavior here? The description and code are not very clear to me. For example</p>\n<ul>\n<li>\n<p>Where does the BATCH_SIZE have been used in the code? maybe I missed that part.</p>\n</li>\n<li>\n<p>I do not understand these two lines</p>\n<pre><code>      n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\n  max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\n</code></pre>\n</li>\n</ul>\n<p>But as I scanned the example code, I noticed you have a stopping condition in your input_fn, which is based on dataset repeat.</p>\n<p>tf.estimator.train_and_evaluate does not work with that in most of cases.</p>\n<p>As mentioned in the docstring (see [1]), \"Stop condition: In order to support both distributed and non-distributed configuration reliably, the only supported stop condition for model training is train_spec.max_steps. If train_spec.max_steps is None, the model is trained forever. \"</p>\n<p>It is recommended, for training, make your input_fn returns data for ever, you can do that by using repeat(), i.e., no argument to repeat. And purely using the train_spec.max_steps to control the training length.</p>\n<p>Hope this works for you.</p>\n<p>[1] <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate</a></p>", "body_text": "Sorry, I do not understand the problem here.  What's the inconsistent behavior here? The description and code are not very clear to me. For example\n\n\nWhere does the BATCH_SIZE have been used in the code? maybe I missed that part.\n\n\nI do not understand these two lines\n      n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\n  max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\n\n\n\nBut as I scanned the example code, I noticed you have a stopping condition in your input_fn, which is based on dataset repeat.\ntf.estimator.train_and_evaluate does not work with that in most of cases.\nAs mentioned in the docstring (see [1]), \"Stop condition: In order to support both distributed and non-distributed configuration reliably, the only supported stop condition for model training is train_spec.max_steps. If train_spec.max_steps is None, the model is trained forever. \"\nIt is recommended, for training, make your input_fn returns data for ever, you can do that by using repeat(), i.e., no argument to repeat. And purely using the train_spec.max_steps to control the training length.\nHope this works for you.\n[1] https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate", "body": "Sorry, I do not understand the problem here.  What's the inconsistent behavior here? The description and code are not very clear to me. For example\r\n\r\n- Where does the BATCH_SIZE have been used in the code? maybe I missed that part. \r\n- I do not understand these two lines \r\n\r\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\r\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\r\n\r\nBut as I scanned the example code, I noticed you have a stopping condition in your input_fn, which is based on dataset repeat. \r\n\r\ntf.estimator.train_and_evaluate does not work with that in most of cases.   \r\n\r\nAs mentioned in the docstring (see [1]), \"Stop condition: In order to support both distributed and non-distributed configuration reliably, the only supported stop condition for model training is train_spec.max_steps. If train_spec.max_steps is None, the model is trained forever. \" \r\n\r\nIt is recommended, for training, make your input_fn returns data for ever, you can do that by using repeat(), i.e., no argument to repeat. And purely using the train_spec.max_steps to control the training length.\r\n \r\nHope this works for you.\r\n\r\n[1] https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"}