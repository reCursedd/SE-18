{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/171", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/171/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/171/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/171/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/171", "id": 116516798, "node_id": "MDU6SXNzdWUxMTY1MTY3OTg=", "number": 171, "title": "Tensorboard creates unecessary loops in graph", "user": {"login": "jorenvs", "id": 725737, "node_id": "MDQ6VXNlcjcyNTczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/725737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorenvs", "html_url": "https://github.com/jorenvs", "followers_url": "https://api.github.com/users/jorenvs/followers", "following_url": "https://api.github.com/users/jorenvs/following{/other_user}", "gists_url": "https://api.github.com/users/jorenvs/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorenvs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorenvs/subscriptions", "organizations_url": "https://api.github.com/users/jorenvs/orgs", "repos_url": "https://api.github.com/users/jorenvs/repos", "events_url": "https://api.github.com/users/jorenvs/events{/privacy}", "received_events_url": "https://api.github.com/users/jorenvs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284285184, "node_id": "MDU6TGFiZWwyODQyODUxODQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:tensorboard", "name": "comp:tensorboard", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "decentralion", "id": 1400023, "node_id": "MDQ6VXNlcjE0MDAwMjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1400023?v=4", "gravatar_id": "", "url": "https://api.github.com/users/decentralion", "html_url": "https://github.com/decentralion", "followers_url": "https://api.github.com/users/decentralion/followers", "following_url": "https://api.github.com/users/decentralion/following{/other_user}", "gists_url": "https://api.github.com/users/decentralion/gists{/gist_id}", "starred_url": "https://api.github.com/users/decentralion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/decentralion/subscriptions", "organizations_url": "https://api.github.com/users/decentralion/orgs", "repos_url": "https://api.github.com/users/decentralion/repos", "events_url": "https://api.github.com/users/decentralion/events{/privacy}", "received_events_url": "https://api.github.com/users/decentralion/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "decentralion", "id": 1400023, "node_id": "MDQ6VXNlcjE0MDAwMjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1400023?v=4", "gravatar_id": "", "url": "https://api.github.com/users/decentralion", "html_url": "https://github.com/decentralion", "followers_url": "https://api.github.com/users/decentralion/followers", "following_url": "https://api.github.com/users/decentralion/following{/other_user}", "gists_url": "https://api.github.com/users/decentralion/gists{/gist_id}", "starred_url": "https://api.github.com/users/decentralion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/decentralion/subscriptions", "organizations_url": "https://api.github.com/users/decentralion/orgs", "repos_url": "https://api.github.com/users/decentralion/repos", "events_url": "https://api.github.com/users/decentralion/events{/privacy}", "received_events_url": "https://api.github.com/users/decentralion/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2015-11-12T10:02:37Z", "updated_at": "2016-01-08T15:32:39Z", "closed_at": "2016-01-08T15:32:39Z", "author_association": "NONE", "body_html": "<p>I've followed the mnist expert tutorial and wanted to see the graph on the tensorboard. The dashboard looks great, except that in the graph unrelated nodes are grouped together, creating weird loops and making it harder to follow what's happening. Especially with Relu_1 &amp; Relu_2, the merging of the arrows are quite confusing.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/725737/11115434/c009582e-892b-11e5-8d55-89666555b14e.png\"><img src=\"https://cloud.githubusercontent.com/assets/725737/11115434/c009582e-892b-11e5-8d55-89666555b14e.png\" alt=\"screenshot tensorboard\" style=\"max-width:100%;\"></a></p>\n<p>Code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> input_data\n\nmnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MNIST_data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">weight_variable</span>(<span class=\"pl-smi\">shape</span>):\n    initial <span class=\"pl-k\">=</span> tf.truncated_normal(shape, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n    <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bias_variable</span>(<span class=\"pl-smi\">shape</span>):\n    initial <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape)\n    <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">conv2d</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">W</span>):\n    <span class=\"pl-k\">return</span> tf.nn.conv2d(x, W, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">max_pool_2x2</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> tf.nn.max_pool(x, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>],\n                          <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\n\nx <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>])\ny_ <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>])\n\nW_conv1 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>])\nb_conv1 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">32</span>])\n\nx_image <span class=\"pl-k\">=</span> tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">1</span>])\n\nh_conv1 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(x_image, W_conv1) <span class=\"pl-k\">+</span> b_conv1)\nh_pool1 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv1)\n\nW_conv2 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>])\nb_conv2 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">64</span>])\n\nh_conv2 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(h_pool1, W_conv2) <span class=\"pl-k\">+</span> b_conv2)\nh_pool2 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv2)\n\nW_fc1 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1024</span>])\nb_fc1 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">1024</span>])\n\nh_pool2_flat <span class=\"pl-k\">=</span> tf.reshape(h_pool2, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">64</span>])\nh_fc1 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) <span class=\"pl-k\">+</span> b_fc1)\n\nkeep_prob <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>)\nh_fc1_drop <span class=\"pl-k\">=</span> tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 <span class=\"pl-k\">=</span> weight_variable([<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">10</span>])\nb_fc2 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">10</span>])\n\ny_conv <span class=\"pl-k\">=</span> tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) <span class=\"pl-k\">+</span> b_fc2)\n\ncross_entropy <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>tf.reduce_sum(y_ <span class=\"pl-k\">*</span> tf.log(y_conv))\ntrain_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-4</span>).minimize(cross_entropy)\ncorrect_prediction <span class=\"pl-k\">=</span> tf.equal(tf.argmax(y_conv, <span class=\"pl-c1\">1</span>), tf.argmax(y_, <span class=\"pl-c1\">1</span>))\naccuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>))\n\n\ntf.scalar_summary(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Training error<span class=\"pl-pds\">'</span></span>, cross_entropy)\ntf.scalar_summary(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Training accuracy<span class=\"pl-pds\">'</span></span>, accuracy)\ntf.scalar_summary(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sparsity<span class=\"pl-pds\">'</span></span>, tf.nn.zero_fraction(h_fc1))\n\nsess.run(tf.initialize_all_variables())\n\n\nmerged_summary_op <span class=\"pl-k\">=</span> tf.merge_all_summaries()\n<span class=\"pl-c1\">print</span> merged_summary_op\nsummary_writer <span class=\"pl-k\">=</span> tf.train.SummaryWriter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tmp/mnist_logs<span class=\"pl-pds\">'</span></span>, sess.graph_def)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">20000</span>):\n    batch <span class=\"pl-k\">=</span> mnist.train.next_batch(<span class=\"pl-c1\">50</span>)\n    sess.run(train_step, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>], keep_prob: <span class=\"pl-c1\">0.5</span>})\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        train_accuracy <span class=\"pl-k\">=</span> accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n            x: batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>], keep_prob: <span class=\"pl-c1\">1.0</span>})\n        <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>step <span class=\"pl-c1\">%d</span>, training accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (i, train_accuracy)\n        summary_str <span class=\"pl-k\">=</span> sess.run(merged_summary_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>], keep_prob: <span class=\"pl-c1\">0.5</span>})\n        summary_writer.add_summary(summary_str, i)\n\n\n\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class=\"pl-c1\">1.0</span>})</pre></div>", "body_text": "I've followed the mnist expert tutorial and wanted to see the graph on the tensorboard. The dashboard looks great, except that in the graph unrelated nodes are grouped together, creating weird loops and making it harder to follow what's happening. Especially with Relu_1 & Relu_2, the merging of the arrows are quite confusing.\n\nCode:\nimport input_data\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\nimport tensorflow as tf\n\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n\n\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(\"float\", shape=[None, 784])\ny_ = tf.placeholder(\"float\", shape=[None, 10])\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n\ntf.scalar_summary('Training error', cross_entropy)\ntf.scalar_summary('Training accuracy', accuracy)\ntf.scalar_summary('sparsity', tf.nn.zero_fraction(h_fc1))\n\nsess.run(tf.initialize_all_variables())\n\n\nmerged_summary_op = tf.merge_all_summaries()\nprint merged_summary_op\nsummary_writer = tf.train.SummaryWriter('/tmp/mnist_logs', sess.graph_def)\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n    sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n    if i % 100 == 0:\n        train_accuracy = accuracy.eval(feed_dict={\n            x: batch[0], y_: batch[1], keep_prob: 1.0})\n        print \"step %d, training accuracy %g\" % (i, train_accuracy)\n        summary_str = sess.run(merged_summary_op, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n        summary_writer.add_summary(summary_str, i)\n\n\n\nprint \"test accuracy %g\" % accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})", "body": "I've followed the mnist expert tutorial and wanted to see the graph on the tensorboard. The dashboard looks great, except that in the graph unrelated nodes are grouped together, creating weird loops and making it harder to follow what's happening. Especially with Relu_1 & Relu_2, the merging of the arrows are quite confusing.\n\n![screenshot tensorboard](https://cloud.githubusercontent.com/assets/725737/11115434/c009582e-892b-11e5-8d55-89666555b14e.png)\n\nCode:\n\n``` python\nimport input_data\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\nimport tensorflow as tf\n\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n\n\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(\"float\", shape=[None, 784])\ny_ = tf.placeholder(\"float\", shape=[None, 10])\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n\ntf.scalar_summary('Training error', cross_entropy)\ntf.scalar_summary('Training accuracy', accuracy)\ntf.scalar_summary('sparsity', tf.nn.zero_fraction(h_fc1))\n\nsess.run(tf.initialize_all_variables())\n\n\nmerged_summary_op = tf.merge_all_summaries()\nprint merged_summary_op\nsummary_writer = tf.train.SummaryWriter('/tmp/mnist_logs', sess.graph_def)\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n    sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n    if i % 100 == 0:\n        train_accuracy = accuracy.eval(feed_dict={\n            x: batch[0], y_: batch[1], keep_prob: 1.0})\n        print \"step %d, training accuracy %g\" % (i, train_accuracy)\n        summary_str = sess.run(merged_summary_op, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n        summary_writer.add_summary(summary_str, i)\n\n\n\nprint \"test accuracy %g\" % accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n```\n"}