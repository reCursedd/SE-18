{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/156780065", "html_url": "https://github.com/tensorflow/tensorflow/issues/225#issuecomment-156780065", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/225", "id": 156780065, "node_id": "MDEyOklzc3VlQ29tbWVudDE1Njc4MDA2NQ==", "user": {"login": "decentralion", "id": 1400023, "node_id": "MDQ6VXNlcjE0MDAwMjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1400023?v=4", "gravatar_id": "", "url": "https://api.github.com/users/decentralion", "html_url": "https://github.com/decentralion", "followers_url": "https://api.github.com/users/decentralion/followers", "following_url": "https://api.github.com/users/decentralion/following{/other_user}", "gists_url": "https://api.github.com/users/decentralion/gists{/gist_id}", "starred_url": "https://api.github.com/users/decentralion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/decentralion/subscriptions", "organizations_url": "https://api.github.com/users/decentralion/orgs", "repos_url": "https://api.github.com/users/decentralion/repos", "events_url": "https://api.github.com/users/decentralion/events{/privacy}", "received_events_url": "https://api.github.com/users/decentralion/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-15T04:51:32Z", "updated_at": "2015-11-15T04:51:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm having trouble reproducing your error, but here's a working example in which the entire mnist_softmax tutorial has been modified into interact well with TensorBoard. (I'll merge this code example into the TensorBoard tutorial to keep people from running into your issue in the future.)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>A very simple MNIST classifer, modified to display data in TensorBoard</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">See extensive documentation for the original model at</span>\n<span class=\"pl-s\">http://tensorflow.org/tutorials/mnist/beginners/index.md</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">See documentaion on the TensorBoard specific pieces at</span>\n<span class=\"pl-s\">http://tensorflow.org/how_tos/summaries_and_tensorboard/index.md</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Import data</span>\n<span class=\"pl-k\">import</span> input_data\nmnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/tmp/data/<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the model</span>\nx <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x-input<span class=\"pl-pds\">\"</span></span>)\nW <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">784</span>,<span class=\"pl-c1\">10</span>]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>)\nw_hist <span class=\"pl-k\">=</span> tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>, W)\nb <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">10</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bias<span class=\"pl-pds\">\"</span></span>))\nb_hist <span class=\"pl-k\">=</span> tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>biases<span class=\"pl-pds\">\"</span></span>, b)\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Wx_b<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> scope:\n  y <span class=\"pl-k\">=</span> tf.nn.softmax(tf.matmul(x,W) <span class=\"pl-k\">+</span> b)\ny_hist <span class=\"pl-k\">=</span> tf.histogram_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>, y)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define loss and optimizer</span>\ny_ <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">None</span>,<span class=\"pl-c1\">10</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y-input<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>xent<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> scope:\n  cross_entropy <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>tf.reduce_sum(y_<span class=\"pl-k\">*</span>tf.log(y))\n  ce_summ <span class=\"pl-k\">=</span> tf.scalar_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cross entropy<span class=\"pl-pds\">\"</span></span>, cross_entropy)\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> scope:\n  train_step <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.01</span>).minimize(cross_entropy)\n\n<span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> scope:\n  correct_prediction <span class=\"pl-k\">=</span> tf.equal(tf.argmax(y,<span class=\"pl-c1\">1</span>), tf.argmax(y_,<span class=\"pl-c1\">1</span>))\n  accuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>))\n  accuracy_summary <span class=\"pl-k\">=</span> tf.scalar_summary(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>, accuracy)\n\nmerged <span class=\"pl-k\">=</span> tf.merge_all_summaries()\nwriter <span class=\"pl-k\">=</span> tf.train.SummaryWriter(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/tmp/mnist_logs<span class=\"pl-pds\">\"</span></span>, sess.graph_def)\ntf.initialize_all_variables().run()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Test trained model</span>\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n  <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Record summary data, and the accuracy</span>\n    feed <span class=\"pl-k\">=</span> {x: mnist.test.images, y_: mnist.test.labels}\n    result <span class=\"pl-k\">=</span> sess.run([merged, accuracy], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed)\n    summary_str <span class=\"pl-k\">=</span> result[<span class=\"pl-c1\">0</span>]\n    acc <span class=\"pl-k\">=</span> result[<span class=\"pl-c1\">1</span>]\n    writer.add_summary(summary_str, i)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Accuracy at step <span class=\"pl-c1\">%s</span>: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (i, acc))\n  <span class=\"pl-k\">else</span>:\n    batch_xs, batch_ys <span class=\"pl-k\">=</span> mnist.train.next_batch(<span class=\"pl-c1\">100</span>)\n    feed <span class=\"pl-k\">=</span> {x: batch_xs, y_: batch_ys}\n    sess.run(train_step, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed)\n\n<span class=\"pl-c1\">print</span>(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))</pre></div>\n<p>If you post your full code so I can run it and repro your error, I'll try to help figure out why TensorFlow was throwing that error :)</p>", "body_text": "I'm having trouble reproducing your error, but here's a working example in which the entire mnist_softmax tutorial has been modified into interact well with TensorBoard. (I'll merge this code example into the TensorBoard tutorial to keep people from running into your issue in the future.)\n\"\"\"A very simple MNIST classifer, modified to display data in TensorBoard\n\nSee extensive documentation for the original model at\nhttp://tensorflow.org/tutorials/mnist/beginners/index.md\n\nSee documentaion on the TensorBoard specific pieces at\nhttp://tensorflow.org/how_tos/summaries_and_tensorboard/index.md\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Import data\nimport input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Create the model\nx = tf.placeholder(\"float\", [None, 784], name=\"x-input\")\nW = tf.Variable(tf.zeros([784,10]), name=\"weights\")\nw_hist = tf.histogram_summary(\"weights\", W)\nb = tf.Variable(tf.zeros([10], name=\"bias\"))\nb_hist = tf.histogram_summary(\"biases\", b)\nwith tf.name_scope(\"Wx_b\") as scope:\n  y = tf.nn.softmax(tf.matmul(x,W) + b)\ny_hist = tf.histogram_summary(\"y\", y)\n\n# Define loss and optimizer\ny_ = tf.placeholder(\"float\", [None,10], name=\"y-input\")\nwith tf.name_scope(\"xent\") as scope:\n  cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n  ce_summ = tf.scalar_summary(\"cross entropy\", cross_entropy)\nwith tf.name_scope(\"train\") as scope:\n  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\nwith tf.name_scope(\"test\") as scope:\n  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n  accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n\nmerged = tf.merge_all_summaries()\nwriter = tf.train.SummaryWriter(\"/tmp/mnist_logs\", sess.graph_def)\ntf.initialize_all_variables().run()\n\n# Test trained model\n\nfor i in range(1000):\n  if i % 10 == 0:  # Record summary data, and the accuracy\n    feed = {x: mnist.test.images, y_: mnist.test.labels}\n    result = sess.run([merged, accuracy], feed_dict=feed)\n    summary_str = result[0]\n    acc = result[1]\n    writer.add_summary(summary_str, i)\n    print(\"Accuracy at step %s: %s\" % (i, acc))\n  else:\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    feed = {x: batch_xs, y_: batch_ys}\n    sess.run(train_step, feed_dict=feed)\n\nprint(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\nIf you post your full code so I can run it and repro your error, I'll try to help figure out why TensorFlow was throwing that error :)", "body": "I'm having trouble reproducing your error, but here's a working example in which the entire mnist_softmax tutorial has been modified into interact well with TensorBoard. (I'll merge this code example into the TensorBoard tutorial to keep people from running into your issue in the future.)\n\n``` python\n\"\"\"A very simple MNIST classifer, modified to display data in TensorBoard\n\nSee extensive documentation for the original model at\nhttp://tensorflow.org/tutorials/mnist/beginners/index.md\n\nSee documentaion on the TensorBoard specific pieces at\nhttp://tensorflow.org/how_tos/summaries_and_tensorboard/index.md\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Import data\nimport input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Create the model\nx = tf.placeholder(\"float\", [None, 784], name=\"x-input\")\nW = tf.Variable(tf.zeros([784,10]), name=\"weights\")\nw_hist = tf.histogram_summary(\"weights\", W)\nb = tf.Variable(tf.zeros([10], name=\"bias\"))\nb_hist = tf.histogram_summary(\"biases\", b)\nwith tf.name_scope(\"Wx_b\") as scope:\n  y = tf.nn.softmax(tf.matmul(x,W) + b)\ny_hist = tf.histogram_summary(\"y\", y)\n\n# Define loss and optimizer\ny_ = tf.placeholder(\"float\", [None,10], name=\"y-input\")\nwith tf.name_scope(\"xent\") as scope:\n  cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n  ce_summ = tf.scalar_summary(\"cross entropy\", cross_entropy)\nwith tf.name_scope(\"train\") as scope:\n  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\nwith tf.name_scope(\"test\") as scope:\n  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n  accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n\nmerged = tf.merge_all_summaries()\nwriter = tf.train.SummaryWriter(\"/tmp/mnist_logs\", sess.graph_def)\ntf.initialize_all_variables().run()\n\n# Test trained model\n\nfor i in range(1000):\n  if i % 10 == 0:  # Record summary data, and the accuracy\n    feed = {x: mnist.test.images, y_: mnist.test.labels}\n    result = sess.run([merged, accuracy], feed_dict=feed)\n    summary_str = result[0]\n    acc = result[1]\n    writer.add_summary(summary_str, i)\n    print(\"Accuracy at step %s: %s\" % (i, acc))\n  else:\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    feed = {x: batch_xs, y_: batch_ys}\n    sess.run(train_step, feed_dict=feed)\n\nprint(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\n```\n\nIf you post your full code so I can run it and repro your error, I'll try to help figure out why TensorFlow was throwing that error :)\n"}