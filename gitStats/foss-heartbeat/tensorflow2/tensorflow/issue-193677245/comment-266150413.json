{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266150413", "html_url": "https://github.com/tensorflow/tensorflow/issues/6111#issuecomment-266150413", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6111", "id": 266150413, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjE1MDQxMw==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-09T23:22:41Z", "updated_at": "2016-12-09T23:22:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Could you post the entire log somewhere? We need to find out how much memory were requested when OOM happens? Also we would need a small model to reproduce the problem.</p>\n<p>allow_growth is expected to introduce some level of fragmentation. So if your model is running very close to GPU memory limit, it is possible that allow_growth can push you over the limit.</p>\n<p>The current allocator design is a trade-off between performance and efficiency, and sure it is not perfect. That's why the option exists. It will get better as we keep improving it. If the system still has a lot of free memory, and the OOM still happens, then it is a more serious problem than if the model is already running close to the limit, where we would recommend not to use allow_growth.</p>", "body_text": "Could you post the entire log somewhere? We need to find out how much memory were requested when OOM happens? Also we would need a small model to reproduce the problem.\nallow_growth is expected to introduce some level of fragmentation. So if your model is running very close to GPU memory limit, it is possible that allow_growth can push you over the limit.\nThe current allocator design is a trade-off between performance and efficiency, and sure it is not perfect. That's why the option exists. It will get better as we keep improving it. If the system still has a lot of free memory, and the OOM still happens, then it is a more serious problem than if the model is already running close to the limit, where we would recommend not to use allow_growth.", "body": "Could you post the entire log somewhere? We need to find out how much memory were requested when OOM happens? Also we would need a small model to reproduce the problem. \r\n\r\nallow_growth is expected to introduce some level of fragmentation. So if your model is running very close to GPU memory limit, it is possible that allow_growth can push you over the limit.\r\n\r\nThe current allocator design is a trade-off between performance and efficiency, and sure it is not perfect. That's why the option exists. It will get better as we keep improving it. If the system still has a lot of free memory, and the OOM still happens, then it is a more serious problem than if the model is already running close to the limit, where we would recommend not to use allow_growth. \r\n"}