{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6111", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6111/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6111/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6111/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6111", "id": 193677245, "node_id": "MDU6SXNzdWUxOTM2NzcyNDU=", "number": 6111, "title": "Flawed memory management: allow_growth=True consumes more memory, causing out-of-memory", "user": {"login": "xuancong84", "id": 10172392, "node_id": "MDQ6VXNlcjEwMTcyMzky", "avatar_url": "https://avatars0.githubusercontent.com/u/10172392?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuancong84", "html_url": "https://github.com/xuancong84", "followers_url": "https://api.github.com/users/xuancong84/followers", "following_url": "https://api.github.com/users/xuancong84/following{/other_user}", "gists_url": "https://api.github.com/users/xuancong84/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuancong84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuancong84/subscriptions", "organizations_url": "https://api.github.com/users/xuancong84/orgs", "repos_url": "https://api.github.com/users/xuancong84/repos", "events_url": "https://api.github.com/users/xuancong84/events{/privacy}", "received_events_url": "https://api.github.com/users/xuancong84/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-12-06T03:56:26Z", "updated_at": "2018-10-05T11:12:40Z", "closed_at": "2017-06-16T23:41:48Z", "author_association": "NONE", "body_html": "<p>To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:</p>\n<pre><code>config = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n</code></pre>\n<p>However, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:</p>\n<pre><code>Training Epoch 0 ; learning_rate= 0.002 :                                                                                                \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \n</code></pre>\n<p>However, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.</p>\n<p>In principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.</p>\n<p>Below are my system info:</p>\n<pre><code>Operating System:\nUbuntu 14.04.5 LTS\n\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda-8.0\ncudnn-8.0-linux-x64-v5.1.tgz\n\nIt is installed from binary pip package\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0\n</code></pre>", "body_text": "To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\nHowever, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:\nTraining Epoch 0 ; learning_rate= 0.002 :                                                                                                \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \n\nHowever, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.\nIn principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.\nBelow are my system info:\nOperating System:\nUbuntu 14.04.5 LTS\n\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda-8.0\ncudnn-8.0-linux-x64-v5.1.tgz\n\nIt is installed from binary pip package\n\nA link to the pip package you installed:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0", "body": "To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```\r\n\r\nHowever, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:\r\n\r\n```\r\nTraining Epoch 0 ; learning_rate= 0.002 :                                                                                                \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\n```\r\nHowever, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.\r\n\r\nIn principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.\r\n\r\n\r\nBelow are my system info:\r\n```\r\nOperating System:\r\nUbuntu 14.04.5 LTS\r\n\r\nInstalled version of CUDA and cuDNN:\r\n/usr/local/cuda-8.0\r\ncudnn-8.0-linux-x64-v5.1.tgz\r\n\r\nIt is installed from binary pip package\r\n\r\nA link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\r\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0\r\n```\r\n"}