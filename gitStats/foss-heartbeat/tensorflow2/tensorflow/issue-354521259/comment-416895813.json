{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416895813", "html_url": "https://github.com/tensorflow/tensorflow/issues/21915#issuecomment-416895813", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21915", "id": 416895813, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjg5NTgxMw==", "user": {"login": "lukmanr", "id": 1057193, "node_id": "MDQ6VXNlcjEwNTcxOTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1057193?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukmanr", "html_url": "https://github.com/lukmanr", "followers_url": "https://api.github.com/users/lukmanr/followers", "following_url": "https://api.github.com/users/lukmanr/following{/other_user}", "gists_url": "https://api.github.com/users/lukmanr/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukmanr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukmanr/subscriptions", "organizations_url": "https://api.github.com/users/lukmanr/orgs", "repos_url": "https://api.github.com/users/lukmanr/repos", "events_url": "https://api.github.com/users/lukmanr/events{/privacy}", "received_events_url": "https://api.github.com/users/lukmanr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-29T09:55:10Z", "updated_at": "2018-08-29T09:55:10Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20493418\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hjmus\">@hjmus</a>,</p>\n<p>Thanks for the references on ordering; I'll comment below.  The bigger issue for the implementation in canned DNNs is probably the lack of use of the mean/variance update ops.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19293677\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ispirmustafa\">@ispirmustafa</a> I don't see any change to train_ops related to BN in the heads.  I also don't see it in the original commit for the feature. You have to add the mean/variance update ops to the train_op like so:</p>\n<pre><code>   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n   with tf.control_dependencies(update_ops):\n          train_op = optimizer.minimize(loss)\n</code></pre>\n<p>Otherwise the mean/variance vars in BN layers never get updated and BN is not really in effect.  This is really easy to miss in the current implementation of layers.batch_normalization, and there have been multiple requests to address this, by adding warnings or make the update ops the default behavior:  <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345587046\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/21229\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/21229/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/21229\">#21229</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"350273360\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/21595\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/21595/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/21595\">#21595</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"276194025\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/14809\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/14809/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/14809\">#14809</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"275133489\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/14699\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/14699/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/14699\">#14699</a>.  It looks like the Canned DNN implementation missed this as well.  I incuded a suggested fix in the PR.</p>\n<p>Re: ordering, I have only previously implemented BN applied before relu.  I find the discussion online confounds multiple issues, including the best explanation for what BN is really doing, with evolving best practices for CNNs.  Worth noting that we are talking about DNNs not CNNs here.  I did try a side-by-side comparison of a custom DNN, with BN before relu (and update ops properly added) against the same architecture in canned DNNs on <a href=\"https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data\" rel=\"nofollow\">this Kaggle data set</a> and got terrible results with the canned DNNs, even after hypertuning, which lead me to investigate BN in canned DNNs.  I can't publish the code at the moment as it has yet to go through Google's open sourcing process.</p>", "body_text": "Hi @hjmus,\nThanks for the references on ordering; I'll comment below.  The bigger issue for the implementation in canned DNNs is probably the lack of use of the mean/variance update ops.\n@ispirmustafa I don't see any change to train_ops related to BN in the heads.  I also don't see it in the original commit for the feature. You have to add the mean/variance update ops to the train_op like so:\n   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n   with tf.control_dependencies(update_ops):\n          train_op = optimizer.minimize(loss)\n\nOtherwise the mean/variance vars in BN layers never get updated and BN is not really in effect.  This is really easy to miss in the current implementation of layers.batch_normalization, and there have been multiple requests to address this, by adding warnings or make the update ops the default behavior:  #21229, #21595, #14809, #14699.  It looks like the Canned DNN implementation missed this as well.  I incuded a suggested fix in the PR.\nRe: ordering, I have only previously implemented BN applied before relu.  I find the discussion online confounds multiple issues, including the best explanation for what BN is really doing, with evolving best practices for CNNs.  Worth noting that we are talking about DNNs not CNNs here.  I did try a side-by-side comparison of a custom DNN, with BN before relu (and update ops properly added) against the same architecture in canned DNNs on this Kaggle data set and got terrible results with the canned DNNs, even after hypertuning, which lead me to investigate BN in canned DNNs.  I can't publish the code at the moment as it has yet to go through Google's open sourcing process.", "body": "Hi @hjmus,\r\n\r\nThanks for the references on ordering; I'll comment below.  The bigger issue for the implementation in canned DNNs is probably the lack of use of the mean/variance update ops.  \r\n\r\n@ispirmustafa I don't see any change to train_ops related to BN in the heads.  I also don't see it in the original commit for the feature. You have to add the mean/variance update ops to the train_op like so:\r\n\r\n       update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n       with tf.control_dependencies(update_ops):\r\n              train_op = optimizer.minimize(loss)\r\n\r\nOtherwise the mean/variance vars in BN layers never get updated and BN is not really in effect.  This is really easy to miss in the current implementation of layers.batch_normalization, and there have been multiple requests to address this, by adding warnings or make the update ops the default behavior:  #21229, #21595, #14809, #14699.  It looks like the Canned DNN implementation missed this as well.  I incuded a suggested fix in the PR.\r\n\r\nRe: ordering, I have only previously implemented BN applied before relu.  I find the discussion online confounds multiple issues, including the best explanation for what BN is really doing, with evolving best practices for CNNs.  Worth noting that we are talking about DNNs not CNNs here.  I did try a side-by-side comparison of a custom DNN, with BN before relu (and update ops properly added) against the same architecture in canned DNNs on [this Kaggle data set](https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data) and got terrible results with the canned DNNs, even after hypertuning, which lead me to investigate BN in canned DNNs.  I can't publish the code at the moment as it has yet to go through Google's open sourcing process. "}