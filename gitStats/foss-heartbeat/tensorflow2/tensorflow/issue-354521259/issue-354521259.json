{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21915", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21915/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21915/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21915/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21915", "id": 354521259, "node_id": "MDU6SXNzdWUzNTQ1MjEyNTk=", "number": 21915, "title": "Batch normalization implemented incorrectly in canned DNN estimators", "user": {"login": "lukmanr", "id": 1057193, "node_id": "MDQ6VXNlcjEwNTcxOTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1057193?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukmanr", "html_url": "https://github.com/lukmanr", "followers_url": "https://api.github.com/users/lukmanr/followers", "following_url": "https://api.github.com/users/lukmanr/following{/other_user}", "gists_url": "https://api.github.com/users/lukmanr/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukmanr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukmanr/subscriptions", "organizations_url": "https://api.github.com/users/lukmanr/orgs", "repos_url": "https://api.github.com/users/lukmanr/repos", "events_url": "https://api.github.com/users/lukmanr/events{/privacy}", "received_events_url": "https://api.github.com/users/lukmanr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-08-28T00:04:36Z", "updated_at": "2018-11-19T23:26:54Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: all</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10</li>\n<li><strong>Python version</strong>: 3.6.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Batch normalization is implemented incorrectly in canned DNN estimators.  In _dnn_logit_fn_builder, batch normalization is applied after the dense layer which includes the activation function.  But batch normalization should be applied to the inputs to a layer, prior to the activation function.  See the original paper by Ioffe, Szegedy: \" ... we focus on transforms that consist of an affine transformation followed by anelement-wise nonlinearity:<br>\nz = g(Wu + b)<br>\nwhere W and b are learned parameters of the model, and g(\u00b7) is the nonlinearity such as sigmoid or ReLU. ...  We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+ b.\"</p>\n<p>Also see the many published reference implementations of batch normalization.</p>\n<p>The current implementation is not only incorrect but produces very poor results.</p>\n<h3>Source code / logs</h3>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): all\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.10\nPython version: 3.6.2\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nBatch normalization is implemented incorrectly in canned DNN estimators.  In _dnn_logit_fn_builder, batch normalization is applied after the dense layer which includes the activation function.  But batch normalization should be applied to the inputs to a layer, prior to the activation function.  See the original paper by Ioffe, Szegedy: \" ... we focus on transforms that consist of an affine transformation followed by anelement-wise nonlinearity:\nz = g(Wu + b)\nwhere W and b are learned parameters of the model, and g(\u00b7) is the nonlinearity such as sigmoid or ReLU. ...  We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+ b.\"\nAlso see the many published reference implementations of batch normalization.\nThe current implementation is not only incorrect but produces very poor results.\nSource code / logs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: all\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nBatch normalization is implemented incorrectly in canned DNN estimators.  In _dnn_logit_fn_builder, batch normalization is applied after the dense layer which includes the activation function.  But batch normalization should be applied to the inputs to a layer, prior to the activation function.  See the original paper by Ioffe, Szegedy: \" ... we focus on transforms that consist of an affine transformation followed by anelement-wise nonlinearity:\r\nz = g(Wu + b)\r\nwhere W and b are learned parameters of the model, and g(\u00b7) is the nonlinearity such as sigmoid or ReLU. ...  We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+ b.\"\r\n\r\nAlso see the many published reference implementations of batch normalization.\r\n\r\nThe current implementation is not only incorrect but produces very poor results.  \r\n\r\n### Source code / logs\r\n"}