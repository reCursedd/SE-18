{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/171829353", "html_url": "https://github.com/tensorflow/tensorflow/issues/640#issuecomment-171829353", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640", "id": 171829353, "node_id": "MDEyOklzc3VlQ29tbWVudDE3MTgyOTM1Mw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-15T00:34:38Z", "updated_at": "2016-01-15T00:34:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I vote we get rid of OutputProjectionWrapper completely.  This is the kind<br>\nof problem that it contributes to.  It's also much slower than doing a<br>\nprojection on the whole output.</p>\n<p>On Thu, Dec 31, 2015 at 3:27 AM, Lukasz Kaiser <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>Thanks for spotting the bug - you're right that the code uses the whole<br>\nmulti-layer state to compute the attention query while in the paper it was<br>\nonly the output! It should be corrected, but I'm not sure if your solution<br>\nwill work well if the cell is already wrapped in an<br>\nOutputProjectionWrapper. I mean - it will work technically, but it will<br>\nbasically first apply the cell to the end (to get a vector of size<br>\ncell.output_size) and then it'll project it back to just output_size (we<br>\nshould change these names too, I'm sorry for the poor choice of variable<br>\nnames in this function).</p>\n<p>I see a few possible solutions. The cleanest might be to forbid the use of<br>\nattention_decoder without output_projection - in fact most use-cases<br>\nalready specify the output projection separately. But, since this is a<br>\nbreaking change, maybe we should start as you suggest. I'll be happy to<br>\nreview a code if you want to write it, of course!</p>\n<p>\u2014<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"124112990\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/640\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/640/hovercard?comment_id=168177312&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/640#issuecomment-168177312\">#640 (comment)</a><br>\n.</p>\n</blockquote>", "body_text": "I vote we get rid of OutputProjectionWrapper completely.  This is the kind\nof problem that it contributes to.  It's also much slower than doing a\nprojection on the whole output.\nOn Thu, Dec 31, 2015 at 3:27 AM, Lukasz Kaiser notifications@github.com\nwrote:\n\nThanks for spotting the bug - you're right that the code uses the whole\nmulti-layer state to compute the attention query while in the paper it was\nonly the output! It should be corrected, but I'm not sure if your solution\nwill work well if the cell is already wrapped in an\nOutputProjectionWrapper. I mean - it will work technically, but it will\nbasically first apply the cell to the end (to get a vector of size\ncell.output_size) and then it'll project it back to just output_size (we\nshould change these names too, I'm sorry for the poor choice of variable\nnames in this function).\nI see a few possible solutions. The cleanest might be to forbid the use of\nattention_decoder without output_projection - in fact most use-cases\nalready specify the output projection separately. But, since this is a\nbreaking change, maybe we should start as you suggest. I'll be happy to\nreview a code if you want to write it, of course!\n\u2014\nReply to this email directly or view it on GitHub\n#640 (comment)\n.", "body": "I vote we get rid of OutputProjectionWrapper completely.  This is the kind\nof problem that it contributes to.  It's also much slower than doing a\nprojection on the whole output.\n\nOn Thu, Dec 31, 2015 at 3:27 AM, Lukasz Kaiser notifications@github.com\nwrote:\n\n> Thanks for spotting the bug - you're right that the code uses the whole\n> multi-layer state to compute the attention query while in the paper it was\n> only the output! It should be corrected, but I'm not sure if your solution\n> will work well if the cell is already wrapped in an\n> OutputProjectionWrapper. I mean - it will work technically, but it will\n> basically first apply the cell to the end (to get a vector of size\n> cell.output_size) and then it'll project it back to just output_size (we\n> should change these names too, I'm sorry for the poor choice of variable\n> names in this function).\n> \n> I see a few possible solutions. The cleanest might be to forbid the use of\n> attention_decoder without output_projection - in fact most use-cases\n> already specify the output projection separately. But, since this is a\n> breaking change, maybe we should start as you suggest. I'll be happy to\n> review a code if you want to write it, of course!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/640#issuecomment-168177312\n> .\n"}