{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/168177312", "html_url": "https://github.com/tensorflow/tensorflow/issues/640#issuecomment-168177312", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640", "id": 168177312, "node_id": "MDEyOklzc3VlQ29tbWVudDE2ODE3NzMxMg==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-31T11:27:23Z", "updated_at": "2015-12-31T11:27:23Z", "author_association": "MEMBER", "body_html": "<p>Thanks for spotting the bug - you're right that the code uses the whole multi-layer state to compute the attention query while in the paper it was only the output! It should be corrected, but I'm not sure if your solution will work well if the cell is already wrapped in an OutputProjectionWrapper. I mean - it will work technically, but it will basically first apply the cell to the end (to get a vector of size cell.output_size) and then it'll project it back to just output_size (we should change these names too, I'm sorry for the poor choice of variable names in this function).</p>\n<p>I see a few possible solutions. The cleanest might be to forbid the use of attention_decoder without output_projection - in fact most use-cases already specify the output projection separately. But, since this is a breaking change, maybe we should start as you suggest. I'll be happy to review a code if you want to write it, of course!</p>", "body_text": "Thanks for spotting the bug - you're right that the code uses the whole multi-layer state to compute the attention query while in the paper it was only the output! It should be corrected, but I'm not sure if your solution will work well if the cell is already wrapped in an OutputProjectionWrapper. I mean - it will work technically, but it will basically first apply the cell to the end (to get a vector of size cell.output_size) and then it'll project it back to just output_size (we should change these names too, I'm sorry for the poor choice of variable names in this function).\nI see a few possible solutions. The cleanest might be to forbid the use of attention_decoder without output_projection - in fact most use-cases already specify the output projection separately. But, since this is a breaking change, maybe we should start as you suggest. I'll be happy to review a code if you want to write it, of course!", "body": "Thanks for spotting the bug - you're right that the code uses the whole multi-layer state to compute the attention query while in the paper it was only the output! It should be corrected, but I'm not sure if your solution will work well if the cell is already wrapped in an OutputProjectionWrapper. I mean - it will work technically, but it will basically first apply the cell to the end (to get a vector of size cell.output_size) and then it'll project it back to just output_size (we should change these names too, I'm sorry for the poor choice of variable names in this function).\n\nI see a few possible solutions. The cleanest might be to forbid the use of attention_decoder without output_projection - in fact most use-cases already specify the output projection separately. But, since this is a breaking change, maybe we should start as you suggest. I'll be happy to review a code if you want to write it, of course!\n"}