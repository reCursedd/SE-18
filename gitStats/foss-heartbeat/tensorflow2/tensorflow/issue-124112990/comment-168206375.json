{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/168206375", "html_url": "https://github.com/tensorflow/tensorflow/issues/640#issuecomment-168206375", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640", "id": 168206375, "node_id": "MDEyOklzc3VlQ29tbWVudDE2ODIwNjM3NQ==", "user": {"login": "PrajitR", "id": 4674442, "node_id": "MDQ6VXNlcjQ2NzQ0NDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4674442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PrajitR", "html_url": "https://github.com/PrajitR", "followers_url": "https://api.github.com/users/PrajitR/followers", "following_url": "https://api.github.com/users/PrajitR/following{/other_user}", "gists_url": "https://api.github.com/users/PrajitR/gists{/gist_id}", "starred_url": "https://api.github.com/users/PrajitR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PrajitR/subscriptions", "organizations_url": "https://api.github.com/users/PrajitR/orgs", "repos_url": "https://api.github.com/users/PrajitR/repos", "events_url": "https://api.github.com/users/PrajitR/events{/privacy}", "received_events_url": "https://api.github.com/users/PrajitR/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-31T14:48:35Z", "updated_at": "2015-12-31T14:48:35Z", "author_association": "NONE", "body_html": "<p>You're totally right about the case when the cell is already wrapped in an OutputProjectionWrapper. However, in this case also, we should remove the line where the cell is again wrapped in OutputProjectionWrapper -- the hidden state would be projected to cell.output_size (which probably equals num_decoder_symbols), and then reprojected to num_decoder_symbols.</p>\n<p>Since forbidding attention_decoder without output_projection would be a breaking change, one thing we can do is to check if <code>isinstance(cell, OutputProjectionWrapper)</code> and print a warning telling the user to specify output_projection instead. Then in future versions of TF, a <code>ValueError</code> can be returned in this case and output_projection will not have a default value.</p>", "body_text": "You're totally right about the case when the cell is already wrapped in an OutputProjectionWrapper. However, in this case also, we should remove the line where the cell is again wrapped in OutputProjectionWrapper -- the hidden state would be projected to cell.output_size (which probably equals num_decoder_symbols), and then reprojected to num_decoder_symbols.\nSince forbidding attention_decoder without output_projection would be a breaking change, one thing we can do is to check if isinstance(cell, OutputProjectionWrapper) and print a warning telling the user to specify output_projection instead. Then in future versions of TF, a ValueError can be returned in this case and output_projection will not have a default value.", "body": "You're totally right about the case when the cell is already wrapped in an OutputProjectionWrapper. However, in this case also, we should remove the line where the cell is again wrapped in OutputProjectionWrapper -- the hidden state would be projected to cell.output_size (which probably equals num_decoder_symbols), and then reprojected to num_decoder_symbols.\n\nSince forbidding attention_decoder without output_projection would be a breaking change, one thing we can do is to check if `isinstance(cell, OutputProjectionWrapper)` and print a warning telling the user to specify output_projection instead. Then in future versions of TF, a `ValueError` can be returned in this case and output_projection will not have a default value.\n"}