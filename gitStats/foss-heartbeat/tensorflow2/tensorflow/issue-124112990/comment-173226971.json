{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/173226971", "html_url": "https://github.com/tensorflow/tensorflow/issues/640#issuecomment-173226971", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640", "id": 173226971, "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzIyNjk3MQ==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-20T14:52:58Z", "updated_at": "2016-01-26T19:49:58Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>(1) I'm not sure that this issue actually makes the models worse. I'll run experiments next week to check it -- it might well be. But it's not clear - does anyone have numbers to support it? I'll try to get some numbers next week.</p>\n</blockquote>\n<p>I don't have numbers for translation, but I will say that I have used this seq2seq example pretty heavily with the attention on all hidden states from each layer. I would say with 300k steps plus I get pretty acceptable results.</p>\n<p>Also, maybe I'm missing something here, but can't we just slice the hidden state before we input it into the attention mechanism? If we are only interested in the hidden state output of the last layer, why don't we just slice that part from the total hidden state?</p>\n<p>Just as an update I did:</p>\n<div class=\"highlight highlight-source-python\"><pre>cell_output, new_state <span class=\"pl-k\">=</span> cell(x, states[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>I need to do some thinking on begin and size args</span>\ninput_state_attn <span class=\"pl-k\">=</span> tf.slice(new_state, [<span class=\"pl-c1\">0</span>, cell.output_size<span class=\"pl-k\">*</span>(num_dec_layers<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, cell.output_size])\n\nattns <span class=\"pl-k\">=</span> attention(input_state_attn)\n\n<span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>AttnOutputProjection<span class=\"pl-pds\">\"</span></span>):\n  output <span class=\"pl-k\">=</span> rnn_cell.linear([cell_output] <span class=\"pl-k\">+</span> attns, output_size, <span class=\"pl-c1\">True</span>)</pre></div>\n<p>However, when I run it, the model blows up and I get inf perplexity after 200 steps, so clearly I'm doing something wrong. I turned down the learning rate by 10x and it still exhibits the same behavior.</p>\n<p>I looked into the output projection wrapper and it doesn't seem to modify the hidden state argument that is passed into it at all. Therefore, I think you are okay passing a slice of the hidden state into attention, regardless if you're using an output projection or not.</p>\n<p>If anyone can explain the cause of the blowup in this approach, would be very much appreciated.</p>", "body_text": "(1) I'm not sure that this issue actually makes the models worse. I'll run experiments next week to check it -- it might well be. But it's not clear - does anyone have numbers to support it? I'll try to get some numbers next week.\n\nI don't have numbers for translation, but I will say that I have used this seq2seq example pretty heavily with the attention on all hidden states from each layer. I would say with 300k steps plus I get pretty acceptable results.\nAlso, maybe I'm missing something here, but can't we just slice the hidden state before we input it into the attention mechanism? If we are only interested in the hidden state output of the last layer, why don't we just slice that part from the total hidden state?\nJust as an update I did:\ncell_output, new_state = cell(x, states[-1])\n\n#I need to do some thinking on begin and size args\ninput_state_attn = tf.slice(new_state, [0, cell.output_size*(num_dec_layers-1)], [-1, cell.output_size])\n\nattns = attention(input_state_attn)\n\nwith vs.variable_scope(\"AttnOutputProjection\"):\n  output = rnn_cell.linear([cell_output] + attns, output_size, True)\nHowever, when I run it, the model blows up and I get inf perplexity after 200 steps, so clearly I'm doing something wrong. I turned down the learning rate by 10x and it still exhibits the same behavior.\nI looked into the output projection wrapper and it doesn't seem to modify the hidden state argument that is passed into it at all. Therefore, I think you are okay passing a slice of the hidden state into attention, regardless if you're using an output projection or not.\nIf anyone can explain the cause of the blowup in this approach, would be very much appreciated.", "body": "> (1) I'm not sure that this issue actually makes the models worse. I'll run experiments next week to check it -- it might well be. But it's not clear - does anyone have numbers to support it? I'll try to get some numbers next week.\n\nI don't have numbers for translation, but I will say that I have used this seq2seq example pretty heavily with the attention on all hidden states from each layer. I would say with 300k steps plus I get pretty acceptable results.\n\nAlso, maybe I'm missing something here, but can't we just slice the hidden state before we input it into the attention mechanism? If we are only interested in the hidden state output of the last layer, why don't we just slice that part from the total hidden state?\n\nJust as an update I did:\n\n``` python\ncell_output, new_state = cell(x, states[-1])\n\n#I need to do some thinking on begin and size args\ninput_state_attn = tf.slice(new_state, [0, cell.output_size*(num_dec_layers-1)], [-1, cell.output_size])\n\nattns = attention(input_state_attn)\n\nwith vs.variable_scope(\"AttnOutputProjection\"):\n  output = rnn_cell.linear([cell_output] + attns, output_size, True)\n```\n\nHowever, when I run it, the model blows up and I get inf perplexity after 200 steps, so clearly I'm doing something wrong. I turned down the learning rate by 10x and it still exhibits the same behavior. \n\nI looked into the output projection wrapper and it doesn't seem to modify the hidden state argument that is passed into it at all. Therefore, I think you are okay passing a slice of the hidden state into attention, regardless if you're using an output projection or not. \n\nIf anyone can explain the cause of the blowup in this approach, would be very much appreciated.\n"}