{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/640/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/640", "id": 124112990, "node_id": "MDU6SXNzdWUxMjQxMTI5OTA=", "number": 640, "title": "Attention modifications", "user": {"login": "PrajitR", "id": 4674442, "node_id": "MDQ6VXNlcjQ2NzQ0NDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4674442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PrajitR", "html_url": "https://github.com/PrajitR", "followers_url": "https://api.github.com/users/PrajitR/followers", "following_url": "https://api.github.com/users/PrajitR/following{/other_user}", "gists_url": "https://api.github.com/users/PrajitR/gists{/gist_id}", "starred_url": "https://api.github.com/users/PrajitR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PrajitR/subscriptions", "organizations_url": "https://api.github.com/users/PrajitR/orgs", "repos_url": "https://api.github.com/users/PrajitR/repos", "events_url": "https://api.github.com/users/PrajitR/events{/privacy}", "received_events_url": "https://api.github.com/users/PrajitR/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2015-12-28T19:39:57Z", "updated_at": "2016-08-04T09:18:21Z", "closed_at": "2016-06-10T22:01:54Z", "author_association": "NONE", "body_html": "<p>In the <code>attention_decoder</code> of <code>seq2seq.py</code>, attention is computed by the statements</p>\n<div class=\"highlight highlight-source-python\"><pre>cell_output, new_state <span class=\"pl-k\">=</span> cell(x, states[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\nattns <span class=\"pl-k\">=</span> attention(new_state)\n<span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>AttnOutputProjection<span class=\"pl-pds\">\"</span></span>):\n  output <span class=\"pl-k\">=</span> rnn_cell.linear([cell_output] <span class=\"pl-k\">+</span> attns, output_size, <span class=\"pl-c1\">True</span>)</pre></div>\n<p>However, in Grammar as a Foreign Language, attention is computed using the last hidden vector of the multi-layer LSTM (e.g. <code>cell_output</code>). Computing attention with <code>new_state</code> involves using every layer's state, including their cell state (which should probably be only used inside the LSTM layer). To match GaaFL, the attention call should be</p>\n<div class=\"highlight highlight-source-python\"><pre>attns <span class=\"pl-k\">=</span> attention(cell_output)</pre></div>\n<p>Of course, if the <code>cell</code> is wrapped by an <code>OutputProjectionWrapper</code> as done in <code>embedding_attention_seq2seq</code>, then this call is computationally expensive. To prevent this, <code>embedding_attention_seq2seq</code> can be modified as such:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Starting from line 606.</span>\noutput_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n<span class=\"pl-k\">if</span> output_projection <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n  output_size <span class=\"pl-k\">=</span> num_decoder_symbols <span class=\"pl-c\"><span class=\"pl-c\">#</span> Projection wrapper removed.</span></pre></div>\n<p>Now, <code>cell_output</code> will be the last hidden vector of the multi-layered LSTM. The reason this works is because <code>output</code> is already being projected for the softmax, so <code>cell_output</code> does not need to be projected to a length of <code>num_decoder_symbols</code>. If as before<code>cell</code> is wrapped by <code>OutputProjectionWrapper</code>, the <code>cell_output</code> will have size <code>num_decoder_symbols</code>. Computing the <code>output</code> will then involve a <code>(num_decoder_symbols + num_heads * attn_size) x num_decoder_symbols</code> matrix, which can be huge. However, if we make the change, then the matrix is only of size <code>(cell.output_size + num_heads * attn_size) x num_decoder_symbols</code>.</p>\n<p>Thoughts? Have I missed something important? If this sounds good, I can make a PR. Thanks!</p>", "body_text": "In the attention_decoder of seq2seq.py, attention is computed by the statements\ncell_output, new_state = cell(x, states[-1])\nattns = attention(new_state)\nwith vs.variable_scope(\"AttnOutputProjection\"):\n  output = rnn_cell.linear([cell_output] + attns, output_size, True)\nHowever, in Grammar as a Foreign Language, attention is computed using the last hidden vector of the multi-layer LSTM (e.g. cell_output). Computing attention with new_state involves using every layer's state, including their cell state (which should probably be only used inside the LSTM layer). To match GaaFL, the attention call should be\nattns = attention(cell_output)\nOf course, if the cell is wrapped by an OutputProjectionWrapper as done in embedding_attention_seq2seq, then this call is computationally expensive. To prevent this, embedding_attention_seq2seq can be modified as such:\n# Starting from line 606.\noutput_size = None\nif output_projection is None:\n  output_size = num_decoder_symbols # Projection wrapper removed.\nNow, cell_output will be the last hidden vector of the multi-layered LSTM. The reason this works is because output is already being projected for the softmax, so cell_output does not need to be projected to a length of num_decoder_symbols. If as beforecell is wrapped by OutputProjectionWrapper, the cell_output will have size num_decoder_symbols. Computing the output will then involve a (num_decoder_symbols + num_heads * attn_size) x num_decoder_symbols matrix, which can be huge. However, if we make the change, then the matrix is only of size (cell.output_size + num_heads * attn_size) x num_decoder_symbols.\nThoughts? Have I missed something important? If this sounds good, I can make a PR. Thanks!", "body": "In the `attention_decoder` of `seq2seq.py`, attention is computed by the statements \n\n``` python\ncell_output, new_state = cell(x, states[-1])\nattns = attention(new_state)\nwith vs.variable_scope(\"AttnOutputProjection\"):\n  output = rnn_cell.linear([cell_output] + attns, output_size, True)\n```\n\nHowever, in Grammar as a Foreign Language, attention is computed using the last hidden vector of the multi-layer LSTM (e.g. `cell_output`). Computing attention with `new_state` involves using every layer's state, including their cell state (which should probably be only used inside the LSTM layer). To match GaaFL, the attention call should be\n\n``` python\nattns = attention(cell_output)\n```\n\nOf course, if the `cell` is wrapped by an `OutputProjectionWrapper` as done in `embedding_attention_seq2seq`, then this call is computationally expensive. To prevent this, `embedding_attention_seq2seq` can be modified as such:\n\n``` python\n# Starting from line 606.\noutput_size = None\nif output_projection is None:\n  output_size = num_decoder_symbols # Projection wrapper removed.\n```\n\nNow, `cell_output` will be the last hidden vector of the multi-layered LSTM. The reason this works is because `output` is already being projected for the softmax, so `cell_output` does not need to be projected to a length of `num_decoder_symbols`. If as before`cell` is wrapped by `OutputProjectionWrapper`, the `cell_output` will have size `num_decoder_symbols`. Computing the `output` will then involve a `(num_decoder_symbols + num_heads * attn_size) x num_decoder_symbols` matrix, which can be huge. However, if we make the change, then the matrix is only of size `(cell.output_size + num_heads * attn_size) x num_decoder_symbols`.\n\nThoughts? Have I missed something important? If this sounds good, I can make a PR. Thanks!\n"}