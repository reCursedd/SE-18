{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/350270784", "html_url": "https://github.com/tensorflow/tensorflow/issues/15198#issuecomment-350270784", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15198", "id": 350270784, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDI3MDc4NA==", "user": {"login": "marcoadurno", "id": 4450528, "node_id": "MDQ6VXNlcjQ0NTA1Mjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4450528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marcoadurno", "html_url": "https://github.com/marcoadurno", "followers_url": "https://api.github.com/users/marcoadurno/followers", "following_url": "https://api.github.com/users/marcoadurno/following{/other_user}", "gists_url": "https://api.github.com/users/marcoadurno/gists{/gist_id}", "starred_url": "https://api.github.com/users/marcoadurno/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marcoadurno/subscriptions", "organizations_url": "https://api.github.com/users/marcoadurno/orgs", "repos_url": "https://api.github.com/users/marcoadurno/repos", "events_url": "https://api.github.com/users/marcoadurno/events{/privacy}", "received_events_url": "https://api.github.com/users/marcoadurno/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-08T14:05:12Z", "updated_at": "2017-12-08T14:05:12Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6171989\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/stecklin\">@stecklin</a> I'm not sure I fully understand the reason why image_batch and label_batch have be be fetched using sess.run to then fed back in using placeholders. If doing so, I think the <code>summary_op</code> should use the <code>image</code> placeholder, same as <code>train_op</code> does but I struggle to see why mixing the Data api with placeholders.</p>\n<p>If things can be shuffled around:</p>\n<div class=\"highlight highlight-source-python\"><pre>dataset <span class=\"pl-k\">=</span> get_data(<span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nt_example <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator().get_next()\nt_logits <span class=\"pl-k\">=</span> infer_fn(t_example[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image<span class=\"pl-pds\">'</span></span>])\nloss_op <span class=\"pl-k\">=</span> loss_fn(t_example[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>], t_logits)\ntrain_op <span class=\"pl-k\">=</span> train_fn(loss_op)\nsummary_op <span class=\"pl-k\">=</span> tf.summary.image(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>preprocessed_image<span class=\"pl-pds\">'</span></span>, t_example[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image<span class=\"pl-pds\">'</span></span>])\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(n_steps):\n  fetches <span class=\"pl-k\">=</span> [loss_op, train_op]\n  <span class=\"pl-k\">if</span> i<span class=\"pl-k\">%</span><span class=\"pl-c1\">100</span>:\n    fetches <span class=\"pl-k\">+=</span> [summary_op]\n  sess.run(fetches)</pre></div>\n<p>Or alternatively you can use a monitored session and specify summary hooks. But the basic idea is that if you have a canonical training loop there is no reason why summaries can't be written using the Data api.</p>", "body_text": "@stecklin I'm not sure I fully understand the reason why image_batch and label_batch have be be fetched using sess.run to then fed back in using placeholders. If doing so, I think the summary_op should use the image placeholder, same as train_op does but I struggle to see why mixing the Data api with placeholders.\nIf things can be shuffled around:\ndataset = get_data(is_training=True)\nt_example = dataset.make_one_shot_iterator().get_next()\nt_logits = infer_fn(t_example['image'])\nloss_op = loss_fn(t_example['labels'], t_logits)\ntrain_op = train_fn(loss_op)\nsummary_op = tf.summary.image('preprocessed_image', t_example['image'])\nfor i in xrange(n_steps):\n  fetches = [loss_op, train_op]\n  if i%100:\n    fetches += [summary_op]\n  sess.run(fetches)\nOr alternatively you can use a monitored session and specify summary hooks. But the basic idea is that if you have a canonical training loop there is no reason why summaries can't be written using the Data api.", "body": "@stecklin I'm not sure I fully understand the reason why image_batch and label_batch have be be fetched using sess.run to then fed back in using placeholders. If doing so, I think the `summary_op` should use the `image` placeholder, same as `train_op` does but I struggle to see why mixing the Data api with placeholders.\r\n\r\nIf things can be shuffled around:\r\n```python\r\ndataset = get_data(is_training=True)\r\nt_example = dataset.make_one_shot_iterator().get_next()\r\nt_logits = infer_fn(t_example['image'])\r\nloss_op = loss_fn(t_example['labels'], t_logits)\r\ntrain_op = train_fn(loss_op)\r\nsummary_op = tf.summary.image('preprocessed_image', t_example['image'])\r\nfor i in xrange(n_steps):\r\n  fetches = [loss_op, train_op]\r\n  if i%100:\r\n    fetches += [summary_op]\r\n  sess.run(fetches)\r\n```\r\nOr alternatively you can use a monitored session and specify summary hooks. But the basic idea is that if you have a canonical training loop there is no reason why summaries can't be written using the Data api."}