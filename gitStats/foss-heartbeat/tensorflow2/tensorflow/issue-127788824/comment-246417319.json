{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246417319", "html_url": "https://github.com/tensorflow/tensorflow/issues/823#issuecomment-246417319", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/823", "id": 246417319, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NjQxNzMxOQ==", "user": {"login": "bsautermeister", "id": 2537736, "node_id": "MDQ6VXNlcjI1Mzc3MzY=", "avatar_url": "https://avatars2.githubusercontent.com/u/2537736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bsautermeister", "html_url": "https://github.com/bsautermeister", "followers_url": "https://api.github.com/users/bsautermeister/followers", "following_url": "https://api.github.com/users/bsautermeister/following{/other_user}", "gists_url": "https://api.github.com/users/bsautermeister/gists{/gist_id}", "starred_url": "https://api.github.com/users/bsautermeister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bsautermeister/subscriptions", "organizations_url": "https://api.github.com/users/bsautermeister/orgs", "repos_url": "https://api.github.com/users/bsautermeister/repos", "events_url": "https://api.github.com/users/bsautermeister/events{/privacy}", "received_events_url": "https://api.github.com/users/bsautermeister/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-12T17:07:26Z", "updated_at": "2016-09-12T18:21:22Z", "author_association": "NONE", "body_html": "<p>Hi, is there any progress already?</p>\n<p>I'm currently facing the same issue. When I save and restore my trained model as explained <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html\" rel=\"nofollow\">here</a> and I would like to continue my training, I was always wondering why the restored model always performs slightly worse that before saving it. I have the impression that restore() is not actually restoring the shadow-variables.</p>\n<p>In case we have to save/restore them manually, that would be OK for me, but could you tell an example. Because I cannot use mae.variables_to_restore(), because I cannot access the ExponentialMovingAverage instance, which has been written in a different module.</p>\n<p>Beside ExponentialMovingAverage, I have the same problem for the shadow-variables within the Optimizers (such as Adam).</p>\n<p><strong>EDIT:</strong></p>\n<p>I found an example of its usage here in the CIFAR10 eval script:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_eval.py\">https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_eval.py</a></p>\n<p>Unfortunately, when I do the same in my script, I still get worse results than before saving and restoring the model. Additionally, I cannot continue with my training when I do it like in the example:</p>\n<pre><code># Restore the moving average version of the learned variables for eval.\nvariable_averages = tf.train.ExponentialMovingAverage(\n    cifar10.MOVING_AVERAGE_DECAY)\nvariables_to_restore = variable_averages.variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore)\n</code></pre>\n<p>Is there no simple way to save and restore <strong>exactly</strong> in the same state?</p>\n<p><strong>EDIT 2:</strong></p>\n<p>For whatever reason, when I now do a simple restore like this, I can cantinue on training and get the same (better) results as before saving it:</p>\n<pre><code># Restore the moving average version of the learned variables for eval.\nsaver = tf.train.Saver(variables_to_restore)\nsaver.restore(sess, \"/path/to/file.ckpt\")\n</code></pre>\n<p>Is there a rule of thumb when I should use these moving averages and when not?<br>\nAdditionally, within the CIFAR-10 example of TensorFlow, I don't get the following thing:<br>\nThe exp. moving averages are tracked during training, but are these values always just copies lying next to the variables actually getting trained? So when I do not restore a model and explicitely load these moving average values, these values would never get used during the evaluation? Because in case I get worse results on these averaged values, I could skip the computation of them during training and get (slightly) better training-performance, right?</p>\n<p><strong>EDIT 3:</strong></p>\n<p>Ignore everything above :) After doing more training, restoring the exp. moving averages for evaluation actually give better results! I'm happy for at least right now! ;-)</p>", "body_text": "Hi, is there any progress already?\nI'm currently facing the same issue. When I save and restore my trained model as explained here and I would like to continue my training, I was always wondering why the restored model always performs slightly worse that before saving it. I have the impression that restore() is not actually restoring the shadow-variables.\nIn case we have to save/restore them manually, that would be OK for me, but could you tell an example. Because I cannot use mae.variables_to_restore(), because I cannot access the ExponentialMovingAverage instance, which has been written in a different module.\nBeside ExponentialMovingAverage, I have the same problem for the shadow-variables within the Optimizers (such as Adam).\nEDIT:\nI found an example of its usage here in the CIFAR10 eval script:\nhttps://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_eval.py\nUnfortunately, when I do the same in my script, I still get worse results than before saving and restoring the model. Additionally, I cannot continue with my training when I do it like in the example:\n# Restore the moving average version of the learned variables for eval.\nvariable_averages = tf.train.ExponentialMovingAverage(\n    cifar10.MOVING_AVERAGE_DECAY)\nvariables_to_restore = variable_averages.variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore)\n\nIs there no simple way to save and restore exactly in the same state?\nEDIT 2:\nFor whatever reason, when I now do a simple restore like this, I can cantinue on training and get the same (better) results as before saving it:\n# Restore the moving average version of the learned variables for eval.\nsaver = tf.train.Saver(variables_to_restore)\nsaver.restore(sess, \"/path/to/file.ckpt\")\n\nIs there a rule of thumb when I should use these moving averages and when not?\nAdditionally, within the CIFAR-10 example of TensorFlow, I don't get the following thing:\nThe exp. moving averages are tracked during training, but are these values always just copies lying next to the variables actually getting trained? So when I do not restore a model and explicitely load these moving average values, these values would never get used during the evaluation? Because in case I get worse results on these averaged values, I could skip the computation of them during training and get (slightly) better training-performance, right?\nEDIT 3:\nIgnore everything above :) After doing more training, restoring the exp. moving averages for evaluation actually give better results! I'm happy for at least right now! ;-)", "body": "Hi, is there any progress already?\n\nI'm currently facing the same issue. When I save and restore my trained model as explained [here](https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html) and I would like to continue my training, I was always wondering why the restored model always performs slightly worse that before saving it. I have the impression that restore() is not actually restoring the shadow-variables.\n\nIn case we have to save/restore them manually, that would be OK for me, but could you tell an example. Because I cannot use mae.variables_to_restore(), because I cannot access the ExponentialMovingAverage instance, which has been written in a different module.\n\nBeside ExponentialMovingAverage, I have the same problem for the shadow-variables within the Optimizers (such as Adam).\n\n**EDIT:**\n\nI found an example of its usage here in the CIFAR10 eval script:\nhttps://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_eval.py\n\nUnfortunately, when I do the same in my script, I still get worse results than before saving and restoring the model. Additionally, I cannot continue with my training when I do it like in the example:\n\n```\n# Restore the moving average version of the learned variables for eval.\nvariable_averages = tf.train.ExponentialMovingAverage(\n    cifar10.MOVING_AVERAGE_DECAY)\nvariables_to_restore = variable_averages.variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore)\n```\n\nIs there no simple way to save and restore **exactly** in the same state?\n\n**EDIT 2:**\n\nFor whatever reason, when I now do a simple restore like this, I can cantinue on training and get the same (better) results as before saving it:\n\n```\n# Restore the moving average version of the learned variables for eval.\nsaver = tf.train.Saver(variables_to_restore)\nsaver.restore(sess, \"/path/to/file.ckpt\")\n```\n\nIs there a rule of thumb when I should use these moving averages and when not?\nAdditionally, within the CIFAR-10 example of TensorFlow, I don't get the following thing:\nThe exp. moving averages are tracked during training, but are these values always just copies lying next to the variables actually getting trained? So when I do not restore a model and explicitely load these moving average values, these values would never get used during the evaluation? Because in case I get worse results on these averaged values, I could skip the computation of them during training and get (slightly) better training-performance, right?\n\n**EDIT 3:**\n\nIgnore everything above :) After doing more training, restoring the exp. moving averages for evaluation actually give better results! I'm happy for at least right now! ;-)\n"}