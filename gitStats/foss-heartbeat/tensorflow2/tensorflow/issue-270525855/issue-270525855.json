{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171", "id": 270525855, "node_id": "MDU6SXNzdWUyNzA1MjU4NTU=", "number": 14171, "title": "Memory leak in conv2d()", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 32, "created_at": "2017-11-02T04:02:27Z", "updated_at": "2018-06-28T18:05:01Z", "closed_at": "2018-06-28T18:05:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>Python: 3.5.2<br>\nTensorFlow version 1.3.0 via pip, 1.3.1 via source, 1.4.0-rc1 from source (confirmed the problem on all three of them)<br>\nBazel: 0.7.0<br>\nUbuntu 16.04<br>\nGPU: GeForce GTX 1080 with 8GB<br>\nNvidia drivers: 384.90<br>\nCUDA: 8.0<br>\nCuDNN: 6.0<br>\nOptimizer: Adam with default parameters</p>\n<h3>Problem Description</h3>\n<p>I will post here some evidence that makes me think that there's a major leak in <code>conv_2d()</code>.<br>\nUnfortunately I can't provide code and data replicate this as my employer doesn't allow me to as both are confidential, but hopefully the information i will provide will make it possible for you to replicate it.</p>\n<p>The setting:<br>\nI'm training a word cnn on text data on GPU. The length of the sequence of text is 256, the dimension of the embeddings is 256, I have 4 convolutional layers in parallel with different sizes (2, 3, 4 and 5 x 256). The inputs to the conv layers are <code>batch_size x 256 x 256 x 1</code>.<br>\nIn my code I run a full epoch of training, then I evaluate on the training set, the validation set and the test set, then I start a new epoch.</p>\n<p>The problem:<br>\nDepending on the batch_size the memory needed by TensorFlow to compute his operations changes, but it is in the order of few hundreds MB. At the last batch of training, the memory consumption increases dramatically, as happens at the last batch of evaluation on validation set and the last batch of evaluation on the test set.<br>\nAs _traini_set_size mod batch_size is not 0 (the same happens for validation and test set too), the last batch has a different dimension with respect to all others batches. For instance in my case with a batch_size of 100, the last batch of the validation set is of size 25.<br>\nWhat happens is that, in the <code>conv2d()</code> function TF allocates a lot of memory if the batch size is not the same that was used so far. In my case, that operation goes from needing 50 MB to 3.18 GB. As it seems too much, I suspect there's a memory leak somehow.</p>\n<p>I'm attaching 2 screenshots taken from TensorBoard. What they show is the same node in the second last batch of the validation set and the last batch of the validation set. The memory consumption is in the node state on the right of the image. I can share the log directory if needed as it contains the graph of the model and the memory information at all steps of training and evaluation.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/349256/32309260-99a53ab4-bf47-11e7-96d0-a429180f0e1c.png\"><img src=\"https://user-images.githubusercontent.com/349256/32309260-99a53ab4-bf47-11e7-96d0-a429180f0e1c.png\" alt=\"eval_vali_step_8\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/349256/32309259-99895934-bf47-11e7-8b3f-08306321cb8e.png\"><img src=\"https://user-images.githubusercontent.com/349256/32309259-99895934-bf47-11e7-8b3f-08306321cb8e.png\" alt=\"eval_vali_step_9\" style=\"max-width:100%;\"></a></p>\n<p>The weird thing is that when batch_size is 256 or 512 it doesn't happen, but with batch_size 100 or 128 or 200 (the other 3 I tested) it happens. testing on another bigger dataset, even with 256 as the batch size the supposed memory leak happens, but in that case TF tries to allocate around 33 GB of ram and goes out of memory. My temporary workaround is just throwing away the last batch of the train, validation and test set. Doing so the memory consumption keeps constant.</p>\n<p>Hopefully this is enough information for investigate the problem, otherwise feel free to request me additional information, even if, as I said, unfortunately I can't provide code and data.</p>", "body_text": "System information\nPython: 3.5.2\nTensorFlow version 1.3.0 via pip, 1.3.1 via source, 1.4.0-rc1 from source (confirmed the problem on all three of them)\nBazel: 0.7.0\nUbuntu 16.04\nGPU: GeForce GTX 1080 with 8GB\nNvidia drivers: 384.90\nCUDA: 8.0\nCuDNN: 6.0\nOptimizer: Adam with default parameters\nProblem Description\nI will post here some evidence that makes me think that there's a major leak in conv_2d().\nUnfortunately I can't provide code and data replicate this as my employer doesn't allow me to as both are confidential, but hopefully the information i will provide will make it possible for you to replicate it.\nThe setting:\nI'm training a word cnn on text data on GPU. The length of the sequence of text is 256, the dimension of the embeddings is 256, I have 4 convolutional layers in parallel with different sizes (2, 3, 4 and 5 x 256). The inputs to the conv layers are batch_size x 256 x 256 x 1.\nIn my code I run a full epoch of training, then I evaluate on the training set, the validation set and the test set, then I start a new epoch.\nThe problem:\nDepending on the batch_size the memory needed by TensorFlow to compute his operations changes, but it is in the order of few hundreds MB. At the last batch of training, the memory consumption increases dramatically, as happens at the last batch of evaluation on validation set and the last batch of evaluation on the test set.\nAs _traini_set_size mod batch_size is not 0 (the same happens for validation and test set too), the last batch has a different dimension with respect to all others batches. For instance in my case with a batch_size of 100, the last batch of the validation set is of size 25.\nWhat happens is that, in the conv2d() function TF allocates a lot of memory if the batch size is not the same that was used so far. In my case, that operation goes from needing 50 MB to 3.18 GB. As it seems too much, I suspect there's a memory leak somehow.\nI'm attaching 2 screenshots taken from TensorBoard. What they show is the same node in the second last batch of the validation set and the last batch of the validation set. The memory consumption is in the node state on the right of the image. I can share the log directory if needed as it contains the graph of the model and the memory information at all steps of training and evaluation.\n\n\nThe weird thing is that when batch_size is 256 or 512 it doesn't happen, but with batch_size 100 or 128 or 200 (the other 3 I tested) it happens. testing on another bigger dataset, even with 256 as the batch size the supposed memory leak happens, but in that case TF tries to allocate around 33 GB of ram and goes out of memory. My temporary workaround is just throwing away the last batch of the train, validation and test set. Doing so the memory consumption keeps constant.\nHopefully this is enough information for investigate the problem, otherwise feel free to request me additional information, even if, as I said, unfortunately I can't provide code and data.", "body": "### System information\r\n\r\nPython: 3.5.2\r\nTensorFlow version 1.3.0 via pip, 1.3.1 via source, 1.4.0-rc1 from source (confirmed the problem on all three of them)\r\nBazel: 0.7.0\r\nUbuntu 16.04\r\nGPU: GeForce GTX 1080 with 8GB\r\nNvidia drivers: 384.90\r\nCUDA: 8.0\r\nCuDNN: 6.0\r\nOptimizer: Adam with default parameters\r\n\r\n### Problem Description\r\n\r\nI will post here some evidence that makes me think that there's a major leak in `conv_2d()`.\r\nUnfortunately I can't provide code and data replicate this as my employer doesn't allow me to as both are confidential, but hopefully the information i will provide will make it possible for you to replicate it.\r\n\r\nThe setting:\r\nI'm training a word cnn on text data on GPU. The length of the sequence of text is 256, the dimension of the embeddings is 256, I have 4 convolutional layers in parallel with different sizes (2, 3, 4 and 5 x 256). The inputs to the conv layers are `batch_size x 256 x 256 x 1`.\r\nIn my code I run a full epoch of training, then I evaluate on the training set, the validation set and the test set, then I start a new epoch.\r\n\r\nThe problem:\r\nDepending on the batch_size the memory needed by TensorFlow to compute his operations changes, but it is in the order of few hundreds MB. At the last batch of training, the memory consumption increases dramatically, as happens at the last batch of evaluation on validation set and the last batch of evaluation on the test set.\r\nAs _traini_set_size mod batch_size is not 0 (the same happens for validation and test set too), the last batch has a different dimension with respect to all others batches. For instance in my case with a batch_size of 100, the last batch of the validation set is of size 25.\r\nWhat happens is that, in the `conv2d()` function TF allocates a lot of memory if the batch size is not the same that was used so far. In my case, that operation goes from needing 50 MB to 3.18 GB. As it seems too much, I suspect there's a memory leak somehow.\r\n\r\nI'm attaching 2 screenshots taken from TensorBoard. What they show is the same node in the second last batch of the validation set and the last batch of the validation set. The memory consumption is in the node state on the right of the image. I can share the log directory if needed as it contains the graph of the model and the memory information at all steps of training and evaluation.\r\n\r\n![eval_vali_step_8](https://user-images.githubusercontent.com/349256/32309260-99a53ab4-bf47-11e7-96d0-a429180f0e1c.png)\r\n![eval_vali_step_9](https://user-images.githubusercontent.com/349256/32309259-99895934-bf47-11e7-8b3f-08306321cb8e.png)\r\n\r\nThe weird thing is that when batch_size is 256 or 512 it doesn't happen, but with batch_size 100 or 128 or 200 (the other 3 I tested) it happens. testing on another bigger dataset, even with 256 as the batch size the supposed memory leak happens, but in that case TF tries to allocate around 33 GB of ram and goes out of memory. My temporary workaround is just throwing away the last batch of the train, validation and test set. Doing so the memory consumption keeps constant.\r\n\r\nHopefully this is enough information for investigate the problem, otherwise feel free to request me additional information, even if, as I said, unfortunately I can't provide code and data.\r\n\r\n"}