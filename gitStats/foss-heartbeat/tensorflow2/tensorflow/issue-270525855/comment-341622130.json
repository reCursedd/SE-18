{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341622130", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171#issuecomment-341622130", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "id": 341622130, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTYyMjEzMA==", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T04:49:52Z", "updated_at": "2017-11-03T04:49:52Z", "author_association": "NONE", "body_html": "<p>Here's a python3 script to reproduce the issue (it's a bit long but I tried to be as faithful to my original setting even if I'm not using a single line of code from my employer):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> math\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> parameters</span>\nskip_last <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\ntrain_set_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">7113</span>\nvali_set_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">925</span>\ntest_set_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1962</span>\n\ntext_length <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\nvocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20000</span>\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">500</span>\n\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create artificial dataset</span>\nx_train <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, vocab_size, [train_set_size, text_length])\ny_train <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, num_classes, [train_set_size])\nx_vali <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, vocab_size, [vali_set_size, text_length])\ny_vali <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, num_classes, [vali_set_size])\nx_test <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, vocab_size, [test_set_size, text_length])\ny_test <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, num_classes, [test_set_size])\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Building model</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">fc_layer</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">in_count</span>, <span class=\"pl-smi\">out_count</span>, <span class=\"pl-smi\">act</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>):\n    weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>, [in_count, out_count])\n    bias <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bias<span class=\"pl-pds\">\"</span></span>, [out_count])\n    layer <span class=\"pl-k\">=</span> tf.matmul(inputs, weights) <span class=\"pl-k\">+</span> bias\n    <span class=\"pl-k\">if</span> act:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">getattr</span>(tf.nn, act)(layer)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> layer\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">conv_layer</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">kernel_shape</span>, <span class=\"pl-smi\">bias_shape</span>, <span class=\"pl-smi\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">act</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-smi\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>, <span class=\"pl-smi\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>, kernel_shape)\n    conv <span class=\"pl-k\">=</span> tf.nn.conv2d(inputs, weights,\n                        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, stride, stride, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span>padding)\n    layer <span class=\"pl-k\">=</span> conv\n    <span class=\"pl-k\">if</span> act:\n        layer <span class=\"pl-k\">=</span> <span class=\"pl-c1\">getattr</span>(tf.nn, act)(layer)\n    <span class=\"pl-k\">if</span> dropout <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        layer <span class=\"pl-k\">=</span> tf.layers.dropout(layer, <span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span>dropout, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)\n    <span class=\"pl-k\">return</span> layer\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">word_cnn</span>(<span class=\"pl-smi\">input_text</span>,\n             <span class=\"pl-smi\">vocab_size</span>,\n             <span class=\"pl-smi\">dropout</span>,\n             <span class=\"pl-smi\">embedding_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>,\n             <span class=\"pl-smi\">filter_sizes</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>),\n             <span class=\"pl-smi\">num_filters</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>),\n             <span class=\"pl-smi\">fully_connected_sizes</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">256</span>),\n             <span class=\"pl-smi\">regularize_layers</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">False</span>, <span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">True</span>),\n             <span class=\"pl-smi\">embeddings_oon_cpu</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n             <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n             <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n             <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n    <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(filter_sizes) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">len</span>(num_filters)\n    <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(regularize_layers) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">len</span>(fully_connected_sizes) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n\n    sequence_length <span class=\"pl-k\">=</span> input_text.get_shape()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>word_cnn_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(name)):\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> ================ Word Embeddings ================</span>\n        <span class=\"pl-k\">if</span> embeddings_oon_cpu:\n            <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n                word_embeddings <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([vocab_size, embedding_size], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>),\n                                              <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>word_embeddings<span class=\"pl-pds\">'</span></span>)\n                initial_input <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(word_embeddings, input_text, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>embeddings_lookup<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">else</span>:\n            word_embeddings <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([vocab_size, embedding_size], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>),\n                                          <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>word_embeddings<span class=\"pl-pds\">'</span></span>)\n            initial_input <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(word_embeddings, input_text, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>embeddings_lookup<span class=\"pl-pds\">'</span></span>)\n        curr_input <span class=\"pl-k\">=</span> tf.expand_dims(initial_input, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> ================ Conv Layers ================</span>\n        parallel_conv_layers <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(filter_sizes)):\n            curr_filter_size <span class=\"pl-k\">=</span> filter_sizes[i]\n            curr_num_filters <span class=\"pl-k\">=</span> num_filters[i]\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>conv_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(curr_filter_size)):\n                filter_shape <span class=\"pl-k\">=</span> [curr_filter_size, embedding_size, <span class=\"pl-c1\">1</span>, curr_num_filters]\n                layer_output <span class=\"pl-k\">=</span> conv_layer(curr_input, filter_shape, [curr_num_filters],\n                                          <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n                layer_output <span class=\"pl-k\">=</span> tf.nn.max_pool(\n                    layer_output,\n                    <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, sequence_length <span class=\"pl-k\">-</span> curr_filter_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n                    <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n                    <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pool_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(curr_filter_size))\n                parallel_conv_layers.append(layer_output)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Flatten to vector</span>\n        num_filters_total <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sum</span>(num_filters)\n        hidden <span class=\"pl-k\">=</span> tf.reshape(tf.concat(parallel_conv_layers, <span class=\"pl-c1\">3</span>), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, num_filters_total], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>flatten<span class=\"pl-pds\">'</span></span>)\n        hidden_size <span class=\"pl-k\">=</span> num_filters_total\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> ================ Fully Connected Layers ================</span>\n        num_fully_connected_layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(fully_connected_sizes)\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_fully_connected_layers):\n            fully_connected_size <span class=\"pl-k\">=</span> fully_connected_sizes[i]\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fc_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(i)):\n                <span class=\"pl-k\">if</span> dropout <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dropout<span class=\"pl-pds\">\"</span></span>):\n                        hidden <span class=\"pl-k\">=</span> tf.layers.dropout(hidden, <span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span>dropout, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Fully Connected Layer</span>\n                hidden <span class=\"pl-k\">=</span> fc_layer(hidden, hidden_size, fully_connected_size)\n                hidden_size <span class=\"pl-k\">=</span> fully_connected_size\n\n    <span class=\"pl-k\">return</span> hidden, hidden_size\n\n\nis_training <span class=\"pl-k\">=</span> tf.placeholder(tf.bool, [], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>is_training<span class=\"pl-pds\">'</span></span>)\nglobal_step <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\ndropout <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dropout<span class=\"pl-pds\">\"</span></span>)\n\ninput_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, text_length], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input_placeholder<span class=\"pl-pds\">\"</span></span>)\nlabel_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>label_placeholder<span class=\"pl-pds\">\"</span></span>)\n\nhidden, hidden_size <span class=\"pl-k\">=</span> word_cnn(input_placeholder, vocab_size, <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span>dropout, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>text<span class=\"pl-pds\">\"</span></span>)\nsoftmax_w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax_weights<span class=\"pl-pds\">\"</span></span>, [hidden_size, num_classes])\nsoftmax_b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax_bias<span class=\"pl-pds\">\"</span></span>, [num_classes])\nlogits <span class=\"pl-k\">=</span> tf.matmul(hidden, softmax_w) <span class=\"pl-k\">+</span> softmax_b\n\nprobabilities <span class=\"pl-k\">=</span> tf.nn.softmax(logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>probabilities<span class=\"pl-pds\">\"</span></span>)\npredictions <span class=\"pl-k\">=</span> tf.argmax(logits, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>predictions<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n    top_k_predictions <span class=\"pl-k\">=</span> tf.nn.top_k(logits, <span class=\"pl-v\">k</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">sorted</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>top_k_predictions<span class=\"pl-pds\">\"</span></span>)\n\ncorrect_predictions <span class=\"pl-k\">=</span> tf.equal(predictions, label_placeholder, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>correct_predictions<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n    hits_at_k <span class=\"pl-k\">=</span> tf.nn.in_top_k(logits, label_placeholder, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hits_at_k<span class=\"pl-pds\">\"</span></span>)\n\nonehot_labels <span class=\"pl-k\">=</span> tf.one_hot(label_placeholder, num_classes, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>onehot_labels<span class=\"pl-pds\">\"</span></span>)\nloss <span class=\"pl-k\">=</span> tf.losses.softmax_cross_entropy(<span class=\"pl-v\">onehot_labels</span><span class=\"pl-k\">=</span>onehot_labels, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n\noptimize <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.0025</span>).minimize(loss)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> training</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">convert_size</span>(<span class=\"pl-smi\">size_bytes</span>):\n    <span class=\"pl-k\">if</span> size_bytes <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>0B<span class=\"pl-pds\">\"</span></span>\n    size_name <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>B<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>KB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>GB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>PB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>EB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ZB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>YB<span class=\"pl-pds\">\"</span></span>)\n    i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(math.floor(math.log(size_bytes, <span class=\"pl-c1\">1024</span>)))\n    p <span class=\"pl-k\">=</span> math.pow(<span class=\"pl-c1\">1024</span>, i)\n    s <span class=\"pl-k\">=</span> <span class=\"pl-c1\">round</span>(size_bytes <span class=\"pl-k\">/</span> p, <span class=\"pl-c1\">2</span>)\n    <span class=\"pl-k\">return</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{}</span> <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(s, size_name[i])\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Batcher</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dataset</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">skip_last</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        <span class=\"pl-c1\">self</span>.dataset <span class=\"pl-k\">=</span> dataset\n        <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> batch_size\n        <span class=\"pl-c1\">self</span>.total_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">min</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, dataset.values()))\n        <span class=\"pl-c1\">self</span>.skip_last <span class=\"pl-k\">=</span> skip_last\n        <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">next_batch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.last_batch():\n            <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        sub_batch <span class=\"pl-k\">=</span> {}\n        <span class=\"pl-k\">for</span> key <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.dataset:\n            sub_batch[key] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.dataset[key][<span class=\"pl-c1\">self</span>.index:<span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.batch_size]\n        <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">self</span>.batch_size\n        <span class=\"pl-k\">return</span> sub_batch\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">last_batch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">self</span>.total_size <span class=\"pl-k\">or</span> (<span class=\"pl-c1\">self</span>.skip_last <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">self</span>.total_size)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">batch_eval</span>(<span class=\"pl-smi\">session</span>, <span class=\"pl-smi\">dataset</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">epoch</span>, <span class=\"pl-smi\">name</span>):\n    batcher <span class=\"pl-k\">=</span> Batcher(dataset, batch_size, skip_last)\n    tot_size <span class=\"pl-k\">=</span> batcher.total_size\n    losses <span class=\"pl-k\">=</span> []\n    correct_preds <span class=\"pl-k\">=</span> []\n    hits_at_ks <span class=\"pl-k\">=</span> []\n    step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> batcher.last_batch():\n        batch <span class=\"pl-k\">=</span> batcher.next_batch()\n        loss_val, correct_predictions_val, hits_at_k_val <span class=\"pl-k\">=</span> session.run(\n            [loss, correct_predictions, hits_at_k],\n            <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{input_placeholder: batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>], label_placeholder: batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>], dropout: <span class=\"pl-c1\">0.3</span>, is_training: <span class=\"pl-c1\">True</span>}\n        )\n        losses.append(loss_val)\n        correct_preds.append(correct_predictions_val)\n        hits_at_ks.append(hits_at_k_val)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch<span class=\"pl-pds\">'</span></span>, epoch, name, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval step<span class=\"pl-pds\">'</span></span>, step)\n        mbiu <span class=\"pl-k\">=</span> session.run(tf.contrib.memory_stats.MaxBytesInUse())\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mbiu<span class=\"pl-pds\">'</span></span>, convert_size(mbiu))\n        step <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n    losses <span class=\"pl-k\">=</span> np.array(losses)\n    correct_preds <span class=\"pl-k\">=</span> np.concatenate(correct_preds)\n    hits_at_ks <span class=\"pl-k\">=</span> np.concatenate(hits_at_ks)\n    <span class=\"pl-k\">return</span> losses.sum() <span class=\"pl-k\">/</span> tot_size, correct_preds.sum() <span class=\"pl-k\">/</span> tot_size, hits_at_ks.sum() <span class=\"pl-k\">/</span> tot_size\n\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> session:\n    tf.global_variables_initializer().run()\n\n    epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    train_batcher <span class=\"pl-k\">=</span> Batcher({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: x_train, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>: y_train}, batch_size, skip_last)\n\n    <span class=\"pl-k\">while</span> epoch <span class=\"pl-k\">&lt;</span> epochs:\n        <span class=\"pl-k\">if</span> step <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Epoch <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(epoch <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>))\n        batch <span class=\"pl-k\">=</span> train_batcher.next_batch()\n        _, loss_val <span class=\"pl-k\">=</span> session.run(\n            [optimize, loss],\n            <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{input_placeholder: batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>], label_placeholder: batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>], dropout: <span class=\"pl-c1\">0.3</span>, is_training: <span class=\"pl-c1\">True</span>}\n        )\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch<span class=\"pl-pds\">'</span></span>, epoch <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>step<span class=\"pl-pds\">'</span></span>, step <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>, loss_val)\n        mbiu <span class=\"pl-k\">=</span> session.run(tf.contrib.memory_stats.MaxBytesInUse())\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mbiu<span class=\"pl-pds\">'</span></span>, convert_size(mbiu))\n        step <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n\n        <span class=\"pl-k\">if</span> train_batcher.last_batch():\n            epoch <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n            step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n            train_loss, train_acc, train_hits <span class=\"pl-k\">=</span> batch_eval(session, {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: x_train, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>: y_train}, batch_size, epoch,\n                                                           <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_loss<span class=\"pl-pds\">'</span></span>, train_loss, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_acc<span class=\"pl-pds\">'</span></span>, train_acc, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_hits<span class=\"pl-pds\">'</span></span>, train_hits)\n\n            vali_loss, vali_acc, vali_hits <span class=\"pl-k\">=</span> batch_eval(session, {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: x_vali, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>: y_vali}, batch_size, epoch, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>vali<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>vali_loss<span class=\"pl-pds\">'</span></span>, vali_loss, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>vali_acc<span class=\"pl-pds\">'</span></span>, vali_acc, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>vali_hits<span class=\"pl-pds\">'</span></span>, vali_hits)\n\n            test_loss, test_acc, test_hits <span class=\"pl-k\">=</span> batch_eval(session, {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: x_test, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>: y_test}, batch_size, epoch, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test_loss<span class=\"pl-pds\">'</span></span>, test_loss, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test_acc<span class=\"pl-pds\">'</span></span>, test_acc, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test_hits<span class=\"pl-pds\">'</span></span>, test_hits)\n</pre></div>\n<p>You can play with <code>skip_last</code>. Setting it to <code>False</code> instructs the batcher to create the last batch with size less than <code>batch_size</code> if <code>dataset_size % batch_size &gt; 0</code>. In this case you can see the leak happening in the last batch of the validation set evaluation. Memory consuption goes suddenly from 461.72 MB to 7.41 GB. See a log here:</p>\n<pre><code>2017-11-02 21:18:28.606838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:04:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2017-11-02 21:18:28.606864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\nEpoch 1\nepoch 1 step 1 loss 6.22924\nmbiu 461.85 MB\nepoch 1 step 2 loss 6.27471\nmbiu 461.85 MB\nepoch 1 step 3 loss 6.37816\nmbiu 461.85 MB\n...\nepoch 1 step 55 loss 6.21585\nmbiu 461.85 MB\nepoch 1 step 56 loss 6.24239\nmbiu 461.85 MB\nepoch 1 train eval step 0\nmbiu 461.85 MB\nepoch 1 train eval step 1\nmbiu 461.85 MB\nepoch 1 train eval step 2\nmbiu 461.85 MB\nepoch 1 train eval step 3\nmbiu 461.85 MB\n...\nepoch 1 train eval step 53\nmbiu 461.85 MB\nepoch 1 train eval step 54\nmbiu 461.85 MB\nepoch 1 train eval step 55\nmbiu 461.85 MB\ntrain_loss 0.048836645724 train_acc 0.00281175312808 train_hits 0.00955996063546\nepoch 1 vali eval step 0\nmbiu 461.85 MB\nepoch 1 vali eval step 1\nmbiu 461.85 MB\nepoch 1 vali eval step 2\nmbiu 461.85 MB\nepoch 1 vali eval step 3\nmbiu 461.85 MB\nepoch 1 vali eval step 4\nmbiu 461.85 MB\nepoch 1 vali eval step 5\nmbiu 461.85 MB\nepoch 1 vali eval step 6\nmbiu 461.85 MB\nepoch 1 vali eval step 7\nmbiu 7.41 GB\nvali_loss 0.0538858032227 vali_acc 0.00108108108108 vali_hits 0.00216216216216\nepoch 1 test eval step 0\nmbiu 7.41 GB\nepoch 1 test eval step 1\nmbiu 7.41 GB\n...\nepoch 1 test eval step 13\nmbiu 7.41 GB\nepoch 1 test eval step 14\nmbiu 7.41 GB\nepoch 1 test eval step 15\nmbiu 7.41 GB\ntest_loss 0.0507940473177 test_acc 0.00254841997961 test_hits 0.00917431192661\nEpoch 2\nepoch 2 step 1 loss 6.18541\nmbiu 7.41 GB\nepoch 2 step 2 loss 6.19003\nmbiu 7.41 GB\nepoch 2 step 3 loss 6.18152\nmbiu 7.41 GB\n...\nepoch 2 test eval step 14\nmbiu 7.41 GB\nepoch 2 test eval step 15\nmbiu 7.41 GB\ntest_loss 0.0509010027186 test_acc 0.00254841997961 test_hits 0.00560652395515\n</code></pre>\n<p>If you set <code>skip_last</code> to <code>True</code> the last betch size with a different size from the usual <code>batch_size</code> will be skipped. As you can see from this log the memory consuption stay constant at 461.72 MB from beginning to the end.</p>\n<pre><code>2017-11-02 21:14:45.784362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:04:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2017-11-02 21:14:45.784395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\nEpoch 1\nepoch 1 step 1 loss 6.22736\nmbiu 461.72 MB\nepoch 1 step 2 loss 6.27873\nmbiu 461.72 MB\nepoch 1 step 3 loss 6.32521\nmbiu 461.72 MB\nepoch 1 step 4 loss 6.34492\nmbiu 461.72 MB\nepoch 1 step 5 loss 6.33192\nmbiu 461.72 MB\nepoch 1 step 6 loss 6.27755\nmbiu 461.72 MB\n...\nepoch 2 test eval step 11\nmbiu 461.78 MB\nepoch 2 test eval step 12\nmbiu 461.78 MB\nepoch 2 test eval step 13\nmbiu 461.78 MB\nepoch 2 test eval step 14\nmbiu 461.78 MB\ntest_loss 0.0475411449125 test_acc 0.00101936799185 test_hits 0.00407747196738\n</code></pre>\n<p>In the case of this example, the leaked memory is still within the 8 GB limit of my GPU so it doesn't go out of memory, but I have a bigger dataset and using it TF at the last batch of validation tries to allocate 33 GB and goes out of memory. To replicate that setting you should set the parameters at the beginning of the script in this way:</p>\n<pre><code>train_set_size = 2896257\nvali_set_size = 97713\ntest_set_size = 97702\n</code></pre>\n<p>Hope this is enough.</p>", "body_text": "Here's a python3 script to reproduce the issue (it's a bit long but I tried to be as faithful to my original setting even if I'm not using a single line of code from my employer):\nimport math\n\nimport numpy as np\nimport tensorflow as tf\n\n# parameters\nskip_last = False\n\ntrain_set_size = 7113\nvali_set_size = 925\ntest_set_size = 1962\n\ntext_length = 256\nvocab_size = 20000\nnum_classes = 500\n\nepochs = 2\nbatch_size = 128\n\n# create artificial dataset\nx_train = np.random.randint(0, vocab_size, [train_set_size, text_length])\ny_train = np.random.randint(0, num_classes, [train_set_size])\nx_vali = np.random.randint(0, vocab_size, [vali_set_size, text_length])\ny_vali = np.random.randint(0, num_classes, [vali_set_size])\nx_test = np.random.randint(0, vocab_size, [test_set_size, text_length])\ny_test = np.random.randint(0, num_classes, [test_set_size])\n\n\n# Building model\ndef fc_layer(inputs, in_count, out_count, act=\"relu\"):\n    weights = tf.get_variable(\"weights\", [in_count, out_count])\n    bias = tf.get_variable(\"bias\", [out_count])\n    layer = tf.matmul(inputs, weights) + bias\n    if act:\n        return getattr(tf.nn, act)(layer)\n    else:\n        return layer\n\n\ndef conv_layer(inputs, kernel_shape, bias_shape, stride=1, act=\"relu\", padding='VALID', dropout=None, is_training=True):\n    weights = tf.get_variable(\"weights\", kernel_shape)\n    conv = tf.nn.conv2d(inputs, weights,\n                        strides=[1, stride, stride, 1], padding=padding)\n    layer = conv\n    if act:\n        layer = getattr(tf.nn, act)(layer)\n    if dropout is not None:\n        layer = tf.layers.dropout(layer, rate=dropout, training=is_training)\n    return layer\n\n\ndef word_cnn(input_text,\n             vocab_size,\n             dropout,\n             embedding_size=256,\n             filter_sizes=(2, 3, 4, 5),\n             num_filters=(256, 256, 256, 256),\n             fully_connected_sizes=(512, 256),\n             regularize_layers=(False, True, True),\n             embeddings_oon_cpu=False,\n             is_training=True,\n             name=None,\n             **kwargs):\n    assert len(filter_sizes) == len(num_filters)\n    assert len(regularize_layers) == len(fully_connected_sizes) + 1\n\n    sequence_length = input_text.get_shape()[-1]\n\n    with tf.variable_scope(\"word_cnn_{}\".format(name)):\n\n        # ================ Word Embeddings ================\n        if embeddings_oon_cpu:\n            with tf.device('/cpu:0'):\n                word_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n                                              name='word_embeddings')\n                initial_input = tf.nn.embedding_lookup(word_embeddings, input_text, name='embeddings_lookup')\n        else:\n            word_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n                                          name='word_embeddings')\n            initial_input = tf.nn.embedding_lookup(word_embeddings, input_text, name='embeddings_lookup')\n        curr_input = tf.expand_dims(initial_input, -1)\n\n        # ================ Conv Layers ================\n        parallel_conv_layers = []\n        for i in range(len(filter_sizes)):\n            curr_filter_size = filter_sizes[i]\n            curr_num_filters = num_filters[i]\n            with tf.variable_scope(\"conv_{}\".format(curr_filter_size)):\n                filter_shape = [curr_filter_size, embedding_size, 1, curr_num_filters]\n                layer_output = conv_layer(curr_input, filter_shape, [curr_num_filters],\n                                          is_training=is_training, dropout=None)\n                layer_output = tf.nn.max_pool(\n                    layer_output,\n                    ksize=[1, sequence_length - curr_filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding='VALID',\n                    name=\"pool_{}\".format(curr_filter_size))\n                parallel_conv_layers.append(layer_output)\n\n        # Flatten to vector\n        num_filters_total = sum(num_filters)\n        hidden = tf.reshape(tf.concat(parallel_conv_layers, 3), [-1, num_filters_total], name='flatten')\n        hidden_size = num_filters_total\n\n        # ================ Fully Connected Layers ================\n        num_fully_connected_layers = len(fully_connected_sizes)\n        for i in range(num_fully_connected_layers):\n            fully_connected_size = fully_connected_sizes[i]\n            with tf.variable_scope(\"fc_{}\".format(i)):\n                if dropout is not None:\n                    with tf.name_scope(\"dropout\"):\n                        hidden = tf.layers.dropout(hidden, rate=dropout, training=is_training)\n\n                # Fully Connected Layer\n                hidden = fc_layer(hidden, hidden_size, fully_connected_size)\n                hidden_size = fully_connected_size\n\n    return hidden, hidden_size\n\n\nis_training = tf.placeholder(tf.bool, [], name='is_training')\nglobal_step = tf.Variable(0, trainable=False)\ndropout = tf.placeholder(tf.float32, name=\"dropout\")\n\ninput_placeholder = tf.placeholder(tf.int32, [None, text_length], name=\"input_placeholder\")\nlabel_placeholder = tf.placeholder(tf.int64, [None], name=\"label_placeholder\")\n\nhidden, hidden_size = word_cnn(input_placeholder, vocab_size, dropout=dropout, is_training=is_training, name=\"text\")\nsoftmax_w = tf.get_variable(\"softmax_weights\", [hidden_size, num_classes])\nsoftmax_b = tf.get_variable(\"softmax_bias\", [num_classes])\nlogits = tf.matmul(hidden, softmax_w) + softmax_b\n\nprobabilities = tf.nn.softmax(logits, name=\"probabilities\")\npredictions = tf.argmax(logits, 1, name=\"predictions\")\nwith tf.device('/cpu:0'):\n    top_k_predictions = tf.nn.top_k(logits, k=3, sorted=True, name=\"top_k_predictions\")\n\ncorrect_predictions = tf.equal(predictions, label_placeholder, name=\"correct_predictions\")\nwith tf.device('/cpu:0'):\n    hits_at_k = tf.nn.in_top_k(logits, label_placeholder, 3, name=\"hits_at_k\")\n\nonehot_labels = tf.one_hot(label_placeholder, num_classes, name=\"onehot_labels\")\nloss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n\noptimize = tf.train.AdamOptimizer(0.0025).minimize(loss)\n\n\n# training\ndef convert_size(size_bytes):\n    if size_bytes == 0:\n        return \"0B\"\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n    i = int(math.floor(math.log(size_bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(size_bytes / p, 2)\n    return \"{} {}\".format(s, size_name[i])\n\n\nclass Batcher(object):\n    def __init__(self, dataset, batch_size, skip_last=False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.total_size = min(map(len, dataset.values()))\n        self.skip_last = skip_last\n        self.index = 0\n\n    def next_batch(self):\n        if self.last_batch():\n            self.index = 0\n        sub_batch = {}\n        for key in self.dataset:\n            sub_batch[key] = self.dataset[key][self.index:self.index + self.batch_size]\n        self.index += self.batch_size\n        return sub_batch\n\n    def last_batch(self):\n        return self.index >= self.total_size or (self.skip_last and self.index + self.batch_size >= self.total_size)\n\n\ndef batch_eval(session, dataset, batch_size, epoch, name):\n    batcher = Batcher(dataset, batch_size, skip_last)\n    tot_size = batcher.total_size\n    losses = []\n    correct_preds = []\n    hits_at_ks = []\n    step = 0\n    while not batcher.last_batch():\n        batch = batcher.next_batch()\n        loss_val, correct_predictions_val, hits_at_k_val = session.run(\n            [loss, correct_predictions, hits_at_k],\n            feed_dict={input_placeholder: batch['x'], label_placeholder: batch['y'], dropout: 0.3, is_training: True}\n        )\n        losses.append(loss_val)\n        correct_preds.append(correct_predictions_val)\n        hits_at_ks.append(hits_at_k_val)\n        print('epoch', epoch, name, 'eval step', step)\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\n        print('mbiu', convert_size(mbiu))\n        step += 1\n    losses = np.array(losses)\n    correct_preds = np.concatenate(correct_preds)\n    hits_at_ks = np.concatenate(hits_at_ks)\n    return losses.sum() / tot_size, correct_preds.sum() / tot_size, hits_at_ks.sum() / tot_size\n\n\nwith tf.Session() as session:\n    tf.global_variables_initializer().run()\n\n    epoch = 0\n    step = 0\n    train_batcher = Batcher({'x': x_train, 'y': y_train}, batch_size, skip_last)\n\n    while epoch < epochs:\n        if step == 0:\n            print(\"Epoch {}\".format(epoch + 1))\n        batch = train_batcher.next_batch()\n        _, loss_val = session.run(\n            [optimize, loss],\n            feed_dict={input_placeholder: batch['x'], label_placeholder: batch['y'], dropout: 0.3, is_training: True}\n        )\n        print('epoch', epoch + 1, 'step', step + 1, 'loss', loss_val)\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\n        print('mbiu', convert_size(mbiu))\n        step += 1\n\n        if train_batcher.last_batch():\n            epoch += 1\n            step = 0\n\n            train_loss, train_acc, train_hits = batch_eval(session, {'x': x_train, 'y': y_train}, batch_size, epoch,\n                                                           'train')\n            print('train_loss', train_loss, 'train_acc', train_acc, 'train_hits', train_hits)\n\n            vali_loss, vali_acc, vali_hits = batch_eval(session, {'x': x_vali, 'y': y_vali}, batch_size, epoch, 'vali')\n            print('vali_loss', vali_loss, 'vali_acc', vali_acc, 'vali_hits', vali_hits)\n\n            test_loss, test_acc, test_hits = batch_eval(session, {'x': x_test, 'y': y_test}, batch_size, epoch, 'test')\n            print('test_loss', test_loss, 'test_acc', test_acc, 'test_hits', test_hits)\n\nYou can play with skip_last. Setting it to False instructs the batcher to create the last batch with size less than batch_size if dataset_size % batch_size > 0. In this case you can see the leak happening in the last batch of the validation set evaluation. Memory consuption goes suddenly from 461.72 MB to 7.41 GB. See a log here:\n2017-11-02 21:18:28.606838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:04:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2017-11-02 21:18:28.606864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\nEpoch 1\nepoch 1 step 1 loss 6.22924\nmbiu 461.85 MB\nepoch 1 step 2 loss 6.27471\nmbiu 461.85 MB\nepoch 1 step 3 loss 6.37816\nmbiu 461.85 MB\n...\nepoch 1 step 55 loss 6.21585\nmbiu 461.85 MB\nepoch 1 step 56 loss 6.24239\nmbiu 461.85 MB\nepoch 1 train eval step 0\nmbiu 461.85 MB\nepoch 1 train eval step 1\nmbiu 461.85 MB\nepoch 1 train eval step 2\nmbiu 461.85 MB\nepoch 1 train eval step 3\nmbiu 461.85 MB\n...\nepoch 1 train eval step 53\nmbiu 461.85 MB\nepoch 1 train eval step 54\nmbiu 461.85 MB\nepoch 1 train eval step 55\nmbiu 461.85 MB\ntrain_loss 0.048836645724 train_acc 0.00281175312808 train_hits 0.00955996063546\nepoch 1 vali eval step 0\nmbiu 461.85 MB\nepoch 1 vali eval step 1\nmbiu 461.85 MB\nepoch 1 vali eval step 2\nmbiu 461.85 MB\nepoch 1 vali eval step 3\nmbiu 461.85 MB\nepoch 1 vali eval step 4\nmbiu 461.85 MB\nepoch 1 vali eval step 5\nmbiu 461.85 MB\nepoch 1 vali eval step 6\nmbiu 461.85 MB\nepoch 1 vali eval step 7\nmbiu 7.41 GB\nvali_loss 0.0538858032227 vali_acc 0.00108108108108 vali_hits 0.00216216216216\nepoch 1 test eval step 0\nmbiu 7.41 GB\nepoch 1 test eval step 1\nmbiu 7.41 GB\n...\nepoch 1 test eval step 13\nmbiu 7.41 GB\nepoch 1 test eval step 14\nmbiu 7.41 GB\nepoch 1 test eval step 15\nmbiu 7.41 GB\ntest_loss 0.0507940473177 test_acc 0.00254841997961 test_hits 0.00917431192661\nEpoch 2\nepoch 2 step 1 loss 6.18541\nmbiu 7.41 GB\nepoch 2 step 2 loss 6.19003\nmbiu 7.41 GB\nepoch 2 step 3 loss 6.18152\nmbiu 7.41 GB\n...\nepoch 2 test eval step 14\nmbiu 7.41 GB\nepoch 2 test eval step 15\nmbiu 7.41 GB\ntest_loss 0.0509010027186 test_acc 0.00254841997961 test_hits 0.00560652395515\n\nIf you set skip_last to True the last betch size with a different size from the usual batch_size will be skipped. As you can see from this log the memory consuption stay constant at 461.72 MB from beginning to the end.\n2017-11-02 21:14:45.784362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:04:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2017-11-02 21:14:45.784395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\nEpoch 1\nepoch 1 step 1 loss 6.22736\nmbiu 461.72 MB\nepoch 1 step 2 loss 6.27873\nmbiu 461.72 MB\nepoch 1 step 3 loss 6.32521\nmbiu 461.72 MB\nepoch 1 step 4 loss 6.34492\nmbiu 461.72 MB\nepoch 1 step 5 loss 6.33192\nmbiu 461.72 MB\nepoch 1 step 6 loss 6.27755\nmbiu 461.72 MB\n...\nepoch 2 test eval step 11\nmbiu 461.78 MB\nepoch 2 test eval step 12\nmbiu 461.78 MB\nepoch 2 test eval step 13\nmbiu 461.78 MB\nepoch 2 test eval step 14\nmbiu 461.78 MB\ntest_loss 0.0475411449125 test_acc 0.00101936799185 test_hits 0.00407747196738\n\nIn the case of this example, the leaked memory is still within the 8 GB limit of my GPU so it doesn't go out of memory, but I have a bigger dataset and using it TF at the last batch of validation tries to allocate 33 GB and goes out of memory. To replicate that setting you should set the parameters at the beginning of the script in this way:\ntrain_set_size = 2896257\nvali_set_size = 97713\ntest_set_size = 97702\n\nHope this is enough.", "body": "Here's a python3 script to reproduce the issue (it's a bit long but I tried to be as faithful to my original setting even if I'm not using a single line of code from my employer):\r\n\r\n```python\r\nimport math\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# parameters\r\nskip_last = False\r\n\r\ntrain_set_size = 7113\r\nvali_set_size = 925\r\ntest_set_size = 1962\r\n\r\ntext_length = 256\r\nvocab_size = 20000\r\nnum_classes = 500\r\n\r\nepochs = 2\r\nbatch_size = 128\r\n\r\n# create artificial dataset\r\nx_train = np.random.randint(0, vocab_size, [train_set_size, text_length])\r\ny_train = np.random.randint(0, num_classes, [train_set_size])\r\nx_vali = np.random.randint(0, vocab_size, [vali_set_size, text_length])\r\ny_vali = np.random.randint(0, num_classes, [vali_set_size])\r\nx_test = np.random.randint(0, vocab_size, [test_set_size, text_length])\r\ny_test = np.random.randint(0, num_classes, [test_set_size])\r\n\r\n\r\n# Building model\r\ndef fc_layer(inputs, in_count, out_count, act=\"relu\"):\r\n    weights = tf.get_variable(\"weights\", [in_count, out_count])\r\n    bias = tf.get_variable(\"bias\", [out_count])\r\n    layer = tf.matmul(inputs, weights) + bias\r\n    if act:\r\n        return getattr(tf.nn, act)(layer)\r\n    else:\r\n        return layer\r\n\r\n\r\ndef conv_layer(inputs, kernel_shape, bias_shape, stride=1, act=\"relu\", padding='VALID', dropout=None, is_training=True):\r\n    weights = tf.get_variable(\"weights\", kernel_shape)\r\n    conv = tf.nn.conv2d(inputs, weights,\r\n                        strides=[1, stride, stride, 1], padding=padding)\r\n    layer = conv\r\n    if act:\r\n        layer = getattr(tf.nn, act)(layer)\r\n    if dropout is not None:\r\n        layer = tf.layers.dropout(layer, rate=dropout, training=is_training)\r\n    return layer\r\n\r\n\r\ndef word_cnn(input_text,\r\n             vocab_size,\r\n             dropout,\r\n             embedding_size=256,\r\n             filter_sizes=(2, 3, 4, 5),\r\n             num_filters=(256, 256, 256, 256),\r\n             fully_connected_sizes=(512, 256),\r\n             regularize_layers=(False, True, True),\r\n             embeddings_oon_cpu=False,\r\n             is_training=True,\r\n             name=None,\r\n             **kwargs):\r\n    assert len(filter_sizes) == len(num_filters)\r\n    assert len(regularize_layers) == len(fully_connected_sizes) + 1\r\n\r\n    sequence_length = input_text.get_shape()[-1]\r\n\r\n    with tf.variable_scope(\"word_cnn_{}\".format(name)):\r\n\r\n        # ================ Word Embeddings ================\r\n        if embeddings_oon_cpu:\r\n            with tf.device('/cpu:0'):\r\n                word_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\r\n                                              name='word_embeddings')\r\n                initial_input = tf.nn.embedding_lookup(word_embeddings, input_text, name='embeddings_lookup')\r\n        else:\r\n            word_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\r\n                                          name='word_embeddings')\r\n            initial_input = tf.nn.embedding_lookup(word_embeddings, input_text, name='embeddings_lookup')\r\n        curr_input = tf.expand_dims(initial_input, -1)\r\n\r\n        # ================ Conv Layers ================\r\n        parallel_conv_layers = []\r\n        for i in range(len(filter_sizes)):\r\n            curr_filter_size = filter_sizes[i]\r\n            curr_num_filters = num_filters[i]\r\n            with tf.variable_scope(\"conv_{}\".format(curr_filter_size)):\r\n                filter_shape = [curr_filter_size, embedding_size, 1, curr_num_filters]\r\n                layer_output = conv_layer(curr_input, filter_shape, [curr_num_filters],\r\n                                          is_training=is_training, dropout=None)\r\n                layer_output = tf.nn.max_pool(\r\n                    layer_output,\r\n                    ksize=[1, sequence_length - curr_filter_size + 1, 1, 1],\r\n                    strides=[1, 1, 1, 1],\r\n                    padding='VALID',\r\n                    name=\"pool_{}\".format(curr_filter_size))\r\n                parallel_conv_layers.append(layer_output)\r\n\r\n        # Flatten to vector\r\n        num_filters_total = sum(num_filters)\r\n        hidden = tf.reshape(tf.concat(parallel_conv_layers, 3), [-1, num_filters_total], name='flatten')\r\n        hidden_size = num_filters_total\r\n\r\n        # ================ Fully Connected Layers ================\r\n        num_fully_connected_layers = len(fully_connected_sizes)\r\n        for i in range(num_fully_connected_layers):\r\n            fully_connected_size = fully_connected_sizes[i]\r\n            with tf.variable_scope(\"fc_{}\".format(i)):\r\n                if dropout is not None:\r\n                    with tf.name_scope(\"dropout\"):\r\n                        hidden = tf.layers.dropout(hidden, rate=dropout, training=is_training)\r\n\r\n                # Fully Connected Layer\r\n                hidden = fc_layer(hidden, hidden_size, fully_connected_size)\r\n                hidden_size = fully_connected_size\r\n\r\n    return hidden, hidden_size\r\n\r\n\r\nis_training = tf.placeholder(tf.bool, [], name='is_training')\r\nglobal_step = tf.Variable(0, trainable=False)\r\ndropout = tf.placeholder(tf.float32, name=\"dropout\")\r\n\r\ninput_placeholder = tf.placeholder(tf.int32, [None, text_length], name=\"input_placeholder\")\r\nlabel_placeholder = tf.placeholder(tf.int64, [None], name=\"label_placeholder\")\r\n\r\nhidden, hidden_size = word_cnn(input_placeholder, vocab_size, dropout=dropout, is_training=is_training, name=\"text\")\r\nsoftmax_w = tf.get_variable(\"softmax_weights\", [hidden_size, num_classes])\r\nsoftmax_b = tf.get_variable(\"softmax_bias\", [num_classes])\r\nlogits = tf.matmul(hidden, softmax_w) + softmax_b\r\n\r\nprobabilities = tf.nn.softmax(logits, name=\"probabilities\")\r\npredictions = tf.argmax(logits, 1, name=\"predictions\")\r\nwith tf.device('/cpu:0'):\r\n    top_k_predictions = tf.nn.top_k(logits, k=3, sorted=True, name=\"top_k_predictions\")\r\n\r\ncorrect_predictions = tf.equal(predictions, label_placeholder, name=\"correct_predictions\")\r\nwith tf.device('/cpu:0'):\r\n    hits_at_k = tf.nn.in_top_k(logits, label_placeholder, 3, name=\"hits_at_k\")\r\n\r\nonehot_labels = tf.one_hot(label_placeholder, num_classes, name=\"onehot_labels\")\r\nloss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\r\n\r\noptimize = tf.train.AdamOptimizer(0.0025).minimize(loss)\r\n\r\n\r\n# training\r\ndef convert_size(size_bytes):\r\n    if size_bytes == 0:\r\n        return \"0B\"\r\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\r\n    i = int(math.floor(math.log(size_bytes, 1024)))\r\n    p = math.pow(1024, i)\r\n    s = round(size_bytes / p, 2)\r\n    return \"{} {}\".format(s, size_name[i])\r\n\r\n\r\nclass Batcher(object):\r\n    def __init__(self, dataset, batch_size, skip_last=False):\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.total_size = min(map(len, dataset.values()))\r\n        self.skip_last = skip_last\r\n        self.index = 0\r\n\r\n    def next_batch(self):\r\n        if self.last_batch():\r\n            self.index = 0\r\n        sub_batch = {}\r\n        for key in self.dataset:\r\n            sub_batch[key] = self.dataset[key][self.index:self.index + self.batch_size]\r\n        self.index += self.batch_size\r\n        return sub_batch\r\n\r\n    def last_batch(self):\r\n        return self.index >= self.total_size or (self.skip_last and self.index + self.batch_size >= self.total_size)\r\n\r\n\r\ndef batch_eval(session, dataset, batch_size, epoch, name):\r\n    batcher = Batcher(dataset, batch_size, skip_last)\r\n    tot_size = batcher.total_size\r\n    losses = []\r\n    correct_preds = []\r\n    hits_at_ks = []\r\n    step = 0\r\n    while not batcher.last_batch():\r\n        batch = batcher.next_batch()\r\n        loss_val, correct_predictions_val, hits_at_k_val = session.run(\r\n            [loss, correct_predictions, hits_at_k],\r\n            feed_dict={input_placeholder: batch['x'], label_placeholder: batch['y'], dropout: 0.3, is_training: True}\r\n        )\r\n        losses.append(loss_val)\r\n        correct_preds.append(correct_predictions_val)\r\n        hits_at_ks.append(hits_at_k_val)\r\n        print('epoch', epoch, name, 'eval step', step)\r\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\r\n        print('mbiu', convert_size(mbiu))\r\n        step += 1\r\n    losses = np.array(losses)\r\n    correct_preds = np.concatenate(correct_preds)\r\n    hits_at_ks = np.concatenate(hits_at_ks)\r\n    return losses.sum() / tot_size, correct_preds.sum() / tot_size, hits_at_ks.sum() / tot_size\r\n\r\n\r\nwith tf.Session() as session:\r\n    tf.global_variables_initializer().run()\r\n\r\n    epoch = 0\r\n    step = 0\r\n    train_batcher = Batcher({'x': x_train, 'y': y_train}, batch_size, skip_last)\r\n\r\n    while epoch < epochs:\r\n        if step == 0:\r\n            print(\"Epoch {}\".format(epoch + 1))\r\n        batch = train_batcher.next_batch()\r\n        _, loss_val = session.run(\r\n            [optimize, loss],\r\n            feed_dict={input_placeholder: batch['x'], label_placeholder: batch['y'], dropout: 0.3, is_training: True}\r\n        )\r\n        print('epoch', epoch + 1, 'step', step + 1, 'loss', loss_val)\r\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\r\n        print('mbiu', convert_size(mbiu))\r\n        step += 1\r\n\r\n        if train_batcher.last_batch():\r\n            epoch += 1\r\n            step = 0\r\n\r\n            train_loss, train_acc, train_hits = batch_eval(session, {'x': x_train, 'y': y_train}, batch_size, epoch,\r\n                                                           'train')\r\n            print('train_loss', train_loss, 'train_acc', train_acc, 'train_hits', train_hits)\r\n\r\n            vali_loss, vali_acc, vali_hits = batch_eval(session, {'x': x_vali, 'y': y_vali}, batch_size, epoch, 'vali')\r\n            print('vali_loss', vali_loss, 'vali_acc', vali_acc, 'vali_hits', vali_hits)\r\n\r\n            test_loss, test_acc, test_hits = batch_eval(session, {'x': x_test, 'y': y_test}, batch_size, epoch, 'test')\r\n            print('test_loss', test_loss, 'test_acc', test_acc, 'test_hits', test_hits)\r\n\r\n```\r\n\r\nYou can play with `skip_last`. Setting it to `False` instructs the batcher to create the last batch with size less than `batch_size` if `dataset_size % batch_size > 0`. In this case you can see the leak happening in the last batch of the validation set evaluation. Memory consuption goes suddenly from 461.72 MB to 7.41 GB. See a log here:\r\n\r\n```\r\n2017-11-02 21:18:28.606838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2017-11-02 21:18:28.606864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nEpoch 1\r\nepoch 1 step 1 loss 6.22924\r\nmbiu 461.85 MB\r\nepoch 1 step 2 loss 6.27471\r\nmbiu 461.85 MB\r\nepoch 1 step 3 loss 6.37816\r\nmbiu 461.85 MB\r\n...\r\nepoch 1 step 55 loss 6.21585\r\nmbiu 461.85 MB\r\nepoch 1 step 56 loss 6.24239\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 0\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 1\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 2\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 3\r\nmbiu 461.85 MB\r\n...\r\nepoch 1 train eval step 53\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 54\r\nmbiu 461.85 MB\r\nepoch 1 train eval step 55\r\nmbiu 461.85 MB\r\ntrain_loss 0.048836645724 train_acc 0.00281175312808 train_hits 0.00955996063546\r\nepoch 1 vali eval step 0\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 1\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 2\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 3\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 4\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 5\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 6\r\nmbiu 461.85 MB\r\nepoch 1 vali eval step 7\r\nmbiu 7.41 GB\r\nvali_loss 0.0538858032227 vali_acc 0.00108108108108 vali_hits 0.00216216216216\r\nepoch 1 test eval step 0\r\nmbiu 7.41 GB\r\nepoch 1 test eval step 1\r\nmbiu 7.41 GB\r\n...\r\nepoch 1 test eval step 13\r\nmbiu 7.41 GB\r\nepoch 1 test eval step 14\r\nmbiu 7.41 GB\r\nepoch 1 test eval step 15\r\nmbiu 7.41 GB\r\ntest_loss 0.0507940473177 test_acc 0.00254841997961 test_hits 0.00917431192661\r\nEpoch 2\r\nepoch 2 step 1 loss 6.18541\r\nmbiu 7.41 GB\r\nepoch 2 step 2 loss 6.19003\r\nmbiu 7.41 GB\r\nepoch 2 step 3 loss 6.18152\r\nmbiu 7.41 GB\r\n...\r\nepoch 2 test eval step 14\r\nmbiu 7.41 GB\r\nepoch 2 test eval step 15\r\nmbiu 7.41 GB\r\ntest_loss 0.0509010027186 test_acc 0.00254841997961 test_hits 0.00560652395515\r\n```\r\n\r\nIf you set `skip_last` to `True` the last betch size with a different size from the usual `batch_size` will be skipped. As you can see from this log the memory consuption stay constant at 461.72 MB from beginning to the end.\r\n\r\n```\r\n2017-11-02 21:14:45.784362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2017-11-02 21:14:45.784395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nEpoch 1\r\nepoch 1 step 1 loss 6.22736\r\nmbiu 461.72 MB\r\nepoch 1 step 2 loss 6.27873\r\nmbiu 461.72 MB\r\nepoch 1 step 3 loss 6.32521\r\nmbiu 461.72 MB\r\nepoch 1 step 4 loss 6.34492\r\nmbiu 461.72 MB\r\nepoch 1 step 5 loss 6.33192\r\nmbiu 461.72 MB\r\nepoch 1 step 6 loss 6.27755\r\nmbiu 461.72 MB\r\n...\r\nepoch 2 test eval step 11\r\nmbiu 461.78 MB\r\nepoch 2 test eval step 12\r\nmbiu 461.78 MB\r\nepoch 2 test eval step 13\r\nmbiu 461.78 MB\r\nepoch 2 test eval step 14\r\nmbiu 461.78 MB\r\ntest_loss 0.0475411449125 test_acc 0.00101936799185 test_hits 0.00407747196738\r\n```\r\n\r\nIn the case of this example, the leaked memory is still within the 8 GB limit of my GPU so it doesn't go out of memory, but I have a bigger dataset and using it TF at the last batch of validation tries to allocate 33 GB and goes out of memory. To replicate that setting you should set the parameters at the beginning of the script in this way:\r\n\r\n```\r\ntrain_set_size = 2896257\r\nvali_set_size = 97713\r\ntest_set_size = 97702\r\n```\r\n\r\nHope this is enough."}