{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364536642", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171#issuecomment-364536642", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "id": 364536642, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDUzNjY0Mg==", "user": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T19:28:58Z", "updated_at": "2018-02-09T19:28:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=349256\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/w4nderlust\">@w4nderlust</a> We had a discussion offline. So as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> just mentioned, autotune could use a large chunk of memory in two cases. In the measurement phase, I think he has explained it very well why there shouldn't be an OOM or any other failure. Just to provide you more info as to help you understand what happens in the execution phase:<br>\nThere will be two algorithms recorded in autotune: the primary one which uses scratch memory, a secondary one which uses zero scratch memory, and regarding cuDNN's documentation, as long as the input shape and other parameters are legal, there should always be a default algorithm without scratch memory working fine.<br>\nSuppose autotune find a best performing algorithm to use but in the execution phase it fails to allocate the scratch memory, we get a warning, but not a fatal error:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2400\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2400</a><br>\nThen in the following few lines, we will fall back to that default algorithm that uses no scratch memory. If an error happens there, it means one of two things:</p>\n<ol>\n<li>there is a cuDNN bug, since the default algorithm doesn't work for a legal set of parameters;</li>\n<li>we somehow fail to set the default algorithm for a legal set of parameters that should have a default algorithm.<br>\nIn either case, we would be happy to fix that for you.</li>\n</ol>\n<p>My suggestion for you is to try more batch size for validation, and see if there will be any error happening during the execution phase of conv. Note that if an OOM happens before the execution, it is the intended behavior as it would imply that TF doesn't have enough memory to even hold the input tensor. If you see error during the execution phase of conv, please report back and we are committed to fix that for you.</p>\n<p>Also note that MaxBytesInUse() is a watermark. So it doesn't reflect the current memory in use, it only reflects the largest memory in use ever. So it could be a large one as long as one of the internal algorithm needs a large chunk of scratch memory.</p>", "body_text": "@w4nderlust We had a discussion offline. So as @zheng-xq just mentioned, autotune could use a large chunk of memory in two cases. In the measurement phase, I think he has explained it very well why there shouldn't be an OOM or any other failure. Just to provide you more info as to help you understand what happens in the execution phase:\nThere will be two algorithms recorded in autotune: the primary one which uses scratch memory, a secondary one which uses zero scratch memory, and regarding cuDNN's documentation, as long as the input shape and other parameters are legal, there should always be a default algorithm without scratch memory working fine.\nSuppose autotune find a best performing algorithm to use but in the execution phase it fails to allocate the scratch memory, we get a warning, but not a fatal error:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2400\nThen in the following few lines, we will fall back to that default algorithm that uses no scratch memory. If an error happens there, it means one of two things:\n\nthere is a cuDNN bug, since the default algorithm doesn't work for a legal set of parameters;\nwe somehow fail to set the default algorithm for a legal set of parameters that should have a default algorithm.\nIn either case, we would be happy to fix that for you.\n\nMy suggestion for you is to try more batch size for validation, and see if there will be any error happening during the execution phase of conv. Note that if an OOM happens before the execution, it is the intended behavior as it would imply that TF doesn't have enough memory to even hold the input tensor. If you see error during the execution phase of conv, please report back and we are committed to fix that for you.\nAlso note that MaxBytesInUse() is a watermark. So it doesn't reflect the current memory in use, it only reflects the largest memory in use ever. So it could be a large one as long as one of the internal algorithm needs a large chunk of scratch memory.", "body": "@w4nderlust We had a discussion offline. So as @zheng-xq just mentioned, autotune could use a large chunk of memory in two cases. In the measurement phase, I think he has explained it very well why there shouldn't be an OOM or any other failure. Just to provide you more info as to help you understand what happens in the execution phase:\r\nThere will be two algorithms recorded in autotune: the primary one which uses scratch memory, a secondary one which uses zero scratch memory, and regarding cuDNN's documentation, as long as the input shape and other parameters are legal, there should always be a default algorithm without scratch memory working fine.\r\nSuppose autotune find a best performing algorithm to use but in the execution phase it fails to allocate the scratch memory, we get a warning, but not a fatal error:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2400\r\nThen in the following few lines, we will fall back to that default algorithm that uses no scratch memory. If an error happens there, it means one of two things:\r\n1) there is a cuDNN bug, since the default algorithm doesn't work for a legal set of parameters;\r\n2) we somehow fail to set the default algorithm for a legal set of parameters that should have a default algorithm.\r\nIn either case, we would be happy to fix that for you.\r\n\r\nMy suggestion for you is to try more batch size for validation, and see if there will be any error happening during the execution phase of conv. Note that if an OOM happens before the execution, it is the intended behavior as it would imply that TF doesn't have enough memory to even hold the input tensor. If you see error during the execution phase of conv, please report back and we are committed to fix that for you.\r\n\r\nAlso note that MaxBytesInUse() is a watermark. So it doesn't reflect the current memory in use, it only reflects the largest memory in use ever. So it could be a large one as long as one of the internal algorithm needs a large chunk of scratch memory."}