{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/363981191", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171#issuecomment-363981191", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "id": 363981191, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Mzk4MTE5MQ==", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-08T02:22:09Z", "updated_at": "2018-02-08T02:23:22Z", "author_association": "NONE", "body_html": "<p>Hey <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=88808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skye\">@skye</a>, here's a much shorter version of the script, only 90 lines:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> math\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> parameters</span>\nskip_last <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\nsequence_length <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\nvocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20000</span>\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">500</span>\ntrain_set_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">925</span>\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n\nembedding_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\nfilter_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\nnum_filters <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create artificial dataset</span>\nx_train <span class=\"pl-k\">=</span> np.random.rand(train_set_size, sequence_length, embedding_size, <span class=\"pl-c1\">1</span>)\ny_train <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, num_classes, [train_set_size])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Model</span>\ninput_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, sequence_length, embedding_size, <span class=\"pl-c1\">1</span>])\nlabel_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, [<span class=\"pl-c1\">None</span>])\nfilter_shape <span class=\"pl-k\">=</span> [filter_size, embedding_size, <span class=\"pl-c1\">1</span>, num_filters]\nweights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>conv_w<span class=\"pl-pds\">\"</span></span>, filter_shape)\nconv <span class=\"pl-k\">=</span> tf.nn.conv2d(input_placeholder, weights, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>)\nconv <span class=\"pl-k\">=</span> tf.nn.relu(conv)\npooled <span class=\"pl-k\">=</span> tf.nn.max_pool(conv, [<span class=\"pl-c1\">1</span>, sequence_length <span class=\"pl-k\">-</span> filter_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>)\nhidden <span class=\"pl-k\">=</span> tf.reshape(pooled, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, num_filters])\nsoftmax_w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax_w<span class=\"pl-pds\">\"</span></span>, [num_filters, num_classes])\nlogits <span class=\"pl-k\">=</span> tf.matmul(hidden, softmax_w)\nprobabilities <span class=\"pl-k\">=</span> tf.nn.softmax(logits)\npredictions <span class=\"pl-k\">=</span> tf.argmax(logits, <span class=\"pl-c1\">1</span>)\ncorrect_predictions <span class=\"pl-k\">=</span> tf.equal(predictions, label_placeholder)\nloss <span class=\"pl-k\">=</span> tf.losses.sparse_softmax_cross_entropy(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>label_placeholder, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\noptimize <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.0025</span>).minimize(loss)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> training</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">convert_size</span>(<span class=\"pl-smi\">size_bytes</span>):\n    <span class=\"pl-k\">if</span> size_bytes <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>0B<span class=\"pl-pds\">\"</span></span>\n    size_name <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>B<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>KB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>GB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>PB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>EB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ZB<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>YB<span class=\"pl-pds\">\"</span></span>)\n    i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(math.floor(math.log(size_bytes, <span class=\"pl-c1\">1024</span>)))\n    p <span class=\"pl-k\">=</span> math.pow(<span class=\"pl-c1\">1024</span>, i)\n    s <span class=\"pl-k\">=</span> <span class=\"pl-c1\">round</span>(size_bytes <span class=\"pl-k\">/</span> p, <span class=\"pl-c1\">2</span>)\n    <span class=\"pl-k\">return</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{}</span> <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(s, size_name[i])\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Batcher</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">skip_last</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(x) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">len</span>(y)\n        <span class=\"pl-c1\">self</span>.x <span class=\"pl-k\">=</span> x\n        <span class=\"pl-c1\">self</span>.y <span class=\"pl-k\">=</span> y\n        <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> batch_size\n        <span class=\"pl-c1\">self</span>.total_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(x)\n        <span class=\"pl-c1\">self</span>.skip_last <span class=\"pl-k\">=</span> skip_last\n        <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">next_batch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.last_batch():\n            <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        x_batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.x[<span class=\"pl-c1\">self</span>.index:<span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.batch_size]\n        y_batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.y[<span class=\"pl-c1\">self</span>.index:<span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.batch_size]\n        <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">self</span>.batch_size\n        <span class=\"pl-k\">return</span> x_batch, y_batch\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">last_batch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">self</span>.total_size <span class=\"pl-k\">or</span> (<span class=\"pl-c1\">self</span>.skip_last <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.index <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">self</span>.total_size)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> session:\n    tf.global_variables_initializer().run()\n    epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    batcher <span class=\"pl-k\">=</span> Batcher(x_train, y_train, batch_size, skip_last)\n    <span class=\"pl-k\">while</span> epoch <span class=\"pl-k\">&lt;</span> epochs:\n        batch_x, batch_y <span class=\"pl-k\">=</span> batcher.next_batch()\n        _, loss_val <span class=\"pl-k\">=</span> session.run(\n            [optimize, loss],\n            <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{input_placeholder: batch_x, label_placeholder: batch_y}\n        )\n        mbiu <span class=\"pl-k\">=</span> session.run(tf.contrib.memory_stats.MaxBytesInUse())\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>epoch: <span class=\"pl-c1\">{<span class=\"pl-k\">:1d</span>}</span> step: <span class=\"pl-c1\">{<span class=\"pl-k\">:2d</span>}</span> loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span> mbiu: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(\n            epoch <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, step <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, loss_val, convert_size(mbiu)))\n        <span class=\"pl-k\">if</span> batcher.last_batch():\n            epoch <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n            step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">else</span>:\n            step <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span></pre></div>\n<p>Here is the output I'm getting from it:</p>\n<pre><code>epoch: 1 step:  1 loss: 6.2207 mbiu: 260.71 MB\nepoch: 1 step:  2 loss: 6.6852 mbiu: 260.71 MB\nepoch: 1 step:  3 loss: 6.5704 mbiu: 260.71 MB\nepoch: 1 step:  4 loss: 6.4245 mbiu: 260.71 MB\nepoch: 1 step:  5 loss: 6.2299 mbiu: 260.71 MB\nepoch: 1 step:  6 loss: 6.2337 mbiu: 260.71 MB\nepoch: 1 step:  7 loss: 6.2340 mbiu: 260.71 MB\nepoch: 1 step:  8 loss: 6.2334 mbiu: 3.7 GB\nepoch: 2 step:  1 loss: 6.2051 mbiu: 3.7 GB\nepoch: 2 step:  2 loss: 6.1781 mbiu: 3.7 GB\nepoch: 2 step:  3 loss: 6.1723 mbiu: 3.7 GB\nepoch: 2 step:  4 loss: 6.1483 mbiu: 3.7 GB\nepoch: 2 step:  5 loss: 6.1409 mbiu: 3.7 GB\nepoch: 2 step:  6 loss: 6.2119 mbiu: 3.7 GB\nepoch: 2 step:  7 loss: 6.2550 mbiu: 3.7 GB\nepoch: 2 step:  8 loss: 6.1255 mbiu: 3.7 GB\n</code></pre>\n<p>As you can see the memory usage increases A LOT, more than 10x the regular usage, at the end of the epoch, when a sample with a different batch size is pushed through the graph. If I set <code>skip_last</code> to <code>True</code>, meaning the last and shorter batch is skipped, the memory usage keeps constant. This suggests there's a memory leak in <code>conv2d()</code> when dealing with batch sizes that are different than the initial one.<br>\nAs this scenario is ubiquitous, it seems to me that this should be a pretty critical thing to fix rather than wait for community contributions.</p>", "body_text": "Hey @skye, here's a much shorter version of the script, only 90 lines:\nimport math\nimport numpy as np\nimport tensorflow as tf\n\n# parameters\nskip_last = False\n\nsequence_length = 256\nvocab_size = 20000\nnum_classes = 500\ntrain_set_size = 925\nepochs = 2\nbatch_size = 128\n\nembedding_size = 256\nfilter_size = 5\nnum_filters = 256\n\n# create artificial dataset\nx_train = np.random.rand(train_set_size, sequence_length, embedding_size, 1)\ny_train = np.random.randint(0, num_classes, [train_set_size])\n\n# Model\ninput_placeholder = tf.placeholder(tf.float32, [None, sequence_length, embedding_size, 1])\nlabel_placeholder = tf.placeholder(tf.int64, [None])\nfilter_shape = [filter_size, embedding_size, 1, num_filters]\nweights = tf.get_variable(\"conv_w\", filter_shape)\nconv = tf.nn.conv2d(input_placeholder, weights, [1, 1, 1, 1], 'VALID')\nconv = tf.nn.relu(conv)\npooled = tf.nn.max_pool(conv, [1, sequence_length - filter_size + 1, 1, 1], [1, 1, 1, 1], 'VALID')\nhidden = tf.reshape(pooled, [-1, num_filters])\nsoftmax_w = tf.get_variable(\"softmax_w\", [num_filters, num_classes])\nlogits = tf.matmul(hidden, softmax_w)\nprobabilities = tf.nn.softmax(logits)\npredictions = tf.argmax(logits, 1)\ncorrect_predictions = tf.equal(predictions, label_placeholder)\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label_placeholder, logits=logits)\noptimize = tf.train.AdamOptimizer(0.0025).minimize(loss)\n\n# training\ndef convert_size(size_bytes):\n    if size_bytes == 0:\n        return \"0B\"\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n    i = int(math.floor(math.log(size_bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(size_bytes / p, 2)\n    return \"{} {}\".format(s, size_name[i])\n\nclass Batcher(object):\n    def __init__(self, x, y, batch_size, skip_last=False):\n        assert len(x) == len(y)\n        self.x = x\n        self.y = y\n        self.batch_size = batch_size\n        self.total_size = len(x)\n        self.skip_last = skip_last\n        self.index = 0\n\n    def next_batch(self):\n        if self.last_batch():\n            self.index = 0\n        x_batch = self.x[self.index:self.index + self.batch_size]\n        y_batch = self.y[self.index:self.index + self.batch_size]\n        self.index += self.batch_size\n        return x_batch, y_batch\n\n    def last_batch(self):\n        return self.index >= self.total_size or (self.skip_last and self.index + self.batch_size >= self.total_size)\n\nwith tf.Session() as session:\n    tf.global_variables_initializer().run()\n    epoch = 0\n    step = 0\n    batcher = Batcher(x_train, y_train, batch_size, skip_last)\n    while epoch < epochs:\n        batch_x, batch_y = batcher.next_batch()\n        _, loss_val = session.run(\n            [optimize, loss],\n            feed_dict={input_placeholder: batch_x, label_placeholder: batch_y}\n        )\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\n        print('epoch: {:1d} step: {:2d} loss: {:.4f} mbiu: {}'.format(\n            epoch + 1, step + 1, loss_val, convert_size(mbiu)))\n        if batcher.last_batch():\n            epoch += 1\n            step = 0\n        else:\n            step += 1\nHere is the output I'm getting from it:\nepoch: 1 step:  1 loss: 6.2207 mbiu: 260.71 MB\nepoch: 1 step:  2 loss: 6.6852 mbiu: 260.71 MB\nepoch: 1 step:  3 loss: 6.5704 mbiu: 260.71 MB\nepoch: 1 step:  4 loss: 6.4245 mbiu: 260.71 MB\nepoch: 1 step:  5 loss: 6.2299 mbiu: 260.71 MB\nepoch: 1 step:  6 loss: 6.2337 mbiu: 260.71 MB\nepoch: 1 step:  7 loss: 6.2340 mbiu: 260.71 MB\nepoch: 1 step:  8 loss: 6.2334 mbiu: 3.7 GB\nepoch: 2 step:  1 loss: 6.2051 mbiu: 3.7 GB\nepoch: 2 step:  2 loss: 6.1781 mbiu: 3.7 GB\nepoch: 2 step:  3 loss: 6.1723 mbiu: 3.7 GB\nepoch: 2 step:  4 loss: 6.1483 mbiu: 3.7 GB\nepoch: 2 step:  5 loss: 6.1409 mbiu: 3.7 GB\nepoch: 2 step:  6 loss: 6.2119 mbiu: 3.7 GB\nepoch: 2 step:  7 loss: 6.2550 mbiu: 3.7 GB\nepoch: 2 step:  8 loss: 6.1255 mbiu: 3.7 GB\n\nAs you can see the memory usage increases A LOT, more than 10x the regular usage, at the end of the epoch, when a sample with a different batch size is pushed through the graph. If I set skip_last to True, meaning the last and shorter batch is skipped, the memory usage keeps constant. This suggests there's a memory leak in conv2d() when dealing with batch sizes that are different than the initial one.\nAs this scenario is ubiquitous, it seems to me that this should be a pretty critical thing to fix rather than wait for community contributions.", "body": "Hey @skye, here's a much shorter version of the script, only 90 lines:\r\n\r\n```python\r\nimport math\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# parameters\r\nskip_last = False\r\n\r\nsequence_length = 256\r\nvocab_size = 20000\r\nnum_classes = 500\r\ntrain_set_size = 925\r\nepochs = 2\r\nbatch_size = 128\r\n\r\nembedding_size = 256\r\nfilter_size = 5\r\nnum_filters = 256\r\n\r\n# create artificial dataset\r\nx_train = np.random.rand(train_set_size, sequence_length, embedding_size, 1)\r\ny_train = np.random.randint(0, num_classes, [train_set_size])\r\n\r\n# Model\r\ninput_placeholder = tf.placeholder(tf.float32, [None, sequence_length, embedding_size, 1])\r\nlabel_placeholder = tf.placeholder(tf.int64, [None])\r\nfilter_shape = [filter_size, embedding_size, 1, num_filters]\r\nweights = tf.get_variable(\"conv_w\", filter_shape)\r\nconv = tf.nn.conv2d(input_placeholder, weights, [1, 1, 1, 1], 'VALID')\r\nconv = tf.nn.relu(conv)\r\npooled = tf.nn.max_pool(conv, [1, sequence_length - filter_size + 1, 1, 1], [1, 1, 1, 1], 'VALID')\r\nhidden = tf.reshape(pooled, [-1, num_filters])\r\nsoftmax_w = tf.get_variable(\"softmax_w\", [num_filters, num_classes])\r\nlogits = tf.matmul(hidden, softmax_w)\r\nprobabilities = tf.nn.softmax(logits)\r\npredictions = tf.argmax(logits, 1)\r\ncorrect_predictions = tf.equal(predictions, label_placeholder)\r\nloss = tf.losses.sparse_softmax_cross_entropy(labels=label_placeholder, logits=logits)\r\noptimize = tf.train.AdamOptimizer(0.0025).minimize(loss)\r\n\r\n# training\r\ndef convert_size(size_bytes):\r\n    if size_bytes == 0:\r\n        return \"0B\"\r\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\r\n    i = int(math.floor(math.log(size_bytes, 1024)))\r\n    p = math.pow(1024, i)\r\n    s = round(size_bytes / p, 2)\r\n    return \"{} {}\".format(s, size_name[i])\r\n\r\nclass Batcher(object):\r\n    def __init__(self, x, y, batch_size, skip_last=False):\r\n        assert len(x) == len(y)\r\n        self.x = x\r\n        self.y = y\r\n        self.batch_size = batch_size\r\n        self.total_size = len(x)\r\n        self.skip_last = skip_last\r\n        self.index = 0\r\n\r\n    def next_batch(self):\r\n        if self.last_batch():\r\n            self.index = 0\r\n        x_batch = self.x[self.index:self.index + self.batch_size]\r\n        y_batch = self.y[self.index:self.index + self.batch_size]\r\n        self.index += self.batch_size\r\n        return x_batch, y_batch\r\n\r\n    def last_batch(self):\r\n        return self.index >= self.total_size or (self.skip_last and self.index + self.batch_size >= self.total_size)\r\n\r\nwith tf.Session() as session:\r\n    tf.global_variables_initializer().run()\r\n    epoch = 0\r\n    step = 0\r\n    batcher = Batcher(x_train, y_train, batch_size, skip_last)\r\n    while epoch < epochs:\r\n        batch_x, batch_y = batcher.next_batch()\r\n        _, loss_val = session.run(\r\n            [optimize, loss],\r\n            feed_dict={input_placeholder: batch_x, label_placeholder: batch_y}\r\n        )\r\n        mbiu = session.run(tf.contrib.memory_stats.MaxBytesInUse())\r\n        print('epoch: {:1d} step: {:2d} loss: {:.4f} mbiu: {}'.format(\r\n            epoch + 1, step + 1, loss_val, convert_size(mbiu)))\r\n        if batcher.last_batch():\r\n            epoch += 1\r\n            step = 0\r\n        else:\r\n            step += 1\r\n```\r\n\r\nHere is the output I'm getting from it:\r\n```\r\nepoch: 1 step:  1 loss: 6.2207 mbiu: 260.71 MB\r\nepoch: 1 step:  2 loss: 6.6852 mbiu: 260.71 MB\r\nepoch: 1 step:  3 loss: 6.5704 mbiu: 260.71 MB\r\nepoch: 1 step:  4 loss: 6.4245 mbiu: 260.71 MB\r\nepoch: 1 step:  5 loss: 6.2299 mbiu: 260.71 MB\r\nepoch: 1 step:  6 loss: 6.2337 mbiu: 260.71 MB\r\nepoch: 1 step:  7 loss: 6.2340 mbiu: 260.71 MB\r\nepoch: 1 step:  8 loss: 6.2334 mbiu: 3.7 GB\r\nepoch: 2 step:  1 loss: 6.2051 mbiu: 3.7 GB\r\nepoch: 2 step:  2 loss: 6.1781 mbiu: 3.7 GB\r\nepoch: 2 step:  3 loss: 6.1723 mbiu: 3.7 GB\r\nepoch: 2 step:  4 loss: 6.1483 mbiu: 3.7 GB\r\nepoch: 2 step:  5 loss: 6.1409 mbiu: 3.7 GB\r\nepoch: 2 step:  6 loss: 6.2119 mbiu: 3.7 GB\r\nepoch: 2 step:  7 loss: 6.2550 mbiu: 3.7 GB\r\nepoch: 2 step:  8 loss: 6.1255 mbiu: 3.7 GB\r\n```\r\n\r\nAs you can see the memory usage increases A LOT, more than 10x the regular usage, at the end of the epoch, when a sample with a different batch size is pushed through the graph. If I set `skip_last` to `True`, meaning the last and shorter batch is skipped, the memory usage keeps constant. This suggests there's a memory leak in `conv2d()` when dealing with batch sizes that are different than the initial one.\r\nAs this scenario is ubiquitous, it seems to me that this should be a pretty critical thing to fix rather than wait for community contributions."}