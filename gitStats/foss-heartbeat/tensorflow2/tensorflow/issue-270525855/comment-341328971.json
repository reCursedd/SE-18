{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341328971", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171#issuecomment-341328971", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "id": 341328971, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTMyODk3MQ==", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-02T06:24:50Z", "updated_at": "2017-11-02T06:28:13Z", "author_association": "NONE", "body_html": "<p>Hey jart, thanks for you answer. (I use you fabulous lib, it\u2019s amazing!)</p>\n<p>I spent 4 days debugging this issue in my code and I ruled out all other possibilities. I looked at the datapoints in the last batches one by one and also reshuffled all the datasets to be sure it was not a data related issue. Unfortunately no, it doesn\u2019t seem to me that it is an issue related with loosing track of the sizes of the tensors, also because the last batch is actually smaller than the other batches, so the operation should actually use less memory and not more.</p>\n<p>I realize that <code>conv2d()</code> is second only to <code>add</code> and <code>matmul</code> in terms of usage, so if no one encountered this issue before it could be that it happens to me because of a weird combination of os+python+gpu+nvidia software version, but I believe it\u2019s worth investigating nonetheless.</p>\n<p>I can try to extract a subset of my codebase and generate some synthetic data with the same dimensions of the offending dataset, but it will require me some time. I wouldn\u2019t be able to do it before the weekend.</p>", "body_text": "Hey jart, thanks for you answer. (I use you fabulous lib, it\u2019s amazing!)\nI spent 4 days debugging this issue in my code and I ruled out all other possibilities. I looked at the datapoints in the last batches one by one and also reshuffled all the datasets to be sure it was not a data related issue. Unfortunately no, it doesn\u2019t seem to me that it is an issue related with loosing track of the sizes of the tensors, also because the last batch is actually smaller than the other batches, so the operation should actually use less memory and not more.\nI realize that conv2d() is second only to add and matmul in terms of usage, so if no one encountered this issue before it could be that it happens to me because of a weird combination of os+python+gpu+nvidia software version, but I believe it\u2019s worth investigating nonetheless.\nI can try to extract a subset of my codebase and generate some synthetic data with the same dimensions of the offending dataset, but it will require me some time. I wouldn\u2019t be able to do it before the weekend.", "body": "Hey jart, thanks for you answer. (I use you fabulous lib, it\u2019s amazing!)\r\n\r\nI spent 4 days debugging this issue in my code and I ruled out all other possibilities. I looked at the datapoints in the last batches one by one and also reshuffled all the datasets to be sure it was not a data related issue. Unfortunately no, it doesn\u2019t seem to me that it is an issue related with loosing track of the sizes of the tensors, also because the last batch is actually smaller than the other batches, so the operation should actually use less memory and not more.\r\n\r\nI realize that `conv2d()` is second only to `add` and `matmul` in terms of usage, so if no one encountered this issue before it could be that it happens to me because of a weird combination of os+python+gpu+nvidia software version, but I believe it\u2019s worth investigating nonetheless.\r\n\r\nI can try to extract a subset of my codebase and generate some synthetic data with the same dimensions of the offending dataset, but it will require me some time. I wouldn\u2019t be able to do it before the weekend."}