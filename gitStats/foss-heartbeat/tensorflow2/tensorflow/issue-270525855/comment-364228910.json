{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364228910", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171#issuecomment-364228910", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "id": 364228910, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDIyODkxMA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-08T19:54:20Z", "updated_at": "2018-02-08T19:54:20Z", "author_association": "MEMBER", "body_html": "<p>Actually I just realized that MaxBytesInUse stays at 3766 MB, since it is never reset, so once it reaches 3766 MB it will never go down.</p>\n<p>So, it seems the issue is that a batch size of 29 uses more memory than a batch size of 128. My guess is that with a batch size of 29, the faster algorithm is different than with a batch size of 128. The batch-size-29 algorithm probably uses a lot more memory than the batch-size-128 algorithm, which explains the difference</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> can you confirm?</p>", "body_text": "Actually I just realized that MaxBytesInUse stays at 3766 MB, since it is never reset, so once it reaches 3766 MB it will never go down.\nSo, it seems the issue is that a batch size of 29 uses more memory than a batch size of 128. My guess is that with a batch size of 29, the faster algorithm is different than with a batch size of 128. The batch-size-29 algorithm probably uses a lot more memory than the batch-size-128 algorithm, which explains the difference\n@yzhwang can you confirm?", "body": "Actually I just realized that MaxBytesInUse stays at 3766 MB, since it is never reset, so once it reaches 3766 MB it will never go down.\r\n\r\nSo, it seems the issue is that a batch size of 29 uses more memory than a batch size of 128. My guess is that with a batch size of 29, the faster algorithm is different than with a batch size of 128. The batch-size-29 algorithm probably uses a lot more memory than the batch-size-128 algorithm, which explains the difference\r\n\r\n@yzhwang can you confirm?"}