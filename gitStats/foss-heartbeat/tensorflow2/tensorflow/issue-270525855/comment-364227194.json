{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364227194", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171#issuecomment-364227194", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "id": 364227194, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDIyNzE5NA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-08T19:47:44Z", "updated_at": "2018-02-08T19:47:44Z", "author_association": "MEMBER", "body_html": "<p>Here is a simpler example the reproduces the problem</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nsequence_length <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\nembedding_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\nfilter_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\nnum_filters <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Model</span>\ninput_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, sequence_length, embedding_size, <span class=\"pl-c1\">1</span>])\nlabel_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, [<span class=\"pl-c1\">None</span>])\nfilter_shape <span class=\"pl-k\">=</span> [filter_size, embedding_size, <span class=\"pl-c1\">1</span>, num_filters]\nweights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>conv_w<span class=\"pl-pds\">\"</span></span>, filter_shape)\nconv <span class=\"pl-k\">=</span> tf.nn.conv2d(input_placeholder, weights, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> session:\n  tf.global_variables_initializer().run()\n  sizes <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">128</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">+</span> [<span class=\"pl-c1\">29</span>] <span class=\"pl-k\">+</span> [<span class=\"pl-c1\">128</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>\n  <span class=\"pl-k\">for</span> size <span class=\"pl-k\">in</span> sizes:\n    batch_x <span class=\"pl-k\">=</span> np.zeros([size, sequence_length, embedding_size, <span class=\"pl-c1\">1</span>])\n    batch_y <span class=\"pl-k\">=</span> np.zeros([size])\n    loss_val <span class=\"pl-k\">=</span> session.run(\n        conv.op,\n        <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{input_placeholder: batch_x, label_placeholder: batch_y}\n    )\n    mbiu, biu <span class=\"pl-k\">=</span> session.run([tf.contrib.memory_stats.MaxBytesInUse(),\n                             tf.contrib.memory_stats.BytesInUse()])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mbiu: <span class=\"pl-c1\">{}</span> MB, bui: <span class=\"pl-c1\">{}</span> MB<span class=\"pl-pds\">'</span></span>.format(mbiu <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">20</span>, biu <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">20</span>))</pre></div>\n<p>The output is:</p>\n<pre><code>mbiu: 255 MB, bui: 1 MB\nmbiu: 255 MB, bui: 1 MB\nmbiu: 3766 MB, bui: 1 MB\nmbiu: 3766 MB, bui: 1 MB\nmbiu: 3766 MB, bui: 1 MB\n</code></pre>\n<p>This is no memory leak, because the current bytes-in-use always goes back to 1MB after the session finishes running. But it is weird how running a single step with a batch-size of 29 causes future steps with a batch size of 128 to use more memory.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> can you take a look? My post has all the info of the root problem, so I don't think you need to read the previous posts. Is this an issue with autotune?</p>", "body_text": "Here is a simpler example the reproduces the problem\nimport numpy as np\nimport tensorflow as tf\n\nsequence_length = 256\nembedding_size = 256\nfilter_size = 5\nnum_filters = 256\n\n# Model\ninput_placeholder = tf.placeholder(tf.float32, [None, sequence_length, embedding_size, 1])\nlabel_placeholder = tf.placeholder(tf.int64, [None])\nfilter_shape = [filter_size, embedding_size, 1, num_filters]\nweights = tf.get_variable(\"conv_w\", filter_shape)\nconv = tf.nn.conv2d(input_placeholder, weights, [1, 1, 1, 1], 'VALID')\n\nwith tf.Session() as session:\n  tf.global_variables_initializer().run()\n  sizes = [128] * 2 + [29] + [128] * 2\n  for size in sizes:\n    batch_x = np.zeros([size, sequence_length, embedding_size, 1])\n    batch_y = np.zeros([size])\n    loss_val = session.run(\n        conv.op,\n        feed_dict={input_placeholder: batch_x, label_placeholder: batch_y}\n    )\n    mbiu, biu = session.run([tf.contrib.memory_stats.MaxBytesInUse(),\n                             tf.contrib.memory_stats.BytesInUse()])\n    print('mbiu: {} MB, bui: {} MB'.format(mbiu / 2**20, biu / 2**20))\nThe output is:\nmbiu: 255 MB, bui: 1 MB\nmbiu: 255 MB, bui: 1 MB\nmbiu: 3766 MB, bui: 1 MB\nmbiu: 3766 MB, bui: 1 MB\nmbiu: 3766 MB, bui: 1 MB\n\nThis is no memory leak, because the current bytes-in-use always goes back to 1MB after the session finishes running. But it is weird how running a single step with a batch-size of 29 causes future steps with a batch size of 128 to use more memory.\n@yzhwang can you take a look? My post has all the info of the root problem, so I don't think you need to read the previous posts. Is this an issue with autotune?", "body": "Here is a simpler example the reproduces the problem\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsequence_length = 256\r\nembedding_size = 256\r\nfilter_size = 5\r\nnum_filters = 256\r\n\r\n# Model\r\ninput_placeholder = tf.placeholder(tf.float32, [None, sequence_length, embedding_size, 1])\r\nlabel_placeholder = tf.placeholder(tf.int64, [None])\r\nfilter_shape = [filter_size, embedding_size, 1, num_filters]\r\nweights = tf.get_variable(\"conv_w\", filter_shape)\r\nconv = tf.nn.conv2d(input_placeholder, weights, [1, 1, 1, 1], 'VALID')\r\n\r\nwith tf.Session() as session:\r\n  tf.global_variables_initializer().run()\r\n  sizes = [128] * 2 + [29] + [128] * 2\r\n  for size in sizes:\r\n    batch_x = np.zeros([size, sequence_length, embedding_size, 1])\r\n    batch_y = np.zeros([size])\r\n    loss_val = session.run(\r\n        conv.op,\r\n        feed_dict={input_placeholder: batch_x, label_placeholder: batch_y}\r\n    )\r\n    mbiu, biu = session.run([tf.contrib.memory_stats.MaxBytesInUse(),\r\n                             tf.contrib.memory_stats.BytesInUse()])\r\n    print('mbiu: {} MB, bui: {} MB'.format(mbiu / 2**20, biu / 2**20))\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\nmbiu: 255 MB, bui: 1 MB\r\nmbiu: 255 MB, bui: 1 MB\r\nmbiu: 3766 MB, bui: 1 MB\r\nmbiu: 3766 MB, bui: 1 MB\r\nmbiu: 3766 MB, bui: 1 MB\r\n```\r\n\r\nThis is no memory leak, because the current bytes-in-use always goes back to 1MB after the session finishes running. But it is weird how running a single step with a batch-size of 29 causes future steps with a batch size of 128 to use more memory.\r\n\r\n@yzhwang can you take a look? My post has all the info of the root problem, so I don't think you need to read the previous posts. Is this an issue with autotune?"}