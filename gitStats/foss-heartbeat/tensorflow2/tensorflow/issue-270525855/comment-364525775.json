{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364525775", "html_url": "https://github.com/tensorflow/tensorflow/issues/14171#issuecomment-364525775", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14171", "id": 364525775, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDUyNTc3NQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T18:57:12Z", "updated_at": "2018-02-09T18:57:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am suspecting something weird is happening. With autotuning, it is not supposed to use much more memory than if autotune is disabled</p>\n<ol>\n<li>During the measurement phase, if an algorithm failed to allocate memory, it is not a fatal error. TF just removes that algorithm from candidacy.</li>\n<li>During the execution phase, even if the op uses scratch memory, it almost immediately releases for later. Only concurrent kernels might be impacted by this. But even with that, other ops should retry and as if no extra memory was used.</li>\n</ol>\n<p>Something might not be following this protocol. Disabling autotune has major performance impact. Also it should have too much memory overhead. So let's root cause this first.</p>\n<p>Reopening the issue and ask <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> to take a look.</p>", "body_text": "I am suspecting something weird is happening. With autotuning, it is not supposed to use much more memory than if autotune is disabled\n\nDuring the measurement phase, if an algorithm failed to allocate memory, it is not a fatal error. TF just removes that algorithm from candidacy.\nDuring the execution phase, even if the op uses scratch memory, it almost immediately releases for later. Only concurrent kernels might be impacted by this. But even with that, other ops should retry and as if no extra memory was used.\n\nSomething might not be following this protocol. Disabling autotune has major performance impact. Also it should have too much memory overhead. So let's root cause this first.\nReopening the issue and ask @yzhwang to take a look.", "body": "I am suspecting something weird is happening. With autotuning, it is not supposed to use much more memory than if autotune is disabled\r\n\r\n1. During the measurement phase, if an algorithm failed to allocate memory, it is not a fatal error. TF just removes that algorithm from candidacy.\r\n2. During the execution phase, even if the op uses scratch memory, it almost immediately releases for later. Only concurrent kernels might be impacted by this. But even with that, other ops should retry and as if no extra memory was used.\r\n\r\nSomething might not be following this protocol. Disabling autotune has major performance impact. Also it should have too much memory overhead. So let's root cause this first. \r\n\r\nReopening the issue and ask @yzhwang to take a look."}