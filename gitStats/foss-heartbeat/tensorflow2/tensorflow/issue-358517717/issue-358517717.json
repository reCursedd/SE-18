{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22182", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22182/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22182/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22182/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22182", "id": 358517717, "node_id": "MDU6SXNzdWUzNTg1MTc3MTc=", "number": 22182, "title": "\"master\" in cluster spec for distributed training ", "user": {"login": "lliimsft", "id": 29957883, "node_id": "MDQ6VXNlcjI5OTU3ODgz", "avatar_url": "https://avatars0.githubusercontent.com/u/29957883?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lliimsft", "html_url": "https://github.com/lliimsft", "followers_url": "https://api.github.com/users/lliimsft/followers", "following_url": "https://api.github.com/users/lliimsft/following{/other_user}", "gists_url": "https://api.github.com/users/lliimsft/gists{/gist_id}", "starred_url": "https://api.github.com/users/lliimsft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lliimsft/subscriptions", "organizations_url": "https://api.github.com/users/lliimsft/orgs", "repos_url": "https://api.github.com/users/lliimsft/repos", "events_url": "https://api.github.com/users/lliimsft/events{/privacy}", "received_events_url": "https://api.github.com/users/lliimsft/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-09-10T08:26:40Z", "updated_at": "2018-11-10T18:50:51Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>Given in the sample code <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py\">mnist_replica.py</a>, <code>tf.train.ClusterSpec()</code> needs to be used to define the cluster for distributed training.  The function <code>tf.train.ClusterSpec()</code> can only take <code>\"ps\"</code> and <code>\"worker\"</code> hosts, but <code>\"master\"</code> is not allowed. As far as I know, <code>\"master\"</code> is usually defined as a special worker and should be a part of the worker hosts, referred as <code>task_id = 0</code>. This makes sense to me.</p>\n<p>However, in the context of <code>tf.estimator</code>, if <code>\"cluster\"</code> is set in <code>TF_CONFIG</code>, it must have one <code>\"chief\"</code> node, in addition to <code>\"worker\"</code>. For example,</p>\n<pre><code>TF_CONFIG='{\n    \"cluster\": {\n        \"chief\": [\"host0:2222\"],\n        \"worker\": [\"host1:2222\", \"host2:2222\", \"host3:2222\"],\n        \"ps\": [\"host4:2222\", \"host5:2222\"]\n    },\n    \"task\": {\"type\": \"chief\", \"index\": 0}\n}'\n</code></pre>\n<p>I am trying to use <code>TF_CONFIG</code> for all our distributed training work, but it looks like to be inconsistent. For example, we need to do some preprocessing in the above <code>TF_CONFIG</code> (i.e., combining <code>\"chief\"</code> and <code>\"worker\"</code> hosts, and re-index them), to use it for <code>tf.train.ClusterSpec()</code>.</p>\n<p>Is there any better standard we can follow to achieve our goal?</p>\n<p>Thanks</p>", "body_text": "Hi,\nGiven in the sample code mnist_replica.py, tf.train.ClusterSpec() needs to be used to define the cluster for distributed training.  The function tf.train.ClusterSpec() can only take \"ps\" and \"worker\" hosts, but \"master\" is not allowed. As far as I know, \"master\" is usually defined as a special worker and should be a part of the worker hosts, referred as task_id = 0. This makes sense to me.\nHowever, in the context of tf.estimator, if \"cluster\" is set in TF_CONFIG, it must have one \"chief\" node, in addition to \"worker\". For example,\nTF_CONFIG='{\n    \"cluster\": {\n        \"chief\": [\"host0:2222\"],\n        \"worker\": [\"host1:2222\", \"host2:2222\", \"host3:2222\"],\n        \"ps\": [\"host4:2222\", \"host5:2222\"]\n    },\n    \"task\": {\"type\": \"chief\", \"index\": 0}\n}'\n\nI am trying to use TF_CONFIG for all our distributed training work, but it looks like to be inconsistent. For example, we need to do some preprocessing in the above TF_CONFIG (i.e., combining \"chief\" and \"worker\" hosts, and re-index them), to use it for tf.train.ClusterSpec().\nIs there any better standard we can follow to achieve our goal?\nThanks", "body": "Hi,\r\n\r\nGiven in the sample code [mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py), `tf.train.ClusterSpec()` needs to be used to define the cluster for distributed training.  The function `tf.train.ClusterSpec()` can only take `\"ps\"` and `\"worker\"` hosts, but `\"master\"` is not allowed. As far as I know, `\"master\"` is usually defined as a special worker and should be a part of the worker hosts, referred as `task_id = 0`. This makes sense to me.\r\n\r\nHowever, in the context of `tf.estimator`, if `\"cluster\"` is set in `TF_CONFIG`, it must have one `\"chief\"` node, in addition to `\"worker\"`. For example,\r\n\r\n```\r\nTF_CONFIG='{\r\n    \"cluster\": {\r\n        \"chief\": [\"host0:2222\"],\r\n        \"worker\": [\"host1:2222\", \"host2:2222\", \"host3:2222\"],\r\n        \"ps\": [\"host4:2222\", \"host5:2222\"]\r\n    },\r\n    \"task\": {\"type\": \"chief\", \"index\": 0}\r\n}'\r\n```\r\nI am trying to use `TF_CONFIG` for all our distributed training work, but it looks like to be inconsistent. For example, we need to do some preprocessing in the above `TF_CONFIG` (i.e., combining `\"chief\"` and `\"worker\"` hosts, and re-index them), to use it for `tf.train.ClusterSpec()`. \r\n\r\nIs there any better standard we can follow to achieve our goal?\r\n\r\nThanks \r\n\r\n"}