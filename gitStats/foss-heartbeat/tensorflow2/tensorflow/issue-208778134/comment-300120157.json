{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300120157", "html_url": "https://github.com/tensorflow/tensorflow/issues/7680#issuecomment-300120157", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7680", "id": 300120157, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDEyMDE1Nw==", "user": {"login": "mangalbhaskar", "id": 735058, "node_id": "MDQ6VXNlcjczNTA1OA==", "avatar_url": "https://avatars1.githubusercontent.com/u/735058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mangalbhaskar", "html_url": "https://github.com/mangalbhaskar", "followers_url": "https://api.github.com/users/mangalbhaskar/followers", "following_url": "https://api.github.com/users/mangalbhaskar/following{/other_user}", "gists_url": "https://api.github.com/users/mangalbhaskar/gists{/gist_id}", "starred_url": "https://api.github.com/users/mangalbhaskar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mangalbhaskar/subscriptions", "organizations_url": "https://api.github.com/users/mangalbhaskar/orgs", "repos_url": "https://api.github.com/users/mangalbhaskar/repos", "events_url": "https://api.github.com/users/mangalbhaskar/events{/privacy}", "received_events_url": "https://api.github.com/users/mangalbhaskar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-09T10:06:27Z", "updated_at": "2017-05-09T10:06:27Z", "author_association": "NONE", "body_html": "<p>I'm new to TF and I'd the same issue following the getting started  tf.contrib.learn tutorial. I'm using TF 1.0.0-rc2 with gpu support. On various forums people have pointed out to limit the GPU fraction memory, it worked; though it also worked only by starting a session without any GPU settings.</p>\n<pre><code>sess = tf.Session() \n</code></pre>\n<p>This solves the issue and does not have to specifically provide the GPU memory fraction. I have tried with following:</p>\n<pre><code>#config = tf.ConfigProto()\n#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n#config.allow_soft_placement = True\n#config.log_device_placement = True\n#sess = tf.Session(config=config)\n\n# Added this line and error was fixed\nsess = tf.Session()\n\n# Declare list of features. We only have one real-valued feature. There are many\n# other types of columns that are more complicated and useful.\nfeatures = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n... \n</code></pre>\n<p>From the documentation on <a href=\"https://www.tensorflow.org/tutorials/using_gpu\" rel=\"nofollow\">GPU usage</a>:</p>\n<blockquote>\n<p>By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process</p>\n</blockquote>\n<p>I can only speculate that starting a session limits the GPU memory allocation, though I have no idea why starting a Session solved the GPU memory issue. It would be great if someone can kindly explain.</p>", "body_text": "I'm new to TF and I'd the same issue following the getting started  tf.contrib.learn tutorial. I'm using TF 1.0.0-rc2 with gpu support. On various forums people have pointed out to limit the GPU fraction memory, it worked; though it also worked only by starting a session without any GPU settings.\nsess = tf.Session() \n\nThis solves the issue and does not have to specifically provide the GPU memory fraction. I have tried with following:\n#config = tf.ConfigProto()\n#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n#config.allow_soft_placement = True\n#config.log_device_placement = True\n#sess = tf.Session(config=config)\n\n# Added this line and error was fixed\nsess = tf.Session()\n\n# Declare list of features. We only have one real-valued feature. There are many\n# other types of columns that are more complicated and useful.\nfeatures = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n... \n\nFrom the documentation on GPU usage:\n\nBy default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process\n\nI can only speculate that starting a session limits the GPU memory allocation, though I have no idea why starting a Session solved the GPU memory issue. It would be great if someone can kindly explain.", "body": "I'm new to TF and I'd the same issue following the getting started  tf.contrib.learn tutorial. I'm using TF 1.0.0-rc2 with gpu support. On various forums people have pointed out to limit the GPU fraction memory, it worked; though it also worked only by starting a session without any GPU settings.\r\n```\r\nsess = tf.Session() \r\n```\r\nThis solves the issue and does not have to specifically provide the GPU memory fraction. I have tried with following:\r\n```\r\n#config = tf.ConfigProto()\r\n#config.gpu_options.per_process_gpu_memory_fraction = 0.3\r\n#config.allow_soft_placement = True\r\n#config.log_device_placement = True\r\n#sess = tf.Session(config=config)\r\n\r\n# Added this line and error was fixed\r\nsess = tf.Session()\r\n\r\n# Declare list of features. We only have one real-valued feature. There are many\r\n# other types of columns that are more complicated and useful.\r\nfeatures = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\r\n... \r\n```\r\nFrom the documentation on [GPU usage](https://www.tensorflow.org/tutorials/using_gpu):\r\n> By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process\r\n\r\nI can only speculate that starting a session limits the GPU memory allocation, though I have no idea why starting a Session solved the GPU memory issue. It would be great if someone can kindly explain."}