{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/424888183", "html_url": "https://github.com/tensorflow/tensorflow/issues/22378#issuecomment-424888183", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22378", "id": 424888183, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDg4ODE4Mw==", "user": {"login": "nairouz", "id": 10966954, "node_id": "MDQ6VXNlcjEwOTY2OTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/10966954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairouz", "html_url": "https://github.com/nairouz", "followers_url": "https://api.github.com/users/nairouz/followers", "following_url": "https://api.github.com/users/nairouz/following{/other_user}", "gists_url": "https://api.github.com/users/nairouz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairouz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairouz/subscriptions", "organizations_url": "https://api.github.com/users/nairouz/orgs", "repos_url": "https://api.github.com/users/nairouz/repos", "events_url": "https://api.github.com/users/nairouz/events{/privacy}", "received_events_url": "https://api.github.com/users/nairouz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T22:18:56Z", "updated_at": "2018-09-26T22:21:14Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785337\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wt-huang\">@wt-huang</a> thank you for the response.<br>\nBut what if I am not the one who created the gradient context (i.e., in Keras it is created internally) and I do not have access to the <code>tape</code> variable. How can I stop the gradient?<br>\nHere is a toy example to illustrate the problem:</p>\n<pre><code>import tensorflow as tf\n\ntfe = tf.contrib.eager\ntf.enable_eager_execution()\n\na = tfe.Variable(0.)\n\ndef custom_loss(y_true, y_pred):\n  #here I want to stop the gradient how can I do so?\n  #I can only write the code of the custom loss function\n\n#This is a simplification of what  Keras high-level API is doing\ndef keras_backprop(loss, model, lr, x, y_true):\n  with tf.GradientTape() as tape:\n    y_pred = model(x)\n    l = loss(y_true, y_pred)\n  dw = tape.gradient(l, model.w)\n  model.w = model.w - lr * dw\n\n</code></pre>", "body_text": "@wt-huang thank you for the response.\nBut what if I am not the one who created the gradient context (i.e., in Keras it is created internally) and I do not have access to the tape variable. How can I stop the gradient?\nHere is a toy example to illustrate the problem:\nimport tensorflow as tf\n\ntfe = tf.contrib.eager\ntf.enable_eager_execution()\n\na = tfe.Variable(0.)\n\ndef custom_loss(y_true, y_pred):\n  #here I want to stop the gradient how can I do so?\n  #I can only write the code of the custom loss function\n\n#This is a simplification of what  Keras high-level API is doing\ndef keras_backprop(loss, model, lr, x, y_true):\n  with tf.GradientTape() as tape:\n    y_pred = model(x)\n    l = loss(y_true, y_pred)\n  dw = tape.gradient(l, model.w)\n  model.w = model.w - lr * dw", "body": "@wt-huang thank you for the response.\r\nBut what if I am not the one who created the gradient context (i.e., in Keras it is created internally) and I do not have access to the `tape` variable. How can I stop the gradient? \r\nHere is a toy example to illustrate the problem:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntfe = tf.contrib.eager\r\ntf.enable_eager_execution()\r\n\r\na = tfe.Variable(0.)\r\n\r\ndef custom_loss(y_true, y_pred):\r\n  #here I want to stop the gradient how can I do so?\r\n  #I can only write the code of the custom loss function\r\n\r\n#This is a simplification of what  Keras high-level API is doing\r\ndef keras_backprop(loss, model, lr, x, y_true):\r\n  with tf.GradientTape() as tape:\r\n    y_pred = model(x)\r\n    l = loss(y_true, y_pred)\r\n  dw = tape.gradient(l, model.w)\r\n  model.w = model.w - lr * dw\r\n\r\n```\r\n"}