{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22378", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22378/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22378/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22378/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22378", "id": 361667730, "node_id": "MDU6SXNzdWUzNjE2Njc3MzA=", "number": 22378, "title": "Stop the gradient computation for Keras eager execution mode", "user": {"login": "nairouz", "id": 10966954, "node_id": "MDQ6VXNlcjEwOTY2OTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/10966954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairouz", "html_url": "https://github.com/nairouz", "followers_url": "https://api.github.com/users/nairouz/followers", "following_url": "https://api.github.com/users/nairouz/following{/other_user}", "gists_url": "https://api.github.com/users/nairouz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairouz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairouz/subscriptions", "organizations_url": "https://api.github.com/users/nairouz/orgs", "repos_url": "https://api.github.com/users/nairouz/repos", "events_url": "https://api.github.com/users/nairouz/events{/privacy}", "received_events_url": "https://api.github.com/users/nairouz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-09-19T09:52:33Z", "updated_at": "2018-10-01T21:28:28Z", "closed_at": "2018-09-29T02:56:02Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Colab</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.1</li>\n<li><strong>Python version</strong>: python 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I  am facing problems when implementing some custom loss function in Keras based on the eager execution mode. The problem is that some loss functions require to stop the gradient computation for some specific variables. When executed in a graph, we can use the op tf.stop_gradient. In fact, for Keras the GradientTape is internally handled by the function <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py\"><code>_process_single_batch</code> </a></p>\n<h3>Source code / logs</h3>\n<p>Here is an example :</p>\n<pre><code>    def virtual_adversarial_loss(X, DAE_encoder):\n        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)\n        tape.reset()\n        tape.watch(X)\n        tape.watch(r_vadv)\n        p =  DAE_encoder(X)\n        p = tape.watch(p)\n        q = DAE_encoder(x+r_vadv)\n        loss = kl(p, q)\n        return tf.identity(loss, name=\"vat_loss\")\n</code></pre>\n<pre><code>    def virtual_adversarial_loss(X, DAE_encoder):\n        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)\n        tf.stop_gradient(X)\n        tf.stop_gradient(r_vadv)\n        p =  DAE_encoder(X)\n        p = tf.stop_gradient(p)\n        q = DAE_encoder(x+r_vadv)\n        loss = kl(p, q)\n        return tf.identity(loss, name=\"vat_loss\")\n</code></pre>\n<p>Both of these functions do not work. The problem with the first one is that I can't use the tape variable created by the internal Keras wrapper function <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py\"><code>_process_single_batch</code> </a>. The problem with the second one is that I can't use tf.stop_gradient for the eager execution mode.</p>\n<p>Is there a way to stop the gradient computation in Keras eager execution mode?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below): 1.10.1\nPython version: python 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nI  am facing problems when implementing some custom loss function in Keras based on the eager execution mode. The problem is that some loss functions require to stop the gradient computation for some specific variables. When executed in a graph, we can use the op tf.stop_gradient. In fact, for Keras the GradientTape is internally handled by the function _process_single_batch \nSource code / logs\nHere is an example :\n    def virtual_adversarial_loss(X, DAE_encoder):\n        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)\n        tape.reset()\n        tape.watch(X)\n        tape.watch(r_vadv)\n        p =  DAE_encoder(X)\n        p = tape.watch(p)\n        q = DAE_encoder(x+r_vadv)\n        loss = kl(p, q)\n        return tf.identity(loss, name=\"vat_loss\")\n\n    def virtual_adversarial_loss(X, DAE_encoder):\n        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)\n        tf.stop_gradient(X)\n        tf.stop_gradient(r_vadv)\n        p =  DAE_encoder(X)\n        p = tf.stop_gradient(p)\n        q = DAE_encoder(x+r_vadv)\n        loss = kl(p, q)\n        return tf.identity(loss, name=\"vat_loss\")\n\nBoth of these functions do not work. The problem with the first one is that I can't use the tape variable created by the internal Keras wrapper function _process_single_batch . The problem with the second one is that I can't use tf.stop_gradient for the eager execution mode.\nIs there a way to stop the gradient computation in Keras eager execution mode?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: python 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI  am facing problems when implementing some custom loss function in Keras based on the eager execution mode. The problem is that some loss functions require to stop the gradient computation for some specific variables. When executed in a graph, we can use the op tf.stop_gradient. In fact, for Keras the GradientTape is internally handled by the function [`_process_single_batch` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py)\r\n\r\n### Source code / logs\r\nHere is an example : \r\n   \r\n```\r\n    def virtual_adversarial_loss(X, DAE_encoder):\r\n        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)\r\n        tape.reset()\r\n        tape.watch(X)\r\n        tape.watch(r_vadv)\r\n        p =  DAE_encoder(X)\r\n        p = tape.watch(p)\r\n        q = DAE_encoder(x+r_vadv)\r\n        loss = kl(p, q)\r\n        return tf.identity(loss, name=\"vat_loss\")\r\n```\r\n\r\n```\r\n    def virtual_adversarial_loss(X, DAE_encoder):\r\n        r_vadv = generate_virtual_adversarial_perturbation(X, DAE_encoder)\r\n        tf.stop_gradient(X)\r\n        tf.stop_gradient(r_vadv)\r\n        p =  DAE_encoder(X)\r\n        p = tf.stop_gradient(p)\r\n        q = DAE_encoder(x+r_vadv)\r\n        loss = kl(p, q)\r\n        return tf.identity(loss, name=\"vat_loss\")\r\n```\r\n\r\nBoth of these functions do not work. The problem with the first one is that I can't use the tape variable created by the internal Keras wrapper function [`_process_single_batch` ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py). The problem with the second one is that I can't use tf.stop_gradient for the eager execution mode.\r\n\r\nIs there a way to stop the gradient computation in Keras eager execution mode?"}