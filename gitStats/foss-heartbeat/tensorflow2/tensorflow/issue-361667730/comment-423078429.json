{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423078429", "html_url": "https://github.com/tensorflow/tensorflow/issues/22378#issuecomment-423078429", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22378", "id": 423078429, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzA3ODQyOQ==", "user": {"login": "nairouz", "id": 10966954, "node_id": "MDQ6VXNlcjEwOTY2OTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/10966954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairouz", "html_url": "https://github.com/nairouz", "followers_url": "https://api.github.com/users/nairouz/followers", "following_url": "https://api.github.com/users/nairouz/following{/other_user}", "gists_url": "https://api.github.com/users/nairouz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairouz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairouz/subscriptions", "organizations_url": "https://api.github.com/users/nairouz/orgs", "repos_url": "https://api.github.com/users/nairouz/repos", "events_url": "https://api.github.com/users/nairouz/events{/privacy}", "received_events_url": "https://api.github.com/users/nairouz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T07:51:06Z", "updated_at": "2018-09-20T13:45:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785337\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wt-huang\">@wt-huang</a> thank you for the response. Yes, I made sure to add all the required imports.</p>\n<p>Please let further explain the problem. Some loss functions require to stop the gradient for some specific variables. In fact, there are two cases:</p>\n<ol>\n<li>\n<p>In the graph execution mode, one can use the  <code>tf.stop_gradient(input=a)</code> and this operator will exclude the \"a\" variable from any gradient backpropagation.</p>\n</li>\n<li>\n<p>In the eager execution mode, a tape should be created based on the GradientTape class to record the operations since we do not have a graph and only then we can exclude some variables from the gradient backpropagation.</p>\n</li>\n</ol>\n<p>Unfortunately in Keras, one should not create a tape himself since the gradient computation will be done using the tape created by tensorflow developers in this function <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py#L479\"><code>_process_single_batch</code></a>. Even if I create a tape myself, it will not be used for the gradient computation performed by Keras engine unless I implement my own training loop function and hence have my own  backpropagation. In this case,  I would be deprived of Keras high-level API functions like <code>fit</code> and <code>predict</code>.</p>\n<p>I think it will be quite useful to extend tensorflow with the following operator:<br>\ntf.get_GradientTape() which can return the actual gradient context manager if there is one.</p>", "body_text": "@wt-huang thank you for the response. Yes, I made sure to add all the required imports.\nPlease let further explain the problem. Some loss functions require to stop the gradient for some specific variables. In fact, there are two cases:\n\n\nIn the graph execution mode, one can use the  tf.stop_gradient(input=a) and this operator will exclude the \"a\" variable from any gradient backpropagation.\n\n\nIn the eager execution mode, a tape should be created based on the GradientTape class to record the operations since we do not have a graph and only then we can exclude some variables from the gradient backpropagation.\n\n\nUnfortunately in Keras, one should not create a tape himself since the gradient computation will be done using the tape created by tensorflow developers in this function _process_single_batch. Even if I create a tape myself, it will not be used for the gradient computation performed by Keras engine unless I implement my own training loop function and hence have my own  backpropagation. In this case,  I would be deprived of Keras high-level API functions like fit and predict.\nI think it will be quite useful to extend tensorflow with the following operator:\ntf.get_GradientTape() which can return the actual gradient context manager if there is one.", "body": "@wt-huang thank you for the response. Yes, I made sure to add all the required imports.\r\n\r\nPlease let further explain the problem. Some loss functions require to stop the gradient for some specific variables. In fact, there are two cases:\r\n\r\n1) In the graph execution mode, one can use the  `tf.stop_gradient(input=a)` and this operator will exclude the \"a\" variable from any gradient backpropagation.\r\n\r\n2) In the eager execution mode, a tape should be created based on the GradientTape class to record the operations since we do not have a graph and only then we can exclude some variables from the gradient backpropagation.\r\n\r\nUnfortunately in Keras, one should not create a tape himself since the gradient computation will be done using the tape created by tensorflow developers in this function [`_process_single_batch`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager.py#L479). Even if I create a tape myself, it will not be used for the gradient computation performed by Keras engine unless I implement my own training loop function and hence have my own  backpropagation. In this case,  I would be deprived of Keras high-level API functions like `fit` and `predict`.\r\n\r\nI think it will be quite useful to extend tensorflow with the following operator:\r\ntf.get_GradientTape() which can return the actual gradient context manager if there is one."}