{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225091049", "html_url": "https://github.com/tensorflow/tensorflow/issues/368#issuecomment-225091049", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/368", "id": 225091049, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTA5MTA0OQ==", "user": {"login": "nikitakit", "id": 252225, "node_id": "MDQ6VXNlcjI1MjIyNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/252225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikitakit", "html_url": "https://github.com/nikitakit", "followers_url": "https://api.github.com/users/nikitakit/followers", "following_url": "https://api.github.com/users/nikitakit/following{/other_user}", "gists_url": "https://api.github.com/users/nikitakit/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikitakit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikitakit/subscriptions", "organizations_url": "https://api.github.com/users/nikitakit/orgs", "repos_url": "https://api.github.com/users/nikitakit/repos", "events_url": "https://api.github.com/users/nikitakit/events{/privacy}", "received_events_url": "https://api.github.com/users/nikitakit/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-10T04:34:51Z", "updated_at": "2016-06-21T03:55:30Z", "author_association": "NONE", "body_html": "<p>If I understand correctly, <code>tf.multinomial</code> is general enough such that a <code>tf.choice</code> is not necessary to implement sampled decoding.</p>\n<p>Here is a snippet of code from my current project, which I believe corresponds to \"sampled decoding\", as requested here.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_and_embed</span>(<span class=\"pl-smi\">choices</span>, <span class=\"pl-smi\">prev</span>, <span class=\"pl-smi\">i</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Loop_function that samples a symbol based on prev logits and embeds it.<span class=\"pl-pds\">\"\"\"</span></span>\n    prev_symbol <span class=\"pl-k\">=</span> tf.multinomial(prev, <span class=\"pl-c1\">1</span>)[:,<span class=\"pl-c1\">0</span>]\n    choices.append(prev_symbol)\n    <span class=\"pl-k\">return</span> embedding_ops.embedding_lookup(embedding_table, prev_symbol)\n\n<span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>decoder<span class=\"pl-pds\">\"</span></span>):\n    choices <span class=\"pl-k\">=</span> []\n    loop_function <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">prev</span>, <span class=\"pl-smi\">i</span>: sample_and_embed(choices, prev, i)\n    outputs, states <span class=\"pl-k\">=</span> seq2seq.rnn_decoder([start_token_value] <span class=\"pl-k\">+</span> [<span class=\"pl-c1\">None</span>] <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">SEQ_LENGTH</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>),\n                                initial_state,\n                                cell,\n                                <span class=\"pl-v\">loop_function</span> <span class=\"pl-k\">=</span> loop_function\n                                                 )\n    loop_function(outputs[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">len</span>(outputs)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Select a value for the last token!</span></pre></div>\n<p>Edit: removed call to <code>tf.nn.log_softmax</code> now that bug in multinomial is fixed</p>", "body_text": "If I understand correctly, tf.multinomial is general enough such that a tf.choice is not necessary to implement sampled decoding.\nHere is a snippet of code from my current project, which I believe corresponds to \"sampled decoding\", as requested here.\ndef sample_and_embed(choices, prev, i):\n    \"\"\"Loop_function that samples a symbol based on prev logits and embeds it.\"\"\"\n    prev_symbol = tf.multinomial(prev, 1)[:,0]\n    choices.append(prev_symbol)\n    return embedding_ops.embedding_lookup(embedding_table, prev_symbol)\n\nwith vs.variable_scope(\"decoder\"):\n    choices = []\n    loop_function = lambda prev, i: sample_and_embed(choices, prev, i)\n    outputs, states = seq2seq.rnn_decoder([start_token_value] + [None] * (SEQ_LENGTH-1),\n                                initial_state,\n                                cell,\n                                loop_function = loop_function\n                                                 )\n    loop_function(outputs[-1], len(outputs)) # Select a value for the last token!\nEdit: removed call to tf.nn.log_softmax now that bug in multinomial is fixed", "body": "If I understand correctly, `tf.multinomial` is general enough such that a `tf.choice` is not necessary to implement sampled decoding.\n\nHere is a snippet of code from my current project, which I believe corresponds to \"sampled decoding\", as requested here.\n\n``` python\ndef sample_and_embed(choices, prev, i):\n    \"\"\"Loop_function that samples a symbol based on prev logits and embeds it.\"\"\"\n    prev_symbol = tf.multinomial(prev, 1)[:,0]\n    choices.append(prev_symbol)\n    return embedding_ops.embedding_lookup(embedding_table, prev_symbol)\n\nwith vs.variable_scope(\"decoder\"):\n    choices = []\n    loop_function = lambda prev, i: sample_and_embed(choices, prev, i)\n    outputs, states = seq2seq.rnn_decoder([start_token_value] + [None] * (SEQ_LENGTH-1),\n                                initial_state,\n                                cell,\n                                loop_function = loop_function\n                                                 )\n    loop_function(outputs[-1], len(outputs)) # Select a value for the last token!\n```\n\nEdit: removed call to `tf.nn.log_softmax` now that bug in multinomial is fixed\n"}