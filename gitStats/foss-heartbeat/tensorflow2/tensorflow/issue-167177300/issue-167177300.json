{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3475", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3475/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3475/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3475/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3475", "id": 167177300, "node_id": "MDU6SXNzdWUxNjcxNzczMDA=", "number": 3475, "title": "Device placement error while using multi gpus on single machine by distributed version", "user": {"login": "guolinke", "id": 16040950, "node_id": "MDQ6VXNlcjE2MDQwOTUw", "avatar_url": "https://avatars0.githubusercontent.com/u/16040950?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guolinke", "html_url": "https://github.com/guolinke", "followers_url": "https://api.github.com/users/guolinke/followers", "following_url": "https://api.github.com/users/guolinke/following{/other_user}", "gists_url": "https://api.github.com/users/guolinke/gists{/gist_id}", "starred_url": "https://api.github.com/users/guolinke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guolinke/subscriptions", "organizations_url": "https://api.github.com/users/guolinke/orgs", "repos_url": "https://api.github.com/users/guolinke/repos", "events_url": "https://api.github.com/users/guolinke/events{/privacy}", "received_events_url": "https://api.github.com/users/guolinke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-07-23T09:51:48Z", "updated_at": "2016-07-27T03:00:39Z", "closed_at": "2016-07-27T03:00:39Z", "author_association": "NONE", "body_html": "<p>I follow <a href=\"https://www.tensorflow.org/versions/r0.9/how_tos/distributed/index.html#distributed-tensorflow\" rel=\"nofollow\">Distributed TensorFlow</a> to try the distributed version of tensorflow, my code is <a href=\"https://github.com/guolinke/temp/blob/master/mnist_expert_dist.py\">here</a>.</p>\n<p>I try to run it on mulit machines with mutil gpus, and every worker will start multi instances of tensorflow.</p>\n<p>To simplify problem, i only run it on single machine with multi-gpu(8*k40m), and only start one ps and one worker. Following is my scripts:</p>\n<pre><code>python ./mnist_expert_dist.py --ps_hosts=localhost:12222 --worker_hosts=localhost:12223 --job_name=ps --task_index=0 --gpu_index=0 --iteration_count=5000 --verbose=true\n\npython ./mnist_expert_dist.py --ps_hosts=localhost:12222 --worker_hosts=localhost:12223 --job_name=worker --task_index=0 --gpu_index=1 --iteration_count=5000 --verbose=true\n</code></pre>\n<p>Following is my error messages:</p>\n<pre><code>Traceback (most recent call last):\n  File \"./mnist_expert_dist.py\", line 187, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_expert_dist.py\", line 184, in main\n    run_training(cluster, server)\n  File \"./mnist_expert_dist.py\", line 121, in run_training\n    sess = sv.prepare_or_wait_for_session(server.target)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 170, in prepare_session\n    max_wait_secs=max_wait_secs, config=config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 209, in recover_session\n    sess.run([self._local_init_op])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/Const': Could not satisfy explicit device specification '/job:worker/task:0/device:GPU:1' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices:\nIdentity: CPU\nConst: CPU\n         [[Node: save/Const = Const[dtype=DT_STRING, value=Tensor&lt;type: string shape: [] values: model&gt;, _device=\"/job:worker/task:0/device:GPU:1\"]()]]\nCaused by op u'save/Const', defined at:\n  File \"./mnist_expert_dist.py\", line 187, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_expert_dist.py\", line 184, in main\n    run_training(cluster, server)\n  File \"./mnist_expert_dist.py\", line 113, in run_training\n    init_op=init_op)#,\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 300, in __init__\n    self._init_saver(saver=saver)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 429, in _init_saver\n    saver = saver_mod.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 504, in build\n    filename_tensor = constant_op.constant(\"model\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n</code></pre>\n<p>I use nvidia-smi to see the status of gpu, it seems ps will take all gpus, and worker cannot access any gpus.<br>\nHow can I solve this problem?<br>\nCan i start the ps only with one gpu? or only use cpu?</p>\n<p>Thanks.</p>", "body_text": "I follow Distributed TensorFlow to try the distributed version of tensorflow, my code is here.\nI try to run it on mulit machines with mutil gpus, and every worker will start multi instances of tensorflow.\nTo simplify problem, i only run it on single machine with multi-gpu(8*k40m), and only start one ps and one worker. Following is my scripts:\npython ./mnist_expert_dist.py --ps_hosts=localhost:12222 --worker_hosts=localhost:12223 --job_name=ps --task_index=0 --gpu_index=0 --iteration_count=5000 --verbose=true\n\npython ./mnist_expert_dist.py --ps_hosts=localhost:12222 --worker_hosts=localhost:12223 --job_name=worker --task_index=0 --gpu_index=1 --iteration_count=5000 --verbose=true\n\nFollowing is my error messages:\nTraceback (most recent call last):\n  File \"./mnist_expert_dist.py\", line 187, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_expert_dist.py\", line 184, in main\n    run_training(cluster, server)\n  File \"./mnist_expert_dist.py\", line 121, in run_training\n    sess = sv.prepare_or_wait_for_session(server.target)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 170, in prepare_session\n    max_wait_secs=max_wait_secs, config=config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 209, in recover_session\n    sess.run([self._local_init_op])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/Const': Could not satisfy explicit device specification '/job:worker/task:0/device:GPU:1' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices:\nIdentity: CPU\nConst: CPU\n         [[Node: save/Const = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: model>, _device=\"/job:worker/task:0/device:GPU:1\"]()]]\nCaused by op u'save/Const', defined at:\n  File \"./mnist_expert_dist.py\", line 187, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_expert_dist.py\", line 184, in main\n    run_training(cluster, server)\n  File \"./mnist_expert_dist.py\", line 113, in run_training\n    init_op=init_op)#,\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 300, in __init__\n    self._init_saver(saver=saver)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 429, in _init_saver\n    saver = saver_mod.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 504, in build\n    filename_tensor = constant_op.constant(\"model\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n\nI use nvidia-smi to see the status of gpu, it seems ps will take all gpus, and worker cannot access any gpus.\nHow can I solve this problem?\nCan i start the ps only with one gpu? or only use cpu?\nThanks.", "body": " I follow [Distributed TensorFlow](https://www.tensorflow.org/versions/r0.9/how_tos/distributed/index.html#distributed-tensorflow) to try the distributed version of tensorflow, my code is [here](https://github.com/guolinke/temp/blob/master/mnist_expert_dist.py).\n\nI try to run it on mulit machines with mutil gpus, and every worker will start multi instances of tensorflow.\n\nTo simplify problem, i only run it on single machine with multi-gpu(8*k40m), and only start one ps and one worker. Following is my scripts:\n\n```\npython ./mnist_expert_dist.py --ps_hosts=localhost:12222 --worker_hosts=localhost:12223 --job_name=ps --task_index=0 --gpu_index=0 --iteration_count=5000 --verbose=true\n\npython ./mnist_expert_dist.py --ps_hosts=localhost:12222 --worker_hosts=localhost:12223 --job_name=worker --task_index=0 --gpu_index=1 --iteration_count=5000 --verbose=true\n```\n\nFollowing is my error messages:\n\n```\nTraceback (most recent call last):\n  File \"./mnist_expert_dist.py\", line 187, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_expert_dist.py\", line 184, in main\n    run_training(cluster, server)\n  File \"./mnist_expert_dist.py\", line 121, in run_training\n    sess = sv.prepare_or_wait_for_session(server.target)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 170, in prepare_session\n    max_wait_secs=max_wait_secs, config=config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 209, in recover_session\n    sess.run([self._local_init_op])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/Const': Could not satisfy explicit device specification '/job:worker/task:0/device:GPU:1' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices:\nIdentity: CPU\nConst: CPU\n         [[Node: save/Const = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: model>, _device=\"/job:worker/task:0/device:GPU:1\"]()]]\nCaused by op u'save/Const', defined at:\n  File \"./mnist_expert_dist.py\", line 187, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./mnist_expert_dist.py\", line 184, in main\n    run_training(cluster, server)\n  File \"./mnist_expert_dist.py\", line 113, in run_training\n    init_op=init_op)#,\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 300, in __init__\n    self._init_saver(saver=saver)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 429, in _init_saver\n    saver = saver_mod.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 504, in build\n    filename_tensor = constant_op.constant(\"model\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n```\n\nI use nvidia-smi to see the status of gpu, it seems ps will take all gpus, and worker cannot access any gpus.\nHow can I solve this problem?\nCan i start the ps only with one gpu? or only use cpu?\n\nThanks.\n"}