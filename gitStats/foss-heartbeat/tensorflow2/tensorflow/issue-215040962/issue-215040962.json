{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8503", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8503/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8503/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8503/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8503", "id": 215040962, "node_id": "MDU6SXNzdWUyMTUwNDA5NjI=", "number": 8503, "title": "Can aggregate all gradients from local wokers on local GPUs before push to parameter server \uff1f", "user": {"login": "backyes", "id": 920727, "node_id": "MDQ6VXNlcjkyMDcyNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/920727?v=4", "gravatar_id": "", "url": "https://api.github.com/users/backyes", "html_url": "https://github.com/backyes", "followers_url": "https://api.github.com/users/backyes/followers", "following_url": "https://api.github.com/users/backyes/following{/other_user}", "gists_url": "https://api.github.com/users/backyes/gists{/gist_id}", "starred_url": "https://api.github.com/users/backyes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/backyes/subscriptions", "organizations_url": "https://api.github.com/users/backyes/orgs", "repos_url": "https://api.github.com/users/backyes/repos", "events_url": "https://api.github.com/users/backyes/events{/privacy}", "received_events_url": "https://api.github.com/users/backyes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-17T15:40:57Z", "updated_at": "2017-03-17T16:45:23Z", "closed_at": "2017-03-17T16:45:23Z", "author_association": "NONE", "body_html": "<p>Can we use Graph mechnism to build the situation that all workers will aggregate all gradients from their replicated sub-graph\uff0c before pushing gradients to all pservers\uff1f</p>\n<p>If OK\uff0c can show us some examples\uff1f</p>\n<p>BTW\uff1a</p>\n<ul>\n<li>\n<p>with r1.0.1 version, inception training with distributed nodes will crash for several old APIs\uff0cand some more unpredictable reason.</p>\n<blockquote>\n<p>TypeError: <strong>init</strong>() got an unexpected keyword argument 'replica_id' .</p>\n</blockquote>\n</li>\n<li>\n<p>some ps error comes if we use non-default DNS IP when multiple NICS exist.</p>\n</li>\n</ul>", "body_text": "Can we use Graph mechnism to build the situation that all workers will aggregate all gradients from their replicated sub-graph\uff0c before pushing gradients to all pservers\uff1f\nIf OK\uff0c can show us some examples\uff1f\nBTW\uff1a\n\n\nwith r1.0.1 version, inception training with distributed nodes will crash for several old APIs\uff0cand some more unpredictable reason.\n\nTypeError: init() got an unexpected keyword argument 'replica_id' .\n\n\n\nsome ps error comes if we use non-default DNS IP when multiple NICS exist.", "body": "Can we use Graph mechnism to build the situation that all workers will aggregate all gradients from their replicated sub-graph\uff0c before pushing gradients to all pservers\uff1f \r\n\r\nIf OK\uff0c can show us some examples\uff1f \r\n\r\nBTW\uff1a \r\n\r\n* with r1.0.1 version, inception training with distributed nodes will crash for several old APIs\uff0cand some more unpredictable reason. \r\n  > TypeError: __init__() got an unexpected keyword argument 'replica_id' . \r\n\r\n* some ps error comes if we use non-default DNS IP when multiple NICS exist.  \r\n\r\n"}