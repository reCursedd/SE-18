{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/378461234", "html_url": "https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-378461234", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17856", "id": 378461234, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODQ2MTIzNA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-04T02:35:10Z", "updated_at": "2018-04-04T02:35:10Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">The graph is rewritten at session.run, yes. That doesn't help you though.\nThousands of ops seems a bit excessive.  Can you identify which forward op\nexactly causes the gradient graph to blow up?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Tue, Apr 3, 2018, 7:09 PM laket ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; thank you for reply.\n\n Do you mean that the graph is shrinked at sess.run, so the temporary big\n graph is not important?\n\n When I use second order differential of fused_batch_norm in a big graph\n like Resnet, memory is exhausted at tf.gradients.\n So my program can't reach sess.run.\n\n If we can optimize graph at graph construction time, I think we should do\n it.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"306812967\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/17856\" href=\"https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-378457131\">#17856 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim8w6KmgEQo0HcrK0ytJvNdIgr5euks5tlCtGgaJpZM4SxqqD\">https://github.com/notifications/unsubscribe-auth/ABtim8w6KmgEQo0HcrK0ytJvNdIgr5euks5tlCtGgaJpZM4SxqqD</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "The graph is rewritten at session.run, yes. That doesn't help you though.\nThousands of ops seems a bit excessive.  Can you identify which forward op\nexactly causes the gradient graph to blow up?\n\u2026\nOn Tue, Apr 3, 2018, 7:09 PM laket ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> thank you for reply.\n\n Do you mean that the graph is shrinked at sess.run, so the temporary big\n graph is not important?\n\n When I use second order differential of fused_batch_norm in a big graph\n like Resnet, memory is exhausted at tf.gradients.\n So my program can't reach sess.run.\n\n If we can optimize graph at graph construction time, I think we should do\n it.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#17856 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim8w6KmgEQo0HcrK0ytJvNdIgr5euks5tlCtGgaJpZM4SxqqD>\n .", "body": "The graph is rewritten at session.run, yes. That doesn't help you though.\nThousands of ops seems a bit excessive.  Can you identify which forward op\nexactly causes the gradient graph to blow up?\n\nOn Tue, Apr 3, 2018, 7:09 PM laket <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> thank you for reply.\n>\n> Do you mean that the graph is shrinked at sess.run, so the temporary big\n> graph is not important?\n>\n> When I use second order differential of fused_batch_norm in a big graph\n> like Resnet, memory is exhausted at tf.gradients.\n> So my program can't reach sess.run.\n>\n> If we can optimize graph at graph construction time, I think we should do\n> it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-378457131>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8w6KmgEQo0HcrK0ytJvNdIgr5euks5tlCtGgaJpZM4SxqqD>\n> .\n>\n"}