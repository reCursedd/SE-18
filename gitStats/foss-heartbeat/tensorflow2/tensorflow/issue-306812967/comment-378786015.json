{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/378786015", "html_url": "https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-378786015", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17856", "id": 378786015, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODc4NjAxNQ==", "user": {"login": "laket", "id": 1290076, "node_id": "MDQ6VXNlcjEyOTAwNzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1290076?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laket", "html_url": "https://github.com/laket", "followers_url": "https://api.github.com/users/laket/followers", "following_url": "https://api.github.com/users/laket/following{/other_user}", "gists_url": "https://api.github.com/users/laket/gists{/gist_id}", "starred_url": "https://api.github.com/users/laket/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laket/subscriptions", "organizations_url": "https://api.github.com/users/laket/orgs", "repos_url": "https://api.github.com/users/laket/repos", "events_url": "https://api.github.com/users/laket/events{/privacy}", "received_events_url": "https://api.github.com/users/laket/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-05T00:28:55Z", "updated_at": "2018-04-06T09:24:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As described in my second <a href=\"https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-375599868\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/17856/hovercard\">post</a> ,<br>\nprocess in FusedBatchNormGradGrad cause the gradient graph to blow up.</p>\n<p>When I use stop_gradient in FusedBatchNormGradGrad or<br>\ntf.nn.batch_normalization instead of fused_batch_norm, the graph is much shrinked.<br>\nSo I think fused_batch_norm cause the gradient to blow up.</p>", "body_text": "As described in my second post ,\nprocess in FusedBatchNormGradGrad cause the gradient graph to blow up.\nWhen I use stop_gradient in FusedBatchNormGradGrad or\ntf.nn.batch_normalization instead of fused_batch_norm, the graph is much shrinked.\nSo I think fused_batch_norm cause the gradient to blow up.", "body": "As described in my second [post](https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-375599868) , \r\nprocess in FusedBatchNormGradGrad cause the gradient graph to blow up.\r\n\r\nWhen I use stop_gradient in FusedBatchNormGradGrad or \r\ntf.nn.batch_normalization instead of fused_batch_norm, the graph is much shrinked. \r\nSo I think fused_batch_norm cause the gradient to blow up."}