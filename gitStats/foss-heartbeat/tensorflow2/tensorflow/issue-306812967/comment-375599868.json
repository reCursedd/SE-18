{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/375599868", "html_url": "https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-375599868", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17856", "id": 375599868, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTU5OTg2OA==", "user": {"login": "laket", "id": 1290076, "node_id": "MDQ6VXNlcjEyOTAwNzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1290076?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laket", "html_url": "https://github.com/laket", "followers_url": "https://api.github.com/users/laket/followers", "following_url": "https://api.github.com/users/laket/following{/other_user}", "gists_url": "https://api.github.com/users/laket/gists{/gist_id}", "starred_url": "https://api.github.com/users/laket/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laket/subscriptions", "organizations_url": "https://api.github.com/users/laket/orgs", "repos_url": "https://api.github.com/users/laket/repos", "events_url": "https://api.github.com/users/laket/events{/privacy}", "received_events_url": "https://api.github.com/users/laket/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-23T09:55:34Z", "updated_at": "2018-03-23T09:55:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>FusedBatchNormGradGrad uses gradients funcion to calculate gradients in  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L936\">nn_grad.py</a>.</p>\n<pre><code>  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial)\n</code></pre>\n<p>Then derivative of (grad_y) with respect to x is calculated. I think this makes a large number of nodes of the above network. The network size become much smaller, when we use stop_gradients in the above snippet like</p>\n<pre><code>  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial, stop_gradients=[grad_y, x, scale])\n</code></pre>\n<p>Can we use stop_gradients?</p>", "body_text": "FusedBatchNormGradGrad uses gradients funcion to calculate gradients in  nn_grad.py.\n  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial)\n\nThen derivative of (grad_y) with respect to x is calculated. I think this makes a large number of nodes of the above network. The network size become much smaller, when we use stop_gradients in the above snippet like\n  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial, stop_gradients=[grad_y, x, scale])\n\nCan we use stop_gradients?", "body": "FusedBatchNormGradGrad uses gradients funcion to calculate gradients in  [nn_grad.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L936).\r\n\r\n```\r\n  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\r\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial)\r\n```\r\n\r\nThen derivative of (grad_y) with respect to x is calculated. I think this makes a large number of nodes of the above network. The network size become much smaller, when we use stop_gradients in the above snippet like\r\n\r\n```\r\n  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\r\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial, stop_gradients=[grad_y, x, scale])\r\n```\r\n\r\nCan we use stop_gradients?"}