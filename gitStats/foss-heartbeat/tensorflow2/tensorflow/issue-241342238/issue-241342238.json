{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11361", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11361/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11361/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11361/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11361", "id": 241342238, "node_id": "MDU6SXNzdWUyNDEzNDIyMzg=", "number": 11361, "title": "Feature Request: Change REGISTER_OP macro to facilitate customization on static initialization sequence", "user": {"login": "liuliu", "id": 127987, "node_id": "MDQ6VXNlcjEyNzk4Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/127987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liuliu", "html_url": "https://github.com/liuliu", "followers_url": "https://api.github.com/users/liuliu/followers", "following_url": "https://api.github.com/users/liuliu/following{/other_user}", "gists_url": "https://api.github.com/users/liuliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/liuliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liuliu/subscriptions", "organizations_url": "https://api.github.com/users/liuliu/orgs", "repos_url": "https://api.github.com/users/liuliu/repos", "events_url": "https://api.github.com/users/liuliu/events{/privacy}", "received_events_url": "https://api.github.com/users/liuliu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2017-07-07T18:21:50Z", "updated_at": "2018-11-22T18:52:10Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: iOS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.0</li>\n<li><strong>Python version</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: iPhone series</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3><code>REGISTER_OP</code> current syntax is not friendly for customization</h3>\n<p><code>REGISTER_OP</code> in its current implementation, which leverages C++ macro trick to do method-chaining is not friendly for customization. For example, a typical <code>REGISTER_OP</code> macro call looks like this:</p>\n<pre><code>REGISTER_OP(\"DynamicPartition\")\n\u00a0   .Input(\"data: T\")\n\u00a0   .Input(\"partitions: int32\")\n\u00a0   .Output(\"outputs: num_partitions * T\")\n\u00a0   .Attr(\"num_partitions: int\")\n\u00a0   .Attr(\"T: type\")\n    ....;\n</code></pre>\n<p>When implementing lazy initialization, this macro, comparing with others from TensorFlow (<code>REGISTER_KERNEL_BUILDER</code> for example) is more difficult to customize. The reason is that the macro doesn't capture subsequent method calls, therefore, cannot scope these method calls into a function unit. But, this is easy to solve if we change the <code>REGISTER_OP</code> syntax a bit:</p>\n<pre><code>REGISTER_OP(Op(\"DynamicPartition\")\n\u00a0   .Input(\"data: T\")\n\u00a0   .Input(\"partitions: int32\")\n\u00a0   .Output(\"outputs: num_partitions * T\")\n\u00a0   .Attr(\"num_partitions: int\")\n\u00a0   .Attr(\"T: type\")\n    ....);\n</code></pre>\n<p>The above example is very close to how <code>REGISTER_KERNEL_BUILDER</code> works:</p>\n<pre><code>REGISTER_KERNEL_BUILDER(Name(\"NoOp\").Device(DEVICE_CPU), NoOp);\n</code></pre>\n<p>so we have some consistencies there.</p>\n<p>A hypothetic change to the <code>REGISTER_OP</code> macro could look like this:</p>\n<pre><code>static inline OpDefBuilderWrapper&lt;true&gt; Op(const char name[]) {\n    return OpDefBuilderWrapper&lt;true&gt;(name);\n}\n}  // namespace register_op\n\n#define REGISTER_OP(op) REGISTER_OP_UNIQ_HELPER(__COUNTER__, op)\n#define REGISTER_OP_UNIQ_HELPER(ctr, op) REGISTER_OP_UNIQ(ctr, op)\n#define REGISTER_OP_UNIQ(ctr, op)                                            \\\n  static ::tensorflow::register_op::OpDefBuilderReceiver register_op##ctr    \\\n      TF_ATTRIBUTE_UNUSED =                                                  \\\n          ::tensorflow::register_op::op;\n</code></pre>\n<p>More importantly, this small change enabled some lazy initialization opportunities regarding ops registration, you can imagine a platform-specific change (not likely to get upstreamed) to this macro:</p>\n<pre><code>#define REGISTER_OP_UNIQ(ctr, op)                                            \\\n__attribute__((used)) static void register_op_init##ctr(void) {              \\\n  static ::tensorflow::register_op::OpDefBuilderReceiver register_op##ctr    \\\n      TF_ATTRIBUTE_UNUSED =                                                  \\\n          ::tensorflow::register_op::op;                                     \\\n}                                                                            \\\n__attribute__((used)) __attribute__((section (\"__DATA,tf_reg_op\"))) static void *register_op_func##ctr = (void *)&amp;register_op_init##ctr\n</code></pre>\n<p>Which put the static initializers into a function and a lazy initializer can call op registrations by scanning the data section and invoke these functions one by one.</p>\n<p>Please let me know if this will violate some core assumptions TensorFlow is making and your concerns.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.0\nPython version: N/A\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: iPhone series\nExact command to reproduce: N/A\n\nREGISTER_OP current syntax is not friendly for customization\nREGISTER_OP in its current implementation, which leverages C++ macro trick to do method-chaining is not friendly for customization. For example, a typical REGISTER_OP macro call looks like this:\nREGISTER_OP(\"DynamicPartition\")\n\u00a0   .Input(\"data: T\")\n\u00a0   .Input(\"partitions: int32\")\n\u00a0   .Output(\"outputs: num_partitions * T\")\n\u00a0   .Attr(\"num_partitions: int\")\n\u00a0   .Attr(\"T: type\")\n    ....;\n\nWhen implementing lazy initialization, this macro, comparing with others from TensorFlow (REGISTER_KERNEL_BUILDER for example) is more difficult to customize. The reason is that the macro doesn't capture subsequent method calls, therefore, cannot scope these method calls into a function unit. But, this is easy to solve if we change the REGISTER_OP syntax a bit:\nREGISTER_OP(Op(\"DynamicPartition\")\n\u00a0   .Input(\"data: T\")\n\u00a0   .Input(\"partitions: int32\")\n\u00a0   .Output(\"outputs: num_partitions * T\")\n\u00a0   .Attr(\"num_partitions: int\")\n\u00a0   .Attr(\"T: type\")\n    ....);\n\nThe above example is very close to how REGISTER_KERNEL_BUILDER works:\nREGISTER_KERNEL_BUILDER(Name(\"NoOp\").Device(DEVICE_CPU), NoOp);\n\nso we have some consistencies there.\nA hypothetic change to the REGISTER_OP macro could look like this:\nstatic inline OpDefBuilderWrapper<true> Op(const char name[]) {\n    return OpDefBuilderWrapper<true>(name);\n}\n}  // namespace register_op\n\n#define REGISTER_OP(op) REGISTER_OP_UNIQ_HELPER(__COUNTER__, op)\n#define REGISTER_OP_UNIQ_HELPER(ctr, op) REGISTER_OP_UNIQ(ctr, op)\n#define REGISTER_OP_UNIQ(ctr, op)                                            \\\n  static ::tensorflow::register_op::OpDefBuilderReceiver register_op##ctr    \\\n      TF_ATTRIBUTE_UNUSED =                                                  \\\n          ::tensorflow::register_op::op;\n\nMore importantly, this small change enabled some lazy initialization opportunities regarding ops registration, you can imagine a platform-specific change (not likely to get upstreamed) to this macro:\n#define REGISTER_OP_UNIQ(ctr, op)                                            \\\n__attribute__((used)) static void register_op_init##ctr(void) {              \\\n  static ::tensorflow::register_op::OpDefBuilderReceiver register_op##ctr    \\\n      TF_ATTRIBUTE_UNUSED =                                                  \\\n          ::tensorflow::register_op::op;                                     \\\n}                                                                            \\\n__attribute__((used)) __attribute__((section (\"__DATA,tf_reg_op\"))) static void *register_op_func##ctr = (void *)&register_op_init##ctr\n\nWhich put the static initializers into a function and a lazy initializer can call op registrations by scanning the data section and invoke these functions one by one.\nPlease let me know if this will violate some core assumptions TensorFlow is making and your concerns.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: iOS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: iPhone series\r\n- **Exact command to reproduce**: N/A\r\n\r\n### `REGISTER_OP` current syntax is not friendly for customization\r\n\r\n`REGISTER_OP` in its current implementation, which leverages C++ macro trick to do method-chaining is not friendly for customization. For example, a typical `REGISTER_OP` macro call looks like this:\r\n\r\n```\r\nREGISTER_OP(\"DynamicPartition\")\r\n\u00a0   .Input(\"data: T\")\r\n\u00a0   .Input(\"partitions: int32\")\r\n\u00a0   .Output(\"outputs: num_partitions * T\")\r\n\u00a0   .Attr(\"num_partitions: int\")\r\n\u00a0   .Attr(\"T: type\")\r\n    ....;\r\n```\r\n\r\nWhen implementing lazy initialization, this macro, comparing with others from TensorFlow (`REGISTER_KERNEL_BUILDER` for example) is more difficult to customize. The reason is that the macro doesn't capture subsequent method calls, therefore, cannot scope these method calls into a function unit. But, this is easy to solve if we change the `REGISTER_OP` syntax a bit:\r\n\r\n```\r\nREGISTER_OP(Op(\"DynamicPartition\")\r\n\u00a0   .Input(\"data: T\")\r\n\u00a0   .Input(\"partitions: int32\")\r\n\u00a0   .Output(\"outputs: num_partitions * T\")\r\n\u00a0   .Attr(\"num_partitions: int\")\r\n\u00a0   .Attr(\"T: type\")\r\n    ....);\r\n```\r\nThe above example is very close to how `REGISTER_KERNEL_BUILDER` works:\r\n```\r\nREGISTER_KERNEL_BUILDER(Name(\"NoOp\").Device(DEVICE_CPU), NoOp);\r\n```\r\nso we have some consistencies there.\r\n\r\nA hypothetic change to the `REGISTER_OP` macro could look like this:\r\n```\r\nstatic inline OpDefBuilderWrapper<true> Op(const char name[]) {\r\n    return OpDefBuilderWrapper<true>(name);\r\n}\r\n}  // namespace register_op\r\n\r\n#define REGISTER_OP(op) REGISTER_OP_UNIQ_HELPER(__COUNTER__, op)\r\n#define REGISTER_OP_UNIQ_HELPER(ctr, op) REGISTER_OP_UNIQ(ctr, op)\r\n#define REGISTER_OP_UNIQ(ctr, op)                                            \\\r\n  static ::tensorflow::register_op::OpDefBuilderReceiver register_op##ctr    \\\r\n      TF_ATTRIBUTE_UNUSED =                                                  \\\r\n          ::tensorflow::register_op::op;\r\n```\r\n\r\nMore importantly, this small change enabled some lazy initialization opportunities regarding ops registration, you can imagine a platform-specific change (not likely to get upstreamed) to this macro:\r\n\r\n```\r\n#define REGISTER_OP_UNIQ(ctr, op)                                            \\\r\n__attribute__((used)) static void register_op_init##ctr(void) {              \\\r\n  static ::tensorflow::register_op::OpDefBuilderReceiver register_op##ctr    \\\r\n      TF_ATTRIBUTE_UNUSED =                                                  \\\r\n          ::tensorflow::register_op::op;                                     \\\r\n}                                                                            \\\r\n__attribute__((used)) __attribute__((section (\"__DATA,tf_reg_op\"))) static void *register_op_func##ctr = (void *)&register_op_init##ctr\r\n```\r\n\r\nWhich put the static initializers into a function and a lazy initializer can call op registrations by scanning the data section and invoke these functions one by one.\r\n\r\nPlease let me know if this will violate some core assumptions TensorFlow is making and your concerns."}