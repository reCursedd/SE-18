{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2755", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2755/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2755/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2755/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/2755", "id": 159387424, "node_id": "MDExOlB1bGxSZXF1ZXN0NzMxOTg5MTM=", "number": 2755, "title": "Apply fully connected activation function on flat data", "user": {"login": "danijar", "id": 2111293, "node_id": "MDQ6VXNlcjIxMTEyOTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2111293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danijar", "html_url": "https://github.com/danijar", "followers_url": "https://api.github.com/users/danijar/followers", "following_url": "https://api.github.com/users/danijar/following{/other_user}", "gists_url": "https://api.github.com/users/danijar/gists{/gist_id}", "starred_url": "https://api.github.com/users/danijar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danijar/subscriptions", "organizations_url": "https://api.github.com/users/danijar/orgs", "repos_url": "https://api.github.com/users/danijar/repos", "events_url": "https://api.github.com/users/danijar/events{/privacy}", "received_events_url": "https://api.github.com/users/danijar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390482148, "node_id": "MDU6TGFiZWwzOTA0ODIxNDg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20review", "name": "awaiting review", "color": "fef2c0", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-06-09T11:38:34Z", "updated_at": "2016-06-11T06:18:23Z", "closed_at": "2016-06-11T06:18:23Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2755", "html_url": "https://github.com/tensorflow/tensorflow/pull/2755", "diff_url": "https://github.com/tensorflow/tensorflow/pull/2755.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/2755.patch"}, "body_html": "<p><code>layers.fully_connected</code> flattens all dimensions of the input but the last, transforms the flat rows using the same weight matrix, and unflattens the result. One use case is to apply the same layer to each output of an RNN like in this example.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> tensorflow.models.rnn <span class=\"pl-k\">import</span> rnn\n<span class=\"pl-k\">from</span> tensorflow.contrib <span class=\"pl-k\">import</span> layers\n\noutput, _ <span class=\"pl-k\">=</span> rnn.dynamic_rnn(<span class=\"pl-c1\">...</span>)\nprediction <span class=\"pl-k\">=</span> layers.fully_connected(output, out_size, tf.nn.softmax)</pre></div>\n<p>However, the example failed using <code>tf.nn.softmax</code> since the activation function was applied on the unflattened result already. This pull request applies the activation function before unflattening so that any activation function that works with normal layers (without flattening) can be used.</p>", "body_text": "layers.fully_connected flattens all dimensions of the input but the last, transforms the flat rows using the same weight matrix, and unflattens the result. One use case is to apply the same layer to each output of an RNN like in this example.\nfrom tensorflow.models.rnn import rnn\nfrom tensorflow.contrib import layers\n\noutput, _ = rnn.dynamic_rnn(...)\nprediction = layers.fully_connected(output, out_size, tf.nn.softmax)\nHowever, the example failed using tf.nn.softmax since the activation function was applied on the unflattened result already. This pull request applies the activation function before unflattening so that any activation function that works with normal layers (without flattening) can be used.", "body": "`layers.fully_connected` flattens all dimensions of the input but the last, transforms the flat rows using the same weight matrix, and unflattens the result. One use case is to apply the same layer to each output of an RNN like in this example.\n\n``` python\nfrom tensorflow.models.rnn import rnn\nfrom tensorflow.contrib import layers\n\noutput, _ = rnn.dynamic_rnn(...)\nprediction = layers.fully_connected(output, out_size, tf.nn.softmax)\n```\n\nHowever, the example failed using `tf.nn.softmax` since the activation function was applied on the unflattened result already. This pull request applies the activation function before unflattening so that any activation function that works with normal layers (without flattening) can be used.\n"}