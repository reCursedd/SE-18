{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17973", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17973/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17973/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17973/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17973", "id": 308284406, "node_id": "MDU6SXNzdWUzMDgyODQ0MDY=", "number": 17973, "title": "Losing Output Shape information of conv2d_transpose layer when importing from .pbtxt file", "user": {"login": "Fenil3510", "id": 20499290, "node_id": "MDQ6VXNlcjIwNDk5Mjkw", "avatar_url": "https://avatars0.githubusercontent.com/u/20499290?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Fenil3510", "html_url": "https://github.com/Fenil3510", "followers_url": "https://api.github.com/users/Fenil3510/followers", "following_url": "https://api.github.com/users/Fenil3510/following{/other_user}", "gists_url": "https://api.github.com/users/Fenil3510/gists{/gist_id}", "starred_url": "https://api.github.com/users/Fenil3510/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Fenil3510/subscriptions", "organizations_url": "https://api.github.com/users/Fenil3510/orgs", "repos_url": "https://api.github.com/users/Fenil3510/repos", "events_url": "https://api.github.com/users/Fenil3510/events{/privacy}", "received_events_url": "https://api.github.com/users/Fenil3510/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-03-24T17:54:56Z", "updated_at": "2018-05-26T18:51:23Z", "closed_at": "2018-05-26T18:38:00Z", "author_association": "NONE", "body_html": "<ul class=\"contains-task-list\">\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> This is a custom code</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> I'm on Mac OSX 10.11.16</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> TensorFlow version 1.6.0</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Bazel version N/A</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\">  Tensorflow installed from source</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> CUDA/cuDNN version N/A</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> GPU model and memory N/A</p>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Exact command to reproduce</p>\n</li>\n</ul>\n<p>The output shape of the node with node. Type <code>Conv2DBackpropInput</code> seems to lose height and width information as in this:<br>\nThis is denconv layer definition and conv3 layer after it<br>\n`deconv = tf.layers.conv2d_transpose(pool2 , filters = 32 ,kernel_size = [2,2],strides=(1, 1) , padding = \"same\")</p>\n<p>conv3 = tf.layers.conv2d(inputs= deconv,filters=64,kernel_size=[1, 1],padding=\"same\", activation=tf.nn.relu)`</p>\n<p><code>graph = tf.get_default_graph()</code> is built in the notebook</p>\n<pre><code>for node in graph.get_operations():\n  if node.type == \"Conv2DBackpropInput\":\n    print(node.type,\"----&gt;&gt;&gt;&gt;&gt;\", \"outputsshape ----\" , node.outputs[0].get_shape())\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>Conv2DBackpropInput ----&gt;&gt;&gt;&gt;&gt; outputsshape ---- (?, 7, 7, 32)\n</code></pre>\n<p>Same graph converted to .pbtxt by:</p>\n<pre><code>tf.train.write_graph(graph, \"graphx/\" , \"sample_this_1.pbtxt\")\n</code></pre>\n<p>Reading it again from .pbtxt</p>\n<pre><code>tf.reset_default_graph()\ngram = tf.get_default_graph()\ngram.get_operations()\nfrom tensorflow.core.framework import graph_pb2\nfrom google.protobuf import text_format as pbtf\n\ngdef = graph_pb2.GraphDef()\n\nwith open('graphx/sample_this_1.pbtxt', 'r') as f:\n    graph_str = f.read()\n\npbtf.Merge(graph_str, gdef)\n\ntf.import_graph_def(gdef)\n</code></pre>\n<p>Now doing the same for <code>gram</code> I get,</p>\n<pre><code>for node in gram.get_operations():\n  if node.type == \"Conv2DBackpropInput\":\n    print(node.type,\"----&gt;&gt;&gt;&gt;&gt;\", \"outputsshape ----\" , node.outputs[0].get_shape())\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>Conv2DBackpropInput ----&gt;&gt;&gt;&gt;&gt; outputsshape ---- (?, ?, ?, 32)\n</code></pre>\n<p>So, the <code>outputs --- shape</code> went from (? ,7 ,7 ,32) to (? , ? , ? ,32) after reading from .pbtxt</p>\n<p>Why is that? Is it a bug because output_shape is a part of model definition I guess?</p>\n<p>Thanks.</p>", "body_text": "This is a custom code\n\n\n I'm on Mac OSX 10.11.16\n\n\n TensorFlow version 1.6.0\n\n\n Bazel version N/A\n\n\n  Tensorflow installed from source\n\n\n CUDA/cuDNN version N/A\n\n\n GPU model and memory N/A\n\n\n Exact command to reproduce\n\n\nThe output shape of the node with node. Type Conv2DBackpropInput seems to lose height and width information as in this:\nThis is denconv layer definition and conv3 layer after it\n`deconv = tf.layers.conv2d_transpose(pool2 , filters = 32 ,kernel_size = [2,2],strides=(1, 1) , padding = \"same\")\nconv3 = tf.layers.conv2d(inputs= deconv,filters=64,kernel_size=[1, 1],padding=\"same\", activation=tf.nn.relu)`\ngraph = tf.get_default_graph() is built in the notebook\nfor node in graph.get_operations():\n  if node.type == \"Conv2DBackpropInput\":\n    print(node.type,\"---->>>>>\", \"outputsshape ----\" , node.outputs[0].get_shape())\n\nOutput:\nConv2DBackpropInput ---->>>>> outputsshape ---- (?, 7, 7, 32)\n\nSame graph converted to .pbtxt by:\ntf.train.write_graph(graph, \"graphx/\" , \"sample_this_1.pbtxt\")\n\nReading it again from .pbtxt\ntf.reset_default_graph()\ngram = tf.get_default_graph()\ngram.get_operations()\nfrom tensorflow.core.framework import graph_pb2\nfrom google.protobuf import text_format as pbtf\n\ngdef = graph_pb2.GraphDef()\n\nwith open('graphx/sample_this_1.pbtxt', 'r') as f:\n    graph_str = f.read()\n\npbtf.Merge(graph_str, gdef)\n\ntf.import_graph_def(gdef)\n\nNow doing the same for gram I get,\nfor node in gram.get_operations():\n  if node.type == \"Conv2DBackpropInput\":\n    print(node.type,\"---->>>>>\", \"outputsshape ----\" , node.outputs[0].get_shape())\n\nOutput:\nConv2DBackpropInput ---->>>>> outputsshape ---- (?, ?, ?, 32)\n\nSo, the outputs --- shape went from (? ,7 ,7 ,32) to (? , ? , ? ,32) after reading from .pbtxt\nWhy is that? Is it a bug because output_shape is a part of model definition I guess?\nThanks.", "body": "- [x] This is a custom code\r\n- [x] I'm on Mac OSX 10.11.16\r\n- [x] TensorFlow version 1.6.0\r\n- [x] Bazel version N/A\r\n- [x]  Tensorflow installed from source\r\n- [x] CUDA/cuDNN version N/A\r\n- [x] GPU model and memory N/A\r\n\r\n- [x] Exact command to reproduce\r\n\r\n\r\n\r\nThe output shape of the node with node. Type `Conv2DBackpropInput` seems to lose height and width information as in this:\r\nThis is denconv layer definition and conv3 layer after it\r\n`deconv = tf.layers.conv2d_transpose(pool2 , filters = 32 ,kernel_size = [2,2],strides=(1, 1) , padding = \"same\")\r\n\r\n\r\nconv3 = tf.layers.conv2d(inputs= deconv,filters=64,kernel_size=[1, 1],padding=\"same\", activation=tf.nn.relu)`\r\n\r\n`graph = tf.get_default_graph()` is built in the notebook\r\n\r\n    for node in graph.get_operations():\r\n      if node.type == \"Conv2DBackpropInput\":\r\n        print(node.type,\"---->>>>>\", \"outputsshape ----\" , node.outputs[0].get_shape())\r\n\r\n**Output:**\r\n \r\n    Conv2DBackpropInput ---->>>>> outputsshape ---- (?, 7, 7, 32)\r\n\r\n\r\nSame graph converted to .pbtxt by:\r\n\r\n    tf.train.write_graph(graph, \"graphx/\" , \"sample_this_1.pbtxt\")\r\n\r\nReading it again from .pbtxt\r\n\r\n    tf.reset_default_graph()\r\n    gram = tf.get_default_graph()\r\n    gram.get_operations()\r\n    from tensorflow.core.framework import graph_pb2\r\n    from google.protobuf import text_format as pbtf\r\n    \r\n    gdef = graph_pb2.GraphDef()\r\n    \r\n    with open('graphx/sample_this_1.pbtxt', 'r') as f:\r\n        graph_str = f.read()\r\n    \r\n    pbtf.Merge(graph_str, gdef)\r\n    \r\n    tf.import_graph_def(gdef)\r\n\r\n\r\nNow doing the same for `gram` I get,\r\n\r\n    for node in gram.get_operations():\r\n      if node.type == \"Conv2DBackpropInput\":\r\n        print(node.type,\"---->>>>>\", \"outputsshape ----\" , node.outputs[0].get_shape())\r\n\r\n**Output:**\r\n\r\n    Conv2DBackpropInput ---->>>>> outputsshape ---- (?, ?, ?, 32)\r\n\r\n\r\nSo, the `outputs --- shape` went from (? ,7 ,7 ,32) to (? , ? , ? ,32) after reading from .pbtxt\r\n\r\nWhy is that? Is it a bug because output_shape is a part of model definition I guess?\r\n\r\nThanks."}