{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16826", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16826/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16826/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16826/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16826", "id": 295074361, "node_id": "MDU6SXNzdWUyOTUwNzQzNjE=", "number": 16826, "title": "first session.Run() when inference too slow on android and mac with c++ api", "user": {"login": "kaierlong", "id": 1741667, "node_id": "MDQ6VXNlcjE3NDE2Njc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1741667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kaierlong", "html_url": "https://github.com/kaierlong", "followers_url": "https://api.github.com/users/kaierlong/followers", "following_url": "https://api.github.com/users/kaierlong/following{/other_user}", "gists_url": "https://api.github.com/users/kaierlong/gists{/gist_id}", "starred_url": "https://api.github.com/users/kaierlong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kaierlong/subscriptions", "organizations_url": "https://api.github.com/users/kaierlong/orgs", "repos_url": "https://api.github.com/users/kaierlong/repos", "events_url": "https://api.github.com/users/kaierlong/events{/privacy}", "received_events_url": "https://api.github.com/users/kaierlong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-07T10:01:27Z", "updated_at": "2018-02-07T22:43:01Z", "closed_at": "2018-02-07T22:43:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Android-armeabi-v7a, macOS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.0 (for building .so file), 1.3.0 (for model train)</li>\n<li><strong>Python version</strong>: 3.5.2 (Just used for train)</li>\n<li><strong>Bazel version (if compiling from source)</strong>:0.8.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: Apple LLVM version 9.0.0</li>\n<li><strong>CUDA/cuDNN version</strong>: V8.0.61 (Just used for train)</li>\n<li><strong>GPU model and memory</strong>: 11GB (Just used for train)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p><em><strong>In a word, after I loaded the model, the cost time of first use session.Run() is longer(4x) than the second use of session.Run() and the third... when coding with C++ API.</strong></em></p>\n<p>First, I used tensorflow-1.3.0 (python) to define and train a rnn model. Then, I used <code>freeze_graph </code> and <code>transform_graph </code> to make the model files to be one file and shrink the model file size. After setting the <code>&lt;WORKSPACE&gt;</code> by adding the following lines, I build the benchmark tool to test the performance.</p>\n<pre><code>android_sdk_repository(\n    name = \"androidsdk\",\n    api_level = 23,\n    build_tools_version = \"27.0.1\",\n    # Replace with path to Android SDK on your system\n    path = \"/Users/XXX/Library/Android/sdk/\",\n)\nandroid_ndk_repository(\n    name=\"androidndk\",\n    path=\"/Users/XXX/Downloads/android-ndk-r12b/\",\n    api_level=14)\n</code></pre>\n<p>building benchmark tool like this:</p>\n<p><code>bazel build -c opt --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic   tensorflow/tools/benchmark:benchmark_model</code></p>\n<p>test model performance like this:</p>\n<p><code>./bechmark_model --graph=\"frozen_model.pb\" --input_layer=\"Placeholder_data/inputs_placeholder:0,Placeholder_data/length_placeholder:0\" --input_layer_shape=\"1,10:1\" --input_layer_type=\"int32,int32\" --input_layer_values=\"6,13:2\" --output_layer=\"Top_ids/topk:1\"</code></p>\n<p>and the performance on Oneplus3T(android 8.0):</p>\n<pre><code>native : stat_summarizer.cc:468 Timings (microseconds): count=300 first=19337 curr=24105 min=19337 max=39753 avg=26515.4 std=4461\nnative : stat_summarizer.cc:468 Memory (bytes): count=300 curr=1382834(all same)\nnative : stat_summarizer.cc:468 188 nodes observed\nnative : stat_summarizer.cc:468\n</code></pre>\n<p>and then I tried to use C++ API to use this model, some major code as following:</p>\n<p><strong>model load:</strong></p>\n<pre><code>Status RNNInference::InitializeSession(\n    int num_threads, const std::string&amp; graph,\n    std::unique_ptr&lt;Session&gt;* session,\n    std::unique_ptr&lt;GraphDef&gt;* graph_def) {\n  tensorflow::SessionOptions options;\n  tensorflow::ConfigProto&amp; config = options.config;\n // here num_threads = -1\n  if (num_threads &gt; 0) {\n    config.set_intra_op_parallelism_threads(num_threads); \n  }\n  LOG(INFO) &lt;&lt; \" Got config, \" &lt;&lt; config.device_count_size() &lt;&lt; \" devices\";\n  session-&gt;reset(tensorflow::NewSession(options));\n  graph_def-&gt;reset(new GraphDef());\n  tensorflow::GraphDef tensorflow_graph;\n\n  Status s = ReadBinaryProto(Env::Default(), graph, graph_def-&gt;get());\n  if (!s.ok()) {\n    LOG(ERROR) &lt;&lt; \"Could not create TensorFlow Graph: \" &lt;&lt; s;\n    return s;\n  }\n\n  s = (*session)-&gt;Create(*(graph_def-&gt;get()));\n  if (!s.ok()) {\n    LOG(ERROR) &lt;&lt; \"Could not create TensorFlow Session: \" &lt;&lt; s;\n    return s;\n  }\n\n  return Status::OK();\n}\n</code></pre>\n<p><strong>first inference:</strong></p>\n<pre><code>int RNNInference::InitializePredict() {\n  int ram_size = GetRamKB();\n  // \u5185\u5b58\u9650\u5236\n  if (ram_size &lt; MIN_MEMORY_SIZE) {\n    return -1;\n  }\n\n  tensorflow::Status s;\n  const int64 start_time = Env::Default()-&gt;NowMicros();\n  s = session_.get()-&gt;Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, {});\n  if (!s.ok()) {\n    // return s;\n    LOG(ERROR) &lt;&lt; \" rnn_inference -- session error : \" &lt;&lt; s;\n    return -1;\n  }\n  const int64 end_time = Env::Default()-&gt;NowMicros();\n  int64 use_time = end_time - start_time;\n  LOG(INFO) &lt;&lt; \" rnn_inference -- init session use time is \" &lt;&lt; use_time;\n\n  // \u901f\u5ea6\u6d4b\u8bd5 + \u9650\u5236 1 - 20\n  if (use_time &lt; 1000000) {\n    return ceil(use_time / 50000);\n  }\n\n  // default\n  return 0;\n}\n</code></pre>\n<p><strong>second inference and so on:</strong></p>\n<pre><code>  // Keep output results\n  std::vector&lt;tensorflow::Tensor&gt; output_tensors;\n  // Assign new data to model input\n  int history_count = input_words_index.size();\n  AssignVaulesFromWordHistroy(input_words_index, history_count);\n  tensorflow::Status s;\n  const int64 start_time = Env::Default()-&gt;NowMicros();\n  s = session-&gt;Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, &amp;output_tensors);\n  const int64 end_time = Env::Default()-&gt;NowMicros();\n  int64 use_time = end_time - start_time;\n  LOG(INFO) &lt;&lt; \" input inference use \" &lt;&lt; use_time;\n  if (!s.ok()) {\n    return s;\n  }\n  auto output_matrix = output_tensors[0].matrix&lt;int32&gt;();\n  // dim_size is int64, which is long long\n  // int first_dim = (int)(output_tensors[0].dim_size(0) - 1);\n  int first_dim = history_count - 1;\n  int second_dim = (int)(output_tensors[0].dim_size(1));\n  for(int n = 0; n &lt; second_dim; ++n){\n    // Pass pad and unk\n    if (output_matrix(first_dim, n) &gt; 1) { \n      // int is equal to int32\n      output_words_index-&gt;push_back(output_matrix(first_dim, n));\n    }\n  }\n</code></pre>\n<p>And I build these file or say my app like this and <code>adb push</code> it to my android device:</p>\n<p><code>bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/contrib/xxx/myapp:demo</code></p>\n<blockquote>\n<p>here I used <code>print_selective_registration_header </code> to get <code>ops_to_register .h</code></p>\n</blockquote>\n<p>Last, I run it on android and test it performance about real application.<br>\n<strong>Output:(only show time cost)</strong></p>\n<pre><code>native : inference.cc:126  rnn_inference -- init session use time is 198629 (first)\nnative : inference.cc:165  input inference use 53291 (second)\nnative : inference.cc:165  input inference use 50341 (third)\nnative : inference.cc:165  input inference use 60115 (fourth)\nnative : inference.cc:165  input inference use 44707 (fifth)\n</code></pre>\n<p><em><strong>If you need other information, please let me know!</strong></em></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android-armeabi-v7a, macOS\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.4.0 (for building .so file), 1.3.0 (for model train)\nPython version: 3.5.2 (Just used for train)\nBazel version (if compiling from source):0.8.0\nGCC/Compiler version (if compiling from source): Apple LLVM version 9.0.0\nCUDA/cuDNN version: V8.0.61 (Just used for train)\nGPU model and memory: 11GB (Just used for train)\nExact command to reproduce:\n\nIn a word, after I loaded the model, the cost time of first use session.Run() is longer(4x) than the second use of session.Run() and the third... when coding with C++ API.\nFirst, I used tensorflow-1.3.0 (python) to define and train a rnn model. Then, I used freeze_graph  and transform_graph  to make the model files to be one file and shrink the model file size. After setting the <WORKSPACE> by adding the following lines, I build the benchmark tool to test the performance.\nandroid_sdk_repository(\n    name = \"androidsdk\",\n    api_level = 23,\n    build_tools_version = \"27.0.1\",\n    # Replace with path to Android SDK on your system\n    path = \"/Users/XXX/Library/Android/sdk/\",\n)\nandroid_ndk_repository(\n    name=\"androidndk\",\n    path=\"/Users/XXX/Downloads/android-ndk-r12b/\",\n    api_level=14)\n\nbuilding benchmark tool like this:\nbazel build -c opt --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic   tensorflow/tools/benchmark:benchmark_model\ntest model performance like this:\n./bechmark_model --graph=\"frozen_model.pb\" --input_layer=\"Placeholder_data/inputs_placeholder:0,Placeholder_data/length_placeholder:0\" --input_layer_shape=\"1,10:1\" --input_layer_type=\"int32,int32\" --input_layer_values=\"6,13:2\" --output_layer=\"Top_ids/topk:1\"\nand the performance on Oneplus3T(android 8.0):\nnative : stat_summarizer.cc:468 Timings (microseconds): count=300 first=19337 curr=24105 min=19337 max=39753 avg=26515.4 std=4461\nnative : stat_summarizer.cc:468 Memory (bytes): count=300 curr=1382834(all same)\nnative : stat_summarizer.cc:468 188 nodes observed\nnative : stat_summarizer.cc:468\n\nand then I tried to use C++ API to use this model, some major code as following:\nmodel load:\nStatus RNNInference::InitializeSession(\n    int num_threads, const std::string& graph,\n    std::unique_ptr<Session>* session,\n    std::unique_ptr<GraphDef>* graph_def) {\n  tensorflow::SessionOptions options;\n  tensorflow::ConfigProto& config = options.config;\n // here num_threads = -1\n  if (num_threads > 0) {\n    config.set_intra_op_parallelism_threads(num_threads); \n  }\n  LOG(INFO) << \" Got config, \" << config.device_count_size() << \" devices\";\n  session->reset(tensorflow::NewSession(options));\n  graph_def->reset(new GraphDef());\n  tensorflow::GraphDef tensorflow_graph;\n\n  Status s = ReadBinaryProto(Env::Default(), graph, graph_def->get());\n  if (!s.ok()) {\n    LOG(ERROR) << \"Could not create TensorFlow Graph: \" << s;\n    return s;\n  }\n\n  s = (*session)->Create(*(graph_def->get()));\n  if (!s.ok()) {\n    LOG(ERROR) << \"Could not create TensorFlow Session: \" << s;\n    return s;\n  }\n\n  return Status::OK();\n}\n\nfirst inference:\nint RNNInference::InitializePredict() {\n  int ram_size = GetRamKB();\n  // \u5185\u5b58\u9650\u5236\n  if (ram_size < MIN_MEMORY_SIZE) {\n    return -1;\n  }\n\n  tensorflow::Status s;\n  const int64 start_time = Env::Default()->NowMicros();\n  s = session_.get()->Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, {});\n  if (!s.ok()) {\n    // return s;\n    LOG(ERROR) << \" rnn_inference -- session error : \" << s;\n    return -1;\n  }\n  const int64 end_time = Env::Default()->NowMicros();\n  int64 use_time = end_time - start_time;\n  LOG(INFO) << \" rnn_inference -- init session use time is \" << use_time;\n\n  // \u901f\u5ea6\u6d4b\u8bd5 + \u9650\u5236 1 - 20\n  if (use_time < 1000000) {\n    return ceil(use_time / 50000);\n  }\n\n  // default\n  return 0;\n}\n\nsecond inference and so on:\n  // Keep output results\n  std::vector<tensorflow::Tensor> output_tensors;\n  // Assign new data to model input\n  int history_count = input_words_index.size();\n  AssignVaulesFromWordHistroy(input_words_index, history_count);\n  tensorflow::Status s;\n  const int64 start_time = Env::Default()->NowMicros();\n  s = session->Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, &output_tensors);\n  const int64 end_time = Env::Default()->NowMicros();\n  int64 use_time = end_time - start_time;\n  LOG(INFO) << \" input inference use \" << use_time;\n  if (!s.ok()) {\n    return s;\n  }\n  auto output_matrix = output_tensors[0].matrix<int32>();\n  // dim_size is int64, which is long long\n  // int first_dim = (int)(output_tensors[0].dim_size(0) - 1);\n  int first_dim = history_count - 1;\n  int second_dim = (int)(output_tensors[0].dim_size(1));\n  for(int n = 0; n < second_dim; ++n){\n    // Pass pad and unk\n    if (output_matrix(first_dim, n) > 1) { \n      // int is equal to int32\n      output_words_index->push_back(output_matrix(first_dim, n));\n    }\n  }\n\nAnd I build these file or say my app like this and adb push it to my android device:\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/contrib/xxx/myapp:demo\n\nhere I used print_selective_registration_header  to get ops_to_register .h\n\nLast, I run it on android and test it performance about real application.\nOutput:(only show time cost)\nnative : inference.cc:126  rnn_inference -- init session use time is 198629 (first)\nnative : inference.cc:165  input inference use 53291 (second)\nnative : inference.cc:165  input inference use 50341 (third)\nnative : inference.cc:165  input inference use 60115 (fourth)\nnative : inference.cc:165  input inference use 44707 (fifth)\n\nIf you need other information, please let me know!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android-armeabi-v7a, macOS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0 (for building .so file), 1.3.0 (for model train)\r\n- **Python version**: 3.5.2 (Just used for train)\r\n- **Bazel version (if compiling from source)**:0.8.0\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0\r\n- **CUDA/cuDNN version**: V8.0.61 (Just used for train)\r\n- **GPU model and memory**: 11GB (Just used for train)\r\n- **Exact command to reproduce**:\r\n\r\n***In a word, after I loaded the model, the cost time of first use session.Run() is longer(4x) than the second use of session.Run() and the third... when coding with C++ API.***\r\n\r\nFirst, I used tensorflow-1.3.0 (python) to define and train a rnn model. Then, I used `freeze_graph ` and `transform_graph ` to make the model files to be one file and shrink the model file size. After setting the `<WORKSPACE>` by adding the following lines, I build the benchmark tool to test the performance.\r\n```\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    build_tools_version = \"27.0.1\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/Users/XXX/Library/Android/sdk/\",\r\n)\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/Users/XXX/Downloads/android-ndk-r12b/\",\r\n    api_level=14)\r\n```\r\nbuilding benchmark tool like this:\r\n\r\n`bazel build -c opt --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic   tensorflow/tools/benchmark:benchmark_model`\r\n\r\ntest model performance like this:\r\n\r\n`./bechmark_model --graph=\"frozen_model.pb\" --input_layer=\"Placeholder_data/inputs_placeholder:0,Placeholder_data/length_placeholder:0\" --input_layer_shape=\"1,10:1\" --input_layer_type=\"int32,int32\" --input_layer_values=\"6,13:2\" --output_layer=\"Top_ids/topk:1\"`\r\n\r\nand the performance on Oneplus3T(android 8.0):\r\n\r\n```\r\nnative : stat_summarizer.cc:468 Timings (microseconds): count=300 first=19337 curr=24105 min=19337 max=39753 avg=26515.4 std=4461\r\nnative : stat_summarizer.cc:468 Memory (bytes): count=300 curr=1382834(all same)\r\nnative : stat_summarizer.cc:468 188 nodes observed\r\nnative : stat_summarizer.cc:468\r\n```\r\n\r\nand then I tried to use C++ API to use this model, some major code as following:\r\n\r\n**model load:**\r\n```\r\nStatus RNNInference::InitializeSession(\r\n    int num_threads, const std::string& graph,\r\n    std::unique_ptr<Session>* session,\r\n    std::unique_ptr<GraphDef>* graph_def) {\r\n  tensorflow::SessionOptions options;\r\n  tensorflow::ConfigProto& config = options.config;\r\n // here num_threads = -1\r\n  if (num_threads > 0) {\r\n    config.set_intra_op_parallelism_threads(num_threads); \r\n  }\r\n  LOG(INFO) << \" Got config, \" << config.device_count_size() << \" devices\";\r\n  session->reset(tensorflow::NewSession(options));\r\n  graph_def->reset(new GraphDef());\r\n  tensorflow::GraphDef tensorflow_graph;\r\n\r\n  Status s = ReadBinaryProto(Env::Default(), graph, graph_def->get());\r\n  if (!s.ok()) {\r\n    LOG(ERROR) << \"Could not create TensorFlow Graph: \" << s;\r\n    return s;\r\n  }\r\n\r\n  s = (*session)->Create(*(graph_def->get()));\r\n  if (!s.ok()) {\r\n    LOG(ERROR) << \"Could not create TensorFlow Session: \" << s;\r\n    return s;\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n```\r\n\r\n**first inference:**\r\n```\r\nint RNNInference::InitializePredict() {\r\n  int ram_size = GetRamKB();\r\n  // \u5185\u5b58\u9650\u5236\r\n  if (ram_size < MIN_MEMORY_SIZE) {\r\n    return -1;\r\n  }\r\n\r\n  tensorflow::Status s;\r\n  const int64 start_time = Env::Default()->NowMicros();\r\n  s = session_.get()->Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, {});\r\n  if (!s.ok()) {\r\n    // return s;\r\n    LOG(ERROR) << \" rnn_inference -- session error : \" << s;\r\n    return -1;\r\n  }\r\n  const int64 end_time = Env::Default()->NowMicros();\r\n  int64 use_time = end_time - start_time;\r\n  LOG(INFO) << \" rnn_inference -- init session use time is \" << use_time;\r\n\r\n  // \u901f\u5ea6\u6d4b\u8bd5 + \u9650\u5236 1 - 20\r\n  if (use_time < 1000000) {\r\n    return ceil(use_time / 50000);\r\n  }\r\n\r\n  // default\r\n  return 0;\r\n}\r\n```\r\n\r\n**second inference and so on:**\r\n\r\n```\r\n  // Keep output results\r\n  std::vector<tensorflow::Tensor> output_tensors;\r\n  // Assign new data to model input\r\n  int history_count = input_words_index.size();\r\n  AssignVaulesFromWordHistroy(input_words_index, history_count);\r\n  tensorflow::Status s;\r\n  const int64 start_time = Env::Default()->NowMicros();\r\n  s = session->Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, &output_tensors);\r\n  const int64 end_time = Env::Default()->NowMicros();\r\n  int64 use_time = end_time - start_time;\r\n  LOG(INFO) << \" input inference use \" << use_time;\r\n  if (!s.ok()) {\r\n    return s;\r\n  }\r\n  auto output_matrix = output_tensors[0].matrix<int32>();\r\n  // dim_size is int64, which is long long\r\n  // int first_dim = (int)(output_tensors[0].dim_size(0) - 1);\r\n  int first_dim = history_count - 1;\r\n  int second_dim = (int)(output_tensors[0].dim_size(1));\r\n  for(int n = 0; n < second_dim; ++n){\r\n    // Pass pad and unk\r\n    if (output_matrix(first_dim, n) > 1) { \r\n      // int is equal to int32\r\n      output_words_index->push_back(output_matrix(first_dim, n));\r\n    }\r\n  }\r\n```\r\n\r\nAnd I build these file or say my app like this and `adb push` it to my android device:\r\n\r\n`bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/contrib/xxx/myapp:demo`\r\n> here I used `print_selective_registration_header ` to get `ops_to_register .h`\r\n\r\nLast, I run it on android and test it performance about real application.\r\n**Output:(only show time cost)**\r\n```\r\nnative : inference.cc:126  rnn_inference -- init session use time is 198629 (first)\r\nnative : inference.cc:165  input inference use 53291 (second)\r\nnative : inference.cc:165  input inference use 50341 (third)\r\nnative : inference.cc:165  input inference use 60115 (fourth)\r\nnative : inference.cc:165  input inference use 44707 (fifth)\r\n```\r\n\r\n***If you need other information, please let me know!***\r\n"}