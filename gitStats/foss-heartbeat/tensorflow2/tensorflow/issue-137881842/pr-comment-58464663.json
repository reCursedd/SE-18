{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/58464663", "pull_request_review_id": null, "id": 58464663, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDY0NjYz", "diff_hunk": "@@ -311,12 +311,14 @@ y_ = tf.placeholder(tf.float32, [None, 10])\n Then we can implement the cross-entropy, \\\\(-\\sum y'\\log(y)\\\\):\n \n ```python\n-cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n+cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n ```\n \n First, `tf.log` computes the logarithm of each element of `y`. Next, we multiply\n-each element of `y_` with the corresponding element of `tf.log(y)`. Finally,\n-`tf.reduce_sum` adds all the elements of the tensor.\n+each element of `y_` with the corresponding element of `tf.log(y)`. Then \n+`tf.reduce_sum` adds the elements in the second dimension of y, due to the \n+`reduction_indices=[1]` parameter. Finally,  `tf.reduce_mean` computes the mean\n+over all the examples in the batch.\n \n Note that this isn't just the cross-entropy of the truth with a single", "path": "tensorflow/g3doc/tutorials/mnist/beginners/index.md", "position": null, "original_position": 16, "commit_id": "999fc893a07b77352795c4f1b81b3d892e5981ec", "original_commit_id": "f2a4db9860ec78cec2e35242904d7366f5f9d5d8", "user": {"login": "Sohl-Dickstein", "id": 498544, "node_id": "MDQ6VXNlcjQ5ODU0NA==", "avatar_url": "https://avatars2.githubusercontent.com/u/498544?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sohl-Dickstein", "html_url": "https://github.com/Sohl-Dickstein", "followers_url": "https://api.github.com/users/Sohl-Dickstein/followers", "following_url": "https://api.github.com/users/Sohl-Dickstein/following{/other_user}", "gists_url": "https://api.github.com/users/Sohl-Dickstein/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sohl-Dickstein/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sohl-Dickstein/subscriptions", "organizations_url": "https://api.github.com/users/Sohl-Dickstein/orgs", "repos_url": "https://api.github.com/users/Sohl-Dickstein/repos", "events_url": "https://api.github.com/users/Sohl-Dickstein/events{/privacy}", "received_events_url": "https://api.github.com/users/Sohl-Dickstein/received_events", "type": "User", "site_admin": false}, "body": "Delete this paragraph:\n\n\"Note that this isn't just the cross-entropy of the truth with a single prediction, but the sum of the cross-entropies for all the images we looked at. In this example, we have 100 images in each batch: how well we are doing on 100 data points is a much better description of how good our model is than a single data point.\"\n\n(it's no longer true)\n", "created_at": "2016-04-04T23:12:06Z", "updated_at": "2016-04-05T17:14:50Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1358#discussion_r58464663", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1358", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/58464663"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1358#discussion_r58464663"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1358"}}, "body_html": "<p>Delete this paragraph:</p>\n<p>\"Note that this isn't just the cross-entropy of the truth with a single prediction, but the sum of the cross-entropies for all the images we looked at. In this example, we have 100 images in each batch: how well we are doing on 100 data points is a much better description of how good our model is than a single data point.\"</p>\n<p>(it's no longer true)</p>", "body_text": "Delete this paragraph:\n\"Note that this isn't just the cross-entropy of the truth with a single prediction, but the sum of the cross-entropies for all the images we looked at. In this example, we have 100 images in each batch: how well we are doing on 100 data points is a much better description of how good our model is than a single data point.\"\n(it's no longer true)"}