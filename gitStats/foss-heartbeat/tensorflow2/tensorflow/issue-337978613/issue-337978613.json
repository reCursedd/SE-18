{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20524", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20524/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20524/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20524/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20524", "id": 337978613, "node_id": "MDU6SXNzdWUzMzc5Nzg2MTM=", "number": 20524, "title": "Explanation for why tf.gradients() no longer propagates through integer tensors", "user": {"login": "tgaddair", "id": 1742912, "node_id": "MDQ6VXNlcjE3NDI5MTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1742912?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgaddair", "html_url": "https://github.com/tgaddair", "followers_url": "https://api.github.com/users/tgaddair/followers", "following_url": "https://api.github.com/users/tgaddair/following{/other_user}", "gists_url": "https://api.github.com/users/tgaddair/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgaddair/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgaddair/subscriptions", "organizations_url": "https://api.github.com/users/tgaddair/orgs", "repos_url": "https://api.github.com/users/tgaddair/repos", "events_url": "https://api.github.com/users/tgaddair/events{/privacy}", "received_events_url": "https://api.github.com/users/tgaddair/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-03T16:49:20Z", "updated_at": "2018-08-14T16:07:23Z", "closed_at": "2018-07-29T02:39:39Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac OSX 10.13.4</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.0-dev20180628 (nightly)</li>\n<li><strong>Python version</strong>: 2.7.15</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>For performance reasons, we sometimes work with integer tensors like <code>tf.uint8</code>.  We have custom ops that operate on these tensors, and in our unit tests we verify the correctness of the gradients using <code>tf.gradients()</code>.  However, it seems as of version 1.9, the behavior of <code>tf.gradients()</code> has been changed so that it no longer backpropagates through integer tensors.</p>\n<p>This was causing <code>tf.gradients()</code> to always return <code>None</code>.</p>\n<p>This seems like a significant change that introduces inconsistent behavior (or at least special cases that need to be accounted for).  I assume there's a good rationale, but could additional documentation be added to explain why this change was necessary (or deemed desirable)?  The only documntation I'm currently aware of is this line from the release notes:</p>\n<blockquote>\n<p>Prevent <code>tf.gradients()</code> from backpropagating through integer tensors.</p>\n</blockquote>\n<p>And the following comment in <code>gradients_impl.py</code>:</p>\n<blockquote>\n<p>All integer tensors are considered constant with respect to all <code>xs</code>, as if they were included in <code>stop_gradients</code>.</p>\n</blockquote>\n<p>Thanks.</p>\n<h3>Source code / logs</h3>\n<pre><code>    with self.test_session(config=self.config) as session:\n        t = tf.ones(5, dtype=tf.float32) * 2\n        t2 = t**2\n\n        grad_ys = tf.ones(5)\n        grad = tf.gradients(t2, t, grad_ys)[0]\n        grad_out = session.run(grad)\n        print(\"grad_out: \", grad_out)\n</code></pre>\n<p>Prints <code>[4. 4. 4. 4. 4.]</code> as expected</p>\n<pre><code>    with self.test_session(config=self.config) as session:\n        t = tf.ones(5, dtype=tf.int32) * 2\n        t2 = t**2\n\n        grad_ys = tf.ones(5)\n        grad = tf.gradients(t2, t, grad_ys)[0]\n        grad_out = session.run(grad)\n        print(\"grad_out: \", grad_out)\n</code></pre>\n<p>Raises <code>TypeError: Fetch argument None has invalid type &lt;type 'NoneType'&gt;</code></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.13.4\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.10.0-dev20180628 (nightly)\nPython version: 2.7.15\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nFor performance reasons, we sometimes work with integer tensors like tf.uint8.  We have custom ops that operate on these tensors, and in our unit tests we verify the correctness of the gradients using tf.gradients().  However, it seems as of version 1.9, the behavior of tf.gradients() has been changed so that it no longer backpropagates through integer tensors.\nThis was causing tf.gradients() to always return None.\nThis seems like a significant change that introduces inconsistent behavior (or at least special cases that need to be accounted for).  I assume there's a good rationale, but could additional documentation be added to explain why this change was necessary (or deemed desirable)?  The only documntation I'm currently aware of is this line from the release notes:\n\nPrevent tf.gradients() from backpropagating through integer tensors.\n\nAnd the following comment in gradients_impl.py:\n\nAll integer tensors are considered constant with respect to all xs, as if they were included in stop_gradients.\n\nThanks.\nSource code / logs\n    with self.test_session(config=self.config) as session:\n        t = tf.ones(5, dtype=tf.float32) * 2\n        t2 = t**2\n\n        grad_ys = tf.ones(5)\n        grad = tf.gradients(t2, t, grad_ys)[0]\n        grad_out = session.run(grad)\n        print(\"grad_out: \", grad_out)\n\nPrints [4. 4. 4. 4. 4.] as expected\n    with self.test_session(config=self.config) as session:\n        t = tf.ones(5, dtype=tf.int32) * 2\n        t2 = t**2\n\n        grad_ys = tf.ones(5)\n        grad = tf.gradients(t2, t, grad_ys)[0]\n        grad_out = session.run(grad)\n        print(\"grad_out: \", grad_out)\n\nRaises TypeError: Fetch argument None has invalid type <type 'NoneType'>", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.13.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0-dev20180628 (nightly)\r\n- **Python version**: 2.7.15\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nFor performance reasons, we sometimes work with integer tensors like `tf.uint8`.  We have custom ops that operate on these tensors, and in our unit tests we verify the correctness of the gradients using `tf.gradients()`.  However, it seems as of version 1.9, the behavior of `tf.gradients()` has been changed so that it no longer backpropagates through integer tensors.\r\n\r\nThis was causing `tf.gradients()` to always return `None`.\r\n\r\nThis seems like a significant change that introduces inconsistent behavior (or at least special cases that need to be accounted for).  I assume there's a good rationale, but could additional documentation be added to explain why this change was necessary (or deemed desirable)?  The only documntation I'm currently aware of is this line from the release notes: \r\n\r\n> Prevent `tf.gradients()` from backpropagating through integer tensors.\r\n\r\nAnd the following comment in `gradients_impl.py`:\r\n\r\n> All integer tensors are considered constant with respect to all `xs`, as if they were included in `stop_gradients`.\r\n\r\nThanks.\r\n\r\n### Source code / logs\r\n\r\n        with self.test_session(config=self.config) as session:\r\n            t = tf.ones(5, dtype=tf.float32) * 2\r\n            t2 = t**2\r\n\r\n            grad_ys = tf.ones(5)\r\n            grad = tf.gradients(t2, t, grad_ys)[0]\r\n            grad_out = session.run(grad)\r\n            print(\"grad_out: \", grad_out)\r\n\r\nPrints `[4. 4. 4. 4. 4.]` as expected\r\n\r\n        with self.test_session(config=self.config) as session:\r\n            t = tf.ones(5, dtype=tf.int32) * 2\r\n            t2 = t**2\r\n\r\n            grad_ys = tf.ones(5)\r\n            grad = tf.gradients(t2, t, grad_ys)[0]\r\n            grad_out = session.run(grad)\r\n            print(\"grad_out: \", grad_out)\r\n\r\nRaises `TypeError: Fetch argument None has invalid type <type 'NoneType'>` "}