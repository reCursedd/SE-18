{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/390342227", "html_url": "https://github.com/tensorflow/tensorflow/issues/3628#issuecomment-390342227", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3628", "id": 390342227, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDM0MjIyNw==", "user": {"login": "bbrister", "id": 9013122, "node_id": "MDQ6VXNlcjkwMTMxMjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/9013122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bbrister", "html_url": "https://github.com/bbrister", "followers_url": "https://api.github.com/users/bbrister/followers", "following_url": "https://api.github.com/users/bbrister/following{/other_user}", "gists_url": "https://api.github.com/users/bbrister/gists{/gist_id}", "starred_url": "https://api.github.com/users/bbrister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bbrister/subscriptions", "organizations_url": "https://api.github.com/users/bbrister/orgs", "repos_url": "https://api.github.com/users/bbrister/repos", "events_url": "https://api.github.com/users/bbrister/events{/privacy}", "received_events_url": "https://api.github.com/users/bbrister/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-18T21:51:12Z", "updated_at": "2018-05-18T21:53:02Z", "author_association": "NONE", "body_html": "<p>I found another work-around for this. Our implementation of batch norm was using tf.cond() to distinguish between training-time and test-time behavior. At training time, the variables in batch norm have to be updated. This causes an error when those variables are converted to constants.</p>\n<p>When freezing a graph for inference only, the update operations are still present in the frozen graph because tf.cond() chooses the behavior at run-time, not compile-time. The easiest solution for me was to generate two graphs that share all of their variables, one for training and one for testing. This way you can eliminate the call to tf.cond() and distinguish behaviors at compile time. Then Tensorflow correctly removes all the update operations when calling tf.graph_util.convert_variables_to_constants() on the inference output.</p>\n<p>As other users have pointed out, one could also fix this using the 'blacklist' option in tf.graph_util.convert_variables_to_constants(). The downside of this is that unneeded ops are still present in the frozen graph.</p>\n<p>Is there going to be a more comprehensive patch for this any time soon?  I am surprised this issue has been open for almost a year with no action. This seems like a very big issue, since batch norm is so useful for training large, complex networks. It is not always practical to re-train the network without batch norm for deployment. Users should not be relying on hacks that edit the graph after the fact.</p>", "body_text": "I found another work-around for this. Our implementation of batch norm was using tf.cond() to distinguish between training-time and test-time behavior. At training time, the variables in batch norm have to be updated. This causes an error when those variables are converted to constants.\nWhen freezing a graph for inference only, the update operations are still present in the frozen graph because tf.cond() chooses the behavior at run-time, not compile-time. The easiest solution for me was to generate two graphs that share all of their variables, one for training and one for testing. This way you can eliminate the call to tf.cond() and distinguish behaviors at compile time. Then Tensorflow correctly removes all the update operations when calling tf.graph_util.convert_variables_to_constants() on the inference output.\nAs other users have pointed out, one could also fix this using the 'blacklist' option in tf.graph_util.convert_variables_to_constants(). The downside of this is that unneeded ops are still present in the frozen graph.\nIs there going to be a more comprehensive patch for this any time soon?  I am surprised this issue has been open for almost a year with no action. This seems like a very big issue, since batch norm is so useful for training large, complex networks. It is not always practical to re-train the network without batch norm for deployment. Users should not be relying on hacks that edit the graph after the fact.", "body": "I found another work-around for this. Our implementation of batch norm was using tf.cond() to distinguish between training-time and test-time behavior. At training time, the variables in batch norm have to be updated. This causes an error when those variables are converted to constants. \r\n\r\nWhen freezing a graph for inference only, the update operations are still present in the frozen graph because tf.cond() chooses the behavior at run-time, not compile-time. The easiest solution for me was to generate two graphs that share all of their variables, one for training and one for testing. This way you can eliminate the call to tf.cond() and distinguish behaviors at compile time. Then Tensorflow correctly removes all the update operations when calling tf.graph_util.convert_variables_to_constants() on the inference output.\r\n\r\nAs other users have pointed out, one could also fix this using the 'blacklist' option in tf.graph_util.convert_variables_to_constants(). The downside of this is that unneeded ops are still present in the frozen graph.\r\n\r\nIs there going to be a more comprehensive patch for this any time soon?  I am surprised this issue has been open for almost a year with no action. This seems like a very big issue, since batch norm is so useful for training large, complex networks. It is not always practical to re-train the network without batch norm for deployment. Users should not be relying on hacks that edit the graph after the fact."}