{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259038706", "html_url": "https://github.com/tensorflow/tensorflow/issues/1325#issuecomment-259038706", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1325", "id": 259038706, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTAzODcwNg==", "user": {"login": "IsaacBanjo", "id": 10407758, "node_id": "MDQ6VXNlcjEwNDA3NzU4", "avatar_url": "https://avatars0.githubusercontent.com/u/10407758?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IsaacBanjo", "html_url": "https://github.com/IsaacBanjo", "followers_url": "https://api.github.com/users/IsaacBanjo/followers", "following_url": "https://api.github.com/users/IsaacBanjo/following{/other_user}", "gists_url": "https://api.github.com/users/IsaacBanjo/gists{/gist_id}", "starred_url": "https://api.github.com/users/IsaacBanjo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IsaacBanjo/subscriptions", "organizations_url": "https://api.github.com/users/IsaacBanjo/orgs", "repos_url": "https://api.github.com/users/IsaacBanjo/repos", "events_url": "https://api.github.com/users/IsaacBanjo/events{/privacy}", "received_events_url": "https://api.github.com/users/IsaacBanjo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-08T03:51:07Z", "updated_at": "2016-11-08T03:51:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a>  Thanks for your comments and I found the same phenomenon (not retrievable by get_variable) with any defining method other than tf.get_variable(). As noticing you are one of the main contributor of TF, may I kindly ask you 2 questions:</p>\n<ol>\n<li>It seems even in the same scope, if using tf.Variable([1], name='weights') to define multiple variables (every parameter are the same including the string name), there won't be any conflict; the resulted variables actually will have different string names, whereby the system will add postfix to each string such as weights1, weights2, ... So my question is, does this imply that the variables defined using tf.Variable() are local variables instead of global (static) variables defined by tf.get_variable()?</li>\n<li>If the answer to question 1 is True, then if we define a variable with tf.Variable() in a function, and when each time this function gets called, will a new local variable be created that does not store previous state? Does that mean when defining a neural network graph, we HAVE TO USE tf.get_variable() to define weights and biases, so that the weights can be shared and previous state got kept? However, I saw some examples that use tf.Variable in graph definition function and the training also works...<br>\nI am so confused.<br>\nThank you very much for your help!</li>\n</ol>", "body_text": "@lukaszkaiser  Thanks for your comments and I found the same phenomenon (not retrievable by get_variable) with any defining method other than tf.get_variable(). As noticing you are one of the main contributor of TF, may I kindly ask you 2 questions:\n\nIt seems even in the same scope, if using tf.Variable([1], name='weights') to define multiple variables (every parameter are the same including the string name), there won't be any conflict; the resulted variables actually will have different string names, whereby the system will add postfix to each string such as weights1, weights2, ... So my question is, does this imply that the variables defined using tf.Variable() are local variables instead of global (static) variables defined by tf.get_variable()?\nIf the answer to question 1 is True, then if we define a variable with tf.Variable() in a function, and when each time this function gets called, will a new local variable be created that does not store previous state? Does that mean when defining a neural network graph, we HAVE TO USE tf.get_variable() to define weights and biases, so that the weights can be shared and previous state got kept? However, I saw some examples that use tf.Variable in graph definition function and the training also works...\nI am so confused.\nThank you very much for your help!", "body": "@lukaszkaiser  Thanks for your comments and I found the same phenomenon (not retrievable by get_variable) with any defining method other than tf.get_variable(). As noticing you are one of the main contributor of TF, may I kindly ask you 2 questions:\n1. It seems even in the same scope, if using tf.Variable([1], name='weights') to define multiple variables (every parameter are the same including the string name), there won't be any conflict; the resulted variables actually will have different string names, whereby the system will add postfix to each string such as weights1, weights2, ... So my question is, does this imply that the variables defined using tf.Variable() are local variables instead of global (static) variables defined by tf.get_variable()?\n2. If the answer to question 1 is True, then if we define a variable with tf.Variable() in a function, and when each time this function gets called, will a new local variable be created that does not store previous state? Does that mean when defining a neural network graph, we HAVE TO USE tf.get_variable() to define weights and biases, so that the weights can be shared and previous state got kept? However, I saw some examples that use tf.Variable in graph definition function and the training also works...\nI am so confused.\nThank you very much for your help! \n"}