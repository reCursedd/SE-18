{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/277122336", "html_url": "https://github.com/tensorflow/tensorflow/issues/1325#issuecomment-277122336", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1325", "id": 277122336, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NzEyMjMzNg==", "user": {"login": "jrajagopal", "id": 7073643, "node_id": "MDQ6VXNlcjcwNzM2NDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/7073643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrajagopal", "html_url": "https://github.com/jrajagopal", "followers_url": "https://api.github.com/users/jrajagopal/followers", "following_url": "https://api.github.com/users/jrajagopal/following{/other_user}", "gists_url": "https://api.github.com/users/jrajagopal/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrajagopal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrajagopal/subscriptions", "organizations_url": "https://api.github.com/users/jrajagopal/orgs", "repos_url": "https://api.github.com/users/jrajagopal/repos", "events_url": "https://api.github.com/users/jrajagopal/events{/privacy}", "received_events_url": "https://api.github.com/users/jrajagopal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-02T23:49:46Z", "updated_at": "2017-02-02T23:51:40Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22591002\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/VerityStone\">@VerityStone</a> Yes, I am using the same. I load the variables into a dictionary where each variable is referenced by its name. For inference, I use the same code which contains tf.nn.bidirectional_dynamic_rnn(), which means it will create the same set of variables but with an \"_1\" suffix in the name. It will also initialize each variable randomly.</p>\n<p>I then overwrite each \"_1\" variable thus initialized, with the value of the corresponding variable in the dictionary, i.e., the ones without the \"_1\" . To do that, I use \"for v in tf.trainable_variables()\" along with sess.run(v.assign()).</p>\n<p>Just keep in mind that this creates two superflous copies of the parameters in memory, the one that is loaded and the one in the dictionary. So, if you are running a multi-layer net, with large node sizes and input and/or output projections, you are looking at many GBs and may run out of memory.</p>", "body_text": "@VerityStone Yes, I am using the same. I load the variables into a dictionary where each variable is referenced by its name. For inference, I use the same code which contains tf.nn.bidirectional_dynamic_rnn(), which means it will create the same set of variables but with an \"_1\" suffix in the name. It will also initialize each variable randomly.\nI then overwrite each \"_1\" variable thus initialized, with the value of the corresponding variable in the dictionary, i.e., the ones without the \"_1\" . To do that, I use \"for v in tf.trainable_variables()\" along with sess.run(v.assign()).\nJust keep in mind that this creates two superflous copies of the parameters in memory, the one that is loaded and the one in the dictionary. So, if you are running a multi-layer net, with large node sizes and input and/or output projections, you are looking at many GBs and may run out of memory.", "body": "@VerityStone Yes, I am using the same. I load the variables into a dictionary where each variable is referenced by its name. For inference, I use the same code which contains tf.nn.bidirectional_dynamic_rnn(), which means it will create the same set of variables but with an \"_1\" suffix in the name. It will also initialize each variable randomly.\r\n\r\nI then overwrite each \"_1\" variable thus initialized, with the value of the corresponding variable in the dictionary, i.e., the ones without the \"_1\" . To do that, I use \"for v in tf.trainable_variables()\" along with sess.run(v.assign()). \r\n\r\nJust keep in mind that this creates two superflous copies of the parameters in memory, the one that is loaded and the one in the dictionary. So, if you are running a multi-layer net, with large node sizes and input and/or output projections, you are looking at many GBs and may run out of memory."}