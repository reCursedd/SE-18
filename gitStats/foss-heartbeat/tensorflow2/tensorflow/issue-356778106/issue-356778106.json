{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22052", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22052/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22052/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22052/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22052", "id": 356778106, "node_id": "MDU6SXNzdWUzNTY3NzgxMDY=", "number": 22052, "title": "Variables in bijectors cannot be reused.", "user": {"login": "shuiruge", "id": 2098947, "node_id": "MDQ6VXNlcjIwOTg5NDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2098947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shuiruge", "html_url": "https://github.com/shuiruge", "followers_url": "https://api.github.com/users/shuiruge/followers", "following_url": "https://api.github.com/users/shuiruge/following{/other_user}", "gists_url": "https://api.github.com/users/shuiruge/gists{/gist_id}", "starred_url": "https://api.github.com/users/shuiruge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shuiruge/subscriptions", "organizations_url": "https://api.github.com/users/shuiruge/orgs", "repos_url": "https://api.github.com/users/shuiruge/repos", "events_url": "https://api.github.com/users/shuiruge/events{/privacy}", "received_events_url": "https://api.github.com/users/shuiruge/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "jvdillon", "id": 1137078, "node_id": "MDQ6VXNlcjExMzcwNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1137078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jvdillon", "html_url": "https://github.com/jvdillon", "followers_url": "https://api.github.com/users/jvdillon/followers", "following_url": "https://api.github.com/users/jvdillon/following{/other_user}", "gists_url": "https://api.github.com/users/jvdillon/gists{/gist_id}", "starred_url": "https://api.github.com/users/jvdillon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jvdillon/subscriptions", "organizations_url": "https://api.github.com/users/jvdillon/orgs", "repos_url": "https://api.github.com/users/jvdillon/repos", "events_url": "https://api.github.com/users/jvdillon/events{/privacy}", "received_events_url": "https://api.github.com/users/jvdillon/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jvdillon", "id": 1137078, "node_id": "MDQ6VXNlcjExMzcwNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1137078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jvdillon", "html_url": "https://github.com/jvdillon", "followers_url": "https://api.github.com/users/jvdillon/followers", "following_url": "https://api.github.com/users/jvdillon/following{/other_user}", "gists_url": "https://api.github.com/users/jvdillon/gists{/gist_id}", "starred_url": "https://api.github.com/users/jvdillon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jvdillon/subscriptions", "organizations_url": "https://api.github.com/users/jvdillon/orgs", "repos_url": "https://api.github.com/users/jvdillon/repos", "events_url": "https://api.github.com/users/jvdillon/events{/privacy}", "received_events_url": "https://api.github.com/users/jvdillon/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-04T12:01:59Z", "updated_at": "2018-11-09T18:53:24Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Centos 6</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0 (tested in 1.10.0 too)</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: NVIDIA Tesla, 24GB</li>\n<li><strong>Exact command to reproduce</strong>: the code within the \"Source code / logs\"</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am trying to reuse the weights and biases in the neural network within the MaskedAutoregressiveFlow bijector, by placing it within a <code>tf.variable_scope</code> with <code>reuse=tf.AUTO_REUSE</code>. But found that the weights and biases are not reused in practice.</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.distributions.python.ops import bijectors as tfb\n\ndef get_bijector(name='my_bijector', reuse=None):\n  \"\"\"Returns a MAF bijector.\"\"\"\n  with tf.variable_scope(name, reuse=reuse):\n    shift_and_log_scale_fn = \\\n        tfb.masked_autoregressive_default_template([128])\n    return tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn)\n  \nx = tf.placeholder(shape=[None, 64], dtype='float32', name='x')\n\nbijector_0 = get_bijector(reuse=tf.AUTO_REUSE)\ny_0 = bijector_0.forward(x)\n\nbijector_1 = get_bijector(reuse=tf.AUTO_REUSE)\ny_1 = bijector_1.forward(x)\n\n# We were expecting that the `y_0` and `y_1` share the same dependent variables,\n# since we used `tf.AUTO_REUSE` within the `tf.variable_scope`. However, the following\n# will return a `False`.\nprint(get_dependent_variables(y_0) == get_dependent_variables(y_1))\n</code></pre>\n<p>wherein we have employed the function that gains all the variables a tensor depends on:</p>\n<pre><code>import collections\n\ndef get_dependent_variables(tensor):\n  \"\"\"Returns all variables that the tensor `tensor` depends on.\n  \n  Forked from: https://stackoverflow.com/a/42861919/1218716\n  \n  Args:\n    tensor: Tensor.\n    \n  Returns:\n    List of variables.\n  \"\"\"  \n  # Initialize\n  starting_op = tensor.op\n  dependent_vars = []\n  queue = collections.deque()\n  queue.append(starting_op)\n  op_to_var = {var.op: var for var in tf.trainable_variables()}\n  visited = {starting_op}\n\n  while queue:\n    op = queue.popleft()\n    try:\n      dependent_vars.append(op_to_var[op])\n    except KeyError:\n      # `op` is not a variable, so search its inputs (if any). \n      for op_input in op.inputs:\n        if op_input.op not in visited:\n          queue.append(op_input.op)\n          visited.add(op_input.op)\n          \n  return dependent_vars\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 6\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.9.0 (tested in 1.10.0 too)\nPython version: 3.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 8.0\nGPU model and memory: NVIDIA Tesla, 24GB\nExact command to reproduce: the code within the \"Source code / logs\"\n\nDescribe the problem\nI am trying to reuse the weights and biases in the neural network within the MaskedAutoregressiveFlow bijector, by placing it within a tf.variable_scope with reuse=tf.AUTO_REUSE. But found that the weights and biases are not reused in practice.\nSource code / logs\nimport tensorflow as tf\nfrom tensorflow.contrib.distributions.python.ops import bijectors as tfb\n\ndef get_bijector(name='my_bijector', reuse=None):\n  \"\"\"Returns a MAF bijector.\"\"\"\n  with tf.variable_scope(name, reuse=reuse):\n    shift_and_log_scale_fn = \\\n        tfb.masked_autoregressive_default_template([128])\n    return tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn)\n  \nx = tf.placeholder(shape=[None, 64], dtype='float32', name='x')\n\nbijector_0 = get_bijector(reuse=tf.AUTO_REUSE)\ny_0 = bijector_0.forward(x)\n\nbijector_1 = get_bijector(reuse=tf.AUTO_REUSE)\ny_1 = bijector_1.forward(x)\n\n# We were expecting that the `y_0` and `y_1` share the same dependent variables,\n# since we used `tf.AUTO_REUSE` within the `tf.variable_scope`. However, the following\n# will return a `False`.\nprint(get_dependent_variables(y_0) == get_dependent_variables(y_1))\n\nwherein we have employed the function that gains all the variables a tensor depends on:\nimport collections\n\ndef get_dependent_variables(tensor):\n  \"\"\"Returns all variables that the tensor `tensor` depends on.\n  \n  Forked from: https://stackoverflow.com/a/42861919/1218716\n  \n  Args:\n    tensor: Tensor.\n    \n  Returns:\n    List of variables.\n  \"\"\"  \n  # Initialize\n  starting_op = tensor.op\n  dependent_vars = []\n  queue = collections.deque()\n  queue.append(starting_op)\n  op_to_var = {var.op: var for var in tf.trainable_variables()}\n  visited = {starting_op}\n\n  while queue:\n    op = queue.popleft()\n    try:\n      dependent_vars.append(op_to_var[op])\n    except KeyError:\n      # `op` is not a variable, so search its inputs (if any). \n      for op_input in op.inputs:\n        if op_input.op not in visited:\n          queue.append(op_input.op)\n          visited.add(op_input.op)\n          \n  return dependent_vars", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0 (tested in 1.10.0 too)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: NVIDIA Tesla, 24GB\r\n- **Exact command to reproduce**: the code within the \"Source code / logs\"\r\n\r\n### Describe the problem\r\nI am trying to reuse the weights and biases in the neural network within the MaskedAutoregressiveFlow bijector, by placing it within a `tf.variable_scope` with `reuse=tf.AUTO_REUSE`. But found that the weights and biases are not reused in practice.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.distributions.python.ops import bijectors as tfb\r\n\r\ndef get_bijector(name='my_bijector', reuse=None):\r\n  \"\"\"Returns a MAF bijector.\"\"\"\r\n  with tf.variable_scope(name, reuse=reuse):\r\n    shift_and_log_scale_fn = \\\r\n        tfb.masked_autoregressive_default_template([128])\r\n    return tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn)\r\n  \r\nx = tf.placeholder(shape=[None, 64], dtype='float32', name='x')\r\n\r\nbijector_0 = get_bijector(reuse=tf.AUTO_REUSE)\r\ny_0 = bijector_0.forward(x)\r\n\r\nbijector_1 = get_bijector(reuse=tf.AUTO_REUSE)\r\ny_1 = bijector_1.forward(x)\r\n\r\n# We were expecting that the `y_0` and `y_1` share the same dependent variables,\r\n# since we used `tf.AUTO_REUSE` within the `tf.variable_scope`. However, the following\r\n# will return a `False`.\r\nprint(get_dependent_variables(y_0) == get_dependent_variables(y_1))\r\n```\r\n\r\nwherein we have employed the function that gains all the variables a tensor depends on:\r\n\r\n```\r\nimport collections\r\n\r\ndef get_dependent_variables(tensor):\r\n  \"\"\"Returns all variables that the tensor `tensor` depends on.\r\n  \r\n  Forked from: https://stackoverflow.com/a/42861919/1218716\r\n  \r\n  Args:\r\n    tensor: Tensor.\r\n    \r\n  Returns:\r\n    List of variables.\r\n  \"\"\"  \r\n  # Initialize\r\n  starting_op = tensor.op\r\n  dependent_vars = []\r\n  queue = collections.deque()\r\n  queue.append(starting_op)\r\n  op_to_var = {var.op: var for var in tf.trainable_variables()}\r\n  visited = {starting_op}\r\n\r\n  while queue:\r\n    op = queue.popleft()\r\n    try:\r\n      dependent_vars.append(op_to_var[op])\r\n    except KeyError:\r\n      # `op` is not a variable, so search its inputs (if any). \r\n      for op_input in op.inputs:\r\n        if op_input.op not in visited:\r\n          queue.append(op_input.op)\r\n          visited.add(op_input.op)\r\n          \r\n  return dependent_vars\r\n```"}