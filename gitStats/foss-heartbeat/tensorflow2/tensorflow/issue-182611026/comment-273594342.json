{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273594342", "html_url": "https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273594342", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4920", "id": 273594342, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzU5NDM0Mg==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-18T20:37:41Z", "updated_at": "2017-01-18T20:37:41Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">You can also use variable.initial_value, it returns the tensor which will\nbe used to initialize the variable.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Jan 18, 2017 at 10:44 AM, Eric Martin ***@***.***&gt; wrote:\n That breaks the (useful) abstraction of not having to worry about whether\n a tensor is a variable or just a usual tensor.\n\n Using initialized_value isn't very nice for cases like\n\n x = tf.Variable(5.0)\n y = x + 1.0\n z = tf.Variable(tf.zeros_like(y))\n\n Beyond this, you can imagine dependent initialization spread out much\n further across a graph. I can't set y = x.initialized_value() + 1.0\n because I want y = x + 1.0 on later graph runs (after I mutate x). Using\n initialized_value would require separate code paths (/ subgraphs) for\n initializing and running the graph.\n\n I think I understand the problem with the default behavior. In my case, z's\n initializer has a data dependency that goes back to x. However, x is\n still uninitialized at this time, which causes an error. I attempted to\n work around this by feeding in the initialized_value for variable.value(),\n but this isn't a workaround because you can't always evaluated\n initialized_value if the initializer depends on other non-initialized\n variables. If there was a mode of Session.run which would default to\n initial variable values, then this strategy could work.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"182611026\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4920\" href=\"https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273562777\">#4920 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxYySDqYcYsqJb5uZFmaIGDTuRdhJks5rTl11gaJpZM4KVHy9\">https://github.com/notifications/unsubscribe-auth/AAATxYySDqYcYsqJb5uZFmaIGDTuRdhJks5rTl11gaJpZM4KVHy9</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "You can also use variable.initial_value, it returns the tensor which will\nbe used to initialize the variable.\n\u2026\nOn Wed, Jan 18, 2017 at 10:44 AM, Eric Martin ***@***.***> wrote:\n That breaks the (useful) abstraction of not having to worry about whether\n a tensor is a variable or just a usual tensor.\n\n Using initialized_value isn't very nice for cases like\n\n x = tf.Variable(5.0)\n y = x + 1.0\n z = tf.Variable(tf.zeros_like(y))\n\n Beyond this, you can imagine dependent initialization spread out much\n further across a graph. I can't set y = x.initialized_value() + 1.0\n because I want y = x + 1.0 on later graph runs (after I mutate x). Using\n initialized_value would require separate code paths (/ subgraphs) for\n initializing and running the graph.\n\n I think I understand the problem with the default behavior. In my case, z's\n initializer has a data dependency that goes back to x. However, x is\n still uninitialized at this time, which causes an error. I attempted to\n work around this by feeding in the initialized_value for variable.value(),\n but this isn't a workaround because you can't always evaluated\n initialized_value if the initializer depends on other non-initialized\n variables. If there was a mode of Session.run which would default to\n initial variable values, then this strategy could work.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#4920 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxYySDqYcYsqJb5uZFmaIGDTuRdhJks5rTl11gaJpZM4KVHy9>\n .\n\n\n-- \n - Alex", "body": "You can also use variable.initial_value, it returns the tensor which will\nbe used to initialize the variable.\n\nOn Wed, Jan 18, 2017 at 10:44 AM, Eric Martin <notifications@github.com>\nwrote:\n\n> That breaks the (useful) abstraction of not having to worry about whether\n> a tensor is a variable or just a usual tensor.\n>\n> Using initialized_value isn't very nice for cases like\n>\n> x = tf.Variable(5.0)\n> y = x + 1.0\n> z = tf.Variable(tf.zeros_like(y))\n>\n> Beyond this, you can imagine dependent initialization spread out much\n> further across a graph. I can't set y = x.initialized_value() + 1.0\n> because I want y = x + 1.0 on later graph runs (after I mutate x). Using\n> initialized_value would require separate code paths (/ subgraphs) for\n> initializing and running the graph.\n>\n> I think I understand the problem with the default behavior. In my case, z's\n> initializer has a data dependency that goes back to x. However, x is\n> still uninitialized at this time, which causes an error. I attempted to\n> work around this by feeding in the initialized_value for variable.value(),\n> but this isn't a workaround because you can't always evaluated\n> initialized_value if the initializer depends on other non-initialized\n> variables. If there was a mode of Session.run which would default to\n> initial variable values, then this strategy could work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273562777>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYySDqYcYsqJb5uZFmaIGDTuRdhJks5rTl11gaJpZM4KVHy9>\n> .\n>\n\n\n\n-- \n - Alex\n"}