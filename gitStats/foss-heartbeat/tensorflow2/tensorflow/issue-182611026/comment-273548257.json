{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273548257", "html_url": "https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273548257", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4920", "id": 273548257, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzU0ODI1Nw==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-18T17:49:17Z", "updated_at": "2017-01-18T17:52:41Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> I think the nicest solution here is having a toposorted list of the variables, and then making a <code>Session.run</code> call for each variables initializer. It might be possible to avoid the overhead of a bunch of <code>Session.run</code> calls by making a chain of control dependencies between initializers, but I doubt it since it appears data dependencies aren't obeyed during initialization.</p>\n<p>I was starting to write a toposort routine, but I realized the graph is constructed with ancestors first and the TF collections are appended to, which means <code>tf.global_variables</code> is already toposorted. This is a little bit of hack since it's not specified that TF collections are appended to, but I think it will always work to do something like</p>\n<pre><code>for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n    sess.run(v.initializer)\n</code></pre>\n<p>I'm running TensorFlow 0.10, but here's a non-trivial dependent initialization that works with this approach:</p>\n<pre><code>x = tf.Variable(np.random.randn(5), name='x')\ny = tf.Variable(x, name='y')\nz = tf.Variable(tf.zeros_like(x) + y, name='z')\nw = tf.Variable(2 * z, name='w')\n\nwith tf.Session() as sess:\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES):\n        sess.run(v.initializer)\n</code></pre>\n<p>It seems that the larger problem (why these workarounds are needed) is that <code>Session</code> treats initializers (or maybe just running Ops instead of Tensors) differently and doesn't obey data or control dependencies.</p>", "body_text": "@yaroslavvb I think the nicest solution here is having a toposorted list of the variables, and then making a Session.run call for each variables initializer. It might be possible to avoid the overhead of a bunch of Session.run calls by making a chain of control dependencies between initializers, but I doubt it since it appears data dependencies aren't obeyed during initialization.\nI was starting to write a toposort routine, but I realized the graph is constructed with ancestors first and the TF collections are appended to, which means tf.global_variables is already toposorted. This is a little bit of hack since it's not specified that TF collections are appended to, but I think it will always work to do something like\nfor v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n    sess.run(v.initializer)\n\nI'm running TensorFlow 0.10, but here's a non-trivial dependent initialization that works with this approach:\nx = tf.Variable(np.random.randn(5), name='x')\ny = tf.Variable(x, name='y')\nz = tf.Variable(tf.zeros_like(x) + y, name='z')\nw = tf.Variable(2 * z, name='w')\n\nwith tf.Session() as sess:\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES):\n        sess.run(v.initializer)\n\nIt seems that the larger problem (why these workarounds are needed) is that Session treats initializers (or maybe just running Ops instead of Tensors) differently and doesn't obey data or control dependencies.", "body": "@yaroslavvb I think the nicest solution here is having a toposorted list of the variables, and then making a ``Session.run`` call for each variables initializer. It might be possible to avoid the overhead of a bunch of ``Session.run`` calls by making a chain of control dependencies between initializers, but I doubt it since it appears data dependencies aren't obeyed during initialization. \r\n\r\nI was starting to write a toposort routine, but I realized the graph is constructed with ancestors first and the TF collections are appended to, which means ``tf.global_variables`` is already toposorted. This is a little bit of hack since it's not specified that TF collections are appended to, but I think it will always work to do something like\r\n```\r\nfor v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\r\n    sess.run(v.initializer)\r\n```\r\n\r\nI'm running TensorFlow 0.10, but here's a non-trivial dependent initialization that works with this approach:\r\n```\r\nx = tf.Variable(np.random.randn(5), name='x')\r\ny = tf.Variable(x, name='y')\r\nz = tf.Variable(tf.zeros_like(x) + y, name='z')\r\nw = tf.Variable(2 * z, name='w')\r\n\r\nwith tf.Session() as sess:\r\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES):\r\n        sess.run(v.initializer)\r\n```\r\n\r\nIt seems that the larger problem (why these workarounds are needed) is that ``Session`` treats initializers (or maybe just running Ops instead of Tensors) differently and doesn't obey data or control dependencies."}