{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/314523933", "html_url": "https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-314523933", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4920", "id": 314523933, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDUyMzkzMw==", "user": {"login": "TheButlah", "id": 6969415, "node_id": "MDQ6VXNlcjY5Njk0MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6969415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheButlah", "html_url": "https://github.com/TheButlah", "followers_url": "https://api.github.com/users/TheButlah/followers", "following_url": "https://api.github.com/users/TheButlah/following{/other_user}", "gists_url": "https://api.github.com/users/TheButlah/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheButlah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheButlah/subscriptions", "organizations_url": "https://api.github.com/users/TheButlah/orgs", "repos_url": "https://api.github.com/users/TheButlah/repos", "events_url": "https://api.github.com/users/TheButlah/events{/privacy}", "received_events_url": "https://api.github.com/users/TheButlah/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-11T17:58:48Z", "updated_at": "2017-07-11T18:01:35Z", "author_association": "NONE", "body_html": "<p>I think this should be reopened. Initializing variables from other variables that depend on placeholders is still really difficult. It would be ideal if it were achievable in a single run call, without depending on the graph editor method that <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> provided. Although the behavior of initialized_value changed so that it uses tf.cond to decide whether or not to run the initializer, this is still not ideal for cases such as below:</p>\n<pre><code>    sess = tf.InteractiveSession()\n\n    x = tf.placeholder(tf.int32, (), name='X')\n    a = tf.Variable(x, name='A', trainable=False)  # First cached variable (depends on placeholder)\n    b = a + 1  # Pretend this is a huge amount of code\n    c = tf.Variable(b+1, name='C', trainable=False)  # Second cached variable\n    d = b + 3  # Pretend this is a bunch more code\n    final_output = d + c\n\n    is_init = tf.is_variable_initialized\n    with tf.control_dependencies([a.initializer]):\n        is_a = tf.Print(is_init(a), [is_init(a)], \"A: \")\n        with tf.control_dependencies([is_a]):\n            with tf.control_dependencies([c.initializer]):\n                is_c = tf.Print(is_init(c), [is_init(c)], \"C: \")\n                init_op = is_c.op\n\n    init_op.run(feed_dict={x: 1})\n\n    print(a.eval())\n    print(c.eval())\n</code></pre>\n<p>That code, seemingly randomly, sometimes fails and sometimes works. Using initial_value doesn't help because then the placeholder would have to always be filled in subsequent runs, and using initialized_value doesn't work for the same reason (also because while its ok to do the tf.cond call once for initialization, repeatedly calling it for each training step for a more complex model would be very slow). Or is there a new, better way that I am not aware of?</p>", "body_text": "I think this should be reopened. Initializing variables from other variables that depend on placeholders is still really difficult. It would be ideal if it were achievable in a single run call, without depending on the graph editor method that @yaroslavvb provided. Although the behavior of initialized_value changed so that it uses tf.cond to decide whether or not to run the initializer, this is still not ideal for cases such as below:\n    sess = tf.InteractiveSession()\n\n    x = tf.placeholder(tf.int32, (), name='X')\n    a = tf.Variable(x, name='A', trainable=False)  # First cached variable (depends on placeholder)\n    b = a + 1  # Pretend this is a huge amount of code\n    c = tf.Variable(b+1, name='C', trainable=False)  # Second cached variable\n    d = b + 3  # Pretend this is a bunch more code\n    final_output = d + c\n\n    is_init = tf.is_variable_initialized\n    with tf.control_dependencies([a.initializer]):\n        is_a = tf.Print(is_init(a), [is_init(a)], \"A: \")\n        with tf.control_dependencies([is_a]):\n            with tf.control_dependencies([c.initializer]):\n                is_c = tf.Print(is_init(c), [is_init(c)], \"C: \")\n                init_op = is_c.op\n\n    init_op.run(feed_dict={x: 1})\n\n    print(a.eval())\n    print(c.eval())\n\nThat code, seemingly randomly, sometimes fails and sometimes works. Using initial_value doesn't help because then the placeholder would have to always be filled in subsequent runs, and using initialized_value doesn't work for the same reason (also because while its ok to do the tf.cond call once for initialization, repeatedly calling it for each training step for a more complex model would be very slow). Or is there a new, better way that I am not aware of?", "body": "I think this should be reopened. Initializing variables from other variables that depend on placeholders is still really difficult. It would be ideal if it were achievable in a single run call, without depending on the graph editor method that @yaroslavvb provided. Although the behavior of initialized_value changed so that it uses tf.cond to decide whether or not to run the initializer, this is still not ideal for cases such as below:\r\n\r\n```\r\n    sess = tf.InteractiveSession()\r\n\r\n    x = tf.placeholder(tf.int32, (), name='X')\r\n    a = tf.Variable(x, name='A', trainable=False)  # First cached variable (depends on placeholder)\r\n    b = a + 1  # Pretend this is a huge amount of code\r\n    c = tf.Variable(b+1, name='C', trainable=False)  # Second cached variable\r\n    d = b + 3  # Pretend this is a bunch more code\r\n    final_output = d + c\r\n\r\n    is_init = tf.is_variable_initialized\r\n    with tf.control_dependencies([a.initializer]):\r\n        is_a = tf.Print(is_init(a), [is_init(a)], \"A: \")\r\n        with tf.control_dependencies([is_a]):\r\n            with tf.control_dependencies([c.initializer]):\r\n                is_c = tf.Print(is_init(c), [is_init(c)], \"C: \")\r\n                init_op = is_c.op\r\n\r\n    init_op.run(feed_dict={x: 1})\r\n\r\n    print(a.eval())\r\n    print(c.eval())\r\n```\r\n\r\nThat code, seemingly randomly, sometimes fails and sometimes works. Using initial_value doesn't help because then the placeholder would have to always be filled in subsequent runs, and using initialized_value doesn't work for the same reason (also because while its ok to do the tf.cond call once for initialization, repeatedly calling it for each training step for a more complex model would be very slow). Or is there a new, better way that I am not aware of?"}