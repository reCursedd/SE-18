{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273562777", "html_url": "https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273562777", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4920", "id": 273562777, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzU2Mjc3Nw==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-18T18:42:40Z", "updated_at": "2017-01-18T18:54:21Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a><br>\nThat breaks the (useful) abstraction of not having to worry about whether a tensor is a variable or just a usual tensor.</p>\n<p>Using <code>initialized_value</code> isn't very nice for cases like</p>\n<pre><code>x = tf.Variable(5.0)\ny = x + 1.0\nz = tf.Variable(tf.zeros_like(y))\n</code></pre>\n<p>Beyond this, you can imagine dependent initialization spread out much further across a graph. I can't set <code>y = x.initialized_value() + 1.0</code> because I want <code>y = x + 1.0</code> on later graph runs (after I mutate <code>x</code>). Using <code>initialized_value</code> would require separate code paths (/ subgraphs) for initializing and running the graph.</p>\n<p>I think I understand the problem with the default behavior. In my case, <code>z</code>'s initializer has a data dependency that goes back to <code>x</code>. However, <code>x</code> is still uninitialized at this time, which causes an error. I attempted to work around this by feeding in the <code>initialized_value</code> for <code>variable.value()</code>, but this isn't a workaround because you can't always evaluate <code>initialized_value</code> if the initializer depends on other non-initialized variables. If there was a mode of <code>Session.run</code> which would default to initial variable values, then this strategy could work. Sadly, this cannot be done with <code>tf.select</code> and an \"init_mode\" placeholder because <code>tf.select(true, v.initialized_value(), v)</code> fails for uninitialized <code>v</code>.</p>", "body_text": "@alextp\nThat breaks the (useful) abstraction of not having to worry about whether a tensor is a variable or just a usual tensor.\nUsing initialized_value isn't very nice for cases like\nx = tf.Variable(5.0)\ny = x + 1.0\nz = tf.Variable(tf.zeros_like(y))\n\nBeyond this, you can imagine dependent initialization spread out much further across a graph. I can't set y = x.initialized_value() + 1.0 because I want y = x + 1.0 on later graph runs (after I mutate x). Using initialized_value would require separate code paths (/ subgraphs) for initializing and running the graph.\nI think I understand the problem with the default behavior. In my case, z's initializer has a data dependency that goes back to x. However, x is still uninitialized at this time, which causes an error. I attempted to work around this by feeding in the initialized_value for variable.value(), but this isn't a workaround because you can't always evaluate initialized_value if the initializer depends on other non-initialized variables. If there was a mode of Session.run which would default to initial variable values, then this strategy could work. Sadly, this cannot be done with tf.select and an \"init_mode\" placeholder because tf.select(true, v.initialized_value(), v) fails for uninitialized v.", "body": "@alextp \r\nThat breaks the (useful) abstraction of not having to worry about whether a tensor is a variable or just a usual tensor.\r\n\r\nUsing ``initialized_value`` isn't very nice for cases like\r\n```\r\nx = tf.Variable(5.0)\r\ny = x + 1.0\r\nz = tf.Variable(tf.zeros_like(y))\r\n```\r\nBeyond this, you can imagine dependent initialization spread out much further across a graph. I can't set ``y = x.initialized_value() + 1.0`` because I want ``y = x + 1.0`` on later graph runs (after I mutate ``x``). Using ``initialized_value`` would require separate code paths (/ subgraphs) for initializing and running the graph.\r\n\r\nI think I understand the problem with the default behavior. In my case, ``z``'s initializer has a data dependency that goes back to ``x``. However, ``x`` is still uninitialized at this time, which causes an error. I attempted to work around this by feeding in the ``initialized_value`` for ``variable.value()``, but this isn't a workaround because you can't always evaluate ``initialized_value`` if the initializer depends on other non-initialized variables. If there was a mode of ``Session.run`` which would default to initial variable values, then this strategy could work. Sadly, this cannot be done with ``tf.select`` and an \"init_mode\" placeholder because ``tf.select(true, v.initialized_value(), v)`` fails for uninitialized ``v``."}