{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5987", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5987/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5987/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5987/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5987", "id": 192626454, "node_id": "MDU6SXNzdWUxOTI2MjY0NTQ=", "number": 5987, "title": "Request for documentation on recommended flow in slim for train, validation, and test sets", "user": {"login": "kmalakoff", "id": 756520, "node_id": "MDQ6VXNlcjc1NjUyMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/756520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kmalakoff", "html_url": "https://github.com/kmalakoff", "followers_url": "https://api.github.com/users/kmalakoff/followers", "following_url": "https://api.github.com/users/kmalakoff/following{/other_user}", "gists_url": "https://api.github.com/users/kmalakoff/gists{/gist_id}", "starred_url": "https://api.github.com/users/kmalakoff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kmalakoff/subscriptions", "organizations_url": "https://api.github.com/users/kmalakoff/orgs", "repos_url": "https://api.github.com/users/kmalakoff/repos", "events_url": "https://api.github.com/users/kmalakoff/events{/privacy}", "received_events_url": "https://api.github.com/users/kmalakoff/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 31, "created_at": "2016-11-30T17:19:04Z", "updated_at": "2018-03-03T06:54:52Z", "closed_at": "2018-02-07T23:59:00Z", "author_association": "NONE", "body_html": "<p>The examples in the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\">slim README.md</a> give basic documentation for training and evaluating models when used separately; however, there is guidance missing on how to do the classic cycle of mini-batch gradient descent using shuffled subsets of the training set, periodically evaluating validation set, and then evaluating on the test set post-training.</p>\n<p>Using the <a href=\"https://www.tensorflow.org/versions/r0.12/tutorials/mnist/tf/index.html\" rel=\"nofollow\">MNIST tutorial</a> and <a href=\"https://github.com/mnuke/tf-slim-mnist\">this tutorial</a> for reference, the best I came up with was something like this where I'm effectively monkey-patching the train_step_fn to periodically output accuracies:</p>\n<pre><code>from tensorflow.contrib.slim.python.slim.learning import train_step\n\ngraph = tf.Graph()\nwith graph.as_default():\n  image, label = input('train', FLAGS.dataset_dir)\n  images, labels = tf.train.shuffle_batch([image, label], batch_size=FLAGS.batch_size, capacity=1000 + 3 * FLAGS.batch_size, min_after_dequeue=1000)\n  images_validation, labels_validation = inputs('validation', FLAGS.dataset_dir, 5000)\n  images_test, labels_test = inputs('test', FLAGS.dataset_dir, 10000)\n \n  with tf.variable_scope(\"model\") as scope:\n    predictions = model(images, FLAGS)\n    scope.reuse_variables()\n    predictions_validation = model(images_validation, FLAGS)\n    predictions_test = model(images_test, FLAGS)\n    \n  slim.losses.softmax_cross_entropy(predictions, labels)\n  optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n  train_op = slim.learning.create_train_op(slim.losses.get_total_loss(), optimizer)\n\n  accuracy_validation = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_validation, 1)), tf.to_int32(tf.argmax(labels_validation, 1)))\n  accuracy_test = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_test, 1)), tf.to_int32(tf.argmax(labels_test, 1)))\n    \ndef train_step_fn(session, *args, **kwargs):\n  total_loss, should_stop = train_step(session, *args, **kwargs)\n\n  if train_step_fn.step % FLAGS.validation_check == 0:\n    accuracy = session.run(train_step_fn.accuracy_validation)\n    print('Step %s - Loss: %.2f Accuracy: %.2f%%' % (str(train_step_fn.step).rjust(6, '0'), total_loss, accuracy * 100))\n\n  if train_step_fn.step == (FLAGS.max_steps - 1):\n    accuracy = session.run(accuracy_test)\n    print('%s - Loss: %.2f Accuracy: %.2f%%' % ('FINAL TEST', total_loss, accuracy * 100))\n    \n  train_step_fn.step += 1\n  return [total_loss, should_stop]\n\ntrain_step_fn.step = 0\ntrain_step_fn.accuracy_validation = accuracy_validation\n\nslim.learning.train(\n  train_op,\n  FLAGS.logs_dir,\n  train_step_fn=train_step_fn,\n  graph=graph,\n  number_of_steps=FLAGS.max_steps\n)\n</code></pre>\n<p><strong>Note</strong>: one problem with this implementation is that the final test set is not guaranteed to be run in the case of early exit.</p>\n<p>I've posted in the slack channel and Googled around, but haven't been able to find any examples for this basic use case. Accordingly, I would like to propose that an example providing the best practice to periodically evaluate batch trained models using the validate set and the trained model against the test set to be added to the slim README.md.</p>\n<p>I think it would really help the community to have a clearer idea on the intentions of the slim team on how the batch training and evaluation paths were designed to be used together during and after training.</p>", "body_text": "The examples in the slim README.md give basic documentation for training and evaluating models when used separately; however, there is guidance missing on how to do the classic cycle of mini-batch gradient descent using shuffled subsets of the training set, periodically evaluating validation set, and then evaluating on the test set post-training.\nUsing the MNIST tutorial and this tutorial for reference, the best I came up with was something like this where I'm effectively monkey-patching the train_step_fn to periodically output accuracies:\nfrom tensorflow.contrib.slim.python.slim.learning import train_step\n\ngraph = tf.Graph()\nwith graph.as_default():\n  image, label = input('train', FLAGS.dataset_dir)\n  images, labels = tf.train.shuffle_batch([image, label], batch_size=FLAGS.batch_size, capacity=1000 + 3 * FLAGS.batch_size, min_after_dequeue=1000)\n  images_validation, labels_validation = inputs('validation', FLAGS.dataset_dir, 5000)\n  images_test, labels_test = inputs('test', FLAGS.dataset_dir, 10000)\n \n  with tf.variable_scope(\"model\") as scope:\n    predictions = model(images, FLAGS)\n    scope.reuse_variables()\n    predictions_validation = model(images_validation, FLAGS)\n    predictions_test = model(images_test, FLAGS)\n    \n  slim.losses.softmax_cross_entropy(predictions, labels)\n  optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n  train_op = slim.learning.create_train_op(slim.losses.get_total_loss(), optimizer)\n\n  accuracy_validation = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_validation, 1)), tf.to_int32(tf.argmax(labels_validation, 1)))\n  accuracy_test = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_test, 1)), tf.to_int32(tf.argmax(labels_test, 1)))\n    \ndef train_step_fn(session, *args, **kwargs):\n  total_loss, should_stop = train_step(session, *args, **kwargs)\n\n  if train_step_fn.step % FLAGS.validation_check == 0:\n    accuracy = session.run(train_step_fn.accuracy_validation)\n    print('Step %s - Loss: %.2f Accuracy: %.2f%%' % (str(train_step_fn.step).rjust(6, '0'), total_loss, accuracy * 100))\n\n  if train_step_fn.step == (FLAGS.max_steps - 1):\n    accuracy = session.run(accuracy_test)\n    print('%s - Loss: %.2f Accuracy: %.2f%%' % ('FINAL TEST', total_loss, accuracy * 100))\n    \n  train_step_fn.step += 1\n  return [total_loss, should_stop]\n\ntrain_step_fn.step = 0\ntrain_step_fn.accuracy_validation = accuracy_validation\n\nslim.learning.train(\n  train_op,\n  FLAGS.logs_dir,\n  train_step_fn=train_step_fn,\n  graph=graph,\n  number_of_steps=FLAGS.max_steps\n)\n\nNote: one problem with this implementation is that the final test set is not guaranteed to be run in the case of early exit.\nI've posted in the slack channel and Googled around, but haven't been able to find any examples for this basic use case. Accordingly, I would like to propose that an example providing the best practice to periodically evaluate batch trained models using the validate set and the trained model against the test set to be added to the slim README.md.\nI think it would really help the community to have a clearer idea on the intentions of the slim team on how the batch training and evaluation paths were designed to be used together during and after training.", "body": "The examples in the [slim README.md](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) give basic documentation for training and evaluating models when used separately; however, there is guidance missing on how to do the classic cycle of mini-batch gradient descent using shuffled subsets of the training set, periodically evaluating validation set, and then evaluating on the test set post-training. \r\n\r\nUsing the [MNIST tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/mnist/tf/index.html) and [this tutorial](https://github.com/mnuke/tf-slim-mnist) for reference, the best I came up with was something like this where I'm effectively monkey-patching the train_step_fn to periodically output accuracies: \r\n\r\n```\r\nfrom tensorflow.contrib.slim.python.slim.learning import train_step\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  image, label = input('train', FLAGS.dataset_dir)\r\n  images, labels = tf.train.shuffle_batch([image, label], batch_size=FLAGS.batch_size, capacity=1000 + 3 * FLAGS.batch_size, min_after_dequeue=1000)\r\n  images_validation, labels_validation = inputs('validation', FLAGS.dataset_dir, 5000)\r\n  images_test, labels_test = inputs('test', FLAGS.dataset_dir, 10000)\r\n \r\n  with tf.variable_scope(\"model\") as scope:\r\n    predictions = model(images, FLAGS)\r\n    scope.reuse_variables()\r\n    predictions_validation = model(images_validation, FLAGS)\r\n    predictions_test = model(images_test, FLAGS)\r\n    \r\n  slim.losses.softmax_cross_entropy(predictions, labels)\r\n  optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\r\n  train_op = slim.learning.create_train_op(slim.losses.get_total_loss(), optimizer)\r\n\r\n  accuracy_validation = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_validation, 1)), tf.to_int32(tf.argmax(labels_validation, 1)))\r\n  accuracy_test = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_test, 1)), tf.to_int32(tf.argmax(labels_test, 1)))\r\n    \r\ndef train_step_fn(session, *args, **kwargs):\r\n  total_loss, should_stop = train_step(session, *args, **kwargs)\r\n\r\n  if train_step_fn.step % FLAGS.validation_check == 0:\r\n    accuracy = session.run(train_step_fn.accuracy_validation)\r\n    print('Step %s - Loss: %.2f Accuracy: %.2f%%' % (str(train_step_fn.step).rjust(6, '0'), total_loss, accuracy * 100))\r\n\r\n  if train_step_fn.step == (FLAGS.max_steps - 1):\r\n    accuracy = session.run(accuracy_test)\r\n    print('%s - Loss: %.2f Accuracy: %.2f%%' % ('FINAL TEST', total_loss, accuracy * 100))\r\n    \r\n  train_step_fn.step += 1\r\n  return [total_loss, should_stop]\r\n\r\ntrain_step_fn.step = 0\r\ntrain_step_fn.accuracy_validation = accuracy_validation\r\n\r\nslim.learning.train(\r\n  train_op,\r\n  FLAGS.logs_dir,\r\n  train_step_fn=train_step_fn,\r\n  graph=graph,\r\n  number_of_steps=FLAGS.max_steps\r\n)\r\n```\r\n\r\n**Note**: one problem with this implementation is that the final test set is not guaranteed to be run in the case of early exit.\r\n \r\nI've posted in the slack channel and Googled around, but haven't been able to find any examples for this basic use case. Accordingly, I would like to propose that an example providing the best practice to periodically evaluate batch trained models using the validate set and the trained model against the test set to be added to the slim README.md.\r\n\r\nI think it would really help the community to have a clearer idea on the intentions of the slim team on how the batch training and evaluation paths were designed to be used together during and after training.\r\n"}