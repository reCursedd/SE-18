{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/298351393", "html_url": "https://github.com/tensorflow/tensorflow/issues/5987#issuecomment-298351393", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5987", "id": 298351393, "node_id": "MDEyOklzc3VlQ29tbWVudDI5ODM1MTM5Mw==", "user": {"login": "GaelReinaudi", "id": 3522815, "node_id": "MDQ6VXNlcjM1MjI4MTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3522815?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GaelReinaudi", "html_url": "https://github.com/GaelReinaudi", "followers_url": "https://api.github.com/users/GaelReinaudi/followers", "following_url": "https://api.github.com/users/GaelReinaudi/following{/other_user}", "gists_url": "https://api.github.com/users/GaelReinaudi/gists{/gist_id}", "starred_url": "https://api.github.com/users/GaelReinaudi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GaelReinaudi/subscriptions", "organizations_url": "https://api.github.com/users/GaelReinaudi/orgs", "repos_url": "https://api.github.com/users/GaelReinaudi/repos", "events_url": "https://api.github.com/users/GaelReinaudi/events{/privacy}", "received_events_url": "https://api.github.com/users/GaelReinaudi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-01T15:12:02Z", "updated_at": "2017-05-01T15:12:02Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1766524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sguada\">@sguada</a><br>\nThanks for your suggestion. I agree that validating in a separate process sounds like a better approach.</p>\n<p>There are a number of things that are known during training time: what data file is used, what samples are used for training (as opposed to reserved for validation), what kink was introduced in the code to perform a certain test....etc. These important pieces of information  need to be passed to the other process reliably.</p>\n<p>Since it seems to be the standard use case, would it be possible to have, in the repo, an example of such mechanics? slim seems incomplete if it provides many awesome ways to train a network easily, but no easy way to monitor/validate its progress.</p>\n<p>Thanks again.</p>", "body_text": "@sguada\nThanks for your suggestion. I agree that validating in a separate process sounds like a better approach.\nThere are a number of things that are known during training time: what data file is used, what samples are used for training (as opposed to reserved for validation), what kink was introduced in the code to perform a certain test....etc. These important pieces of information  need to be passed to the other process reliably.\nSince it seems to be the standard use case, would it be possible to have, in the repo, an example of such mechanics? slim seems incomplete if it provides many awesome ways to train a network easily, but no easy way to monitor/validate its progress.\nThanks again.", "body": "@sguada \r\nThanks for your suggestion. I agree that validating in a separate process sounds like a better approach. \r\n\r\nThere are a number of things that are known during training time: what data file is used, what samples are used for training (as opposed to reserved for validation), what kink was introduced in the code to perform a certain test....etc. These important pieces of information  need to be passed to the other process reliably.\r\n\r\nSince it seems to be the standard use case, would it be possible to have, in the repo, an example of such mechanics? slim seems incomplete if it provides many awesome ways to train a network easily, but no easy way to monitor/validate its progress.\r\n\r\nThanks again.\r\n"}