{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/232089184", "html_url": "https://github.com/tensorflow/tensorflow/issues/3268#issuecomment-232089184", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3268", "id": 232089184, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMjA4OTE4NA==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-12T15:42:19Z", "updated_at": "2016-07-12T15:42:19Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>Update: zooming out a lot in the browser make the dots appear, but that's more of a workaround than a real solution.</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6617696\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/stepelu\">@stepelu</a> I'm not sure exactly what is going on in this case to produce the UI issues you are seeing - from your description of things changing with zoom levels, it could very easily be an issue with the chrome viewer (which is nothing to do with TensorFlow).  To reproduce it would be helpful if you could provide a copy of the <code>StepStats</code> protobuf (preferably) or at least the JSON of the chrome trace.</p>\n<blockquote>\n<p>By the way, are the colors related in any way to the size of memory allocations?</p>\n</blockquote>\n<p>The colors of everything in the GUI are chosen by the chrome trace viewer based on the 'name' of the event (usually the Op name).  They are not under the control of TensorFlow unfortunately  :-(</p>\n<p><strong>Note:</strong> There are a number of reasons that the show_memory option is disabled by default:</p>\n<ul>\n<li>The most important is that there isn't really enough information in the <code>StepStats</code> to correctly determine the order of allocation/frees (which is non-deterministic in reality) and so the histogram is at best an approximation of the truth.  The <code>Timeline</code> code is effectively replaying a trace of allocs and frees where the timestamps are taken with a very coarse clock, and simulating the memory usage.  Fixing this would require adding much more expensive instrumentation which would hurt the common case performance.</li>\n<li>The second problem is that the ops which are executed and recorded in the <code>StepStats</code> are not necessarily those in the original <code>GraphDef</code> due to substantial graph rewriting (e.g. adding Send/Recv nodes when partitioning the graph across devices) and various optimization passes (constant folding, CSE, etc).  It is impossible for the <code>Timeline</code> code to correctly determine the dataflow dependencies of these ops - and it currently has a very fragile heuristic based on parsing the string 'description' field of the op to guess its inputs.  It would be possible to do a much better job if somebody implemented a way to retrieve the post-optimization graphs in the <code>RunMetadata</code>.</li>\n<li>Another, often more important issue, is that a lot of memory consumption is by <strong>stateful</strong> ops such as Queues and Variables which aren't included in this analysis.  e.g. I have seen multiple cases where poorly configured Queues have been consuming many gigabytes of memory.</li>\n</ul>\n<p>So - in summary, I wouldn't recommend relying too heavily on the Timeline memory analysis at the moment.</p>", "body_text": "Update: zooming out a lot in the browser make the dots appear, but that's more of a workaround than a real solution.\n\n@stepelu I'm not sure exactly what is going on in this case to produce the UI issues you are seeing - from your description of things changing with zoom levels, it could very easily be an issue with the chrome viewer (which is nothing to do with TensorFlow).  To reproduce it would be helpful if you could provide a copy of the StepStats protobuf (preferably) or at least the JSON of the chrome trace.\n\nBy the way, are the colors related in any way to the size of memory allocations?\n\nThe colors of everything in the GUI are chosen by the chrome trace viewer based on the 'name' of the event (usually the Op name).  They are not under the control of TensorFlow unfortunately  :-(\nNote: There are a number of reasons that the show_memory option is disabled by default:\n\nThe most important is that there isn't really enough information in the StepStats to correctly determine the order of allocation/frees (which is non-deterministic in reality) and so the histogram is at best an approximation of the truth.  The Timeline code is effectively replaying a trace of allocs and frees where the timestamps are taken with a very coarse clock, and simulating the memory usage.  Fixing this would require adding much more expensive instrumentation which would hurt the common case performance.\nThe second problem is that the ops which are executed and recorded in the StepStats are not necessarily those in the original GraphDef due to substantial graph rewriting (e.g. adding Send/Recv nodes when partitioning the graph across devices) and various optimization passes (constant folding, CSE, etc).  It is impossible for the Timeline code to correctly determine the dataflow dependencies of these ops - and it currently has a very fragile heuristic based on parsing the string 'description' field of the op to guess its inputs.  It would be possible to do a much better job if somebody implemented a way to retrieve the post-optimization graphs in the RunMetadata.\nAnother, often more important issue, is that a lot of memory consumption is by stateful ops such as Queues and Variables which aren't included in this analysis.  e.g. I have seen multiple cases where poorly configured Queues have been consuming many gigabytes of memory.\n\nSo - in summary, I wouldn't recommend relying too heavily on the Timeline memory analysis at the moment.", "body": "> Update: zooming out a lot in the browser make the dots appear, but that's more of a workaround than a real solution.\n\n@stepelu I'm not sure exactly what is going on in this case to produce the UI issues you are seeing - from your description of things changing with zoom levels, it could very easily be an issue with the chrome viewer (which is nothing to do with TensorFlow).  To reproduce it would be helpful if you could provide a copy of the `StepStats` protobuf (preferably) or at least the JSON of the chrome trace.\n\n> By the way, are the colors related in any way to the size of memory allocations?\n\nThe colors of everything in the GUI are chosen by the chrome trace viewer based on the 'name' of the event (usually the Op name).  They are not under the control of TensorFlow unfortunately  :-(\n\n**Note:** There are a number of reasons that the show_memory option is disabled by default:\n- The most important is that there isn't really enough information in the `StepStats` to correctly determine the order of allocation/frees (which is non-deterministic in reality) and so the histogram is at best an approximation of the truth.  The `Timeline` code is effectively replaying a trace of allocs and frees where the timestamps are taken with a very coarse clock, and simulating the memory usage.  Fixing this would require adding much more expensive instrumentation which would hurt the common case performance.\n- The second problem is that the ops which are executed and recorded in the `StepStats` are not necessarily those in the original `GraphDef` due to substantial graph rewriting (e.g. adding Send/Recv nodes when partitioning the graph across devices) and various optimization passes (constant folding, CSE, etc).  It is impossible for the `Timeline` code to correctly determine the dataflow dependencies of these ops - and it currently has a very fragile heuristic based on parsing the string 'description' field of the op to guess its inputs.  It would be possible to do a much better job if somebody implemented a way to retrieve the post-optimization graphs in the `RunMetadata`.  \n- Another, often more important issue, is that a lot of memory consumption is by **stateful** ops such as Queues and Variables which aren't included in this analysis.  e.g. I have seen multiple cases where poorly configured Queues have been consuming many gigabytes of memory.  \n\nSo - in summary, I wouldn't recommend relying too heavily on the Timeline memory analysis at the moment.  \n"}