{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354142214", "html_url": "https://github.com/tensorflow/tensorflow/issues/15643#issuecomment-354142214", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15643", "id": 354142214, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDE0MjIxNA==", "user": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-27T16:54:24Z", "updated_at": "2017-12-27T16:54:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thank you @alirezadavoudi ! We are aware of this and we have observed this on P100. We are communicating with NVIDIA with solution for this.</p>\n<p>First of all, could you try with CUDA 9 + cudnn 7 and see if the same result still shows up?</p>\n<p>Second, in your setting, you are forcing cudnn to use TRUE_HALF_CONFIG and this mode is only supported by one internal kernel: CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM as for now.  Sometimes for the same input shape, switching from FP32 to FP16 will cause the change of the tile size for this kernel. For example, an original 128<em>32 tile size could become 256</em>64 when switching from FP32 to FP16, while in this case, the gain from doubling the math is lost for the tile utilization.</p>\n<p>In the future we will make PSEUDO_HALF_CONFIG and TRUE_HALF_CONFIG transparent to the users by adding these two configs to autotune.</p>", "body_text": "Thank you @alirezadavoudi ! We are aware of this and we have observed this on P100. We are communicating with NVIDIA with solution for this.\nFirst of all, could you try with CUDA 9 + cudnn 7 and see if the same result still shows up?\nSecond, in your setting, you are forcing cudnn to use TRUE_HALF_CONFIG and this mode is only supported by one internal kernel: CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM as for now.  Sometimes for the same input shape, switching from FP32 to FP16 will cause the change of the tile size for this kernel. For example, an original 12832 tile size could become 25664 when switching from FP32 to FP16, while in this case, the gain from doubling the math is lost for the tile utilization.\nIn the future we will make PSEUDO_HALF_CONFIG and TRUE_HALF_CONFIG transparent to the users by adding these two configs to autotune.", "body": "Thank you @alirezadavoudi ! We are aware of this and we have observed this on P100. We are communicating with NVIDIA with solution for this.\r\n\r\nFirst of all, could you try with CUDA 9 + cudnn 7 and see if the same result still shows up?\r\n\r\nSecond, in your setting, you are forcing cudnn to use TRUE_HALF_CONFIG and this mode is only supported by one internal kernel: CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM as for now.  Sometimes for the same input shape, switching from FP32 to FP16 will cause the change of the tile size for this kernel. For example, an original 128*32 tile size could become 256*64 when switching from FP32 to FP16, while in this case, the gain from doubling the math is lost for the tile utilization.\r\n\r\nIn the future we will make PSEUDO_HALF_CONFIG and TRUE_HALF_CONFIG transparent to the users by adding these two configs to autotune."}