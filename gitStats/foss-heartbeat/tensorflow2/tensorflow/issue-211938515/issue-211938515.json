{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8098", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8098/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8098/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8098/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8098", "id": 211938515, "node_id": "MDU6SXNzdWUyMTE5Mzg1MTU=", "number": 8098, "title": "ValueError: setting an array element with a sequence. (tf 1.0.0's invalid use of np)", "user": {"login": "jakubLangr", "id": 1884004, "node_id": "MDQ6VXNlcjE4ODQwMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1884004?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubLangr", "html_url": "https://github.com/jakubLangr", "followers_url": "https://api.github.com/users/jakubLangr/followers", "following_url": "https://api.github.com/users/jakubLangr/following{/other_user}", "gists_url": "https://api.github.com/users/jakubLangr/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubLangr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubLangr/subscriptions", "organizations_url": "https://api.github.com/users/jakubLangr/orgs", "repos_url": "https://api.github.com/users/jakubLangr/repos", "events_url": "https://api.github.com/users/jakubLangr/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubLangr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-03-05T08:17:37Z", "updated_at": "2018-03-09T19:37:09Z", "closed_at": "2017-03-17T16:05:42Z", "author_association": "NONE", "body_html": "<p>Using <code>tf 1.0.0</code> I get the following error.</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-153-fcd2318ab5dc&gt; in &lt;module&gt;()\n     56         batch_ys = labels[randidx, :]\n     57         # Fit training using batch data\n---&gt; 58         sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n     59         # Compute average loss\n     60         avg_cost += sess.run(cost, \n\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    765     try:\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\n--&gt; 767                          run_metadata_ptr)\n    768       if run_metadata:\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    936                 ' to a larger type (e.g. int64).')\n    937 \n--&gt; 938           np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\n    939 \n    940           if not subfeed_t.get_shape().is_compatible_with(np_val.shape):\n\n/opt/conda/lib/python3.5/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)\n    529 \n    530     \"\"\"\n--&gt; 531     return array(a, dtype, copy=False, order=order)\n    532 \n    533 \n\nValueError: setting an array element with a sequence.\n</code></pre>\n<p>This is a minimal example that should replicate this. I came across this problem also using several higher level abstractions to tf but not until I got to barebones <code>tf</code> I believed that this is a bug and not just an error on my part.</p>\n<pre><code>from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nlabels = ['Male', 'Female'] * 5 \nencoder = LabelEncoder()\nencoder.fit(labels)\nlabels = encoder.transform(labels)\nlabels = np_utils.to_categorical(labels)\nlabels.shape\n\nresults = np.ndarray(shape=(10,2623))\n\ntf.set_random_seed(0)\n# Parameters\nlearning_rate   = 0.001\ntraining_epochs = 200\nbatch_size      = 5\ndisplay_step    = 5\nntrain =  10\n\n# Network Parameters\nn_hidden_1 = 50 # 1st layer num features\nn_input    = 2623 # data input \nn_classes  = 2 # total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder(\"float\", [None, n_input])\ny = tf.placeholder(\"float\", [None, n_classes])\n\n# Create model\ndef multilayer_perceptron(_X, _weights, _biases):\n    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n    return tf.matmul(layer_1, _weights['out']) + _biases['out']\n    \n# Store layers weight &amp; bias\nstddev = 0.1 # &lt;== This greatly affects accuracy!! \nweights = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=stddev))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\nprint (\"Network Ready to Go!\")\n\npred = multilayer_perceptron(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) \noptm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \naccr = tf.reduce_mean(tf.cast(corr, \"float\"))\n\n# Initializing the variables\ninit = tf.initialize_all_variables()\nprint (\"Functions ready\")\nsess = tf.Session()\nsess.run(init)\n# Training cycle\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(ntrain/batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        randidx = np.random.randint(ntrain, size=batch_size)\n        batch_xs = results[randidx, :]\n        batch_ys = labels[randidx, :]   \n        # Fit training using batch data\n        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n        # Compute average loss\n        avg_cost += sess.run(cost, \n                feed_dict={x: batch_xs, y: batch_ys})/total_batch\n        # Display logs per epoch step\n    if epoch % display_step == 0:\n        print (\"Epoch: %03d/%03d cost: %.9f\" % \n               (epoch, training_epochs, avg_cost))\n        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\n        print (\" Training accuracy: %.3f\" % (train_acc))\n\n        print (\"Optimization Finished!\")\n</code></pre>", "body_text": "Using tf 1.0.0 I get the following error.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-153-fcd2318ab5dc> in <module>()\n     56         batch_ys = labels[randidx, :]\n     57         # Fit training using batch data\n---> 58         sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n     59         # Compute average loss\n     60         avg_cost += sess.run(cost, \n\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    765     try:\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 767                          run_metadata_ptr)\n    768       if run_metadata:\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    936                 ' to a larger type (e.g. int64).')\n    937 \n--> 938           np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\n    939 \n    940           if not subfeed_t.get_shape().is_compatible_with(np_val.shape):\n\n/opt/conda/lib/python3.5/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)\n    529 \n    530     \"\"\"\n--> 531     return array(a, dtype, copy=False, order=order)\n    532 \n    533 \n\nValueError: setting an array element with a sequence.\n\nThis is a minimal example that should replicate this. I came across this problem also using several higher level abstractions to tf but not until I got to barebones tf I believed that this is a bug and not just an error on my part.\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nlabels = ['Male', 'Female'] * 5 \nencoder = LabelEncoder()\nencoder.fit(labels)\nlabels = encoder.transform(labels)\nlabels = np_utils.to_categorical(labels)\nlabels.shape\n\nresults = np.ndarray(shape=(10,2623))\n\ntf.set_random_seed(0)\n# Parameters\nlearning_rate   = 0.001\ntraining_epochs = 200\nbatch_size      = 5\ndisplay_step    = 5\nntrain =  10\n\n# Network Parameters\nn_hidden_1 = 50 # 1st layer num features\nn_input    = 2623 # data input \nn_classes  = 2 # total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder(\"float\", [None, n_input])\ny = tf.placeholder(\"float\", [None, n_classes])\n\n# Create model\ndef multilayer_perceptron(_X, _weights, _biases):\n    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n    return tf.matmul(layer_1, _weights['out']) + _biases['out']\n    \n# Store layers weight & bias\nstddev = 0.1 # <== This greatly affects accuracy!! \nweights = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=stddev))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\nprint (\"Network Ready to Go!\")\n\npred = multilayer_perceptron(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) \noptm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \naccr = tf.reduce_mean(tf.cast(corr, \"float\"))\n\n# Initializing the variables\ninit = tf.initialize_all_variables()\nprint (\"Functions ready\")\nsess = tf.Session()\nsess.run(init)\n# Training cycle\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(ntrain/batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        randidx = np.random.randint(ntrain, size=batch_size)\n        batch_xs = results[randidx, :]\n        batch_ys = labels[randidx, :]   \n        # Fit training using batch data\n        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n        # Compute average loss\n        avg_cost += sess.run(cost, \n                feed_dict={x: batch_xs, y: batch_ys})/total_batch\n        # Display logs per epoch step\n    if epoch % display_step == 0:\n        print (\"Epoch: %03d/%03d cost: %.9f\" % \n               (epoch, training_epochs, avg_cost))\n        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\n        print (\" Training accuracy: %.3f\" % (train_acc))\n\n        print (\"Optimization Finished!\")", "body": "Using `tf 1.0.0` I get the following error.\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-153-fcd2318ab5dc> in <module>()\r\n     56         batch_ys = labels[randidx, :]\r\n     57         # Fit training using batch data\r\n---> 58         sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\r\n     59         # Compute average loss\r\n     60         avg_cost += sess.run(cost, \r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    936                 ' to a larger type (e.g. int64).')\r\n    937 \r\n--> 938           np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\r\n    939 \r\n    940           if not subfeed_t.get_shape().is_compatible_with(np_val.shape):\r\n\r\n/opt/conda/lib/python3.5/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)\r\n    529 \r\n    530     \"\"\"\r\n--> 531     return array(a, dtype, copy=False, order=order)\r\n    532 \r\n    533 \r\n\r\nValueError: setting an array element with a sequence.\r\n```\r\n\r\nThis is a minimal example that should replicate this. I came across this problem also using several higher level abstractions to tf but not until I got to barebones `tf` I believed that this is a bug and not just an error on my part.\r\n\r\n```\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom keras.utils import np_utils\r\nlabels = ['Male', 'Female'] * 5 \r\nencoder = LabelEncoder()\r\nencoder.fit(labels)\r\nlabels = encoder.transform(labels)\r\nlabels = np_utils.to_categorical(labels)\r\nlabels.shape\r\n\r\nresults = np.ndarray(shape=(10,2623))\r\n\r\ntf.set_random_seed(0)\r\n# Parameters\r\nlearning_rate   = 0.001\r\ntraining_epochs = 200\r\nbatch_size      = 5\r\ndisplay_step    = 5\r\nntrain =  10\r\n\r\n# Network Parameters\r\nn_hidden_1 = 50 # 1st layer num features\r\nn_input    = 2623 # data input \r\nn_classes  = 2 # total classes (0-9 digits)\r\n\r\n# tf Graph input\r\nx = tf.placeholder(\"float\", [None, n_input])\r\ny = tf.placeholder(\"float\", [None, n_classes])\r\n\r\n# Create model\r\ndef multilayer_perceptron(_X, _weights, _biases):\r\n    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \r\n    return tf.matmul(layer_1, _weights['out']) + _biases['out']\r\n    \r\n# Store layers weight & bias\r\nstddev = 0.1 # <== This greatly affects accuracy!! \r\nweights = {\r\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\r\n    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=stddev))\r\n}\r\nbiases = {\r\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\r\n    'out': tf.Variable(tf.random_normal([n_classes]))\r\n}\r\nprint (\"Network Ready to Go!\")\r\n\r\npred = multilayer_perceptron(x, weights, biases)\r\n\r\n# Define loss and optimizer\r\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) \r\noptm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\ncorr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \r\naccr = tf.reduce_mean(tf.cast(corr, \"float\"))\r\n\r\n# Initializing the variables\r\ninit = tf.initialize_all_variables()\r\nprint (\"Functions ready\")\r\nsess = tf.Session()\r\nsess.run(init)\r\n# Training cycle\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0.\r\n    total_batch = int(ntrain/batch_size)\r\n    # Loop over all batches\r\n    for i in range(total_batch):\r\n        randidx = np.random.randint(ntrain, size=batch_size)\r\n        batch_xs = results[randidx, :]\r\n        batch_ys = labels[randidx, :]   \r\n        # Fit training using batch data\r\n        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\r\n        # Compute average loss\r\n        avg_cost += sess.run(cost, \r\n                feed_dict={x: batch_xs, y: batch_ys})/total_batch\r\n        # Display logs per epoch step\r\n    if epoch % display_step == 0:\r\n        print (\"Epoch: %03d/%03d cost: %.9f\" % \r\n               (epoch, training_epochs, avg_cost))\r\n        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\r\n        print (\" Training accuracy: %.3f\" % (train_acc))\r\n\r\n        print (\"Optimization Finished!\")\r\n```"}