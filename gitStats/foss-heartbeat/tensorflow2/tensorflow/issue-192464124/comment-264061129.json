{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264061129", "html_url": "https://github.com/tensorflow/tensorflow/issues/5965#issuecomment-264061129", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5965", "id": 264061129, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDA2MTEyOQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-01T02:27:02Z", "updated_at": "2016-12-01T02:27:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I marked this contribution-welcome. GPU is not particularly fast for those operations. But it is conceivable that it is faster than copying the data from GPU-to-CPU, operate on CPU, and then copy the data back.</p>\n<p>The kernel itself may need some consideration. For tf.unique, if the range is limited, we can first convert it into a bit-mask, and then gather the active location, through either atomic operations, or a multiple-pass algorithm that figure out the size with preprocessing.</p>\n<p>As for TensorFlow itself, it is a bit tricky to deal with variable-size output Tensor on GPU. I think the best way is to mark them as async-op and wait for the result to come back and then set the tensor size. I can elaborate it a bit more if someone is interested in taking this up.</p>", "body_text": "I marked this contribution-welcome. GPU is not particularly fast for those operations. But it is conceivable that it is faster than copying the data from GPU-to-CPU, operate on CPU, and then copy the data back.\nThe kernel itself may need some consideration. For tf.unique, if the range is limited, we can first convert it into a bit-mask, and then gather the active location, through either atomic operations, or a multiple-pass algorithm that figure out the size with preprocessing.\nAs for TensorFlow itself, it is a bit tricky to deal with variable-size output Tensor on GPU. I think the best way is to mark them as async-op and wait for the result to come back and then set the tensor size. I can elaborate it a bit more if someone is interested in taking this up.", "body": "I marked this contribution-welcome. GPU is not particularly fast for those operations. But it is conceivable that it is faster than copying the data from GPU-to-CPU, operate on CPU, and then copy the data back. \r\n\r\nThe kernel itself may need some consideration. For tf.unique, if the range is limited, we can first convert it into a bit-mask, and then gather the active location, through either atomic operations, or a multiple-pass algorithm that figure out the size with preprocessing. \r\n\r\nAs for TensorFlow itself, it is a bit tricky to deal with variable-size output Tensor on GPU. I think the best way is to mark them as async-op and wait for the result to come back and then set the tensor size. I can elaborate it a bit more if someone is interested in taking this up.\r\n"}