{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/427466218", "html_url": "https://github.com/tensorflow/tensorflow/issues/22682#issuecomment-427466218", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22682", "id": 427466218, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzQ2NjIxOA==", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-05T18:56:28Z", "updated_at": "2018-10-05T18:56:28Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6732996\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ChengshuLi\">@ChengshuLi</a> regarding <code>I tried tensorflow-native, FP32 and FP16. All three models seem to have the same inference speed.</code> I think the main reason is that the conversion creates a lot of small TRT engines, most of which contains &lt;=5 nodes and the largest one contains only 30 nodes. TRT prefers large engines so it can optimize it better, so it may be worthwhile to investigate why it is not the case for your graph.</p>\n<p>For TRT5, I believed as of today it's still an RC, and TF do not release with RC libraries. We're working on testing TRT5 and once it's final version is released we'll get it into TF ASAP.</p>\n<p>Thanks.</p>", "body_text": "@ChengshuLi regarding I tried tensorflow-native, FP32 and FP16. All three models seem to have the same inference speed. I think the main reason is that the conversion creates a lot of small TRT engines, most of which contains <=5 nodes and the largest one contains only 30 nodes. TRT prefers large engines so it can optimize it better, so it may be worthwhile to investigate why it is not the case for your graph.\nFor TRT5, I believed as of today it's still an RC, and TF do not release with RC libraries. We're working on testing TRT5 and once it's final version is released we'll get it into TF ASAP.\nThanks.", "body": "@ChengshuLi regarding `I tried tensorflow-native, FP32 and FP16. All three models seem to have the same inference speed.` I think the main reason is that the conversion creates a lot of small TRT engines, most of which contains <=5 nodes and the largest one contains only 30 nodes. TRT prefers large engines so it can optimize it better, so it may be worthwhile to investigate why it is not the case for your graph.\r\n\r\nFor TRT5, I believed as of today it's still an RC, and TF do not release with RC libraries. We're working on testing TRT5 and once it's final version is released we'll get it into TF ASAP.\r\n\r\nThanks."}