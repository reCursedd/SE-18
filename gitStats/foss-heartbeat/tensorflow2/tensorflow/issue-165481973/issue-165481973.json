{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3307", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3307/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3307/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3307/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3307", "id": 165481973, "node_id": "MDU6SXNzdWUxNjU0ODE5NzM=", "number": 3307, "title": "NaN values in the learnable parameters with custom initialization, along with lasso regularization", "user": {"login": "asaydin", "id": 5859763, "node_id": "MDQ6VXNlcjU4NTk3NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5859763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asaydin", "html_url": "https://github.com/asaydin", "followers_url": "https://api.github.com/users/asaydin/followers", "following_url": "https://api.github.com/users/asaydin/following{/other_user}", "gists_url": "https://api.github.com/users/asaydin/gists{/gist_id}", "starred_url": "https://api.github.com/users/asaydin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asaydin/subscriptions", "organizations_url": "https://api.github.com/users/asaydin/orgs", "repos_url": "https://api.github.com/users/asaydin/repos", "events_url": "https://api.github.com/users/asaydin/events{/privacy}", "received_events_url": "https://api.github.com/users/asaydin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-07-14T05:59:21Z", "updated_at": "2017-07-19T19:04:48Z", "closed_at": "2016-07-14T22:06:09Z", "author_association": "NONE", "body_html": "<p>From what I read online, getting NaN cost values can happen if someone uses a custom cross entropy calculation while doing classification. However, I use the built-in cross entropy calculation provided.</p>\n<p>My problem is different. I have a network that consists of an RNN and a convolutional layer before that. When I initialize the convolutional layer with random values(e.g., normal random or uniform random), I do not experience any problem. However, I use a custom initialization, and start getting NaN gradients right away(i.e., starting from the first batch).</p>\n<p>My filter initialization is as follows: All values in the filter are zero, except a single value is 1. When I initialize the filters in this manner, I start to get NaN gradients starting from the first batch, hence NaN weights and NaN cost in the following batches.</p>\n<h3>Environment info</h3>\n<p>Operating System: Mac v10.11.3 (El Capitan)</p>\n<p>The output of <code>ls -l /path/to/cuda/lib/libcud*</code>:</p>\n<p>-rw-r--r-- 1 root root 189170 Mar 24 16:02 libcudadevrt.a<br>\nlrwxrwxrwx 1 root root     16 Mar 24 16:02 libcudart.so -&gt; libcudart.so.7.5<br>\nlrwxrwxrwx 1 root root     19 Mar 24 16:02 libcudart.so.7.5 -&gt; libcudart.so.7.5.18<br>\n-rwxr-xr-x 1 root root 311596 Mar 24 16:02 libcudart.so.7.5.18<br>\n-rw-r--r-- 1 root root 558020 Mar 24 16:02 libcudart_static.a</p>\n<ul>\n<li>The tensorflow version is: 0.9.0rc0</li>\n</ul>\n<p>The whole code is quite messy and complex, . But reproducing the code should not be hard: I initialize convolutional filters with all zeros but one \"1\" value, and I get nan values as gradients in the locations where 0's appear.</p>\n<p>One thing to note is that I apply group lasso regularization to the convolutional filter weights. I realized when regularization lambda is set to a small value, (like 10e-3), the problem did not occur. However, when I chose values that have larger order of magnitude (like 10e-2 or 1), I observed this problem. Values other than the group initialized as \"1\" becomes all Nan.</p>\n<p>To exemplify, suppose I initialize the weights as the matrix on the top, and each column is a group (in group lasso objective). After the first batch of data, the matrix on the bottom is what I get:</p>\n<p>0 0 0 0 0<br>\n0 0 0 0 0<br>\n0 0 1 0 0<br>\n0 0 0 0 0<br>\n0 0 0 0 0</p>\n<p>becomes:</p>\n<p>NaN NaN 0.3 NaN NaN<br>\nNaN NaN 0.7 NaN NaN<br>\nNaN NaN  0.1  NaN NaN<br>\nNaN NaN 0.5 NaN NaN<br>\nNaN NaN 0.2 NaN NaN</p>\n<p>Long story short; I know how to avoid this problem (using smaller lambda values), but I just wanted to know if there is an explanation for why this problem occur. Is it because of a numerical error, or it is because I am doing something wrong?</p>", "body_text": "From what I read online, getting NaN cost values can happen if someone uses a custom cross entropy calculation while doing classification. However, I use the built-in cross entropy calculation provided.\nMy problem is different. I have a network that consists of an RNN and a convolutional layer before that. When I initialize the convolutional layer with random values(e.g., normal random or uniform random), I do not experience any problem. However, I use a custom initialization, and start getting NaN gradients right away(i.e., starting from the first batch).\nMy filter initialization is as follows: All values in the filter are zero, except a single value is 1. When I initialize the filters in this manner, I start to get NaN gradients starting from the first batch, hence NaN weights and NaN cost in the following batches.\nEnvironment info\nOperating System: Mac v10.11.3 (El Capitan)\nThe output of ls -l /path/to/cuda/lib/libcud*:\n-rw-r--r-- 1 root root 189170 Mar 24 16:02 libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Mar 24 16:02 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Mar 24 16:02 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Mar 24 16:02 libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Mar 24 16:02 libcudart_static.a\n\nThe tensorflow version is: 0.9.0rc0\n\nThe whole code is quite messy and complex, . But reproducing the code should not be hard: I initialize convolutional filters with all zeros but one \"1\" value, and I get nan values as gradients in the locations where 0's appear.\nOne thing to note is that I apply group lasso regularization to the convolutional filter weights. I realized when regularization lambda is set to a small value, (like 10e-3), the problem did not occur. However, when I chose values that have larger order of magnitude (like 10e-2 or 1), I observed this problem. Values other than the group initialized as \"1\" becomes all Nan.\nTo exemplify, suppose I initialize the weights as the matrix on the top, and each column is a group (in group lasso objective). After the first batch of data, the matrix on the bottom is what I get:\n0 0 0 0 0\n0 0 0 0 0\n0 0 1 0 0\n0 0 0 0 0\n0 0 0 0 0\nbecomes:\nNaN NaN 0.3 NaN NaN\nNaN NaN 0.7 NaN NaN\nNaN NaN  0.1  NaN NaN\nNaN NaN 0.5 NaN NaN\nNaN NaN 0.2 NaN NaN\nLong story short; I know how to avoid this problem (using smaller lambda values), but I just wanted to know if there is an explanation for why this problem occur. Is it because of a numerical error, or it is because I am doing something wrong?", "body": "From what I read online, getting NaN cost values can happen if someone uses a custom cross entropy calculation while doing classification. However, I use the built-in cross entropy calculation provided.\n\nMy problem is different. I have a network that consists of an RNN and a convolutional layer before that. When I initialize the convolutional layer with random values(e.g., normal random or uniform random), I do not experience any problem. However, I use a custom initialization, and start getting NaN gradients right away(i.e., starting from the first batch).\n\nMy filter initialization is as follows: All values in the filter are zero, except a single value is 1. When I initialize the filters in this manner, I start to get NaN gradients starting from the first batch, hence NaN weights and NaN cost in the following batches. \n### Environment info\n\nOperating System: Mac v10.11.3 (El Capitan)\n\nThe output of `ls -l /path/to/cuda/lib/libcud*`:\n\n-rw-r--r-- 1 root root 189170 Mar 24 16:02 libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Mar 24 16:02 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Mar 24 16:02 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Mar 24 16:02 libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Mar 24 16:02 libcudart_static.a\n- The tensorflow version is: 0.9.0rc0\n\nThe whole code is quite messy and complex, . But reproducing the code should not be hard: I initialize convolutional filters with all zeros but one \"1\" value, and I get nan values as gradients in the locations where 0's appear. \n\nOne thing to note is that I apply group lasso regularization to the convolutional filter weights. I realized when regularization lambda is set to a small value, (like 10e-3), the problem did not occur. However, when I chose values that have larger order of magnitude (like 10e-2 or 1), I observed this problem. Values other than the group initialized as \"1\" becomes all Nan.\n\nTo exemplify, suppose I initialize the weights as the matrix on the top, and each column is a group (in group lasso objective). After the first batch of data, the matrix on the bottom is what I get: \n\n0 0 0 0 0\n0 0 0 0 0\n0 0 1 0 0\n0 0 0 0 0\n0 0 0 0 0\n\nbecomes:        \n\nNaN NaN 0.3 NaN NaN \nNaN NaN 0.7 NaN NaN\nNaN NaN  0.1  NaN NaN\nNaN NaN 0.5 NaN NaN\nNaN NaN 0.2 NaN NaN \n\nLong story short; I know how to avoid this problem (using smaller lambda values), but I just wanted to know if there is an explanation for why this problem occur. Is it because of a numerical error, or it is because I am doing something wrong?\n"}