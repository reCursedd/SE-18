{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11433", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11433/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11433/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11433/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11433", "id": 242052072, "node_id": "MDU6SXNzdWUyNDIwNTIwNzI=", "number": 11433, "title": "My CNN is not learning , who can help me fix this porblem ?", "user": {"login": "FightingZhen", "id": 26176607, "node_id": "MDQ6VXNlcjI2MTc2NjA3", "avatar_url": "https://avatars2.githubusercontent.com/u/26176607?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FightingZhen", "html_url": "https://github.com/FightingZhen", "followers_url": "https://api.github.com/users/FightingZhen/followers", "following_url": "https://api.github.com/users/FightingZhen/following{/other_user}", "gists_url": "https://api.github.com/users/FightingZhen/gists{/gist_id}", "starred_url": "https://api.github.com/users/FightingZhen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FightingZhen/subscriptions", "organizations_url": "https://api.github.com/users/FightingZhen/orgs", "repos_url": "https://api.github.com/users/FightingZhen/repos", "events_url": "https://api.github.com/users/FightingZhen/events{/privacy}", "received_events_url": "https://api.github.com/users/FightingZhen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-07-11T13:42:18Z", "updated_at": "2017-07-11T13:42:50Z", "closed_at": "2017-07-11T13:42:50Z", "author_association": "NONE", "body_html": "<p>import tensorflow as tf<br>\nimport Transform_Data_New as td<br>\nimport numpy as np</p>\n<p>max_accuracy=0<br>\ntimer=0<br>\nEpoch=300<br>\nlearning_rate=1e-3</p>\n<p>training_batch_size=100<br>\nvalidation_batch_size=40<br>\ntest_batch_size=40</p>\n<p>len_training_data, validation, test = td.Initialization(validation_batch_size,test_batch_size)<br>\niteration_time=int(len_training_data/training_batch_size)+1</p>\n<p>sess=tf.InteractiveSession()</p>\n<p>def weight_variable(shape,Name):<br>\ninitial = tf.random_normal(shape,stddev=0.2,mean=0.5)<br>\nreturn tf.Variable(initial,name=Name)</p>\n<p>def bias_variable(shape,Name):<br>\ninitial=tf.constant(0.1, shape=shape,)<br>\nreturn tf.Variable(initial,name=Name)</p>\n<p>def conv2d(x,w):<br>\nreturn tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')</p>\n<p>def max_pool_2x2(x):<br>\nreturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME')</p>\n<p>with tf.name_scope('Input'):<br>\nwith tf.name_scope('Input_x'):<br>\nx = tf.placeholder(tf.float32,shape=[None,1024])<br>\nwith tf.name_scope('Input_y'):<br>\ny_ = tf.placeholder(tf.float32,shape=[None,7])</p>\n<p>with tf.name_scope('Conv_1'):<br>\nwith tf.name_scope('W_conv1'):<br>\nw_conv1=weight_variable([3,3,1,8],'w_conv1')<br>\nwith tf.name_scope('B_conv1'):<br>\nb_conv1=bias_variable([8],'b_conv1')<br>\nwith tf.name_scope('x_image'):<br>\nx_image=tf.reshape(x,[-1,32,32,1])<br>\nwith tf.name_scope('H_conv1'):<br>\nh_conv1=tf.nn.relu(tf.nn.bias_add(conv2d(x_image,w_conv1),b_conv1))</p>\n<p>with tf.name_scope('Conv_2'):<br>\nwith tf.name_scope('W_conv2'):<br>\nw_conv2=weight_variable([3,3,8,16],'w_conv2')<br>\nwith tf.name_scope('B_conv2'):<br>\nb_conv2=bias_variable([16],'b_conv2')<br>\nwith tf.name_scope('H_conv2'):<br>\nh_conv2=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv1,w_conv2),b_conv2))<br>\nwith tf.name_scope('H_pool2'):<br>\nh_pool2=max_pool_2x2(h_conv2)</p>\n<p>with tf.name_scope('Conv_3'):<br>\nwith tf.name_scope('W_conv3'):<br>\nw_conv3=weight_variable([3,3,16,32],'w_conv3')<br>\nwith tf.name_scope('B_conv3'):<br>\nb_conv3=bias_variable([32],'b_conv3')<br>\nwith tf.name_scope('H_conv3'):<br>\nh_conv3=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool2,w_conv3),b_conv3))</p>\n<p>with tf.name_scope('Conv_4'):<br>\nwith tf.name_scope('W_conv4'):<br>\nw_conv4=weight_variable([3,3,32,64],'w_conv4')<br>\nwith tf.name_scope('B_conv4'):<br>\nb_conv4=bias_variable([64],'b_conv4')<br>\nwith tf.name_scope('H_conv4'):<br>\nh_conv4=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv3,w_conv4),b_conv4))<br>\nwith tf.name_scope('H_pool4'):<br>\nh_pool4=max_pool_2x2(h_conv4)</p>\n<p>with tf.name_scope('Conv_5'):<br>\nwith tf.name_scope('W_conv5'):<br>\nw_conv5=weight_variable([3,3,64,128],'w_conv5')<br>\nwith tf.name_scope('B_conv5'):<br>\nb_conv5=bias_variable([128],'b_conv5')<br>\nwith tf.name_scope('H_conv5'):<br>\nh_conv5=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool4,w_conv5),b_conv5))</p>\n<p>with tf.name_scope('Conv_6'):<br>\nwith tf.name_scope('W_conv6'):<br>\nw_conv6=weight_variable([3,3,128,256],'w_conv6')<br>\nwith tf.name_scope('B_conv6'):<br>\nb_conv6=bias_variable([256],'b_conv6')<br>\nwith tf.name_scope('H_conv6'):<br>\nh_conv6=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv5,w_conv6),b_conv6))<br>\nwith tf.name_scope('H_pool6'):<br>\nh_pool6=max_pool_2x2(h_conv6)</p>\n<p>with tf.name_scope('Full_Connected_Layer_1'):<br>\nwith tf.name_scope('W_fc1'):<br>\nw_fc1=weight_variable([4<em>4</em>256,1024],'w_fc1')<br>\nwith tf.name_scope('B_fc1'):<br>\nb_fc1=bias_variable([1024],'b_fc1')<br>\nwith tf.name_scope('H_pool_flat'):<br>\nh_pool_flat=tf.reshape(h_pool6,[-1,4<em>4</em>256])<br>\nwith tf.name_scope('H_fc1'):<br>\nh_fc1=tf.nn.relu(tf.matmul(h_pool_flat,w_fc1)+b_fc1)</p>\n<p>with tf.name_scope('Full_Connected_Layer_2'):<br>\nwith tf.name_scope('W_fc2'):<br>\nw_fc2=weight_variable([1024,7],'w_fc2')<br>\nwith tf.name_scope('B_fc2'):<br>\nb_fc2=bias_variable([7],'b_fc2')<br>\nwith tf.name_scope('Y_conv'):<br>\ny_conv=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)</p>\n<p>with tf.name_scope('Cross_Entropy'):<br>\ncross_entropy=-tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))<br>\ntf.summary.scalar('Cross_Entropy',cross_entropy)<br>\nwith tf.name_scope('Train_Step'):<br>\ntrain_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)<br>\nwith tf.name_scope('Correct_prediction'):<br>\ndistribution=[tf.arg_max(y_,1),tf.arg_max(y_conv,1)]<br>\ncorrect_prediction=tf.equal(distribution[0],distribution[1])<br>\nwith tf.name_scope('Accuracy'):<br>\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,\"float\"))<br>\ntf.summary.scalar('Accuracy',accuracy)</p>\n<p>merged=tf.summary.merge_all()<br>\nwriter=tf.summary.FileWriter('D:/Log',sess.graph)<br>\nepoch=0</p>\n<p>saver=tf.train.Saver()<br>\nFirst_training=True<br>\ncheckpoint_dir='D:\\Checkpoint\\model.ckpt'</p>\n<p>if First_training==False:<br>\nckpt=tf.train.get_checkpoint_state(checkpoint_dir)<br>\nif ckpt and ckpt.model_checkpoint_path:<br>\nsaver.restore(sess,checkpoint_dir)<br>\nelse:<br>\nsess.run(tf.global_variables_initializer())</p>\n<p>for i in range(Epoch*iteration_time+1):<br>\n#Batch Size<br>\nbatch = td.next_batch(training_batch_size)<br>\ntrain_step.run(feed_dict={x: batch[0], y_: batch[1]})<br>\nif i % (iteration_time) == 0 and i != 0:<br>\nepoch += 1<br>\ntrain_accuracy=accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})<br>\nvalidation_accuracy_resultSet = []<br>\nfor j in range(len(validation)):<br>\nvalidation_accuracy = accuracy.eval(feed_dict={x: validation[j][0], y_: validation[j][1]})<br>\nvalidation_accuracy_resultSet.append(validation_accuracy)<br>\nvalidation_accuracy = int(np.sum(validation_accuracy_resultSet)) / len(validation_accuracy_resultSet)<br>\nif validation_accuracy&lt;=0.7:<br>\nlearning_rate=1e-3<br>\nif validation_accuracy&gt;0.7 and validation_accuracy&lt;=0.8:<br>\nlearning_rate=5e-4<br>\nif validation_accuracy&gt;0.8 and validation_accuracy&lt;=0.9:<br>\nlearning_rate=1e-4<br>\nif validation_accuracy&gt;0.9:<br>\nlearning_rate=5e-5<br>\nprint(\"Epoch %d , training accuracy %g,Validation Accuracy: %g\" % (epoch, train_accuracy, validation_accuracy))</p>\n<pre><code>    result = sess.run(merged, feed_dict={x: batch[0], y_: batch[1]})\n    writer.add_summary(result, epoch)\n    saver.save(sess,checkpoint_dir,global_step=epoch)\n    if validation_accuracy&gt;max_accuracy:\n        max_accuracy=validation_accuracy\n        timer=0\n    else:\n        timer+=1\n        if timer&gt;10:   #\u539f\u6765\u662f10\n            break\n</code></pre>\n<p>confusion_matrics=np.zeros([7,7],dtype=\"int\")<br>\ntest_accuracy_resultSet=[]<br>\nfor j in range(len(test)):<br>\nmatrix_row, matrix_col = sess.run(distribution, feed_dict={x: test[j][0], y_: test[j][1]})<br>\nfor m, n in zip(matrix_row, matrix_col):<br>\nconfusion_matrics[m][n] += 1<br>\ntest_accuracy = accuracy.eval(feed_dict={x: test[j][0], y_: test[j][1]})<br>\ntest_accuracy_resultSet.append(test_accuracy)<br>\ntest_accuracy = np.sum(test_accuracy_resultSet) / len(test_accuracy_resultSet)</p>\n<p>print(\"Test Accuracy :\",test_accuracy)<br>\nprint(np.array(confusion_matrics.tolist()))</p>\n<p>That's the whole code of my CNN, the data for training, validation, test are all organized myself, which is not the problem.<br>\nWhile training this CNN, the result in Console showed that all weights and biases did not change. If I replace the RELU in six convolutional layers with Sigmoid, the CNN performed well.<br>\nWho can help me fix this problem? Thanks anyway ~</p>", "body_text": "import tensorflow as tf\nimport Transform_Data_New as td\nimport numpy as np\nmax_accuracy=0\ntimer=0\nEpoch=300\nlearning_rate=1e-3\ntraining_batch_size=100\nvalidation_batch_size=40\ntest_batch_size=40\nlen_training_data, validation, test = td.Initialization(validation_batch_size,test_batch_size)\niteration_time=int(len_training_data/training_batch_size)+1\nsess=tf.InteractiveSession()\ndef weight_variable(shape,Name):\ninitial = tf.random_normal(shape,stddev=0.2,mean=0.5)\nreturn tf.Variable(initial,name=Name)\ndef bias_variable(shape,Name):\ninitial=tf.constant(0.1, shape=shape,)\nreturn tf.Variable(initial,name=Name)\ndef conv2d(x,w):\nreturn tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\nreturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME')\nwith tf.name_scope('Input'):\nwith tf.name_scope('Input_x'):\nx = tf.placeholder(tf.float32,shape=[None,1024])\nwith tf.name_scope('Input_y'):\ny_ = tf.placeholder(tf.float32,shape=[None,7])\nwith tf.name_scope('Conv_1'):\nwith tf.name_scope('W_conv1'):\nw_conv1=weight_variable([3,3,1,8],'w_conv1')\nwith tf.name_scope('B_conv1'):\nb_conv1=bias_variable([8],'b_conv1')\nwith tf.name_scope('x_image'):\nx_image=tf.reshape(x,[-1,32,32,1])\nwith tf.name_scope('H_conv1'):\nh_conv1=tf.nn.relu(tf.nn.bias_add(conv2d(x_image,w_conv1),b_conv1))\nwith tf.name_scope('Conv_2'):\nwith tf.name_scope('W_conv2'):\nw_conv2=weight_variable([3,3,8,16],'w_conv2')\nwith tf.name_scope('B_conv2'):\nb_conv2=bias_variable([16],'b_conv2')\nwith tf.name_scope('H_conv2'):\nh_conv2=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv1,w_conv2),b_conv2))\nwith tf.name_scope('H_pool2'):\nh_pool2=max_pool_2x2(h_conv2)\nwith tf.name_scope('Conv_3'):\nwith tf.name_scope('W_conv3'):\nw_conv3=weight_variable([3,3,16,32],'w_conv3')\nwith tf.name_scope('B_conv3'):\nb_conv3=bias_variable([32],'b_conv3')\nwith tf.name_scope('H_conv3'):\nh_conv3=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool2,w_conv3),b_conv3))\nwith tf.name_scope('Conv_4'):\nwith tf.name_scope('W_conv4'):\nw_conv4=weight_variable([3,3,32,64],'w_conv4')\nwith tf.name_scope('B_conv4'):\nb_conv4=bias_variable([64],'b_conv4')\nwith tf.name_scope('H_conv4'):\nh_conv4=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv3,w_conv4),b_conv4))\nwith tf.name_scope('H_pool4'):\nh_pool4=max_pool_2x2(h_conv4)\nwith tf.name_scope('Conv_5'):\nwith tf.name_scope('W_conv5'):\nw_conv5=weight_variable([3,3,64,128],'w_conv5')\nwith tf.name_scope('B_conv5'):\nb_conv5=bias_variable([128],'b_conv5')\nwith tf.name_scope('H_conv5'):\nh_conv5=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool4,w_conv5),b_conv5))\nwith tf.name_scope('Conv_6'):\nwith tf.name_scope('W_conv6'):\nw_conv6=weight_variable([3,3,128,256],'w_conv6')\nwith tf.name_scope('B_conv6'):\nb_conv6=bias_variable([256],'b_conv6')\nwith tf.name_scope('H_conv6'):\nh_conv6=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv5,w_conv6),b_conv6))\nwith tf.name_scope('H_pool6'):\nh_pool6=max_pool_2x2(h_conv6)\nwith tf.name_scope('Full_Connected_Layer_1'):\nwith tf.name_scope('W_fc1'):\nw_fc1=weight_variable([44256,1024],'w_fc1')\nwith tf.name_scope('B_fc1'):\nb_fc1=bias_variable([1024],'b_fc1')\nwith tf.name_scope('H_pool_flat'):\nh_pool_flat=tf.reshape(h_pool6,[-1,44256])\nwith tf.name_scope('H_fc1'):\nh_fc1=tf.nn.relu(tf.matmul(h_pool_flat,w_fc1)+b_fc1)\nwith tf.name_scope('Full_Connected_Layer_2'):\nwith tf.name_scope('W_fc2'):\nw_fc2=weight_variable([1024,7],'w_fc2')\nwith tf.name_scope('B_fc2'):\nb_fc2=bias_variable([7],'b_fc2')\nwith tf.name_scope('Y_conv'):\ny_conv=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)\nwith tf.name_scope('Cross_Entropy'):\ncross_entropy=-tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))\ntf.summary.scalar('Cross_Entropy',cross_entropy)\nwith tf.name_scope('Train_Step'):\ntrain_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\nwith tf.name_scope('Correct_prediction'):\ndistribution=[tf.arg_max(y_,1),tf.arg_max(y_conv,1)]\ncorrect_prediction=tf.equal(distribution[0],distribution[1])\nwith tf.name_scope('Accuracy'):\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\ntf.summary.scalar('Accuracy',accuracy)\nmerged=tf.summary.merge_all()\nwriter=tf.summary.FileWriter('D:/Log',sess.graph)\nepoch=0\nsaver=tf.train.Saver()\nFirst_training=True\ncheckpoint_dir='D:\\Checkpoint\\model.ckpt'\nif First_training==False:\nckpt=tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt and ckpt.model_checkpoint_path:\nsaver.restore(sess,checkpoint_dir)\nelse:\nsess.run(tf.global_variables_initializer())\nfor i in range(Epoch*iteration_time+1):\n#Batch Size\nbatch = td.next_batch(training_batch_size)\ntrain_step.run(feed_dict={x: batch[0], y_: batch[1]})\nif i % (iteration_time) == 0 and i != 0:\nepoch += 1\ntrain_accuracy=accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\nvalidation_accuracy_resultSet = []\nfor j in range(len(validation)):\nvalidation_accuracy = accuracy.eval(feed_dict={x: validation[j][0], y_: validation[j][1]})\nvalidation_accuracy_resultSet.append(validation_accuracy)\nvalidation_accuracy = int(np.sum(validation_accuracy_resultSet)) / len(validation_accuracy_resultSet)\nif validation_accuracy<=0.7:\nlearning_rate=1e-3\nif validation_accuracy>0.7 and validation_accuracy<=0.8:\nlearning_rate=5e-4\nif validation_accuracy>0.8 and validation_accuracy<=0.9:\nlearning_rate=1e-4\nif validation_accuracy>0.9:\nlearning_rate=5e-5\nprint(\"Epoch %d , training accuracy %g,Validation Accuracy: %g\" % (epoch, train_accuracy, validation_accuracy))\n    result = sess.run(merged, feed_dict={x: batch[0], y_: batch[1]})\n    writer.add_summary(result, epoch)\n    saver.save(sess,checkpoint_dir,global_step=epoch)\n    if validation_accuracy>max_accuracy:\n        max_accuracy=validation_accuracy\n        timer=0\n    else:\n        timer+=1\n        if timer>10:   #\u539f\u6765\u662f10\n            break\n\nconfusion_matrics=np.zeros([7,7],dtype=\"int\")\ntest_accuracy_resultSet=[]\nfor j in range(len(test)):\nmatrix_row, matrix_col = sess.run(distribution, feed_dict={x: test[j][0], y_: test[j][1]})\nfor m, n in zip(matrix_row, matrix_col):\nconfusion_matrics[m][n] += 1\ntest_accuracy = accuracy.eval(feed_dict={x: test[j][0], y_: test[j][1]})\ntest_accuracy_resultSet.append(test_accuracy)\ntest_accuracy = np.sum(test_accuracy_resultSet) / len(test_accuracy_resultSet)\nprint(\"Test Accuracy :\",test_accuracy)\nprint(np.array(confusion_matrics.tolist()))\nThat's the whole code of my CNN, the data for training, validation, test are all organized myself, which is not the problem.\nWhile training this CNN, the result in Console showed that all weights and biases did not change. If I replace the RELU in six convolutional layers with Sigmoid, the CNN performed well.\nWho can help me fix this problem? Thanks anyway ~", "body": "import tensorflow as tf\r\nimport Transform_Data_New as td\r\nimport numpy as np\r\n\r\nmax_accuracy=0\r\ntimer=0\r\nEpoch=300\r\nlearning_rate=1e-3\r\n\r\ntraining_batch_size=100\r\nvalidation_batch_size=40\r\ntest_batch_size=40\r\n\r\nlen_training_data, validation, test = td.Initialization(validation_batch_size,test_batch_size)\r\niteration_time=int(len_training_data/training_batch_size)+1\r\n\r\nsess=tf.InteractiveSession()\r\n\r\ndef weight_variable(shape,Name):\r\n    initial = tf.random_normal(shape,stddev=0.2,mean=0.5)\r\n    return tf.Variable(initial,name=Name)\r\n\r\ndef bias_variable(shape,Name):\r\n    initial=tf.constant(0.1, shape=shape,)\r\n    return tf.Variable(initial,name=Name)\r\n\r\ndef conv2d(x,w):\r\n    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME')\r\n\r\nwith tf.name_scope('Input'):\r\n    with tf.name_scope('Input_x'):\r\n        x = tf.placeholder(tf.float32,shape=[None,1024])\r\n    with tf.name_scope('Input_y'):\r\n        y_ = tf.placeholder(tf.float32,shape=[None,7])\r\n\r\nwith tf.name_scope('Conv_1'):\r\n    with tf.name_scope('W_conv1'):\r\n        w_conv1=weight_variable([3,3,1,8],'w_conv1')\r\n    with tf.name_scope('B_conv1'):\r\n        b_conv1=bias_variable([8],'b_conv1')\r\n    with tf.name_scope('x_image'):\r\n        x_image=tf.reshape(x,[-1,32,32,1])\r\n    with tf.name_scope('H_conv1'):\r\n        h_conv1=tf.nn.relu(tf.nn.bias_add(conv2d(x_image,w_conv1),b_conv1))\r\n\r\nwith tf.name_scope('Conv_2'):\r\n    with tf.name_scope('W_conv2'):\r\n        w_conv2=weight_variable([3,3,8,16],'w_conv2')\r\n    with tf.name_scope('B_conv2'):\r\n        b_conv2=bias_variable([16],'b_conv2')\r\n    with tf.name_scope('H_conv2'):\r\n        h_conv2=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv1,w_conv2),b_conv2))\r\n    with tf.name_scope('H_pool2'):\r\n        h_pool2=max_pool_2x2(h_conv2)\r\n\r\nwith tf.name_scope('Conv_3'):\r\n    with tf.name_scope('W_conv3'):\r\n        w_conv3=weight_variable([3,3,16,32],'w_conv3')\r\n    with tf.name_scope('B_conv3'):\r\n        b_conv3=bias_variable([32],'b_conv3')\r\n    with tf.name_scope('H_conv3'):\r\n        h_conv3=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool2,w_conv3),b_conv3))\r\n\r\nwith tf.name_scope('Conv_4'):\r\n    with tf.name_scope('W_conv4'):\r\n        w_conv4=weight_variable([3,3,32,64],'w_conv4')\r\n    with tf.name_scope('B_conv4'):\r\n        b_conv4=bias_variable([64],'b_conv4')\r\n    with tf.name_scope('H_conv4'):\r\n        h_conv4=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv3,w_conv4),b_conv4))\r\n    with tf.name_scope('H_pool4'):\r\n        h_pool4=max_pool_2x2(h_conv4)\r\n\r\nwith tf.name_scope('Conv_5'):\r\n    with tf.name_scope('W_conv5'):\r\n        w_conv5=weight_variable([3,3,64,128],'w_conv5')\r\n    with tf.name_scope('B_conv5'):\r\n        b_conv5=bias_variable([128],'b_conv5')\r\n    with tf.name_scope('H_conv5'):\r\n        h_conv5=tf.nn.relu(tf.nn.bias_add(conv2d(h_pool4,w_conv5),b_conv5))\r\n\r\nwith tf.name_scope('Conv_6'):\r\n    with tf.name_scope('W_conv6'):\r\n        w_conv6=weight_variable([3,3,128,256],'w_conv6')\r\n    with tf.name_scope('B_conv6'):\r\n        b_conv6=bias_variable([256],'b_conv6')\r\n    with tf.name_scope('H_conv6'):\r\n        h_conv6=tf.nn.relu(tf.nn.bias_add(conv2d(h_conv5,w_conv6),b_conv6))\r\n    with tf.name_scope('H_pool6'):\r\n        h_pool6=max_pool_2x2(h_conv6)\r\n\r\nwith tf.name_scope('Full_Connected_Layer_1'):\r\n    with tf.name_scope('W_fc1'):\r\n        w_fc1=weight_variable([4*4*256,1024],'w_fc1')\r\n    with tf.name_scope('B_fc1'):\r\n        b_fc1=bias_variable([1024],'b_fc1')\r\n    with tf.name_scope('H_pool_flat'):\r\n        h_pool_flat=tf.reshape(h_pool6,[-1,4*4*256])\r\n    with tf.name_scope('H_fc1'):\r\n        h_fc1=tf.nn.relu(tf.matmul(h_pool_flat,w_fc1)+b_fc1)\r\n\r\nwith tf.name_scope('Full_Connected_Layer_2'):\r\n    with tf.name_scope('W_fc2'):\r\n        w_fc2=weight_variable([1024,7],'w_fc2')\r\n    with tf.name_scope('B_fc2'):\r\n        b_fc2=bias_variable([7],'b_fc2')\r\n    with tf.name_scope('Y_conv'):\r\n        y_conv=tf.nn.softmax(tf.matmul(h_fc1,w_fc2)+b_fc2)\r\n\r\nwith tf.name_scope('Cross_Entropy'):\r\n    cross_entropy=-tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))\r\n    tf.summary.scalar('Cross_Entropy',cross_entropy)\r\nwith tf.name_scope('Train_Step'):\r\n    train_step=tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\r\nwith tf.name_scope('Correct_prediction'):\r\n    distribution=[tf.arg_max(y_,1),tf.arg_max(y_conv,1)]\r\n    correct_prediction=tf.equal(distribution[0],distribution[1])\r\nwith tf.name_scope('Accuracy'):\r\n    accuracy=tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\r\n    tf.summary.scalar('Accuracy',accuracy)\r\n\r\nmerged=tf.summary.merge_all()\r\nwriter=tf.summary.FileWriter('D:/Log',sess.graph)\r\nepoch=0\r\n\r\nsaver=tf.train.Saver()\r\nFirst_training=True\r\ncheckpoint_dir='D:\\\\Checkpoint\\\\model.ckpt'\r\n\r\nif First_training==False:\r\n    ckpt=tf.train.get_checkpoint_state(checkpoint_dir)\r\n    if ckpt and ckpt.model_checkpoint_path:\r\n        saver.restore(sess,checkpoint_dir)\r\nelse:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\nfor i in range(Epoch*iteration_time+1):\r\n    #Batch Size\r\n    batch = td.next_batch(training_batch_size)\r\n    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\r\n    if i % (iteration_time) == 0 and i != 0:\r\n        epoch += 1\r\n        train_accuracy=accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\r\n        validation_accuracy_resultSet = []\r\n        for j in range(len(validation)):\r\n            validation_accuracy = accuracy.eval(feed_dict={x: validation[j][0], y_: validation[j][1]})\r\n            validation_accuracy_resultSet.append(validation_accuracy)\r\n        validation_accuracy = int(np.sum(validation_accuracy_resultSet)) / len(validation_accuracy_resultSet)\r\n        if validation_accuracy<=0.7:\r\n            learning_rate=1e-3\r\n        if validation_accuracy>0.7 and validation_accuracy<=0.8:\r\n            learning_rate=5e-4\r\n        if validation_accuracy>0.8 and validation_accuracy<=0.9:\r\n            learning_rate=1e-4\r\n        if validation_accuracy>0.9:\r\n            learning_rate=5e-5\r\n        print(\"Epoch %d , training accuracy %g,Validation Accuracy: %g\" % (epoch, train_accuracy, validation_accuracy))\r\n\r\n        result = sess.run(merged, feed_dict={x: batch[0], y_: batch[1]})\r\n        writer.add_summary(result, epoch)\r\n        saver.save(sess,checkpoint_dir,global_step=epoch)\r\n        if validation_accuracy>max_accuracy:\r\n            max_accuracy=validation_accuracy\r\n            timer=0\r\n        else:\r\n            timer+=1\r\n            if timer>10:   #\u539f\u6765\u662f10\r\n                break\r\n\r\nconfusion_matrics=np.zeros([7,7],dtype=\"int\")\r\ntest_accuracy_resultSet=[]\r\nfor j in range(len(test)):\r\n    matrix_row, matrix_col = sess.run(distribution, feed_dict={x: test[j][0], y_: test[j][1]})\r\n    for m, n in zip(matrix_row, matrix_col):\r\n        confusion_matrics[m][n] += 1\r\n    test_accuracy = accuracy.eval(feed_dict={x: test[j][0], y_: test[j][1]})\r\n    test_accuracy_resultSet.append(test_accuracy)\r\ntest_accuracy = np.sum(test_accuracy_resultSet) / len(test_accuracy_resultSet)\r\n\r\nprint(\"Test Accuracy :\",test_accuracy)\r\nprint(np.array(confusion_matrics.tolist()))\r\n\r\n\r\n\r\nThat's the whole code of my CNN, the data for training, validation, test are all organized myself, which is not the problem.\r\nWhile training this CNN, the result in Console showed that all weights and biases did not change. If I replace the RELU in six convolutional layers with Sigmoid, the CNN performed well.\r\nWho can help me fix this problem? Thanks anyway ~\r\n"}