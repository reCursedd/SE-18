{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/367054592", "html_url": "https://github.com/tensorflow/tensorflow/issues/17016#issuecomment-367054592", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17016", "id": 367054592, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzA1NDU5Mg==", "user": {"login": "AakashKumarNain", "id": 11736571, "node_id": "MDQ6VXNlcjExNzM2NTcx", "avatar_url": "https://avatars3.githubusercontent.com/u/11736571?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AakashKumarNain", "html_url": "https://github.com/AakashKumarNain", "followers_url": "https://api.github.com/users/AakashKumarNain/followers", "following_url": "https://api.github.com/users/AakashKumarNain/following{/other_user}", "gists_url": "https://api.github.com/users/AakashKumarNain/gists{/gist_id}", "starred_url": "https://api.github.com/users/AakashKumarNain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AakashKumarNain/subscriptions", "organizations_url": "https://api.github.com/users/AakashKumarNain/orgs", "repos_url": "https://api.github.com/users/AakashKumarNain/repos", "events_url": "https://api.github.com/users/AakashKumarNain/events{/privacy}", "received_events_url": "https://api.github.com/users/AakashKumarNain/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-20T17:31:11Z", "updated_at": "2018-02-20T17:31:11Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> Here is the completer code. For downloading the data, visit <a href=\"https://www.kaggle.com/zalando-research/fashionmnist\" rel=\"nofollow\">this</a> page.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> glob\n<span class=\"pl-k\">from</span> pathlib <span class=\"pl-k\">import</span> Path\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np <span class=\"pl-c\"><span class=\"pl-c\">#</span> linear algebra</span>\n<span class=\"pl-k\">import</span> pandas <span class=\"pl-k\">as</span> pd <span class=\"pl-c\"><span class=\"pl-c\">#</span> data processing, CSV file I/O (e.g. pd.read_csv)</span>\n<span class=\"pl-k\">import</span> matplotlib.pyplot <span class=\"pl-k\">as</span> plt\n<span class=\"pl-k\">import</span> seaborn <span class=\"pl-k\">as</span> sns\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\n<span class=\"pl-k\">from</span> keras.preprocessing <span class=\"pl-k\">import</span> image\n<span class=\"pl-k\">from</span> skimage.io <span class=\"pl-k\">import</span> imread, imsave, imshow\n<span class=\"pl-k\">from</span> keras.utils <span class=\"pl-k\">import</span> to_categorical\n<span class=\"pl-k\">from</span> sklearn.model_selection <span class=\"pl-k\">import</span> train_test_split\n\nnp.random.seed(<span class=\"pl-c1\">111</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Read the train and test csv first</span>\ntrain <span class=\"pl-k\">=</span> pd.read_csv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>../input/fashion-mnist_train.csv<span class=\"pl-pds\">'</span></span>)\ntest <span class=\"pl-k\">=</span> pd.read_csv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>../input/fashion-mnist_test.csv<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Number of training samples: <span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">len</span>(train))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Number of test samples: <span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">len</span>(test))\n\n\nlabels <span class=\"pl-k\">=</span> train[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>label<span class=\"pl-pds\">\"</span></span>]\ntrain <span class=\"pl-k\">=</span> train.drop([<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>label<span class=\"pl-pds\">\"</span></span>], <span class=\"pl-v\">axis</span> <span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Split the training dataset into training and validation sets</span>\nX_train, X_valid, y_train, y_valid <span class=\"pl-k\">=</span> train_test_split(train, labels, <span class=\"pl-v\">test_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.2</span>, <span class=\"pl-v\">random_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">111</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Number of samples in the train set: <span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">len</span>(X_train))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Number of samples in the validation set: <span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">len</span>(X_valid))\n\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Reshape the data</span>\nX_train <span class=\"pl-k\">=</span> np.array(X_train.iloc[:, :]).reshape(<span class=\"pl-c1\">len</span>(X_train),<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">1</span>)\nX_valid <span class=\"pl-k\">=</span> np.array(X_valid.iloc[:, :]).reshape(<span class=\"pl-c1\">len</span>(X_valid), <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">1</span>)\nX_test <span class=\"pl-k\">=</span> np.array(test.iloc[:,<span class=\"pl-c1\">1</span>:]).reshape(<span class=\"pl-c1\">len</span>(test), <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X_train shape: <span class=\"pl-pds\">\"</span></span>, X_train.shape)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X_valid.shape: <span class=\"pl-pds\">\"</span></span>, X_valid.shape)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X_test.shape: <span class=\"pl-pds\">\"</span></span>, X_test.shape)\n\n\nX_train <span class=\"pl-k\">=</span> X_train.astype(np.float32)\nX_valid <span class=\"pl-k\">=</span> X_valid.astype(np.float32)\nX_test <span class=\"pl-k\">=</span> X_test.astype(np.float32)\n\ntrain_mean <span class=\"pl-k\">=</span> X_train.mean()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Mean subtraction from pixels</span>\nX_train <span class=\"pl-k\">-=</span> train_mean\nX_valid <span class=\"pl-k\">-=</span> train_mean\nX_test <span class=\"pl-k\">-=</span> train_mean\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Normalization</span>\nX_train <span class=\"pl-k\">/=</span><span class=\"pl-c1\">255</span>.\nX_valid <span class=\"pl-k\">/=</span><span class=\"pl-c1\">255</span>.\nX_test <span class=\"pl-k\">/=</span><span class=\"pl-c1\">255</span>.\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> One Hot Encoding(OHE)</span>\ny_train <span class=\"pl-k\">=</span> to_categorical(y_train, <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>).astype(np.int8)\ny_valid <span class=\"pl-k\">=</span> to_categorical(y_valid, <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>).astype(np.int8)\n\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> We will define our model now.</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MNIST</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">data_format</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set the input shape according to the availability of GPU </span>\n        <span class=\"pl-k\">if</span> data_format <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-c1\">self</span>._input_shape <span class=\"pl-k\">=</span> [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>]\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">self</span>._input_shape <span class=\"pl-k\">=</span> [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">1</span>]\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Start defining the type of layers that you want in your network</span>\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> tf.layers.Conv2D(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n        <span class=\"pl-c1\">self</span>.maxpool <span class=\"pl-k\">=</span> tf.layers.MaxPooling2D((<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>), (<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> tf.layers.Conv2D(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n        <span class=\"pl-c1\">self</span>.dense1 <span class=\"pl-k\">=</span> tf.layers.Dense(<span class=\"pl-c1\">1024</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu)\n        <span class=\"pl-c1\">self</span>.dropout <span class=\"pl-k\">=</span> tf.layers.Dropout(<span class=\"pl-c1\">0.5</span>)\n        <span class=\"pl-c1\">self</span>.dense2 <span class=\"pl-k\">=</span> tf.layers.Dense(<span class=\"pl-c1\">10</span>)\n        \n        \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Combine the layers to form the architecture</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">predict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">drop</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        x <span class=\"pl-k\">=</span> tf.reshape(inputs, <span class=\"pl-c1\">self</span>._input_shape)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.maxpool(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv2(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.maxpool(x)\n        x <span class=\"pl-k\">=</span> tf.layers.flatten(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.dense1(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.dropout(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>drop) <span class=\"pl-c\"><span class=\"pl-c\">#</span>enable at training and disable at testing</span>\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.dense2(x)\n        <span class=\"pl-k\">return</span> x\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define loss functions</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">targets</span>, <span class=\"pl-smi\">drop</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-k\">return</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>model.predict(inputs, <span class=\"pl-v\">drop</span><span class=\"pl-k\">=</span>drop), <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>targets))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Calculate accuracy</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">compute_accuracy</span>(<span class=\"pl-smi\">predictions</span>, <span class=\"pl-smi\">labels</span>):\n    model_pred <span class=\"pl-k\">=</span> tf.argmax(predictions, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,<span class=\"pl-v\">output_type</span><span class=\"pl-k\">=</span>tf.int64)\n    actual_labels <span class=\"pl-k\">=</span> tf.argmax(labels, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">output_type</span><span class=\"pl-k\">=</span>tf.int64)\n    <span class=\"pl-k\">return</span> tf.reduce_sum(tf.cast(tf.equal(model_pred, actual_labels)),<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">float</span>(predictions.shape[<span class=\"pl-c1\">0</span>].value)        \n\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>A simple generator</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">data_gen</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">8</span>):\n    n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(data)\n    batch_data <span class=\"pl-k\">=</span> np.zeros((batch_size, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n    batch_labels <span class=\"pl-k\">=</span> np.zeros((batch_size,<span class=\"pl-c1\">10</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int8)\n    indices <span class=\"pl-k\">=</span> np.arange(n)\n    i <span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        np.random.shuffle(indices)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(indices,\"\\n\")</span>\n        next_batch <span class=\"pl-k\">=</span> indices[(i<span class=\"pl-k\">*</span>batch_size):(i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">*</span>batch_size]\n        <span class=\"pl-k\">for</span> j, idx <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(next_batch):\n            batch_data[j] <span class=\"pl-k\">=</span> data[idx]\n            batch_labels[j] <span class=\"pl-k\">=</span> labels[idx]\n        \n        <span class=\"pl-k\">yield</span> batch_data, batch_labels\n        i <span class=\"pl-k\">+=</span><span class=\"pl-c1\">1</span>  \n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create generator instance for training and validation </span>\ntrain_data_gen <span class=\"pl-k\">=</span> data_gen(X_train, y_train)\nvalid_data_gen <span class=\"pl-k\">=</span> data_gen(X_valid, y_valid)\n\ndevice <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gpu:0<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> tfe.num_gpus() <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu:0<span class=\"pl-pds\">\"</span></span>\nmodel <span class=\"pl-k\">=</span> MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> tfe.num_gpus() <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_last<span class=\"pl-pds\">'</span></span>)\noptimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\ngrad <span class=\"pl-k\">=</span> tfe.implicit_gradients(loss)\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\ntrain_batches <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(X_train) <span class=\"pl-k\">//</span> batch_size\nvalid_batches <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(X_valid) <span class=\"pl-k\">//</span> batch_size\nnb_epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(nb_epochs):\n    <span class=\"pl-k\">with</span> tf.device(device):\n        <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(train_batches):\n            inputs, targets <span class=\"pl-k\">=</span> <span class=\"pl-c1\">next</span>(train_data_gen)\n            optimizer.apply_gradients(grad(model, inputs, targets))\n            <span class=\"pl-k\">if</span> j <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Step <span class=\"pl-c1\">%d</span>: Loss on training set : <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>(i, loss(model, inputs, targets, <span class=\"pl-v\">drop</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>).numpy()))</pre></div>", "body_text": "@asimshankar Here is the completer code. For downloading the data, visit this page.\nimport os\nimport glob\nfrom pathlib import Path\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nfrom keras.preprocessing import image\nfrom skimage.io import imread, imsave, imshow\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(111)\n\n#Read the train and test csv first\ntrain = pd.read_csv('../input/fashion-mnist_train.csv')\ntest = pd.read_csv('../input/fashion-mnist_test.csv')\nprint(\"Number of training samples: \", len(train))\nprint(\"Number of test samples: \", len(test))\n\n\nlabels = train[\"label\"]\ntrain = train.drop([\"label\"], axis =1)\n\n# Split the training dataset into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(train, labels, test_size=0.2, random_state=111)\nprint(\"Number of samples in the train set: \", len(X_train))\nprint(\"Number of samples in the validation set: \", len(X_valid))\n\n\n\n# Reshape the data\nX_train = np.array(X_train.iloc[:, :]).reshape(len(X_train),28,28,1)\nX_valid = np.array(X_valid.iloc[:, :]).reshape(len(X_valid), 28, 28,1)\nX_test = np.array(test.iloc[:,1:]).reshape(len(test), 28, 28,1)\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_valid.shape: \", X_valid.shape)\nprint(\"X_test.shape: \", X_test.shape)\n\n\nX_train = X_train.astype(np.float32)\nX_valid = X_valid.astype(np.float32)\nX_test = X_test.astype(np.float32)\n\ntrain_mean = X_train.mean()\n\n# Mean subtraction from pixels\nX_train -= train_mean\nX_valid -= train_mean\nX_test -= train_mean\n\n# Normalization\nX_train /=255.\nX_valid /=255.\nX_test /=255.\n\n# One Hot Encoding(OHE)\ny_train = to_categorical(y_train, num_classes=10).astype(np.int8)\ny_valid = to_categorical(y_valid, num_classes=10).astype(np.int8)\n\n\n\n# We will define our model now.\nclass MNIST(object):\n    def __init__(self, data_format):\n        # Set the input shape according to the availability of GPU \n        if data_format == 'channels_first':\n            self._input_shape = [-1, 1, 28, 28]\n        else:\n            self._input_shape = [-1, 28, 28, 1]\n        \n        # Start defining the type of layers that you want in your network\n        self.conv1 = tf.layers.Conv2D(32, 3, activation=tf.nn.relu, padding='same', data_format=data_format)\n        self.maxpool = tf.layers.MaxPooling2D((2,2), (2,2), padding='same', data_format=data_format)\n        self.conv2 = tf.layers.Conv2D(64, 3, activation=tf.nn.relu, padding='same', data_format=data_format)\n        self.dense1 = tf.layers.Dense(1024, activation=tf.nn.relu)\n        self.dropout = tf.layers.Dropout(0.5)\n        self.dense2 = tf.layers.Dense(10)\n        \n        \n    #Combine the layers to form the architecture\n    def predict(self, inputs, drop=False):\n        x = tf.reshape(inputs, self._input_shape)\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.conv2(x)\n        x = self.maxpool(x)\n        x = tf.layers.flatten(x)\n        x = self.dense1(x)\n        x = self.dropout(x, training=drop) #enable at training and disable at testing\n        x = self.dense2(x)\n        return x\n\n# Define loss functions\ndef loss(model, inputs, targets, drop=False):\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model.predict(inputs, drop=drop), labels=targets))\n\n# Calculate accuracy\ndef compute_accuracy(predictions, labels):\n    model_pred = tf.argmax(predictions, axis=1,output_type=tf.int64)\n    actual_labels = tf.argmax(labels, axis=1, output_type=tf.int64)\n    return tf.reduce_sum(tf.cast(tf.equal(model_pred, actual_labels)),dtype=tf.float32) / float(predictions.shape[0].value)        \n\n\n\n#A simple generator\ndef data_gen(data, labels, batch_size=8):\n    n = len(data)\n    batch_data = np.zeros((batch_size, 28, 28, 1), dtype=np.float32)\n    batch_labels = np.zeros((batch_size,10), dtype=np.int8)\n    indices = np.arange(n)\n    i =0\n    while True:\n        np.random.shuffle(indices)\n        #print(indices,\"\\n\")\n        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n        for j, idx in enumerate(next_batch):\n            batch_data[j] = data[idx]\n            batch_labels[j] = labels[idx]\n        \n        yield batch_data, batch_labels\n        i +=1  \n\n# create generator instance for training and validation \ntrain_data_gen = data_gen(X_train, y_train)\nvalid_data_gen = data_gen(X_valid, y_valid)\n\ndevice = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\nmodel = MNIST('channels_first' if tfe.num_gpus() else 'channels_last')\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\ngrad = tfe.implicit_gradients(loss)\n\nbatch_size = 8\ntrain_batches = len(X_train) // batch_size\nvalid_batches = len(X_valid) // batch_size\nnb_epochs = 5\n\nfor i in range(nb_epochs):\n    with tf.device(device):\n        for j in range(train_batches):\n            inputs, targets = next(train_data_gen)\n            optimizer.apply_gradients(grad(model, inputs, targets))\n            if j % 10 == 0:\n                print(\"Step %d: Loss on training set : %f\" %(i, loss(model, inputs, targets, drop=True).numpy()))", "body": "@asimshankar Here is the completer code. For downloading the data, visit [this](https://www.kaggle.com/zalando-research/fashionmnist) page.\r\n\r\n```python\r\nimport os\r\nimport glob\r\nfrom pathlib import Path\r\nimport numpy as np # linear algebra\r\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nfrom keras.preprocessing import image\r\nfrom skimage.io import imread, imsave, imshow\r\nfrom keras.utils import to_categorical\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nnp.random.seed(111)\r\n\r\n#Read the train and test csv first\r\ntrain = pd.read_csv('../input/fashion-mnist_train.csv')\r\ntest = pd.read_csv('../input/fashion-mnist_test.csv')\r\nprint(\"Number of training samples: \", len(train))\r\nprint(\"Number of test samples: \", len(test))\r\n\r\n\r\nlabels = train[\"label\"]\r\ntrain = train.drop([\"label\"], axis =1)\r\n\r\n# Split the training dataset into training and validation sets\r\nX_train, X_valid, y_train, y_valid = train_test_split(train, labels, test_size=0.2, random_state=111)\r\nprint(\"Number of samples in the train set: \", len(X_train))\r\nprint(\"Number of samples in the validation set: \", len(X_valid))\r\n\r\n\r\n\r\n# Reshape the data\r\nX_train = np.array(X_train.iloc[:, :]).reshape(len(X_train),28,28,1)\r\nX_valid = np.array(X_valid.iloc[:, :]).reshape(len(X_valid), 28, 28,1)\r\nX_test = np.array(test.iloc[:,1:]).reshape(len(test), 28, 28,1)\r\nprint(\"X_train shape: \", X_train.shape)\r\nprint(\"X_valid.shape: \", X_valid.shape)\r\nprint(\"X_test.shape: \", X_test.shape)\r\n\r\n\r\nX_train = X_train.astype(np.float32)\r\nX_valid = X_valid.astype(np.float32)\r\nX_test = X_test.astype(np.float32)\r\n\r\ntrain_mean = X_train.mean()\r\n\r\n# Mean subtraction from pixels\r\nX_train -= train_mean\r\nX_valid -= train_mean\r\nX_test -= train_mean\r\n\r\n# Normalization\r\nX_train /=255.\r\nX_valid /=255.\r\nX_test /=255.\r\n\r\n# One Hot Encoding(OHE)\r\ny_train = to_categorical(y_train, num_classes=10).astype(np.int8)\r\ny_valid = to_categorical(y_valid, num_classes=10).astype(np.int8)\r\n\r\n\r\n\r\n# We will define our model now.\r\nclass MNIST(object):\r\n    def __init__(self, data_format):\r\n        # Set the input shape according to the availability of GPU \r\n        if data_format == 'channels_first':\r\n            self._input_shape = [-1, 1, 28, 28]\r\n        else:\r\n            self._input_shape = [-1, 28, 28, 1]\r\n        \r\n        # Start defining the type of layers that you want in your network\r\n        self.conv1 = tf.layers.Conv2D(32, 3, activation=tf.nn.relu, padding='same', data_format=data_format)\r\n        self.maxpool = tf.layers.MaxPooling2D((2,2), (2,2), padding='same', data_format=data_format)\r\n        self.conv2 = tf.layers.Conv2D(64, 3, activation=tf.nn.relu, padding='same', data_format=data_format)\r\n        self.dense1 = tf.layers.Dense(1024, activation=tf.nn.relu)\r\n        self.dropout = tf.layers.Dropout(0.5)\r\n        self.dense2 = tf.layers.Dense(10)\r\n        \r\n        \r\n    #Combine the layers to form the architecture\r\n    def predict(self, inputs, drop=False):\r\n        x = tf.reshape(inputs, self._input_shape)\r\n        x = self.conv1(x)\r\n        x = self.maxpool(x)\r\n        x = self.conv2(x)\r\n        x = self.maxpool(x)\r\n        x = tf.layers.flatten(x)\r\n        x = self.dense1(x)\r\n        x = self.dropout(x, training=drop) #enable at training and disable at testing\r\n        x = self.dense2(x)\r\n        return x\r\n\r\n# Define loss functions\r\ndef loss(model, inputs, targets, drop=False):\r\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model.predict(inputs, drop=drop), labels=targets))\r\n\r\n# Calculate accuracy\r\ndef compute_accuracy(predictions, labels):\r\n    model_pred = tf.argmax(predictions, axis=1,output_type=tf.int64)\r\n    actual_labels = tf.argmax(labels, axis=1, output_type=tf.int64)\r\n    return tf.reduce_sum(tf.cast(tf.equal(model_pred, actual_labels)),dtype=tf.float32) / float(predictions.shape[0].value)        \r\n\r\n\r\n\r\n#A simple generator\r\ndef data_gen(data, labels, batch_size=8):\r\n    n = len(data)\r\n    batch_data = np.zeros((batch_size, 28, 28, 1), dtype=np.float32)\r\n    batch_labels = np.zeros((batch_size,10), dtype=np.int8)\r\n    indices = np.arange(n)\r\n    i =0\r\n    while True:\r\n        np.random.shuffle(indices)\r\n        #print(indices,\"\\n\")\r\n        next_batch = indices[(i*batch_size):(i+1)*batch_size]\r\n        for j, idx in enumerate(next_batch):\r\n            batch_data[j] = data[idx]\r\n            batch_labels[j] = labels[idx]\r\n        \r\n        yield batch_data, batch_labels\r\n        i +=1  \r\n\r\n# create generator instance for training and validation \r\ntrain_data_gen = data_gen(X_train, y_train)\r\nvalid_data_gen = data_gen(X_valid, y_valid)\r\n\r\ndevice = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\r\nmodel = MNIST('channels_first' if tfe.num_gpus() else 'channels_last')\r\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\r\ngrad = tfe.implicit_gradients(loss)\r\n\r\nbatch_size = 8\r\ntrain_batches = len(X_train) // batch_size\r\nvalid_batches = len(X_valid) // batch_size\r\nnb_epochs = 5\r\n\r\nfor i in range(nb_epochs):\r\n    with tf.device(device):\r\n        for j in range(train_batches):\r\n            inputs, targets = next(train_data_gen)\r\n            optimizer.apply_gradients(grad(model, inputs, targets))\r\n            if j % 10 == 0:\r\n                print(\"Step %d: Loss on training set : %f\" %(i, loss(model, inputs, targets, drop=True).numpy()))\r\n```"}