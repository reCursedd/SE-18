{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366416732", "html_url": "https://github.com/tensorflow/tensorflow/issues/17016#issuecomment-366416732", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17016", "id": 366416732, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjQxNjczMg==", "user": {"login": "AakashKumarNain", "id": 11736571, "node_id": "MDQ6VXNlcjExNzM2NTcx", "avatar_url": "https://avatars3.githubusercontent.com/u/11736571?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AakashKumarNain", "html_url": "https://github.com/AakashKumarNain", "followers_url": "https://api.github.com/users/AakashKumarNain/followers", "following_url": "https://api.github.com/users/AakashKumarNain/following{/other_user}", "gists_url": "https://api.github.com/users/AakashKumarNain/gists{/gist_id}", "starred_url": "https://api.github.com/users/AakashKumarNain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AakashKumarNain/subscriptions", "organizations_url": "https://api.github.com/users/AakashKumarNain/orgs", "repos_url": "https://api.github.com/users/AakashKumarNain/repos", "events_url": "https://api.github.com/users/AakashKumarNain/events{/privacy}", "received_events_url": "https://api.github.com/users/AakashKumarNain/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-17T04:53:52Z", "updated_at": "2018-02-17T04:53:52Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=29663194\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cy89\">@cy89</a> I don't know if this helps much but this is all I got. As soon as the first batch of data comes from the generator, I think forward pass works fine but during backprop, something is wrong. Please see the full error log below:</p>\n<pre><code>Incoming data shape: (8, 28, 28, 1) (8, 10)\nWARNING:tensorflow:From &lt;ipython-input-14-22dc84f65208&gt;:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee tf.nn.softmax_cross_entropy_with_logits_v2.\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-16-7bd0e7654c19&gt; in &lt;module&gt;()\n      9             inputs, targets = next(train_data_gen)\n     10             print(\"Incoming data shape:\", inputs.shape, targets.shape)\n---&gt; 11             optimizer.apply_gradients(grad(model, inputs, targets))\n     12             if j % 10 == 0:\n     13                 print(\"Step %d: Loss on training set : %f\" %(i, loss(model, inputs, targets).numpy()))\n\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in grad_fn(*args, **kwds)\n    416   def grad_fn(*args, **kwds):\n    417     \"\"\"Computes the gradient of the wrapped function.\"\"\"\n--&gt; 418     return implicit_val_and_grad(f)(*args, **kwds)[1]\n    419 \n    420   return grad_fn\n\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in grad_fn(*args)\n    361 \n    362     if not sources:\n--&gt; 363       raise ValueError(\"No trainable variables were accessed while the \"\n    364                        \"function was being computed.\")\n    365     grad = imperative_grad.imperative_grad(_default_vspace,\n\nValueError: No trainable variables were accessed while the function was being computed.\n\n\n\n</code></pre>", "body_text": "@cy89 I don't know if this helps much but this is all I got. As soon as the first batch of data comes from the generator, I think forward pass works fine but during backprop, something is wrong. Please see the full error log below:\nIncoming data shape: (8, 28, 28, 1) (8, 10)\nWARNING:tensorflow:From <ipython-input-14-22dc84f65208>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee tf.nn.softmax_cross_entropy_with_logits_v2.\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-16-7bd0e7654c19> in <module>()\n      9             inputs, targets = next(train_data_gen)\n     10             print(\"Incoming data shape:\", inputs.shape, targets.shape)\n---> 11             optimizer.apply_gradients(grad(model, inputs, targets))\n     12             if j % 10 == 0:\n     13                 print(\"Step %d: Loss on training set : %f\" %(i, loss(model, inputs, targets).numpy()))\n\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in grad_fn(*args, **kwds)\n    416   def grad_fn(*args, **kwds):\n    417     \"\"\"Computes the gradient of the wrapped function.\"\"\"\n--> 418     return implicit_val_and_grad(f)(*args, **kwds)[1]\n    419 \n    420   return grad_fn\n\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in grad_fn(*args)\n    361 \n    362     if not sources:\n--> 363       raise ValueError(\"No trainable variables were accessed while the \"\n    364                        \"function was being computed.\")\n    365     grad = imperative_grad.imperative_grad(_default_vspace,\n\nValueError: No trainable variables were accessed while the function was being computed.", "body": "@cy89 I don't know if this helps much but this is all I got. As soon as the first batch of data comes from the generator, I think forward pass works fine but during backprop, something is wrong. Please see the full error log below:\r\n\r\n```\r\nIncoming data shape: (8, 28, 28, 1) (8, 10)\r\nWARNING:tensorflow:From <ipython-input-14-22dc84f65208>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee tf.nn.softmax_cross_entropy_with_logits_v2.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-16-7bd0e7654c19> in <module>()\r\n      9             inputs, targets = next(train_data_gen)\r\n     10             print(\"Incoming data shape:\", inputs.shape, targets.shape)\r\n---> 11             optimizer.apply_gradients(grad(model, inputs, targets))\r\n     12             if j % 10 == 0:\r\n     13                 print(\"Step %d: Loss on training set : %f\" %(i, loss(model, inputs, targets).numpy()))\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in grad_fn(*args, **kwds)\r\n    416   def grad_fn(*args, **kwds):\r\n    417     \"\"\"Computes the gradient of the wrapped function.\"\"\"\r\n--> 418     return implicit_val_and_grad(f)(*args, **kwds)[1]\r\n    419 \r\n    420   return grad_fn\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in grad_fn(*args)\r\n    361 \r\n    362     if not sources:\r\n--> 363       raise ValueError(\"No trainable variables were accessed while the \"\r\n    364                        \"function was being computed.\")\r\n    365     grad = imperative_grad.imperative_grad(_default_vspace,\r\n\r\nValueError: No trainable variables were accessed while the function was being computed.\r\n\r\n\r\n\r\n```"}