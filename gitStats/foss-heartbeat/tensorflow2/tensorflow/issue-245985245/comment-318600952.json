{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318600952", "html_url": "https://github.com/tensorflow/tensorflow/issues/11810#issuecomment-318600952", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11810", "id": 318600952, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODYwMDk1Mg==", "user": {"login": "anjany", "id": 10849307, "node_id": "MDQ6VXNlcjEwODQ5MzA3", "avatar_url": "https://avatars1.githubusercontent.com/u/10849307?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anjany", "html_url": "https://github.com/anjany", "followers_url": "https://api.github.com/users/anjany/followers", "following_url": "https://api.github.com/users/anjany/following{/other_user}", "gists_url": "https://api.github.com/users/anjany/gists{/gist_id}", "starred_url": "https://api.github.com/users/anjany/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anjany/subscriptions", "organizations_url": "https://api.github.com/users/anjany/orgs", "repos_url": "https://api.github.com/users/anjany/repos", "events_url": "https://api.github.com/users/anjany/events{/privacy}", "received_events_url": "https://api.github.com/users/anjany/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-28T09:03:59Z", "updated_at": "2017-07-28T09:06:24Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a> : Here you go! Find the partially shape-defined placeholders (input_pl and mask_pl) toward the end of the code block. In my environment, this block does give me the above mentioned errors.</p>\n<p>Note that the shape of all trainable variables (<code>w</code> and <code>b</code> here) is absolutely defined.</p>\n<pre><code>import tensorflow as tf\n\n### Definition routines ###\n\nconv_filter_size = 3\ndeconv_filter_size = 2\n\ndef _conv_block(x, channels_in, channels_out):\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((conv_filter_size**2 * channels_in)))), tf.float32)\n    with tf.variable_scope('conv'):\n        shape = [conv_filter_size, conv_filter_size, channels_in, channels_out]\n        w = tf.get_variable('weights', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n        b = tf.Variable(tf.zeros([channels_out]), name = 'bias')\n        conv = tf.nn.relu(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b)\n    return conv\n\n\ndef _upconv_block(x, channels_in, channels_out):\n    output_shape = [tf.shape(x)[0], 2*tf.shape(x)[1], 2*tf.shape(x)[2], channels_in]\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((deconv_filter_size**2 * channels_in)))), tf.float32)\n    with tf.variable_scope('deconv'):\n        shape = [deconv_filter_size, deconv_filter_size, channels_in, channels_out]\n        w = tf.get_variable('weights_up', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n        b = tf.Variable(tf.zeros([channels_out]), name='bias', validate_shape=False)\n        upconv = tf.nn.relu(tf.nn.conv2d_transpose(x, w, output_shape, strides=[1,2,2,1], padding='VALID', name='upconv') + b)\n    return upconv\n\ndef inference(image_batch):\n    with tf.variable_scope('contracting'):\n        with tf.variable_scope('step1'):\n            conv_block_1 = _conv_block(image_batch, 1, 32)\n            conv_pool_1 = tf.nn.max_pool(conv_block_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n        with tf.variable_scope('step2'):\n            conv_block_2 = _conv_block(conv_pool_1, 32, 64)\n            conv_pool_2 = tf.nn.max_pool(conv_block_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n        with tf.variable_scope('step3'):\n            conv_block_3 = _conv_block(conv_pool_2, 64, 128)\n    with tf.variable_scope('expanding'):\n        with tf.variable_scope('step2_up'):\n            upconv_block_1 = _upconv_block(conv_block_3, 128, 128)\n        with tf.variable_scope('step1_up'):\n            upconv_block_2 = _upconv_block(upconv_block_1, 128, 128)\n    with tf.variable_scope('scoring'):\n        with tf.variable_scope('conv'):\n            channels_in = 128\n            channels_out = 2 # Binary-class segmentation\n            shape = [1, 1, channels_in, channels_out]\n            stddev = tf.sqrt(2.0 / (conv_filter_size**2 * channels_in))\n            w = tf.get_variable('weights_scoring', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.zeros([channels_out]), name='bias')\n            score = tf.nn.conv2d(upconv_block_2, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b\n            prediction = score\n    return prediction\n\ndef loss(mask, prediction):\n    mask = tf.cast(mask, tf.int32) \n    loss_im = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=mask, logits=prediction, name='cross_entropy_softmax')\n    loss = tf.reduce_mean(loss_im)\n    return loss\n\ndef training(loss, learning_rate):\n    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.99) ### Fails\n    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) ### Fails\n    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) ### Fails\n    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) ### Succeeds!\n    train_op = optimizer.minimize(loss)\n    return train_op\n\n\n### Graph contruction ###\n\nlearning_rate = 1e-6\nwith tf.Graph().as_default():\n    # PLACEHOLDERS\n    input_pl = tf.placeholder(tf.float32, shape=(1, None, None, 1))\n    mask_pl = tf.placeholder(tf.float32, shape=(1, None, None))\n    # OPs\n    prediction_op = inference(input_pl)\n    loss_op = loss(mask_pl, prediction_op)\n    train_op = training(loss_op, learning_rate)\n</code></pre>", "body_text": "@ppwwyyxx : Here you go! Find the partially shape-defined placeholders (input_pl and mask_pl) toward the end of the code block. In my environment, this block does give me the above mentioned errors.\nNote that the shape of all trainable variables (w and b here) is absolutely defined.\nimport tensorflow as tf\n\n### Definition routines ###\n\nconv_filter_size = 3\ndeconv_filter_size = 2\n\ndef _conv_block(x, channels_in, channels_out):\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((conv_filter_size**2 * channels_in)))), tf.float32)\n    with tf.variable_scope('conv'):\n        shape = [conv_filter_size, conv_filter_size, channels_in, channels_out]\n        w = tf.get_variable('weights', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n        b = tf.Variable(tf.zeros([channels_out]), name = 'bias')\n        conv = tf.nn.relu(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b)\n    return conv\n\n\ndef _upconv_block(x, channels_in, channels_out):\n    output_shape = [tf.shape(x)[0], 2*tf.shape(x)[1], 2*tf.shape(x)[2], channels_in]\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((deconv_filter_size**2 * channels_in)))), tf.float32)\n    with tf.variable_scope('deconv'):\n        shape = [deconv_filter_size, deconv_filter_size, channels_in, channels_out]\n        w = tf.get_variable('weights_up', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n        b = tf.Variable(tf.zeros([channels_out]), name='bias', validate_shape=False)\n        upconv = tf.nn.relu(tf.nn.conv2d_transpose(x, w, output_shape, strides=[1,2,2,1], padding='VALID', name='upconv') + b)\n    return upconv\n\ndef inference(image_batch):\n    with tf.variable_scope('contracting'):\n        with tf.variable_scope('step1'):\n            conv_block_1 = _conv_block(image_batch, 1, 32)\n            conv_pool_1 = tf.nn.max_pool(conv_block_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n        with tf.variable_scope('step2'):\n            conv_block_2 = _conv_block(conv_pool_1, 32, 64)\n            conv_pool_2 = tf.nn.max_pool(conv_block_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n        with tf.variable_scope('step3'):\n            conv_block_3 = _conv_block(conv_pool_2, 64, 128)\n    with tf.variable_scope('expanding'):\n        with tf.variable_scope('step2_up'):\n            upconv_block_1 = _upconv_block(conv_block_3, 128, 128)\n        with tf.variable_scope('step1_up'):\n            upconv_block_2 = _upconv_block(upconv_block_1, 128, 128)\n    with tf.variable_scope('scoring'):\n        with tf.variable_scope('conv'):\n            channels_in = 128\n            channels_out = 2 # Binary-class segmentation\n            shape = [1, 1, channels_in, channels_out]\n            stddev = tf.sqrt(2.0 / (conv_filter_size**2 * channels_in))\n            w = tf.get_variable('weights_scoring', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.zeros([channels_out]), name='bias')\n            score = tf.nn.conv2d(upconv_block_2, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b\n            prediction = score\n    return prediction\n\ndef loss(mask, prediction):\n    mask = tf.cast(mask, tf.int32) \n    loss_im = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=mask, logits=prediction, name='cross_entropy_softmax')\n    loss = tf.reduce_mean(loss_im)\n    return loss\n\ndef training(loss, learning_rate):\n    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.99) ### Fails\n    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) ### Fails\n    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) ### Fails\n    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) ### Succeeds!\n    train_op = optimizer.minimize(loss)\n    return train_op\n\n\n### Graph contruction ###\n\nlearning_rate = 1e-6\nwith tf.Graph().as_default():\n    # PLACEHOLDERS\n    input_pl = tf.placeholder(tf.float32, shape=(1, None, None, 1))\n    mask_pl = tf.placeholder(tf.float32, shape=(1, None, None))\n    # OPs\n    prediction_op = inference(input_pl)\n    loss_op = loss(mask_pl, prediction_op)\n    train_op = training(loss_op, learning_rate)", "body": "@ppwwyyxx : Here you go! Find the partially shape-defined placeholders (input_pl and mask_pl) toward the end of the code block. In my environment, this block does give me the above mentioned errors. \r\n\r\nNote that the shape of all trainable variables (`w` and `b` here) is absolutely defined.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n### Definition routines ###\r\n\r\nconv_filter_size = 3\r\ndeconv_filter_size = 2\r\n\r\ndef _conv_block(x, channels_in, channels_out):\r\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((conv_filter_size**2 * channels_in)))), tf.float32)\r\n    with tf.variable_scope('conv'):\r\n        shape = [conv_filter_size, conv_filter_size, channels_in, channels_out]\r\n        w = tf.get_variable('weights', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\r\n        b = tf.Variable(tf.zeros([channels_out]), name = 'bias')\r\n        conv = tf.nn.relu(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b)\r\n    return conv\r\n\r\n\r\ndef _upconv_block(x, channels_in, channels_out):\r\n    output_shape = [tf.shape(x)[0], 2*tf.shape(x)[1], 2*tf.shape(x)[2], channels_in]\r\n    stddev = tf.cast(tf.sqrt(tf.divide(2,((deconv_filter_size**2 * channels_in)))), tf.float32)\r\n    with tf.variable_scope('deconv'):\r\n        shape = [deconv_filter_size, deconv_filter_size, channels_in, channels_out]\r\n        w = tf.get_variable('weights_up', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\r\n        b = tf.Variable(tf.zeros([channels_out]), name='bias', validate_shape=False)\r\n        upconv = tf.nn.relu(tf.nn.conv2d_transpose(x, w, output_shape, strides=[1,2,2,1], padding='VALID', name='upconv') + b)\r\n    return upconv\r\n\r\ndef inference(image_batch):\r\n    with tf.variable_scope('contracting'):\r\n        with tf.variable_scope('step1'):\r\n            conv_block_1 = _conv_block(image_batch, 1, 32)\r\n            conv_pool_1 = tf.nn.max_pool(conv_block_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\r\n        with tf.variable_scope('step2'):\r\n            conv_block_2 = _conv_block(conv_pool_1, 32, 64)\r\n            conv_pool_2 = tf.nn.max_pool(conv_block_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\r\n        with tf.variable_scope('step3'):\r\n            conv_block_3 = _conv_block(conv_pool_2, 64, 128)\r\n    with tf.variable_scope('expanding'):\r\n        with tf.variable_scope('step2_up'):\r\n            upconv_block_1 = _upconv_block(conv_block_3, 128, 128)\r\n        with tf.variable_scope('step1_up'):\r\n            upconv_block_2 = _upconv_block(upconv_block_1, 128, 128)\r\n    with tf.variable_scope('scoring'):\r\n        with tf.variable_scope('conv'):\r\n            channels_in = 128\r\n            channels_out = 2 # Binary-class segmentation\r\n            shape = [1, 1, channels_in, channels_out]\r\n            stddev = tf.sqrt(2.0 / (conv_filter_size**2 * channels_in))\r\n            w = tf.get_variable('weights_scoring', shape=shape, initializer=tf.contrib.layers.xavier_initializer())\r\n            b = tf.Variable(tf.zeros([channels_out]), name='bias')\r\n            score = tf.nn.conv2d(upconv_block_2, w, strides=[1, 1, 1, 1], padding='VALID', name='conv') + b\r\n            prediction = score\r\n    return prediction\r\n\r\ndef loss(mask, prediction):\r\n    mask = tf.cast(mask, tf.int32) \r\n    loss_im = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=mask, logits=prediction, name='cross_entropy_softmax')\r\n    loss = tf.reduce_mean(loss_im)\r\n    return loss\r\n\r\ndef training(loss, learning_rate):\r\n    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.99) ### Fails\r\n    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) ### Fails\r\n    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) ### Fails\r\n    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) ### Succeeds!\r\n    train_op = optimizer.minimize(loss)\r\n    return train_op\r\n\r\n\r\n### Graph contruction ###\r\n\r\nlearning_rate = 1e-6\r\nwith tf.Graph().as_default():\r\n    # PLACEHOLDERS\r\n    input_pl = tf.placeholder(tf.float32, shape=(1, None, None, 1))\r\n    mask_pl = tf.placeholder(tf.float32, shape=(1, None, None))\r\n    # OPs\r\n    prediction_op = inference(input_pl)\r\n    loss_op = loss(mask_pl, prediction_op)\r\n    train_op = training(loss_op, learning_rate)\r\n```"}