{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/382937268", "html_url": "https://github.com/tensorflow/tensorflow/issues/18635#issuecomment-382937268", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18635", "id": 382937268, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjkzNzI2OA==", "user": {"login": "weichiche", "id": 7419189, "node_id": "MDQ6VXNlcjc0MTkxODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/7419189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weichiche", "html_url": "https://github.com/weichiche", "followers_url": "https://api.github.com/users/weichiche/followers", "following_url": "https://api.github.com/users/weichiche/following{/other_user}", "gists_url": "https://api.github.com/users/weichiche/gists{/gist_id}", "starred_url": "https://api.github.com/users/weichiche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weichiche/subscriptions", "organizations_url": "https://api.github.com/users/weichiche/orgs", "repos_url": "https://api.github.com/users/weichiche/repos", "events_url": "https://api.github.com/users/weichiche/events{/privacy}", "received_events_url": "https://api.github.com/users/weichiche/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-20T01:39:52Z", "updated_at": "2018-04-20T02:23:28Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5361725\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Benyuel\">@Benyuel</a>: I said that <em>I tried explicitly making the divisor <code>float32</code></em>. Let's make it more clear. For <strong>Numpy</strong>,</p>\n<div class=\"highlight highlight-source-python\"><pre>np_dtype <span class=\"pl-k\">=</span> np.float32\n\nx <span class=\"pl-k\">=</span> np.array(<span class=\"pl-c1\">247</span>., <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np_dtype)\nz <span class=\"pl-k\">=</span> np.array(<span class=\"pl-c1\">255</span>., <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np_dtype)\ny <span class=\"pl-k\">=</span> x <span class=\"pl-k\">/</span> z\n\n<span class=\"pl-c1\">print</span>(x.dtype, x)\n<span class=\"pl-c1\">print</span>(z.dtype, z)\n<span class=\"pl-c1\">print</span>(y.dtype, <span class=\"pl-c1\">list</span>(struct.pack(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>!f<span class=\"pl-pds\">'</span></span>, y)))</pre></div>\n<p>prints</p>\n<pre><code>float32 247.0\nfloat32 255.0\nfloat32 [63, 119, 247, 248]\n</code></pre>\n<p>And for <strong>TensorFlow</strong>,</p>\n<div class=\"highlight highlight-source-python\"><pre>tf_dtype <span class=\"pl-k\">=</span> tf.float32\n\na <span class=\"pl-k\">=</span> tf.placeholder(tf_dtype)\nc <span class=\"pl-k\">=</span> tf.constant(z, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf_dtype)\nb <span class=\"pl-k\">=</span> a <span class=\"pl-k\">/</span> c\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    _a, _b, _c <span class=\"pl-k\">=</span> sess.run([a, b, c], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{a: x})\n    \n<span class=\"pl-c1\">print</span>(a.dtype.name, _a)\n<span class=\"pl-c1\">print</span>(c.dtype.name, _c)\n<span class=\"pl-c1\">print</span>(b.dtype.name, <span class=\"pl-c1\">list</span>(struct.pack(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>!f<span class=\"pl-pds\">'</span></span>, _b)))</pre></div>\n<p>prints</p>\n<pre><code>float32 247.0\nfloat32 255.0\nfloat32 [63, 119, 247, 249]\n</code></pre>\n<p>The number of printed digits may be inappropriate. But both <code>y</code> and <code>_b</code> are Numpy data, thus they should be represented in the same way. Anyhow, I turn to print the bytes which should reflect the true difference.</p>\n<p>As to your question</p>\n<blockquote>\n<p>... and if so, the question then becomes if tf operations using <code>tf.placeholder</code> should mirror numpy logic or not?</p>\n</blockquote>\n<p>I hope so. As many TensorFlow users also use Numpy a lot, consistency makes less confusion, I suppose.</p>", "body_text": "@Benyuel: I said that I tried explicitly making the divisor float32. Let's make it more clear. For Numpy,\nnp_dtype = np.float32\n\nx = np.array(247., dtype=np_dtype)\nz = np.array(255., dtype=np_dtype)\ny = x / z\n\nprint(x.dtype, x)\nprint(z.dtype, z)\nprint(y.dtype, list(struct.pack('!f', y)))\nprints\nfloat32 247.0\nfloat32 255.0\nfloat32 [63, 119, 247, 248]\n\nAnd for TensorFlow,\ntf_dtype = tf.float32\n\na = tf.placeholder(tf_dtype)\nc = tf.constant(z, dtype=tf_dtype)\nb = a / c\n\nwith tf.Session() as sess:\n    _a, _b, _c = sess.run([a, b, c], feed_dict={a: x})\n    \nprint(a.dtype.name, _a)\nprint(c.dtype.name, _c)\nprint(b.dtype.name, list(struct.pack('!f', _b)))\nprints\nfloat32 247.0\nfloat32 255.0\nfloat32 [63, 119, 247, 249]\n\nThe number of printed digits may be inappropriate. But both y and _b are Numpy data, thus they should be represented in the same way. Anyhow, I turn to print the bytes which should reflect the true difference.\nAs to your question\n\n... and if so, the question then becomes if tf operations using tf.placeholder should mirror numpy logic or not?\n\nI hope so. As many TensorFlow users also use Numpy a lot, consistency makes less confusion, I suppose.", "body": "@Benyuel: I said that _I tried explicitly making the divisor `float32`_. Let's make it more clear. For **Numpy**,\r\n\r\n```python\r\nnp_dtype = np.float32\r\n\r\nx = np.array(247., dtype=np_dtype)\r\nz = np.array(255., dtype=np_dtype)\r\ny = x / z\r\n\r\nprint(x.dtype, x)\r\nprint(z.dtype, z)\r\nprint(y.dtype, list(struct.pack('!f', y)))\r\n```\r\n\r\nprints\r\n\r\n```\r\nfloat32 247.0\r\nfloat32 255.0\r\nfloat32 [63, 119, 247, 248]\r\n```\r\n\r\nAnd for **TensorFlow**,\r\n\r\n```python\r\ntf_dtype = tf.float32\r\n\r\na = tf.placeholder(tf_dtype)\r\nc = tf.constant(z, dtype=tf_dtype)\r\nb = a / c\r\n\r\nwith tf.Session() as sess:\r\n    _a, _b, _c = sess.run([a, b, c], feed_dict={a: x})\r\n    \r\nprint(a.dtype.name, _a)\r\nprint(c.dtype.name, _c)\r\nprint(b.dtype.name, list(struct.pack('!f', _b)))\r\n```\r\n\r\nprints\r\n\r\n```\r\nfloat32 247.0\r\nfloat32 255.0\r\nfloat32 [63, 119, 247, 249]\r\n```\r\n\r\nThe number of printed digits may be inappropriate. But both `y` and `_b` are Numpy data, thus they should be represented in the same way. Anyhow, I turn to print the bytes which should reflect the true difference.\r\n\r\nAs to your question\r\n\r\n> ... and if so, the question then becomes if tf operations using `tf.placeholder` should mirror numpy logic or not?\r\n\r\nI hope so. As many TensorFlow users also use Numpy a lot, consistency makes less confusion, I suppose."}