{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/382422880", "html_url": "https://github.com/tensorflow/tensorflow/issues/18635#issuecomment-382422880", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18635", "id": 382422880, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjQyMjg4MA==", "user": {"login": "Benyuel", "id": 5361725, "node_id": "MDQ6VXNlcjUzNjE3MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5361725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Benyuel", "html_url": "https://github.com/Benyuel", "followers_url": "https://api.github.com/users/Benyuel/followers", "following_url": "https://api.github.com/users/Benyuel/following{/other_user}", "gists_url": "https://api.github.com/users/Benyuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Benyuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Benyuel/subscriptions", "organizations_url": "https://api.github.com/users/Benyuel/orgs", "repos_url": "https://api.github.com/users/Benyuel/repos", "events_url": "https://api.github.com/users/Benyuel/events{/privacy}", "received_events_url": "https://api.github.com/users/Benyuel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-18T15:14:46Z", "updated_at": "2018-04-18T15:14:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7419189\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/weichiche\">@weichiche</a>: I'm not a tensorflow expert, but I suspect you're seeing a difference in the behavior in your example is because of casting between types and precision handled inside tensorflow vs inside numpy.</p>\n<p>Python's standard float type is a C double.  Numpy's standard <code>np.float</code> is a reference to python's float.  Therefore your divisor in <code>y</code>, 255., is actually np.float64.  You've specifically declared x as <code>np.float32</code>.  So because of numpy's casting logic, your resulting y is <code>np.float32 / np.float64 = np.float32</code>.</p>\n<p>In tensorflow, it looks like your <code>a</code> is declared as <code>tf.float32</code> but when tf runs <code>tf.convert_to_tensor</code> on 255., the type is inferred according to tensorflow and <code>255.</code> is treated as a <code>Constant tf.float32</code>.  So your op is <code>tf.float32 / tf.float32 = tf.float32</code></p>\n<p>If I do the following in float64, the tf result matches numpy:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nx = np.array([247.], dtype=np.float64)\ny = x / 255.\nprint('{:12.10f}'.format(y[0]))\n# 0.9686274510\n\na = tf.placeholder(tf.float64)\nb = a / 255.\nwith tf.Session() as sess:\n    _b = sess.run(b, feed_dict={a: x})\nprint('{:12.10f}'.format(_b[0]))\n# 0.9686274510\n</code></pre>\n<p>For <code>tf.placeholder</code> and using <code>feed_dict</code>, If the feed_dict key is a tf.Tensor, the value may be a Python scalar, string, list, or numpy ndarray that can be converted to the same dtype as that tensor.</p>", "body_text": "@weichiche: I'm not a tensorflow expert, but I suspect you're seeing a difference in the behavior in your example is because of casting between types and precision handled inside tensorflow vs inside numpy.\nPython's standard float type is a C double.  Numpy's standard np.float is a reference to python's float.  Therefore your divisor in y, 255., is actually np.float64.  You've specifically declared x as np.float32.  So because of numpy's casting logic, your resulting y is np.float32 / np.float64 = np.float32.\nIn tensorflow, it looks like your a is declared as tf.float32 but when tf runs tf.convert_to_tensor on 255., the type is inferred according to tensorflow and 255. is treated as a Constant tf.float32.  So your op is tf.float32 / tf.float32 = tf.float32\nIf I do the following in float64, the tf result matches numpy:\nimport numpy as np\nimport tensorflow as tf\n\nx = np.array([247.], dtype=np.float64)\ny = x / 255.\nprint('{:12.10f}'.format(y[0]))\n# 0.9686274510\n\na = tf.placeholder(tf.float64)\nb = a / 255.\nwith tf.Session() as sess:\n    _b = sess.run(b, feed_dict={a: x})\nprint('{:12.10f}'.format(_b[0]))\n# 0.9686274510\n\nFor tf.placeholder and using feed_dict, If the feed_dict key is a tf.Tensor, the value may be a Python scalar, string, list, or numpy ndarray that can be converted to the same dtype as that tensor.", "body": "@weichiche: I'm not a tensorflow expert, but I suspect you're seeing a difference in the behavior in your example is because of casting between types and precision handled inside tensorflow vs inside numpy. \r\n\r\nPython's standard float type is a C double.  Numpy's standard `np.float` is a reference to python's float.  Therefore your divisor in `y`, 255., is actually np.float64.  You've specifically declared x as `np.float32`.  So because of numpy's casting logic, your resulting y is `np.float32 / np.float64 = np.float32`.  \r\n\r\nIn tensorflow, it looks like your `a` is declared as `tf.float32` but when tf runs `tf.convert_to_tensor` on 255., the type is inferred according to tensorflow and `255.` is treated as a `Constant tf.float32`.  So your op is `tf.float32 / tf.float32 = tf.float32`\r\n\r\n If I do the following in float64, the tf result matches numpy:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.array([247.], dtype=np.float64)\r\ny = x / 255.\r\nprint('{:12.10f}'.format(y[0]))\r\n# 0.9686274510\r\n\r\na = tf.placeholder(tf.float64)\r\nb = a / 255.\r\nwith tf.Session() as sess:\r\n    _b = sess.run(b, feed_dict={a: x})\r\nprint('{:12.10f}'.format(_b[0]))\r\n# 0.9686274510\r\n```\r\n\r\nFor `tf.placeholder` and using `feed_dict`, If the feed_dict key is a tf.Tensor, the value may be a Python scalar, string, list, or numpy ndarray that can be converted to the same dtype as that tensor.  \r\n\r\n\r\n"}