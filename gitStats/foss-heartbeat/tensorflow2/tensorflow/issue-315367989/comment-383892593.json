{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/383892593", "html_url": "https://github.com/tensorflow/tensorflow/issues/18635#issuecomment-383892593", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18635", "id": 383892593, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Mzg5MjU5Mw==", "user": {"login": "Benyuel", "id": 5361725, "node_id": "MDQ6VXNlcjUzNjE3MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5361725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Benyuel", "html_url": "https://github.com/Benyuel", "followers_url": "https://api.github.com/users/Benyuel/followers", "following_url": "https://api.github.com/users/Benyuel/following{/other_user}", "gists_url": "https://api.github.com/users/Benyuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Benyuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Benyuel/subscriptions", "organizations_url": "https://api.github.com/users/Benyuel/orgs", "repos_url": "https://api.github.com/users/Benyuel/repos", "events_url": "https://api.github.com/users/Benyuel/events{/privacy}", "received_events_url": "https://api.github.com/users/Benyuel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-24T11:08:02Z", "updated_at": "2018-04-24T11:08:58Z", "author_association": "NONE", "body_html": "<p>Correct, I'm not saying that device matters.</p>\n<p>I think I've narrowed it down to one of 2 or both possible causes, not 100% certain though:</p>\n<ol>\n<li>How type conversion is handled</li>\n<li>How a constant tensor is embedded into the graph def which means it is stored in the client and in the runtime.  Placeholders values are as you would expect not stored in the graph def.</li>\n</ol>\n<p>Sorry for all of these snippets, but here's another one.</p>\n<pre><code>a = 247.\nb = 255.\nx = np.array([247.], dtype=np.float32)\ny = x / b\n# x.dtype = float32\n# x.tobytes() = b'\\x00\\x00wC'\n# np.array(b).dtype = float64\n# np.array(b).tobytes() = b'\\x00\\x00\\x00\\x00\\x00\\xe0o@'\n# y = 0.9686274529\n</code></pre>\n<pre><code>a_placeholder_32 = tf.placeholder(tf.float32)\nc = a_placeholder_32 / b\n# b is treated as a tf.constant by default\nsess = tf.Session()\n_c  = sess.run(c,  feed_dict={a_placeholder_32:x})\n# numerator = float32\n# denominator = float64\nprint(sess.graph_def.node[0].attr['dtype'])\nprint(sess.graph_def.node[1].attr['dtype'])\n# numerator &amp; denominator in graph_def = DT_FLOAT\n# np.array(b).tobytes() = b'\\x00\\x00\\x00\\x00\\x00\\xe0o@'\n# _c =  0.9686275125\n</code></pre>\n<pre><code>a_placeholder_32 = tf.placeholder(tf.float32)\nb_placeholder_32 = tf.placeholder(tf.float32)\nc = a_placeholder_32 / b_placeholder_32\nsess = tf.Session()\n_b_placeholder_32 = sess.run(b_placeholder_32, feed_dict={b_placeholder_32:b})\n_c  = sess.run(c,  feed_dict={a_placeholder_32:x, b_placeholder_32:b})\n# numerator = float32\n# denominator = float32\nprint(sess.graph_def.node[0].attr['dtype'])\nprint(sess.graph_def.node[1].attr['dtype'])\n# numerator &amp; denominator in graph_def = DT_FLOAT\nprint(_b_placeholder_32.tobytes())\n# b'\\x00\\x00\\x7fc'\n_c =  0.9686274529\n</code></pre>\n<p>Note the difference in bytes in the denominators.<br>\nNote the difference in denominator dtypes.</p>", "body_text": "Correct, I'm not saying that device matters.\nI think I've narrowed it down to one of 2 or both possible causes, not 100% certain though:\n\nHow type conversion is handled\nHow a constant tensor is embedded into the graph def which means it is stored in the client and in the runtime.  Placeholders values are as you would expect not stored in the graph def.\n\nSorry for all of these snippets, but here's another one.\na = 247.\nb = 255.\nx = np.array([247.], dtype=np.float32)\ny = x / b\n# x.dtype = float32\n# x.tobytes() = b'\\x00\\x00wC'\n# np.array(b).dtype = float64\n# np.array(b).tobytes() = b'\\x00\\x00\\x00\\x00\\x00\\xe0o@'\n# y = 0.9686274529\n\na_placeholder_32 = tf.placeholder(tf.float32)\nc = a_placeholder_32 / b\n# b is treated as a tf.constant by default\nsess = tf.Session()\n_c  = sess.run(c,  feed_dict={a_placeholder_32:x})\n# numerator = float32\n# denominator = float64\nprint(sess.graph_def.node[0].attr['dtype'])\nprint(sess.graph_def.node[1].attr['dtype'])\n# numerator & denominator in graph_def = DT_FLOAT\n# np.array(b).tobytes() = b'\\x00\\x00\\x00\\x00\\x00\\xe0o@'\n# _c =  0.9686275125\n\na_placeholder_32 = tf.placeholder(tf.float32)\nb_placeholder_32 = tf.placeholder(tf.float32)\nc = a_placeholder_32 / b_placeholder_32\nsess = tf.Session()\n_b_placeholder_32 = sess.run(b_placeholder_32, feed_dict={b_placeholder_32:b})\n_c  = sess.run(c,  feed_dict={a_placeholder_32:x, b_placeholder_32:b})\n# numerator = float32\n# denominator = float32\nprint(sess.graph_def.node[0].attr['dtype'])\nprint(sess.graph_def.node[1].attr['dtype'])\n# numerator & denominator in graph_def = DT_FLOAT\nprint(_b_placeholder_32.tobytes())\n# b'\\x00\\x00\\x7fc'\n_c =  0.9686274529\n\nNote the difference in bytes in the denominators.\nNote the difference in denominator dtypes.", "body": "Correct, I'm not saying that device matters.\r\n\r\nI think I've narrowed it down to one of 2 or both possible causes, not 100% certain though:\r\n1.  How type conversion is handled\r\n2.  How a constant tensor is embedded into the graph def which means it is stored in the client and in the runtime.  Placeholders values are as you would expect not stored in the graph def. \r\n\r\nSorry for all of these snippets, but here's another one.\r\n```\r\na = 247.\r\nb = 255.\r\nx = np.array([247.], dtype=np.float32)\r\ny = x / b\r\n# x.dtype = float32\r\n# x.tobytes() = b'\\x00\\x00wC'\r\n# np.array(b).dtype = float64\r\n# np.array(b).tobytes() = b'\\x00\\x00\\x00\\x00\\x00\\xe0o@'\r\n# y = 0.9686274529\r\n```\r\n\r\n```\r\na_placeholder_32 = tf.placeholder(tf.float32)\r\nc = a_placeholder_32 / b\r\n# b is treated as a tf.constant by default\r\nsess = tf.Session()\r\n_c  = sess.run(c,  feed_dict={a_placeholder_32:x})\r\n# numerator = float32\r\n# denominator = float64\r\nprint(sess.graph_def.node[0].attr['dtype'])\r\nprint(sess.graph_def.node[1].attr['dtype'])\r\n# numerator & denominator in graph_def = DT_FLOAT\r\n# np.array(b).tobytes() = b'\\x00\\x00\\x00\\x00\\x00\\xe0o@'\r\n# _c =  0.9686275125\r\n```\r\n\r\n```\r\na_placeholder_32 = tf.placeholder(tf.float32)\r\nb_placeholder_32 = tf.placeholder(tf.float32)\r\nc = a_placeholder_32 / b_placeholder_32\r\nsess = tf.Session()\r\n_b_placeholder_32 = sess.run(b_placeholder_32, feed_dict={b_placeholder_32:b})\r\n_c  = sess.run(c,  feed_dict={a_placeholder_32:x, b_placeholder_32:b})\r\n# numerator = float32\r\n# denominator = float32\r\nprint(sess.graph_def.node[0].attr['dtype'])\r\nprint(sess.graph_def.node[1].attr['dtype'])\r\n# numerator & denominator in graph_def = DT_FLOAT\r\nprint(_b_placeholder_32.tobytes())\r\n# b'\\x00\\x00\\x7fc'\r\n_c =  0.9686274529\r\n```\r\n\r\nNote the difference in bytes in the denominators.\r\nNote the difference in denominator dtypes."}