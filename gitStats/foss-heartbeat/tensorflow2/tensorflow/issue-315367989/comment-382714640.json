{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/382714640", "html_url": "https://github.com/tensorflow/tensorflow/issues/18635#issuecomment-382714640", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18635", "id": 382714640, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjcxNDY0MA==", "user": {"login": "Benyuel", "id": 5361725, "node_id": "MDQ6VXNlcjUzNjE3MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5361725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Benyuel", "html_url": "https://github.com/Benyuel", "followers_url": "https://api.github.com/users/Benyuel/followers", "following_url": "https://api.github.com/users/Benyuel/following{/other_user}", "gists_url": "https://api.github.com/users/Benyuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Benyuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Benyuel/subscriptions", "organizations_url": "https://api.github.com/users/Benyuel/orgs", "repos_url": "https://api.github.com/users/Benyuel/repos", "events_url": "https://api.github.com/users/Benyuel/events{/privacy}", "received_events_url": "https://api.github.com/users/Benyuel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-19T12:26:24Z", "updated_at": "2018-04-19T12:26:24Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7419189\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/weichiche\">@weichiche</a>:    I suspect that tensorflow core is doing the division or casting dtypes differently, and if so, the question then becomes if tf operations using <code>tf.placeholder</code> should mirror numpy logic or not?<br>\nWith a constant, the value is a part of the computation graph itself, specified when the constant is created. With a placeholder, every time you run the computation graph, you can feed in a different value in your feed_dict, and tf core has to do some checking/casting.</p>\n<pre><code>import numpy as np\nnp.can_cast(np.float32, np.float64)\n# True\nnp.can_cast(np.float64, np.float32)\n# False\n</code></pre>\n<p>Internal numpy casting logic: <code>np.float64</code> cannot be automatically cast to <code>np.float32</code></p>\n<pre><code># Returns the data type with the smallest size and smallest scalar kind to which both type1 and type2 may be safely cast. The returned data type is always in native byte order.\n# Per my previous comment, x = np.float32, y = np.float64\nnp.promote_types(np.float32, np.float64)\n# dtype('float64')\n</code></pre>\n<p>Therefore, numpy core is actually performing <code>np.float64 / np.float64</code> and then casting to <code>np.float32</code>.</p>\n<p>Also, you're printing 10 digits, so you're likely within the rounding error (Single precision (float32) ~= 7 decimal digits, Double precision(float64) ~= 16 decimal digits).  Instead of printing and checking equality for floating point, you probably just check <code>np.allclose</code>.</p>", "body_text": "@weichiche:    I suspect that tensorflow core is doing the division or casting dtypes differently, and if so, the question then becomes if tf operations using tf.placeholder should mirror numpy logic or not?\nWith a constant, the value is a part of the computation graph itself, specified when the constant is created. With a placeholder, every time you run the computation graph, you can feed in a different value in your feed_dict, and tf core has to do some checking/casting.\nimport numpy as np\nnp.can_cast(np.float32, np.float64)\n# True\nnp.can_cast(np.float64, np.float32)\n# False\n\nInternal numpy casting logic: np.float64 cannot be automatically cast to np.float32\n# Returns the data type with the smallest size and smallest scalar kind to which both type1 and type2 may be safely cast. The returned data type is always in native byte order.\n# Per my previous comment, x = np.float32, y = np.float64\nnp.promote_types(np.float32, np.float64)\n# dtype('float64')\n\nTherefore, numpy core is actually performing np.float64 / np.float64 and then casting to np.float32.\nAlso, you're printing 10 digits, so you're likely within the rounding error (Single precision (float32) ~= 7 decimal digits, Double precision(float64) ~= 16 decimal digits).  Instead of printing and checking equality for floating point, you probably just check np.allclose.", "body": "@weichiche:    I suspect that tensorflow core is doing the division or casting dtypes differently, and if so, the question then becomes if tf operations using `tf.placeholder` should mirror numpy logic or not?   \r\nWith a constant, the value is a part of the computation graph itself, specified when the constant is created. With a placeholder, every time you run the computation graph, you can feed in a different value in your feed_dict, and tf core has to do some checking/casting.\r\n\r\n```\r\nimport numpy as np\r\nnp.can_cast(np.float32, np.float64)\r\n# True\r\nnp.can_cast(np.float64, np.float32)\r\n# False\r\n```\r\nInternal numpy casting logic: `np.float64` cannot be automatically cast to `np.float32`\r\n```\r\n# Returns the data type with the smallest size and smallest scalar kind to which both type1 and type2 may be safely cast. The returned data type is always in native byte order.\r\n# Per my previous comment, x = np.float32, y = np.float64\r\nnp.promote_types(np.float32, np.float64)\r\n# dtype('float64')\r\n```\r\nTherefore, numpy core is actually performing `np.float64 / np.float64` and then casting to `np.float32`.\r\n\r\nAlso, you're printing 10 digits, so you're likely within the rounding error (Single precision (float32) ~= 7 decimal digits, Double precision(float64) ~= 16 decimal digits).  Instead of printing and checking equality for floating point, you probably just check `np.allclose`.  "}