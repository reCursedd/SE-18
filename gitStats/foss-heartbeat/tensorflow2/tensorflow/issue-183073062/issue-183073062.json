{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4968", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4968/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4968/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4968/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4968", "id": 183073062, "node_id": "MDU6SXNzdWUxODMwNzMwNjI=", "number": 4968, "title": "TensorFlow produces arbitrary results without error when using large amounts of GPU memory", "user": {"login": "jonasrauber", "id": 5837385, "node_id": "MDQ6VXNlcjU4MzczODU=", "avatar_url": "https://avatars1.githubusercontent.com/u/5837385?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jonasrauber", "html_url": "https://github.com/jonasrauber", "followers_url": "https://api.github.com/users/jonasrauber/followers", "following_url": "https://api.github.com/users/jonasrauber/following{/other_user}", "gists_url": "https://api.github.com/users/jonasrauber/gists{/gist_id}", "starred_url": "https://api.github.com/users/jonasrauber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jonasrauber/subscriptions", "organizations_url": "https://api.github.com/users/jonasrauber/orgs", "repos_url": "https://api.github.com/users/jonasrauber/repos", "events_url": "https://api.github.com/users/jonasrauber/events{/privacy}", "received_events_url": "https://api.github.com/users/jonasrauber/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-10-14T15:01:10Z", "updated_at": "2018-05-11T10:33:02Z", "closed_at": "2016-10-31T20:02:41Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Docker container based on <code>nvidia/cuda:7.5-cudnn5-devel</code> running on CentOS</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed: <code>pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl</code></li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0rc0\n</code></pre>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nmb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nsz <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(mb <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1e6</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">4</span>)\ntotal_gb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">11</span>\nn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(total_gb <span class=\"pl-k\">/</span> mb <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>)\nx <span class=\"pl-k\">=</span> np.array(np.random.randn(sz), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    place <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[sz])\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n        x_gpu <span class=\"pl-k\">=</span> [tf.Variable(place, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n)]\n    init <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(init, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{place: x})\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> I know it's not optimal, but to make it more comparable to the Theano code below</span>\n        x_out <span class=\"pl-k\">=</span> [sess.run(v) <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> x_gpu]\n\ndelta <span class=\"pl-k\">=</span> [np.sum(np.abs(xi <span class=\"pl-k\">-</span> x)) <span class=\"pl-k\">for</span> xi <span class=\"pl-k\">in</span> x_out]\n<span class=\"pl-c1\">print</span>(delta)</pre></div>\n<p>Executed using <code>python3 memcheck_tensorflow.py</code></p>\n<h4>Expected output:</h4>\n<pre><code>some tensorflow output ...\n\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n</code></pre>\n<h4>Actual output:</h4>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:81:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K40m, pci bus id: 0000:81:00.0)\n[28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28212806.0, 28209880.0, 28209880.0, 28208760.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28206538.0, 28206538.0, 28209880.0, 28208760.0, 28212806.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28207726.0, 28208760.0, 28206538.0, 28209880.0, 28206538.0, 28210806.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209880.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28206602.0, 28209472.0, 28209472.0, 28208760.0, 28208760.0, 28209880.0, 28208760.0, 28207726.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28206602.0, 28209880.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28208760.0, 28206538.0, 28209880.0, 28209880.0, 28208760.0, 28208760.0, 28206602.0, 28207726.0, 28209880.0, 28207726.0, 28209880.0, 28209880.0, 28209880.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28206538.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28210806.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28209880.0, 28206602.0, 28208760.0, 28208760.0]\n</code></pre>\n<h3>What other attempted solutions have you tried?</h3>\n<p>I've verified that this problem exists on at least one other GPU (I've seen it on Nvidia GPUs K40m and K80).</p>\n<p>I've run similar code in Theano without any problems (same GPU, same Docker container):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> theano\n\nmb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nsz <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(mb <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1e6</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">4</span>)\ntotal_gb <span class=\"pl-k\">=</span> <span class=\"pl-c1\">11</span>\nn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(total_gb <span class=\"pl-k\">/</span> mb <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>)\nx <span class=\"pl-k\">=</span> np.array(np.random.randn(sz), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\nx_gpu <span class=\"pl-k\">=</span> [theano.shared(x) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n)]\nx_out <span class=\"pl-k\">=</span> [v.get_value() <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> x_gpu]\n\ndelta <span class=\"pl-k\">=</span> [np.sum(np.abs(xi <span class=\"pl-k\">-</span> x)) <span class=\"pl-k\">for</span> xi <span class=\"pl-k\">in</span> x_out]\n<span class=\"pl-c1\">print</span>(delta)</pre></div>\n<p>Executed using <code>THEANO_FLAGS=device=gpu,floatX=float32 python3 memcheck_theano.py</code></p>\n<h4>Output</h4>\n<pre><code>Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5103)\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n</code></pre>", "body_text": "Environment info\nOperating System: Docker container based on nvidia/cuda:7.5-cudnn5-devel running on CentOS\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0rc0\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport numpy as np\nimport tensorflow as tf\n\nmb = 100\nsz = int(mb * 1e6 / 4)\ntotal_gb = 11\nn = int(total_gb / mb * 1000)\nx = np.array(np.random.randn(sz), dtype=np.float32)\n\nwith tf.Graph().as_default():\n    place = tf.placeholder(tf.float32, shape=[sz])\n    with tf.device('/gpu:0'):\n        x_gpu = [tf.Variable(place, dtype=tf.float32) for i in range(n)]\n    init = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init, feed_dict={place: x})\n        # I know it's not optimal, but to make it more comparable to the Theano code below\n        x_out = [sess.run(v) for v in x_gpu]\n\ndelta = [np.sum(np.abs(xi - x)) for xi in x_out]\nprint(delta)\nExecuted using python3 memcheck_tensorflow.py\nExpected output:\nsome tensorflow output ...\n\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\nActual output:\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:81:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:81:00.0)\n[28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28212806.0, 28209880.0, 28209880.0, 28208760.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28206538.0, 28206538.0, 28209880.0, 28208760.0, 28212806.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28207726.0, 28208760.0, 28206538.0, 28209880.0, 28206538.0, 28210806.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209880.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28206602.0, 28209472.0, 28209472.0, 28208760.0, 28208760.0, 28209880.0, 28208760.0, 28207726.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28206602.0, 28209880.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28208760.0, 28206538.0, 28209880.0, 28209880.0, 28208760.0, 28208760.0, 28206602.0, 28207726.0, 28209880.0, 28207726.0, 28209880.0, 28209880.0, 28209880.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28206538.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28210806.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28209880.0, 28206602.0, 28208760.0, 28208760.0]\n\nWhat other attempted solutions have you tried?\nI've verified that this problem exists on at least one other GPU (I've seen it on Nvidia GPUs K40m and K80).\nI've run similar code in Theano without any problems (same GPU, same Docker container):\nimport numpy as np\nimport theano\n\nmb = 100\nsz = int(mb * 1e6 / 4)\ntotal_gb = 11\nn = int(total_gb / mb * 1000)\nx = np.array(np.random.randn(sz), dtype=np.float32)\n\nx_gpu = [theano.shared(x) for i in range(n)]\nx_out = [v.get_value() for v in x_gpu]\n\ndelta = [np.sum(np.abs(xi - x)) for xi in x_out]\nprint(delta)\nExecuted using THEANO_FLAGS=device=gpu,floatX=float32 python3 memcheck_theano.py\nOutput\nUsing gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5103)\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "body": "### Environment info\n\nOperating System: Docker container based on `nvidia/cuda:7.5-cudnn5-devel` running on CentOS\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed: `pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl`\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0rc0\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nmb = 100\nsz = int(mb * 1e6 / 4)\ntotal_gb = 11\nn = int(total_gb / mb * 1000)\nx = np.array(np.random.randn(sz), dtype=np.float32)\n\nwith tf.Graph().as_default():\n    place = tf.placeholder(tf.float32, shape=[sz])\n    with tf.device('/gpu:0'):\n        x_gpu = [tf.Variable(place, dtype=tf.float32) for i in range(n)]\n    init = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init, feed_dict={place: x})\n        # I know it's not optimal, but to make it more comparable to the Theano code below\n        x_out = [sess.run(v) for v in x_gpu]\n\ndelta = [np.sum(np.abs(xi - x)) for xi in x_out]\nprint(delta)\n```\n\nExecuted using `python3 memcheck_tensorflow.py`\n#### Expected output:\n\n```\nsome tensorflow output ...\n\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n```\n#### Actual output:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:81:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:81:00.0)\n[28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28212806.0, 28209880.0, 28209880.0, 28208760.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28206538.0, 28206538.0, 28209880.0, 28208760.0, 28212806.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28207726.0, 28208760.0, 28206538.0, 28209880.0, 28206538.0, 28210806.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209880.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28209472.0, 28206602.0, 28209472.0, 28209472.0, 28208760.0, 28208760.0, 28209880.0, 28208760.0, 28207726.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28206602.0, 28209880.0, 28209880.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28208760.0, 28206538.0, 28209880.0, 28209880.0, 28208760.0, 28208760.0, 28206602.0, 28207726.0, 28209880.0, 28207726.0, 28209880.0, 28209880.0, 28209880.0, 28211926.0, 28209880.0, 28208760.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28206538.0, 28209880.0, 28209880.0, 28209880.0, 28209880.0, 28210806.0, 28209880.0, 28208760.0, 28208760.0, 28209880.0, 28206538.0, 28208760.0, 28209880.0, 28206602.0, 28208760.0, 28208760.0]\n```\n### What other attempted solutions have you tried?\n\nI've verified that this problem exists on at least one other GPU (I've seen it on Nvidia GPUs K40m and K80).\n\nI've run similar code in Theano without any problems (same GPU, same Docker container):\n\n``` python\nimport numpy as np\nimport theano\n\nmb = 100\nsz = int(mb * 1e6 / 4)\ntotal_gb = 11\nn = int(total_gb / mb * 1000)\nx = np.array(np.random.randn(sz), dtype=np.float32)\n\nx_gpu = [theano.shared(x) for i in range(n)]\nx_out = [v.get_value() for v in x_gpu]\n\ndelta = [np.sum(np.abs(xi - x)) for xi in x_out]\nprint(delta)\n```\n\nExecuted using `THEANO_FLAGS=device=gpu,floatX=float32 python3 memcheck_theano.py`\n#### Output\n\n```\nUsing gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5103)\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n```\n"}