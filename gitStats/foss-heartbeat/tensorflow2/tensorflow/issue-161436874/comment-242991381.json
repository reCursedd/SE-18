{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242991381", "html_url": "https://github.com/tensorflow/tensorflow/issues/2973#issuecomment-242991381", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2973", "id": 242991381, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mjk5MTM4MQ==", "user": {"login": "rwightman", "id": 5702664, "node_id": "MDQ6VXNlcjU3MDI2NjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702664?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rwightman", "html_url": "https://github.com/rwightman", "followers_url": "https://api.github.com/users/rwightman/followers", "following_url": "https://api.github.com/users/rwightman/following{/other_user}", "gists_url": "https://api.github.com/users/rwightman/gists{/gist_id}", "starred_url": "https://api.github.com/users/rwightman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rwightman/subscriptions", "organizations_url": "https://api.github.com/users/rwightman/orgs", "repos_url": "https://api.github.com/users/rwightman/repos", "events_url": "https://api.github.com/users/rwightman/events{/privacy}", "received_events_url": "https://api.github.com/users/rwightman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-28T18:38:29Z", "updated_at": "2016-08-28T18:38:29Z", "author_association": "NONE", "body_html": "<p>I was going to create a separate issue for this but seems like my issue could be addressed by a fix for this one. I'm having the same issue as @jared-mess. I'm currently training a network on one machine, and want to run validation on another with the train folder mapped over network. Unfortunately the mappings on the machines result in different absolute paths so checkpoint restore fails as in jared's example.</p>\n<p>The latest_checkpoint ('checkpoint') file stores absolute paths when the save path is absolute and relative when the save path is relative as per <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L675\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L675</a></p>\n<p>When the path is relative in the above code, it re-writes as relative to the save dir. Why isn't the path always be re-written as relative to the save dir, even for the absolute case? Is there a use case I'm missing where it makes any sense to have the paths saved in 'checkpoint' as absolute?</p>\n<p>If there is a use case for absolute checkpoint paths in the latest_checkpoint file, would it make sense to have a 'force_relative_paths' flag on the saver to force absolute paths to be re-written as relative?</p>\n<p>My current ugly hack for the issue is to read and re-write the 'checkpoint' file with a different script on the training machine and strip out the absolute portion of the paths. This works.</p>", "body_text": "I was going to create a separate issue for this but seems like my issue could be addressed by a fix for this one. I'm having the same issue as @jared-mess. I'm currently training a network on one machine, and want to run validation on another with the train folder mapped over network. Unfortunately the mappings on the machines result in different absolute paths so checkpoint restore fails as in jared's example.\nThe latest_checkpoint ('checkpoint') file stores absolute paths when the save path is absolute and relative when the save path is relative as per https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L675\nWhen the path is relative in the above code, it re-writes as relative to the save dir. Why isn't the path always be re-written as relative to the save dir, even for the absolute case? Is there a use case I'm missing where it makes any sense to have the paths saved in 'checkpoint' as absolute?\nIf there is a use case for absolute checkpoint paths in the latest_checkpoint file, would it make sense to have a 'force_relative_paths' flag on the saver to force absolute paths to be re-written as relative?\nMy current ugly hack for the issue is to read and re-write the 'checkpoint' file with a different script on the training machine and strip out the absolute portion of the paths. This works.", "body": "I was going to create a separate issue for this but seems like my issue could be addressed by a fix for this one. I'm having the same issue as @jared-mess. I'm currently training a network on one machine, and want to run validation on another with the train folder mapped over network. Unfortunately the mappings on the machines result in different absolute paths so checkpoint restore fails as in jared's example. \n\nThe latest_checkpoint ('checkpoint') file stores absolute paths when the save path is absolute and relative when the save path is relative as per https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L675\n\nWhen the path is relative in the above code, it re-writes as relative to the save dir. Why isn't the path always be re-written as relative to the save dir, even for the absolute case? Is there a use case I'm missing where it makes any sense to have the paths saved in 'checkpoint' as absolute? \n\nIf there is a use case for absolute checkpoint paths in the latest_checkpoint file, would it make sense to have a 'force_relative_paths' flag on the saver to force absolute paths to be re-written as relative? \n\nMy current ugly hack for the issue is to read and re-write the 'checkpoint' file with a different script on the training machine and strip out the absolute portion of the paths. This works.\n"}