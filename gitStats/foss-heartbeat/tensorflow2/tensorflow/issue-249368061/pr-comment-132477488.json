{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/132477488", "pull_request_review_id": 55562088, "id": 132477488, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMjQ3NzQ4OA==", "diff_hunk": "@@ -51,14 +51,18 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n   return Status::OK();\n }\n \n+#if GOOGLE_CUDA", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 4, "commit_id": "506f1b6b6bf83da4557502898f918eafb24dc43a", "original_commit_id": "277d2137a4eb59dfebd4d28fe980ef491826d24d", "user": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "body": "Now I think about this a bit more closely, there's a problem here.\r\n\r\nThere are two separate things going on here:\r\na) whether you have the CUDA libraries and enabled CUDA at \"configure\" time.\r\nb) whether you are building with a CUDA compiler (by passing --config=cuda to Bazel).\r\n\r\nFor most parts of TensorFlow, these are identical. However for XLA, we need only (a) but not (b) --- XLA generates its own GPU code without needing nvcc.\r\n\r\nThe PR as written compiles the GPU registrations only in case (b). However we do support (and indeed have continuous builds that test) that you can run the XLA tests if (a) is true but (b) is not.\r\n\r\nThe correct way to do this would be to use the \"if_cuda_is_configured()\" bazel macro to conditionally compile this code.\r\n\r\nYou can see an example of how this works here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/BUILD#L41\r\nThe if_cuda_is_configured macro expands to the empty list if (a) is false, or its argument, if (a) is true. So we can use it to add or remove files from the build if CUDA is configured.\r\n\r\nFor xla_gpu_device.cc, you can probably achieve this by wrapping the two GPU dependencies of this build target in a \"if_cuda_is_configured\" block. That target is the one linked in by the Tensorflow Python runtime if you enable XLA.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/BUILD#L35\r\n(You'll need to add if_cuda_is_configured to the \"load\" statements, as in the stream_executor BUILD file.)\r\n\r\nFor xla_op_registry.cc, you'll need to move the REGISTER_XLA_BACKEND and GpuOpFilter into a separate file (say xla_gpu_backend.cc) and compile that file conditionally as well.\r\n\r\nMake sense?\r\n", "created_at": "2017-08-10T14:56:04Z", "updated_at": "2017-08-11T07:07:29Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12182#discussion_r132477488", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12182", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/132477488"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12182#discussion_r132477488"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12182"}}, "body_html": "<p>Now I think about this a bit more closely, there's a problem here.</p>\n<p>There are two separate things going on here:<br>\na) whether you have the CUDA libraries and enabled CUDA at \"configure\" time.<br>\nb) whether you are building with a CUDA compiler (by passing --config=cuda to Bazel).</p>\n<p>For most parts of TensorFlow, these are identical. However for XLA, we need only (a) but not (b) --- XLA generates its own GPU code without needing nvcc.</p>\n<p>The PR as written compiles the GPU registrations only in case (b). However we do support (and indeed have continuous builds that test) that you can run the XLA tests if (a) is true but (b) is not.</p>\n<p>The correct way to do this would be to use the \"if_cuda_is_configured()\" bazel macro to conditionally compile this code.</p>\n<p>You can see an example of how this works here:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/BUILD#L41\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/BUILD#L41</a><br>\nThe if_cuda_is_configured macro expands to the empty list if (a) is false, or its argument, if (a) is true. So we can use it to add or remove files from the build if CUDA is configured.</p>\n<p>For xla_gpu_device.cc, you can probably achieve this by wrapping the two GPU dependencies of this build target in a \"if_cuda_is_configured\" block. That target is the one linked in by the Tensorflow Python runtime if you enable XLA.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/BUILD#L35\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/BUILD#L35</a><br>\n(You'll need to add if_cuda_is_configured to the \"load\" statements, as in the stream_executor BUILD file.)</p>\n<p>For xla_op_registry.cc, you'll need to move the REGISTER_XLA_BACKEND and GpuOpFilter into a separate file (say xla_gpu_backend.cc) and compile that file conditionally as well.</p>\n<p>Make sense?</p>", "body_text": "Now I think about this a bit more closely, there's a problem here.\nThere are two separate things going on here:\na) whether you have the CUDA libraries and enabled CUDA at \"configure\" time.\nb) whether you are building with a CUDA compiler (by passing --config=cuda to Bazel).\nFor most parts of TensorFlow, these are identical. However for XLA, we need only (a) but not (b) --- XLA generates its own GPU code without needing nvcc.\nThe PR as written compiles the GPU registrations only in case (b). However we do support (and indeed have continuous builds that test) that you can run the XLA tests if (a) is true but (b) is not.\nThe correct way to do this would be to use the \"if_cuda_is_configured()\" bazel macro to conditionally compile this code.\nYou can see an example of how this works here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/BUILD#L41\nThe if_cuda_is_configured macro expands to the empty list if (a) is false, or its argument, if (a) is true. So we can use it to add or remove files from the build if CUDA is configured.\nFor xla_gpu_device.cc, you can probably achieve this by wrapping the two GPU dependencies of this build target in a \"if_cuda_is_configured\" block. That target is the one linked in by the Tensorflow Python runtime if you enable XLA.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/BUILD#L35\n(You'll need to add if_cuda_is_configured to the \"load\" statements, as in the stream_executor BUILD file.)\nFor xla_op_registry.cc, you'll need to move the REGISTER_XLA_BACKEND and GpuOpFilter into a separate file (say xla_gpu_backend.cc) and compile that file conditionally as well.\nMake sense?"}