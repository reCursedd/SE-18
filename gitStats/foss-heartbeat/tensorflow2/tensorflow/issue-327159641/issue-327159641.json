{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19607", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19607/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19607/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19607/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19607", "id": 327159641, "node_id": "MDU6SXNzdWUzMjcxNTk2NDE=", "number": 19607, "title": "Android tensorflow lite kernel_util.cc:34 input_product_scale < output_scale", "user": {"login": "weiting4802", "id": 28512065, "node_id": "MDQ6VXNlcjI4NTEyMDY1", "avatar_url": "https://avatars1.githubusercontent.com/u/28512065?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiting4802", "html_url": "https://github.com/weiting4802", "followers_url": "https://api.github.com/users/weiting4802/followers", "following_url": "https://api.github.com/users/weiting4802/following{/other_user}", "gists_url": "https://api.github.com/users/weiting4802/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiting4802/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiting4802/subscriptions", "organizations_url": "https://api.github.com/users/weiting4802/orgs", "repos_url": "https://api.github.com/users/weiting4802/repos", "events_url": "https://api.github.com/users/weiting4802/events{/privacy}", "received_events_url": "https://api.github.com/users/weiting4802/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "liyunlu0618", "id": 9705880, "node_id": "MDQ6VXNlcjk3MDU4ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/9705880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liyunlu0618", "html_url": "https://github.com/liyunlu0618", "followers_url": "https://api.github.com/users/liyunlu0618/followers", "following_url": "https://api.github.com/users/liyunlu0618/following{/other_user}", "gists_url": "https://api.github.com/users/liyunlu0618/gists{/gist_id}", "starred_url": "https://api.github.com/users/liyunlu0618/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liyunlu0618/subscriptions", "organizations_url": "https://api.github.com/users/liyunlu0618/orgs", "repos_url": "https://api.github.com/users/liyunlu0618/repos", "events_url": "https://api.github.com/users/liyunlu0618/events{/privacy}", "received_events_url": "https://api.github.com/users/liyunlu0618/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "liyunlu0618", "id": 9705880, "node_id": "MDQ6VXNlcjk3MDU4ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/9705880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liyunlu0618", "html_url": "https://github.com/liyunlu0618", "followers_url": "https://api.github.com/users/liyunlu0618/followers", "following_url": "https://api.github.com/users/liyunlu0618/following{/other_user}", "gists_url": "https://api.github.com/users/liyunlu0618/gists{/gist_id}", "starred_url": "https://api.github.com/users/liyunlu0618/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liyunlu0618/subscriptions", "organizations_url": "https://api.github.com/users/liyunlu0618/orgs", "repos_url": "https://api.github.com/users/liyunlu0618/repos", "events_url": "https://api.github.com/users/liyunlu0618/events{/privacy}", "received_events_url": "https://api.github.com/users/liyunlu0618/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-05-29T03:32:50Z", "updated_at": "2018-08-24T21:52:07Z", "closed_at": "2018-08-24T20:11:25Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.8</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>1.I retrained and quantized the openpose model(add tf.contrib.quantize.create_training_graph() in my code\uff09</p>\n<p>2.Generate .pb (add tf.contrib.quantize.create_eval_graph() in my code)</p>\n<p>3.Then, I converted my own pb model to lite.<br>\nIt was succeed,the command:</p>\n<p>./bazel-bin/tensorflow/contrib/lite/toco/toco <br>\n--input_file=models/freeze/frozen_q_256.pb <br>\n--output_file=tf_files/lite/frozen_q_256.lite <br>\n--input_format=TENSORFLOW_GRAPHDEF <br>\n--output_format=TFLITE <br>\n--inference_type=QUANTIZED_UINT8 <br>\n--input_array=image <br>\n--input_shape=1,256,256,3<br>\n--output_array=Openpose/concat_stage7 <br>\n--std_value=127.5 <br>\n--mean_value=127.5 \\</p>\n<p>4.But when I put it on android, I got an error.</p>\n<h3>Source code / logs</h3>\n<p>ava.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/kernel_util.cc:34 input_product_scale &lt; output_scale was not true.<br>\nat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)<br>\nat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)<br>\nat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)<br>\nat org.tensorflow.lite.Interpreter.run(Interpreter.java:93)<br>\nat com.asus.android.poseestimator.ImageClassifier.classifyFrame(ImageClassifier.java:167)<br>\nat com.asus.android.poseestimator.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:740)<br>\nat com.asus.android.poseestimator.Camera2BasicFragment.access$1000(Camera2BasicFragment.java:74)<br>\nat com.asus.android.poseestimator.Camera2BasicFragment$5.run(Camera2BasicFragment.java:621)<br>\nat android.os.Handler.handleCallback(Handler.java:836)<br>\nat android.os.Handler.dispatchMessage(Handler.java:103)<br>\nat android.os.Looper.loop(Looper.java:208)<br>\nat android.os.HandlerThread.run(HandlerThread.java:61)</p>\n<p>my tensorflow lite version is 0.1.1. Does this version support qauntized lite??<br>\nHow can I solve this problem?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):1.8\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\n1.I retrained and quantized the openpose model(add tf.contrib.quantize.create_training_graph() in my code\uff09\n2.Generate .pb (add tf.contrib.quantize.create_eval_graph() in my code)\n3.Then, I converted my own pb model to lite.\nIt was succeed,the command:\n./bazel-bin/tensorflow/contrib/lite/toco/toco \n--input_file=models/freeze/frozen_q_256.pb \n--output_file=tf_files/lite/frozen_q_256.lite \n--input_format=TENSORFLOW_GRAPHDEF \n--output_format=TFLITE \n--inference_type=QUANTIZED_UINT8 \n--input_array=image \n--input_shape=1,256,256,3\n--output_array=Openpose/concat_stage7 \n--std_value=127.5 \n--mean_value=127.5 \\\n4.But when I put it on android, I got an error.\nSource code / logs\nava.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/kernel_util.cc:34 input_product_scale < output_scale was not true.\nat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\nat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)\nat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)\nat org.tensorflow.lite.Interpreter.run(Interpreter.java:93)\nat com.asus.android.poseestimator.ImageClassifier.classifyFrame(ImageClassifier.java:167)\nat com.asus.android.poseestimator.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:740)\nat com.asus.android.poseestimator.Camera2BasicFragment.access$1000(Camera2BasicFragment.java:74)\nat com.asus.android.poseestimator.Camera2BasicFragment$5.run(Camera2BasicFragment.java:621)\nat android.os.Handler.handleCallback(Handler.java:836)\nat android.os.Handler.dispatchMessage(Handler.java:103)\nat android.os.Looper.loop(Looper.java:208)\nat android.os.HandlerThread.run(HandlerThread.java:61)\nmy tensorflow lite version is 0.1.1. Does this version support qauntized lite??\nHow can I solve this problem?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n1.I retrained and quantized the openpose model(add tf.contrib.quantize.create_training_graph() in my code\uff09\r\n\r\n2.Generate .pb (add tf.contrib.quantize.create_eval_graph() in my code) \r\n\r\n3.Then, I converted my own pb model to lite.\r\nIt was succeed,the command:\r\n\r\n./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=models/freeze/frozen_q_256.pb \\\r\n  --output_file=tf_files/lite/frozen_q_256.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_array=image \\\r\n  --input_shape=1,256,256,3\\\r\n  --output_array=Openpose/concat_stage7 \\\r\n  --std_value=127.5 \\\r\n  --mean_value=127.5 \\\r\n\r\n4.But when I put it on android, I got an error.\r\n\r\n### Source code / logs\r\nava.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/kernel_util.cc:34 input_product_scale < output_scale was not true.\r\n                                                                                  at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n                                                                                  at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)\r\n                                                                                  at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)\r\n                                                                                  at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)\r\n                                                                                  at com.asus.android.poseestimator.ImageClassifier.classifyFrame(ImageClassifier.java:167)\r\n                                                                                  at com.asus.android.poseestimator.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:740)\r\n                                                                                  at com.asus.android.poseestimator.Camera2BasicFragment.access$1000(Camera2BasicFragment.java:74)\r\n                                                                                  at com.asus.android.poseestimator.Camera2BasicFragment$5.run(Camera2BasicFragment.java:621)\r\n                                                                                  at android.os.Handler.handleCallback(Handler.java:836)\r\n                                                                                  at android.os.Handler.dispatchMessage(Handler.java:103)\r\n                                                                                  at android.os.Looper.loop(Looper.java:208)\r\n                                                                                  at android.os.HandlerThread.run(HandlerThread.java:61)\r\n\r\nmy tensorflow lite version is 0.1.1. Does this version support qauntized lite??\r\nHow can I solve this problem?\r\n\r\n"}