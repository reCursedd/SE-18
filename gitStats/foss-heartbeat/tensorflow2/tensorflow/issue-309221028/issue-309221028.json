{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18040", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18040/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18040/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18040/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18040", "id": 309221028, "node_id": "MDU6SXNzdWUzMDkyMjEwMjg=", "number": 18040, "title": "[distributed tensorflow] End of sequence after a few batch with dataset.shard", "user": {"login": "malomarrec", "id": 25070988, "node_id": "MDQ6VXNlcjI1MDcwOTg4", "avatar_url": "https://avatars3.githubusercontent.com/u/25070988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malomarrec", "html_url": "https://github.com/malomarrec", "followers_url": "https://api.github.com/users/malomarrec/followers", "following_url": "https://api.github.com/users/malomarrec/following{/other_user}", "gists_url": "https://api.github.com/users/malomarrec/gists{/gist_id}", "starred_url": "https://api.github.com/users/malomarrec/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malomarrec/subscriptions", "organizations_url": "https://api.github.com/users/malomarrec/orgs", "repos_url": "https://api.github.com/users/malomarrec/repos", "events_url": "https://api.github.com/users/malomarrec/events{/privacy}", "received_events_url": "https://api.github.com/users/malomarrec/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-03-28T04:56:22Z", "updated_at": "2018-05-02T18:49:21Z", "closed_at": "2018-05-02T18:49:21Z", "author_association": "NONE", "body_html": "<p>When running distributed training, my session gets terminated after a few batches when using <code>dataset.shard</code>. The issue disappears when I run distributed training with an independent dataset object on each worker (in which case the same data gets shuffled and read on each worker).</p>\n<p>Have I written custom code: No.<br>\nOS Platform and Distribution: ubuntu<br>\nTensorFlow installed from: standard TF1.5 distribution<br>\nBazel version: NA<br>\nCUDA/cuDNN version: CUDA 9.1<br>\nGPU model and memory: No GPU<br>\nExact command to reproduce: Cf Below</p>\n<p>Dataset construction:</p>\n<pre><code>def construct_dataset(filenames, labels, batch_size, num_workers,worker_index):\n    dataset = tf.data.TextLineDataset(filenames)\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\n    dataset = dataset.shard(num_workers, worker_index) #works fine when commenting this out\n    dataset = dataset.shuffle(buffer_size=10000)  # Equivalent to min_after_dequeue=10000.\n    dataset = dataset.map(_parse_function)\n    dataset = dataset.batch(batch_size)\n    return dataset\n</code></pre>\n<p>main</p>\n<pre><code>with tf.device(device):\n        filelist, labels = get_filelist(FLAGS.data_dir)\n        dataset = construct_dataset(...)\n        iterator = dataset.make_one_shot_iterator()\n        batch = iterator.get_next()\n        img_batch, filepath_batch, label_batch = batch\n        ...\n\n       hooks=[tf.train.StopAtStepHook(last_step=1000000)] \n\n    with tf.train.MonitoredTrainingSession(master=target,\n        is_chief=(FLAGS.task_index == 0),checkpoint_dir=logs,hooks = hooks) as sess:\n        try:\n            while not sess.should_stop():\n                sess.run(train_op)\n        except Exception as e:\n            print(e)\n</code></pre>\n<p>Stacktrace -</p>\n<pre lang=\"End\" data-meta=\"of sequence\"><code>\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,300,300,?], [?], [?]], output_types=[DT_UINT8, DT_STRING, DT_INT32], _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\t [[Node: Training_Loss_S17 = _HostRecv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:ps/replica:0/task:0/device:CPU:0\", send_device_incarnation=-379211586651304706, tensor_name=\"edge_133_Training_Loss\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\n</code></pre>", "body_text": "When running distributed training, my session gets terminated after a few batches when using dataset.shard. The issue disappears when I run distributed training with an independent dataset object on each worker (in which case the same data gets shuffled and read on each worker).\nHave I written custom code: No.\nOS Platform and Distribution: ubuntu\nTensorFlow installed from: standard TF1.5 distribution\nBazel version: NA\nCUDA/cuDNN version: CUDA 9.1\nGPU model and memory: No GPU\nExact command to reproduce: Cf Below\nDataset construction:\ndef construct_dataset(filenames, labels, batch_size, num_workers,worker_index):\n    dataset = tf.data.TextLineDataset(filenames)\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\n    dataset = dataset.shard(num_workers, worker_index) #works fine when commenting this out\n    dataset = dataset.shuffle(buffer_size=10000)  # Equivalent to min_after_dequeue=10000.\n    dataset = dataset.map(_parse_function)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\nmain\nwith tf.device(device):\n        filelist, labels = get_filelist(FLAGS.data_dir)\n        dataset = construct_dataset(...)\n        iterator = dataset.make_one_shot_iterator()\n        batch = iterator.get_next()\n        img_batch, filepath_batch, label_batch = batch\n        ...\n\n       hooks=[tf.train.StopAtStepHook(last_step=1000000)] \n\n    with tf.train.MonitoredTrainingSession(master=target,\n        is_chief=(FLAGS.task_index == 0),checkpoint_dir=logs,hooks = hooks) as sess:\n        try:\n            while not sess.should_stop():\n                sess.run(train_op)\n        except Exception as e:\n            print(e)\n\nStacktrace -\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,300,300,?], [?], [?]], output_types=[DT_UINT8, DT_STRING, DT_INT32], _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\t [[Node: Training_Loss_S17 = _HostRecv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:ps/replica:0/task:0/device:CPU:0\", send_device_incarnation=-379211586651304706, tensor_name=\"edge_133_Training_Loss\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]", "body": "When running distributed training, my session gets terminated after a few batches when using `dataset.shard`. The issue disappears when I run distributed training with an independent dataset object on each worker (in which case the same data gets shuffled and read on each worker).\r\n\r\n\r\nHave I written custom code: No.\r\nOS Platform and Distribution: ubuntu\r\nTensorFlow installed from: standard TF1.5 distribution\r\nBazel version: NA\r\nCUDA/cuDNN version: CUDA 9.1\r\nGPU model and memory: No GPU\r\nExact command to reproduce: Cf Below\r\n\r\n\r\n\r\nDataset construction:\r\n```\r\ndef construct_dataset(filenames, labels, batch_size, num_workers,worker_index):\r\n    dataset = tf.data.TextLineDataset(filenames)\r\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\r\n    dataset = dataset.shard(num_workers, worker_index) #works fine when commenting this out\r\n    dataset = dataset.shuffle(buffer_size=10000)  # Equivalent to min_after_dequeue=10000.\r\n    dataset = dataset.map(_parse_function)\r\n    dataset = dataset.batch(batch_size)\r\n    return dataset\r\n```\r\n\r\nmain\r\n```\r\nwith tf.device(device):\r\n        filelist, labels = get_filelist(FLAGS.data_dir)\r\n        dataset = construct_dataset(...)\r\n        iterator = dataset.make_one_shot_iterator()\r\n        batch = iterator.get_next()\r\n        img_batch, filepath_batch, label_batch = batch\r\n        ...\r\n\r\n       hooks=[tf.train.StopAtStepHook(last_step=1000000)] \r\n\r\n    with tf.train.MonitoredTrainingSession(master=target,\r\n        is_chief=(FLAGS.task_index == 0),checkpoint_dir=logs,hooks = hooks) as sess:\r\n        try:\r\n            while not sess.should_stop():\r\n                sess.run(train_op)\r\n        except Exception as e:\r\n            print(e)\r\n```\r\n\r\nStacktrace - \r\n```End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,300,300,?], [?], [?]], output_types=[DT_UINT8, DT_STRING, DT_INT32], _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\t [[Node: Training_Loss_S17 = _HostRecv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:ps/replica:0/task:0/device:CPU:0\", send_device_incarnation=-379211586651304706, tensor_name=\"edge_133_Training_Loss\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n"}