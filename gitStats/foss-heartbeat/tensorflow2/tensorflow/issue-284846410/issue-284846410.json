{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15682", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15682/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15682/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15682/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15682", "id": 284846410, "node_id": "MDU6SXNzdWUyODQ4NDY0MTA=", "number": 15682, "title": "Eager: tfe.implicit_value_and_gradients uses functions operating on raw tf variables", "user": {"login": "lxuechen", "id": 12689993, "node_id": "MDQ6VXNlcjEyNjg5OTkz", "avatar_url": "https://avatars1.githubusercontent.com/u/12689993?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lxuechen", "html_url": "https://github.com/lxuechen", "followers_url": "https://api.github.com/users/lxuechen/followers", "following_url": "https://api.github.com/users/lxuechen/following{/other_user}", "gists_url": "https://api.github.com/users/lxuechen/gists{/gist_id}", "starred_url": "https://api.github.com/users/lxuechen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lxuechen/subscriptions", "organizations_url": "https://api.github.com/users/lxuechen/orgs", "repos_url": "https://api.github.com/users/lxuechen/repos", "events_url": "https://api.github.com/users/lxuechen/events{/privacy}", "received_events_url": "https://api.github.com/users/lxuechen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 736653459, "node_id": "MDU6TGFiZWw3MzY2NTM0NTk=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:eager", "name": "comp:eager", "color": "0052cc", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-28T05:50:28Z", "updated_at": "2018-02-07T18:08:07Z", "closed_at": "2017-12-29T22:05:16Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>System information</h2>\n<ul>\n<li>Tensorflow version: 1.5.0-dev20171126</li>\n<li>Python version: Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)</li>\n</ul>\n<h2>Problem</h2>\n<p>Forgive me if I'm re-iterating something that's discussed before. Even though I don't think the issue described here is a bug, I nevertheless believe it is worthy to point out. The specific issue is that when we pass a loss function, e.g. <code>loss</code>, to <code>tfe.implicit_value_and_gradients</code>, it seems that backprop only happens if the variables used by <code>loss</code> are is their \"raw states\". Here's an example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\n\ntfe.enable_eager_execution()\nv <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>v<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>., <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nv_add_1 <span class=\"pl-k\">=</span> v <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>.  <span class=\"pl-c\"><span class=\"pl-c\">#</span> this causes the problem</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>():\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">2</span>. <span class=\"pl-k\">*</span> v_add_1\nvalue_and_gradients_fn <span class=\"pl-k\">=</span> tfe.implicit_value_and_gradients(loss)\n<span class=\"pl-c1\">print</span> (value_and_gradients_fn())</pre></div>\n<p>In this case I get the error as follows:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 17, in &lt;module&gt;\n    val = g()\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\", line 360, in grad_fn\n    raise ValueError(\"No trainable variables were accessed while the \"\nValueError: No trainable variables were accessed while the function was being computed.\n</code></pre>\n<p>After a little bit of pondering, I found the problem to be the line <code>v = v + 1.</code>. As soon as we <strong>delete</strong> this line, the program runs without bugs. My understanding of this behavior is that, the gradients and the backprop process somehow only \"live\" in the scope of the loss function. We <strong>cannot</strong> backprop to some variable that is modified outside of <code>loss</code>, even if, implicitly, the computed loss depends on that variable.</p>\n<p>Here's a more obscure example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\n\ntfe.enable_eager_execution()\n\nv <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>v<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>., <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nv_add_1 <span class=\"pl-k\">=</span> v <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>.\nu <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>u<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>., <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>():\n    result <span class=\"pl-k\">=</span> v_add_1 <span class=\"pl-k\">+</span> u\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">2</span>. <span class=\"pl-k\">*</span> result\n\nvalue_and_gradients_fn <span class=\"pl-k\">=</span> tfe.implicit_value_and_gradients(loss)\noptimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> before training</span>\n<span class=\"pl-c1\">print</span> (v)\n<span class=\"pl-c1\">print</span> (u)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n    _, gradients_and_variables <span class=\"pl-k\">=</span> value_and_gradients_fn()\n    optimizer.apply_gradients(gradients_and_variables)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> after training</span>\n<span class=\"pl-c1\">print</span> (v)\n<span class=\"pl-c1\">print</span> (u)</pre></div>\n<p>After running, we could see that <code>u</code> is updated and <code>v</code> is not:</p>\n<pre><code>&lt;tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0&gt;\n&lt;tf.Variable 'u:0' shape=() dtype=float32, numpy=20.0&gt;\n&lt;tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0&gt;\n&lt;tf.Variable 'u:0' shape=() dtype=float32, numpy=9.9999638&gt;\n</code></pre>\n<p>This might be irrelevant, but is there a way we could by pass the restriction and have gradient pass outside the function given to <code>tfe.implicit_value_and_gradients</code>?</p>", "body_text": "System information\n\nTensorflow version: 1.5.0-dev20171126\nPython version: Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)\n\nProblem\nForgive me if I'm re-iterating something that's discussed before. Even though I don't think the issue described here is a bug, I nevertheless believe it is worthy to point out. The specific issue is that when we pass a loss function, e.g. loss, to tfe.implicit_value_and_gradients, it seems that backprop only happens if the variables used by loss are is their \"raw states\". Here's an example:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution()\nv = tf.get_variable(name='v', initializer=1., trainable=True)\nv_add_1 = v + 1.  # this causes the problem\n\ndef loss():\n    return 2. * v_add_1\nvalue_and_gradients_fn = tfe.implicit_value_and_gradients(loss)\nprint (value_and_gradients_fn())\nIn this case I get the error as follows:\nTraceback (most recent call last):\n  File \"test.py\", line 17, in <module>\n    val = g()\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\", line 360, in grad_fn\n    raise ValueError(\"No trainable variables were accessed while the \"\nValueError: No trainable variables were accessed while the function was being computed.\n\nAfter a little bit of pondering, I found the problem to be the line v = v + 1.. As soon as we delete this line, the program runs without bugs. My understanding of this behavior is that, the gradients and the backprop process somehow only \"live\" in the scope of the loss function. We cannot backprop to some variable that is modified outside of loss, even if, implicitly, the computed loss depends on that variable.\nHere's a more obscure example:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntfe.enable_eager_execution()\n\nv = tf.get_variable(name='v', initializer=1., trainable=True)\nv_add_1 = v + 1.\nu = tf.get_variable(name='u', initializer=20., trainable=True)\n\n\ndef loss():\n    result = v_add_1 + u\n    return 2. * result\n\nvalue_and_gradients_fn = tfe.implicit_value_and_gradients(loss)\noptimizer = tf.train.AdamOptimizer(1e-1)\n\n# before training\nprint (v)\nprint (u)\n\nfor i in range(100):\n    _, gradients_and_variables = value_and_gradients_fn()\n    optimizer.apply_gradients(gradients_and_variables)\n\n# after training\nprint (v)\nprint (u)\nAfter running, we could see that u is updated and v is not:\n<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>\n<tf.Variable 'u:0' shape=() dtype=float32, numpy=20.0>\n<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>\n<tf.Variable 'u:0' shape=() dtype=float32, numpy=9.9999638>\n\nThis might be irrelevant, but is there a way we could by pass the restriction and have gradient pass outside the function given to tfe.implicit_value_and_gradients?", "body": "## System information\r\n- Tensorflow version: 1.5.0-dev20171126\r\n- Python version: Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)\r\n\r\n## Problem\r\nForgive me if I'm re-iterating something that's discussed before. Even though I don't think the issue described here is a bug, I nevertheless believe it is worthy to point out. The specific issue is that when we pass a loss function, e.g. ```loss```, to ```tfe.implicit_value_and_gradients```, it seems that backprop only happens if the variables used by ```loss``` are is their \"raw states\". Here's an example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\nv = tf.get_variable(name='v', initializer=1., trainable=True)\r\nv_add_1 = v + 1.  # this causes the problem\r\n\r\ndef loss():\r\n    return 2. * v_add_1\r\nvalue_and_gradients_fn = tfe.implicit_value_and_gradients(loss)\r\nprint (value_and_gradients_fn())\r\n```\r\nIn this case I get the error as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 17, in <module>\r\n    val = g()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\", line 360, in grad_fn\r\n    raise ValueError(\"No trainable variables were accessed while the \"\r\nValueError: No trainable variables were accessed while the function was being computed.\r\n```\r\nAfter a little bit of pondering, I found the problem to be the line ```v = v + 1.```. As soon as we **delete** this line, the program runs without bugs. My understanding of this behavior is that, the gradients and the backprop process somehow only \"live\" in the scope of the loss function. We **cannot** backprop to some variable that is modified outside of ```loss```, even if, implicitly, the computed loss depends on that variable. \r\n\r\nHere's a more obscure example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nv = tf.get_variable(name='v', initializer=1., trainable=True)\r\nv_add_1 = v + 1.\r\nu = tf.get_variable(name='u', initializer=20., trainable=True)\r\n\r\n\r\ndef loss():\r\n    result = v_add_1 + u\r\n    return 2. * result\r\n\r\nvalue_and_gradients_fn = tfe.implicit_value_and_gradients(loss)\r\noptimizer = tf.train.AdamOptimizer(1e-1)\r\n\r\n# before training\r\nprint (v)\r\nprint (u)\r\n\r\nfor i in range(100):\r\n    _, gradients_and_variables = value_and_gradients_fn()\r\n    optimizer.apply_gradients(gradients_and_variables)\r\n\r\n# after training\r\nprint (v)\r\nprint (u)\r\n```\r\nAfter running, we could see that ```u``` is updated and ```v``` is not:\r\n```\r\n<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>\r\n<tf.Variable 'u:0' shape=() dtype=float32, numpy=20.0>\r\n<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>\r\n<tf.Variable 'u:0' shape=() dtype=float32, numpy=9.9999638>\r\n```\r\n\r\nThis might be irrelevant, but is there a way we could by pass the restriction and have gradient pass outside the function given to ```tfe.implicit_value_and_gradients```?"}