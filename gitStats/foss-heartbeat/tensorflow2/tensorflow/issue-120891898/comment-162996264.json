{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/162996264", "html_url": "https://github.com/tensorflow/tensorflow/issues/436#issuecomment-162996264", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/436", "id": 162996264, "node_id": "MDEyOklzc3VlQ29tbWVudDE2Mjk5NjI2NA==", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-08T19:55:03Z", "updated_at": "2015-12-08T19:55:03Z", "author_association": "NONE", "body_html": "<p>Sorry about that! I posted more on what the bridge creates below. This version is just a linear mapping using cosine similarity.</p>\n<pre><code>class Bridge(object):\n    def __init__(self, state_size, bucket_sizes, bridge_learning_rate, \n                        bridge_learning_decay_factor, bridge_regularizer_factor):\n        bridge_learning_rate = tf.get_variable(\n            'bridge_learning_rate', shape=[], trainable=False,\n            initializer=tf.constant_initializer(float(bridge_learning_rate)))   \n        bridge_learning_decay_factor_op = self.bridge_learning_rate.assign(\n            self.bridge_learning_rate * bridge_learning_decay_factor)\n        bridge_global_step = tf.get_variable(\n            'bridge_global_step', shape=[], trainable=False,\n            initializer=tf.constant_initializer(0))\n\n        initializer = tf.truncated_normal_initializer\n\n        self.s1 = tf.placeholder(\"float\", [None, state_size])\n        self.s2 = tf.placeholder(\"float\", [None, state_size])\n\n        # W, b are the weight / bias matrices for the states.                                                                                                                                               \n        # shape: [state_size x state_size], [state_size]                                                                                                                                                    \n        W = tf.get_variable(\n            'weights', [state_size, state_size],\n            initializer=initializer())\n        b = tf.get_variable(\n            'bias', [state_size],\n            initializer=initializer())\n\n        self.y = tf.nn.xw_plus_b(self.s1, W, b)\n\n        cost = self._cosine_cost(self.s2, self.y)\n        reg = tf.reduce_sum(tf.square(W)) * bridge_regularizer_factor\n        self.cost = tf.add(cost, reg)\n\n        self.train_step = tf.train.AdagradOptimizer(\n            bridge_learning_rate\n            ).minimize(self.cost, global_step=bridge_global_step)\n\n        tf.initialize_all_variables().run()\n\n    @staticmethod\n    def _cosine_cost(y, y_):\n        def norm(v):\n            return tf.sqrt(tf.reduce_sum(tf.square(v), 1, keep_dims=True))\n\n        y = y / norm(y)\n        y_ = y_ / norm(y_)\n        return -tf.reduce_mean(tf.matmul(y, y_, transpose_b=True))\n</code></pre>\n<p>After creating the bridge, I then looked at <code>tf.all_variables()</code> and this was the result:</p>\n<pre><code>[u'bridge/bridge_learning_rate:0', u'bridge/bridge_global_step:0', \n u'bridge/weights:0', u'bridge/bias:0', \n u'bridge/bridge/weights/Adagrad:0', u'bridge/bridge/bias/Adagrad:0']\n</code></pre>", "body_text": "Sorry about that! I posted more on what the bridge creates below. This version is just a linear mapping using cosine similarity.\nclass Bridge(object):\n    def __init__(self, state_size, bucket_sizes, bridge_learning_rate, \n                        bridge_learning_decay_factor, bridge_regularizer_factor):\n        bridge_learning_rate = tf.get_variable(\n            'bridge_learning_rate', shape=[], trainable=False,\n            initializer=tf.constant_initializer(float(bridge_learning_rate)))   \n        bridge_learning_decay_factor_op = self.bridge_learning_rate.assign(\n            self.bridge_learning_rate * bridge_learning_decay_factor)\n        bridge_global_step = tf.get_variable(\n            'bridge_global_step', shape=[], trainable=False,\n            initializer=tf.constant_initializer(0))\n\n        initializer = tf.truncated_normal_initializer\n\n        self.s1 = tf.placeholder(\"float\", [None, state_size])\n        self.s2 = tf.placeholder(\"float\", [None, state_size])\n\n        # W, b are the weight / bias matrices for the states.                                                                                                                                               \n        # shape: [state_size x state_size], [state_size]                                                                                                                                                    \n        W = tf.get_variable(\n            'weights', [state_size, state_size],\n            initializer=initializer())\n        b = tf.get_variable(\n            'bias', [state_size],\n            initializer=initializer())\n\n        self.y = tf.nn.xw_plus_b(self.s1, W, b)\n\n        cost = self._cosine_cost(self.s2, self.y)\n        reg = tf.reduce_sum(tf.square(W)) * bridge_regularizer_factor\n        self.cost = tf.add(cost, reg)\n\n        self.train_step = tf.train.AdagradOptimizer(\n            bridge_learning_rate\n            ).minimize(self.cost, global_step=bridge_global_step)\n\n        tf.initialize_all_variables().run()\n\n    @staticmethod\n    def _cosine_cost(y, y_):\n        def norm(v):\n            return tf.sqrt(tf.reduce_sum(tf.square(v), 1, keep_dims=True))\n\n        y = y / norm(y)\n        y_ = y_ / norm(y_)\n        return -tf.reduce_mean(tf.matmul(y, y_, transpose_b=True))\n\nAfter creating the bridge, I then looked at tf.all_variables() and this was the result:\n[u'bridge/bridge_learning_rate:0', u'bridge/bridge_global_step:0', \n u'bridge/weights:0', u'bridge/bias:0', \n u'bridge/bridge/weights/Adagrad:0', u'bridge/bridge/bias/Adagrad:0']", "body": "Sorry about that! I posted more on what the bridge creates below. This version is just a linear mapping using cosine similarity. \n\n```\nclass Bridge(object):\n    def __init__(self, state_size, bucket_sizes, bridge_learning_rate, \n                        bridge_learning_decay_factor, bridge_regularizer_factor):\n        bridge_learning_rate = tf.get_variable(\n            'bridge_learning_rate', shape=[], trainable=False,\n            initializer=tf.constant_initializer(float(bridge_learning_rate)))   \n        bridge_learning_decay_factor_op = self.bridge_learning_rate.assign(\n            self.bridge_learning_rate * bridge_learning_decay_factor)\n        bridge_global_step = tf.get_variable(\n            'bridge_global_step', shape=[], trainable=False,\n            initializer=tf.constant_initializer(0))\n\n        initializer = tf.truncated_normal_initializer\n\n        self.s1 = tf.placeholder(\"float\", [None, state_size])\n        self.s2 = tf.placeholder(\"float\", [None, state_size])\n\n        # W, b are the weight / bias matrices for the states.                                                                                                                                               \n        # shape: [state_size x state_size], [state_size]                                                                                                                                                    \n        W = tf.get_variable(\n            'weights', [state_size, state_size],\n            initializer=initializer())\n        b = tf.get_variable(\n            'bias', [state_size],\n            initializer=initializer())\n\n        self.y = tf.nn.xw_plus_b(self.s1, W, b)\n\n        cost = self._cosine_cost(self.s2, self.y)\n        reg = tf.reduce_sum(tf.square(W)) * bridge_regularizer_factor\n        self.cost = tf.add(cost, reg)\n\n        self.train_step = tf.train.AdagradOptimizer(\n            bridge_learning_rate\n            ).minimize(self.cost, global_step=bridge_global_step)\n\n        tf.initialize_all_variables().run()\n\n    @staticmethod\n    def _cosine_cost(y, y_):\n        def norm(v):\n            return tf.sqrt(tf.reduce_sum(tf.square(v), 1, keep_dims=True))\n\n        y = y / norm(y)\n        y_ = y_ / norm(y_)\n        return -tf.reduce_mean(tf.matmul(y, y_, transpose_b=True))\n```\n\nAfter creating the bridge, I then looked at `tf.all_variables()` and this was the result: \n\n```\n[u'bridge/bridge_learning_rate:0', u'bridge/bridge_global_step:0', \n u'bridge/weights:0', u'bridge/bias:0', \n u'bridge/bridge/weights/Adagrad:0', u'bridge/bridge/bias/Adagrad:0']\n```\n"}