{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318390219", "html_url": "https://github.com/tensorflow/tensorflow/issues/6698#issuecomment-318390219", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6698", "id": 318390219, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODM5MDIxOQ==", "user": {"login": "RawthiL", "id": 141699, "node_id": "MDQ6VXNlcjE0MTY5OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/141699?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RawthiL", "html_url": "https://github.com/RawthiL", "followers_url": "https://api.github.com/users/RawthiL/followers", "following_url": "https://api.github.com/users/RawthiL/following{/other_user}", "gists_url": "https://api.github.com/users/RawthiL/gists{/gist_id}", "starred_url": "https://api.github.com/users/RawthiL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RawthiL/subscriptions", "organizations_url": "https://api.github.com/users/RawthiL/orgs", "repos_url": "https://api.github.com/users/RawthiL/repos", "events_url": "https://api.github.com/users/RawthiL/events{/privacy}", "received_events_url": "https://api.github.com/users/RawthiL/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T15:05:52Z", "updated_at": "2017-07-27T15:05:52Z", "author_association": "NONE", "body_html": "<p>I found that in some cases resetting the jupyter kernel wont work. Actually it happened to me while using jupyterhub.<br>\nI restarted the kernel, deactivated my virtualenv and the GPU memory was still being held by some process. The<code> nvidia-smi</code> command said that there was no process using the GPU and when I  tried to reset it with <code>sudo nvidia-smi --gpu-reset -i 0</code> (for the 0 gpu core) it said the following:</p>\n<blockquote>\n<p>Unable to reset this GPU because it's being used by some other process (e.g. CUDA application, graphics application like X server, monitoring application like other instance of nvidia-smi). Please first kill all processes using this GPU and all compute applications running in the system (even when they are running on other GPUs) and then try to reset the GPU again.<br>\nTerminating early due to previous errors.</p>\n</blockquote>\n<p>So there was some process holding the GPU, and I looked for them using <code>sudo fuser -v /dev/nvidia*</code> which said that there was actually something holding the GPU... python itself... killing it and re-launching virtualenv and jupyter did the trick.<br>\nI might not be the best way to solve this, but is better than resetting the computer when all other options fail.</p>", "body_text": "I found that in some cases resetting the jupyter kernel wont work. Actually it happened to me while using jupyterhub.\nI restarted the kernel, deactivated my virtualenv and the GPU memory was still being held by some process. The nvidia-smi command said that there was no process using the GPU and when I  tried to reset it with sudo nvidia-smi --gpu-reset -i 0 (for the 0 gpu core) it said the following:\n\nUnable to reset this GPU because it's being used by some other process (e.g. CUDA application, graphics application like X server, monitoring application like other instance of nvidia-smi). Please first kill all processes using this GPU and all compute applications running in the system (even when they are running on other GPUs) and then try to reset the GPU again.\nTerminating early due to previous errors.\n\nSo there was some process holding the GPU, and I looked for them using sudo fuser -v /dev/nvidia* which said that there was actually something holding the GPU... python itself... killing it and re-launching virtualenv and jupyter did the trick.\nI might not be the best way to solve this, but is better than resetting the computer when all other options fail.", "body": "I found that in some cases resetting the jupyter kernel wont work. Actually it happened to me while using jupyterhub.\r\nI restarted the kernel, deactivated my virtualenv and the GPU memory was still being held by some process. The` nvidia-smi` command said that there was no process using the GPU and when I  tried to reset it with `sudo nvidia-smi --gpu-reset -i 0` (for the 0 gpu core) it said the following:\r\n\r\n> Unable to reset this GPU because it's being used by some other process (e.g. CUDA application, graphics application like X server, monitoring application like other instance of nvidia-smi). Please first kill all processes using this GPU and all compute applications running in the system (even when they are running on other GPUs) and then try to reset the GPU again.\r\n> Terminating early due to previous errors.\r\n\r\nSo there was some process holding the GPU, and I looked for them using `sudo fuser -v /dev/nvidia*` which said that there was actually something holding the GPU... python itself... killing it and re-launching virtualenv and jupyter did the trick.\r\nI might not be the best way to solve this, but is better than resetting the computer when all other options fail."}