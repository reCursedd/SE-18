{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/265515220", "html_url": "https://github.com/tensorflow/tensorflow/issues/6144#issuecomment-265515220", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6144", "id": 265515220, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NTUxNTIyMA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-07T17:35:08Z", "updated_at": "2016-12-07T17:37:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I just tried importing vgg example from <a href=\"https://www.cs.toronto.edu/~frossard/post/vgg16/\" rel=\"nofollow\">Toronto site</a> and GraphDef is 85k, so most of your graphdef is coming from somewhere else. You may get better community support in refactoring your graph creation on stack overflow.</p>\n<p>2GB may seem like an arbitrary limit, but it's actually quite large and hitting it usually means user error, like node creation \"leak\". For instance VGG example is 85k graphdef and it has 266 nodes. So it's an average of 320 bytes per node. So to exceed 2GB with similar kind of network, you'd need 6.2 million nodes. There's a 5usec delay on schedule a GPU op, so you will be spending 31 seconds just on op schedule overhead to run these 6 million operations</p>\n<p>Another way to hit the limit is having large graph-inlined constants (<code>tf.constant</code>). Storing large data inlined in Graph is inefficient because of locks, so you would be better off moving them into variables anyway.</p>", "body_text": "I just tried importing vgg example from Toronto site and GraphDef is 85k, so most of your graphdef is coming from somewhere else. You may get better community support in refactoring your graph creation on stack overflow.\n2GB may seem like an arbitrary limit, but it's actually quite large and hitting it usually means user error, like node creation \"leak\". For instance VGG example is 85k graphdef and it has 266 nodes. So it's an average of 320 bytes per node. So to exceed 2GB with similar kind of network, you'd need 6.2 million nodes. There's a 5usec delay on schedule a GPU op, so you will be spending 31 seconds just on op schedule overhead to run these 6 million operations\nAnother way to hit the limit is having large graph-inlined constants (tf.constant). Storing large data inlined in Graph is inefficient because of locks, so you would be better off moving them into variables anyway.", "body": "I just tried importing vgg example from [Toronto site](https://www.cs.toronto.edu/~frossard/post/vgg16/) and GraphDef is 85k, so most of your graphdef is coming from somewhere else. You may get better community support in refactoring your graph creation on stack overflow.\r\n\r\n2GB may seem like an arbitrary limit, but it's actually quite large and hitting it usually means user error, like node creation \"leak\". For instance VGG example is 85k graphdef and it has 266 nodes. So it's an average of 320 bytes per node. So to exceed 2GB with similar kind of network, you'd need 6.2 million nodes. There's a 5usec delay on schedule a GPU op, so you will be spending 31 seconds just on op schedule overhead to run these 6 million operations\r\n\r\nAnother way to hit the limit is having large graph-inlined constants (`tf.constant`). Storing large data inlined in Graph is inefficient because of locks, so you would be better off moving them into variables anyway."}