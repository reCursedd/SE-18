{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364500656", "html_url": "https://github.com/tensorflow/tensorflow/issues/16831#issuecomment-364500656", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16831", "id": 364500656, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDUwMDY1Ng==", "user": {"login": "mhuen", "id": 23478431, "node_id": "MDQ6VXNlcjIzNDc4NDMx", "avatar_url": "https://avatars1.githubusercontent.com/u/23478431?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mhuen", "html_url": "https://github.com/mhuen", "followers_url": "https://api.github.com/users/mhuen/followers", "following_url": "https://api.github.com/users/mhuen/following{/other_user}", "gists_url": "https://api.github.com/users/mhuen/gists{/gist_id}", "starred_url": "https://api.github.com/users/mhuen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mhuen/subscriptions", "organizations_url": "https://api.github.com/users/mhuen/orgs", "repos_url": "https://api.github.com/users/mhuen/repos", "events_url": "https://api.github.com/users/mhuen/events{/privacy}", "received_events_url": "https://api.github.com/users/mhuen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-09T17:25:46Z", "updated_at": "2018-02-09T17:27:47Z", "author_association": "NONE", "body_html": "<p>I guess something like this shouldn't be too much of a slowdown:</p>\n<pre><code>def dynamic_conv(\n                input, \n                filter, \n                strides=[1,1,1], \n                padding='SAME',\n                dilation_rate=None,\n                ):\n    '''\n    Equivalent to tf.nn.convolution, but filter has additional\n    batch dimension. This allows the filter to be a function\n    of some input, hence, enabling dynamic convolutions.\n\n    Parameters\n    ----------\n    input:  A Tensor. Must be one of the following types: float32, float64, int64, int32, \n            uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half. \n            2d case:\n            Shape [batch, in_depth, in_height, in_channels].\n            3d case:\n            Shape [batch, in_depth, in_height, in_width, in_channels].\n\n    filter: A Tensor. Must have the same type as input. \n            in_channels must match between input and filter.\n            2d case:\n            Shape [batch, filter_x, filter_y, in_ch, out_ch].\n            3d case:\n            Shape [batch, filter_x, filter_y, filter_z, in_ch, out_ch] . \n\n    strides:    A list of ints that has length &gt;= 5. 1-D tensor of length 5. \n                The stride of the sliding window for each dimension of input.\n                Must have strides[0] = strides[4] = 1.\n    padding:    A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n\n    dilation_rate: Optional. \n                Sequence of N ints &gt;= 1. Specifies the filter upsampling/input downsampling rate. \n                In the literature, the same parameter is sometimes called input stride or dilation. \n                The effective filter size used for the convolution will be\n                spatial_filter_shape + (spatial_filter_shape - 1) * (rate - 1), \n                obtained by inserting (dilation_rate[i]-1) zeros between consecutive elements of \n                the original filter in each spatial dimension i. \n                If any value of dilation_rate is &gt; 1, then all values of strides must be 1.\n\n    Returns\n    -------\n            A Tensor. Has the same type as input.\n    '''\n\n    assert len(filter.get_shape()) == len(input.get_shape()) + 1\n    assert filter.get_shape()[0] == input.get_shape()[0]\n\n    split_inputs = tf.split(input, \n                            input.get_shape().as_list()[0], \n                            axis=0)\n    split_filters = tf.unstack(filter, \n                            input.get_shape().as_list()[0], \n                            axis=0)\n\n    output_list = []\n    for split_input, split_filter in zip(split_inputs,split_filters):\n        output_list.append(\n              tf.nn.convolution( split_input,\n                            split_filter, \n                            strides=strides, \n                            padding=padding,\n                            dilation_rate=dilation_rate,\n                            )\n          )\n    output = tf.concat(output_list, axis=0)\n    return output\n</code></pre>", "body_text": "I guess something like this shouldn't be too much of a slowdown:\ndef dynamic_conv(\n                input, \n                filter, \n                strides=[1,1,1], \n                padding='SAME',\n                dilation_rate=None,\n                ):\n    '''\n    Equivalent to tf.nn.convolution, but filter has additional\n    batch dimension. This allows the filter to be a function\n    of some input, hence, enabling dynamic convolutions.\n\n    Parameters\n    ----------\n    input:  A Tensor. Must be one of the following types: float32, float64, int64, int32, \n            uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half. \n            2d case:\n            Shape [batch, in_depth, in_height, in_channels].\n            3d case:\n            Shape [batch, in_depth, in_height, in_width, in_channels].\n\n    filter: A Tensor. Must have the same type as input. \n            in_channels must match between input and filter.\n            2d case:\n            Shape [batch, filter_x, filter_y, in_ch, out_ch].\n            3d case:\n            Shape [batch, filter_x, filter_y, filter_z, in_ch, out_ch] . \n\n    strides:    A list of ints that has length >= 5. 1-D tensor of length 5. \n                The stride of the sliding window for each dimension of input.\n                Must have strides[0] = strides[4] = 1.\n    padding:    A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n\n    dilation_rate: Optional. \n                Sequence of N ints >= 1. Specifies the filter upsampling/input downsampling rate. \n                In the literature, the same parameter is sometimes called input stride or dilation. \n                The effective filter size used for the convolution will be\n                spatial_filter_shape + (spatial_filter_shape - 1) * (rate - 1), \n                obtained by inserting (dilation_rate[i]-1) zeros between consecutive elements of \n                the original filter in each spatial dimension i. \n                If any value of dilation_rate is > 1, then all values of strides must be 1.\n\n    Returns\n    -------\n            A Tensor. Has the same type as input.\n    '''\n\n    assert len(filter.get_shape()) == len(input.get_shape()) + 1\n    assert filter.get_shape()[0] == input.get_shape()[0]\n\n    split_inputs = tf.split(input, \n                            input.get_shape().as_list()[0], \n                            axis=0)\n    split_filters = tf.unstack(filter, \n                            input.get_shape().as_list()[0], \n                            axis=0)\n\n    output_list = []\n    for split_input, split_filter in zip(split_inputs,split_filters):\n        output_list.append(\n              tf.nn.convolution( split_input,\n                            split_filter, \n                            strides=strides, \n                            padding=padding,\n                            dilation_rate=dilation_rate,\n                            )\n          )\n    output = tf.concat(output_list, axis=0)\n    return output", "body": "I guess something like this shouldn't be too much of a slowdown:\r\n\r\n```\r\ndef dynamic_conv(\r\n                input, \r\n                filter, \r\n                strides=[1,1,1], \r\n                padding='SAME',\r\n                dilation_rate=None,\r\n                ):\r\n    '''\r\n    Equivalent to tf.nn.convolution, but filter has additional\r\n    batch dimension. This allows the filter to be a function\r\n    of some input, hence, enabling dynamic convolutions.\r\n\r\n    Parameters\r\n    ----------\r\n    input:  A Tensor. Must be one of the following types: float32, float64, int64, int32, \r\n            uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half. \r\n            2d case:\r\n            Shape [batch, in_depth, in_height, in_channels].\r\n            3d case:\r\n            Shape [batch, in_depth, in_height, in_width, in_channels].\r\n\r\n    filter: A Tensor. Must have the same type as input. \r\n            in_channels must match between input and filter.\r\n            2d case:\r\n            Shape [batch, filter_x, filter_y, in_ch, out_ch].\r\n            3d case:\r\n            Shape [batch, filter_x, filter_y, filter_z, in_ch, out_ch] . \r\n\r\n    strides:    A list of ints that has length >= 5. 1-D tensor of length 5. \r\n                The stride of the sliding window for each dimension of input.\r\n                Must have strides[0] = strides[4] = 1.\r\n    padding:    A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\r\n\r\n    dilation_rate: Optional. \r\n                Sequence of N ints >= 1. Specifies the filter upsampling/input downsampling rate. \r\n                In the literature, the same parameter is sometimes called input stride or dilation. \r\n                The effective filter size used for the convolution will be\r\n                spatial_filter_shape + (spatial_filter_shape - 1) * (rate - 1), \r\n                obtained by inserting (dilation_rate[i]-1) zeros between consecutive elements of \r\n                the original filter in each spatial dimension i. \r\n                If any value of dilation_rate is > 1, then all values of strides must be 1.\r\n\r\n    Returns\r\n    -------\r\n            A Tensor. Has the same type as input.\r\n    '''\r\n\r\n    assert len(filter.get_shape()) == len(input.get_shape()) + 1\r\n    assert filter.get_shape()[0] == input.get_shape()[0]\r\n\r\n    split_inputs = tf.split(input, \r\n                            input.get_shape().as_list()[0], \r\n                            axis=0)\r\n    split_filters = tf.unstack(filter, \r\n                            input.get_shape().as_list()[0], \r\n                            axis=0)\r\n\r\n    output_list = []\r\n    for split_input, split_filter in zip(split_inputs,split_filters):\r\n        output_list.append(\r\n              tf.nn.convolution( split_input,\r\n                            split_filter, \r\n                            strides=strides, \r\n                            padding=padding,\r\n                            dilation_rate=dilation_rate,\r\n                            )\r\n          )\r\n    output = tf.concat(output_list, axis=0)\r\n    return output\r\n```"}