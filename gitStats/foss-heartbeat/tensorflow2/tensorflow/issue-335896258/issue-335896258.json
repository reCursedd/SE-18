{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20309", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20309/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20309/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20309/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20309", "id": 335896258, "node_id": "MDU6SXNzdWUzMzU4OTYyNTg=", "number": 20309, "title": "Very high discrepancy in memory usage and computation time in convolution operation between tf versions", "user": {"login": "moboehle", "id": 26657721, "node_id": "MDQ6VXNlcjI2NjU3NzIx", "avatar_url": "https://avatars0.githubusercontent.com/u/26657721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/moboehle", "html_url": "https://github.com/moboehle", "followers_url": "https://api.github.com/users/moboehle/followers", "following_url": "https://api.github.com/users/moboehle/following{/other_user}", "gists_url": "https://api.github.com/users/moboehle/gists{/gist_id}", "starred_url": "https://api.github.com/users/moboehle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/moboehle/subscriptions", "organizations_url": "https://api.github.com/users/moboehle/orgs", "repos_url": "https://api.github.com/users/moboehle/repos", "events_url": "https://api.github.com/users/moboehle/events{/privacy}", "received_events_url": "https://api.github.com/users/moboehle/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097543484, "node_id": "MDU6TGFiZWwxMDk3NTQzNDg0", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:runtime", "name": "comp:runtime", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-06-26T16:19:09Z", "updated_at": "2018-11-14T15:38:51Z", "closed_at": "2018-11-13T23:57:29Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</p>\n<p>I attach a small code example that should explain the issue,  but in principle it is nothing but multiple 1x1 convolutions with a conv1D layer.</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</p>\n<p>VERSION=\"16.04.3 LTS (Xenial Xerus)\"<br>\nVERSION_ID=\"16.04\"<br>\nVERSION_CODENAME=xenial</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:</p>\n<p>pip install tensorflow-gpu==1.3.0<br>\nand<br>\npip install tensorflow-gpu==1.8.0</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\nThe following tf versions were used to recreate the issue</p>\n<p>tf.GIT_VERSION = v1.8.0-0-g93bc2e2072<br>\ntf.VERSION = 1.8.0<br>\nand<br>\ntf.GIT_VERSION = b'unknown'<br>\ntf.VERSION = 1.3.0</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\nPython 3.5.5 :: Anaconda, Inc.</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nFor tf 1.3.0:<br>\nnvcc: NVIDIA (R) Cuda compiler driver<br>\nCopyright (c) 2005-2016 NVIDIA Corporation<br>\nBuilt on Tue_Jan_10_13:22:03_CST_2017<br>\nCuda compilation tools, release 8.0, V8.0.61</p>\n<p>For tf 1.8.0<br>\nnvcc: NVIDIA (R) Cuda compiler driver<br>\nCopyright (c) 2005-2017 NVIDIA Corporation<br>\nBuilt on Fri_Sep__1_21:08:03_CDT_2017<br>\nCuda compilation tools, release 9.0, V9.0.176</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nGeForce GTX 1080 Ti<br>\ntotal memory shown as 10.91GiB</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\npython 'script shown below'</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When trying to update from tensorflow 1.3.0 to 1.8.0, we noticed that memory consumption and computation time increased significantly for our networks (both &gt;10%).<br>\nWe tried to find out what was causing this issue and realized that there is a large discrepancy for memory and computation time in our 1D convolutional layers, both increasing roughly by a factor of two when going from tf 1.3 to tf 1.8.<br>\nSomething similar happens in the tf 1.3 version when changing the data format from NWC to NCW, but tf 1.8 is slow regardless of the data format.<br>\nI attach a small code snippet, which creates a network of 30 1x1 1D convolutinal layers with some arbitrary parameters for batch size, width, and number of filters.<br>\nThe network output is evaluated 10,000 times and the time spent for this is written to stdout, for different data formats and two different libraries for the 1D convolution (nn and layers).<br>\nThe following output is generated for the different tf versions.</p>\n<pre><code>tf version 1.3.0 \t order NWC \t library layers\t time 5.840178s\ntf version 1.3.0 \t order NWC \t library nn\t time 5.809642s\ntf version 1.3.0 \t order NCW \t library layers\t time 10.344764s\ntf version 1.3.0 \t order NCW \t library nn\t time 9.630965s\n\ntf version 1.8.0 \t order NWC \t library layers\t time 13.982041s\ntf version 1.8.0 \t order NWC \t library nn\t time 13.616668s\ntf version 1.8.0 \t order NCW \t library layers\t time 11.336001s\ntf version 1.8.0 \t order NCW \t library nn\t time 10.919789s\n\n</code></pre>\n<p>Tracking memory consumption for this network with nvidia-smi showed that tf 1.8 allocates around 467MiB for all different configurations, whereas tf 1.3 allocates 223MiB for NWC data format and up to 449MiB for the NCW format.</p>\n<p>Moreover, we compared the outputs of the two different data formats. They appear to be the same.</p>\n<h3>Source code / logs</h3>\n<pre><code>#############################################\n## Created by Moritz Boehle moritz@audatic.ai\n#############################################\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport os\n\n# Exclude tf logs in output for better overview\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Some arbitrary parameters for the network\nnum_filters = 16\nnum_layers = 30\nwidth = 100\nbatch_size = 20\nsteps = 10000\n\n# Allow GPU growth\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n# Below, we will compare performance of the two different data formats.\nformats = [\"NWC\", \"NCW\"]\n\n# Tested with 1.3.0 and 1.8.0\ntf_v = tf.__version__\n\n# Different libs for performing 1D convolution.\nlibs = [\"layers\", \"nn\"]\n\n# Options for the two different data formats.\nncw_opts = {\"shape\": [batch_size, num_filters, width],\n            \"data_format_layers\": \"channels_first\",\n            \"data_format_nn\": \"NCW\" if tf_v == \"1.8.0\" else \"NCHW\"}\nnwc_opts = {\"shape\": [batch_size, width, num_filters],\n            \"data_format_layers\": \"channels_last\",\n            \"data_format_nn\": \"NWC\" if tf_v == \"1.8.0\" else \"NHWC\"}\ndata_format_opts = {\"NCW\": ncw_opts, \"NWC\": nwc_opts}\n\nfor data_format in formats:\n    opts = data_format_opts[data_format]\n    for lib in libs:\n        with tf.Session(config=config) as sess:\n            with tf.device(\"/gpu:0\"):\n                shape = opts[\"shape\"]\n                format_str = opts[\"data_format_\"+lib]\n\n                # Create arbitrary input.\n                layer = tf.constant(np.random.random(shape), dtype=tf.float32)\n\n                # Create num_layers of 1D convolutions.\n                for _ in range(num_layers):\n                    if lib == \"layers\":\n                        layer = tf.layers.conv1d(layer, filters=num_filters,\n                                                 kernel_size=[1], strides=[1],\n                                                 data_format=format_str)\n                    elif lib == \"nn\":\n                        filters = tf.Variable(tf.random_normal(\n                            [1, num_filters, num_filters]))\n                        bias = tf.Variable(tf.random_normal(shape))\n                        layer = tf.nn.conv1d(layer, filters, 1, \"VALID\",\n                                             data_format=format_str)\n                        layer += bias\n                sess.run(tf.global_variables_initializer())\n\n                # Measure time to run 'steps' steps.\n                start = time.time()\n                for i in range(steps):\n                    sess.run(layer)\n                total = time.time() - start\n                print(\"tf version {tf} \\t order {order} \\t library {lib}\"\n                      \"\\t time {total:f}s\".format(tf=tf_v, total=total,\n                                                  order=data_format, lib=lib))\n\n\n</code></pre>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI attach a small code example that should explain the issue,  but in principle it is nothing but multiple 1x1 convolutions with a conv1D layer.\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n\nTensorFlow installed from (source or binary):\npip install tensorflow-gpu==1.3.0\nand\npip install tensorflow-gpu==1.8.0\n\n\nTensorFlow version (use command below):\nThe following tf versions were used to recreate the issue\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\ntf.VERSION = 1.8.0\nand\ntf.GIT_VERSION = b'unknown'\ntf.VERSION = 1.3.0\n\n\nPython version:\nPython 3.5.5 :: Anaconda, Inc.\n\n\nCUDA/cuDNN version:\nFor tf 1.3.0:\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Tue_Jan_10_13:22:03_CST_2017\nCuda compilation tools, release 8.0, V8.0.61\nFor tf 1.8.0\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\nCuda compilation tools, release 9.0, V9.0.176\n\n\nGPU model and memory:\nGeForce GTX 1080 Ti\ntotal memory shown as 10.91GiB\n\n\nExact command to reproduce:\npython 'script shown below'\n\n\nDescribe the problem\nWhen trying to update from tensorflow 1.3.0 to 1.8.0, we noticed that memory consumption and computation time increased significantly for our networks (both >10%).\nWe tried to find out what was causing this issue and realized that there is a large discrepancy for memory and computation time in our 1D convolutional layers, both increasing roughly by a factor of two when going from tf 1.3 to tf 1.8.\nSomething similar happens in the tf 1.3 version when changing the data format from NWC to NCW, but tf 1.8 is slow regardless of the data format.\nI attach a small code snippet, which creates a network of 30 1x1 1D convolutinal layers with some arbitrary parameters for batch size, width, and number of filters.\nThe network output is evaluated 10,000 times and the time spent for this is written to stdout, for different data formats and two different libraries for the 1D convolution (nn and layers).\nThe following output is generated for the different tf versions.\ntf version 1.3.0 \t order NWC \t library layers\t time 5.840178s\ntf version 1.3.0 \t order NWC \t library nn\t time 5.809642s\ntf version 1.3.0 \t order NCW \t library layers\t time 10.344764s\ntf version 1.3.0 \t order NCW \t library nn\t time 9.630965s\n\ntf version 1.8.0 \t order NWC \t library layers\t time 13.982041s\ntf version 1.8.0 \t order NWC \t library nn\t time 13.616668s\ntf version 1.8.0 \t order NCW \t library layers\t time 11.336001s\ntf version 1.8.0 \t order NCW \t library nn\t time 10.919789s\n\n\nTracking memory consumption for this network with nvidia-smi showed that tf 1.8 allocates around 467MiB for all different configurations, whereas tf 1.3 allocates 223MiB for NWC data format and up to 449MiB for the NCW format.\nMoreover, we compared the outputs of the two different data formats. They appear to be the same.\nSource code / logs\n#############################################\n## Created by Moritz Boehle moritz@audatic.ai\n#############################################\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport os\n\n# Exclude tf logs in output for better overview\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# Some arbitrary parameters for the network\nnum_filters = 16\nnum_layers = 30\nwidth = 100\nbatch_size = 20\nsteps = 10000\n\n# Allow GPU growth\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n# Below, we will compare performance of the two different data formats.\nformats = [\"NWC\", \"NCW\"]\n\n# Tested with 1.3.0 and 1.8.0\ntf_v = tf.__version__\n\n# Different libs for performing 1D convolution.\nlibs = [\"layers\", \"nn\"]\n\n# Options for the two different data formats.\nncw_opts = {\"shape\": [batch_size, num_filters, width],\n            \"data_format_layers\": \"channels_first\",\n            \"data_format_nn\": \"NCW\" if tf_v == \"1.8.0\" else \"NCHW\"}\nnwc_opts = {\"shape\": [batch_size, width, num_filters],\n            \"data_format_layers\": \"channels_last\",\n            \"data_format_nn\": \"NWC\" if tf_v == \"1.8.0\" else \"NHWC\"}\ndata_format_opts = {\"NCW\": ncw_opts, \"NWC\": nwc_opts}\n\nfor data_format in formats:\n    opts = data_format_opts[data_format]\n    for lib in libs:\n        with tf.Session(config=config) as sess:\n            with tf.device(\"/gpu:0\"):\n                shape = opts[\"shape\"]\n                format_str = opts[\"data_format_\"+lib]\n\n                # Create arbitrary input.\n                layer = tf.constant(np.random.random(shape), dtype=tf.float32)\n\n                # Create num_layers of 1D convolutions.\n                for _ in range(num_layers):\n                    if lib == \"layers\":\n                        layer = tf.layers.conv1d(layer, filters=num_filters,\n                                                 kernel_size=[1], strides=[1],\n                                                 data_format=format_str)\n                    elif lib == \"nn\":\n                        filters = tf.Variable(tf.random_normal(\n                            [1, num_filters, num_filters]))\n                        bias = tf.Variable(tf.random_normal(shape))\n                        layer = tf.nn.conv1d(layer, filters, 1, \"VALID\",\n                                             data_format=format_str)\n                        layer += bias\n                sess.run(tf.global_variables_initializer())\n\n                # Measure time to run 'steps' steps.\n                start = time.time()\n                for i in range(steps):\n                    sess.run(layer)\n                total = time.time() - start\n                print(\"tf version {tf} \\t order {order} \\t library {lib}\"\n                      \"\\t time {total:f}s\".format(tf=tf_v, total=total,\n                                                  order=data_format, lib=lib))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\n    I attach a small code example that should explain the issue,  but in principle it is nothing but multiple 1x1 convolutions with a conv1D layer.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\n    VERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\n    VERSION_ID=\"16.04\"\r\n    VERSION_CODENAME=xenial\r\n\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\n    pip install tensorflow-gpu==1.3.0\r\n    and\r\n    pip install tensorflow-gpu==1.8.0\r\n\r\n- **TensorFlow version (use command below)**:\r\nThe following tf versions were used to recreate the issue\r\n    \r\n    tf.GIT_VERSION = v1.8.0-0-g93bc2e2072 \r\n    tf.VERSION = 1.8.0\r\n    and\r\n    tf.GIT_VERSION = b'unknown' \r\n    tf.VERSION = 1.3.0\r\n\r\n- **Python version**:\r\nPython 3.5.5 :: Anaconda, Inc.\r\n- **CUDA/cuDNN version**:\r\n    For tf 1.3.0:\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2016 NVIDIA Corporation\r\n    Built on Tue_Jan_10_13:22:03_CST_2017\r\n    Cuda compilation tools, release 8.0, V8.0.61\r\n    \r\n    For tf 1.8.0\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2017 NVIDIA Corporation\r\n    Built on Fri_Sep__1_21:08:03_CDT_2017\r\n    Cuda compilation tools, release 9.0, V9.0.176\r\n\r\n- **GPU model and memory**:\r\n    GeForce GTX 1080 Ti\r\n    total memory shown as 10.91GiB\r\n- **Exact command to reproduce**:\r\n    python 'script shown below'\r\n\r\n### Describe the problem\r\nWhen trying to update from tensorflow 1.3.0 to 1.8.0, we noticed that memory consumption and computation time increased significantly for our networks (both >10%). \r\nWe tried to find out what was causing this issue and realized that there is a large discrepancy for memory and computation time in our 1D convolutional layers, both increasing roughly by a factor of two when going from tf 1.3 to tf 1.8. \r\nSomething similar happens in the tf 1.3 version when changing the data format from NWC to NCW, but tf 1.8 is slow regardless of the data format.\r\nI attach a small code snippet, which creates a network of 30 1x1 1D convolutinal layers with some arbitrary parameters for batch size, width, and number of filters. \r\nThe network output is evaluated 10,000 times and the time spent for this is written to stdout, for different data formats and two different libraries for the 1D convolution (nn and layers). \r\nThe following output is generated for the different tf versions. \r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 5.840178s\r\ntf version 1.3.0 \t order NWC \t library nn\t time 5.809642s\r\ntf version 1.3.0 \t order NCW \t library layers\t time 10.344764s\r\ntf version 1.3.0 \t order NCW \t library nn\t time 9.630965s\r\n\r\ntf version 1.8.0 \t order NWC \t library layers\t time 13.982041s\r\ntf version 1.8.0 \t order NWC \t library nn\t time 13.616668s\r\ntf version 1.8.0 \t order NCW \t library layers\t time 11.336001s\r\ntf version 1.8.0 \t order NCW \t library nn\t time 10.919789s\r\n\r\n~~~~\r\n\r\nTracking memory consumption for this network with nvidia-smi showed that tf 1.8 allocates around 467MiB for all different configurations, whereas tf 1.3 allocates 223MiB for NWC data format and up to 449MiB for the NCW format.\r\n\r\nMoreover, we compared the outputs of the two different data formats. They appear to be the same.\r\n\r\n### Source code / logs\r\n~~~~\r\n#############################################\r\n## Created by Moritz Boehle moritz@audatic.ai\r\n#############################################\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\n# Exclude tf logs in output for better overview\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\n# Some arbitrary parameters for the network\r\nnum_filters = 16\r\nnum_layers = 30\r\nwidth = 100\r\nbatch_size = 20\r\nsteps = 10000\r\n\r\n# Allow GPU growth\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\n# Below, we will compare performance of the two different data formats.\r\nformats = [\"NWC\", \"NCW\"]\r\n\r\n# Tested with 1.3.0 and 1.8.0\r\ntf_v = tf.__version__\r\n\r\n# Different libs for performing 1D convolution.\r\nlibs = [\"layers\", \"nn\"]\r\n\r\n# Options for the two different data formats.\r\nncw_opts = {\"shape\": [batch_size, num_filters, width],\r\n            \"data_format_layers\": \"channels_first\",\r\n            \"data_format_nn\": \"NCW\" if tf_v == \"1.8.0\" else \"NCHW\"}\r\nnwc_opts = {\"shape\": [batch_size, width, num_filters],\r\n            \"data_format_layers\": \"channels_last\",\r\n            \"data_format_nn\": \"NWC\" if tf_v == \"1.8.0\" else \"NHWC\"}\r\ndata_format_opts = {\"NCW\": ncw_opts, \"NWC\": nwc_opts}\r\n\r\nfor data_format in formats:\r\n    opts = data_format_opts[data_format]\r\n    for lib in libs:\r\n        with tf.Session(config=config) as sess:\r\n            with tf.device(\"/gpu:0\"):\r\n                shape = opts[\"shape\"]\r\n                format_str = opts[\"data_format_\"+lib]\r\n\r\n                # Create arbitrary input.\r\n                layer = tf.constant(np.random.random(shape), dtype=tf.float32)\r\n\r\n                # Create num_layers of 1D convolutions.\r\n                for _ in range(num_layers):\r\n                    if lib == \"layers\":\r\n                        layer = tf.layers.conv1d(layer, filters=num_filters,\r\n                                                 kernel_size=[1], strides=[1],\r\n                                                 data_format=format_str)\r\n                    elif lib == \"nn\":\r\n                        filters = tf.Variable(tf.random_normal(\r\n                            [1, num_filters, num_filters]))\r\n                        bias = tf.Variable(tf.random_normal(shape))\r\n                        layer = tf.nn.conv1d(layer, filters, 1, \"VALID\",\r\n                                             data_format=format_str)\r\n                        layer += bias\r\n                sess.run(tf.global_variables_initializer())\r\n\r\n                # Measure time to run 'steps' steps.\r\n                start = time.time()\r\n                for i in range(steps):\r\n                    sess.run(layer)\r\n                total = time.time() - start\r\n                print(\"tf version {tf} \\t order {order} \\t library {lib}\"\r\n                      \"\\t time {total:f}s\".format(tf=tf_v, total=total,\r\n                                                  order=data_format, lib=lib))\r\n\r\n\r\n~~~~\r\n"}