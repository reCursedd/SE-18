{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/400517924", "html_url": "https://github.com/tensorflow/tensorflow/issues/20309#issuecomment-400517924", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20309", "id": 400517924, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDUxNzkyNA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-27T01:56:45Z", "updated_at": "2018-06-27T01:56:45Z", "author_association": "MEMBER", "body_html": "<p>Thanks for the very detailed report <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26657721\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/moboehle\">@moboehle</a> . Simply running your program on various versions (and looking at the timings, I didn't focus on memory yet), it seems this happened between versions 1.6 and 1.7:</p>\n<pre><code>tf version 1.3.0         order NWC       library layers  time 8.185683s\ntf version 1.3.0         order NWC       library nn      time 7.606029s\ntf version 1.3.0         order NCW       library layers  time 15.392093s\ntf version 1.3.0         order NCW       library nn      time 13.677395s\n\ntf version 1.4.0         order NWC       library layers  time 8.245580s\ntf version 1.4.0         order NWC       library nn      time 7.654847s\ntf version 1.4.0         order NCW       library layers  time 15.371269s\ntf version 1.4.0         order NCW       library nn      time 14.107784s\n\ntf version 1.5.0         order NWC       library layers  time 8.871874s\ntf version 1.5.0         order NWC       library nn      time 8.447964s\ntf version 1.5.0         order NCW       library layers  time 16.345905s\ntf version 1.5.0         order NCW       library nn      time 14.314268s\n\ntf version 1.6.0         order NWC       library layers  time 8.817178s\ntf version 1.6.0         order NWC       library nn      time 7.883244s\ntf version 1.6.0         order NCW       library layers  time 16.071247s\ntf version 1.6.0         order NCW       library nn      time 14.459396s\n\ntf version 1.7.0         order NWC       library layers  time 20.263543s\ntf version 1.7.0         order NWC       library nn      time 19.896915s\ntf version 1.7.0         order NCW       library layers  time 15.326541s\ntf version 1.7.0         order NCW       library nn      time 16.310148s\n\ntf version 1.8.0         order NWC       library layers  time 20.150042s\ntf version 1.8.0         order NWC       library nn      time 20.640730s\ntf version 1.8.0         order NCW       library layers  time 16.102693s\ntf version 1.8.0         order NCW       library nn      time 15.144002s\n</code></pre>\n<p>(I was using the release docker images).</p>\n<p>I believe one of the changes between 1.6 and 1.7 was that the layout optimizer was turned on by default<br>\n(i.e., it had to be <a href=\"https://github.com/tensorflow/tensorflow/blob/080d59b76ca27b184f0fce605db7f5339ea5a8cf/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L100\">turned on explicitly in 1.6</a> and starting <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L117\">1.7 had to be turned explicitly off</a>). Since <code>tf.nn.conv1d</code> is actually implemented as <a href=\"https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/nn_ops.py#L2458\">a wrapper over the <code>conv2d</code> operation</a>, this \"optimization\" kicks in but is probably not actually helpful :)</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16907534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rmlarsen\">@rmlarsen</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1034716\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhangyaobit\">@zhangyaobit</a> - Please do take a look. Perhaps <code>LayoutOptimizer</code> needs to  be made aware of 1D convolutions and handle them differently?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26657721\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/moboehle\">@moboehle</a> - In the mean time, you could work around this problem by explicitly turning off the layout optimizer using something like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> google.protobuf <span class=\"pl-k\">import</span> text_format\nconfig <span class=\"pl-k\">=</span> <span class=\"pl-k\">from</span> google.protobuf <span class=\"pl-k\">import</span> text_format\nconfig <span class=\"pl-k\">=</span> text_format.Parse(<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">graph_options {</span>\n<span class=\"pl-s\">  rewrite_options {</span>\n<span class=\"pl-s\">    layout_optimizer: OFF</span>\n<span class=\"pl-s\">  }</span>\n<span class=\"pl-s\">}</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>, tf.ConfigProto())</pre></div>\n<p>If I do that, I see the following numbers in 1.7 and 1.8:</p>\n<pre><code>tf version 1.7.0         order NWC       library layers  time 8.370198s\ntf version 1.7.0         order NWC       library nn      time 7.873047s                      \ntf version 1.7.0         order NCW       library layers  time 15.633171s\ntf version 1.7.0         order NCW       library nn      time 13.691546s \n\ntf version 1.8.0         order NWC       library layers  time 9.071059s\ntf version 1.8.0         order NWC       library nn      time 8.561103s\ntf version 1.8.0         order NCW       library layers  time 16.833199s\ntf version 1.8.0         order NCW       library nn      time 14.826180s\n</code></pre>\n<p>Hopefully this workaround tides you over for the short term and we'll look into an appropriate fix for how the layout optimizer handles 1D convolutions. I suspect the memory increase is also explained by the added transposition operation required for the layout change.</p>\n<p>Thanks!</p>", "body_text": "Thanks for the very detailed report @moboehle . Simply running your program on various versions (and looking at the timings, I didn't focus on memory yet), it seems this happened between versions 1.6 and 1.7:\ntf version 1.3.0         order NWC       library layers  time 8.185683s\ntf version 1.3.0         order NWC       library nn      time 7.606029s\ntf version 1.3.0         order NCW       library layers  time 15.392093s\ntf version 1.3.0         order NCW       library nn      time 13.677395s\n\ntf version 1.4.0         order NWC       library layers  time 8.245580s\ntf version 1.4.0         order NWC       library nn      time 7.654847s\ntf version 1.4.0         order NCW       library layers  time 15.371269s\ntf version 1.4.0         order NCW       library nn      time 14.107784s\n\ntf version 1.5.0         order NWC       library layers  time 8.871874s\ntf version 1.5.0         order NWC       library nn      time 8.447964s\ntf version 1.5.0         order NCW       library layers  time 16.345905s\ntf version 1.5.0         order NCW       library nn      time 14.314268s\n\ntf version 1.6.0         order NWC       library layers  time 8.817178s\ntf version 1.6.0         order NWC       library nn      time 7.883244s\ntf version 1.6.0         order NCW       library layers  time 16.071247s\ntf version 1.6.0         order NCW       library nn      time 14.459396s\n\ntf version 1.7.0         order NWC       library layers  time 20.263543s\ntf version 1.7.0         order NWC       library nn      time 19.896915s\ntf version 1.7.0         order NCW       library layers  time 15.326541s\ntf version 1.7.0         order NCW       library nn      time 16.310148s\n\ntf version 1.8.0         order NWC       library layers  time 20.150042s\ntf version 1.8.0         order NWC       library nn      time 20.640730s\ntf version 1.8.0         order NCW       library layers  time 16.102693s\ntf version 1.8.0         order NCW       library nn      time 15.144002s\n\n(I was using the release docker images).\nI believe one of the changes between 1.6 and 1.7 was that the layout optimizer was turned on by default\n(i.e., it had to be turned on explicitly in 1.6 and starting 1.7 had to be turned explicitly off). Since tf.nn.conv1d is actually implemented as a wrapper over the conv2d operation, this \"optimization\" kicks in but is probably not actually helpful :)\n@rmlarsen @benoitsteiner @zhangyaobit - Please do take a look. Perhaps LayoutOptimizer needs to  be made aware of 1D convolutions and handle them differently?\n@moboehle - In the mean time, you could work around this problem by explicitly turning off the layout optimizer using something like:\nfrom google.protobuf import text_format\nconfig = from google.protobuf import text_format\nconfig = text_format.Parse(\"\"\"\ngraph_options {\n  rewrite_options {\n    layout_optimizer: OFF\n  }\n}\n\"\"\", tf.ConfigProto())\nIf I do that, I see the following numbers in 1.7 and 1.8:\ntf version 1.7.0         order NWC       library layers  time 8.370198s\ntf version 1.7.0         order NWC       library nn      time 7.873047s                      \ntf version 1.7.0         order NCW       library layers  time 15.633171s\ntf version 1.7.0         order NCW       library nn      time 13.691546s \n\ntf version 1.8.0         order NWC       library layers  time 9.071059s\ntf version 1.8.0         order NWC       library nn      time 8.561103s\ntf version 1.8.0         order NCW       library layers  time 16.833199s\ntf version 1.8.0         order NCW       library nn      time 14.826180s\n\nHopefully this workaround tides you over for the short term and we'll look into an appropriate fix for how the layout optimizer handles 1D convolutions. I suspect the memory increase is also explained by the added transposition operation required for the layout change.\nThanks!", "body": "Thanks for the very detailed report @moboehle . Simply running your program on various versions (and looking at the timings, I didn't focus on memory yet), it seems this happened between versions 1.6 and 1.7:\r\n\r\n```\r\ntf version 1.3.0         order NWC       library layers  time 8.185683s\r\ntf version 1.3.0         order NWC       library nn      time 7.606029s\r\ntf version 1.3.0         order NCW       library layers  time 15.392093s\r\ntf version 1.3.0         order NCW       library nn      time 13.677395s\r\n\r\ntf version 1.4.0         order NWC       library layers  time 8.245580s\r\ntf version 1.4.0         order NWC       library nn      time 7.654847s\r\ntf version 1.4.0         order NCW       library layers  time 15.371269s\r\ntf version 1.4.0         order NCW       library nn      time 14.107784s\r\n\r\ntf version 1.5.0         order NWC       library layers  time 8.871874s\r\ntf version 1.5.0         order NWC       library nn      time 8.447964s\r\ntf version 1.5.0         order NCW       library layers  time 16.345905s\r\ntf version 1.5.0         order NCW       library nn      time 14.314268s\r\n\r\ntf version 1.6.0         order NWC       library layers  time 8.817178s\r\ntf version 1.6.0         order NWC       library nn      time 7.883244s\r\ntf version 1.6.0         order NCW       library layers  time 16.071247s\r\ntf version 1.6.0         order NCW       library nn      time 14.459396s\r\n\r\ntf version 1.7.0         order NWC       library layers  time 20.263543s\r\ntf version 1.7.0         order NWC       library nn      time 19.896915s\r\ntf version 1.7.0         order NCW       library layers  time 15.326541s\r\ntf version 1.7.0         order NCW       library nn      time 16.310148s\r\n\r\ntf version 1.8.0         order NWC       library layers  time 20.150042s\r\ntf version 1.8.0         order NWC       library nn      time 20.640730s\r\ntf version 1.8.0         order NCW       library layers  time 16.102693s\r\ntf version 1.8.0         order NCW       library nn      time 15.144002s\r\n```\r\n(I was using the release docker images).\r\n\r\nI believe one of the changes between 1.6 and 1.7 was that the layout optimizer was turned on by default\r\n(i.e., it had to be [turned on explicitly in 1.6](https://github.com/tensorflow/tensorflow/blob/080d59b76ca27b184f0fce605db7f5339ea5a8cf/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L100) and starting [1.7 had to be turned explicitly off](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L117)). Since `tf.nn.conv1d` is actually implemented as [a wrapper over the `conv2d` operation](https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/nn_ops.py#L2458), this \"optimization\" kicks in but is probably not actually helpful :)\r\n\r\n@rmlarsen @benoitsteiner @zhangyaobit - Please do take a look. Perhaps `LayoutOptimizer` needs to  be made aware of 1D convolutions and handle them differently?\r\n\r\n@moboehle - In the mean time, you could work around this problem by explicitly turning off the layout optimizer using something like:\r\n\r\n```python\r\nfrom google.protobuf import text_format\r\nconfig = from google.protobuf import text_format\r\nconfig = text_format.Parse(\"\"\"\r\ngraph_options {\r\n  rewrite_options {\r\n    layout_optimizer: OFF\r\n  }\r\n}\r\n\"\"\", tf.ConfigProto())\r\n```\r\n\r\nIf I do that, I see the following numbers in 1.7 and 1.8:\r\n```\r\ntf version 1.7.0         order NWC       library layers  time 8.370198s\r\ntf version 1.7.0         order NWC       library nn      time 7.873047s                      \r\ntf version 1.7.0         order NCW       library layers  time 15.633171s\r\ntf version 1.7.0         order NCW       library nn      time 13.691546s \r\n\r\ntf version 1.8.0         order NWC       library layers  time 9.071059s\r\ntf version 1.8.0         order NWC       library nn      time 8.561103s\r\ntf version 1.8.0         order NCW       library layers  time 16.833199s\r\ntf version 1.8.0         order NCW       library nn      time 14.826180s\r\n```\r\n\r\nHopefully this workaround tides you over for the short term and we'll look into an appropriate fix for how the layout optimizer handles 1D convolutions. I suspect the memory increase is also explained by the added transposition operation required for the layout change.\r\n\r\nThanks!"}