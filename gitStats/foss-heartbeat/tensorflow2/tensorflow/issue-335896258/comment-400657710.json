{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/400657710", "html_url": "https://github.com/tensorflow/tensorflow/issues/20309#issuecomment-400657710", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20309", "id": 400657710, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDY1NzcxMA==", "user": {"login": "moboehle", "id": 26657721, "node_id": "MDQ6VXNlcjI2NjU3NzIx", "avatar_url": "https://avatars0.githubusercontent.com/u/26657721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/moboehle", "html_url": "https://github.com/moboehle", "followers_url": "https://api.github.com/users/moboehle/followers", "following_url": "https://api.github.com/users/moboehle/following{/other_user}", "gists_url": "https://api.github.com/users/moboehle/gists{/gist_id}", "starred_url": "https://api.github.com/users/moboehle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/moboehle/subscriptions", "organizations_url": "https://api.github.com/users/moboehle/orgs", "repos_url": "https://api.github.com/users/moboehle/repos", "events_url": "https://api.github.com/users/moboehle/events{/privacy}", "received_events_url": "https://api.github.com/users/moboehle/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-27T12:41:37Z", "updated_at": "2018-06-27T12:41:37Z", "author_association": "NONE", "body_html": "<p>Sorry for going back and forth, but my previous test of the conv2D was insufficiently thorough as it seems. For conv2D with a large height, the layout optimizer indeed boosts performance, as seen in the following output</p>\n<pre><code>tf version 1.3.0 \t order NWC \t library layers\t time 18.740896s\ntf version 1.5.0 \t order NWC \t library layers\t time 17.115397s\ntf version 1.8.0 \t order NWC \t library layers\t time 17.450639s\n</code></pre>\n<p>Here, I only tested the data format NWC for an input of height 20 for the different tensorflow versions, and I explicitly turned on the optimizer also for tf 1.5.</p>\n<p>The issue with the optimizer arises, once the height becomes too small, for example with height of 2 I get the following results:</p>\n<pre><code>tf version 1.3.0 \t order NWC \t library layers\t time 4.977753s\ntf version 1.5.0 \t order NWC \t library layers\t time 9.492385s\ntf version 1.8.0 \t order NWC \t library layers\t time 9.792400s\n</code></pre>\n<p>which is what I remembered seeing when first testing this.</p>\n<p>Moreover, while for large heights the computation time is indeed decreased, it should be noted that the memory consumption suffers heavily from this optimization, which could be more costly than computation time for some users. E.g. for an input with width and height of 200, memory consumption almost triples(!) from 479MiB to 1359MiB in order to achieve a time speed-up of about 18% in the following output</p>\n<pre><code>tf version 1.3.0 \t order NWC \t library layers\t time 31.275929s\ntf version 1.5.0 \t order NWC \t library layers\t time 25.768378s\ntf version 1.8.0 \t order NWC \t library layers\t time 26.140614s\n</code></pre>", "body_text": "Sorry for going back and forth, but my previous test of the conv2D was insufficiently thorough as it seems. For conv2D with a large height, the layout optimizer indeed boosts performance, as seen in the following output\ntf version 1.3.0 \t order NWC \t library layers\t time 18.740896s\ntf version 1.5.0 \t order NWC \t library layers\t time 17.115397s\ntf version 1.8.0 \t order NWC \t library layers\t time 17.450639s\n\nHere, I only tested the data format NWC for an input of height 20 for the different tensorflow versions, and I explicitly turned on the optimizer also for tf 1.5.\nThe issue with the optimizer arises, once the height becomes too small, for example with height of 2 I get the following results:\ntf version 1.3.0 \t order NWC \t library layers\t time 4.977753s\ntf version 1.5.0 \t order NWC \t library layers\t time 9.492385s\ntf version 1.8.0 \t order NWC \t library layers\t time 9.792400s\n\nwhich is what I remembered seeing when first testing this.\nMoreover, while for large heights the computation time is indeed decreased, it should be noted that the memory consumption suffers heavily from this optimization, which could be more costly than computation time for some users. E.g. for an input with width and height of 200, memory consumption almost triples(!) from 479MiB to 1359MiB in order to achieve a time speed-up of about 18% in the following output\ntf version 1.3.0 \t order NWC \t library layers\t time 31.275929s\ntf version 1.5.0 \t order NWC \t library layers\t time 25.768378s\ntf version 1.8.0 \t order NWC \t library layers\t time 26.140614s", "body": "Sorry for going back and forth, but my previous test of the conv2D was insufficiently thorough as it seems. For conv2D with a large height, the layout optimizer indeed boosts performance, as seen in the following output \r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 18.740896s\r\ntf version 1.5.0 \t order NWC \t library layers\t time 17.115397s\r\ntf version 1.8.0 \t order NWC \t library layers\t time 17.450639s\r\n~~~~\r\nHere, I only tested the data format NWC for an input of height 20 for the different tensorflow versions, and I explicitly turned on the optimizer also for tf 1.5.\r\n\r\nThe issue with the optimizer arises, once the height becomes too small, for example with height of 2 I get the following results:\r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 4.977753s\r\ntf version 1.5.0 \t order NWC \t library layers\t time 9.492385s\r\ntf version 1.8.0 \t order NWC \t library layers\t time 9.792400s\r\n~~~~\r\nwhich is what I remembered seeing when first testing this.\r\n\r\nMoreover, while for large heights the computation time is indeed decreased, it should be noted that the memory consumption suffers heavily from this optimization, which could be more costly than computation time for some users. E.g. for an input with width and height of 200, memory consumption almost triples(!) from 479MiB to 1359MiB in order to achieve a time speed-up of about 18% in the following output\r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 31.275929s\r\ntf version 1.5.0 \t order NWC \t library layers\t time 25.768378s\r\ntf version 1.8.0 \t order NWC \t library layers\t time 26.140614s\r\n~~~~\r\n"}