{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10054", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10054/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10054/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10054/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10054", "id": 230112811, "node_id": "MDU6SXNzdWUyMzAxMTI4MTE=", "number": 10054, "title": "Inconsistent functionality from tf.Graph with simple linear regression (not caused from random-seed-state)", "user": {"login": "jolespin", "id": 9061708, "node_id": "MDQ6VXNlcjkwNjE3MDg=", "avatar_url": "https://avatars1.githubusercontent.com/u/9061708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jolespin", "html_url": "https://github.com/jolespin", "followers_url": "https://api.github.com/users/jolespin/followers", "following_url": "https://api.github.com/users/jolespin/following{/other_user}", "gists_url": "https://api.github.com/users/jolespin/gists{/gist_id}", "starred_url": "https://api.github.com/users/jolespin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jolespin/subscriptions", "organizations_url": "https://api.github.com/users/jolespin/orgs", "repos_url": "https://api.github.com/users/jolespin/repos", "events_url": "https://api.github.com/users/jolespin/events{/privacy}", "received_events_url": "https://api.github.com/users/jolespin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-05-19T23:35:50Z", "updated_at": "2017-06-30T18:16:06Z", "closed_at": "2017-06-30T18:16:06Z", "author_association": "NONE", "body_html": "<p>I am getting very inconsistent behavior with my <code>tf.Graph</code> objects and I can't explain why it's happening.  I'm running this in a Jupyter notebook.  Could this affect it at all?</p>\n<p>My versions:</p>\n<pre><code>tf.__version__\n'1.0.1'\nsys.version\n'3.6.1 |Anaconda custom (x86_64)| (default, Mar 22 2017, 19:25:17) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'\n</code></pre>\n<pre><code>from sklearn.datasets import *\nimport multiprocessing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf_max_threads = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count())\ndef iris_data():\n    # Iris dataset\n    X = pd.DataFrame(load_iris().data,\n                           index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])],\n                           columns = [x.split(\" (cm)\")[0].replace(\" \",\"_\") for x in load_iris().feature_names])\n\n    y = pd.Series(load_iris().target,\n                           index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])],\n                           name = \"Species\")\n    color_list = [{0:\"red\",1:\"green\",2:\"blue\"}[x] for x in y]\n    cmap = {k:v for k,v in zip(X.index, color_list)}\n    return (X, y, cmap)\n\n# Data\nX,y,c = iris_data()\nx = X[\"petal_width\"].as_matrix()\ny = X[\"sepal_length\"].as_matrix()\nn = 50\n\n# Containers\nlasso_data = list()\nA_data = list()\nb_data = list()\n\n# Graph\nG_3_78 = tf.Graph()\n\n# Iterations\nn_iter = 1500\n\n# Functions\ndef lasso_penalty(coef, alpha=0.9):\n    lasso_param = tf.constant(alpha)\n    heavyside_step = tf.truediv(1.0, tf.add(1.0, tf.exp(tf.multiply(-100.0, tf.subtract(coef, lasso_param)))))\n    regularization_param = tf.multiply(heavyside_step, 99.0)\n    return regularization_param\ndef l2(y, y_model):\n    return tf.square(y - y_model)\n\n# Build Graph\nwith G_3_78.as_default():\n    # Placeholders\n    pH_x_petal_width = tf.placeholder(tf.float32, shape=[None,1], name=\"pH_x_petal_width\")\n    pH_y_hat = tf.placeholder(tf.float32, shape=[None,1])\n    \n    # Model\n    A = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=\"A\")\n    b = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=\"b\")\n    model = tf.add(tf.matmul(pH_x_petal_width, A), b)\n    \n    # Loss\n    loss_lasso = tf.add(tf.reduce_mean(l2(pH_y_hat, model)), lasso_penalty(A, 0.9))\n    \n    with tf.Session(graph=G_3_78, config=tf_max_threads) as sess:\n        sess.run(tf.global_variables_initializer())\n        # Optimizer\n        op = tf.train.GradientDescentOptimizer(0.001)\n        train_step = op.minimize(loss_lasso)\n        # Train linear model \n        for i in range(n_iter):\n            idx_random = np.random.RandomState(i).choice(y.shape[0], size=n)\n            tr_x = x[idx_random].reshape(-1,1)\n            tr_y = y[idx_random].reshape(-1,1)\n            sess.run(train_step, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})\n            # Iterations\n            A_iter = sess.run(A)[0][0]\n            b_iter = sess.run(b)[0][0]\n            \n            lasso_iter = sess.run(loss_lasso, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})[0][0]\n            lasso_data.append(lasso_iter)\n            A_data.append(A_iter)\n            b_data.append(b_iter)\n            # Log\n            if (i + 1) % 500 == 0:\n                print(f\"Step #{i + 1}\\n\\tA = {A_iter}\")\n                print(f\"\\tb = {b_iter}\")\n                print(f\"\\tLoss = {lasso_iter}\")\n                print()        \n# Plot path\nwith plt.style.context(\"seaborn-white\"):\n    fig, ax = plt.subplots(nrows=3, figsize=(6,6))\n    pd.Series(lasso_data,).plot(ax=ax[0], label=\"loss(lasso)\", legend=True)\n    pd.Series(A_data,).plot(ax=ax[1], color=\"red\", label=\"A\", legend=True)\n    pd.Series(b_data,).plot(ax=ax[2], color=\"black\", label=\"b\", legend=True)\n    fig.suptitle(\"training-process\", fontsize=15, y=0.95)\n\n    # Plot linear separation\nwith plt.style.context(\"seaborn-white\"):\n    fig, ax = plt.subplots(figsize=(5,5))\n    ax.scatter(x, y, c=X.index.map(lambda x:c[x]))\n    x_lims = ax.get_xlim()\n#     y_lims = ax.get_ylim()\n    for i in range(n_iter):\n        ax.plot(x, x*A_data[i] + b_data[i], alpha=i/(n_iter*100), color=\"black\")\n    ax.set_xlim(x_lims)\n#     ax.set_ylim(y_lims)\n</code></pre>\n<p>I will run this code block and get the desired results:</p>\n<pre><code>Step #500\n\tA = 0.8209055066108704\n\tb = 3.338909149169922\n\tLoss = 2.8257622718811035\n\nStep #1000\n\tA = 0.812068521976471\n\tb = 4.303615570068359\n\tLoss = 0.5661464929580688\n\nStep #1500\n\tA = 0.8036581873893738\n\tb = 4.6621904373168945\n\tLoss = 0.23930419981479645\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/9061708/26270528/6378cea2-3cb0-11e7-9a7c-3ab10850bd5d.png\"><img src=\"https://cloud.githubusercontent.com/assets/9061708/26270528/6378cea2-3cb0-11e7-9a7c-3ab10850bd5d.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>and then I will run the exact same code block again:</p>\n<pre><code>Step #500\n\tA = nan\n\tb = nan\n\tLoss = nan\n\nStep #1000\n\tA = nan\n\tb = nan\n\tLoss = nan\n\nStep #1500\n\tA = nan\n\tb = nan\n\tLoss = nan\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/9061708/26270550/946e57ca-3cb0-11e7-877a-0674c45ddd66.png\"><img src=\"https://cloud.githubusercontent.com/assets/9061708/26270550/946e57ca-3cb0-11e7-877a-0674c45ddd66.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>and then one more time:</p>\n<pre><code>Step #500\n\tA = 2.6116130352020264\n\tb = 1.91126549243927\n\tLoss = 101.70783233642578\n\nStep #1000\n\tA = 2.3814141750335693\n\tb = 2.4998815059661865\n\tLoss = 100.85863494873047\n\nStep #1500\n\tA = 2.11511492729187\n\tb = 2.9327337741851807\n\tLoss = 99.995849609375\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/9061708/26270564/a97558ee-3cb0-11e7-8bad-85cb2f5f4f1c.png\"><img src=\"https://cloud.githubusercontent.com/assets/9061708/26270564/a97558ee-3cb0-11e7-8bad-85cb2f5f4f1c.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>", "body_text": "I am getting very inconsistent behavior with my tf.Graph objects and I can't explain why it's happening.  I'm running this in a Jupyter notebook.  Could this affect it at all?\nMy versions:\ntf.__version__\n'1.0.1'\nsys.version\n'3.6.1 |Anaconda custom (x86_64)| (default, Mar 22 2017, 19:25:17) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'\n\nfrom sklearn.datasets import *\nimport multiprocessing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf_max_threads = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count())\ndef iris_data():\n    # Iris dataset\n    X = pd.DataFrame(load_iris().data,\n                           index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])],\n                           columns = [x.split(\" (cm)\")[0].replace(\" \",\"_\") for x in load_iris().feature_names])\n\n    y = pd.Series(load_iris().target,\n                           index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])],\n                           name = \"Species\")\n    color_list = [{0:\"red\",1:\"green\",2:\"blue\"}[x] for x in y]\n    cmap = {k:v for k,v in zip(X.index, color_list)}\n    return (X, y, cmap)\n\n# Data\nX,y,c = iris_data()\nx = X[\"petal_width\"].as_matrix()\ny = X[\"sepal_length\"].as_matrix()\nn = 50\n\n# Containers\nlasso_data = list()\nA_data = list()\nb_data = list()\n\n# Graph\nG_3_78 = tf.Graph()\n\n# Iterations\nn_iter = 1500\n\n# Functions\ndef lasso_penalty(coef, alpha=0.9):\n    lasso_param = tf.constant(alpha)\n    heavyside_step = tf.truediv(1.0, tf.add(1.0, tf.exp(tf.multiply(-100.0, tf.subtract(coef, lasso_param)))))\n    regularization_param = tf.multiply(heavyside_step, 99.0)\n    return regularization_param\ndef l2(y, y_model):\n    return tf.square(y - y_model)\n\n# Build Graph\nwith G_3_78.as_default():\n    # Placeholders\n    pH_x_petal_width = tf.placeholder(tf.float32, shape=[None,1], name=\"pH_x_petal_width\")\n    pH_y_hat = tf.placeholder(tf.float32, shape=[None,1])\n    \n    # Model\n    A = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=\"A\")\n    b = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=\"b\")\n    model = tf.add(tf.matmul(pH_x_petal_width, A), b)\n    \n    # Loss\n    loss_lasso = tf.add(tf.reduce_mean(l2(pH_y_hat, model)), lasso_penalty(A, 0.9))\n    \n    with tf.Session(graph=G_3_78, config=tf_max_threads) as sess:\n        sess.run(tf.global_variables_initializer())\n        # Optimizer\n        op = tf.train.GradientDescentOptimizer(0.001)\n        train_step = op.minimize(loss_lasso)\n        # Train linear model \n        for i in range(n_iter):\n            idx_random = np.random.RandomState(i).choice(y.shape[0], size=n)\n            tr_x = x[idx_random].reshape(-1,1)\n            tr_y = y[idx_random].reshape(-1,1)\n            sess.run(train_step, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})\n            # Iterations\n            A_iter = sess.run(A)[0][0]\n            b_iter = sess.run(b)[0][0]\n            \n            lasso_iter = sess.run(loss_lasso, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})[0][0]\n            lasso_data.append(lasso_iter)\n            A_data.append(A_iter)\n            b_data.append(b_iter)\n            # Log\n            if (i + 1) % 500 == 0:\n                print(f\"Step #{i + 1}\\n\\tA = {A_iter}\")\n                print(f\"\\tb = {b_iter}\")\n                print(f\"\\tLoss = {lasso_iter}\")\n                print()        \n# Plot path\nwith plt.style.context(\"seaborn-white\"):\n    fig, ax = plt.subplots(nrows=3, figsize=(6,6))\n    pd.Series(lasso_data,).plot(ax=ax[0], label=\"loss(lasso)\", legend=True)\n    pd.Series(A_data,).plot(ax=ax[1], color=\"red\", label=\"A\", legend=True)\n    pd.Series(b_data,).plot(ax=ax[2], color=\"black\", label=\"b\", legend=True)\n    fig.suptitle(\"training-process\", fontsize=15, y=0.95)\n\n    # Plot linear separation\nwith plt.style.context(\"seaborn-white\"):\n    fig, ax = plt.subplots(figsize=(5,5))\n    ax.scatter(x, y, c=X.index.map(lambda x:c[x]))\n    x_lims = ax.get_xlim()\n#     y_lims = ax.get_ylim()\n    for i in range(n_iter):\n        ax.plot(x, x*A_data[i] + b_data[i], alpha=i/(n_iter*100), color=\"black\")\n    ax.set_xlim(x_lims)\n#     ax.set_ylim(y_lims)\n\nI will run this code block and get the desired results:\nStep #500\n\tA = 0.8209055066108704\n\tb = 3.338909149169922\n\tLoss = 2.8257622718811035\n\nStep #1000\n\tA = 0.812068521976471\n\tb = 4.303615570068359\n\tLoss = 0.5661464929580688\n\nStep #1500\n\tA = 0.8036581873893738\n\tb = 4.6621904373168945\n\tLoss = 0.23930419981479645\n\n\nand then I will run the exact same code block again:\nStep #500\n\tA = nan\n\tb = nan\n\tLoss = nan\n\nStep #1000\n\tA = nan\n\tb = nan\n\tLoss = nan\n\nStep #1500\n\tA = nan\n\tb = nan\n\tLoss = nan\n\n\nand then one more time:\nStep #500\n\tA = 2.6116130352020264\n\tb = 1.91126549243927\n\tLoss = 101.70783233642578\n\nStep #1000\n\tA = 2.3814141750335693\n\tb = 2.4998815059661865\n\tLoss = 100.85863494873047\n\nStep #1500\n\tA = 2.11511492729187\n\tb = 2.9327337741851807\n\tLoss = 99.995849609375", "body": "I am getting very inconsistent behavior with my `tf.Graph` objects and I can't explain why it's happening.  I'm running this in a Jupyter notebook.  Could this affect it at all?  \r\n\r\nMy versions: \r\n```\r\ntf.__version__\r\n'1.0.1'\r\nsys.version\r\n'3.6.1 |Anaconda custom (x86_64)| (default, Mar 22 2017, 19:25:17) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'\r\n```\r\n```\r\nfrom sklearn.datasets import *\r\nimport multiprocessing\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ntf_max_threads = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count())\r\ndef iris_data():\r\n    # Iris dataset\r\n    X = pd.DataFrame(load_iris().data,\r\n                           index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])],\r\n                           columns = [x.split(\" (cm)\")[0].replace(\" \",\"_\") for x in load_iris().feature_names])\r\n\r\n    y = pd.Series(load_iris().target,\r\n                           index = [\"iris_%d\" % i for i in range(load_iris().data.shape[0])],\r\n                           name = \"Species\")\r\n    color_list = [{0:\"red\",1:\"green\",2:\"blue\"}[x] for x in y]\r\n    cmap = {k:v for k,v in zip(X.index, color_list)}\r\n    return (X, y, cmap)\r\n\r\n# Data\r\nX,y,c = iris_data()\r\nx = X[\"petal_width\"].as_matrix()\r\ny = X[\"sepal_length\"].as_matrix()\r\nn = 50\r\n\r\n# Containers\r\nlasso_data = list()\r\nA_data = list()\r\nb_data = list()\r\n\r\n# Graph\r\nG_3_78 = tf.Graph()\r\n\r\n# Iterations\r\nn_iter = 1500\r\n\r\n# Functions\r\ndef lasso_penalty(coef, alpha=0.9):\r\n    lasso_param = tf.constant(alpha)\r\n    heavyside_step = tf.truediv(1.0, tf.add(1.0, tf.exp(tf.multiply(-100.0, tf.subtract(coef, lasso_param)))))\r\n    regularization_param = tf.multiply(heavyside_step, 99.0)\r\n    return regularization_param\r\ndef l2(y, y_model):\r\n    return tf.square(y - y_model)\r\n\r\n# Build Graph\r\nwith G_3_78.as_default():\r\n    # Placeholders\r\n    pH_x_petal_width = tf.placeholder(tf.float32, shape=[None,1], name=\"pH_x_petal_width\")\r\n    pH_y_hat = tf.placeholder(tf.float32, shape=[None,1])\r\n    \r\n    # Model\r\n    A = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=\"A\")\r\n    b = tf.Variable(tf.random_normal(mean=0, stddev=1, shape=[1,1]), name=\"b\")\r\n    model = tf.add(tf.matmul(pH_x_petal_width, A), b)\r\n    \r\n    # Loss\r\n    loss_lasso = tf.add(tf.reduce_mean(l2(pH_y_hat, model)), lasso_penalty(A, 0.9))\r\n    \r\n    with tf.Session(graph=G_3_78, config=tf_max_threads) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        # Optimizer\r\n        op = tf.train.GradientDescentOptimizer(0.001)\r\n        train_step = op.minimize(loss_lasso)\r\n        # Train linear model \r\n        for i in range(n_iter):\r\n            idx_random = np.random.RandomState(i).choice(y.shape[0], size=n)\r\n            tr_x = x[idx_random].reshape(-1,1)\r\n            tr_y = y[idx_random].reshape(-1,1)\r\n            sess.run(train_step, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})\r\n            # Iterations\r\n            A_iter = sess.run(A)[0][0]\r\n            b_iter = sess.run(b)[0][0]\r\n            \r\n            lasso_iter = sess.run(loss_lasso, feed_dict={ pH_x_petal_width:tr_x, pH_y_hat:tr_y})[0][0]\r\n            lasso_data.append(lasso_iter)\r\n            A_data.append(A_iter)\r\n            b_data.append(b_iter)\r\n            # Log\r\n            if (i + 1) % 500 == 0:\r\n                print(f\"Step #{i + 1}\\n\\tA = {A_iter}\")\r\n                print(f\"\\tb = {b_iter}\")\r\n                print(f\"\\tLoss = {lasso_iter}\")\r\n                print()        \r\n# Plot path\r\nwith plt.style.context(\"seaborn-white\"):\r\n    fig, ax = plt.subplots(nrows=3, figsize=(6,6))\r\n    pd.Series(lasso_data,).plot(ax=ax[0], label=\"loss(lasso)\", legend=True)\r\n    pd.Series(A_data,).plot(ax=ax[1], color=\"red\", label=\"A\", legend=True)\r\n    pd.Series(b_data,).plot(ax=ax[2], color=\"black\", label=\"b\", legend=True)\r\n    fig.suptitle(\"training-process\", fontsize=15, y=0.95)\r\n\r\n    # Plot linear separation\r\nwith plt.style.context(\"seaborn-white\"):\r\n    fig, ax = plt.subplots(figsize=(5,5))\r\n    ax.scatter(x, y, c=X.index.map(lambda x:c[x]))\r\n    x_lims = ax.get_xlim()\r\n#     y_lims = ax.get_ylim()\r\n    for i in range(n_iter):\r\n        ax.plot(x, x*A_data[i] + b_data[i], alpha=i/(n_iter*100), color=\"black\")\r\n    ax.set_xlim(x_lims)\r\n#     ax.set_ylim(y_lims)\r\n```\r\n\r\nI will run this code block and get the desired results: \r\n```\r\nStep #500\r\n\tA = 0.8209055066108704\r\n\tb = 3.338909149169922\r\n\tLoss = 2.8257622718811035\r\n\r\nStep #1000\r\n\tA = 0.812068521976471\r\n\tb = 4.303615570068359\r\n\tLoss = 0.5661464929580688\r\n\r\nStep #1500\r\n\tA = 0.8036581873893738\r\n\tb = 4.6621904373168945\r\n\tLoss = 0.23930419981479645\r\n```\r\n![image](https://cloud.githubusercontent.com/assets/9061708/26270528/6378cea2-3cb0-11e7-9a7c-3ab10850bd5d.png)\r\n\r\nand then I will run the exact same code block again: \r\n```\r\nStep #500\r\n\tA = nan\r\n\tb = nan\r\n\tLoss = nan\r\n\r\nStep #1000\r\n\tA = nan\r\n\tb = nan\r\n\tLoss = nan\r\n\r\nStep #1500\r\n\tA = nan\r\n\tb = nan\r\n\tLoss = nan\r\n```\r\n![image](https://cloud.githubusercontent.com/assets/9061708/26270550/946e57ca-3cb0-11e7-877a-0674c45ddd66.png)\r\n\r\n\r\nand then one more time: \r\n```\r\nStep #500\r\n\tA = 2.6116130352020264\r\n\tb = 1.91126549243927\r\n\tLoss = 101.70783233642578\r\n\r\nStep #1000\r\n\tA = 2.3814141750335693\r\n\tb = 2.4998815059661865\r\n\tLoss = 100.85863494873047\r\n\r\nStep #1500\r\n\tA = 2.11511492729187\r\n\tb = 2.9327337741851807\r\n\tLoss = 99.995849609375\r\n```\r\n![image](https://cloud.githubusercontent.com/assets/9061708/26270564/a97558ee-3cb0-11e7-8bad-85cb2f5f4f1c.png)\r\n\r\n\r\n"}