{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7767", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7767/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7767/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7767/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7767", "id": 209383768, "node_id": "MDU6SXNzdWUyMDkzODM3Njg=", "number": 7767, "title": "ParameterServer restart crashes distributed training process", "user": {"login": "alsrgv", "id": 16640218, "node_id": "MDQ6VXNlcjE2NjQwMjE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16640218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alsrgv", "html_url": "https://github.com/alsrgv", "followers_url": "https://api.github.com/users/alsrgv/followers", "following_url": "https://api.github.com/users/alsrgv/following{/other_user}", "gists_url": "https://api.github.com/users/alsrgv/gists{/gist_id}", "starred_url": "https://api.github.com/users/alsrgv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alsrgv/subscriptions", "organizations_url": "https://api.github.com/users/alsrgv/orgs", "repos_url": "https://api.github.com/users/alsrgv/repos", "events_url": "https://api.github.com/users/alsrgv/events{/privacy}", "received_events_url": "https://api.github.com/users/alsrgv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2017-02-22T08:28:12Z", "updated_at": "2018-02-24T02:33:22Z", "closed_at": "2017-02-27T19:14:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted.  Code is available at <a href=\"http://pastebin.com/HBUicRp2\" rel=\"nofollow\">http://pastebin.com/HBUicRp2</a>.  I'm using TensorFlow freshly built from r1.0.</p>\n<p>At first, this error happens which is OK:</p>\n<pre><code>INFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framework.errors_impl.UnavailableError'&gt;, {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"ex\nternal/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\n235,\"grpc_status\":14}\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\n235,\"grpc_status\":14}\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\n235,\"grpc_status\":14}\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<p>However, after Parameter Server is restarted, I see this error in logs and training worker crashes:</p>\n<pre><code>I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000729. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000727. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000731. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000723. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000072b. Possibly, this worker just restarted.\nTraceback (most recent call last):\n  File \"tf_dist_mnist.py\", line 140, in &lt;module&gt;\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"tf_dist_mnist.py\", line 107, in main\n    mon_sess.run(train_op, feed_dict={image: image_, label: label_})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\n    self._close_internal(exception_type)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\n    self._sess.close()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 739, in close\n    self._sess.close()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in close\n    self._coord.join()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\n</code></pre>\n<p>I expect it to recover from latest checkpoint instead.</p>", "body_text": "I've tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted.  Code is available at http://pastebin.com/HBUicRp2.  I'm using TensorFlow freshly built from r1.0.\nAt first, this error happens which is OK:\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnavailableError'>, {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"ex\nternal/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\n235,\"grpc_status\":14}\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\n235,\"grpc_status\":14}\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\n235,\"grpc_status\":14}\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nHowever, after Parameter Server is restarted, I see this error in logs and training worker crashes:\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000729. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000727. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000731. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000723. Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000072b. Possibly, this worker just restarted.\nTraceback (most recent call last):\n  File \"tf_dist_mnist.py\", line 140, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"tf_dist_mnist.py\", line 107, in main\n    mon_sess.run(train_op, feed_dict={image: image_, label: label_})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\n    self._close_internal(exception_type)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\n    self._sess.close()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 739, in close\n    self._sess.close()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in close\n    self._coord.join()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\n\nI expect it to recover from latest checkpoint instead.", "body": "I've tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted.  Code is available at http://pastebin.com/HBUicRp2.  I'm using TensorFlow freshly built from r1.0.\r\n\r\nAt first, this error happens which is OK:\r\n```\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnavailableError'>, {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"ex\r\nternal/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\r\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\r\n235,\"grpc_status\":14}\r\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\r\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\r\n235,\"grpc_status\":14}\r\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\r\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\nW tensorflow/core/framework/op_kernel.cc:993] Unavailable: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":\r\n235,\"grpc_status\":14}\r\n         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_\r\ndevice_incarnation=8036561443230364107, tensor_name=\"edge_30_Assign_1\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nHowever, after Parameter Server is restarted, I see this error in logs and training worker crashes:\r\n```\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000729. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000727. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000731. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000723. Possibly, this worker just restarted.\r\nI tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000072b. Possibly, this worker just restarted.\r\nTraceback (most recent call last):\r\n  File \"tf_dist_mnist.py\", line 140, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tf_dist_mnist.py\", line 107, in main\r\n    mon_sess.run(train_op, feed_dict={image: image_, label: label_})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 739, in close\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in close\r\n    self._coord.join()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: {\"created\":\"@1487751897.191020037\",\"description\":\"EOF\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":235,\"grpc_status\":14}\r\n```\r\n\r\nI expect it to recover from latest checkpoint instead."}