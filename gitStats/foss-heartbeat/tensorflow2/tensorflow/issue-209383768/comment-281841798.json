{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281841798", "html_url": "https://github.com/tensorflow/tensorflow/issues/7767#issuecomment-281841798", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7767", "id": 281841798, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTg0MTc5OA==", "user": {"login": "alsrgv", "id": 16640218, "node_id": "MDQ6VXNlcjE2NjQwMjE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16640218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alsrgv", "html_url": "https://github.com/alsrgv", "followers_url": "https://api.github.com/users/alsrgv/followers", "following_url": "https://api.github.com/users/alsrgv/following{/other_user}", "gists_url": "https://api.github.com/users/alsrgv/gists{/gist_id}", "starred_url": "https://api.github.com/users/alsrgv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alsrgv/subscriptions", "organizations_url": "https://api.github.com/users/alsrgv/orgs", "repos_url": "https://api.github.com/users/alsrgv/repos", "events_url": "https://api.github.com/users/alsrgv/events{/privacy}", "received_events_url": "https://api.github.com/users/alsrgv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-22T23:39:52Z", "updated_at": "2017-02-22T23:39:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>AFAIK it is has similar level of recoverability as MonitoredTrainingSession (before your fix).  Here's my observations for Supervisor's managed_session:</p>\n<ul>\n<li>Parameter Server failure.  Training throws UnavailableError on all workers.  All workers crash.  After restart of failed PS and all workers, training continues from the last checkpoint.</li>\n<li>Chief worker failure.  Training freezes.  After chief worker is restarted, training continues from the last checkpoint.</li>\n<li>Non-Chief worker failure.  Training freezes.  After failed worker is restarted, training continues from the last iteration.</li>\n<li>Backup worker failure.  Training continues, with potential slowdown.  After failed backup worker is restarted, training continues with original pace.</li>\n</ul>\n<p>Since it's still widely advertised a lot of people may jump to use it.</p>", "body_text": "AFAIK it is has similar level of recoverability as MonitoredTrainingSession (before your fix).  Here's my observations for Supervisor's managed_session:\n\nParameter Server failure.  Training throws UnavailableError on all workers.  All workers crash.  After restart of failed PS and all workers, training continues from the last checkpoint.\nChief worker failure.  Training freezes.  After chief worker is restarted, training continues from the last checkpoint.\nNon-Chief worker failure.  Training freezes.  After failed worker is restarted, training continues from the last iteration.\nBackup worker failure.  Training continues, with potential slowdown.  After failed backup worker is restarted, training continues with original pace.\n\nSince it's still widely advertised a lot of people may jump to use it.", "body": "AFAIK it is has similar level of recoverability as MonitoredTrainingSession (before your fix).  Here's my observations for Supervisor's managed_session:\r\n\r\n* Parameter Server failure.  Training throws UnavailableError on all workers.  All workers crash.  After restart of failed PS and all workers, training continues from the last checkpoint.\r\n* Chief worker failure.  Training freezes.  After chief worker is restarted, training continues from the last checkpoint.\r\n* Non-Chief worker failure.  Training freezes.  After failed worker is restarted, training continues from the last iteration.\r\n* Backup worker failure.  Training continues, with potential slowdown.  After failed backup worker is restarted, training continues with original pace.\r\n\r\nSince it's still widely advertised a lot of people may jump to use it."}