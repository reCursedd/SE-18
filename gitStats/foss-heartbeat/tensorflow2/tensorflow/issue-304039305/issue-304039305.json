{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17612", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17612/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17612/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17612/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17612", "id": 304039305, "node_id": "MDU6SXNzdWUzMDQwMzkzMDU=", "number": 17612, "title": "Changing parallel_iterations of dynamic_rnn doesn't affect GPU memory consumption", "user": {"login": "WanCC1995", "id": 16075617, "node_id": "MDQ6VXNlcjE2MDc1NjE3", "avatar_url": "https://avatars2.githubusercontent.com/u/16075617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WanCC1995", "html_url": "https://github.com/WanCC1995", "followers_url": "https://api.github.com/users/WanCC1995/followers", "following_url": "https://api.github.com/users/WanCC1995/following{/other_user}", "gists_url": "https://api.github.com/users/WanCC1995/gists{/gist_id}", "starred_url": "https://api.github.com/users/WanCC1995/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WanCC1995/subscriptions", "organizations_url": "https://api.github.com/users/WanCC1995/orgs", "repos_url": "https://api.github.com/users/WanCC1995/repos", "events_url": "https://api.github.com/users/WanCC1995/events{/privacy}", "received_events_url": "https://api.github.com/users/WanCC1995/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-03-10T04:01:32Z", "updated_at": "2018-07-11T01:45:20Z", "closed_at": "2018-07-11T01:45:20Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (through pip)</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow-gpu 1.6</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>: Tesla P100</li>\n<li><strong>Exact command to reproduce</strong>: Download mnist dataset to dir \"input_data/\" and run the source code provided as below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm using dynamic_rnn to train an RNN on mnist dataset. I run a simple test (code as below) to find the best parallel_iterations number. However, checking with nvidia-smi, it seems that the GPU memory consumption remains the same, which infers dynamic_rnn doesn't run in parallel.</p>\n<p>Looking forward to your reply!</p>\n<h3>Source code / logs</h3>\n<p>Test Code:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.layers import fully_connected\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.contrib import rnn\nimport sys\nimport numpy as np\n\n\ndef train(batch_size, parallel_iterations, mnist):\n\tlr = 1e-3\n\tbatch_size_holder = tf.placeholder(tf.int32) \n\tinput_size = 28\n\ttimestep_size = 28\n\thidden_size = 28\n\tlayer_num = 50\n\tclass_num = 10\n\n\t_X = tf.placeholder(tf.float32, [None, 784])\n\ty = tf.placeholder(tf.float32, [None, class_num])\n\tkeep_prob = tf.placeholder(tf.float32)\n\tX = tf.reshape(_X, [-1, 28, 28])\n\n\trnn_cell = rnn.BasicRNNCell(num_units=hidden_size)\n\trnn_cell = rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n\tmrnn_cell = rnn.MultiRNNCell([rnn_cell] * layer_num, state_is_tuple=True)\n\toutputs, state = tf.nn.dynamic_rnn(mrnn_cell, inputs=X, dtype=tf.float32, time_major=False, parallel_iterations=parallel_iterations)\n\th_state = outputs[:, -1, :]\n\n\tW = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\n\tbias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n\ty_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\n\tcross_entropy = -tf.reduce_mean(y * tf.log(y_pre))\n\ttrain_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n\n\tcorrect_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n\tsession_config = tf.ConfigProto()\n\tsession_config.gpu_options.allow_growth = True\n\n\twith tf.Session(config=session_config) as sess:\n\t\tsess.run(tf.global_variables_initializer())\n\t\tfor i in range(50):\n\t\t\tbatch = mnist.train.next_batch(batch_size)\n\t\t\tonehot_labels = np.eye(class_num)[batch[1]]\n\t\t\tsess.run(train_op, feed_dict={_X: batch[0], y: onehot_labels, keep_prob: 0.5, batch_size_holder: batch_size}, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\n\n\ttf.reset_default_graph()\n\n\n\ndef main(_):\n\tmnist = input_data.read_data_sets(\"input_data/\")\n\n\tfor num in range(1,10):\n\t\ttrain(128, num*10, mnist)\n\tfor num in range(10,-1,-1):\n\t\tif (num &gt;0):\n\t\t\ttrain(128, num*10, mnist)\n\nif __name__ == '__main__':\n\ttf.app.run(main=main, argv=[sys.argv[0]])\n\n\n</code></pre>\n<p>nvidia-smi result:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:03:00.0 Off |                    0 |\n| N/A   25C    P0    37W / 250W |    517MiB / 16276MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla P100-PCIE...  Off  | 00000000:82:00.0 Off |                    0 |\n| N/A   23C    P0    37W / 250W |    359MiB / 16276MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      2893      C   python                                       507MiB |\n|    1      2893      C   python                                       349MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary (through pip)\nTensorFlow version (use command below): tensorflow-gpu 1.6\nPython version: 2.7\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 9.0\nGPU model and memory: Tesla P100\nExact command to reproduce: Download mnist dataset to dir \"input_data/\" and run the source code provided as below\n\nDescribe the problem\nI'm using dynamic_rnn to train an RNN on mnist dataset. I run a simple test (code as below) to find the best parallel_iterations number. However, checking with nvidia-smi, it seems that the GPU memory consumption remains the same, which infers dynamic_rnn doesn't run in parallel.\nLooking forward to your reply!\nSource code / logs\nTest Code:\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import fully_connected\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.contrib import rnn\nimport sys\nimport numpy as np\n\n\ndef train(batch_size, parallel_iterations, mnist):\n\tlr = 1e-3\n\tbatch_size_holder = tf.placeholder(tf.int32) \n\tinput_size = 28\n\ttimestep_size = 28\n\thidden_size = 28\n\tlayer_num = 50\n\tclass_num = 10\n\n\t_X = tf.placeholder(tf.float32, [None, 784])\n\ty = tf.placeholder(tf.float32, [None, class_num])\n\tkeep_prob = tf.placeholder(tf.float32)\n\tX = tf.reshape(_X, [-1, 28, 28])\n\n\trnn_cell = rnn.BasicRNNCell(num_units=hidden_size)\n\trnn_cell = rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n\tmrnn_cell = rnn.MultiRNNCell([rnn_cell] * layer_num, state_is_tuple=True)\n\toutputs, state = tf.nn.dynamic_rnn(mrnn_cell, inputs=X, dtype=tf.float32, time_major=False, parallel_iterations=parallel_iterations)\n\th_state = outputs[:, -1, :]\n\n\tW = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\n\tbias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n\ty_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\n\tcross_entropy = -tf.reduce_mean(y * tf.log(y_pre))\n\ttrain_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n\n\tcorrect_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n\tsession_config = tf.ConfigProto()\n\tsession_config.gpu_options.allow_growth = True\n\n\twith tf.Session(config=session_config) as sess:\n\t\tsess.run(tf.global_variables_initializer())\n\t\tfor i in range(50):\n\t\t\tbatch = mnist.train.next_batch(batch_size)\n\t\t\tonehot_labels = np.eye(class_num)[batch[1]]\n\t\t\tsess.run(train_op, feed_dict={_X: batch[0], y: onehot_labels, keep_prob: 0.5, batch_size_holder: batch_size}, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\n\n\ttf.reset_default_graph()\n\n\n\ndef main(_):\n\tmnist = input_data.read_data_sets(\"input_data/\")\n\n\tfor num in range(1,10):\n\t\ttrain(128, num*10, mnist)\n\tfor num in range(10,-1,-1):\n\t\tif (num >0):\n\t\t\ttrain(128, num*10, mnist)\n\nif __name__ == '__main__':\n\ttf.app.run(main=main, argv=[sys.argv[0]])\n\n\n\nnvidia-smi result:\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:03:00.0 Off |                    0 |\n| N/A   25C    P0    37W / 250W |    517MiB / 16276MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla P100-PCIE...  Off  | 00000000:82:00.0 Off |                    0 |\n| N/A   23C    P0    37W / 250W |    359MiB / 16276MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      2893      C   python                                       507MiB |\n|    1      2893      C   python                                       349MiB |\n+-----------------------------------------------------------------------------+", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (through pip)\r\n- **TensorFlow version (use command below)**: tensorflow-gpu 1.6\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Tesla P100\r\n- **Exact command to reproduce**: Download mnist dataset to dir \"input_data/\" and run the source code provided as below\r\n\r\n\r\n### Describe the problem\r\nI'm using dynamic_rnn to train an RNN on mnist dataset. I run a simple test (code as below) to find the best parallel_iterations number. However, checking with nvidia-smi, it seems that the GPU memory consumption remains the same, which infers dynamic_rnn doesn't run in parallel.\r\n\r\nLooking forward to your reply!\r\n\r\n### Source code / logs\r\nTest Code:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers import fully_connected\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.contrib import rnn\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\ndef train(batch_size, parallel_iterations, mnist):\r\n\tlr = 1e-3\r\n\tbatch_size_holder = tf.placeholder(tf.int32) \r\n\tinput_size = 28\r\n\ttimestep_size = 28\r\n\thidden_size = 28\r\n\tlayer_num = 50\r\n\tclass_num = 10\r\n\r\n\t_X = tf.placeholder(tf.float32, [None, 784])\r\n\ty = tf.placeholder(tf.float32, [None, class_num])\r\n\tkeep_prob = tf.placeholder(tf.float32)\r\n\tX = tf.reshape(_X, [-1, 28, 28])\r\n\r\n\trnn_cell = rnn.BasicRNNCell(num_units=hidden_size)\r\n\trnn_cell = rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\r\n\tmrnn_cell = rnn.MultiRNNCell([rnn_cell] * layer_num, state_is_tuple=True)\r\n\toutputs, state = tf.nn.dynamic_rnn(mrnn_cell, inputs=X, dtype=tf.float32, time_major=False, parallel_iterations=parallel_iterations)\r\n\th_state = outputs[:, -1, :]\r\n\r\n\tW = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\r\n\tbias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\r\n\ty_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\r\n\tcross_entropy = -tf.reduce_mean(y * tf.log(y_pre))\r\n\ttrain_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\r\n\r\n\tcorrect_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))\r\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n\r\n\tsession_config = tf.ConfigProto()\r\n\tsession_config.gpu_options.allow_growth = True\r\n\r\n\twith tf.Session(config=session_config) as sess:\r\n\t\tsess.run(tf.global_variables_initializer())\r\n\t\tfor i in range(50):\r\n\t\t\tbatch = mnist.train.next_batch(batch_size)\r\n\t\t\tonehot_labels = np.eye(class_num)[batch[1]]\r\n\t\t\tsess.run(train_op, feed_dict={_X: batch[0], y: onehot_labels, keep_prob: 0.5, batch_size_holder: batch_size}, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n\r\n\ttf.reset_default_graph()\r\n\r\n\r\n\r\ndef main(_):\r\n\tmnist = input_data.read_data_sets(\"input_data/\")\r\n\r\n\tfor num in range(1,10):\r\n\t\ttrain(128, num*10, mnist)\r\n\tfor num in range(10,-1,-1):\r\n\t\tif (num >0):\r\n\t\t\ttrain(128, num*10, mnist)\r\n\r\nif __name__ == '__main__':\r\n\ttf.app.run(main=main, argv=[sys.argv[0]])\r\n\r\n\r\n```\r\n\r\nnvidia-smi result:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:03:00.0 Off |                    0 |\r\n| N/A   25C    P0    37W / 250W |    517MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P100-PCIE...  Off  | 00000000:82:00.0 Off |                    0 |\r\n| N/A   23C    P0    37W / 250W |    359MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2893      C   python                                       507MiB |\r\n|    1      2893      C   python                                       349MiB |\r\n+-----------------------------------------------------------------------------+\r\n```"}