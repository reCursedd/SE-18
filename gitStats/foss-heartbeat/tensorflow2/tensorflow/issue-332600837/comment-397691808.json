{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/397691808", "html_url": "https://github.com/tensorflow/tensorflow/issues/20042#issuecomment-397691808", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20042", "id": 397691808, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzY5MTgwOA==", "user": {"login": "lun0522", "id": 24245729, "node_id": "MDQ6VXNlcjI0MjQ1NzI5", "avatar_url": "https://avatars0.githubusercontent.com/u/24245729?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lun0522", "html_url": "https://github.com/lun0522", "followers_url": "https://api.github.com/users/lun0522/followers", "following_url": "https://api.github.com/users/lun0522/following{/other_user}", "gists_url": "https://api.github.com/users/lun0522/gists{/gist_id}", "starred_url": "https://api.github.com/users/lun0522/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lun0522/subscriptions", "organizations_url": "https://api.github.com/users/lun0522/orgs", "repos_url": "https://api.github.com/users/lun0522/repos", "events_url": "https://api.github.com/users/lun0522/events{/privacy}", "received_events_url": "https://api.github.com/users/lun0522/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-15T17:35:31Z", "updated_at": "2018-06-22T03:56:47Z", "author_association": "NONE", "body_html": "<p>I found out how to avoid using these layers.</p>\n<p>Keras may use <strong>TensorFlowMinimum</strong> for relu6. In its source code:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/24245729/41481524-297a2e1a-7087-11e8-8521-eda7b4135b19.png\"><img width=\"754\" alt=\"screen shot 2018-06-15 at 9 47 44 am\" src=\"https://user-images.githubusercontent.com/24245729/41481524-297a2e1a-7087-11e8-8521-eda7b4135b19.png\" style=\"max-width:100%;\"></a><br>\nKeras implements it as a combination of a normal relu and a minimum layer. To avoid this, use tf.nn.relu6 instead (also use a Keras Lambda layer to wrap it).</p>\n<p>Keras may use <strong>TensorFlowMax</strong> and <strong>TensorFlowSum</strong> for softmax. In its source code:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/24245729/41481604-7f9149e6-7087-11e8-8a71-4bfd0c23abc0.png\"><img width=\"870\" alt=\"screen shot 2018-06-15 at 9 52 21 am\" src=\"https://user-images.githubusercontent.com/24245729/41481604-7f9149e6-7087-11e8-8a71-4bfd0c23abc0.png\" style=\"max-width:100%;\"></a><br>\nI forgot to reshape the input tensor, and the dimension of it was greater than 2, so Keras breaks it down to several sublayers. We can avoid this by reshaping the tensor before softmax.</p>", "body_text": "I found out how to avoid using these layers.\nKeras may use TensorFlowMinimum for relu6. In its source code:\n\nKeras implements it as a combination of a normal relu and a minimum layer. To avoid this, use tf.nn.relu6 instead (also use a Keras Lambda layer to wrap it).\nKeras may use TensorFlowMax and TensorFlowSum for softmax. In its source code:\n\nI forgot to reshape the input tensor, and the dimension of it was greater than 2, so Keras breaks it down to several sublayers. We can avoid this by reshaping the tensor before softmax.", "body": "I found out how to avoid using these layers. \r\n\r\nKeras may use **TensorFlowMinimum** for relu6. In its source code:\r\n<img width=\"754\" alt=\"screen shot 2018-06-15 at 9 47 44 am\" src=\"https://user-images.githubusercontent.com/24245729/41481524-297a2e1a-7087-11e8-8521-eda7b4135b19.png\">\r\nKeras implements it as a combination of a normal relu and a minimum layer. To avoid this, use tf.nn.relu6 instead (also use a Keras Lambda layer to wrap it).\r\n\r\nKeras may use **TensorFlowMax** and **TensorFlowSum** for softmax. In its source code:\r\n<img width=\"870\" alt=\"screen shot 2018-06-15 at 9 52 21 am\" src=\"https://user-images.githubusercontent.com/24245729/41481604-7f9149e6-7087-11e8-8a71-4bfd0c23abc0.png\">\r\nI forgot to reshape the input tensor, and the dimension of it was greater than 2, so Keras breaks it down to several sublayers. We can avoid this by reshaping the tensor before softmax."}