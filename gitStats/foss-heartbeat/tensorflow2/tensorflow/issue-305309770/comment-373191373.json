{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/373191373", "html_url": "https://github.com/tensorflow/tensorflow/issues/17720#issuecomment-373191373", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17720", "id": 373191373, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzE5MTM3Mw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T22:08:20Z", "updated_at": "2018-03-14T22:08:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It looks like this has been broken since <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/8693bf519399495cedd91293ec82b492ea401f6f/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/8693bf519399495cedd91293ec82b492ea401f6f\"><tt>8693bf5</tt></a>, which did not update the Python shape inference logic when adding the ability for <code>map_and_batch()</code> to return a smaller final batch. Sorry for the inconvenience, and we'll have a fix for this soon. In the meantime, you can fix by using <code>tf.placeholder_with_default()</code> to inhibit the shape inference:</p>\n<pre><code>     dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            map_func=parse_fn,\n            batch_size=tf.placeholder_with_default(tf.constant(128, dtype=tf.int64)),\n            num_parallel_batches=28\n        )\n     )\n</code></pre>\n<p>/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1072079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jsimsa\">@jsimsa</a></p>", "body_text": "It looks like this has been broken since 8693bf5, which did not update the Python shape inference logic when adding the ability for map_and_batch() to return a smaller final batch. Sorry for the inconvenience, and we'll have a fix for this soon. In the meantime, you can fix by using tf.placeholder_with_default() to inhibit the shape inference:\n     dataset = dataset.apply(\n        tf.contrib.data.map_and_batch(\n            map_func=parse_fn,\n            batch_size=tf.placeholder_with_default(tf.constant(128, dtype=tf.int64)),\n            num_parallel_batches=28\n        )\n     )\n\n/cc @jsimsa", "body": "It looks like this has been broken since 8693bf519399495cedd91293ec82b492ea401f6f, which did not update the Python shape inference logic when adding the ability for `map_and_batch()` to return a smaller final batch. Sorry for the inconvenience, and we'll have a fix for this soon. In the meantime, you can fix by using `tf.placeholder_with_default()` to inhibit the shape inference:\r\n\r\n         dataset = dataset.apply(\r\n            tf.contrib.data.map_and_batch(\r\n                map_func=parse_fn,\r\n                batch_size=tf.placeholder_with_default(tf.constant(128, dtype=tf.int64)),\r\n                num_parallel_batches=28\r\n            )\r\n         )\r\n\r\n\r\n/cc @jsimsa "}