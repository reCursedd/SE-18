{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17720", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17720/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17720/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17720/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17720", "id": 305309770, "node_id": "MDU6SXNzdWUzMDUzMDk3NzA=", "number": 17720, "title": "map_and_batch tensor shape does not match value of tensor in the same way that calling map and batch individually does", "user": {"login": "mbrio", "id": 13557, "node_id": "MDQ6VXNlcjEzNTU3", "avatar_url": "https://avatars2.githubusercontent.com/u/13557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbrio", "html_url": "https://github.com/mbrio", "followers_url": "https://api.github.com/users/mbrio/followers", "following_url": "https://api.github.com/users/mbrio/following{/other_user}", "gists_url": "https://api.github.com/users/mbrio/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbrio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbrio/subscriptions", "organizations_url": "https://api.github.com/users/mbrio/orgs", "repos_url": "https://api.github.com/users/mbrio/repos", "events_url": "https://api.github.com/users/mbrio/events{/privacy}", "received_events_url": "https://api.github.com/users/mbrio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-03-14T20:05:55Z", "updated_at": "2018-03-16T12:39:52Z", "closed_at": "2018-03-16T12:39:52Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nHappens with stock code</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu Server 17.10.1</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nSource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nb'v1.6.0-0-gd2e24b6039' 1.6.0</li>\n<li><strong>Python version</strong>:<br>\n3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.11.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\ngcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.1/7.0.5</li>\n<li><strong>GPU model and memory</strong>:<br>\nnot applicable</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nnot applicable</li>\n</ul>\n<p>When I create a tf.data.Dataset from tfrecord files that utilizes a call to <code>map</code> to parse the tfrecord file and a call to <code>batch</code> to batch the dataset I am able to filter out the last small batch utilizing a straight forward call to <code>filter</code>. This same function does not work correctly when utilizing the combined <code>map_and_batch</code> function. The filter function in question is:</p>\n<pre><code>dataset = dataset.filter(\n    lambda *args: tf.equal(tf.shape(args[0])[0], batch_size)\n)\n</code></pre>\n<p>The reason this does not work is because every tensor passed through <code>tf.shape</code> when utilizing <code>map_and_batch</code> has the same shape even though the contents of the tensor does not. This is not the case when executing <code>map</code> and <code>batch</code> separately, the last batch has a shape returned from <code>tf.shape</code> that correctly matches the shape of the value.</p>\n<p>My real world example has 7153 batches in 1 epoch, using <code>map</code> and <code>batch</code> separately I am returned 7152 batches that have a shape returned from <code>tf.shape</code> of 128 and the final 1 batch returned as 70, in this case the filter function works correctly. When I swap out this configuration with <code>map_and_batch</code> I am returned 7153 batches with shape returned from <code>tf.shape</code> of 128, in this case my filter function does not work and my estimator throws an exception because it receives a batch of 70 (when it was expecting a batch of 128).</p>\n<p>I would very much like to use <code>map_and_batch</code> because it takes 1/3 the time of <code>map</code> and <code>batch</code> separately.</p>\n<p>Here is an example script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> example.py</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nflags <span class=\"pl-k\">=</span> tf.app.flags\n\nflags.DEFINE_boolean(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>use_broken_map_and_batch<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">False</span>,\n                     <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Directory to write the model and <span class=\"pl-pds\">'</span></span>)\nflags.DEFINE_string(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>data.tfrecord<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Training tfrecord file. <span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> flags.<span class=\"pl-c1\">FLAGS</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">parse_fn</span>(<span class=\"pl-smi\">example</span>):\n    example_fmt <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>src<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], tf.int64),\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dst<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], tf.int64),\n    }\n\n    features <span class=\"pl-k\">=</span> tf.parse_single_example(\n        example,\n        <span class=\"pl-v\">features</span><span class=\"pl-k\">=</span>example_fmt\n    )\n\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">tuple</span>(features[k] <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> example_fmt)\n\n\ngraph <span class=\"pl-k\">=</span> tf.Graph()\n\n<span class=\"pl-k\">with</span> graph.as_default():\n    files <span class=\"pl-k\">=</span> tf.data.Dataset.list_files(<span class=\"pl-c1\">FLAGS</span>.train_data)\n    dataset <span class=\"pl-k\">=</span> files.interleave(tf.data.TFRecordDataset, <span class=\"pl-v\">cycle_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.use_broken_map_and_batch:\n        dataset <span class=\"pl-k\">=</span> dataset.apply(\n            tf.contrib.data.map_and_batch(\n                <span class=\"pl-v\">map_func</span><span class=\"pl-k\">=</span>parse_fn,\n                <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n                <span class=\"pl-v\">num_parallel_batches</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">28</span>\n            )\n        )\n    <span class=\"pl-k\">else</span>:\n        dataset <span class=\"pl-k\">=</span> dataset.map(parse_fn, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">28</span>)\n        dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">128</span>)\n\n    dataset <span class=\"pl-k\">=</span> dataset.filter(\n        <span class=\"pl-k\">lambda</span> <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>: tf.equal(tf.shape(args[<span class=\"pl-c1\">0</span>])[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">128</span>)\n    )\n\n    iterator <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator()\n    src, dst <span class=\"pl-k\">=</span> iterator.get_next()\n\n    train_op <span class=\"pl-k\">=</span> src <span class=\"pl-k\">*</span> dst\n\n    init_op <span class=\"pl-k\">=</span> tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph) <span class=\"pl-k\">as</span> sess:\n    sess.run(init_op)\n\n    shapes <span class=\"pl-k\">=</span> []\n\n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            val <span class=\"pl-k\">=</span> sess.run(train_op)\n            shapes.append(val.shape[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n        <span class=\"pl-c1\">print</span>(shapes[<span class=\"pl-k\">-</span><span class=\"pl-c1\">10</span>:])</pre></div>\n<p>When executed with the following parameters you should see the output:</p>\n<pre><code>$ python example.py --train_data data.tfrecord\n\n[128, 128, 128, 128, 128, 128, 128]\n\n$ python example.py --train_data data.tfrecord \u200a--use_broken_map_and_batch\n\n[128, 128, 128, 128, 128, 128, 128, 104]\n</code></pre>\n<p>To be clear the execution with the \u200a<code>\u200a--use_broken_map_and_batch</code> shows a 104 at the end, this is because that batch was not filtered out, you can recreate this by using the following code to generate a tfrecord file:</p>\n<pre><code># data_generation.py\nimport numpy as np\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')\n\nFLAGS = flags.FLAGS\n\nvals = np.random.randint(1, 1000, (1000, 2))\n\nwith tf.python_io.TFRecordWriter(FLAGS.train_data) as writer:\n    for src, dst in vals:\n        example = tf.train.Example(\n            features=tf.train.Features(\n                feature={\n                    'src': tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=[src])),\n                    'dst': tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=[dst])),\n                }\n            )\n        )\n\n        serialized = example.SerializeToString()\n        writer.write(serialized)\n</code></pre>\n<p>And you can execute in this way:</p>\n<pre><code>$ python data_generation.py --train_data data.tfrecord\n</code></pre>\n<p>Any help on this issue is greatly appreciated.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nHappens with stock code\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu Server 17.10.1\nTensorFlow installed from (source or binary):\nSource\nTensorFlow version (use command below):\nb'v1.6.0-0-gd2e24b6039' 1.6.0\nPython version:\n3.6\nBazel version (if compiling from source):\n0.11.0\nGCC/Compiler version (if compiling from source):\ngcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010\nCUDA/cuDNN version:\n9.1/7.0.5\nGPU model and memory:\nnot applicable\nExact command to reproduce:\nnot applicable\n\nWhen I create a tf.data.Dataset from tfrecord files that utilizes a call to map to parse the tfrecord file and a call to batch to batch the dataset I am able to filter out the last small batch utilizing a straight forward call to filter. This same function does not work correctly when utilizing the combined map_and_batch function. The filter function in question is:\ndataset = dataset.filter(\n    lambda *args: tf.equal(tf.shape(args[0])[0], batch_size)\n)\n\nThe reason this does not work is because every tensor passed through tf.shape when utilizing map_and_batch has the same shape even though the contents of the tensor does not. This is not the case when executing map and batch separately, the last batch has a shape returned from tf.shape that correctly matches the shape of the value.\nMy real world example has 7153 batches in 1 epoch, using map and batch separately I am returned 7152 batches that have a shape returned from tf.shape of 128 and the final 1 batch returned as 70, in this case the filter function works correctly. When I swap out this configuration with map_and_batch I am returned 7153 batches with shape returned from tf.shape of 128, in this case my filter function does not work and my estimator throws an exception because it receives a batch of 70 (when it was expecting a batch of 128).\nI would very much like to use map_and_batch because it takes 1/3 the time of map and batch separately.\nHere is an example script:\n# example.py\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_boolean('use_broken_map_and_batch', False,\n                     'Directory to write the model and ')\nflags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')\n\nFLAGS = flags.FLAGS\n\ndef parse_fn(example):\n    example_fmt = {\n        'src': tf.FixedLenFeature([], tf.int64),\n        'dst': tf.FixedLenFeature([], tf.int64),\n    }\n\n    features = tf.parse_single_example(\n        example,\n        features=example_fmt\n    )\n\n    return tuple(features[k] for k in example_fmt)\n\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    files = tf.data.Dataset.list_files(FLAGS.train_data)\n    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\n\n    if FLAGS.use_broken_map_and_batch:\n        dataset = dataset.apply(\n            tf.contrib.data.map_and_batch(\n                map_func=parse_fn,\n                batch_size=128,\n                num_parallel_batches=28\n            )\n        )\n    else:\n        dataset = dataset.map(parse_fn, num_parallel_calls=28)\n        dataset = dataset.batch(128)\n\n    dataset = dataset.filter(\n        lambda *args: tf.equal(tf.shape(args[0])[0], 128)\n    )\n\n    iterator = dataset.make_one_shot_iterator()\n    src, dst = iterator.get_next()\n\n    train_op = src * dst\n\n    init_op = tf.group(tf.global_variables_initializer(),\n                       tf.local_variables_initializer())\n\nwith tf.Session(graph=graph) as sess:\n    sess.run(init_op)\n\n    shapes = []\n\n    try:\n        while True:\n            val = sess.run(train_op)\n            shapes.append(val.shape[0])\n    except tf.errors.OutOfRangeError:\n        print(shapes[-10:])\nWhen executed with the following parameters you should see the output:\n$ python example.py --train_data data.tfrecord\n\n[128, 128, 128, 128, 128, 128, 128]\n\n$ python example.py --train_data data.tfrecord \u200a--use_broken_map_and_batch\n\n[128, 128, 128, 128, 128, 128, 128, 104]\n\nTo be clear the execution with the \u200a\u200a--use_broken_map_and_batch shows a 104 at the end, this is because that batch was not filtered out, you can recreate this by using the following code to generate a tfrecord file:\n# data_generation.py\nimport numpy as np\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')\n\nFLAGS = flags.FLAGS\n\nvals = np.random.randint(1, 1000, (1000, 2))\n\nwith tf.python_io.TFRecordWriter(FLAGS.train_data) as writer:\n    for src, dst in vals:\n        example = tf.train.Example(\n            features=tf.train.Features(\n                feature={\n                    'src': tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=[src])),\n                    'dst': tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=[dst])),\n                }\n            )\n        )\n\n        serialized = example.SerializeToString()\n        writer.write(serialized)\n\nAnd you can execute in this way:\n$ python data_generation.py --train_data data.tfrecord\n\nAny help on this issue is greatly appreciated.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nHappens with stock code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu Server 17.10.1\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nb'v1.6.0-0-gd2e24b6039' 1.6.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010\r\n- **CUDA/cuDNN version**:\r\n9.1/7.0.5\r\n- **GPU model and memory**:\r\nnot applicable\r\n- **Exact command to reproduce**:\r\nnot applicable\r\n\r\nWhen I create a tf.data.Dataset from tfrecord files that utilizes a call to `map` to parse the tfrecord file and a call to `batch` to batch the dataset I am able to filter out the last small batch utilizing a straight forward call to `filter`. This same function does not work correctly when utilizing the combined `map_and_batch` function. The filter function in question is:\r\n\r\n```\r\ndataset = dataset.filter(\r\n    lambda *args: tf.equal(tf.shape(args[0])[0], batch_size)\r\n)\r\n``` \r\n\r\nThe reason this does not work is because every tensor passed through `tf.shape` when utilizing `map_and_batch` has the same shape even though the contents of the tensor does not. This is not the case when executing `map` and `batch` separately, the last batch has a shape returned from `tf.shape` that correctly matches the shape of the value.\r\n\r\nMy real world example has 7153 batches in 1 epoch, using `map` and `batch` separately I am returned 7152 batches that have a shape returned from `tf.shape` of 128 and the final 1 batch returned as 70, in this case the filter function works correctly. When I swap out this configuration with `map_and_batch` I am returned 7153 batches with shape returned from `tf.shape` of 128, in this case my filter function does not work and my estimator throws an exception because it receives a batch of 70 (when it was expecting a batch of 128).\r\n\r\nI would very much like to use `map_and_batch` because it takes 1/3 the time of `map` and `batch` separately.\r\n\r\nHere is an example script:\r\n\r\n```python\r\n# example.py\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\n\r\nflags.DEFINE_boolean('use_broken_map_and_batch', False,\r\n                     'Directory to write the model and ')\r\nflags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')\r\n\r\nFLAGS = flags.FLAGS\r\n\r\ndef parse_fn(example):\r\n    example_fmt = {\r\n        'src': tf.FixedLenFeature([], tf.int64),\r\n        'dst': tf.FixedLenFeature([], tf.int64),\r\n    }\r\n\r\n    features = tf.parse_single_example(\r\n        example,\r\n        features=example_fmt\r\n    )\r\n\r\n    return tuple(features[k] for k in example_fmt)\r\n\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    files = tf.data.Dataset.list_files(FLAGS.train_data)\r\n    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\r\n\r\n    if FLAGS.use_broken_map_and_batch:\r\n        dataset = dataset.apply(\r\n            tf.contrib.data.map_and_batch(\r\n                map_func=parse_fn,\r\n                batch_size=128,\r\n                num_parallel_batches=28\r\n            )\r\n        )\r\n    else:\r\n        dataset = dataset.map(parse_fn, num_parallel_calls=28)\r\n        dataset = dataset.batch(128)\r\n\r\n    dataset = dataset.filter(\r\n        lambda *args: tf.equal(tf.shape(args[0])[0], 128)\r\n    )\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    src, dst = iterator.get_next()\r\n\r\n    train_op = src * dst\r\n\r\n    init_op = tf.group(tf.global_variables_initializer(),\r\n                       tf.local_variables_initializer())\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    sess.run(init_op)\r\n\r\n    shapes = []\r\n\r\n    try:\r\n        while True:\r\n            val = sess.run(train_op)\r\n            shapes.append(val.shape[0])\r\n    except tf.errors.OutOfRangeError:\r\n        print(shapes[-10:])\r\n```\r\n\r\nWhen executed with the following parameters you should see the output:\r\n\r\n```\r\n$ python example.py --train_data data.tfrecord\r\n\r\n[128, 128, 128, 128, 128, 128, 128]\r\n\r\n$ python example.py --train_data data.tfrecord \u200a--use_broken_map_and_batch\r\n\r\n[128, 128, 128, 128, 128, 128, 128, 104]\r\n```\r\n\r\nTo be clear the execution with the \u200a`\u200a--use_broken_map_and_batch` shows a 104 at the end, this is because that batch was not filtered out, you can recreate this by using the following code to generate a tfrecord file:\r\n\r\n```\r\n# data_generation.py\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\n\r\nflags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nvals = np.random.randint(1, 1000, (1000, 2))\r\n\r\nwith tf.python_io.TFRecordWriter(FLAGS.train_data) as writer:\r\n    for src, dst in vals:\r\n        example = tf.train.Example(\r\n            features=tf.train.Features(\r\n                feature={\r\n                    'src': tf.train.Feature(\r\n                        int64_list=tf.train.Int64List(value=[src])),\r\n                    'dst': tf.train.Feature(\r\n                        int64_list=tf.train.Int64List(value=[dst])),\r\n                }\r\n            )\r\n        )\r\n\r\n        serialized = example.SerializeToString()\r\n        writer.write(serialized)\r\n```\r\n\r\nAnd you can execute in this way:\r\n\r\n```\r\n$ python data_generation.py --train_data data.tfrecord\r\n```\r\n\r\nAny help on this issue is greatly appreciated."}