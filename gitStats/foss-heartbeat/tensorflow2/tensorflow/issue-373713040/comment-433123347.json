{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433123347", "html_url": "https://github.com/tensorflow/tensorflow/pull/23233#issuecomment-433123347", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23233", "id": 433123347, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzEyMzM0Nw==", "user": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-25T16:43:49Z", "updated_at": "2018-10-25T17:09:01Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5057740\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/feihugis\">@feihugis</a> thank you for the initiative. Unfortunately, this is not the right way to address the TODO.</p>\n<p>The problem with your solution is that it assumes that the threadpool use for executing parallel map is always the interop threadpool, which is not the case. The input pipeline can be configured with a custom threadpool (see <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/threadpool.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/threadpool.py</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc</a>).</p>\n<p>I think that the proper way to address the TODO is to introduce a new member to the <code>IteratorContext</code> called <code>runner_threadpool_size</code>, which is initialized with <code>ctx-&gt;device()-&gt;tensorflow_cpu_worker_threads()</code> when created and this value is overridden when appropriate (e.g. when <code>override_threadpool</code>).</p>\n<p>EDIT: Talked to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> offline and updated my original response accordingly (removing connection to <code>use_inter_op_parallelism</code> as that was misguided).</p>", "body_text": "Hi @feihugis thank you for the initiative. Unfortunately, this is not the right way to address the TODO.\nThe problem with your solution is that it assumes that the threadpool use for executing parallel map is always the interop threadpool, which is not the case. The input pipeline can be configured with a custom threadpool (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/threadpool.py and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc).\nI think that the proper way to address the TODO is to introduce a new member to the IteratorContext called runner_threadpool_size, which is initialized with ctx->device()->tensorflow_cpu_worker_threads() when created and this value is overridden when appropriate (e.g. when override_threadpool).\nEDIT: Talked to @mrry offline and updated my original response accordingly (removing connection to use_inter_op_parallelism as that was misguided).", "body": "Hi @feihugis thank you for the initiative. Unfortunately, this is not the right way to address the TODO.\r\n\r\nThe problem with your solution is that it assumes that the threadpool use for executing parallel map is always the interop threadpool, which is not the case. The input pipeline can be configured with a custom threadpool (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/threadpool.py and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc).\r\n\r\nI think that the proper way to address the TODO is to introduce a new member to the `IteratorContext` called `runner_threadpool_size`, which is initialized with `ctx->device()->tensorflow_cpu_worker_threads()` when created and this value is overridden when appropriate (e.g. when `override_threadpool`).\r\n\r\nEDIT: Talked to @mrry offline and updated my original response accordingly (removing connection to `use_inter_op_parallelism` as that was misguided)."}