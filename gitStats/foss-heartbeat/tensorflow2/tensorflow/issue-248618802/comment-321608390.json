{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321608390", "html_url": "https://github.com/tensorflow/tensorflow/pull/12100#issuecomment-321608390", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12100", "id": 321608390, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTYwODM5MA==", "user": {"login": "alanyee", "id": 1873994, "node_id": "MDQ6VXNlcjE4NzM5OTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1873994?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanyee", "html_url": "https://github.com/alanyee", "followers_url": "https://api.github.com/users/alanyee/followers", "following_url": "https://api.github.com/users/alanyee/following{/other_user}", "gists_url": "https://api.github.com/users/alanyee/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanyee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanyee/subscriptions", "organizations_url": "https://api.github.com/users/alanyee/orgs", "repos_url": "https://api.github.com/users/alanyee/repos", "events_url": "https://api.github.com/users/alanyee/events{/privacy}", "received_events_url": "https://api.github.com/users/alanyee/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-10T16:46:18Z", "updated_at": "2017-08-10T16:46:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Ah, I have been trying to get around that problem which has occurred for another PR:</p>\n<pre lang=\"----------------------------------------------------------------------\"><code>Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 1444, in testMultiClassWithLabelKeysEvalAccuracy1\n    _assert_summary_tags(self, [\"loss\"])\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 90, in _assert_summary_tags\n    test_case.assertItemsEqual(expected_tags or [], actual_tags)\nAssertionError: Element counts were not equal:\nFirst has 1, Second has 0:  'loss'\nFirst has 0, Second has 1:  u'multi_class_head/loss'\n</code></pre>\n<p>The problem is essentially that the training and eval losses show up in different graphs which is why the deprecated API was used in the first place.</p>", "body_text": "Ah, I have been trying to get around that problem which has occurred for another PR:\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 1444, in testMultiClassWithLabelKeysEvalAccuracy1\n    _assert_summary_tags(self, [\"loss\"])\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 90, in _assert_summary_tags\n    test_case.assertItemsEqual(expected_tags or [], actual_tags)\nAssertionError: Element counts were not equal:\nFirst has 1, Second has 0:  'loss'\nFirst has 0, Second has 1:  u'multi_class_head/loss'\n\nThe problem is essentially that the training and eval losses show up in different graphs which is why the deprecated API was used in the first place.", "body": "Ah, I have been trying to get around that problem which has occurred for another PR: \r\n```----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 1444, in testMultiClassWithLabelKeysEvalAccuracy1\r\n    _assert_summary_tags(self, [\"loss\"])\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 90, in _assert_summary_tags\r\n    test_case.assertItemsEqual(expected_tags or [], actual_tags)\r\nAssertionError: Element counts were not equal:\r\nFirst has 1, Second has 0:  'loss'\r\nFirst has 0, Second has 1:  u'multi_class_head/loss'\r\n```\r\n\r\nThe problem is essentially that the training and eval losses show up in different graphs which is why the deprecated API was used in the first place."}