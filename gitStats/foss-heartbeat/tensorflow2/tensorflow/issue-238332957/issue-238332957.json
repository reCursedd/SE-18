{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11036", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11036/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11036/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11036/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11036", "id": 238332957, "node_id": "MDU6SXNzdWUyMzgzMzI5NTc=", "number": 11036, "title": "Issue while backpropagating through sparse_tensor_dense_multiply", "user": {"login": "saisrivatsan", "id": 5208499, "node_id": "MDQ6VXNlcjUyMDg0OTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5208499?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saisrivatsan", "html_url": "https://github.com/saisrivatsan", "followers_url": "https://api.github.com/users/saisrivatsan/followers", "following_url": "https://api.github.com/users/saisrivatsan/following{/other_user}", "gists_url": "https://api.github.com/users/saisrivatsan/gists{/gist_id}", "starred_url": "https://api.github.com/users/saisrivatsan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saisrivatsan/subscriptions", "organizations_url": "https://api.github.com/users/saisrivatsan/orgs", "repos_url": "https://api.github.com/users/saisrivatsan/repos", "events_url": "https://api.github.com/users/saisrivatsan/events{/privacy}", "received_events_url": "https://api.github.com/users/saisrivatsan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-24T18:11:49Z", "updated_at": "2017-09-30T01:03:18Z", "closed_at": "2017-09-30T01:03:06Z", "author_association": "NONE", "body_html": "<p>I have a simple network defined as follows:</p>\n<pre><code>h1 = tf.sparse_tensor_dense_matmul(x, W1)\nh2 = tf.matmul(h1, W2)\ny = tf.matmul(h2, W3)\nloss = tf.nn.l2_loss(y - y_)\ntrain = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n</code></pre>\n<p>where x is a sparseTensor, rest are dense.<br>\nDimensions (shapes) of W1 = [1000,200], W2 = [200,400] and W3 = [ 400, 500].</p>\n<p>When I run the following:</p>\n<pre><code>sess.run([train], feed_dict={x:X, y_:Y})\n</code></pre>\n<p>where X is sparseTensor of shape [N, 1000] and Y is a tensor of shape [N, 500]</p>\n<p>I get an error saying: OOM when allocating tensor with shape[3684773,200].<br>\nThis is happening while the the the gradient for W is being computed. 3684773 also happens to be the number of non-zero elements in X.</p>\n<p>Note:</p>\n<ol>\n<li>When I compute gradients using tf.gradients, they work completely<br>\nfine.</li>\n<li>When I run the same network using dense X and dense multiply( tf.matmul ), it works completely fine.</li>\n</ol>", "body_text": "I have a simple network defined as follows:\nh1 = tf.sparse_tensor_dense_matmul(x, W1)\nh2 = tf.matmul(h1, W2)\ny = tf.matmul(h2, W3)\nloss = tf.nn.l2_loss(y - y_)\ntrain = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n\nwhere x is a sparseTensor, rest are dense.\nDimensions (shapes) of W1 = [1000,200], W2 = [200,400] and W3 = [ 400, 500].\nWhen I run the following:\nsess.run([train], feed_dict={x:X, y_:Y})\n\nwhere X is sparseTensor of shape [N, 1000] and Y is a tensor of shape [N, 500]\nI get an error saying: OOM when allocating tensor with shape[3684773,200].\nThis is happening while the the the gradient for W is being computed. 3684773 also happens to be the number of non-zero elements in X.\nNote:\n\nWhen I compute gradients using tf.gradients, they work completely\nfine.\nWhen I run the same network using dense X and dense multiply( tf.matmul ), it works completely fine.", "body": "I have a simple network defined as follows:\r\n\r\n    h1 = tf.sparse_tensor_dense_matmul(x, W1)\r\n    h2 = tf.matmul(h1, W2)\r\n    y = tf.matmul(h2, W3)\r\n    loss = tf.nn.l2_loss(y - y_)\r\n    train = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\r\n\r\nwhere x is a sparseTensor, rest are dense. \r\nDimensions (shapes) of W1 = [1000,200], W2 = [200,400] and W3 = [ 400, 500]. \r\n\r\nWhen I run the following:\r\n\r\n    sess.run([train], feed_dict={x:X, y_:Y})\r\n\r\nwhere X is sparseTensor of shape [N, 1000] and Y is a tensor of shape [N, 500]\r\n\r\nI get an error saying: OOM when allocating tensor with shape[3684773,200].\r\nThis is happening while the the the gradient for W is being computed. 3684773 also happens to be the number of non-zero elements in X.\r\n\r\nNote:\r\n\r\n 1. When I compute gradients using tf.gradients, they work completely\r\n    fine.\r\n 2. When I run the same network using dense X and dense multiply( tf.matmul ), it works completely fine.\r\n\r\n    \r\n\r\n"}