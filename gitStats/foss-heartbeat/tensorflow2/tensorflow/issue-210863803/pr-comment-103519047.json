{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/103519047", "pull_request_review_id": 24306976, "id": 103519047, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMzUxOTA0Nw==", "diff_hunk": "@@ -171,28 +171,41 @@ def testGradient(self):\n       x = nn_ops.softmax_cross_entropy_with_logits(labels=l, logits=f,\n                                                    name=\"xent\")\n       err = gradient_checker.compute_gradient_error(f, [3, 4], x, [3])\n+\n+      # Check that no extra computation performed. When only first derivative is requested,\n+      # second derivative must not be computed. So when there is no second derivative,\n+      # there is no `BatchMatMul` op in the graph.\n+      op_names = [op.op_def.name for op in sess.graph.get_operations() if op.op_def]\n+      self.assertNotIn('BatchMatMul', op_names)\n+\n     print(\"cross entropy gradient err = \", err)\n     self.assertLess(err, 5e-8)\n \n   def testSecondGradient(self):\n-    with self.test_session():\n-      l = constant_op.constant([0.0, 0.0, 1.0, 0.0,\n-                                1.0, 0.0, 0.0, 0.0,\n-                                0.0, 0.5, 0.0, 0.5], shape=[12],\n+    with self.test_session() as sess:\n+      l = constant_op.constant([0.0, 0.0, 1.0/3, 0.0,\n+                                1.0/3, 0.0, 0.0, 0.0,\n+                                0.0, 0.5/3, 0.0, 0.5/3], shape=[12],\n                                dtype=dtypes.float64, name=\"l\")\n       f = constant_op.constant([0.1, 0.2, 0.3, 0.4,\n                                 0.1, 0.4, 0.9, 1.6,\n                                 0.1, 0.8, 2.7, 6.4], shape=[12],\n                                dtype=dtypes.float64, name=\"f\")\n       x = nn_ops.softmax_cross_entropy_with_logits(labels=l, logits=f,\n                                                    name=\"xent\")\n-      loss = math_ops.reduce_mean(x)\n+      loss = math_ops.reduce_sum(x)\n+\n+      trivia_loss = -math_ops.reduce_sum(l * math_ops.log(nn_ops.softmax(f)))\n+\n+      hessians = sess.run(gradients_impl.hessians(loss, [f]))\n+      trivia_hessians = sess.run(gradients_impl.hessians(trivia_loss, [f]))\n+\n+      self.assertAllClose(hessians, trivia_hessians)", "path": "tensorflow/python/kernel_tests/xent_op_test.py", "position": null, "original_position": 47, "commit_id": "b1a0d86b9673ee06fe670b4c0cef312b801860a2", "original_commit_id": "686cb56edec6e2f3762ede90076b9e58323a7c58", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "body": "Use `gradient_checker.compute_gradient_error` instead of writing out the gradient manually a second time.", "created_at": "2017-02-28T18:17:27Z", "updated_at": "2017-03-01T07:35:51Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/7948#discussion_r103519047", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7948", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/103519047"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/7948#discussion_r103519047"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7948"}}, "body_html": "<p>Use <code>gradient_checker.compute_gradient_error</code> instead of writing out the gradient manually a second time.</p>", "body_text": "Use gradient_checker.compute_gradient_error instead of writing out the gradient manually a second time."}