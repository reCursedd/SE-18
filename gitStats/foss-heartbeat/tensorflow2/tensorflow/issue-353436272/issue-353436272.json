{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21831", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21831/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21831/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21831/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21831", "id": 353436272, "node_id": "MDU6SXNzdWUzNTM0MzYyNzI=", "number": 21831, "title": "Running Tensorflow example graph with TensorRT 4 backend fails", "user": {"login": "fferroni", "id": 16327442, "node_id": "MDQ6VXNlcjE2MzI3NDQy", "avatar_url": "https://avatars1.githubusercontent.com/u/16327442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fferroni", "html_url": "https://github.com/fferroni", "followers_url": "https://api.github.com/users/fferroni/followers", "following_url": "https://api.github.com/users/fferroni/following{/other_user}", "gists_url": "https://api.github.com/users/fferroni/gists{/gist_id}", "starred_url": "https://api.github.com/users/fferroni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fferroni/subscriptions", "organizations_url": "https://api.github.com/users/fferroni/orgs", "repos_url": "https://api.github.com/users/fferroni/repos", "events_url": "https://api.github.com/users/fferroni/events{/privacy}", "received_events_url": "https://api.github.com/users/fferroni/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-23T15:30:39Z", "updated_at": "2018-11-20T02:30:03Z", "closed_at": "2018-09-01T10:15:15Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: r1.10</li>\n<li><strong>Python version</strong>: python 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 1.6</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5</li>\n<li><strong>CUDA/cuDNN version</strong>: 7.0</li>\n<li><strong>GPU model and memory</strong>: 4GB Quadro M4000M</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am trying to follow the examples provided in tensorflow/contrib/tensorrt/test and the one provided by the Google blog post <a href=\"https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html\" rel=\"nofollow\">here</a> which provides code on running a more complex ResNet architecture <a href=\"https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz\" rel=\"nofollow\">here</a>.</p>\n<p>The tests test_tftrt.py and tf_trt_integration_test.py are marked as passed. However, the code provided in the other link fails. In both situations I get information like this, which claims it found no eligible GPUs (?) I am definitely using the GPU with Tensorflow though, I can see this in nvidia-smi and via a simple log placement test (separately).</p>\n<pre><code>2018-08-23 17:20:22.067239: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count &gt;= 8): 0\n2018-08-23 17:20:22.078438: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:756] MULTIPLE tensorrt candidate conversion: 2\n2018-08-23 17:20:22.078599: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\n2018-08-23 17:20:22.079079: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\n2018-08-23 17:20:22.079471: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n2018-08-23 17:20:22.186648: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n</code></pre>\n<p>The output message of running the ResNet execution is the following. It complains that it's out of memory, even though I try to reduce the batch size to 1, reduce the image size to a tiny size.</p>\n<p>Am I missing something? The 4GB memory is not huge, but it's the same as a DrivePX2 discrete GPU...</p>\n<pre><code>francesco@franny:~/Downloads/tftrt$ ./run_all.sh \n/home/francesco/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nNamespace(FP16=True, FP32=True, INT8=True, batch_size=1, dump_diff=False, native=True, num_loops=10, topN=5, update_graphdef=False, with_timeline=False, workspace_size=2048)\nStarting at 2018-08-23 17:26:13.644094\n2018-08-23 17:26:13.719781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-08-23 17:26:13.720536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \nname: Quadro M2000M major: 5 minor: 0 memoryClockRate(GHz): 1.137\npciBusID: 0000:01:00.0\ntotalMemory: 3.95GiB freeMemory: 3.69GiB\n2018-08-23 17:26:13.720568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-08-23 17:26:14.058416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-23 17:26:14.058443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-08-23 17:26:14.058451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-08-23 17:26:14.058669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -&gt; physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\nINFO:tensorflow:Starting execution\n2018-08-23 17:26:14.508197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-08-23 17:26:14.508248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-23 17:26:14.508254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-08-23 17:26:14.508258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-08-23 17:26:14.508394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -&gt; physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\nINFO:tensorflow:Starting Warmup cycle\n2018-08-23 17:26:17.058524: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2018-08-23 17:26:17.072068: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\nINFO:tensorflow:Warmup done. Starting real timing\niter  0   0.025112714767456055\niter  1   0.025174999237060548\niter  2   0.02513185977935791\niter  3   0.025080199241638183\niter  4   0.02514298439025879\niter  5   0.025306134223937987\niter  6   0.02516295909881592\niter  7   0.02516087532043457\niter  8   0.025412368774414062\niter  9   0.025518035888671874\nComparison= True\nINFO:tensorflow:Timing loop done!\nimages/s : 39.7 +/- 0.2, s/batch: 0.02522 +/- 0.00014\nRES, Native, 1, 39.65, 0.21, 0.02522, 0.00014\nINFO:tensorflow:Running against TensorRT version 4.0.1\n2018-08-23 17:26:30.386025: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count &gt;= 8): 0\n2018-08-23 17:26:30.831639: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'resnet_v1_50/', converted to graph\n2018-08-23 17:26:30.929338: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n2018-08-23 17:26:41.310315: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB.  Current allocation summary follows.\n2018-08-23 17:26:41.310388: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310413: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310434: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310453: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310473: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310492: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310515: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310535: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310555: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310574: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310594: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310622: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 620.0KiB allocated for chunks. 620.0KiB in use in bin. 620.0KiB client-requested in use in bin.\n2018-08-23 17:26:41.310643: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310666: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\n2018-08-23 17:26:41.310698: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310730: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310762: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310794: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310827: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310860: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310887: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 1.97GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310910: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 2.00GiB was 256.00MiB, Chunk State: \n2018-08-23 17:26:41.310937: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 1.97GiB | Requested Size: 3.9KiB | in_use: 0, prev:   Size: 3.06MiB | Requested Size: 3.06MiB | in_use: 1\n2018-08-23 17:26:41.310959: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb03140000 of size 634880\n2018-08-23 17:26:41.310975: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb031db000 of size 3211264\n2018-08-23 17:26:41.310991: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb034eb000 of size 2114605056\n2018-08-23 17:26:41.311006: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: \n2018-08-23 17:26:41.311026: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 634880 totalling 620.0KiB\n2018-08-23 17:26:41.311044: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3211264 totalling 3.06MiB\n2018-08-23 17:26:41.311060: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 3.67MiB\n2018-08-23 17:26:41.311083: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \nLimit:                  2118451200\nInUse:                     3846144\nMaxInUse:               1317554688\nNumAllocs:                  119350\nMaxAllocSize:           1212416000\n\n2018-08-23 17:26:41.311112: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________\n2018-08-23 17:26:41.311161: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\n2018-08-23 17:26:41.311420: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger GPU memory allocation failed during tactic selection for layer: resnet_v1_50/conv1/Conv2D + (Unnamed Layer* 2) [Activation]\n2018-08-23 17:26:41.319106: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\n2018-08-23 17:26:41.319785: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:857] Engine creation for segment 0, composed of 452 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\nINFO:tensorflow:Starting execution\n2018-08-23 17:26:45.089652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-08-23 17:26:45.089705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-23 17:26:45.089714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-08-23 17:26:45.089721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-08-23 17:26:45.089863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -&gt; physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\nINFO:tensorflow:Starting Warmup cycle\n... etc\n</code></pre>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): r1.10\nPython version: python 3.5\nBazel version (if compiling from source): 1.6\nGCC/Compiler version (if compiling from source): 5\nCUDA/cuDNN version: 7.0\nGPU model and memory: 4GB Quadro M4000M\nExact command to reproduce:\n\nDescribe the problem\nI am trying to follow the examples provided in tensorflow/contrib/tensorrt/test and the one provided by the Google blog post here which provides code on running a more complex ResNet architecture here.\nThe tests test_tftrt.py and tf_trt_integration_test.py are marked as passed. However, the code provided in the other link fails. In both situations I get information like this, which claims it found no eligible GPUs (?) I am definitely using the GPU with Tensorflow though, I can see this in nvidia-smi and via a simple log placement test (separately).\n2018-08-23 17:20:22.067239: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\n2018-08-23 17:20:22.078438: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:756] MULTIPLE tensorrt candidate conversion: 2\n2018-08-23 17:20:22.078599: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\n2018-08-23 17:20:22.079079: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\n2018-08-23 17:20:22.079471: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n2018-08-23 17:20:22.186648: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n\nThe output message of running the ResNet execution is the following. It complains that it's out of memory, even though I try to reduce the batch size to 1, reduce the image size to a tiny size.\nAm I missing something? The 4GB memory is not huge, but it's the same as a DrivePX2 discrete GPU...\nfrancesco@franny:~/Downloads/tftrt$ ./run_all.sh \n/home/francesco/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nNamespace(FP16=True, FP32=True, INT8=True, batch_size=1, dump_diff=False, native=True, num_loops=10, topN=5, update_graphdef=False, with_timeline=False, workspace_size=2048)\nStarting at 2018-08-23 17:26:13.644094\n2018-08-23 17:26:13.719781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-08-23 17:26:13.720536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \nname: Quadro M2000M major: 5 minor: 0 memoryClockRate(GHz): 1.137\npciBusID: 0000:01:00.0\ntotalMemory: 3.95GiB freeMemory: 3.69GiB\n2018-08-23 17:26:13.720568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-08-23 17:26:14.058416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-23 17:26:14.058443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-08-23 17:26:14.058451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-08-23 17:26:14.058669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\nINFO:tensorflow:Starting execution\n2018-08-23 17:26:14.508197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-08-23 17:26:14.508248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-23 17:26:14.508254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-08-23 17:26:14.508258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-08-23 17:26:14.508394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\nINFO:tensorflow:Starting Warmup cycle\n2018-08-23 17:26:17.058524: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2018-08-23 17:26:17.072068: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\nINFO:tensorflow:Warmup done. Starting real timing\niter  0   0.025112714767456055\niter  1   0.025174999237060548\niter  2   0.02513185977935791\niter  3   0.025080199241638183\niter  4   0.02514298439025879\niter  5   0.025306134223937987\niter  6   0.02516295909881592\niter  7   0.02516087532043457\niter  8   0.025412368774414062\niter  9   0.025518035888671874\nComparison= True\nINFO:tensorflow:Timing loop done!\nimages/s : 39.7 +/- 0.2, s/batch: 0.02522 +/- 0.00014\nRES, Native, 1, 39.65, 0.21, 0.02522, 0.00014\nINFO:tensorflow:Running against TensorRT version 4.0.1\n2018-08-23 17:26:30.386025: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\n2018-08-23 17:26:30.831639: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'resnet_v1_50/', converted to graph\n2018-08-23 17:26:30.929338: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n2018-08-23 17:26:41.310315: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB.  Current allocation summary follows.\n2018-08-23 17:26:41.310388: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310413: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310434: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310453: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310473: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310492: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310515: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310535: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310555: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310574: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310594: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310622: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 620.0KiB allocated for chunks. 620.0KiB in use in bin. 620.0KiB client-requested in use in bin.\n2018-08-23 17:26:41.310643: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310666: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\n2018-08-23 17:26:41.310698: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310730: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310762: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310794: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310827: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310860: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310887: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 1.97GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-08-23 17:26:41.310910: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 2.00GiB was 256.00MiB, Chunk State: \n2018-08-23 17:26:41.310937: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 1.97GiB | Requested Size: 3.9KiB | in_use: 0, prev:   Size: 3.06MiB | Requested Size: 3.06MiB | in_use: 1\n2018-08-23 17:26:41.310959: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb03140000 of size 634880\n2018-08-23 17:26:41.310975: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb031db000 of size 3211264\n2018-08-23 17:26:41.310991: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb034eb000 of size 2114605056\n2018-08-23 17:26:41.311006: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: \n2018-08-23 17:26:41.311026: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 634880 totalling 620.0KiB\n2018-08-23 17:26:41.311044: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3211264 totalling 3.06MiB\n2018-08-23 17:26:41.311060: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 3.67MiB\n2018-08-23 17:26:41.311083: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \nLimit:                  2118451200\nInUse:                     3846144\nMaxInUse:               1317554688\nNumAllocs:                  119350\nMaxAllocSize:           1212416000\n\n2018-08-23 17:26:41.311112: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________\n2018-08-23 17:26:41.311161: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\n2018-08-23 17:26:41.311420: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger GPU memory allocation failed during tactic selection for layer: resnet_v1_50/conv1/Conv2D + (Unnamed Layer* 2) [Activation]\n2018-08-23 17:26:41.319106: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\n2018-08-23 17:26:41.319785: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:857] Engine creation for segment 0, composed of 452 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\nINFO:tensorflow:Starting execution\n2018-08-23 17:26:45.089652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-08-23 17:26:45.089705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-23 17:26:45.089714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \n2018-08-23 17:26:45.089721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \n2018-08-23 17:26:45.089863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\nINFO:tensorflow:Starting Warmup cycle\n... etc\n\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Python version**: python 3.5\r\n- **Bazel version (if compiling from source)**: 1.6\r\n- **GCC/Compiler version (if compiling from source)**: 5\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: 4GB Quadro M4000M\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am trying to follow the examples provided in tensorflow/contrib/tensorrt/test and the one provided by the Google blog post [here](https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html) which provides code on running a more complex ResNet architecture [here](https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz).\r\n\r\nThe tests test_tftrt.py and tf_trt_integration_test.py are marked as passed. However, the code provided in the other link fails. In both situations I get information like this, which claims it found no eligible GPUs (?) I am definitely using the GPU with Tensorflow though, I can see this in nvidia-smi and via a simple log placement test (separately).\r\n\r\n```\r\n2018-08-23 17:20:22.067239: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-08-23 17:20:22.078438: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:756] MULTIPLE tensorrt candidate conversion: 2\r\n2018-08-23 17:20:22.078599: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-08-23 17:20:22.079079: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-08-23 17:20:22.079471: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n2018-08-23 17:20:22.186648: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n```\r\n\r\nThe output message of running the ResNet execution is the following. It complains that it's out of memory, even though I try to reduce the batch size to 1, reduce the image size to a tiny size.\r\n\r\nAm I missing something? The 4GB memory is not huge, but it's the same as a DrivePX2 discrete GPU...\r\n\r\n```\r\nfrancesco@franny:~/Downloads/tftrt$ ./run_all.sh \r\n/home/francesco/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nNamespace(FP16=True, FP32=True, INT8=True, batch_size=1, dump_diff=False, native=True, num_loops=10, topN=5, update_graphdef=False, with_timeline=False, workspace_size=2048)\r\nStarting at 2018-08-23 17:26:13.644094\r\n2018-08-23 17:26:13.719781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-23 17:26:13.720536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: Quadro M2000M major: 5 minor: 0 memoryClockRate(GHz): 1.137\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.69GiB\r\n2018-08-23 17:26:13.720568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-23 17:26:14.058416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-23 17:26:14.058443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-23 17:26:14.058451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-23 17:26:14.058669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting execution\r\n2018-08-23 17:26:14.508197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-23 17:26:14.508248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-23 17:26:14.508254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-23 17:26:14.508258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-23 17:26:14.508394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting Warmup cycle\r\n2018-08-23 17:26:17.058524: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-08-23 17:26:17.072068: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\nINFO:tensorflow:Warmup done. Starting real timing\r\niter  0   0.025112714767456055\r\niter  1   0.025174999237060548\r\niter  2   0.02513185977935791\r\niter  3   0.025080199241638183\r\niter  4   0.02514298439025879\r\niter  5   0.025306134223937987\r\niter  6   0.02516295909881592\r\niter  7   0.02516087532043457\r\niter  8   0.025412368774414062\r\niter  9   0.025518035888671874\r\nComparison= True\r\nINFO:tensorflow:Timing loop done!\r\nimages/s : 39.7 +/- 0.2, s/batch: 0.02522 +/- 0.00014\r\nRES, Native, 1, 39.65, 0.21, 0.02522, 0.00014\r\nINFO:tensorflow:Running against TensorRT version 4.0.1\r\n2018-08-23 17:26:30.386025: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-08-23 17:26:30.831639: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'resnet_v1_50/', converted to graph\r\n2018-08-23 17:26:30.929338: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n2018-08-23 17:26:41.310315: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB.  Current allocation summary follows.\r\n2018-08-23 17:26:41.310388: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310413: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310434: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310453: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310473: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310492: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310515: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310535: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310555: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310574: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310594: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310622: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 620.0KiB allocated for chunks. 620.0KiB in use in bin. 620.0KiB client-requested in use in bin.\r\n2018-08-23 17:26:41.310643: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310666: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\r\n2018-08-23 17:26:41.310698: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310730: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310762: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310794: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310827: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310860: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310887: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 1.97GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310910: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 2.00GiB was 256.00MiB, Chunk State: \r\n2018-08-23 17:26:41.310937: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 1.97GiB | Requested Size: 3.9KiB | in_use: 0, prev:   Size: 3.06MiB | Requested Size: 3.06MiB | in_use: 1\r\n2018-08-23 17:26:41.310959: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb03140000 of size 634880\r\n2018-08-23 17:26:41.310975: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb031db000 of size 3211264\r\n2018-08-23 17:26:41.310991: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb034eb000 of size 2114605056\r\n2018-08-23 17:26:41.311006: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: \r\n2018-08-23 17:26:41.311026: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 634880 totalling 620.0KiB\r\n2018-08-23 17:26:41.311044: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3211264 totalling 3.06MiB\r\n2018-08-23 17:26:41.311060: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 3.67MiB\r\n2018-08-23 17:26:41.311083: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \r\nLimit:                  2118451200\r\nInUse:                     3846144\r\nMaxInUse:               1317554688\r\nNumAllocs:                  119350\r\nMaxAllocSize:           1212416000\r\n\r\n2018-08-23 17:26:41.311112: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________\r\n2018-08-23 17:26:41.311161: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\r\n2018-08-23 17:26:41.311420: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger GPU memory allocation failed during tactic selection for layer: resnet_v1_50/conv1/Conv2D + (Unnamed Layer* 2) [Activation]\r\n2018-08-23 17:26:41.319106: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\r\n2018-08-23 17:26:41.319785: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:857] Engine creation for segment 0, composed of 452 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\r\nINFO:tensorflow:Starting execution\r\n2018-08-23 17:26:45.089652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-23 17:26:45.089705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-23 17:26:45.089714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-23 17:26:45.089721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-23 17:26:45.089863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting Warmup cycle\r\n... etc\r\n```\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n"}