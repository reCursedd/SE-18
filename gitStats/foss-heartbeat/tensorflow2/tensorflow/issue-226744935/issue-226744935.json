{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9715", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9715/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9715/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9715/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9715", "id": 226744935, "node_id": "MDU6SXNzdWUyMjY3NDQ5MzU=", "number": 9715, "title": "[inputs] instead of inputs for rnn_cell?", "user": {"login": "yhg0112", "id": 5001738, "node_id": "MDQ6VXNlcjUwMDE3Mzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5001738?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yhg0112", "html_url": "https://github.com/yhg0112", "followers_url": "https://api.github.com/users/yhg0112/followers", "following_url": "https://api.github.com/users/yhg0112/following{/other_user}", "gists_url": "https://api.github.com/users/yhg0112/gists{/gist_id}", "starred_url": "https://api.github.com/users/yhg0112/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yhg0112/subscriptions", "organizations_url": "https://api.github.com/users/yhg0112/orgs", "repos_url": "https://api.github.com/users/yhg0112/repos", "events_url": "https://api.github.com/users/yhg0112/events{/privacy}", "received_events_url": "https://api.github.com/users/yhg0112/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-05-06T06:47:45Z", "updated_at": "2017-05-12T04:13:23Z", "closed_at": "2017-05-12T04:13:23Z", "author_association": "NONE", "body_html": "<p>In current rnn_cell implementation, It gets inputs as 2-d tensor for cell inputs,</p>\n<p>Hence embedding layer before the actual rnn_cell gets a little bit annoying for making 'decoder'.</p>\n<p>I really like the \"wrapper\" concept in current implementation that expand by each layer with embedding, residual net, or dropouts.</p>\n<p>If rnn_cell gets list of '2-d' tensor like [inputs1, inputs2, ...], then the concept of wrapper can be fully used in decoder as well as simple rnn or encoder;</p>\n<p>for a little more concretely,<br>\nif \"embedding wrapped cell\" gets input list [inputs, context_inputs_from_attention],<br>\nthen the \"embedding wrapped cell\" can just pass [embedded_inputs, context_inputs_from_attention] to the real \"LSTM_cell\" and \"LSTM_cell\" can weight sum those inputs in inputs list.</p>\n<p>what do you think about this idea?</p>", "body_text": "In current rnn_cell implementation, It gets inputs as 2-d tensor for cell inputs,\nHence embedding layer before the actual rnn_cell gets a little bit annoying for making 'decoder'.\nI really like the \"wrapper\" concept in current implementation that expand by each layer with embedding, residual net, or dropouts.\nIf rnn_cell gets list of '2-d' tensor like [inputs1, inputs2, ...], then the concept of wrapper can be fully used in decoder as well as simple rnn or encoder;\nfor a little more concretely,\nif \"embedding wrapped cell\" gets input list [inputs, context_inputs_from_attention],\nthen the \"embedding wrapped cell\" can just pass [embedded_inputs, context_inputs_from_attention] to the real \"LSTM_cell\" and \"LSTM_cell\" can weight sum those inputs in inputs list.\nwhat do you think about this idea?", "body": "In current rnn_cell implementation, It gets inputs as 2-d tensor for cell inputs,\r\n\r\nHence embedding layer before the actual rnn_cell gets a little bit annoying for making 'decoder'.\r\n\r\nI really like the \"wrapper\" concept in current implementation that expand by each layer with embedding, residual net, or dropouts. \r\n\r\nIf rnn_cell gets list of '2-d' tensor like [inputs1, inputs2, ...], then the concept of wrapper can be fully used in decoder as well as simple rnn or encoder;\r\n\r\nfor a little more concretely, \r\nif \"embedding wrapped cell\" gets input list [inputs, context_inputs_from_attention],\r\nthen the \"embedding wrapped cell\" can just pass [embedded_inputs, context_inputs_from_attention] to the real \"LSTM_cell\" and \"LSTM_cell\" can weight sum those inputs in inputs list.\r\n\r\nwhat do you think about this idea?"}