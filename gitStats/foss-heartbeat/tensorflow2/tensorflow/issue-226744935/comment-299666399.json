{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299666399", "html_url": "https://github.com/tensorflow/tensorflow/issues/9715#issuecomment-299666399", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9715", "id": 299666399, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY2NjM5OQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-06T21:15:34Z", "updated_at": "2017-05-06T21:15:34Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Does the new attention + decoder API in tf.contrib.seq2seq (in the\nnightlies) do what you want?\n\nIt seems like what you're currently proposing is a pretty major overhaul of\nthe LSTM and embedding wrapper APIs that would not be backwards\ncompatible.  I think we try to avoid doing this in the new AttentionWrapper\nin tf.contrib.seq2seq, and provide a variety of decoding mechanisms there\nas well.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sat, May 6, 2017 at 12:06 PM, drpngx ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; does that make sense?\n\n <a class=\"user-mention\" href=\"https://github.com/yhg0112\">@yhg0112</a> &lt;<a href=\"https://github.com/yhg0112\">https://github.com/yhg0112</a>&gt; would you be willing to send a PR?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"226744935\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9715\" href=\"https://github.com/tensorflow/tensorflow/issues/9715#issuecomment-299659814\">#9715 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim1QWmzf7BDeEFTeAkTGhU93XUSLDks5r3MS3gaJpZM4NSpk6\">https://github.com/notifications/unsubscribe-auth/ABtim1QWmzf7BDeEFTeAkTGhU93XUSLDks5r3MS3gaJpZM4NSpk6</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Does the new attention + decoder API in tf.contrib.seq2seq (in the\nnightlies) do what you want?\n\nIt seems like what you're currently proposing is a pretty major overhaul of\nthe LSTM and embedding wrapper APIs that would not be backwards\ncompatible.  I think we try to avoid doing this in the new AttentionWrapper\nin tf.contrib.seq2seq, and provide a variety of decoding mechanisms there\nas well.\n\u2026\nOn Sat, May 6, 2017 at 12:06 PM, drpngx ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> does that make sense?\n\n @yhg0112 <https://github.com/yhg0112> would you be willing to send a PR?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#9715 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim1QWmzf7BDeEFTeAkTGhU93XUSLDks5r3MS3gaJpZM4NSpk6>\n .", "body": "Does the new attention + decoder API in tf.contrib.seq2seq (in the\nnightlies) do what you want?\n\nIt seems like what you're currently proposing is a pretty major overhaul of\nthe LSTM and embedding wrapper APIs that would not be backwards\ncompatible.  I think we try to avoid doing this in the new AttentionWrapper\nin tf.contrib.seq2seq, and provide a variety of decoding mechanisms there\nas well.\n\nOn Sat, May 6, 2017 at 12:06 PM, drpngx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> does that make sense?\n>\n> @yhg0112 <https://github.com/yhg0112> would you be willing to send a PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9715#issuecomment-299659814>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1QWmzf7BDeEFTeAkTGhU93XUSLDks5r3MS3gaJpZM4NSpk6>\n> .\n>\n"}