{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18316", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18316/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18316/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18316/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18316", "id": 312255830, "node_id": "MDU6SXNzdWUzMTIyNTU4MzA=", "number": 18316, "title": "TensorFlow-GPU random GPU crashes while training", "user": {"login": "NightCrawler96", "id": 37083039, "node_id": "MDQ6VXNlcjM3MDgzMDM5", "avatar_url": "https://avatars3.githubusercontent.com/u/37083039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NightCrawler96", "html_url": "https://github.com/NightCrawler96", "followers_url": "https://api.github.com/users/NightCrawler96/followers", "following_url": "https://api.github.com/users/NightCrawler96/following{/other_user}", "gists_url": "https://api.github.com/users/NightCrawler96/gists{/gist_id}", "starred_url": "https://api.github.com/users/NightCrawler96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NightCrawler96/subscriptions", "organizations_url": "https://api.github.com/users/NightCrawler96/orgs", "repos_url": "https://api.github.com/users/NightCrawler96/repos", "events_url": "https://api.github.com/users/NightCrawler96/events{/privacy}", "received_events_url": "https://api.github.com/users/NightCrawler96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-04-08T00:18:36Z", "updated_at": "2018-09-08T14:47:14Z", "closed_at": "2018-08-30T22:50:00Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Custom code - Simple q learning model using Keras with TF and Open AI Gym</li>\n<li>Windows 10</li>\n<li>Tensorflow installed with pip</li>\n<li>TensorFlow-gpu 1.7.0</li>\n<li>Python 3.6.4</li>\n<li>CUDA 9.0, cuDNN 7</li>\n<li>Nvidia GeForce GTX 1050 Ti GDDR5 4GB</li>\n</ul>\n<h3>Describtion</h3>\n<p>When I'm running a script randomly two types of errors happen:</p>\n<ul>\n<li>Blue Screen with Driver Power State Failure</li>\n<li>TensorFlow cannot locate gpu \" failed call to cuInit: CUDA_ERROR_NO_DEVICE\"</li>\n</ul>\n<p>This happens approximately once per 10 runs. GeForce is my secondary gpu card, so I assume, that it is not related to watchdog timer.</p>\n<p>Result od deviceQuery:</p>\n<blockquote>\n<p>Device 0: \"GeForce GTX 1050 Ti\"<br>\nCUDA Driver Version / Runtime Version          9.0 / 9.0<br>\nCUDA Capability Major/Minor version number:    6.1<br>\nTotal amount of global memory:                 4096 MBytes (4294967296 bytes)<br>\n( 6) Multiprocessors, (128) CUDA Cores/MP:     768 CUDA Cores<br>\nGPU Max Clock rate:                            1620 MHz (1.62 GHz)<br>\nMemory Clock rate:                             3504 Mhz<br>\nMemory Bus Width:                              128-bit<br>\nL2 Cache Size:                                 1048576 bytes<br>\nMaximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)<br>\nMaximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers<br>\nMaximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers<br>\nTotal amount of constant memory:               65536 bytes<br>\nTotal amount of shared memory per block:       49152 bytes<br>\nTotal number of registers available per block: 65536<br>\nWarp size:                                     32<br>\nMaximum number of threads per multiprocessor:  2048<br>\nMaximum number of threads per block:           1024<br>\nMax dimension size of a thread block (x,y,z): (1024, 1024, 64)<br>\nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)<br>\nMaximum memory pitch:                          2147483647 bytes<br>\nTexture alignment:                             512 bytes<br>\nConcurrent copy and kernel execution:          Yes with 2 copy engine(s)<br>\nRun time limit on kernels:                     Yes<br>\nIntegrated GPU sharing Host Memory:            No<br>\nSupport host page-locked memory mapping:       Yes<br>\nAlignment requirement for Surfaces:            Yes<br>\nDevice has ECC support:                        Disabled<br>\nCUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)<br>\nDevice supports Unified Addressing (UVA):      Yes<br>\nSupports Cooperative Kernel Launch:            No<br>\nSupports MultiDevice Co-op Kernel Launch:      No<br>\nDevice PCI Domain ID / Bus ID / location ID:   0 / 1 / 0<br>\nCompute Mode:<br>\n&lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</p>\n<p>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1<br>\nResult = PASS</p>\n</blockquote>", "body_text": "System information\n\nCustom code - Simple q learning model using Keras with TF and Open AI Gym\nWindows 10\nTensorflow installed with pip\nTensorFlow-gpu 1.7.0\nPython 3.6.4\nCUDA 9.0, cuDNN 7\nNvidia GeForce GTX 1050 Ti GDDR5 4GB\n\nDescribtion\nWhen I'm running a script randomly two types of errors happen:\n\nBlue Screen with Driver Power State Failure\nTensorFlow cannot locate gpu \" failed call to cuInit: CUDA_ERROR_NO_DEVICE\"\n\nThis happens approximately once per 10 runs. GeForce is my secondary gpu card, so I assume, that it is not related to watchdog timer.\nResult od deviceQuery:\n\nDevice 0: \"GeForce GTX 1050 Ti\"\nCUDA Driver Version / Runtime Version          9.0 / 9.0\nCUDA Capability Major/Minor version number:    6.1\nTotal amount of global memory:                 4096 MBytes (4294967296 bytes)\n( 6) Multiprocessors, (128) CUDA Cores/MP:     768 CUDA Cores\nGPU Max Clock rate:                            1620 MHz (1.62 GHz)\nMemory Clock rate:                             3504 Mhz\nMemory Bus Width:                              128-bit\nL2 Cache Size:                                 1048576 bytes\nMaximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\nMaximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\nMaximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\nTotal amount of constant memory:               65536 bytes\nTotal amount of shared memory per block:       49152 bytes\nTotal number of registers available per block: 65536\nWarp size:                                     32\nMaximum number of threads per multiprocessor:  2048\nMaximum number of threads per block:           1024\nMax dimension size of a thread block (x,y,z): (1024, 1024, 64)\nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\nMaximum memory pitch:                          2147483647 bytes\nTexture alignment:                             512 bytes\nConcurrent copy and kernel execution:          Yes with 2 copy engine(s)\nRun time limit on kernels:                     Yes\nIntegrated GPU sharing Host Memory:            No\nSupport host page-locked memory mapping:       Yes\nAlignment requirement for Surfaces:            Yes\nDevice has ECC support:                        Disabled\nCUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\nDevice supports Unified Addressing (UVA):      Yes\nSupports Cooperative Kernel Launch:            No\nSupports MultiDevice Co-op Kernel Launch:      No\nDevice PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\nCompute Mode:\n< Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1\nResult = PASS", "body": "### System information\r\n- Custom code - Simple q learning model using Keras with TF and Open AI Gym\r\n- Windows 10\r\n- Tensorflow installed with pip\r\n- TensorFlow-gpu 1.7.0\r\n- Python 3.6.4\r\n- CUDA 9.0, cuDNN 7\r\n- Nvidia GeForce GTX 1050 Ti GDDR5 4GB\r\n\r\n### Describtion\r\nWhen I'm running a script randomly two types of errors happen:\r\n\r\n-  Blue Screen with Driver Power State Failure\r\n- TensorFlow cannot locate gpu \" failed call to cuInit: CUDA_ERROR_NO_DEVICE\"\r\n\r\nThis happens approximately once per 10 runs. GeForce is my secondary gpu card, so I assume, that it is not related to watchdog timer.\r\n\r\nResult od deviceQuery:\r\n> Device 0: \"GeForce GTX 1050 Ti\"\r\n>   CUDA Driver Version / Runtime Version          9.0 / 9.0\r\n>   CUDA Capability Major/Minor version number:    6.1\r\n>   Total amount of global memory:                 4096 MBytes (4294967296 bytes)\r\n>   ( 6) Multiprocessors, (128) CUDA Cores/MP:     768 CUDA Cores\r\n>   GPU Max Clock rate:                            1620 MHz (1.62 GHz)\r\n>   Memory Clock rate:                             3504 Mhz\r\n>   Memory Bus Width:                              128-bit\r\n>   L2 Cache Size:                                 1048576 bytes\r\n>   Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n>   Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n>   Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n>   Total amount of constant memory:               65536 bytes\r\n>   Total amount of shared memory per block:       49152 bytes\r\n>   Total number of registers available per block: 65536\r\n>   Warp size:                                     32\r\n>   Maximum number of threads per multiprocessor:  2048\r\n>   Maximum number of threads per block:           1024\r\n>   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n>   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n>   Maximum memory pitch:                          2147483647 bytes\r\n>   Texture alignment:                             512 bytes\r\n>   Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n>   Run time limit on kernels:                     Yes\r\n>   Integrated GPU sharing Host Memory:            No\r\n>   Support host page-locked memory mapping:       Yes\r\n>   Alignment requirement for Surfaces:            Yes\r\n>   Device has ECC support:                        Disabled\r\n>   CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n>   Device supports Unified Addressing (UVA):      Yes\r\n>   Supports Cooperative Kernel Launch:            No\r\n>   Supports MultiDevice Co-op Kernel Launch:      No\r\n>   Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n>   Compute Mode:\r\n>      < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n> \r\n> deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.0, CUDA Runtime Version = 9.0, NumDevs = 1\r\n> Result = PASS\r\n"}