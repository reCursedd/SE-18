{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286118241", "html_url": "https://github.com/tensorflow/tensorflow/issues/6414#issuecomment-286118241", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6414", "id": 286118241, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjExODI0MQ==", "user": {"login": "Hvass-Labs", "id": 13588114, "node_id": "MDQ6VXNlcjEzNTg4MTE0", "avatar_url": "https://avatars2.githubusercontent.com/u/13588114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hvass-Labs", "html_url": "https://github.com/Hvass-Labs", "followers_url": "https://api.github.com/users/Hvass-Labs/followers", "following_url": "https://api.github.com/users/Hvass-Labs/following{/other_user}", "gists_url": "https://api.github.com/users/Hvass-Labs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hvass-Labs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hvass-Labs/subscriptions", "organizations_url": "https://api.github.com/users/Hvass-Labs/orgs", "repos_url": "https://api.github.com/users/Hvass-Labs/repos", "events_url": "https://api.github.com/users/Hvass-Labs/events{/privacy}", "received_events_url": "https://api.github.com/users/Hvass-Labs/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-13T14:11:44Z", "updated_at": "2017-03-13T14:11:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm sorry for the delay, I was doing some research that dragged out.</p>\n<p>When I proposed this feature I thought it might be useful in many other scenarios. But after more consideration, I don't think the idea is sufficiently general and mature at this stage to be included in TensorFlow core.</p>\n<p>I have made a Python Notebook which demonstrates the idea I had in mind:</p>\n<p><a href=\"https://gist.github.com/Hvass-Labs/3744f05fa05d6ef82dc873c4c32737ef\">https://gist.github.com/Hvass-Labs/3744f05fa05d6ef82dc873c4c32737ef</a></p>\n<p>Try switching the bool <code>use_loss_normalization</code> and see what happens to the resulting image at the bottom. However, this is not the best example because the two loss-functions are fairly close in value.</p>\n<p>This type of loss-normalization would work better for loss-functions that are several orders of magnitude in difference, where one loss-function would tend to dominate, and whenever you change something in the configuration of the problem, the relative values of the loss-functions would change so you also needed to change the weights. In such cases, automatic loss-normalization would be useful.</p>\n<p>Perhaps someone in the future might find the idea useful and could build on this. This type of loss-normalization might make a nice little research paper if someone is interested in doing it (I don't have time and I have many other ideas to work on).</p>", "body_text": "I'm sorry for the delay, I was doing some research that dragged out.\nWhen I proposed this feature I thought it might be useful in many other scenarios. But after more consideration, I don't think the idea is sufficiently general and mature at this stage to be included in TensorFlow core.\nI have made a Python Notebook which demonstrates the idea I had in mind:\nhttps://gist.github.com/Hvass-Labs/3744f05fa05d6ef82dc873c4c32737ef\nTry switching the bool use_loss_normalization and see what happens to the resulting image at the bottom. However, this is not the best example because the two loss-functions are fairly close in value.\nThis type of loss-normalization would work better for loss-functions that are several orders of magnitude in difference, where one loss-function would tend to dominate, and whenever you change something in the configuration of the problem, the relative values of the loss-functions would change so you also needed to change the weights. In such cases, automatic loss-normalization would be useful.\nPerhaps someone in the future might find the idea useful and could build on this. This type of loss-normalization might make a nice little research paper if someone is interested in doing it (I don't have time and I have many other ideas to work on).", "body": "I'm sorry for the delay, I was doing some research that dragged out.\r\n\r\nWhen I proposed this feature I thought it might be useful in many other scenarios. But after more consideration, I don't think the idea is sufficiently general and mature at this stage to be included in TensorFlow core.\r\n\r\nI have made a Python Notebook which demonstrates the idea I had in mind:\r\n\r\nhttps://gist.github.com/Hvass-Labs/3744f05fa05d6ef82dc873c4c32737ef\r\n\r\nTry switching the bool `use_loss_normalization` and see what happens to the resulting image at the bottom. However, this is not the best example because the two loss-functions are fairly close in value.\r\n\r\nThis type of loss-normalization would work better for loss-functions that are several orders of magnitude in difference, where one loss-function would tend to dominate, and whenever you change something in the configuration of the problem, the relative values of the loss-functions would change so you also needed to change the weights. In such cases, automatic loss-normalization would be useful.\r\n\r\nPerhaps someone in the future might find the idea useful and could build on this. This type of loss-normalization might make a nice little research paper if someone is interested in doing it (I don't have time and I have many other ideas to work on).\r\n"}