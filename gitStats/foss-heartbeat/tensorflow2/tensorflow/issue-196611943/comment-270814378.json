{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/270814378", "html_url": "https://github.com/tensorflow/tensorflow/issues/6414#issuecomment-270814378", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6414", "id": 270814378, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MDgxNDM3OA==", "user": {"login": "ilblackdragon", "id": 175486, "node_id": "MDQ6VXNlcjE3NTQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/175486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilblackdragon", "html_url": "https://github.com/ilblackdragon", "followers_url": "https://api.github.com/users/ilblackdragon/followers", "following_url": "https://api.github.com/users/ilblackdragon/following{/other_user}", "gists_url": "https://api.github.com/users/ilblackdragon/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilblackdragon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilblackdragon/subscriptions", "organizations_url": "https://api.github.com/users/ilblackdragon/orgs", "repos_url": "https://api.github.com/users/ilblackdragon/repos", "events_url": "https://api.github.com/users/ilblackdragon/events{/privacy}", "received_events_url": "https://api.github.com/users/ilblackdragon/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-06T02:18:07Z", "updated_at": "2017-01-06T02:18:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13588114\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Hvass-Labs\">@Hvass-Labs</a> You current implementation of <code>loss_normalize</code> will always return 1.0 if losses are already scalars [which they usually are after calling for example <code>tf.losses.*</code>]:</p>\n<pre><code>In [8]: def loss_normalize(loss):\n        loss_value = tf.Variable(1.0)\n        return loss / loss_value.assign(loss)\n\nIn [10]: with tf.Session() as sess:\n    print(sess.run(loss_normalize(1.5)))\n\n1.0\n</code></pre>\n<p>If you are just trying to normalize for example a vector or matrix of raw errors [for example <code>tf.abs(a - b)</code> part of <code>tf.reduce_mean(tf.abs(a - b))</code> - the absolute mean error loss], you can use one of <a href=\"https://www.tensorflow.org/api_docs/python/nn/normalization\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/nn/normalization</a> tools.</p>\n<p>If I understood your Ipython Notebook - you are suggesting to use normalization by dividing on previous value of this tensor. You can add it to the tools in the Normalization list - this doesn't need to be loss specific. To perform updates at correct time add the assign op to <code>GraphKeys.UPDATE_OPS</code>, which will be auto-added to list of ran tensors by <code>optimize_loss</code>.</p>\n<p>Also consider using tf.nn.moments and tf.nn.normalize_moments as it may be a better estimation of what you are trying to do.</p>\n<p>Let me know if this makes sense to you!</p>", "body_text": "@Hvass-Labs You current implementation of loss_normalize will always return 1.0 if losses are already scalars [which they usually are after calling for example tf.losses.*]:\nIn [8]: def loss_normalize(loss):\n        loss_value = tf.Variable(1.0)\n        return loss / loss_value.assign(loss)\n\nIn [10]: with tf.Session() as sess:\n    print(sess.run(loss_normalize(1.5)))\n\n1.0\n\nIf you are just trying to normalize for example a vector or matrix of raw errors [for example tf.abs(a - b) part of tf.reduce_mean(tf.abs(a - b)) - the absolute mean error loss], you can use one of https://www.tensorflow.org/api_docs/python/nn/normalization tools.\nIf I understood your Ipython Notebook - you are suggesting to use normalization by dividing on previous value of this tensor. You can add it to the tools in the Normalization list - this doesn't need to be loss specific. To perform updates at correct time add the assign op to GraphKeys.UPDATE_OPS, which will be auto-added to list of ran tensors by optimize_loss.\nAlso consider using tf.nn.moments and tf.nn.normalize_moments as it may be a better estimation of what you are trying to do.\nLet me know if this makes sense to you!", "body": "@Hvass-Labs You current implementation of `loss_normalize` will always return 1.0 if losses are already scalars [which they usually are after calling for example `tf.losses.*`]:\r\n```\r\nIn [8]: def loss_normalize(loss):\r\n        loss_value = tf.Variable(1.0)\r\n        return loss / loss_value.assign(loss)\r\n\r\nIn [10]: with tf.Session() as sess:\r\n    print(sess.run(loss_normalize(1.5)))\r\n\r\n1.0\r\n```\r\n\r\nIf you are just trying to normalize for example a vector or matrix of raw errors [for example `tf.abs(a - b)` part of `tf.reduce_mean(tf.abs(a - b))` - the absolute mean error loss], you can use one of https://www.tensorflow.org/api_docs/python/nn/normalization tools.  \r\n\r\nIf I understood your Ipython Notebook - you are suggesting to use normalization by dividing on previous value of this tensor. You can add it to the tools in the Normalization list - this doesn't need to be loss specific. To perform updates at correct time add the assign op to `GraphKeys.UPDATE_OPS`, which will be auto-added to list of ran tensors by `optimize_loss`. \r\n\r\nAlso consider using tf.nn.moments and tf.nn.normalize_moments as it may be a better estimation of what you are trying to do.\r\n\r\nLet me know if this makes sense to you!"}