{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259164523", "html_url": "https://github.com/tensorflow/tensorflow/issues/5455#issuecomment-259164523", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5455", "id": 259164523, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTE2NDUyMw==", "user": {"login": "vasantivmahajan", "id": 17990840, "node_id": "MDQ6VXNlcjE3OTkwODQw", "avatar_url": "https://avatars0.githubusercontent.com/u/17990840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vasantivmahajan", "html_url": "https://github.com/vasantivmahajan", "followers_url": "https://api.github.com/users/vasantivmahajan/followers", "following_url": "https://api.github.com/users/vasantivmahajan/following{/other_user}", "gists_url": "https://api.github.com/users/vasantivmahajan/gists{/gist_id}", "starred_url": "https://api.github.com/users/vasantivmahajan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vasantivmahajan/subscriptions", "organizations_url": "https://api.github.com/users/vasantivmahajan/orgs", "repos_url": "https://api.github.com/users/vasantivmahajan/repos", "events_url": "https://api.github.com/users/vasantivmahajan/events{/privacy}", "received_events_url": "https://api.github.com/users/vasantivmahajan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-08T15:18:53Z", "updated_at": "2016-11-08T15:18:53Z", "author_association": "NONE", "body_html": "<p>This is the entire piece of code</p>\n<pre><code>\"\"\"Example code for TensorFlow Wide &amp; Deep Tutorial using TF.Learn API.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom tensorflow.contrib.session_bundle import exporter\nimport tempfile\nimport sys\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\"model_dir\", \"/tmp/model_dir\", \"Base directory for output models.\")\nflags.DEFINE_string(\"model_type\", \"wide_n_deep\",\n                    \"Valid model types: {'wide', 'deep', 'wide_n_deep', 'regressor'}.\")\nflags.DEFINE_integer(\"train_steps\", 100, \"Number of training steps.\")\nflags.DEFINE_string(\n    \"train_data\",\n    \"\",\n    \"Path to the training data.\")\nflags.DEFINE_string(\n    \"test_data\",\n    \"\",\n    \"Path to the test data.\")\nflags.DEFINE_string(\n    \"predict_data\",\n    \"\",\n    \"Path to the prediction data.\")\n\nCOLUMNS = [\"bid\", \"bidrequesttime\", \"crid\", \"domain\", \"lid\",\n           \"floor\", \"ip\", \"device\", \"os\", \"pubid\",\n           \"userid\", \"winprice\",\"impressionflag\"]\nLABEL_COLUMN = \"impressionflag\"\nCATEGORICAL_COLUMNS = [\"bid\",\"crid\",\"domain\",\"lid\",\"ip\",\"device\",\"os\",\"userid\"]\nCONTINUOUS_COLUMNS = [\"bidrequesttime\",\"floor\",\"pubid\",\"winprice\"]\n\n\ndef maybe_download():\n  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n  if FLAGS.train_data:\n    train_file_name = FLAGS.train_data\n  else:\n    #train_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)  # pylint: disable=line-too-long\n    #train_file_name = train_file.name\n    #train_file.close()\n    train_file_name=pd.read_csv('TrainingData_v1.0.csv', skipinitialspace=True,\n                                skiprows=1, names=COLUMNS)\n\n\n  if FLAGS.test_data:\n    test_file_name = FLAGS.test_data\n  else:\n    #test_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)  # pylint: disable=line-too-long\n    #test_file_name = test_file.name\n    #test_file.close()\n    test_file_name=pd.read_csv('TestData_v1.0.csv', skipinitialspace=True,\n                               skiprows=1, names=COLUMNS)\n\n  if FLAGS.predict_data:\n    predict_file_name = FLAGS.predict_data\n  else:\n    predict_file_name=pd.read_csv('PredictData_v1.0.csv', skipinitialspace=True,\n                                  skiprows=1, names=COLUMNS)\n\n  return train_file_name, test_file_name,predict_file_name\n\n\ndef build_estimator(model_dir):\n  \"\"\"Build an estimator.\"\"\"\n  # Sparse base columns.\n  bid=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"bid\", hash_bucket_size=10000)\n  crid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"crid\", hash_bucket_size=1000)\n  domain=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"domain\", hash_bucket_size=1000)\n  lid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"lid\", hash_bucket_size=1000)\n  ip = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"ip\", hash_bucket_size=1000)\n  device = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\"device\", keys=[\"Personal Computer\",\"Mobile/Tablet\",\"None\"])\n  os = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"os\", hash_bucket_size=1000)   \n  userid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"userid\", hash_bucket_size=1000)\n\n\n  # Continuous base columns.\n  bidrequesttime = tf.contrib.layers.real_valued_column(\"bidrequesttime\")\n  floor = tf.contrib.layers.real_valued_column(\"floor\")\n  pubid = tf.contrib.layers.real_valued_column(\"pubid\")\n  winprice = tf.contrib.layers.real_valued_column(\"winprice\")\n\n\n  # Wide columns and deep columns.\n  wide_columns = [bid,crid,domain,lid,ip,device,os,userid]\n\n  deep_columns = [\n      tf.contrib.layers.embedding_column(bid, dimension=8),\n      tf.contrib.layers.embedding_column(crid, dimension=8),\n      tf.contrib.layers.embedding_column(domain,dimension=8),\n      tf.contrib.layers.embedding_column(lid, dimension=8),\n      tf.contrib.layers.embedding_column(ip, dimension=8),\n      tf.contrib.layers.embedding_column(device, dimension=8),\n      tf.contrib.layers.embedding_column(os,dimension=8),\n      tf.contrib.layers.embedding_column(userid, dimension=8),\n      bidrequesttime,\n      floor,\n      pubid,\n      winprice,\n  ]\n\n  if FLAGS.model_type == \"wide\":\n    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n                                          feature_columns=wide_columns,\n                                          optimizer=tf.train.FtrlOptimizer(\n                                            learning_rate=0.1,\n                                            l1_regularization_strength=0.001\n                                          ))\n\n  elif FLAGS.model_type == \"deep\":\n    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n                                       feature_columns=deep_columns,\n                                       hidden_units=[10,10],\n                                       optimizer=tf.train.ProximalAdagradOptimizer(\n                                         learning_rate=0.1,\n                                         l1_regularization_strength=0.1\n                                      ) )\n\n  else:\n    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[10,20,20,10])\n\n\n  return m\n\n\ndef input_fn(df):\n  \"\"\"Input builder function.\"\"\"\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  print(\"Label was created successfully\")\n  # Returns the feature columns and the label.\n  return feature_cols, label\n\n\ndef train_and_eval():\n  \"\"\"Train and evaluate the model.\"\"\"\n\n  train_file_name, test_file_name, predict_file_name = maybe_download()\n  df_train=train_file_name\n  df_test=test_file_name\n  df_predict=predict_file_name\n  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n  print(\"model directory = %s\" % model_dir)\n  m = build_estimator(model_dir)\n  print('model successfully build!!')\n\n  with tf.Session() as sess:\n    init_op=tf.initialize_all_variables()\n    saver=tf.train.Saver()\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n    print('model successfully fit!!')\n    results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n    for key in sorted(results):\n      print(\"%s: %s\" % (key, results[key]))\n    model_exporter = exporter.Exporter(saver)\n    model_exporter.init(\n    sess.graph.as_graph_def(),\n    named_graph_signatures={\n        'inputs': exporter.generic_signature({'input': df_train}),\n        'outputs': exporter.generic_signature({'output': df_train[impressionflag]})})\n    model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\n    y = m.predict(input_fn=lambda: input_fn(df_predict))\n    print ('Predictions: {}'.format(str(y)))\n\ndef main(_):\n  train_and_eval()\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n\n</code></pre>", "body_text": "This is the entire piece of code\n\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom tensorflow.contrib.session_bundle import exporter\nimport tempfile\nimport sys\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\"model_dir\", \"/tmp/model_dir\", \"Base directory for output models.\")\nflags.DEFINE_string(\"model_type\", \"wide_n_deep\",\n                    \"Valid model types: {'wide', 'deep', 'wide_n_deep', 'regressor'}.\")\nflags.DEFINE_integer(\"train_steps\", 100, \"Number of training steps.\")\nflags.DEFINE_string(\n    \"train_data\",\n    \"\",\n    \"Path to the training data.\")\nflags.DEFINE_string(\n    \"test_data\",\n    \"\",\n    \"Path to the test data.\")\nflags.DEFINE_string(\n    \"predict_data\",\n    \"\",\n    \"Path to the prediction data.\")\n\nCOLUMNS = [\"bid\", \"bidrequesttime\", \"crid\", \"domain\", \"lid\",\n           \"floor\", \"ip\", \"device\", \"os\", \"pubid\",\n           \"userid\", \"winprice\",\"impressionflag\"]\nLABEL_COLUMN = \"impressionflag\"\nCATEGORICAL_COLUMNS = [\"bid\",\"crid\",\"domain\",\"lid\",\"ip\",\"device\",\"os\",\"userid\"]\nCONTINUOUS_COLUMNS = [\"bidrequesttime\",\"floor\",\"pubid\",\"winprice\"]\n\n\ndef maybe_download():\n  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n  if FLAGS.train_data:\n    train_file_name = FLAGS.train_data\n  else:\n    #train_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)  # pylint: disable=line-too-long\n    #train_file_name = train_file.name\n    #train_file.close()\n    train_file_name=pd.read_csv('TrainingData_v1.0.csv', skipinitialspace=True,\n                                skiprows=1, names=COLUMNS)\n\n\n  if FLAGS.test_data:\n    test_file_name = FLAGS.test_data\n  else:\n    #test_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)  # pylint: disable=line-too-long\n    #test_file_name = test_file.name\n    #test_file.close()\n    test_file_name=pd.read_csv('TestData_v1.0.csv', skipinitialspace=True,\n                               skiprows=1, names=COLUMNS)\n\n  if FLAGS.predict_data:\n    predict_file_name = FLAGS.predict_data\n  else:\n    predict_file_name=pd.read_csv('PredictData_v1.0.csv', skipinitialspace=True,\n                                  skiprows=1, names=COLUMNS)\n\n  return train_file_name, test_file_name,predict_file_name\n\n\ndef build_estimator(model_dir):\n  \"\"\"Build an estimator.\"\"\"\n  # Sparse base columns.\n  bid=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"bid\", hash_bucket_size=10000)\n  crid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"crid\", hash_bucket_size=1000)\n  domain=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"domain\", hash_bucket_size=1000)\n  lid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"lid\", hash_bucket_size=1000)\n  ip = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"ip\", hash_bucket_size=1000)\n  device = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\"device\", keys=[\"Personal Computer\",\"Mobile/Tablet\",\"None\"])\n  os = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"os\", hash_bucket_size=1000)   \n  userid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"userid\", hash_bucket_size=1000)\n\n\n  # Continuous base columns.\n  bidrequesttime = tf.contrib.layers.real_valued_column(\"bidrequesttime\")\n  floor = tf.contrib.layers.real_valued_column(\"floor\")\n  pubid = tf.contrib.layers.real_valued_column(\"pubid\")\n  winprice = tf.contrib.layers.real_valued_column(\"winprice\")\n\n\n  # Wide columns and deep columns.\n  wide_columns = [bid,crid,domain,lid,ip,device,os,userid]\n\n  deep_columns = [\n      tf.contrib.layers.embedding_column(bid, dimension=8),\n      tf.contrib.layers.embedding_column(crid, dimension=8),\n      tf.contrib.layers.embedding_column(domain,dimension=8),\n      tf.contrib.layers.embedding_column(lid, dimension=8),\n      tf.contrib.layers.embedding_column(ip, dimension=8),\n      tf.contrib.layers.embedding_column(device, dimension=8),\n      tf.contrib.layers.embedding_column(os,dimension=8),\n      tf.contrib.layers.embedding_column(userid, dimension=8),\n      bidrequesttime,\n      floor,\n      pubid,\n      winprice,\n  ]\n\n  if FLAGS.model_type == \"wide\":\n    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n                                          feature_columns=wide_columns,\n                                          optimizer=tf.train.FtrlOptimizer(\n                                            learning_rate=0.1,\n                                            l1_regularization_strength=0.001\n                                          ))\n\n  elif FLAGS.model_type == \"deep\":\n    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n                                       feature_columns=deep_columns,\n                                       hidden_units=[10,10],\n                                       optimizer=tf.train.ProximalAdagradOptimizer(\n                                         learning_rate=0.1,\n                                         l1_regularization_strength=0.1\n                                      ) )\n\n  else:\n    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[10,20,20,10])\n\n\n  return m\n\n\ndef input_fn(df):\n  \"\"\"Input builder function.\"\"\"\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  print(\"Label was created successfully\")\n  # Returns the feature columns and the label.\n  return feature_cols, label\n\n\ndef train_and_eval():\n  \"\"\"Train and evaluate the model.\"\"\"\n\n  train_file_name, test_file_name, predict_file_name = maybe_download()\n  df_train=train_file_name\n  df_test=test_file_name\n  df_predict=predict_file_name\n  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n  print(\"model directory = %s\" % model_dir)\n  m = build_estimator(model_dir)\n  print('model successfully build!!')\n\n  with tf.Session() as sess:\n    init_op=tf.initialize_all_variables()\n    saver=tf.train.Saver()\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n    print('model successfully fit!!')\n    results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n    for key in sorted(results):\n      print(\"%s: %s\" % (key, results[key]))\n    model_exporter = exporter.Exporter(saver)\n    model_exporter.init(\n    sess.graph.as_graph_def(),\n    named_graph_signatures={\n        'inputs': exporter.generic_signature({'input': df_train}),\n        'outputs': exporter.generic_signature({'output': df_train[impressionflag]})})\n    model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\n    y = m.predict(input_fn=lambda: input_fn(df_predict))\n    print ('Predictions: {}'.format(str(y)))\n\ndef main(_):\n  train_and_eval()\n\n\nif __name__ == \"__main__\":\n  tf.app.run()", "body": "This is the entire piece of code\n\n```\n\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom tensorflow.contrib.session_bundle import exporter\nimport tempfile\nimport sys\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\"model_dir\", \"/tmp/model_dir\", \"Base directory for output models.\")\nflags.DEFINE_string(\"model_type\", \"wide_n_deep\",\n                    \"Valid model types: {'wide', 'deep', 'wide_n_deep', 'regressor'}.\")\nflags.DEFINE_integer(\"train_steps\", 100, \"Number of training steps.\")\nflags.DEFINE_string(\n    \"train_data\",\n    \"\",\n    \"Path to the training data.\")\nflags.DEFINE_string(\n    \"test_data\",\n    \"\",\n    \"Path to the test data.\")\nflags.DEFINE_string(\n    \"predict_data\",\n    \"\",\n    \"Path to the prediction data.\")\n\nCOLUMNS = [\"bid\", \"bidrequesttime\", \"crid\", \"domain\", \"lid\",\n           \"floor\", \"ip\", \"device\", \"os\", \"pubid\",\n           \"userid\", \"winprice\",\"impressionflag\"]\nLABEL_COLUMN = \"impressionflag\"\nCATEGORICAL_COLUMNS = [\"bid\",\"crid\",\"domain\",\"lid\",\"ip\",\"device\",\"os\",\"userid\"]\nCONTINUOUS_COLUMNS = [\"bidrequesttime\",\"floor\",\"pubid\",\"winprice\"]\n\n\ndef maybe_download():\n  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n  if FLAGS.train_data:\n    train_file_name = FLAGS.train_data\n  else:\n    #train_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)  # pylint: disable=line-too-long\n    #train_file_name = train_file.name\n    #train_file.close()\n    train_file_name=pd.read_csv('TrainingData_v1.0.csv', skipinitialspace=True,\n                                skiprows=1, names=COLUMNS)\n\n\n  if FLAGS.test_data:\n    test_file_name = FLAGS.test_data\n  else:\n    #test_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)  # pylint: disable=line-too-long\n    #test_file_name = test_file.name\n    #test_file.close()\n    test_file_name=pd.read_csv('TestData_v1.0.csv', skipinitialspace=True,\n                               skiprows=1, names=COLUMNS)\n\n  if FLAGS.predict_data:\n    predict_file_name = FLAGS.predict_data\n  else:\n    predict_file_name=pd.read_csv('PredictData_v1.0.csv', skipinitialspace=True,\n                                  skiprows=1, names=COLUMNS)\n\n  return train_file_name, test_file_name,predict_file_name\n\n\ndef build_estimator(model_dir):\n  \"\"\"Build an estimator.\"\"\"\n  # Sparse base columns.\n  bid=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"bid\", hash_bucket_size=10000)\n  crid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"crid\", hash_bucket_size=1000)\n  domain=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"domain\", hash_bucket_size=1000)\n  lid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"lid\", hash_bucket_size=1000)\n  ip = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"ip\", hash_bucket_size=1000)\n  device = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\"device\", keys=[\"Personal Computer\",\"Mobile/Tablet\",\"None\"])\n  os = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"os\", hash_bucket_size=1000)   \n  userid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"userid\", hash_bucket_size=1000)\n\n\n  # Continuous base columns.\n  bidrequesttime = tf.contrib.layers.real_valued_column(\"bidrequesttime\")\n  floor = tf.contrib.layers.real_valued_column(\"floor\")\n  pubid = tf.contrib.layers.real_valued_column(\"pubid\")\n  winprice = tf.contrib.layers.real_valued_column(\"winprice\")\n\n\n  # Wide columns and deep columns.\n  wide_columns = [bid,crid,domain,lid,ip,device,os,userid]\n\n  deep_columns = [\n      tf.contrib.layers.embedding_column(bid, dimension=8),\n      tf.contrib.layers.embedding_column(crid, dimension=8),\n      tf.contrib.layers.embedding_column(domain,dimension=8),\n      tf.contrib.layers.embedding_column(lid, dimension=8),\n      tf.contrib.layers.embedding_column(ip, dimension=8),\n      tf.contrib.layers.embedding_column(device, dimension=8),\n      tf.contrib.layers.embedding_column(os,dimension=8),\n      tf.contrib.layers.embedding_column(userid, dimension=8),\n      bidrequesttime,\n      floor,\n      pubid,\n      winprice,\n  ]\n\n  if FLAGS.model_type == \"wide\":\n    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n                                          feature_columns=wide_columns,\n                                          optimizer=tf.train.FtrlOptimizer(\n                                            learning_rate=0.1,\n                                            l1_regularization_strength=0.001\n                                          ))\n\n  elif FLAGS.model_type == \"deep\":\n    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n                                       feature_columns=deep_columns,\n                                       hidden_units=[10,10],\n                                       optimizer=tf.train.ProximalAdagradOptimizer(\n                                         learning_rate=0.1,\n                                         l1_regularization_strength=0.1\n                                      ) )\n\n  else:\n    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[10,20,20,10])\n\n\n  return m\n\n\ndef input_fn(df):\n  \"\"\"Input builder function.\"\"\"\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  print(\"Label was created successfully\")\n  # Returns the feature columns and the label.\n  return feature_cols, label\n\n\ndef train_and_eval():\n  \"\"\"Train and evaluate the model.\"\"\"\n\n  train_file_name, test_file_name, predict_file_name = maybe_download()\n  df_train=train_file_name\n  df_test=test_file_name\n  df_predict=predict_file_name\n  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n  print(\"model directory = %s\" % model_dir)\n  m = build_estimator(model_dir)\n  print('model successfully build!!')\n\n  with tf.Session() as sess:\n    init_op=tf.initialize_all_variables()\n    saver=tf.train.Saver()\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n    print('model successfully fit!!')\n    results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n    for key in sorted(results):\n      print(\"%s: %s\" % (key, results[key]))\n    model_exporter = exporter.Exporter(saver)\n    model_exporter.init(\n    sess.graph.as_graph_def(),\n    named_graph_signatures={\n        'inputs': exporter.generic_signature({'input': df_train}),\n        'outputs': exporter.generic_signature({'output': df_train[impressionflag]})})\n    model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\n    y = m.predict(input_fn=lambda: input_fn(df_predict))\n    print ('Predictions: {}'.format(str(y)))\n\ndef main(_):\n  train_and_eval()\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n\n```\n"}