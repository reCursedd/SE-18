{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17905", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17905/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17905/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17905/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17905", "id": 307418358, "node_id": "MDU6SXNzdWUzMDc0MTgzNTg=", "number": 17905, "title": "GPU sits idle for increasing time between consecutive training runs", "user": {"login": "androidbeepboop", "id": 37384864, "node_id": "MDQ6VXNlcjM3Mzg0ODY0", "avatar_url": "https://avatars0.githubusercontent.com/u/37384864?v=4", "gravatar_id": "", "url": "https://api.github.com/users/androidbeepboop", "html_url": "https://github.com/androidbeepboop", "followers_url": "https://api.github.com/users/androidbeepboop/followers", "following_url": "https://api.github.com/users/androidbeepboop/following{/other_user}", "gists_url": "https://api.github.com/users/androidbeepboop/gists{/gist_id}", "starred_url": "https://api.github.com/users/androidbeepboop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/androidbeepboop/subscriptions", "organizations_url": "https://api.github.com/users/androidbeepboop/orgs", "repos_url": "https://api.github.com/users/androidbeepboop/repos", "events_url": "https://api.github.com/users/androidbeepboop/events{/privacy}", "received_events_url": "https://api.github.com/users/androidbeepboop/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-03-21T21:07:21Z", "updated_at": "2018-05-04T19:36:50Z", "closed_at": "2018-05-04T19:36:50Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10 Version 10.0.16299</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: not sure (is this important here?)</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: not sure (is this important here?)</li>\n<li><strong>CUDA/cuDNN version</strong>: not sure (is this important here?)</li>\n<li><strong>GPU model and memory</strong>: NVIDIA Quadro P5000 16GB</li>\n<li><strong>Exact command to reproduce</strong>: Not exactly an \"exact command\", but what I'm trying to do is perform CNN training runs consecutively without the GPU stalling between subsequent runs.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<p>I'm using an evolutionary algorithm to optimize the hyperparameters of a CNN. However, over time, the GPU \"waits\" for greater and greater times between training cycles (afterburner visualization):</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/37384864/37736912-93ab0f72-2d28-11e8-837b-19c4da783251.png\"><img src=\"https://user-images.githubusercontent.com/37384864/37736912-93ab0f72-2d28-11e8-837b-19c4da783251.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>I assume this is a memory issue in a similar vein to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"297508374\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/17048\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/17048/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/17048\">#17048</a> and <a href=\"http://forums.fast.ai/t/tip-clear-tensorflow-gpu-memory/1979\" rel=\"nofollow\">http://forums.fast.ai/t/tip-clear-tensorflow-gpu-memory/1979</a>, however, the fixes mentioned there are not working.</p>\n<p>Any ideas of a workaround? The second link above uses Keras to reset GPU memory after each run, and have tried this, but it has not solved the problem (it may have somewhat shortened the lag between training runs, but not enough to constitute a fix).</p>\n<p>What do you suppose is going on here? I.e. what may be the source of the problem, and resources that could help me address it?</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.<br>\n(I'm not sure source code will be helpful in this general issue; let me know if you'd like to see my code).</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 10.0.16299\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.5.0\nPython version: 3.6\nBazel version (if compiling from source): not sure (is this important here?)\nGCC/Compiler version (if compiling from source): not sure (is this important here?)\nCUDA/cuDNN version: not sure (is this important here?)\nGPU model and memory: NVIDIA Quadro P5000 16GB\nExact command to reproduce: Not exactly an \"exact command\", but what I'm trying to do is perform CNN training runs consecutively without the GPU stalling between subsequent runs.\n\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nI'm using an evolutionary algorithm to optimize the hyperparameters of a CNN. However, over time, the GPU \"waits\" for greater and greater times between training cycles (afterburner visualization):\n\nI assume this is a memory issue in a similar vein to #17048 and http://forums.fast.ai/t/tip-clear-tensorflow-gpu-memory/1979, however, the fixes mentioned there are not working.\nAny ideas of a workaround? The second link above uses Keras to reset GPU memory after each run, and have tried this, but it has not solved the problem (it may have somewhat shortened the lag between training runs, but not enough to constitute a fix).\nWhat do you suppose is going on here? I.e. what may be the source of the problem, and resources that could help me address it?\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\n(I'm not sure source code will be helpful in this general issue; let me know if you'd like to see my code).", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Version 10.0.16299 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: not sure (is this important here?)\r\n- **GCC/Compiler version (if compiling from source)**: not sure (is this important here?)\r\n- **CUDA/cuDNN version**: not sure (is this important here?)\r\n- **GPU model and memory**: NVIDIA Quadro P5000 16GB\r\n- **Exact command to reproduce**: Not exactly an \"exact command\", but what I'm trying to do is perform CNN training runs consecutively without the GPU stalling between subsequent runs. \r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI'm using an evolutionary algorithm to optimize the hyperparameters of a CNN. However, over time, the GPU \"waits\" for greater and greater times between training cycles (afterburner visualization):\r\n\r\n![image](https://user-images.githubusercontent.com/37384864/37736912-93ab0f72-2d28-11e8-837b-19c4da783251.png)\r\n\r\nI assume this is a memory issue in a similar vein to https://github.com/tensorflow/tensorflow/issues/17048 and http://forums.fast.ai/t/tip-clear-tensorflow-gpu-memory/1979, however, the fixes mentioned there are not working. \r\n\r\nAny ideas of a workaround? The second link above uses Keras to reset GPU memory after each run, and have tried this, but it has not solved the problem (it may have somewhat shortened the lag between training runs, but not enough to constitute a fix). \r\n\r\nWhat do you suppose is going on here? I.e. what may be the source of the problem, and resources that could help me address it?\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n(I'm not sure source code will be helpful in this general issue; let me know if you'd like to see my code)."}