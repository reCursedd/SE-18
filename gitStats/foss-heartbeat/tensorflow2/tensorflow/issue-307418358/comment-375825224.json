{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/375825224", "html_url": "https://github.com/tensorflow/tensorflow/issues/17905#issuecomment-375825224", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17905", "id": 375825224, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTgyNTIyNA==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-23T23:36:07Z", "updated_at": "2018-03-23T23:36:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't think it is related to GPU memory usage, as you show it gets reset properly.  My only guess is that the size of the graph being built is increasing each time, rather than creating a new graph from scratch for every new hyperparameter setting.  Theoretically these should be pruned effectively, but I wonder if this is a problem.  I don't know much about Keras details, but there may be a way to reset the \"graph\" or \"session\" between model training runs.</p>\n<p>Beyond that, I don't think we have enough information here to really know why this is happening (e.g., where is the time being idle; is it in your application program, in python, at the C++ level, etc.)</p>", "body_text": "I don't think it is related to GPU memory usage, as you show it gets reset properly.  My only guess is that the size of the graph being built is increasing each time, rather than creating a new graph from scratch for every new hyperparameter setting.  Theoretically these should be pruned effectively, but I wonder if this is a problem.  I don't know much about Keras details, but there may be a way to reset the \"graph\" or \"session\" between model training runs.\nBeyond that, I don't think we have enough information here to really know why this is happening (e.g., where is the time being idle; is it in your application program, in python, at the C++ level, etc.)", "body": "I don't think it is related to GPU memory usage, as you show it gets reset properly.  My only guess is that the size of the graph being built is increasing each time, rather than creating a new graph from scratch for every new hyperparameter setting.  Theoretically these should be pruned effectively, but I wonder if this is a problem.  I don't know much about Keras details, but there may be a way to reset the \"graph\" or \"session\" between model training runs.\r\n\r\nBeyond that, I don't think we have enough information here to really know why this is happening (e.g., where is the time being idle; is it in your application program, in python, at the C++ level, etc.)"}