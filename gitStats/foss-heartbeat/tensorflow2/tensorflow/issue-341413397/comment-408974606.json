{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/408974606", "html_url": "https://github.com/tensorflow/tensorflow/issues/20829#issuecomment-408974606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20829", "id": 408974606, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODk3NDYwNg==", "user": {"login": "amadupu", "id": 14251460, "node_id": "MDQ6VXNlcjE0MjUxNDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/14251460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amadupu", "html_url": "https://github.com/amadupu", "followers_url": "https://api.github.com/users/amadupu/followers", "following_url": "https://api.github.com/users/amadupu/following{/other_user}", "gists_url": "https://api.github.com/users/amadupu/gists{/gist_id}", "starred_url": "https://api.github.com/users/amadupu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amadupu/subscriptions", "organizations_url": "https://api.github.com/users/amadupu/orgs", "repos_url": "https://api.github.com/users/amadupu/repos", "events_url": "https://api.github.com/users/amadupu/events{/privacy}", "received_events_url": "https://api.github.com/users/amadupu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-30T19:02:16Z", "updated_at": "2018-07-30T19:03:42Z", "author_association": "NONE", "body_html": "<p>Yeah...currently im setting <code>allow_smaller_final_batch=False</code> to overcome this issue. As you have rightly pointed out, im having a trouble in feeding continous sequences. so ive truncated my data into shorter segments , and relying on state to carry forward to the next batch instead resetting the state.</p>\n<p>i realize this approach is not entirely correct as the batched data is no more continous as i expect to be.<br>\nim struggling on this point for a while may you can suggest a better way to do this</p>\n<p>for eg:<br>\nlet us say, continous data resembles<code> 1,2,3........so on to 1000.</code><br>\ni have stored this data as follows<br>\nsegment input data into smaller units of length 20</p>\n<pre><code>1,2,3,....20  (seg-1)\n21,22,,,,40  (seg-2)\n981,982,,,1000   (seg- 50)\n</code></pre>\n<p>all these segments are written to tfrecords as serialized sequence examples.</p>\n<p>while training im reading the tfrecord file of fixed batch size (say 3)<br>\nRNN state S is of shape (batch size x cell size)</p>\n<pre><code>State     Batch -1                    New State\n0          1,2,3..20                      S1  \n0          21,22,...40                   S2\n0          41,42...60                    S3\n</code></pre>\n<pre><code>State     Batch -2                   New State\nS1         61,62,...80                       S4  \nS2          81,81...100                     S5\nS3         101,102,...120                 S6\n</code></pre>\n<p><code>Question: </code>is state S is correct representation of continous data? As S1,S2,S3 are representing states for different batch streams, im having a slight confusion if i need to ever use batching here or just always use batch-size of 1 with a single state</p>\n<p><code>Question: </code>Are RNN models mostly suitable for smaller length sequences rather learning patterns in continous streams? though i dont believe so, but most of the documentation are talking around mini batches with state resets, im trying to get a basic idea where RNN are perfect fit for.</p>\n<p><code>Question:</code> In what scenarios do we exactly carry forward state from one batch to other</p>", "body_text": "Yeah...currently im setting allow_smaller_final_batch=False to overcome this issue. As you have rightly pointed out, im having a trouble in feeding continous sequences. so ive truncated my data into shorter segments , and relying on state to carry forward to the next batch instead resetting the state.\ni realize this approach is not entirely correct as the batched data is no more continous as i expect to be.\nim struggling on this point for a while may you can suggest a better way to do this\nfor eg:\nlet us say, continous data resembles 1,2,3........so on to 1000.\ni have stored this data as follows\nsegment input data into smaller units of length 20\n1,2,3,....20  (seg-1)\n21,22,,,,40  (seg-2)\n981,982,,,1000   (seg- 50)\n\nall these segments are written to tfrecords as serialized sequence examples.\nwhile training im reading the tfrecord file of fixed batch size (say 3)\nRNN state S is of shape (batch size x cell size)\nState     Batch -1                    New State\n0          1,2,3..20                      S1  \n0          21,22,...40                   S2\n0          41,42...60                    S3\n\nState     Batch -2                   New State\nS1         61,62,...80                       S4  \nS2          81,81...100                     S5\nS3         101,102,...120                 S6\n\nQuestion: is state S is correct representation of continous data? As S1,S2,S3 are representing states for different batch streams, im having a slight confusion if i need to ever use batching here or just always use batch-size of 1 with a single state\nQuestion: Are RNN models mostly suitable for smaller length sequences rather learning patterns in continous streams? though i dont believe so, but most of the documentation are talking around mini batches with state resets, im trying to get a basic idea where RNN are perfect fit for.\nQuestion: In what scenarios do we exactly carry forward state from one batch to other", "body": "Yeah...currently im setting `allow_smaller_final_batch=False` to overcome this issue. As you have rightly pointed out, im having a trouble in feeding continous sequences. so ive truncated my data into shorter segments , and relying on state to carry forward to the next batch instead resetting the state. \r\n\r\ni realize this approach is not entirely correct as the batched data is no more continous as i expect to be.\r\nim struggling on this point for a while may you can suggest a better way to do this\r\n\r\nfor eg:\r\nlet us say, continous data resembles` 1,2,3........so on to 1000.`\r\ni have stored this data as follows\r\nsegment input data into smaller units of length 20\r\n\r\n```\r\n1,2,3,....20  (seg-1)\r\n21,22,,,,40  (seg-2)\r\n981,982,,,1000   (seg- 50)\r\n```\r\nall these segments are written to tfrecords as serialized sequence examples.\r\n\r\nwhile training im reading the tfrecord file of fixed batch size (say 3)\r\nRNN state S is of shape (batch size x cell size)\r\n\r\n```\r\nState     Batch -1                    New State\r\n0          1,2,3..20                      S1  \r\n0          21,22,...40                   S2\r\n0          41,42...60                    S3\r\n```\r\n\r\n```\r\nState     Batch -2                   New State\r\nS1         61,62,...80                       S4  \r\nS2          81,81...100                     S5\r\nS3         101,102,...120                 S6\r\n```\r\n\r\n\r\n`Question: `is state S is correct representation of continous data? As S1,S2,S3 are representing states for different batch streams, im having a slight confusion if i need to ever use batching here or just always use batch-size of 1 with a single state \r\n\r\n`Question: `Are RNN models mostly suitable for smaller length sequences rather learning patterns in continous streams? though i dont believe so, but most of the documentation are talking around mini batches with state resets, im trying to get a basic idea where RNN are perfect fit for. \r\n\r\n`Question:` In what scenarios do we exactly carry forward state from one batch to other\r\n\r\n"}