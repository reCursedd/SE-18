{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/409063160", "html_url": "https://github.com/tensorflow/tensorflow/issues/20829#issuecomment-409063160", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20829", "id": 409063160, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTA2MzE2MA==", "user": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T01:23:35Z", "updated_at": "2018-07-31T01:24:30Z", "author_association": "MEMBER", "body_html": "<p>So,</p>\n<ol>\n<li>I'm not sure what you mean by continuous. Is that just long sequences?</li>\n<li>RNNs are good for smaller sequences, they will not work well long sequences, even if you use LSTMs.</li>\n<li>Yes, you can carry state forward, as long as you don't care about the gradient. If the sequence is long, it's probably OK but not great.</li>\n</ol>\n<p>Imagine you have two sequences, <code>s</code>, and <code>r</code>. The batch size will be 2.</p>\n<p>The way you would truncate the gradient is to consider the ending state fixed, so</p>\n<pre><code>State    Batch 0                New State\n0        s1, s2, ..., s20           S1\n0        r1, r2, ..., r20           R1\n</code></pre>\n<p>Then for the next batch:</p>\n<pre><code>State    Batch 1                New State\nS1       s21, s22, ..., s40         S2\nR1       r21, r22, ..., r40         R2\n</code></pre>\n<p>If you are intent on doing the right thing, you can use <code>tf.contrib.recurrent</code> to implement re-materialization (otherwise you will run out of memory). You would create a cell that performs 20 steps, then the outer cell will connect these steps together. When computing the gradient, the code will compute the forward by doing, notionally:</p>\n<pre><code>S[0] = zero_state\nfor k in xrange(0, T/20):\n  intra_S[0] = S[k]\n  for j in xrange(20):\n      intra_S[j+1] = LSTM(x[k * 20 + j], intra_S[j])\n  S[k+1] = intra_S[20]\n</code></pre>\n<p>The memory required is for <code>S</code>, and <code>intra_S</code> (which get reset every 20 steps). During the backward step, the intermediate states are computed automatically for you.</p>", "body_text": "So,\n\nI'm not sure what you mean by continuous. Is that just long sequences?\nRNNs are good for smaller sequences, they will not work well long sequences, even if you use LSTMs.\nYes, you can carry state forward, as long as you don't care about the gradient. If the sequence is long, it's probably OK but not great.\n\nImagine you have two sequences, s, and r. The batch size will be 2.\nThe way you would truncate the gradient is to consider the ending state fixed, so\nState    Batch 0                New State\n0        s1, s2, ..., s20           S1\n0        r1, r2, ..., r20           R1\n\nThen for the next batch:\nState    Batch 1                New State\nS1       s21, s22, ..., s40         S2\nR1       r21, r22, ..., r40         R2\n\nIf you are intent on doing the right thing, you can use tf.contrib.recurrent to implement re-materialization (otherwise you will run out of memory). You would create a cell that performs 20 steps, then the outer cell will connect these steps together. When computing the gradient, the code will compute the forward by doing, notionally:\nS[0] = zero_state\nfor k in xrange(0, T/20):\n  intra_S[0] = S[k]\n  for j in xrange(20):\n      intra_S[j+1] = LSTM(x[k * 20 + j], intra_S[j])\n  S[k+1] = intra_S[20]\n\nThe memory required is for S, and intra_S (which get reset every 20 steps). During the backward step, the intermediate states are computed automatically for you.", "body": "So,\r\n\r\n1. I'm not sure what you mean by continuous. Is that just long sequences?\r\n2. RNNs are good for smaller sequences, they will not work well long sequences, even if you use LSTMs.\r\n3. Yes, you can carry state forward, as long as you don't care about the gradient. If the sequence is long, it's probably OK but not great.\r\n\r\nImagine you have two sequences, `s`, and `r`. The batch size will be 2.\r\n\r\nThe way you would truncate the gradient is to consider the ending state fixed, so\r\n```\r\nState    Batch 0                New State\r\n0        s1, s2, ..., s20           S1\r\n0        r1, r2, ..., r20           R1\r\n```\r\n\r\nThen for the next batch:\r\n```\r\nState    Batch 1                New State\r\nS1       s21, s22, ..., s40         S2\r\nR1       r21, r22, ..., r40         R2\r\n```\r\n\r\nIf you are intent on doing the right thing, you can use `tf.contrib.recurrent` to implement re-materialization (otherwise you will run out of memory). You would create a cell that performs 20 steps, then the outer cell will connect these steps together. When computing the gradient, the code will compute the forward by doing, notionally:\r\n```\r\nS[0] = zero_state\r\nfor k in xrange(0, T/20):\r\n  intra_S[0] = S[k]\r\n  for j in xrange(20):\r\n      intra_S[j+1] = LSTM(x[k * 20 + j], intra_S[j])\r\n  S[k+1] = intra_S[20]\r\n```\r\nThe memory required is for `S`, and `intra_S` (which get reset every 20 steps). During the backward step, the intermediate states are computed automatically for you.\r\n"}