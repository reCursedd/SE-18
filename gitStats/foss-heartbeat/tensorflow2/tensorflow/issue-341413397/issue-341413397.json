{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20829", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20829/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20829/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20829/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20829", "id": 341413397, "node_id": "MDU6SXNzdWUzNDE0MTMzOTc=", "number": 20829, "title": "feeding back curr state to next state causing dynamic_rnn to crash if batch size is varying", "user": {"login": "amadupu", "id": 14251460, "node_id": "MDQ6VXNlcjE0MjUxNDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/14251460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amadupu", "html_url": "https://github.com/amadupu", "followers_url": "https://api.github.com/users/amadupu/followers", "following_url": "https://api.github.com/users/amadupu/following{/other_user}", "gists_url": "https://api.github.com/users/amadupu/gists{/gist_id}", "starred_url": "https://api.github.com/users/amadupu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amadupu/subscriptions", "organizations_url": "https://api.github.com/users/amadupu/orgs", "repos_url": "https://api.github.com/users/amadupu/repos", "events_url": "https://api.github.com/users/amadupu/events{/privacy}", "received_events_url": "https://api.github.com/users/amadupu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-07-16T07:23:56Z", "updated_at": "2018-08-08T16:40:25Z", "closed_at": "2018-08-08T16:40:25Z", "author_association": "NONE", "body_html": "<p>Im feeding data to the graph with tf.train.batch (dynamic_pad=True,allow_smaller_final_batch=True).<br>\nalso im feeding in the final state returned by tf.train.dynamic_rnn as the next state during the training process. (initial state configured to zeros)</p>\n<p>it was working fine, however when end of epoch is reached, due to allow_smaller_final_batch=True above, it is trying to fetch smaller batch sizes. this seems to crash the program as the last state is of standard batch size(say 100) and current batch is lesser than standard batch size (eg: 90)</p>\n<p>if i dont feed the last state, things are working fine as the state always initialized to zeros based on the current batch size. I could not find proper explanation for dynamic batch sizes (pls not not im not referring to variable sequence lenghts here)  assoiated with state feedback to ascertain if this is a bug or a coding issue</p>\n<p>kindly help</p>", "body_text": "Im feeding data to the graph with tf.train.batch (dynamic_pad=True,allow_smaller_final_batch=True).\nalso im feeding in the final state returned by tf.train.dynamic_rnn as the next state during the training process. (initial state configured to zeros)\nit was working fine, however when end of epoch is reached, due to allow_smaller_final_batch=True above, it is trying to fetch smaller batch sizes. this seems to crash the program as the last state is of standard batch size(say 100) and current batch is lesser than standard batch size (eg: 90)\nif i dont feed the last state, things are working fine as the state always initialized to zeros based on the current batch size. I could not find proper explanation for dynamic batch sizes (pls not not im not referring to variable sequence lenghts here)  assoiated with state feedback to ascertain if this is a bug or a coding issue\nkindly help", "body": "Im feeding data to the graph with tf.train.batch (dynamic_pad=True,allow_smaller_final_batch=True).\r\nalso im feeding in the final state returned by tf.train.dynamic_rnn as the next state during the training process. (initial state configured to zeros)\r\n\r\nit was working fine, however when end of epoch is reached, due to allow_smaller_final_batch=True above, it is trying to fetch smaller batch sizes. this seems to crash the program as the last state is of standard batch size(say 100) and current batch is lesser than standard batch size (eg: 90)\r\n\r\nif i dont feed the last state, things are working fine as the state always initialized to zeros based on the current batch size. I could not find proper explanation for dynamic batch sizes (pls not not im not referring to variable sequence lenghts here)  assoiated with state feedback to ascertain if this is a bug or a coding issue\r\n\r\nkindly help"}