{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8390", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8390/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8390/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8390/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8390", "id": 214008946, "node_id": "MDU6SXNzdWUyMTQwMDg5NDY=", "number": 8390, "title": "How can I use BatchNorm in a multi-GPU model", "user": {"login": "boluoweifenda", "id": 5405385, "node_id": "MDQ6VXNlcjU0MDUzODU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5405385?v=4", "gravatar_id": "", "url": "https://api.github.com/users/boluoweifenda", "html_url": "https://github.com/boluoweifenda", "followers_url": "https://api.github.com/users/boluoweifenda/followers", "following_url": "https://api.github.com/users/boluoweifenda/following{/other_user}", "gists_url": "https://api.github.com/users/boluoweifenda/gists{/gist_id}", "starred_url": "https://api.github.com/users/boluoweifenda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/boluoweifenda/subscriptions", "organizations_url": "https://api.github.com/users/boluoweifenda/orgs", "repos_url": "https://api.github.com/users/boluoweifenda/repos", "events_url": "https://api.github.com/users/boluoweifenda/events{/privacy}", "received_events_url": "https://api.github.com/users/boluoweifenda/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-14T09:14:24Z", "updated_at": "2018-09-07T06:31:16Z", "closed_at": "2017-03-15T05:59:09Z", "author_association": "NONE", "body_html": "<p>I want to use batch normalization in the <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\">cifar10 example</a> in a multi-GPU structure. All variables are stored in CPU, I build 2 Queues one for training batch and one for testing batch;  5 models( 4 for training and another for testing),</p>\n<p>GPU_tower codes like this<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/5405385/23892678/8d4c4e10-08d6-11e7-8d67-50d3bc32aec0.png\"><img src=\"https://cloud.githubusercontent.com/assets/5405385/23892678/8d4c4e10-08d6-11e7-8d67-50d3bc32aec0.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p><strong><strong>Problem is</strong>: when I add tf.contrib.batch_norm layers in my NN model . How can I  reuse those batchnorm variables ? I set reuse = True and pass namescope but get valueError. Is there a simple way to make it work? I don't want to modify a lot.</strong></p>\n<p>I know there are some high level framework such as <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py#L253\">slim with the inception</a> model with a built-in batch-norm<br>\nbut I can hardly do personal implementations (I need to add many instructions in slim.conv2d and tf.contrib.conv2d, so it's more convenient to use tf.nn.conv2d),</p>\n<p><strong>Could I just follow the inception Sync method but without high level frame like slim ?</strong><br>\nThanks a lot!</p>", "body_text": "I want to use batch normalization in the cifar10 example in a multi-GPU structure. All variables are stored in CPU, I build 2 Queues one for training batch and one for testing batch;  5 models( 4 for training and another for testing),\nGPU_tower codes like this\n\nProblem is: when I add tf.contrib.batch_norm layers in my NN model . How can I  reuse those batchnorm variables ? I set reuse = True and pass namescope but get valueError. Is there a simple way to make it work? I don't want to modify a lot.\nI know there are some high level framework such as slim with the inception model with a built-in batch-norm\nbut I can hardly do personal implementations (I need to add many instructions in slim.conv2d and tf.contrib.conv2d, so it's more convenient to use tf.nn.conv2d),\nCould I just follow the inception Sync method but without high level frame like slim ?\nThanks a lot!", "body": "I want to use batch normalization in the [cifar10 example](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py) in a multi-GPU structure. All variables are stored in CPU, I build 2 Queues one for training batch and one for testing batch;  5 models( 4 for training and another for testing),\r\n\r\n GPU_tower codes like this\r\n![image](https://cloud.githubusercontent.com/assets/5405385/23892678/8d4c4e10-08d6-11e7-8d67-50d3bc32aec0.png)\r\n\r\n****Problem is**: when I add tf.contrib.batch_norm layers in my NN model . How can I  reuse those batchnorm variables ? I set reuse = True and pass namescope but get valueError. Is there a simple way to make it work? I don't want to modify a lot.**\r\n\r\nI know there are some high level framework such as [slim with the inception](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py#L253) model with a built-in batch-norm \r\nbut I can hardly do personal implementations (I need to add many instructions in slim.conv2d and tf.contrib.conv2d, so it's more convenient to use tf.nn.conv2d), \r\n\r\n**Could I just follow the inception Sync method but without high level frame like slim ?**\r\nThanks a lot!\r\n\r\n"}