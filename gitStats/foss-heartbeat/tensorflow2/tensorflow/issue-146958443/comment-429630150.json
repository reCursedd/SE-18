{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/429630150", "html_url": "https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-429630150", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1824", "id": 429630150, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTYzMDE1MA==", "user": {"login": "Vamix", "id": 25604857, "node_id": "MDQ6VXNlcjI1NjA0ODU3", "avatar_url": "https://avatars0.githubusercontent.com/u/25604857?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Vamix", "html_url": "https://github.com/Vamix", "followers_url": "https://api.github.com/users/Vamix/followers", "following_url": "https://api.github.com/users/Vamix/following{/other_user}", "gists_url": "https://api.github.com/users/Vamix/gists{/gist_id}", "starred_url": "https://api.github.com/users/Vamix/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Vamix/subscriptions", "organizations_url": "https://api.github.com/users/Vamix/orgs", "repos_url": "https://api.github.com/users/Vamix/repos", "events_url": "https://api.github.com/users/Vamix/events{/privacy}", "received_events_url": "https://api.github.com/users/Vamix/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-14T14:18:19Z", "updated_at": "2018-10-14T14:18:19Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a>  Could you please have a look at my problem?<br>\nI have two GPUs and want to try some distributed training(model-parallelism) in TensorFlow.<br>\nMy plan is to divide LeNet into two parts, assign each part to one GPU.<br>\nLeNet has 5 layers, I use <code>with tf.device('/gpu:0'):</code> to assign layer 1 to GPU 0, <code>with tf.device('/gpu:1'):</code>to assign layer2-layer5 to GPU 1.<br>\nThe device mapping log shows that, all the ops have been assigned to the device as I wish:</p>\n<pre><code>layer5/fc3_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\ninit/NoOp_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:1\nlayer1/conv1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n</code></pre>\n<p>I use Timeline to profile this model, but it gives the unexpected result.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/25604857/46917875-4caf4d80-cffe-11e8-8cfa-64da3c23bec0.jpg\"><img src=\"https://user-images.githubusercontent.com/25604857/46917875-4caf4d80-cffe-11e8-8cfa-64da3c23bec0.jpg\" alt=\"inlvu\" style=\"max-width:100%;\"></a></p>\n<p>It seems that ops of layer2-layer5 are launched in GPU1 but run on GPU0. I don't think this is what I want by using with tf.device('/gpu:1'):.</p>\n<p>Is this expected in TensorFlow?</p>\n<p>I've posted this <a href=\"https://stackoverflow.com/questions/52791296/with-tf-device-does-not-work-in-tensorflow\" rel=\"nofollow\">question</a> on StackOverflow</p>", "body_text": "@prb12  Could you please have a look at my problem?\nI have two GPUs and want to try some distributed training(model-parallelism) in TensorFlow.\nMy plan is to divide LeNet into two parts, assign each part to one GPU.\nLeNet has 5 layers, I use with tf.device('/gpu:0'): to assign layer 1 to GPU 0, with tf.device('/gpu:1'):to assign layer2-layer5 to GPU 1.\nThe device mapping log shows that, all the ops have been assigned to the device as I wish:\nlayer5/fc3_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer5/fc3_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer4/fc2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer3/fc1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\nlayer2/conv2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\ninit/NoOp_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:1\nlayer1/conv1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\nlayer1/conv1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n\nI use Timeline to profile this model, but it gives the unexpected result.\n\nIt seems that ops of layer2-layer5 are launched in GPU1 but run on GPU0. I don't think this is what I want by using with tf.device('/gpu:1'):.\nIs this expected in TensorFlow?\nI've posted this question on StackOverflow", "body": "@prb12  Could you please have a look at my problem?\r\nI have two GPUs and want to try some distributed training(model-parallelism) in TensorFlow.\r\nMy plan is to divide LeNet into two parts, assign each part to one GPU.\r\nLeNet has 5 layers, I use `with tf.device('/gpu:0'):` to assign layer 1 to GPU 0, `with tf.device('/gpu:1'):`to assign layer2-layer5 to GPU 1.\r\nThe device mapping log shows that, all the ops have been assigned to the device as I wish:\r\n````\r\nlayer5/fc3_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\ninit/NoOp_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer1/conv1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\r\n````\r\n\r\nI use Timeline to profile this model, but it gives the unexpected result.\r\n\r\n![inlvu](https://user-images.githubusercontent.com/25604857/46917875-4caf4d80-cffe-11e8-8cfa-64da3c23bec0.jpg)\r\n\r\nIt seems that ops of layer2-layer5 are launched in GPU1 but run on GPU0. I don't think this is what I want by using with tf.device('/gpu:1'):.\r\n\r\nIs this expected in TensorFlow?\r\n\r\nI've posted this [question](https://stackoverflow.com/questions/52791296/with-tf-device-does-not-work-in-tensorflow) on StackOverflow"}