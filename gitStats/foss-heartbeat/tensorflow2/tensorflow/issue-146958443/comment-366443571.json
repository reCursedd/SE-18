{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366443571", "html_url": "https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-366443571", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1824", "id": 366443571, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjQ0MzU3MQ==", "user": {"login": "georgh", "id": 1831252, "node_id": "MDQ6VXNlcjE4MzEyNTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1831252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgh", "html_url": "https://github.com/georgh", "followers_url": "https://api.github.com/users/georgh/followers", "following_url": "https://api.github.com/users/georgh/following{/other_user}", "gists_url": "https://api.github.com/users/georgh/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgh/subscriptions", "organizations_url": "https://api.github.com/users/georgh/orgs", "repos_url": "https://api.github.com/users/georgh/repos", "events_url": "https://api.github.com/users/georgh/events{/privacy}", "received_events_url": "https://api.github.com/users/georgh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-17T14:05:26Z", "updated_at": "2018-02-17T14:05:34Z", "author_association": "NONE", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=654434\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rizar\">@rizar</a> The tensor (and memory) views generated by tf.Timeline for Chrome Trace Viewer do not typically represent what's going on very well, hence those options are turned off by default.</p>\n<p>As you pointed out, the NodeExecStats proto doesn't really provide enough information to track tensor dependencies accurately. To do this you really need to know the complete dataflow graph which was executed - and this is not the same as the input GraphDef since the placer and optimizer can make quite substantial changes.</p>\n<p>When tf.Timeline was written, it was possible to get a reasonable approximation of the graph by parsing the timeline_label field of NodeExecStats to reconstruct some of the dataflow dependencies. This has become less and less robust over time.</p>\n<p>However - it is now possible to programmatically retrieve the optimized graphs via the partition_graphs field of the RunMetadata proto, and these ought to provide an accurate idea of tensor lifetime. The tensor lifetime and memory views of tf.Timeline should probably be rewritten to use these GraphDefs if available. (however, I don't have any time to do this!)</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a>  Is this still true? Did anyone start to rewrite tf.Timeline as you suggested? Or ist there another way to debug memory usage?</p>", "body_text": "@rizar The tensor (and memory) views generated by tf.Timeline for Chrome Trace Viewer do not typically represent what's going on very well, hence those options are turned off by default.\nAs you pointed out, the NodeExecStats proto doesn't really provide enough information to track tensor dependencies accurately. To do this you really need to know the complete dataflow graph which was executed - and this is not the same as the input GraphDef since the placer and optimizer can make quite substantial changes.\nWhen tf.Timeline was written, it was possible to get a reasonable approximation of the graph by parsing the timeline_label field of NodeExecStats to reconstruct some of the dataflow dependencies. This has become less and less robust over time.\nHowever - it is now possible to programmatically retrieve the optimized graphs via the partition_graphs field of the RunMetadata proto, and these ought to provide an accurate idea of tensor lifetime. The tensor lifetime and memory views of tf.Timeline should probably be rewritten to use these GraphDefs if available. (however, I don't have any time to do this!)\n\n@prb12  Is this still true? Did anyone start to rewrite tf.Timeline as you suggested? Or ist there another way to debug memory usage?", "body": "> @rizar The tensor (and memory) views generated by tf.Timeline for Chrome Trace Viewer do not typically represent what's going on very well, hence those options are turned off by default.\r\n> \r\n> As you pointed out, the NodeExecStats proto doesn't really provide enough information to track tensor dependencies accurately. To do this you really need to know the complete dataflow graph which was executed - and this is not the same as the input GraphDef since the placer and optimizer can make quite substantial changes.\r\n> \r\n> When tf.Timeline was written, it was possible to get a reasonable approximation of the graph by parsing the timeline_label field of NodeExecStats to reconstruct some of the dataflow dependencies. This has become less and less robust over time.\r\n> \r\n> However - it is now possible to programmatically retrieve the optimized graphs via the partition_graphs field of the RunMetadata proto, and these ought to provide an accurate idea of tensor lifetime. The tensor lifetime and memory views of tf.Timeline should probably be rewritten to use these GraphDefs if available. (however, I don't have any time to do this!)\r\n\r\n@prb12  Is this still true? Did anyone start to rewrite tf.Timeline as you suggested? Or ist there another way to debug memory usage?"}