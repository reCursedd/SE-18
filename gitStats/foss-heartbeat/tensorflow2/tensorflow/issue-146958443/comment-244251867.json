{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244251867", "html_url": "https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-244251867", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1824", "id": 244251867, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDI1MTg2Nw==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-02T00:29:32Z", "updated_at": "2016-09-02T00:30:05Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=456665\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kbrems\">@kbrems</a> This is not really meant to be a discussion forum, ... . but...</p>\n<blockquote>\n<p>I am seeing the bulk of time spent in rows with no label in the left column but grouped under rows like /job:localhost/replica:0/task:0/gpu:0</p>\n</blockquote>\n<p>All of the 'rows' under this heading are ops being <strong>dispatched</strong> on the same Tensorflow gpu device. Because multiple ops can be fired off in parallel on the host, their execution may overlap in time.  A simple bin-packing algorithm is used to assign them to multiple rows so that they don't overlap in the UI.  (Note- this doesn't correspond to 1:1 with host threads)</p>\n<blockquote>\n<p>...and very little time in /gpu:0/stream:all Compute</p>\n</blockquote>\n<p>This usually means that your ops are running very small GPU kernels which take a long time to enqueue compared to the time they take to execute.  Or that the critical path of your computation includes a bunch of ops executing on a different device.</p>\n<blockquote>\n<p>Are the localhost/replica times some kind of GPU setup/scheduling tasks?</p>\n</blockquote>\n<p>Technically, they show the time spend in the TensorFlow <code>OpKernel::Compute</code> method of each op executed on that device.</p>\n<p>For CPU devices the <code>Compute</code> method actually performs the work required by the op, and the timings show the <strong>elapsed</strong> time for the op to complete.  Note that the work may actually be parallelized across many threads/cores in a shared threadpool.  So, if more than one parallel CPU op is running at the same time they contend for processor resources and most likely the elapsed time will appear longer than the actual amount of processor cycles consumed.  (If you care about measuring this you can either run without op-parallelism, or use a standard sampling profiler).</p>\n<p>For GPU, the <code>OpKernel::Compute</code> method usually just enqueues a CUDA kernel launch or DMA transfer on a GPU stream.  This typically only takes a few microseconds (in fact, the overheads of tracing are often in this range!)  For GPU devices, the 'real' kernel timings come from the NVidia GPU profiler and are shown in the <code>/stream:*</code> lines.  When perf debugging it is often useful to see both.</p>\n<blockquote>\n<p>I'm also confused as to why there are so many pids. There are 2 pids each for ...</p>\n</blockquote>\n<p>This is just an unfortunate property of the builtin Chrome Trace Visualizer GUI - it isn't very flexible in terms of labels/hierarchy and which parts of the trace you can expand/collapse.  But it's installed in every browser and saved us having to write a separate visualizer tool.</p>\n<p>We chose to map TensorFlow devices to 'PID's in CTV so that all the activity on a device was grouped nicely (and could be selectively expanded/collapsed).   All of the numeric 'PID's and 'TID's in the UI should be ignored- they were just invented to get CTV to lay out the trace better but there's no way to stop the GUI displaying them!</p>", "body_text": "@kbrems This is not really meant to be a discussion forum, ... . but...\n\nI am seeing the bulk of time spent in rows with no label in the left column but grouped under rows like /job:localhost/replica:0/task:0/gpu:0\n\nAll of the 'rows' under this heading are ops being dispatched on the same Tensorflow gpu device. Because multiple ops can be fired off in parallel on the host, their execution may overlap in time.  A simple bin-packing algorithm is used to assign them to multiple rows so that they don't overlap in the UI.  (Note- this doesn't correspond to 1:1 with host threads)\n\n...and very little time in /gpu:0/stream:all Compute\n\nThis usually means that your ops are running very small GPU kernels which take a long time to enqueue compared to the time they take to execute.  Or that the critical path of your computation includes a bunch of ops executing on a different device.\n\nAre the localhost/replica times some kind of GPU setup/scheduling tasks?\n\nTechnically, they show the time spend in the TensorFlow OpKernel::Compute method of each op executed on that device.\nFor CPU devices the Compute method actually performs the work required by the op, and the timings show the elapsed time for the op to complete.  Note that the work may actually be parallelized across many threads/cores in a shared threadpool.  So, if more than one parallel CPU op is running at the same time they contend for processor resources and most likely the elapsed time will appear longer than the actual amount of processor cycles consumed.  (If you care about measuring this you can either run without op-parallelism, or use a standard sampling profiler).\nFor GPU, the OpKernel::Compute method usually just enqueues a CUDA kernel launch or DMA transfer on a GPU stream.  This typically only takes a few microseconds (in fact, the overheads of tracing are often in this range!)  For GPU devices, the 'real' kernel timings come from the NVidia GPU profiler and are shown in the /stream:* lines.  When perf debugging it is often useful to see both.\n\nI'm also confused as to why there are so many pids. There are 2 pids each for ...\n\nThis is just an unfortunate property of the builtin Chrome Trace Visualizer GUI - it isn't very flexible in terms of labels/hierarchy and which parts of the trace you can expand/collapse.  But it's installed in every browser and saved us having to write a separate visualizer tool.\nWe chose to map TensorFlow devices to 'PID's in CTV so that all the activity on a device was grouped nicely (and could be selectively expanded/collapsed).   All of the numeric 'PID's and 'TID's in the UI should be ignored- they were just invented to get CTV to lay out the trace better but there's no way to stop the GUI displaying them!", "body": "@kbrems This is not really meant to be a discussion forum, ... . but...\n\n>  I am seeing the bulk of time spent in rows with no label in the left column but grouped under rows like /job:localhost/replica:0/task:0/gpu:0\n\nAll of the 'rows' under this heading are ops being **dispatched** on the same Tensorflow gpu device. Because multiple ops can be fired off in parallel on the host, their execution may overlap in time.  A simple bin-packing algorithm is used to assign them to multiple rows so that they don't overlap in the UI.  (Note- this doesn't correspond to 1:1 with host threads)\n\n> ...and very little time in /gpu:0/stream:all Compute\n\nThis usually means that your ops are running very small GPU kernels which take a long time to enqueue compared to the time they take to execute.  Or that the critical path of your computation includes a bunch of ops executing on a different device.\n\n> Are the localhost/replica times some kind of GPU setup/scheduling tasks?\n\nTechnically, they show the time spend in the TensorFlow `OpKernel::Compute` method of each op executed on that device.  \n\nFor CPU devices the `Compute` method actually performs the work required by the op, and the timings show the **elapsed** time for the op to complete.  Note that the work may actually be parallelized across many threads/cores in a shared threadpool.  So, if more than one parallel CPU op is running at the same time they contend for processor resources and most likely the elapsed time will appear longer than the actual amount of processor cycles consumed.  (If you care about measuring this you can either run without op-parallelism, or use a standard sampling profiler). \n\nFor GPU, the `OpKernel::Compute` method usually just enqueues a CUDA kernel launch or DMA transfer on a GPU stream.  This typically only takes a few microseconds (in fact, the overheads of tracing are often in this range!)  For GPU devices, the 'real' kernel timings come from the NVidia GPU profiler and are shown in the `/stream:*` lines.  When perf debugging it is often useful to see both.\n\n> I'm also confused as to why there are so many pids. There are 2 pids each for ...\n\nThis is just an unfortunate property of the builtin Chrome Trace Visualizer GUI - it isn't very flexible in terms of labels/hierarchy and which parts of the trace you can expand/collapse.  But it's installed in every browser and saved us having to write a separate visualizer tool.\n\nWe chose to map TensorFlow devices to 'PID's in CTV so that all the activity on a device was grouped nicely (and could be selectively expanded/collapsed).   All of the numeric 'PID's and 'TID's in the UI should be ignored- they were just invented to get CTV to lay out the trace better but there's no way to stop the GUI displaying them!\n"}