{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8841", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8841/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8841/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8841/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8841", "id": 218229109, "node_id": "MDU6SXNzdWUyMTgyMjkxMDk=", "number": 8841, "title": "Gradient of reduce_prod not available on GPU", "user": {"login": "Namnamseo", "id": 12743061, "node_id": "MDQ6VXNlcjEyNzQzMDYx", "avatar_url": "https://avatars1.githubusercontent.com/u/12743061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Namnamseo", "html_url": "https://github.com/Namnamseo", "followers_url": "https://api.github.com/users/Namnamseo/followers", "following_url": "https://api.github.com/users/Namnamseo/following{/other_user}", "gists_url": "https://api.github.com/users/Namnamseo/gists{/gist_id}", "starred_url": "https://api.github.com/users/Namnamseo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Namnamseo/subscriptions", "organizations_url": "https://api.github.com/users/Namnamseo/orgs", "repos_url": "https://api.github.com/users/Namnamseo/repos", "events_url": "https://api.github.com/users/Namnamseo/events{/privacy}", "received_events_url": "https://api.github.com/users/Namnamseo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-30T15:06:38Z", "updated_at": "2018-11-10T03:40:23Z", "closed_at": "2018-11-10T03:40:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The following example fails to colocate the values:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:2<span class=\"pl-pds\">'</span></span>):\n    x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">100</span>])\n    weight_dense_1 <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>]))\n    dense_1_out <span class=\"pl-k\">=</span> tf.matmul(x, weight_dense_1)\n    y <span class=\"pl-k\">=</span> tf.reduce_prod(tf.cast(tf.shape(dense_1_out), tf.float32))\n    grad <span class=\"pl-k\">=</span> tf.gradients(y, [weight_dense_1], <span class=\"pl-v\">colocate_gradients_with_ops</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>A bunch of warnings like this is displayed:</p>\n<pre><code>WARNING:tensorflow:Tried to colocate gradients_1/Prod_1_grad/Rank with an op Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\n</code></pre>\n<p>The symptom is similar to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"166443628\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3397\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3397/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3397\">#3397</a>. Using CPU or specifying all input dimensions solves the problem. But the cause seems different.</p>\n<p>Gradient of <code>Prod</code> operation is defined in <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/math_grad.py#L102-L143\">python/ops/math_grad.py</a>. There, the operation is forced to run on CPU (see <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/182fef1b55640906637e4bf0d205e508c24549e7/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/182fef1b55640906637e4bf0d205e508c24549e7\"><tt>182fef1</tt></a>), mentioning the <code>listdiff()</code> operation is CPU-only.<br>\nI tried remove the forcing line and run this. It yields a kind explanation:</p>\n<pre><code>InvalidArgumentError: Cannot assign a device to node 'gradients/Prod_grad/range_1': Could not satisfy explicit device specification '/device:GPU:2' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nInvertPermutation: GPU CPU \nTranspose: GPU CPU \nConcatV2: GPU CPU \nPack: GPU CPU \nCumprod: GPU CPU \nListDiff: CPU \nShape: GPU CPU \n    ...(many GPU CPU ops)\nReshape: GPU CPU \nGather: CPU \n</code></pre>\n<p>Two operations used here, namely <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/gather_op.cc\">Gather</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/listdiff_op.cc\">ListDiff</a>, are defined on CPU-only. As some of the operations needed in calculating the gradient are CPU-only, by the colocation rule, they get grouped into CPU-only.</p>\n<p>This also occurs when using <code>moments()</code> or <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/nn_impl.py#L495-L541\"><code>sufficient_statistics()</code></a> (the former calls the latter). There, when some of the axes of the tensor are unknown (like batch size), the total number of values (which is needed for the mean and variance) is calculated by <code>reduce_prod()</code> on <code>shape()</code>.<br>\nWhen the value of mean or variance is differentiated in some way (which is the case in batch normalization), a colocation between tensors named like <code>gradients/moments/sufficient_statistics/count_grad/Rank</code> and <code>moments/sufficient_statistics/count</code> fails.</p>\n<p>Though <code>listdiff()</code> is renamed later on Python interface to <code>setdiff1d()</code>, it's still named <code>ListDiff</code> internally.<br>\n<code>gather()</code> operation is defined on GPU too, but only on float types.</p>\n<p>It seems there hasn't been any issue on this. Would it mean that <code>Prod()</code> op is not differentiated in most of the cases?<br>\nHow this can be solved? I'm not sure if the <code>setdiff1d()</code> operation is needed.<br>\nFor me, this occured when using <code>moments()</code>, where the reciprocal of number of values is multiplicated to the sum of values. I think this is unnecessary, as it can be done with <code>reduce_mean()</code>. Is it right?</p>\n<h3>Environment info</h3>\n<p>Operating System: <strong>Ubuntu 16.04</strong>.<br>\nInstalled version of CUDA and cuDNN: <strong>CUDA 8.0.61</strong> / <strong>cuDNN 5.1.10</strong>.<br>\npip3-installed <code>tensorflow-gpu==1.0.1</code>; all links here pointed to <code>r1.0</code>, but the problematic parts are the same as <code>master</code>.<br>\n<code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code> yields: <code>1.0.1</code>.</p>", "body_text": "The following example fails to colocate the values:\nwith tf.device('/gpu:2'):\n    x = tf.placeholder(tf.float32, shape=[None, 100])\n    weight_dense_1 = tf.Variable(tf.zeros([100, 10]))\n    dense_1_out = tf.matmul(x, weight_dense_1)\n    y = tf.reduce_prod(tf.cast(tf.shape(dense_1_out), tf.float32))\n    grad = tf.gradients(y, [weight_dense_1], colocate_gradients_with_ops=True)\nA bunch of warnings like this is displayed:\nWARNING:tensorflow:Tried to colocate gradients_1/Prod_1_grad/Rank with an op Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\n\nThe symptom is similar to #3397. Using CPU or specifying all input dimensions solves the problem. But the cause seems different.\nGradient of Prod operation is defined in python/ops/math_grad.py. There, the operation is forced to run on CPU (see 182fef1), mentioning the listdiff() operation is CPU-only.\nI tried remove the forcing line and run this. It yields a kind explanation:\nInvalidArgumentError: Cannot assign a device to node 'gradients/Prod_grad/range_1': Could not satisfy explicit device specification '/device:GPU:2' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nInvertPermutation: GPU CPU \nTranspose: GPU CPU \nConcatV2: GPU CPU \nPack: GPU CPU \nCumprod: GPU CPU \nListDiff: CPU \nShape: GPU CPU \n    ...(many GPU CPU ops)\nReshape: GPU CPU \nGather: CPU \n\nTwo operations used here, namely Gather and ListDiff, are defined on CPU-only. As some of the operations needed in calculating the gradient are CPU-only, by the colocation rule, they get grouped into CPU-only.\nThis also occurs when using moments() or sufficient_statistics() (the former calls the latter). There, when some of the axes of the tensor are unknown (like batch size), the total number of values (which is needed for the mean and variance) is calculated by reduce_prod() on shape().\nWhen the value of mean or variance is differentiated in some way (which is the case in batch normalization), a colocation between tensors named like gradients/moments/sufficient_statistics/count_grad/Rank and moments/sufficient_statistics/count fails.\nThough listdiff() is renamed later on Python interface to setdiff1d(), it's still named ListDiff internally.\ngather() operation is defined on GPU too, but only on float types.\nIt seems there hasn't been any issue on this. Would it mean that Prod() op is not differentiated in most of the cases?\nHow this can be solved? I'm not sure if the setdiff1d() operation is needed.\nFor me, this occured when using moments(), where the reciprocal of number of values is multiplicated to the sum of values. I think this is unnecessary, as it can be done with reduce_mean(). Is it right?\nEnvironment info\nOperating System: Ubuntu 16.04.\nInstalled version of CUDA and cuDNN: CUDA 8.0.61 / cuDNN 5.1.10.\npip3-installed tensorflow-gpu==1.0.1; all links here pointed to r1.0, but the problematic parts are the same as master.\npython -c \"import tensorflow; print(tensorflow.__version__)\" yields: 1.0.1.", "body": "The following example fails to colocate the values:\r\n```python\r\nwith tf.device('/gpu:2'):\r\n    x = tf.placeholder(tf.float32, shape=[None, 100])\r\n    weight_dense_1 = tf.Variable(tf.zeros([100, 10]))\r\n    dense_1_out = tf.matmul(x, weight_dense_1)\r\n    y = tf.reduce_prod(tf.cast(tf.shape(dense_1_out), tf.float32))\r\n    grad = tf.gradients(y, [weight_dense_1], colocate_gradients_with_ops=True)\r\n```\r\nA bunch of warnings like this is displayed:\r\n```\r\nWARNING:tensorflow:Tried to colocate gradients_1/Prod_1_grad/Rank with an op Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\n```\r\nThe symptom is similar to #3397. Using CPU or specifying all input dimensions solves the problem. But the cause seems different.\r\n\r\nGradient of `Prod` operation is defined in [python/ops/math_grad.py](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/math_grad.py#L102-L143). There, the operation is forced to run on CPU (see 182fef1b55640906637e4bf0d205e508c24549e7), mentioning the `listdiff()` operation is CPU-only.\r\nI tried remove the forcing line and run this. It yields a kind explanation:\r\n```\r\nInvalidArgumentError: Cannot assign a device to node 'gradients/Prod_grad/range_1': Could not satisfy explicit device specification '/device:GPU:2' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nInvertPermutation: GPU CPU \r\nTranspose: GPU CPU \r\nConcatV2: GPU CPU \r\nPack: GPU CPU \r\nCumprod: GPU CPU \r\nListDiff: CPU \r\nShape: GPU CPU \r\n    ...(many GPU CPU ops)\r\nReshape: GPU CPU \r\nGather: CPU \r\n```\r\n\r\nTwo operations used here, namely [Gather](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/gather_op.cc) and [ListDiff](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/listdiff_op.cc), are defined on CPU-only. As some of the operations needed in calculating the gradient are CPU-only, by the colocation rule, they get grouped into CPU-only.\r\n\r\nThis also occurs when using `moments()` or [`sufficient_statistics()`](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/nn_impl.py#L495-L541) (the former calls the latter). There, when some of the axes of the tensor are unknown (like batch size), the total number of values (which is needed for the mean and variance) is calculated by `reduce_prod()` on `shape()`.\r\nWhen the value of mean or variance is differentiated in some way (which is the case in batch normalization), a colocation between tensors named like `gradients/moments/sufficient_statistics/count_grad/Rank` and `moments/sufficient_statistics/count` fails.\r\n\r\nThough `listdiff()` is renamed later on Python interface to `setdiff1d()`, it's still named `ListDiff` internally.\r\n`gather()` operation is defined on GPU too, but only on float types.\r\n\r\nIt seems there hasn't been any issue on this. Would it mean that `Prod()` op is not differentiated in most of the cases?\r\nHow this can be solved? I'm not sure if the `setdiff1d()` operation is needed.\r\nFor me, this occured when using `moments()`, where the reciprocal of number of values is multiplicated to the sum of values. I think this is unnecessary, as it can be done with `reduce_mean()`. Is it right?\r\n\r\n### Environment info\r\nOperating System: **Ubuntu 16.04**.\r\nInstalled version of CUDA and cuDNN: **CUDA 8.0.61** / **cuDNN 5.1.10**.\r\npip3-installed `tensorflow-gpu==1.0.1`; all links here pointed to `r1.0`, but the problematic parts are the same as `master`.\r\n`python -c \"import tensorflow; print(tensorflow.__version__)\"` yields: `1.0.1`."}