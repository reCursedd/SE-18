{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/431547680", "html_url": "https://github.com/tensorflow/tensorflow/issues/23086#issuecomment-431547680", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23086", "id": 431547680, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTU0NzY4MA==", "user": {"login": "Santosh-Gupta", "id": 5524261, "node_id": "MDQ6VXNlcjU1MjQyNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5524261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Santosh-Gupta", "html_url": "https://github.com/Santosh-Gupta", "followers_url": "https://api.github.com/users/Santosh-Gupta/followers", "following_url": "https://api.github.com/users/Santosh-Gupta/following{/other_user}", "gists_url": "https://api.github.com/users/Santosh-Gupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/Santosh-Gupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Santosh-Gupta/subscriptions", "organizations_url": "https://api.github.com/users/Santosh-Gupta/orgs", "repos_url": "https://api.github.com/users/Santosh-Gupta/repos", "events_url": "https://api.github.com/users/Santosh-Gupta/events{/privacy}", "received_events_url": "https://api.github.com/users/Santosh-Gupta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-20T04:29:07Z", "updated_at": "2018-10-20T04:39:11Z", "author_association": "NONE", "body_html": "<p>Here it is. I'll also edit this into the original post.</p>\n<blockquote>\n<p>Have I written custom code</p>\n</blockquote>\n<p>Here is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook.</p>\n<p><a href=\"https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\" rel=\"nofollow\">https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl</a></p>\n<p>I wrote a custom function to replace <code>true_expected_count</code> and <code>sampled_expected_count</code> generated from <code>uniform_candidate_sampler</code> because I wanted to isolate the issue if the slow down is due to using float16 instead of float32. The specific code to replace those variables is</p>\n<pre><code>    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \n    sampled_values = tf.nn.uniform_candidate_sampler(\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\n          num_true=1,\n          unique=True,\n          range_max=vocabulary_size,\n          seed=None)\n    \n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue2 = tf.cast(ray, testDataType)\n\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue = tf.cast(jay, testDataType)\n\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\n\n    sampled_value_16 = LogUniformCandidateSampler(\n        sampled_values.sampled_candidates,\n        true_expected_count2,\n        sampled_expected_count2 )\n</code></pre>\n<p>You can switch between float16 and float32 by changing the <code>testDataType</code> variable at the top. You can also change the embedding size at the top by changing the <code>embedding_size</code> variable, which I recommend for switching between TPU mode and GPU mode in Google Colab.</p>\n<p>After you select which datatype and embedding size you want to use, select 'Runtime' -&gt; 'Run all', and you can check the time it takes between steps at the training code at the bottom.</p>\n<p>In GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).</p>\n<p>In TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).</p>\n<blockquote>\n<p>OS Platform and Distribution</p>\n</blockquote>\n<p>Python 3. Not sure about the rest, whatever Google Colab uses.</p>\n<blockquote>\n<p>TensorFlow installed from</p>\n</blockquote>\n<p>Not sure, whatever Google Colab uses.</p>\n<blockquote>\n<p>CUDA/cuDNN version</p>\n</blockquote>\n<p>Not sure, whatever Google Colab uses.</p>\n<blockquote>\n<p>GPU model and memory</p>\n</blockquote>\n<p>I believe Colab for GPU uses a Tesla k80, and for TPU they are using V2.</p>\n<blockquote>\n<p>Exact command to reproduce</p>\n</blockquote>\n<p>Same code as linked above</p>\n<p><a href=\"https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\" rel=\"nofollow\">https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl</a></p>\n<blockquote>\n<p>Mobile device</p>\n</blockquote>\n<p>N/A</p>", "body_text": "Here it is. I'll also edit this into the original post.\n\nHave I written custom code\n\nHere is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook.\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\nI wrote a custom function to replace true_expected_count and sampled_expected_count generated from uniform_candidate_sampler because I wanted to isolate the issue if the slow down is due to using float16 instead of float32. The specific code to replace those variables is\n    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \n    sampled_values = tf.nn.uniform_candidate_sampler(\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\n          num_true=1,\n          unique=True,\n          range_max=vocabulary_size,\n          seed=None)\n    \n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue2 = tf.cast(ray, testDataType)\n\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue = tf.cast(jay, testDataType)\n\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\n\n    sampled_value_16 = LogUniformCandidateSampler(\n        sampled_values.sampled_candidates,\n        true_expected_count2,\n        sampled_expected_count2 )\n\nYou can switch between float16 and float32 by changing the testDataType variable at the top. You can also change the embedding size at the top by changing the embedding_size variable, which I recommend for switching between TPU mode and GPU mode in Google Colab.\nAfter you select which datatype and embedding size you want to use, select 'Runtime' -> 'Run all', and you can check the time it takes between steps at the training code at the bottom.\nIn GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).\nIn TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).\n\nOS Platform and Distribution\n\nPython 3. Not sure about the rest, whatever Google Colab uses.\n\nTensorFlow installed from\n\nNot sure, whatever Google Colab uses.\n\nCUDA/cuDNN version\n\nNot sure, whatever Google Colab uses.\n\nGPU model and memory\n\nI believe Colab for GPU uses a Tesla k80, and for TPU they are using V2.\n\nExact command to reproduce\n\nSame code as linked above\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\n\nMobile device\n\nN/A", "body": "Here it is. I'll also edit this into the original post. \r\n\r\n>Have I written custom code\r\n\r\nHere is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook. \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\nI wrote a custom function to replace `true_expected_count` and `sampled_expected_count` generated from `uniform_candidate_sampler` because I wanted to isolate the issue if the slow down is due to using float16 instead of float32. The specific code to replace those variables is\r\n\r\n```\r\n    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \r\n    sampled_values = tf.nn.uniform_candidate_sampler(\r\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\r\n          num_true=1,\r\n          unique=True,\r\n          range_max=vocabulary_size,\r\n          seed=None)\r\n    \r\n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue2 = tf.cast(ray, testDataType)\r\n\r\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue = tf.cast(jay, testDataType)\r\n\r\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\r\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\r\n\r\n    sampled_value_16 = LogUniformCandidateSampler(\r\n        sampled_values.sampled_candidates,\r\n        true_expected_count2,\r\n        sampled_expected_count2 )\r\n```\r\n\r\nYou can switch between float16 and float32 by changing the `testDataType` variable at the top. You can also change the embedding size at the top by changing the `embedding_size` variable, which I recommend for switching between TPU mode and GPU mode in Google Colab. \r\n\r\nAfter you select which datatype and embedding size you want to use, select 'Runtime' -> 'Run all', and you can check the time it takes between steps at the training code at the bottom. \r\n\r\nIn GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\nIn TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\n>OS Platform and Distribution\r\n\r\nPython 3. Not sure about the rest, whatever Google Colab uses. \r\n\r\n>TensorFlow installed from\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>CUDA/cuDNN version\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>GPU model and memory\r\n\r\nI believe Colab for GPU uses a Tesla k80, and for TPU they are using V2. \r\n\r\n>Exact command to reproduce\r\n\r\nSame code as linked above \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\n>Mobile device\r\n\r\nN/A"}