{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23086", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23086/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23086/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23086/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23086", "id": 371737044, "node_id": "MDU6SXNzdWUzNzE3MzcwNDQ=", "number": 23086, "title": "Float16 support for log_uniform_candidate_sampler/uniform_candidate_sampler", "user": {"login": "Santosh-Gupta", "id": 5524261, "node_id": "MDQ6VXNlcjU1MjQyNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5524261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Santosh-Gupta", "html_url": "https://github.com/Santosh-Gupta", "followers_url": "https://api.github.com/users/Santosh-Gupta/followers", "following_url": "https://api.github.com/users/Santosh-Gupta/following{/other_user}", "gists_url": "https://api.github.com/users/Santosh-Gupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/Santosh-Gupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Santosh-Gupta/subscriptions", "organizations_url": "https://api.github.com/users/Santosh-Gupta/orgs", "repos_url": "https://api.github.com/users/Santosh-Gupta/repos", "events_url": "https://api.github.com/users/Santosh-Gupta/events{/privacy}", "received_events_url": "https://api.github.com/users/Santosh-Gupta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097545817, "node_id": "MDU6TGFiZWwxMDk3NTQ1ODE3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:apis", "name": "comp:apis", "color": "0052cc", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-10-18T22:23:43Z", "updated_at": "2018-11-20T16:57:18Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a feature request. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>TensorFlow version (you are using): 1.11.0</li>\n<li>Are you willing to contribute it (Yes/No): Yes</li>\n</ul>\n<p><strong>Describe the feature and the current behavior/state.</strong></p>\n<p>Float16 architectures are promising for significantly increase the model size / training speed using the same amount of memory. So I am requesting Float16 support for <code>log_uniform_candidate_sampler</code> and <code>uniform_candidate_sampler</code>. Currently, in the returned tuple,  <code>true_expected_count</code> and <code>sampled_expected_count</code> as returned as float32. This causes issues when using <code>sampled_softmax_loss</code> since everything needs to be in the same datatype.</p>\n<p>I was able to find a hack work around</p>\n<p><a href=\"https://stackoverflow.com/questions/52711895/how-to-run-define-tensorflow-graph-were-all-variables-are-in-float16-instead-ins\" rel=\"nofollow\">https://stackoverflow.com/questions/52711895/how-to-run-define-tensorflow-graph-were-all-variables-are-in-float16-instead-ins</a></p>\n<p>However, this actually increases the training speed; I am not sure why (more details below)</p>\n<p>I have tried to develop a replacement function, however, I was unable to interpret the machine generated code where <code>log_uniform_candidate_sampler</code> and <code>uniform_candidate_sampler</code> are defined. I describe my attempt to interpret the code here</p>\n<p><a href=\"https://stackoverflow.com/questions/52734709/how-to-interpret-this-machine-generated-python-code\" rel=\"nofollow\">https://stackoverflow.com/questions/52734709/how-to-interpret-this-machine-generated-python-code</a></p>\n<p><strong>Will this change the current api? How?</strong></p>\n<p>Yes, most likely the best way is to have an optional arg for what datatype the user would like use, ie datatype=tf.float16. Otherwise defaults to float32</p>\n<p><strong>Who will benefit with this feature?</strong></p>\n<p>Those who are developing embedding items (ie Word2Vec) using float16 embeddings.</p>\n<p><strong>Any Other info.</strong></p>\n<blockquote>\n<p>Have I written custom code</p>\n</blockquote>\n<p>Here is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook.</p>\n<p><a href=\"https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\" rel=\"nofollow\">https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl</a></p>\n<p>I wrote a custom function to replace <code>true_expected_count</code> and <code>sampled_expected_count</code> generated from <code>uniform_candidate_sampler</code> because I wanted to isolate the issue if the slow-down is due to using float16 instead of float32. The specific code to replace those variables is</p>\n<pre><code>    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \n    sampled_values = tf.nn.uniform_candidate_sampler(\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\n          num_true=1,\n          unique=True,\n          range_max=vocabulary_size,\n          seed=None)\n    \n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue2 = tf.cast(ray, testDataType)\n\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue = tf.cast(jay, testDataType)\n\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\n\n    sampled_value_16 = LogUniformCandidateSampler(\n        sampled_values.sampled_candidates,\n        true_expected_count2,\n        sampled_expected_count2 )\n</code></pre>\n<p>You can switch between float16 and float32 by changing the <code>testDataType</code> variable at the top. You can also change the embedding size at the top by changing the <code>embedding_size</code> variable, which I recommend for switching between TPU mode and GPU mode in Google Colab.</p>\n<p>After you select which datatype and embedding size you want to use, select 'Runtime' -&gt; 'Run all', and you can check the time it takes between steps at the training code at the bottom.</p>\n<p>In GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).</p>\n<p>In TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).</p>\n<blockquote>\n<p>OS Platform and Distribution</p>\n</blockquote>\n<p>Python 3. Not sure about the rest, whatever Google Colab uses.</p>\n<blockquote>\n<p>TensorFlow installed from</p>\n</blockquote>\n<p>Not sure, whatever Google Colab uses.</p>\n<blockquote>\n<p>CUDA/cuDNN version</p>\n</blockquote>\n<p>Not sure, whatever Google Colab uses.</p>\n<blockquote>\n<p>GPU model and memory</p>\n</blockquote>\n<p>I believe Colab for GPU uses a Tesla k80, and for TPU they are using V2.</p>\n<blockquote>\n<p>Exact command to reproduce</p>\n</blockquote>\n<p>Same code as linked above</p>\n<p><a href=\"https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\" rel=\"nofollow\">https://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl</a></p>\n<blockquote>\n<p>Mobile device</p>\n</blockquote>\n<p>N/A</p>", "body_text": "Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template\nSystem information\n\nTensorFlow version (you are using): 1.11.0\nAre you willing to contribute it (Yes/No): Yes\n\nDescribe the feature and the current behavior/state.\nFloat16 architectures are promising for significantly increase the model size / training speed using the same amount of memory. So I am requesting Float16 support for log_uniform_candidate_sampler and uniform_candidate_sampler. Currently, in the returned tuple,  true_expected_count and sampled_expected_count as returned as float32. This causes issues when using sampled_softmax_loss since everything needs to be in the same datatype.\nI was able to find a hack work around\nhttps://stackoverflow.com/questions/52711895/how-to-run-define-tensorflow-graph-were-all-variables-are-in-float16-instead-ins\nHowever, this actually increases the training speed; I am not sure why (more details below)\nI have tried to develop a replacement function, however, I was unable to interpret the machine generated code where log_uniform_candidate_sampler and uniform_candidate_sampler are defined. I describe my attempt to interpret the code here\nhttps://stackoverflow.com/questions/52734709/how-to-interpret-this-machine-generated-python-code\nWill this change the current api? How?\nYes, most likely the best way is to have an optional arg for what datatype the user would like use, ie datatype=tf.float16. Otherwise defaults to float32\nWho will benefit with this feature?\nThose who are developing embedding items (ie Word2Vec) using float16 embeddings.\nAny Other info.\n\nHave I written custom code\n\nHere is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook.\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\nI wrote a custom function to replace true_expected_count and sampled_expected_count generated from uniform_candidate_sampler because I wanted to isolate the issue if the slow-down is due to using float16 instead of float32. The specific code to replace those variables is\n    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \n    sampled_values = tf.nn.uniform_candidate_sampler(\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\n          num_true=1,\n          unique=True,\n          range_max=vocabulary_size,\n          seed=None)\n    \n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue2 = tf.cast(ray, testDataType)\n\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \n    fillvalue = tf.cast(jay, testDataType)\n\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\n\n    sampled_value_16 = LogUniformCandidateSampler(\n        sampled_values.sampled_candidates,\n        true_expected_count2,\n        sampled_expected_count2 )\n\nYou can switch between float16 and float32 by changing the testDataType variable at the top. You can also change the embedding size at the top by changing the embedding_size variable, which I recommend for switching between TPU mode and GPU mode in Google Colab.\nAfter you select which datatype and embedding size you want to use, select 'Runtime' -> 'Run all', and you can check the time it takes between steps at the training code at the bottom.\nIn GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).\nIn TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).\n\nOS Platform and Distribution\n\nPython 3. Not sure about the rest, whatever Google Colab uses.\n\nTensorFlow installed from\n\nNot sure, whatever Google Colab uses.\n\nCUDA/cuDNN version\n\nNot sure, whatever Google Colab uses.\n\nGPU model and memory\n\nI believe Colab for GPU uses a Tesla k80, and for TPU they are using V2.\n\nExact command to reproduce\n\nSame code as linked above\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\n\nMobile device\n\nN/A", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.11.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFloat16 architectures are promising for significantly increase the model size / training speed using the same amount of memory. So I am requesting Float16 support for `log_uniform_candidate_sampler` and `uniform_candidate_sampler`. Currently, in the returned tuple,  `true_expected_count` and `sampled_expected_count` as returned as float32. This causes issues when using `sampled_softmax_loss` since everything needs to be in the same datatype. \r\n\r\nI was able to find a hack work around\r\n\r\nhttps://stackoverflow.com/questions/52711895/how-to-run-define-tensorflow-graph-were-all-variables-are-in-float16-instead-ins\r\n\r\nHowever, this actually increases the training speed; I am not sure why (more details below)\r\n\r\nI have tried to develop a replacement function, however, I was unable to interpret the machine generated code where `log_uniform_candidate_sampler` and `uniform_candidate_sampler` are defined. I describe my attempt to interpret the code here\r\n\r\nhttps://stackoverflow.com/questions/52734709/how-to-interpret-this-machine-generated-python-code\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, most likely the best way is to have an optional arg for what datatype the user would like use, ie datatype=tf.float16. Otherwise defaults to float32\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThose who are developing embedding items (ie Word2Vec) using float16 embeddings. \r\n\r\n**Any Other info.**\r\n\r\n>Have I written custom code\r\n\r\nHere is the code I used, for convenience it's in a Google Colab notebook so you can just run the code in the notebook. \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\nI wrote a custom function to replace `true_expected_count` and `sampled_expected_count` generated from `uniform_candidate_sampler` because I wanted to isolate the issue if the slow-down is due to using float16 instead of float32. The specific code to replace those variables is  \r\n\r\n```\r\n    LogUniformCandidateSampler = namedtuple(\"namedtuple\", [\"sampled_candidates\", \"true_expected_count\", \"sampled_expected_count\"]) \r\n    sampled_values = tf.nn.uniform_candidate_sampler(\r\n          true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,\r\n          num_true=1,\r\n          unique=True,\r\n          range_max=vocabulary_size,\r\n          seed=None)\r\n    \r\n    ray= tf.fill( dims =[num_sampled] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue2 = tf.cast(ray, testDataType)\r\n\r\n    jay= tf.fill( dims =[ batch_size  ,1] ,  value= (num_sampled/vocabulary_size)   ) \r\n    fillvalue = tf.cast(jay, testDataType)\r\n\r\n    true_expected_count2 = tf.get_variable( 'this1' ,dtype=testDataType, initializer = fillvalue  )\r\n    sampled_expected_count2 = tf.get_variable( 'this2' , dtype=testDataType, initializer = fillvalue2   )\r\n\r\n    sampled_value_16 = LogUniformCandidateSampler(\r\n        sampled_values.sampled_candidates,\r\n        true_expected_count2,\r\n        sampled_expected_count2 )\r\n```\r\n\r\nYou can switch between float16 and float32 by changing the `testDataType` variable at the top. You can also change the embedding size at the top by changing the `embedding_size` variable, which I recommend for switching between TPU mode and GPU mode in Google Colab. \r\n\r\nAfter you select which datatype and embedding size you want to use, select 'Runtime' -> 'Run all', and you can check the time it takes between steps at the training code at the bottom. \r\n\r\nIn GPU mode, 100 steps takes about 11 seconds for float32, but takes 15 seconds for float16. I imagine float16 would have been faster. For GPU mode I used an embedding size of 52 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\nIn TPU mode, 100 steps takes about 10 seconds for float32, but takes 6 minutes 5 seconds for float16. So 36x slower for float16 than float32. For TPU mode I used an embedding size of 154 (highest embedding size I could use before the system would crash during training for float32).\r\n\r\n>OS Platform and Distribution\r\n\r\nPython 3. Not sure about the rest, whatever Google Colab uses. \r\n\r\n>TensorFlow installed from\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>CUDA/cuDNN version\r\n\r\nNot sure, whatever Google Colab uses. \r\n\r\n>GPU model and memory\r\n\r\nI believe Colab for GPU uses a Tesla k80, and for TPU they are using V2. \r\n\r\n>Exact command to reproduce\r\n\r\nSame code as linked above \r\n\r\nhttps://drive.google.com/open?id=1bnlSfezPqtwapvURRTAqGR4mkcCl9QZl\r\n\r\n>Mobile device\r\n\r\nN/A"}