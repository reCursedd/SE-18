{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21991", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21991/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21991/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21991/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21991", "id": 355948265, "node_id": "MDU6SXNzdWUzNTU5NDgyNjU=", "number": 21991, "title": "Strange behavior of loss function in WALSMatrixFactorization TF 1.8", "user": {"login": "NatLun091238", "id": 26412310, "node_id": "MDQ6VXNlcjI2NDEyMzEw", "avatar_url": "https://avatars0.githubusercontent.com/u/26412310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NatLun091238", "html_url": "https://github.com/NatLun091238", "followers_url": "https://api.github.com/users/NatLun091238/followers", "following_url": "https://api.github.com/users/NatLun091238/following{/other_user}", "gists_url": "https://api.github.com/users/NatLun091238/gists{/gist_id}", "starred_url": "https://api.github.com/users/NatLun091238/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NatLun091238/subscriptions", "organizations_url": "https://api.github.com/users/NatLun091238/orgs", "repos_url": "https://api.github.com/users/NatLun091238/repos", "events_url": "https://api.github.com/users/NatLun091238/events{/privacy}", "received_events_url": "https://api.github.com/users/NatLun091238/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "walidk", "id": 734669, "node_id": "MDQ6VXNlcjczNDY2OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/734669?v=4", "gravatar_id": "", "url": "https://api.github.com/users/walidk", "html_url": "https://github.com/walidk", "followers_url": "https://api.github.com/users/walidk/followers", "following_url": "https://api.github.com/users/walidk/following{/other_user}", "gists_url": "https://api.github.com/users/walidk/gists{/gist_id}", "starred_url": "https://api.github.com/users/walidk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/walidk/subscriptions", "organizations_url": "https://api.github.com/users/walidk/orgs", "repos_url": "https://api.github.com/users/walidk/repos", "events_url": "https://api.github.com/users/walidk/events{/privacy}", "received_events_url": "https://api.github.com/users/walidk/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "walidk", "id": 734669, "node_id": "MDQ6VXNlcjczNDY2OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/734669?v=4", "gravatar_id": "", "url": "https://api.github.com/users/walidk", "html_url": "https://github.com/walidk", "followers_url": "https://api.github.com/users/walidk/followers", "following_url": "https://api.github.com/users/walidk/following{/other_user}", "gists_url": "https://api.github.com/users/walidk/gists{/gist_id}", "starred_url": "https://api.github.com/users/walidk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/walidk/subscriptions", "organizations_url": "https://api.github.com/users/walidk/orgs", "repos_url": "https://api.github.com/users/walidk/repos", "events_url": "https://api.github.com/users/walidk/events{/privacy}", "received_events_url": "https://api.github.com/users/walidk/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-08-31T11:44:33Z", "updated_at": "2018-11-23T18:39:16Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Our current implementation of recommendation system use TF 1.8 and WALS algorithm. The model was trained using self.fit(input_fn=input_fn) and ML Engine with run time version 1.8. Data set was formed following <a href=\"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/10_recommend/wals.ipynb\">example</a> using tensorflow.train.Example(...) Extraction from training logs shown below.</p>\n<p><a href=\"https://drive.google.com/open?id=1HS4dPXC31Qv42YAWb_PT5y_C_uFD03GM\" rel=\"nofollow\">Extraction from training logs</a></p>\n<p>The fit was performed with some default parameters. The loss value did decreased on second evaluation. However loss did not changed after that. The final Root weighted squared error (rwse) in this training became 0.126.</p>\n<p>Hyperparameter tuning was performed later and the best parameter set was used in the following training. The result of that training is shown below.</p>\n<p><a href=\"https://drive.google.com/open?id=1zrt1jsfVdFAtuEegzquV-8sU8AJtlmKY\" rel=\"nofollow\">Post hyperparameter tuning log extraction</a></p>\n<p>Tree things to note here. First, the loss value at the beginning is lower than at later evaluation steps. Low value in the beginning most likely due to choice of parameters from the results of hyperparameter tuning. Increase of the loss value later on looks strange. Second, it\u2019s unchanged loss value after second evaluation. This pattern remains the same while self.fit(input_fn=input_fn) is used for model training. Third, the final rwse in this training became 0.487 while during hyperparameter tuning with the same parameter set rwse=0.015.</p>\n<p>The question is if anyone observed something similar? Could the reason of unchanged loss value be due to fixed learning rate? If yes, is it possible to change it while using WALSMatrixFactorization class and self.fit(input_fn=input_fn)??</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li>\n<p>GCP VM instance  created-with-datalab-version\t20180503</p>\n</li>\n<li>\n<p>TensorFlow version 1.8.0</p>\n</li>\n</ul>", "body_text": "Our current implementation of recommendation system use TF 1.8 and WALS algorithm. The model was trained using self.fit(input_fn=input_fn) and ML Engine with run time version 1.8. Data set was formed following example using tensorflow.train.Example(...) Extraction from training logs shown below.\nExtraction from training logs\nThe fit was performed with some default parameters. The loss value did decreased on second evaluation. However loss did not changed after that. The final Root weighted squared error (rwse) in this training became 0.126.\nHyperparameter tuning was performed later and the best parameter set was used in the following training. The result of that training is shown below.\nPost hyperparameter tuning log extraction\nTree things to note here. First, the loss value at the beginning is lower than at later evaluation steps. Low value in the beginning most likely due to choice of parameters from the results of hyperparameter tuning. Increase of the loss value later on looks strange. Second, it\u2019s unchanged loss value after second evaluation. This pattern remains the same while self.fit(input_fn=input_fn) is used for model training. Third, the final rwse in this training became 0.487 while during hyperparameter tuning with the same parameter set rwse=0.015.\nThe question is if anyone observed something similar? Could the reason of unchanged loss value be due to fixed learning rate? If yes, is it possible to change it while using WALSMatrixFactorization class and self.fit(input_fn=input_fn)??\n\nSystem information\n\n\nGCP VM instance  created-with-datalab-version\t20180503\n\n\nTensorFlow version 1.8.0", "body": "Our current implementation of recommendation system use TF 1.8 and WALS algorithm. The model was trained using self.fit(input_fn=input_fn) and ML Engine with run time version 1.8. Data set was formed following [example](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/10_recommend/wals.ipynb) using tensorflow.train.Example(...) Extraction from training logs shown below. \r\n\r\n[Extraction from training logs](https://drive.google.com/open?id=1HS4dPXC31Qv42YAWb_PT5y_C_uFD03GM) \r\n\r\nThe fit was performed with some default parameters. The loss value did decreased on second evaluation. However loss did not changed after that. The final Root weighted squared error (rwse) in this training became 0.126.\r\n\r\nHyperparameter tuning was performed later and the best parameter set was used in the following training. The result of that training is shown below.\r\n\r\n[Post hyperparameter tuning log extraction](https://drive.google.com/open?id=1zrt1jsfVdFAtuEegzquV-8sU8AJtlmKY)\r\n\r\nTree things to note here. First, the loss value at the beginning is lower than at later evaluation steps. Low value in the beginning most likely due to choice of parameters from the results of hyperparameter tuning. Increase of the loss value later on looks strange. Second, it\u2019s unchanged loss value after second evaluation. This pattern remains the same while self.fit(input_fn=input_fn) is used for model training. Third, the final rwse in this training became 0.487 while during hyperparameter tuning with the same parameter set rwse=0.015. \r\n\r\nThe question is if anyone observed something similar? Could the reason of unchanged loss value be due to fixed learning rate? If yes, is it possible to change it while using WALSMatrixFactorization class and self.fit(input_fn=input_fn)??\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n- GCP VM instance  created-with-datalab-version\t20180503\r\n\r\n- TensorFlow version 1.8.0\r\n\r\n"}