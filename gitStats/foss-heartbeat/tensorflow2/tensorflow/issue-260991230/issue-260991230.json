{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13342", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13342/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13342/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13342/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13342", "id": 260991230, "node_id": "MDU6SXNzdWUyNjA5OTEyMzA=", "number": 13342, "title": "slim.learning.train can't restore variables if new variable have created.", "user": {"login": "gauss-clb", "id": 11674304, "node_id": "MDQ6VXNlcjExNjc0MzA0", "avatar_url": "https://avatars2.githubusercontent.com/u/11674304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gauss-clb", "html_url": "https://github.com/gauss-clb", "followers_url": "https://api.github.com/users/gauss-clb/followers", "following_url": "https://api.github.com/users/gauss-clb/following{/other_user}", "gists_url": "https://api.github.com/users/gauss-clb/gists{/gist_id}", "starred_url": "https://api.github.com/users/gauss-clb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gauss-clb/subscriptions", "organizations_url": "https://api.github.com/users/gauss-clb/orgs", "repos_url": "https://api.github.com/users/gauss-clb/repos", "events_url": "https://api.github.com/users/gauss-clb/events{/privacy}", "received_events_url": "https://api.github.com/users/gauss-clb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-09-27T14:16:28Z", "updated_at": "2018-10-13T13:27:20Z", "closed_at": "2017-09-30T13:24:22Z", "author_association": "NONE", "body_html": "<p>Look at my code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">slim_train_init_fn_test</span>():\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">        In the model_ckpt/slim_train_init_fn_test.ckpt,</span>\n<span class=\"pl-s\">        {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    x_y <span class=\"pl-k\">=</span> slim.variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x/y<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4.0</span>)\n    x_z <span class=\"pl-k\">=</span> slim.variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x/z<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5.0</span>)\n    y_z <span class=\"pl-k\">=</span> slim.variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y/z<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">6.0</span>)\n\n    variables_to_restore <span class=\"pl-k\">=</span> slim.get_variables_to_restore(<span class=\"pl-v\">include</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>])\n\n    init_fn <span class=\"pl-k\">=</span> tf.contrib.framework.assign_from_checkpoint_fn(\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_ckpt/slim_train_init_fn_test.ckpt<span class=\"pl-pds\">'</span></span>, variables_to_restore)\n    optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">.001</span>)\n    loss <span class=\"pl-k\">=</span> slim.variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10.0</span>)\n    train_op <span class=\"pl-k\">=</span> slim.learning.create_train_op(loss, optimizer)\n    slim.learning.train(train_op, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>log<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">init_fn</span><span class=\"pl-k\">=</span>init_fn, <span class=\"pl-v\">number_of_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)</pre></div>\n<p>In the <code>model_ckpt/slim_train_init_fn_test.ckpt</code>,  there is a map  <code>{'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}</code>, and I use<br>\n<code>get_variables_to_restore(include=['x'])</code> to restore variable <code>x_y</code> and  <code>x_z</code> which are in  <code>model_ckpt/slim_train_init_fn_test.ckpt</code>, but I create a new variable <code>loss</code> which isn't in <code>model_ckpt/slim_train_init_fn_test.ckpt</code>, the code raise an error:  Can't find key <code>loss</code> in check point file.<br>\nSo I can't understand why check point file should have key <code>loss</code>,  does <code>init_fn</code> not pass <code>variables_to_restore</code> to <code>tf.train.Saver</code> ? Is it a bug?</p>\n<p>View the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L659\">assign_from_checkpoint_fn</a>, it indeed uses <code>assign_from_checkpoint_fn</code> to initialize <code>tf.train.Saver</code> .  And there is another parameter <code>saver</code>  in  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L729\">slim.learning.train</a>, it is  just used to <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L774\">save</a>  parameters of model, not <code>restore</code>.</p>\n<p>So what's  wrong?</p>", "body_text": "Look at my code:\ndef slim_train_init_fn_test():\n    '''\n        In the model_ckpt/slim_train_init_fn_test.ckpt,\n        {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}\n    '''\n    x_y = slim.variable('x/y', initializer=4.0)\n    x_z = slim.variable('x/z', initializer=5.0)\n    y_z = slim.variable('y/z', initializer=6.0)\n\n    variables_to_restore = slim.get_variables_to_restore(include=['x'])\n\n    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n        'model_ckpt/slim_train_init_fn_test.ckpt', variables_to_restore)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\n    loss = slim.variable('loss', initializer=10.0)\n    train_op = slim.learning.create_train_op(loss, optimizer)\n    slim.learning.train(train_op, 'log', init_fn=init_fn, number_of_steps=1)\nIn the model_ckpt/slim_train_init_fn_test.ckpt,  there is a map  {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}, and I use\nget_variables_to_restore(include=['x']) to restore variable x_y and  x_z which are in  model_ckpt/slim_train_init_fn_test.ckpt, but I create a new variable loss which isn't in model_ckpt/slim_train_init_fn_test.ckpt, the code raise an error:  Can't find key loss in check point file.\nSo I can't understand why check point file should have key loss,  does init_fn not pass variables_to_restore to tf.train.Saver ? Is it a bug?\nView the assign_from_checkpoint_fn, it indeed uses assign_from_checkpoint_fn to initialize tf.train.Saver .  And there is another parameter saver  in  slim.learning.train, it is  just used to save  parameters of model, not restore.\nSo what's  wrong?", "body": "Look at my code:\r\n\r\n```python\r\ndef slim_train_init_fn_test():\r\n    '''\r\n        In the model_ckpt/slim_train_init_fn_test.ckpt,\r\n        {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}\r\n    '''\r\n    x_y = slim.variable('x/y', initializer=4.0)\r\n    x_z = slim.variable('x/z', initializer=5.0)\r\n    y_z = slim.variable('y/z', initializer=6.0)\r\n\r\n    variables_to_restore = slim.get_variables_to_restore(include=['x'])\r\n\r\n    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(\r\n        'model_ckpt/slim_train_init_fn_test.ckpt', variables_to_restore)\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\r\n    loss = slim.variable('loss', initializer=10.0)\r\n    train_op = slim.learning.create_train_op(loss, optimizer)\r\n    slim.learning.train(train_op, 'log', init_fn=init_fn, number_of_steps=1)\r\n```\r\n\r\n In the `model_ckpt/slim_train_init_fn_test.ckpt`,  there is a map  `{'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}`, and I use \r\n`get_variables_to_restore(include=['x'])` to restore variable `x_y` and  `x_z` which are in  `model_ckpt/slim_train_init_fn_test.ckpt`, but I create a new variable `loss` which isn't in `model_ckpt/slim_train_init_fn_test.ckpt`, the code raise an error:  Can't find key `loss` in check point file. \r\nSo I can't understand why check point file should have key `loss`,  does `init_fn` not pass `variables_to_restore` to `tf.train.Saver` ? Is it a bug?\r\n\r\n\r\nView the [assign_from_checkpoint_fn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L659), it indeed uses `assign_from_checkpoint_fn` to initialize `tf.train.Saver` .  And there is another parameter `saver`  in  [slim.learning.train](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L729), it is  just used to [save](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L774)  parameters of model, not `restore`. \r\n\r\nSo what's  wrong?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}