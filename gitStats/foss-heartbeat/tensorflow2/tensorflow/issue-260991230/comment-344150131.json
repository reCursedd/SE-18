{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/344150131", "html_url": "https://github.com/tensorflow/tensorflow/issues/13342#issuecomment-344150131", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13342", "id": 344150131, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NDE1MDEzMQ==", "user": {"login": "Amitayus", "id": 2138952, "node_id": "MDQ6VXNlcjIxMzg5NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2138952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Amitayus", "html_url": "https://github.com/Amitayus", "followers_url": "https://api.github.com/users/Amitayus/followers", "following_url": "https://api.github.com/users/Amitayus/following{/other_user}", "gists_url": "https://api.github.com/users/Amitayus/gists{/gist_id}", "starred_url": "https://api.github.com/users/Amitayus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Amitayus/subscriptions", "organizations_url": "https://api.github.com/users/Amitayus/orgs", "repos_url": "https://api.github.com/users/Amitayus/repos", "events_url": "https://api.github.com/users/Amitayus/events{/privacy}", "received_events_url": "https://api.github.com/users/Amitayus/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-14T05:23:57Z", "updated_at": "2017-11-14T05:47:52Z", "author_association": "NONE", "body_html": "<p>I have the same problem with you. I solve it by doing following two steps.</p>\n<h3>1. pass the parameter <code>saver</code> to <code>slim.learning.train()</code></h3>\n<div class=\"highlight highlight-source-python\"><pre>ckpt <span class=\"pl-k\">=</span> tf.train.get_checkpoint_state(<span class=\"pl-c1\">FLAGS</span>.train_dir)\nsaver <span class=\"pl-k\">=</span> tf.train.Saver(<span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>optimistic_restore_vars(ckpt.model_checkpoint_path) <span class=\"pl-k\">if</span> ckpt <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>)</pre></div>\n<p>where function <a href=\"https://github.com/tensorflow/tensorflow/issues/312\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/312/hovercard\">optimistic_restore_vars</a> is defined as</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">optimistic_restore_vars</span>(<span class=\"pl-smi\">model_checkpoint_path</span>):\n    reader <span class=\"pl-k\">=</span> tf.train.NewCheckpointReader(model_checkpoint_path)\n    saved_shapes <span class=\"pl-k\">=</span> reader.get_variable_to_shape_map()\n    var_names <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sorted</span>([(var.name, var.name.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>:<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>]) <span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> tf.global_variables()\n                        <span class=\"pl-k\">if</span> var.name.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>:<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">in</span> saved_shapes])\n    restore_vars <span class=\"pl-k\">=</span> []\n    name2var <span class=\"pl-k\">=</span> <span class=\"pl-c1\">dict</span>(<span class=\"pl-c1\">zip</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>:x.name.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>:<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>], tf.global_variables()), tf.global_variables()))\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-k\">for</span> var_name, saved_var_name <span class=\"pl-k\">in</span> var_names:\n            curr_var <span class=\"pl-k\">=</span> name2var[saved_var_name]\n            var_shape <span class=\"pl-k\">=</span> curr_var.get_shape().as_list()\n            <span class=\"pl-k\">if</span> var_shape <span class=\"pl-k\">==</span> saved_shapes[saved_var_name]:\n                restore_vars.append(curr_var)\n    <span class=\"pl-k\">return</span> restore_vars</pre></div>\n<h3>2. pass the parameter <code>local_init_op</code> to <code>slim.learning.train()</code> to initialize the added new variables</h3>\n<div class=\"highlight highlight-source-python\"><pre>local_init_op <span class=\"pl-k\">=</span> tf.global_variables_initializer()</pre></div>\n<h3>In last, the code should look like this</h3>\n<div class=\"highlight highlight-source-python\"><pre>ckpt <span class=\"pl-k\">=</span> tf.train.get_checkpoint_state(<span class=\"pl-c1\">FLAGS</span>.train_dir)\nsaver <span class=\"pl-k\">=</span> tf.train.Saver(<span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>optimistic_restore_vars(ckpt.model_checkpoint_path) <span class=\"pl-k\">if</span> ckpt <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>)\nlocal_init_op <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>##########################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Kicks off the training. #</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>##########################</span>\nlearning.train(\n    train_tensor,\n    <span class=\"pl-v\">saver</span><span class=\"pl-k\">=</span>saver,\n    <span class=\"pl-v\">local_init_op</span><span class=\"pl-k\">=</span>local_init_op,\n    <span class=\"pl-v\">logdir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.train_dir,\n    <span class=\"pl-v\">master</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.master,\n    <span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">FLAGS</span>.task <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>),\n    <span class=\"pl-v\">init_fn</span><span class=\"pl-k\">=</span>_get_init_fn(),\n    <span class=\"pl-v\">summary_op</span><span class=\"pl-k\">=</span>summary_op,\n    <span class=\"pl-v\">number_of_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.max_number_of_steps,\n    <span class=\"pl-v\">log_every_n_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.log_every_n_steps,\n    <span class=\"pl-v\">save_summaries_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.save_summaries_secs,\n    <span class=\"pl-v\">save_interval_secs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.save_interval_secs,\n    <span class=\"pl-v\">sync_optimizer</span><span class=\"pl-k\">=</span>optimizer <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.sync_replicas <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>)</pre></div>", "body_text": "I have the same problem with you. I solve it by doing following two steps.\n1. pass the parameter saver to slim.learning.train()\nckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\nsaver = tf.train.Saver(var_list=optimistic_restore_vars(ckpt.model_checkpoint_path) if ckpt else None)\nwhere function optimistic_restore_vars is defined as\ndef optimistic_restore_vars(model_checkpoint_path):\n    reader = tf.train.NewCheckpointReader(model_checkpoint_path)\n    saved_shapes = reader.get_variable_to_shape_map()\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\n                        if var.name.split(':')[0] in saved_shapes])\n    restore_vars = []\n    name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\n    with tf.variable_scope('', reuse=True):\n        for var_name, saved_var_name in var_names:\n            curr_var = name2var[saved_var_name]\n            var_shape = curr_var.get_shape().as_list()\n            if var_shape == saved_shapes[saved_var_name]:\n                restore_vars.append(curr_var)\n    return restore_vars\n2. pass the parameter local_init_op to slim.learning.train() to initialize the added new variables\nlocal_init_op = tf.global_variables_initializer()\nIn last, the code should look like this\nckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\nsaver = tf.train.Saver(var_list=optimistic_restore_vars(ckpt.model_checkpoint_path) if ckpt else None)\nlocal_init_op = tf.global_variables_initializer()\n\n###########################\n# Kicks off the training. #\n###########################\nlearning.train(\n    train_tensor,\n    saver=saver,\n    local_init_op=local_init_op,\n    logdir=FLAGS.train_dir,\n    master=FLAGS.master,\n    is_chief=(FLAGS.task == 0),\n    init_fn=_get_init_fn(),\n    summary_op=summary_op,\n    number_of_steps=FLAGS.max_number_of_steps,\n    log_every_n_steps=FLAGS.log_every_n_steps,\n    save_summaries_secs=FLAGS.save_summaries_secs,\n    save_interval_secs=FLAGS.save_interval_secs,\n    sync_optimizer=optimizer if FLAGS.sync_replicas else None)", "body": "I have the same problem with you. I solve it by doing following two steps.\r\n\r\n### 1. pass the parameter `saver` to `slim.learning.train()`\r\n```python\r\nckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\r\nsaver = tf.train.Saver(var_list=optimistic_restore_vars(ckpt.model_checkpoint_path) if ckpt else None)\r\n```\r\nwhere function [optimistic_restore_vars](https://github.com/tensorflow/tensorflow/issues/312) is defined as\r\n```python\r\ndef optimistic_restore_vars(model_checkpoint_path):\r\n    reader = tf.train.NewCheckpointReader(model_checkpoint_path)\r\n    saved_shapes = reader.get_variable_to_shape_map()\r\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n                        if var.name.split(':')[0] in saved_shapes])\r\n    restore_vars = []\r\n    name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\r\n    with tf.variable_scope('', reuse=True):\r\n        for var_name, saved_var_name in var_names:\r\n            curr_var = name2var[saved_var_name]\r\n            var_shape = curr_var.get_shape().as_list()\r\n            if var_shape == saved_shapes[saved_var_name]:\r\n                restore_vars.append(curr_var)\r\n    return restore_vars\r\n```\r\n\r\n### 2. pass the parameter `local_init_op` to `slim.learning.train()` to initialize the added new variables\r\n```python  \r\nlocal_init_op = tf.global_variables_initializer()\r\n```\r\n\r\n### In last, the code should look like this\r\n```python\r\nckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\r\nsaver = tf.train.Saver(var_list=optimistic_restore_vars(ckpt.model_checkpoint_path) if ckpt else None)\r\nlocal_init_op = tf.global_variables_initializer()\r\n\r\n###########################\r\n# Kicks off the training. #\r\n###########################\r\nlearning.train(\r\n    train_tensor,\r\n    saver=saver,\r\n    local_init_op=local_init_op,\r\n    logdir=FLAGS.train_dir,\r\n    master=FLAGS.master,\r\n    is_chief=(FLAGS.task == 0),\r\n    init_fn=_get_init_fn(),\r\n    summary_op=summary_op,\r\n    number_of_steps=FLAGS.max_number_of_steps,\r\n    log_every_n_steps=FLAGS.log_every_n_steps,\r\n    save_summaries_secs=FLAGS.save_summaries_secs,\r\n    save_interval_secs=FLAGS.save_interval_secs,\r\n    sync_optimizer=optimizer if FLAGS.sync_replicas else None)\r\n```\r\n"}