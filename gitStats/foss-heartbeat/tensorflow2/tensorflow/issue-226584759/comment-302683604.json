{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/302683604", "html_url": "https://github.com/tensorflow/tensorflow/pull/9686#issuecomment-302683604", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9686", "id": 302683604, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjY4MzYwNA==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-19T11:55:56Z", "updated_at": "2017-05-19T11:55:56Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>BTW, can you expand on your use case for the map version of the staging area? Just out of curiosity...</p>\n</blockquote>\n<ol>\n<li>Partial inserts. Feeding one Staging Area (key, tuple) from multiple (possibly, distributed) locations. i.e. scatter portions of the problem from master/chief and feed other portions of the problem on a local worker. I've a <a href=\"https://github.com/ska-sa/montblanc/blob/75e47e5a4ee8e7f599b6e5805333a39754beadd8/montblanc/impl/rime/tensorflow/sources/ms_source_provider.py#L84-L213\">data source</a> interface to my computation and I want to allow my users to feed data in multiple places.</li>\n<li>Capacity and Memory Budget. Feeding a potentially very large set of Tensors onto the GPU asynchronously and making sure that only a restricted portion of the set is on the Staging Area at one time.</li>\n<li>Peek functionality. Caching Tensors on GPU for the duration of other operations (i.e. point 2).</li>\n<li>Ordering. I'm grouping data (and computation) by a integer key defined by data tile extents. There's a partial ordering here thats useful to me. Ultimately I want to use this key for <a href=\"https://en.wikipedia.org/wiki/Distributed_hash_table\" rel=\"nofollow\">DHT</a> functionality, but a priority queue is always useful.</li>\n<li>Holding ref-counted Tensors, rather than memcpying tensors slices around.</li>\n</ol>\n<p>In general, having a stateful hash table in the Graph, logically grouping ref-counted Tensor objects together on either CPUs or GPUs is pretty useful. The Queues always seemed heavyweight (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"161912688\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3009\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3009/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3009\">#3009</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"192051647\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5907\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5907/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/5907\">#5907</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"200787789\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6845\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6845/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6845\">#6845</a>) so I wanted to avoid memcpys. I saw some tensor slicing memcpys in Barrier, and it's not always clear what's happening lower down in the PriorityQueue/FIFOQueue level.</p>", "body_text": "BTW, can you expand on your use case for the map version of the staging area? Just out of curiosity...\n\n\nPartial inserts. Feeding one Staging Area (key, tuple) from multiple (possibly, distributed) locations. i.e. scatter portions of the problem from master/chief and feed other portions of the problem on a local worker. I've a data source interface to my computation and I want to allow my users to feed data in multiple places.\nCapacity and Memory Budget. Feeding a potentially very large set of Tensors onto the GPU asynchronously and making sure that only a restricted portion of the set is on the Staging Area at one time.\nPeek functionality. Caching Tensors on GPU for the duration of other operations (i.e. point 2).\nOrdering. I'm grouping data (and computation) by a integer key defined by data tile extents. There's a partial ordering here thats useful to me. Ultimately I want to use this key for DHT functionality, but a priority queue is always useful.\nHolding ref-counted Tensors, rather than memcpying tensors slices around.\n\nIn general, having a stateful hash table in the Graph, logically grouping ref-counted Tensor objects together on either CPUs or GPUs is pretty useful. The Queues always seemed heavyweight (#3009, #5907, #6845) so I wanted to avoid memcpys. I saw some tensor slicing memcpys in Barrier, and it's not always clear what's happening lower down in the PriorityQueue/FIFOQueue level.", "body": "> BTW, can you expand on your use case for the map version of the staging area? Just out of curiosity...\r\n\r\n1. Partial inserts. Feeding one Staging Area (key, tuple) from multiple (possibly, distributed) locations. i.e. scatter portions of the problem from master/chief and feed other portions of the problem on a local worker. I've a [data source](https://github.com/ska-sa/montblanc/blob/75e47e5a4ee8e7f599b6e5805333a39754beadd8/montblanc/impl/rime/tensorflow/sources/ms_source_provider.py#L84-L213) interface to my computation and I want to allow my users to feed data in multiple places.\r\n2. Capacity and Memory Budget. Feeding a potentially very large set of Tensors onto the GPU asynchronously and making sure that only a restricted portion of the set is on the Staging Area at one time.\r\n3. Peek functionality. Caching Tensors on GPU for the duration of other operations (i.e. point 2).\r\n4. Ordering. I'm grouping data (and computation) by a integer key defined by data tile extents. There's a partial ordering here thats useful to me. Ultimately I want to use this key for [DHT](https://en.wikipedia.org/wiki/Distributed_hash_table) functionality, but a priority queue is always useful.\r\n5. Holding ref-counted Tensors, rather than memcpying tensors slices around.\r\n\r\nIn general, having a stateful hash table in the Graph, logically grouping ref-counted Tensor objects together on either CPUs or GPUs is pretty useful. The Queues always seemed heavyweight (https://github.com/tensorflow/tensorflow/issues/3009, https://github.com/tensorflow/tensorflow/issues/5907, https://github.com/tensorflow/tensorflow/issues/6845) so I wanted to avoid memcpys. I saw some tensor slicing memcpys in Barrier, and it's not always clear what's happening lower down in the PriorityQueue/FIFOQueue level.\r\n\r\n"}