{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301720132", "html_url": "https://github.com/tensorflow/tensorflow/pull/9686#issuecomment-301720132", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9686", "id": 301720132, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTcyMDEzMg==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-16T09:00:47Z", "updated_at": "2017-05-16T09:07:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>One question. I see that structures like FIFOQueue use PersistentTensors, presumably for reasons suggested <a href=\"https://github.com/tensorflow/tensorflow/blob/554b57f74a99ea18baf616ae5e3cff8b137430e6/tensorflow/core/framework/op_kernel.h#L193-L198\">here</a>:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> Wraps a tensor that is held by an Op across calls to Compute(). For</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> memory safety when using asynchronous devices like GPUs, the system</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> must be notified when a Tensor is used inside an Op execution. The</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> wrapper ensures that all uses of the Tensor are tracked, because in</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> order to retrieve the Tensor the caller must use AccessTensor which</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> notifies the context.</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">PersistentTensor</span> { ... };</pre></div>\n<ol>\n<li>Should the tensors in the Staging Area be wrapped in PersistentTensors?</li>\n<li>Also, should they be wrapped in PersistentTensors so that they can live in the container across multiple Sessions?</li>\n</ol>\n<p>The answer to</p>\n<ol>\n<li>Might be maybe, because peek() provides access to Tensors inside the container...</li>\n<li>I think is no.</li>\n</ol>", "body_text": "One question. I see that structures like FIFOQueue use PersistentTensors, presumably for reasons suggested here:\n// Wraps a tensor that is held by an Op across calls to Compute(). For\n// memory safety when using asynchronous devices like GPUs, the system\n// must be notified when a Tensor is used inside an Op execution. The\n// wrapper ensures that all uses of the Tensor are tracked, because in\n// order to retrieve the Tensor the caller must use AccessTensor which\n// notifies the context.\nclass PersistentTensor { ... };\n\nShould the tensors in the Staging Area be wrapped in PersistentTensors?\nAlso, should they be wrapped in PersistentTensors so that they can live in the container across multiple Sessions?\n\nThe answer to\n\nMight be maybe, because peek() provides access to Tensors inside the container...\nI think is no.", "body": "One question. I see that structures like FIFOQueue use PersistentTensors, presumably for reasons suggested [here](https://github.com/tensorflow/tensorflow/blob/554b57f74a99ea18baf616ae5e3cff8b137430e6/tensorflow/core/framework/op_kernel.h#L193-L198):\r\n\r\n```c++\r\n// Wraps a tensor that is held by an Op across calls to Compute(). For\r\n// memory safety when using asynchronous devices like GPUs, the system\r\n// must be notified when a Tensor is used inside an Op execution. The\r\n// wrapper ensures that all uses of the Tensor are tracked, because in\r\n// order to retrieve the Tensor the caller must use AccessTensor which\r\n// notifies the context.\r\nclass PersistentTensor { ... };\r\n```\r\n\r\n1. Should the tensors in the Staging Area be wrapped in PersistentTensors?\r\n2. Also, should they be wrapped in PersistentTensors so that they can live in the container across multiple Sessions?\r\n\r\nThe answer to\r\n\r\n1. Might be maybe, because peek() provides access to Tensors inside the container...\r\n2. I think is no."}