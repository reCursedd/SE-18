{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328953735", "html_url": "https://github.com/tensorflow/tensorflow/issues/10749#issuecomment-328953735", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10749", "id": 328953735, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODk1MzczNQ==", "user": {"login": "sirgogo", "id": 5591329, "node_id": "MDQ6VXNlcjU1OTEzMjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5591329?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sirgogo", "html_url": "https://github.com/sirgogo", "followers_url": "https://api.github.com/users/sirgogo/followers", "following_url": "https://api.github.com/users/sirgogo/following{/other_user}", "gists_url": "https://api.github.com/users/sirgogo/gists{/gist_id}", "starred_url": "https://api.github.com/users/sirgogo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sirgogo/subscriptions", "organizations_url": "https://api.github.com/users/sirgogo/orgs", "repos_url": "https://api.github.com/users/sirgogo/repos", "events_url": "https://api.github.com/users/sirgogo/events{/privacy}", "received_events_url": "https://api.github.com/users/sirgogo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-12T19:15:08Z", "updated_at": "2017-10-11T18:31:10Z", "author_association": "NONE", "body_html": "<p>This is still an issue. I am not sure how much it will matter in the long run if we are training a network around it anyway..., but it is quite shocking that the error is so high. This also makes debugging hard, because TF's FFT is undoubtedly not doing what we think it should be. Perhaps cuFFT's algorithm is not numerically stable? The error is there even for 1D, but it is much much smaller. This makes me think that the 2D implementation is not optimal, with a lot of fill in or non-optimal number of operations. If someone can point me to the code, maybe we can debug / suggest a fix without changing too much.</p>\n<p>Some python code to verify error using complex64 for both numpy and tf (i.e. based on float32):</p>\n<pre><code>## DEFINE FFTs\nfft1D_np = np.fft.fft\nfft1D_tf = tf.fft\nifft1D_np = np.fft.ifft\nifft1D_tf = tf.ifft\nfft2D_np = np.fft.fft2\nfft2D_tf = tf.fft2d\nifft2D_np = np.fft.ifft2\nifft2D_tf = tf.ifft2d\n</code></pre>\n<pre><code>  ## TEST 2D FFT\n   np.random.seed(13)\n   M = np.random.random([1,2,2]).astype(np.complex64)\n   M_tf = tf.placeholder(dtype=M.dtype, shape=[None,*M.shape[1:]], name='M_tf')\n   eval_dict = {M_tf: M}\n   sess, gsaver = nn_utils.init_network()\n   Mhat = fft2D_np(M) \n   Mhat_tf = fft2D_tf(M_tf)\n   Mhat_tfeval = Mhat_tf.eval(feed_dict=eval_dict)\n   e = Mhat - Mhat_tfeval\n   err = np.linalg.norm(e.reshape([-1,1]))\n   assert( np.isclose(err, 0.0) )\n</code></pre>\n<p>Doing some prints in my session:</p>\n<pre><code>&gt;&gt;&gt; M\narray([[[ 0.77770239+0.j,  0.23754121+0.j],\n        [ 0.82427853+0.j,  0.96574920+0.j]]], dtype=complex64)\n&gt;&gt;&gt; Mhat\narray([[[ 2.80527139+0.j,  0.39869052+0.j],\n        [-0.77478409+0.j,  0.68163186+0.j]]], dtype=complex64)\n&gt;&gt;&gt; Mhat_tfeval\narray([[[ 2.80527139+0.j,  0.39869046+0.j],\n        [-0.77478415+0.j,  0.68163186+0.j]]], dtype=complex64)\n&gt;&gt;&gt; e\narray([[[  0.00000000e+00+0.j,   5.96046448e-08+0.j],\n        [  5.96046448e-08+0.j,   0.00000000e+00+0.j]]], dtype=complex64)\n&gt;&gt;&gt; err\n8.4293696e-08\n&gt;&gt;&gt; np.isclose(err, 0.0)\nFalse\n</code></pre>", "body_text": "This is still an issue. I am not sure how much it will matter in the long run if we are training a network around it anyway..., but it is quite shocking that the error is so high. This also makes debugging hard, because TF's FFT is undoubtedly not doing what we think it should be. Perhaps cuFFT's algorithm is not numerically stable? The error is there even for 1D, but it is much much smaller. This makes me think that the 2D implementation is not optimal, with a lot of fill in or non-optimal number of operations. If someone can point me to the code, maybe we can debug / suggest a fix without changing too much.\nSome python code to verify error using complex64 for both numpy and tf (i.e. based on float32):\n## DEFINE FFTs\nfft1D_np = np.fft.fft\nfft1D_tf = tf.fft\nifft1D_np = np.fft.ifft\nifft1D_tf = tf.ifft\nfft2D_np = np.fft.fft2\nfft2D_tf = tf.fft2d\nifft2D_np = np.fft.ifft2\nifft2D_tf = tf.ifft2d\n\n  ## TEST 2D FFT\n   np.random.seed(13)\n   M = np.random.random([1,2,2]).astype(np.complex64)\n   M_tf = tf.placeholder(dtype=M.dtype, shape=[None,*M.shape[1:]], name='M_tf')\n   eval_dict = {M_tf: M}\n   sess, gsaver = nn_utils.init_network()\n   Mhat = fft2D_np(M) \n   Mhat_tf = fft2D_tf(M_tf)\n   Mhat_tfeval = Mhat_tf.eval(feed_dict=eval_dict)\n   e = Mhat - Mhat_tfeval\n   err = np.linalg.norm(e.reshape([-1,1]))\n   assert( np.isclose(err, 0.0) )\n\nDoing some prints in my session:\n>>> M\narray([[[ 0.77770239+0.j,  0.23754121+0.j],\n        [ 0.82427853+0.j,  0.96574920+0.j]]], dtype=complex64)\n>>> Mhat\narray([[[ 2.80527139+0.j,  0.39869052+0.j],\n        [-0.77478409+0.j,  0.68163186+0.j]]], dtype=complex64)\n>>> Mhat_tfeval\narray([[[ 2.80527139+0.j,  0.39869046+0.j],\n        [-0.77478415+0.j,  0.68163186+0.j]]], dtype=complex64)\n>>> e\narray([[[  0.00000000e+00+0.j,   5.96046448e-08+0.j],\n        [  5.96046448e-08+0.j,   0.00000000e+00+0.j]]], dtype=complex64)\n>>> err\n8.4293696e-08\n>>> np.isclose(err, 0.0)\nFalse", "body": "This is still an issue. I am not sure how much it will matter in the long run if we are training a network around it anyway..., but it is quite shocking that the error is so high. This also makes debugging hard, because TF's FFT is undoubtedly not doing what we think it should be. Perhaps cuFFT's algorithm is not numerically stable? The error is there even for 1D, but it is much much smaller. This makes me think that the 2D implementation is not optimal, with a lot of fill in or non-optimal number of operations. If someone can point me to the code, maybe we can debug / suggest a fix without changing too much.\r\n\r\nSome python code to verify error using complex64 for both numpy and tf (i.e. based on float32):\r\n```\r\n## DEFINE FFTs\r\nfft1D_np = np.fft.fft\r\nfft1D_tf = tf.fft\r\nifft1D_np = np.fft.ifft\r\nifft1D_tf = tf.ifft\r\nfft2D_np = np.fft.fft2\r\nfft2D_tf = tf.fft2d\r\nifft2D_np = np.fft.ifft2\r\nifft2D_tf = tf.ifft2d\r\n```\r\n ```\r\n   ## TEST 2D FFT\r\n    np.random.seed(13)\r\n    M = np.random.random([1,2,2]).astype(np.complex64)\r\n    M_tf = tf.placeholder(dtype=M.dtype, shape=[None,*M.shape[1:]], name='M_tf')\r\n    eval_dict = {M_tf: M}\r\n    sess, gsaver = nn_utils.init_network()\r\n    Mhat = fft2D_np(M) \r\n    Mhat_tf = fft2D_tf(M_tf)\r\n    Mhat_tfeval = Mhat_tf.eval(feed_dict=eval_dict)\r\n    e = Mhat - Mhat_tfeval\r\n    err = np.linalg.norm(e.reshape([-1,1]))\r\n    assert( np.isclose(err, 0.0) )\r\n```\r\n\r\nDoing some prints in my session:\r\n```\r\n>>> M\r\narray([[[ 0.77770239+0.j,  0.23754121+0.j],\r\n        [ 0.82427853+0.j,  0.96574920+0.j]]], dtype=complex64)\r\n>>> Mhat\r\narray([[[ 2.80527139+0.j,  0.39869052+0.j],\r\n        [-0.77478409+0.j,  0.68163186+0.j]]], dtype=complex64)\r\n>>> Mhat_tfeval\r\narray([[[ 2.80527139+0.j,  0.39869046+0.j],\r\n        [-0.77478415+0.j,  0.68163186+0.j]]], dtype=complex64)\r\n>>> e\r\narray([[[  0.00000000e+00+0.j,   5.96046448e-08+0.j],\r\n        [  5.96046448e-08+0.j,   0.00000000e+00+0.j]]], dtype=complex64)\r\n>>> err\r\n8.4293696e-08\r\n>>> np.isclose(err, 0.0)\r\nFalse\r\n```"}