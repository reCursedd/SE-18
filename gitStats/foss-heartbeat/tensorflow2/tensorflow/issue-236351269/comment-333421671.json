{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333421671", "html_url": "https://github.com/tensorflow/tensorflow/issues/10749#issuecomment-333421671", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10749", "id": 333421671, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzQyMTY3MQ==", "user": {"login": "rryan", "id": 26527, "node_id": "MDQ6VXNlcjI2NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/26527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rryan", "html_url": "https://github.com/rryan", "followers_url": "https://api.github.com/users/rryan/followers", "following_url": "https://api.github.com/users/rryan/following{/other_user}", "gists_url": "https://api.github.com/users/rryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rryan/subscriptions", "organizations_url": "https://api.github.com/users/rryan/orgs", "repos_url": "https://api.github.com/users/rryan/repos", "events_url": "https://api.github.com/users/rryan/events{/privacy}", "received_events_url": "https://api.github.com/users/rryan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-02T01:27:43Z", "updated_at": "2017-10-02T01:28:29Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5591329\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sirgogo\">@sirgogo</a>, can you please post the source to <code>fft2D_np</code> and <code>fft2D_tf</code> in your example?</p>\n<p>Are your ops running on GPU or CPU?<br>\n(have you verified that with <code>tf.ConfigProto(log_device_placement=True)</code>?)</p>\n<p>FWIW, NumPy is the oracle used for all of <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/fft_ops_test.py\">the FFT unit tests</a>.  The tolerance on those checks is <code>1e-4</code> at the moment, which is pretty high, but it's because we're using a one-size-fits all bound against Eigen's TensorFFT (CPU), and cuFFT (GPU). Please also note that the FFT unit tests include gradient tests, that verify the numerical gradient matches the symbolic gradient.</p>\n<p>Due to differences in the floating point hardware across your CPU and GPU, the results between NumPy and cuFFT will differ by some amount for an identical sequence of floating point operations. Additionally, NumPy uses fftpack for its FFTs -- so whether your ops are running on CPU or GPU, it's a different FFT implementation from TensorFlow's. That's not to say there isn't a bug in TensorFlow's invocation of these FFT implementations or the implementations themselves!</p>\n<p>Here are code pointers:</p>\n<ul>\n<li><a href=\"https://bitbucket.org/eigen/eigen/src/699b6595fc471456896fb27193c8ca51389b7850/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h?at=default&amp;fileviewer=file-view-default\" rel=\"nofollow\">Eigen TensorFFT</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fft_ops.cc\">TensorFlow FFT kernels</a>. Used to invoke either Eigen TensorFFT or cuFFT.</li>\n<li><a href=\"http://docs.nvidia.com/cuda/cufft/index.html\" rel=\"nofollow\">cuFFT documentation</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_fft.cc\">StreamExecutor invocation of cuFFT</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/spectral_grad.py\">TensorFlow FFT gradient definitions</a></li>\n</ul>\n<p>What is your expectation here?  e.g. would you be happy if <code>np.isclose</code> returned <code>True</code> in your example? That would imply you need something like <code>rtol=1e-5</code>, <code>atol=1e-8</code> ?</p>", "body_text": "@sirgogo, can you please post the source to fft2D_np and fft2D_tf in your example?\nAre your ops running on GPU or CPU?\n(have you verified that with tf.ConfigProto(log_device_placement=True)?)\nFWIW, NumPy is the oracle used for all of the FFT unit tests.  The tolerance on those checks is 1e-4 at the moment, which is pretty high, but it's because we're using a one-size-fits all bound against Eigen's TensorFFT (CPU), and cuFFT (GPU). Please also note that the FFT unit tests include gradient tests, that verify the numerical gradient matches the symbolic gradient.\nDue to differences in the floating point hardware across your CPU and GPU, the results between NumPy and cuFFT will differ by some amount for an identical sequence of floating point operations. Additionally, NumPy uses fftpack for its FFTs -- so whether your ops are running on CPU or GPU, it's a different FFT implementation from TensorFlow's. That's not to say there isn't a bug in TensorFlow's invocation of these FFT implementations or the implementations themselves!\nHere are code pointers:\n\nEigen TensorFFT\nTensorFlow FFT kernels. Used to invoke either Eigen TensorFFT or cuFFT.\ncuFFT documentation\nStreamExecutor invocation of cuFFT\nTensorFlow FFT gradient definitions\n\nWhat is your expectation here?  e.g. would you be happy if np.isclose returned True in your example? That would imply you need something like rtol=1e-5, atol=1e-8 ?", "body": "@sirgogo, can you please post the source to `fft2D_np` and `fft2D_tf` in your example?\r\n\r\nAre your ops running on GPU or CPU? \r\n(have you verified that with `tf.ConfigProto(log_device_placement=True)`?)\r\n\r\nFWIW, NumPy is the oracle used for all of [the FFT unit tests](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/fft_ops_test.py).  The tolerance on those checks is `1e-4` at the moment, which is pretty high, but it's because we're using a one-size-fits all bound against Eigen's TensorFFT (CPU), and cuFFT (GPU). Please also note that the FFT unit tests include gradient tests, that verify the numerical gradient matches the symbolic gradient.\r\n\r\nDue to differences in the floating point hardware across your CPU and GPU, the results between NumPy and cuFFT will differ by some amount for an identical sequence of floating point operations. Additionally, NumPy uses fftpack for its FFTs -- so whether your ops are running on CPU or GPU, it's a different FFT implementation from TensorFlow's. That's not to say there isn't a bug in TensorFlow's invocation of these FFT implementations or the implementations themselves!\r\n\r\nHere are code pointers:\r\n* [Eigen TensorFFT](https://bitbucket.org/eigen/eigen/src/699b6595fc471456896fb27193c8ca51389b7850/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h?at=default&fileviewer=file-view-default)\r\n* [TensorFlow FFT kernels](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fft_ops.cc). Used to invoke either Eigen TensorFFT or cuFFT.\r\n* [cuFFT documentation](http://docs.nvidia.com/cuda/cufft/index.html)\r\n* [StreamExecutor invocation of cuFFT](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_fft.cc)\r\n* [TensorFlow FFT gradient definitions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/spectral_grad.py)\r\n\r\nWhat is your expectation here?  e.g. would you be happy if `np.isclose` returned `True` in your example? That would imply you need something like `rtol=1e-5`, `atol=1e-8` ? "}