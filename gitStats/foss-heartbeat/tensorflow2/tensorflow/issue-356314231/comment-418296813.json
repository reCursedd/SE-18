{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/418296813", "html_url": "https://github.com/tensorflow/tensorflow/issues/22013#issuecomment-418296813", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22013", "id": 418296813, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODI5NjgxMw==", "user": {"login": "mpekalski", "id": 2975068, "node_id": "MDQ6VXNlcjI5NzUwNjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2975068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpekalski", "html_url": "https://github.com/mpekalski", "followers_url": "https://api.github.com/users/mpekalski/followers", "following_url": "https://api.github.com/users/mpekalski/following{/other_user}", "gists_url": "https://api.github.com/users/mpekalski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpekalski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpekalski/subscriptions", "organizations_url": "https://api.github.com/users/mpekalski/orgs", "repos_url": "https://api.github.com/users/mpekalski/repos", "events_url": "https://api.github.com/users/mpekalski/events{/privacy}", "received_events_url": "https://api.github.com/users/mpekalski/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-04T09:10:55Z", "updated_at": "2018-09-04T09:10:55Z", "author_association": "NONE", "body_html": "<p>I've got the code running by explicitly specifying shape in <code>tf.get_variable()</code>, and then making sure that the corresponding tensors in assign ops had the same shape (so not assigning 1.0 but [1.0]). Still, I could not make it work with dynamic shapes, and segmentation fault should not take place but throw some error message.</p>\n<p>Here is the working code</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\n                               , trainable=False, use_resource=True, shape=[1])             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, [1.0], name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\n                                   , trainable=False, use_resource=True, shape=[1])         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            print(y)\n            print(inputs)\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), [1.0])))\n            with tf.control_dependencies([assign_two]):\n                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\n                    return y.read_value(), label\n\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                    .map(scope_2)\n                    .batch(1)\n                    .repeat(1)        \n                    )\n    return dataset\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1()\n\nwith tf.variable_scope(\"iterator\"):\n    # Define iterator from_string_handle. In general it is useful to have\n    # this kind of iterator if one wants to switch between train and validation\n    # within the training loop.        \n    iterator_t = dataset_fn.make_initializable_iterator()\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                iterator_t.output_types,\n                                                iterator_t.output_shapes)\n    \n    def get_next_item():\n        next_elem = iterator.get_next(name=\"next_element\")\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n        return x, y    \n        \nwith tf.Session() as sess:\n\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    handle_t = sess.run(iterator_t.string_handle())\n    # Run data iterator initialisation\n    sess.run(iterator_t.initializer)\n    print(sess.graph.get_operations()) \n    while True:\n        try:\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n        except tf.errors.OutOfRangeError:\n                        print(\"End of training dataset.\")\n                        break        \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n</code></pre>\n<p>and the output</p>\n<pre><code>DS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32&gt;\ninitial scope: \nDS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32&gt;\n=============\n&lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32&gt;\nTensor(\"arg0:0\", shape=(), dtype=int32)\n[&lt;tf.Operation 'Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'scope_0/scope_1/Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'scope_0_1/scope_1/Placeholder' type=Placeholder&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Initializer/zeros/shape_as_tensor' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Initializer/zeros/Const' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Fill&gt;, &lt;tf.Operation 'scope_0/scope_1/x' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0_1/scope_1/Const' type=Const&gt;, &lt;tf.Operation 'scope_0_1/scope_1/x_is_one' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0_1/scope_1/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0_1/tensors/component_0' type=Const&gt;, &lt;tf.Operation 'scope_0_1/tensors/component_1' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/shape_as_tensor' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/Const' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Fill&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0_1/batch_size' type=Const&gt;, &lt;tf.Operation 'scope_0_1/count' type=Const&gt;, &lt;tf.Operation 'iterator/Iterator' type=Iterator&gt;, &lt;tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset&gt;, &lt;tf.Operation 'iterator/MapDataset' type=MapDataset&gt;, &lt;tf.Operation 'iterator/BatchDataset' type=BatchDataset&gt;, &lt;tf.Operation 'iterator/RepeatDataset' type=RepeatDataset&gt;, &lt;tf.Operation 'iterator/MakeIterator' type=MakeIterator&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'iterator/iterator_handle' type=Placeholder&gt;, &lt;tf.Operation 'iterator/IteratorFromStringHandle' type=IteratorFromStringHandle&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'init' type=NoOp&gt;, &lt;tf.Operation 'init_1' type=NoOp&gt;]\n(array([[0.22]], dtype=float32), array([-1], dtype=int32))\n(array([[0.22]], dtype=float32), array([-2], dtype=int32))\n(array([[0.22]], dtype=float32), array([-3], dtype=int32))\n(array([[0.22]], dtype=float32), array([-4], dtype=int32))\n(array([[0.22]], dtype=float32), array([-5], dtype=int32))\nEnd of training dataset.\n\nglobal vars: [&lt;tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32&gt;, &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32&gt;]\nlocal vars: []\n</code></pre>", "body_text": "I've got the code running by explicitly specifying shape in tf.get_variable(), and then making sure that the corresponding tensors in assign ops had the same shape (so not assigning 1.0 but [1.0]). Still, I could not make it work with dynamic shapes, and segmentation fault should not take place but throw some error message.\nHere is the working code\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\n                               , trainable=False, use_resource=True, shape=[1])             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, [1.0], name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\n                                   , trainable=False, use_resource=True, shape=[1])         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            print(y)\n            print(inputs)\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), [1.0])))\n            with tf.control_dependencies([assign_two]):\n                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\n                    return y.read_value(), label\n\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                    .map(scope_2)\n                    .batch(1)\n                    .repeat(1)        \n                    )\n    return dataset\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1()\n\nwith tf.variable_scope(\"iterator\"):\n    # Define iterator from_string_handle. In general it is useful to have\n    # this kind of iterator if one wants to switch between train and validation\n    # within the training loop.        \n    iterator_t = dataset_fn.make_initializable_iterator()\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                iterator_t.output_types,\n                                                iterator_t.output_shapes)\n    \n    def get_next_item():\n        next_elem = iterator.get_next(name=\"next_element\")\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n        return x, y    \n        \nwith tf.Session() as sess:\n\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    handle_t = sess.run(iterator_t.string_handle())\n    # Run data iterator initialisation\n    sess.run(iterator_t.initializer)\n    print(sess.graph.get_operations()) \n    while True:\n        try:\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n        except tf.errors.OutOfRangeError:\n                        print(\"End of training dataset.\")\n                        break        \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n\nand the output\nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>\ninitial scope: \nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\n=============\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\nTensor(\"arg0:0\", shape=(), dtype=int32)\n[<tf.Operation 'Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0_1/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/scope_1/Const' type=Const>, <tf.Operation 'scope_0_1/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0_1/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/tensors/component_0' type=Const>, <tf.Operation 'scope_0_1/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/batch_size' type=Const>, <tf.Operation 'scope_0_1/count' type=Const>, <tf.Operation 'iterator/Iterator' type=Iterator>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDataset' type=BatchDataset>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandle' type=IteratorFromStringHandle>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\n(array([[0.22]], dtype=float32), array([-1], dtype=int32))\n(array([[0.22]], dtype=float32), array([-2], dtype=int32))\n(array([[0.22]], dtype=float32), array([-3], dtype=int32))\n(array([[0.22]], dtype=float32), array([-4], dtype=int32))\n(array([[0.22]], dtype=float32), array([-5], dtype=int32))\nEnd of training dataset.\n\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>]\nlocal vars: []", "body": "I've got the code running by explicitly specifying shape in `tf.get_variable()`, and then making sure that the corresponding tensors in assign ops had the same shape (so not assigning 1.0 but [1.0]). Still, I could not make it work with dynamic shapes, and segmentation fault should not take place but throw some error message.\r\n\r\n\r\nHere is the working code\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\r\n                               , trainable=False, use_resource=True, shape=[1])             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, [1.0], name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\r\n                                   , trainable=False, use_resource=True, shape=[1])         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            print(y)\r\n            print(inputs)\r\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\r\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), [1.0])))\r\n            with tf.control_dependencies([assign_two]):\r\n                with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\r\n                    return y.read_value(), label\r\n\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\r\n                    .map(scope_2)\r\n                    .batch(1)\r\n                    .repeat(1)        \r\n                    )\r\n    return dataset\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1()\r\n\r\nwith tf.variable_scope(\"iterator\"):\r\n    # Define iterator from_string_handle. In general it is useful to have\r\n    # this kind of iterator if one wants to switch between train and validation\r\n    # within the training loop.        \r\n    iterator_t = dataset_fn.make_initializable_iterator()\r\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\r\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \r\n                                                iterator_t.output_types,\r\n                                                iterator_t.output_shapes)\r\n    \r\n    def get_next_item():\r\n        next_elem = iterator.get_next(name=\"next_element\")\r\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\r\n        return x, y    \r\n        \r\nwith tf.Session() as sess:\r\n\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    handle_t = sess.run(iterator_t.string_handle())\r\n    # Run data iterator initialisation\r\n    sess.run(iterator_t.initializer)\r\n    print(sess.graph.get_operations()) \r\n    while True:\r\n        try:\r\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\r\n        except tf.errors.OutOfRangeError:\r\n                        print(\"End of training dataset.\")\r\n                        break        \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n```\r\n\r\nand the output\r\n\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\r\n=============\r\n<tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\r\nTensor(\"arg0:0\", shape=(), dtype=int32)\r\n[<tf.Operation 'Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0_1/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/scope_1/Const' type=Const>, <tf.Operation 'scope_0_1/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0_1/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/tensors/component_0' type=Const>, <tf.Operation 'scope_0_1/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/batch_size' type=Const>, <tf.Operation 'scope_0_1/count' type=Const>, <tf.Operation 'iterator/Iterator' type=Iterator>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDataset' type=BatchDataset>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandle' type=IteratorFromStringHandle>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\r\n(array([[0.22]], dtype=float32), array([-1], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-2], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-3], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-4], dtype=int32))\r\n(array([[0.22]], dtype=float32), array([-5], dtype=int32))\r\nEnd of training dataset.\r\n\r\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>]\r\nlocal vars: []\r\n```"}