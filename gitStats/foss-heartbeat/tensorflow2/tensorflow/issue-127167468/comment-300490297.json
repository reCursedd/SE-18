{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300490297", "html_url": "https://github.com/tensorflow/tensorflow/issues/799#issuecomment-300490297", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/799", "id": 300490297, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDQ5MDI5Nw==", "user": {"login": "aparecidovieira", "id": 23358668, "node_id": "MDQ6VXNlcjIzMzU4NjY4", "avatar_url": "https://avatars1.githubusercontent.com/u/23358668?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aparecidovieira", "html_url": "https://github.com/aparecidovieira", "followers_url": "https://api.github.com/users/aparecidovieira/followers", "following_url": "https://api.github.com/users/aparecidovieira/following{/other_user}", "gists_url": "https://api.github.com/users/aparecidovieira/gists{/gist_id}", "starred_url": "https://api.github.com/users/aparecidovieira/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aparecidovieira/subscriptions", "organizations_url": "https://api.github.com/users/aparecidovieira/orgs", "repos_url": "https://api.github.com/users/aparecidovieira/repos", "events_url": "https://api.github.com/users/aparecidovieira/events{/privacy}", "received_events_url": "https://api.github.com/users/aparecidovieira/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-10T13:58:28Z", "updated_at": "2017-05-10T13:58:28Z", "author_association": "NONE", "body_html": "<p>Im still having the same issue. Any sugestions? I have tried the approch proposed by salomons, same results. Doenst return any tuple as a result.<br>\n((encoder_fw_outputs,<br>\nencoder_bw_outputs),<br>\n(encoder_fw_final_state,<br>\nencoder_bw_final_state)) = (<br>\ntf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,<br>\ncell_bw=encoder_cell,<br>\ninputs=encoder_inputs_embedded,<br>\nsequence_length=encoder_inputs_length,<br>\ndtype=tf.float64, time_major=True)<br>\n)</p>\n<p>ValueError                   Traceback (most recent call last)<br>\n in ()<br>\n20                                     inputs=encoder_inputs_embedded,<br>\n21                                     sequence_length=encoder_inputs_length,<br>\n---&gt; 22                                     dtype=tf.float32, time_major=True)<br>\n23     )<br>\n24</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)<br>\n348           initial_state=initial_state_fw, dtype=dtype,<br>\n349           parallel_iterations=parallel_iterations, swap_memory=swap_memory,<br>\n--&gt; 350           time_major=time_major, scope=fw_scope)<br>\n351<br>\n352     # Backward direction</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)<br>\n544         swap_memory=swap_memory,<br>\n545         sequence_length=sequence_length,<br>\n--&gt; 546         dtype=dtype)<br>\n547<br>\n548     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)<br>\n711       loop_vars=(time, output_ta, state),<br>\n712       parallel_iterations=parallel_iterations,<br>\n--&gt; 713       swap_memory=swap_memory)<br>\n714<br>\n715   # Unpack final output if not using output tuples.</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)<br>\n2603     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)<br>\n2604     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)<br>\n-&gt; 2605     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)<br>\n2606     return result<br>\n2607</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)<br>\n2436       self.Enter()<br>\n2437       original_body_result, exit_vars = self._BuildLoop(<br>\n-&gt; 2438           pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n2439     finally:<br>\n2440       self.Exit()</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n2386         structure=original_loop_vars,<br>\n2387         flat_sequence=vars_for_body_with_tensor_arrays)<br>\n-&gt; 2388     body_result = body(*packed_vars_for_body)<br>\n2389     if not nest.is_sequence(body_result):<br>\n2390       body_result = [body_result]</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)<br>\n694           call_cell=call_cell,<br>\n695           state_size=state_size,<br>\n--&gt; 696           skip_conditionals=True)<br>\n697     else:<br>\n698       (output, new_state) = call_cell()</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _rnn_step(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)<br>\n175     # steps.  This is faster when max_seq_len is equal to the number of unrolls<br>\n176     # (which is typical for dynamic_rnn).<br>\n--&gt; 177     new_output, new_state = call_cell()<br>\n178     nest.assert_same_structure(state, new_state)<br>\n179     new_state = nest.flatten(new_state)</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in ()<br>\n682<br>\n683     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)<br>\n--&gt; 684     call_cell = lambda: cell(input_t, state)<br>\n685<br>\n686     if sequence_length is not None:</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in <strong>call</strong>(self, inputs, state, scope)<br>\n336       # i = input_gate, j = new_input, f = forget_gate, o = output_gate<br>\n337       lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True,<br>\n--&gt; 338                             scope=scope)<br>\n339       i, j, f, o = array_ops.split(<br>\n340           value=lstm_matrix, num_or_size_splits=4, axis=1)</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in _linear(args, output_size, bias, bias_start, scope)<br>\n745   with vs.variable_scope(scope) as outer_scope:<br>\n746     weights = vs.get_variable(<br>\n--&gt; 747         \"weights\", [total_arg_size, output_size], dtype=dtype)<br>\n748     if len(args) == 1:<br>\n749       res = math_ops.matmul(args[0], weights)</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)<br>\n986       collections=collections, caching_device=caching_device,<br>\n987       partitioner=partitioner, validate_shape=validate_shape,<br>\n--&gt; 988       custom_getter=custom_getter)<br>\n989 get_variable_or_local_docstring = (<br>\n990     \"\"\"%s</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)<br>\n888           collections=collections, caching_device=caching_device,<br>\n889           partitioner=partitioner, validate_shape=validate_shape,<br>\n--&gt; 890           custom_getter=custom_getter)<br>\n891<br>\n892   def _get_partitioned_variable(self,</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)<br>\n346           reuse=reuse, trainable=trainable, collections=collections,<br>\n347           caching_device=caching_device, partitioner=partitioner,<br>\n--&gt; 348           validate_shape=validate_shape)<br>\n349<br>\n350   def _get_partitioned_variable(</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)<br>\n331           initializer=initializer, regularizer=regularizer, reuse=reuse,<br>\n332           trainable=trainable, collections=collections,<br>\n--&gt; 333           caching_device=caching_device, validate_shape=validate_shape)<br>\n334<br>\n335     if custom_getter is not None:</p>\n<p>/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)<br>\n637                          \" Did you mean to set reuse=True in VarScope? \"<br>\n638                          \"Originally defined at:\\n\\n%s\" % (<br>\n--&gt; 639                              name, \"\".join(traceback.format_list(tb))))<br>\n640       found_var = self._vars[name]<br>\n641       if not shape.is_compatible_with(found_var.get_shape()):</p>\n<p>ValueError: Variable bidirectional_rnn/fw/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:</p>\n<p>File \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear<br>\n\"weights\", [total_arg_size, output_size], dtype=dtype)<br>\nFile \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 338, in <strong>call</strong><br>\nscope=scope)<br>\nFile \"\", line 24, in <br>\ntime_major=True</p>", "body_text": "Im still having the same issue. Any sugestions? I have tried the approch proposed by salomons, same results. Doenst return any tuple as a result.\n((encoder_fw_outputs,\nencoder_bw_outputs),\n(encoder_fw_final_state,\nencoder_bw_final_state)) = (\ntf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\ncell_bw=encoder_cell,\ninputs=encoder_inputs_embedded,\nsequence_length=encoder_inputs_length,\ndtype=tf.float64, time_major=True)\n)\nValueError                   Traceback (most recent call last)\n in ()\n20                                     inputs=encoder_inputs_embedded,\n21                                     sequence_length=encoder_inputs_length,\n---> 22                                     dtype=tf.float32, time_major=True)\n23     )\n24\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\n348           initial_state=initial_state_fw, dtype=dtype,\n349           parallel_iterations=parallel_iterations, swap_memory=swap_memory,\n--> 350           time_major=time_major, scope=fw_scope)\n351\n352     # Backward direction\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\n544         swap_memory=swap_memory,\n545         sequence_length=sequence_length,\n--> 546         dtype=dtype)\n547\n548     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\n711       loop_vars=(time, output_ta, state),\n712       parallel_iterations=parallel_iterations,\n--> 713       swap_memory=swap_memory)\n714\n715   # Unpack final output if not using output tuples.\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\n2603     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\n2604     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\n-> 2605     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n2606     return result\n2607\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n2436       self.Enter()\n2437       original_body_result, exit_vars = self._BuildLoop(\n-> 2438           pred, body, original_loop_vars, loop_vars, shape_invariants)\n2439     finally:\n2440       self.Exit()\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n2386         structure=original_loop_vars,\n2387         flat_sequence=vars_for_body_with_tensor_arrays)\n-> 2388     body_result = body(*packed_vars_for_body)\n2389     if not nest.is_sequence(body_result):\n2390       body_result = [body_result]\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)\n694           call_cell=call_cell,\n695           state_size=state_size,\n--> 696           skip_conditionals=True)\n697     else:\n698       (output, new_state) = call_cell()\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _rnn_step(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\n175     # steps.  This is faster when max_seq_len is equal to the number of unrolls\n176     # (which is typical for dynamic_rnn).\n--> 177     new_output, new_state = call_cell()\n178     nest.assert_same_structure(state, new_state)\n179     new_state = nest.flatten(new_state)\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in ()\n682\n683     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n--> 684     call_cell = lambda: cell(input_t, state)\n685\n686     if sequence_length is not None:\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in call(self, inputs, state, scope)\n336       # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n337       lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True,\n--> 338                             scope=scope)\n339       i, j, f, o = array_ops.split(\n340           value=lstm_matrix, num_or_size_splits=4, axis=1)\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in _linear(args, output_size, bias, bias_start, scope)\n745   with vs.variable_scope(scope) as outer_scope:\n746     weights = vs.get_variable(\n--> 747         \"weights\", [total_arg_size, output_size], dtype=dtype)\n748     if len(args) == 1:\n749       res = math_ops.matmul(args[0], weights)\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\n986       collections=collections, caching_device=caching_device,\n987       partitioner=partitioner, validate_shape=validate_shape,\n--> 988       custom_getter=custom_getter)\n989 get_variable_or_local_docstring = (\n990     \"\"\"%s\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\n888           collections=collections, caching_device=caching_device,\n889           partitioner=partitioner, validate_shape=validate_shape,\n--> 890           custom_getter=custom_getter)\n891\n892   def _get_partitioned_variable(self,\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\n346           reuse=reuse, trainable=trainable, collections=collections,\n347           caching_device=caching_device, partitioner=partitioner,\n--> 348           validate_shape=validate_shape)\n349\n350   def _get_partitioned_variable(\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\n331           initializer=initializer, regularizer=regularizer, reuse=reuse,\n332           trainable=trainable, collections=collections,\n--> 333           caching_device=caching_device, validate_shape=validate_shape)\n334\n335     if custom_getter is not None:\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\n637                          \" Did you mean to set reuse=True in VarScope? \"\n638                          \"Originally defined at:\\n\\n%s\" % (\n--> 639                              name, \"\".join(traceback.format_list(tb))))\n640       found_var = self._vars[name]\n641       if not shape.is_compatible_with(found_var.get_shape()):\nValueError: Variable bidirectional_rnn/fw/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\nFile \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\n\"weights\", [total_arg_size, output_size], dtype=dtype)\nFile \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 338, in call\nscope=scope)\nFile \"\", line 24, in \ntime_major=True", "body": "Im still having the same issue. Any sugestions? I have tried the approch proposed by salomons, same results. Doenst return any tuple as a result.\r\n((encoder_fw_outputs,\r\n  encoder_bw_outputs),\r\n (encoder_fw_final_state,\r\n  encoder_bw_final_state)) = (\r\n    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\r\n                                    cell_bw=encoder_cell,\r\n                                    inputs=encoder_inputs_embedded,\r\n                                    sequence_length=encoder_inputs_length,\r\n                                    dtype=tf.float64, time_major=True)\r\n    )\r\n\r\nValueError                   Traceback (most recent call last)\r\n<ipython-input-141-f7ed36f14c22> in <module>()\r\n     20                                     inputs=encoder_inputs_embedded,\r\n     21                                     sequence_length=encoder_inputs_length,\r\n---> 22                                     dtype=tf.float32, time_major=True)\r\n     23     )\r\n     24 \r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    348           initial_state=initial_state_fw, dtype=dtype,\r\n    349           parallel_iterations=parallel_iterations, swap_memory=swap_memory,\r\n--> 350           time_major=time_major, scope=fw_scope)\r\n    351 \r\n    352     # Backward direction\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    544         swap_memory=swap_memory,\r\n    545         sequence_length=sequence_length,\r\n--> 546         dtype=dtype)\r\n    547 \r\n    548     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    711       loop_vars=(time, output_ta, state),\r\n    712       parallel_iterations=parallel_iterations,\r\n--> 713       swap_memory=swap_memory)\r\n    714 \r\n    715   # Unpack final output if not using output tuples.\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2603     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\r\n   2604     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\r\n-> 2605     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2606     return result\r\n   2607 \r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2436       self.Enter()\r\n   2437       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2438           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2439     finally:\r\n   2440       self.Exit()\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2386         structure=original_loop_vars,\r\n   2387         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2388     body_result = body(*packed_vars_for_body)\r\n   2389     if not nest.is_sequence(body_result):\r\n   2390       body_result = [body_result]\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)\r\n    694           call_cell=call_cell,\r\n    695           state_size=state_size,\r\n--> 696           skip_conditionals=True)\r\n    697     else:\r\n    698       (output, new_state) = call_cell()\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in _rnn_step(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\r\n    175     # steps.  This is faster when max_seq_len is equal to the number of unrolls\r\n    176     # (which is typical for dynamic_rnn).\r\n--> 177     new_output, new_state = call_cell()\r\n    178     nest.assert_same_structure(state, new_state)\r\n    179     new_state = nest.flatten(new_state)\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc in <lambda>()\r\n    682 \r\n    683     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\r\n--> 684     call_cell = lambda: cell(input_t, state)\r\n    685 \r\n    686     if sequence_length is not None:\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\r\n    336       # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n    337       lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True,\r\n--> 338                             scope=scope)\r\n    339       i, j, f, o = array_ops.split(\r\n    340           value=lstm_matrix, num_or_size_splits=4, axis=1)\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in _linear(args, output_size, bias, bias_start, scope)\r\n    745   with vs.variable_scope(scope) as outer_scope:\r\n    746     weights = vs.get_variable(\r\n--> 747         \"weights\", [total_arg_size, output_size], dtype=dtype)\r\n    748     if len(args) == 1:\r\n    749       res = math_ops.matmul(args[0], weights)\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    986       collections=collections, caching_device=caching_device,\r\n    987       partitioner=partitioner, validate_shape=validate_shape,\r\n--> 988       custom_getter=custom_getter)\r\n    989 get_variable_or_local_docstring = (\r\n    990     \"\"\"%s\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    888           collections=collections, caching_device=caching_device,\r\n    889           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 890           custom_getter=custom_getter)\r\n    891 \r\n    892   def _get_partitioned_variable(self,\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\r\n    346           reuse=reuse, trainable=trainable, collections=collections,\r\n    347           caching_device=caching_device, partitioner=partitioner,\r\n--> 348           validate_shape=validate_shape)\r\n    349 \r\n    350   def _get_partitioned_variable(\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\r\n    331           initializer=initializer, regularizer=regularizer, reuse=reuse,\r\n    332           trainable=trainable, collections=collections,\r\n--> 333           caching_device=caching_device, validate_shape=validate_shape)\r\n    334 \r\n    335     if custom_getter is not None:\r\n\r\n/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\r\n    637                          \" Did you mean to set reuse=True in VarScope? \"\r\n    638                          \"Originally defined at:\\n\\n%s\" % (\r\n--> 639                              name, \"\".join(traceback.format_list(tb))))\r\n    640       found_var = self._vars[name]\r\n    641       if not shape.is_compatible_with(found_var.get_shape()):\r\n\r\nValueError: Variable bidirectional_rnn/fw/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\r\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\r\n  File \"/home/cesar/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 338, in __call__\r\n    scope=scope)\r\n  File \"<ipython-input-23-f4f28501e56f>\", line 24, in <module>\r\n    time_major=True\r\n"}