{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/799", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/799/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/799/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/799/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/799", "id": 127167468, "node_id": "MDU6SXNzdWUxMjcxNjc0Njg=", "number": 799, "title": "Got error when initialize bidirectional rnn with LSTM cell ", "user": {"login": "indiejoseph", "id": 577551, "node_id": "MDQ6VXNlcjU3NzU1MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/577551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/indiejoseph", "html_url": "https://github.com/indiejoseph", "followers_url": "https://api.github.com/users/indiejoseph/followers", "following_url": "https://api.github.com/users/indiejoseph/following{/other_user}", "gists_url": "https://api.github.com/users/indiejoseph/gists{/gist_id}", "starred_url": "https://api.github.com/users/indiejoseph/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/indiejoseph/subscriptions", "organizations_url": "https://api.github.com/users/indiejoseph/orgs", "repos_url": "https://api.github.com/users/indiejoseph/repos", "events_url": "https://api.github.com/users/indiejoseph/events{/privacy}", "received_events_url": "https://api.github.com/users/indiejoseph/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ludimagister", "id": 15710643, "node_id": "MDQ6VXNlcjE1NzEwNjQz", "avatar_url": "https://avatars3.githubusercontent.com/u/15710643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ludimagister", "html_url": "https://github.com/ludimagister", "followers_url": "https://api.github.com/users/ludimagister/followers", "following_url": "https://api.github.com/users/ludimagister/following{/other_user}", "gists_url": "https://api.github.com/users/ludimagister/gists{/gist_id}", "starred_url": "https://api.github.com/users/ludimagister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ludimagister/subscriptions", "organizations_url": "https://api.github.com/users/ludimagister/orgs", "repos_url": "https://api.github.com/users/ludimagister/repos", "events_url": "https://api.github.com/users/ludimagister/events{/privacy}", "received_events_url": "https://api.github.com/users/ludimagister/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ludimagister", "id": 15710643, "node_id": "MDQ6VXNlcjE1NzEwNjQz", "avatar_url": "https://avatars3.githubusercontent.com/u/15710643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ludimagister", "html_url": "https://github.com/ludimagister", "followers_url": "https://api.github.com/users/ludimagister/followers", "following_url": "https://api.github.com/users/ludimagister/following{/other_user}", "gists_url": "https://api.github.com/users/ludimagister/gists{/gist_id}", "starred_url": "https://api.github.com/users/ludimagister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ludimagister/subscriptions", "organizations_url": "https://api.github.com/users/ludimagister/orgs", "repos_url": "https://api.github.com/users/ludimagister/repos", "events_url": "https://api.github.com/users/ludimagister/events{/privacy}", "received_events_url": "https://api.github.com/users/ludimagister/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-01-18T07:13:56Z", "updated_at": "2017-05-10T13:58:29Z", "closed_at": "2016-01-19T17:15:58Z", "author_association": "NONE", "body_html": "<p>I want to build a bi-rnn model with tensorflow with LSTM cell, when i try to initialize bidirectional_rnn,<br>\nit gives: <code>ValueError: Over-sharing: Variable BiRNN_FW/RNN/BasicLSTMCell/Linear/Matrix already exists, disallowed. Did you mean to set reuse=True in VarScope?</code></p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.models.rnn import rnn, rnn_cell\nfrom tensorflow.python.ops.constant_op import constant\nimport numpy as np\n\nclass Model(object):\n    def __init__(self, batch_size, len_word, num_chars, dim_embed, dim_hidden):\n        self.batch_size = batch_size\n        self.dim_embed = dim_embed\n        self.dim_hidden = dim_hidden\n        self.num_chars = num_chars\n        self.len_word = len_word\n\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([num_chars, dim_embed], -0.1, 0.1), name='embedding')\n\n        self.W_emb = tf.Variable(tf.random_uniform([dim_hidden*2, dim_embed], -0.1, 0.1), name='W_emb')\n        self.b_emb = tf.Variable(tf.zeros([dim_embed]), name='b_emb')\n        self.lstm_fw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n        self.lstm_bw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n\n    def build_model(self):\n        inputs = tf.placeholder(tf.int32, [self.batch_size, self.len_word])\n        input_length = tf.placeholder(tf.int64, [self.batch_size])\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.batch_size, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.batch_size, tf.float32)\n\n        with tf.device(\"/cpu:0\"):\n            embedded_input = tf.nn.embedding_lookup(self.embedding, tf.transpose(inputs))\n\n        brnn_output = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(embedded_input),\n            sequence_length=input_length,\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n        )\n\n        pooled_output = tf.reduce_sum( tf.pack(brnn_output), 0 )\n        pooled_output = pooled_output / tf.expand_dims( tf.to_float(input_length) + 1e-6, 1)\n        final_emb = tf.nn.xw_plus_b(pooled_output, self.W_emb, self.b_emb)\n        final_emb = tf.nn.l2_normalize(final_emb, dim=1, epsilon=1e-7)\n\n        return final_emb\n</code></pre>", "body_text": "I want to build a bi-rnn model with tensorflow with LSTM cell, when i try to initialize bidirectional_rnn,\nit gives: ValueError: Over-sharing: Variable BiRNN_FW/RNN/BasicLSTMCell/Linear/Matrix already exists, disallowed. Did you mean to set reuse=True in VarScope?\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn, rnn_cell\nfrom tensorflow.python.ops.constant_op import constant\nimport numpy as np\n\nclass Model(object):\n    def __init__(self, batch_size, len_word, num_chars, dim_embed, dim_hidden):\n        self.batch_size = batch_size\n        self.dim_embed = dim_embed\n        self.dim_hidden = dim_hidden\n        self.num_chars = num_chars\n        self.len_word = len_word\n\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([num_chars, dim_embed], -0.1, 0.1), name='embedding')\n\n        self.W_emb = tf.Variable(tf.random_uniform([dim_hidden*2, dim_embed], -0.1, 0.1), name='W_emb')\n        self.b_emb = tf.Variable(tf.zeros([dim_embed]), name='b_emb')\n        self.lstm_fw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n        self.lstm_bw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n\n    def build_model(self):\n        inputs = tf.placeholder(tf.int32, [self.batch_size, self.len_word])\n        input_length = tf.placeholder(tf.int64, [self.batch_size])\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.batch_size, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.batch_size, tf.float32)\n\n        with tf.device(\"/cpu:0\"):\n            embedded_input = tf.nn.embedding_lookup(self.embedding, tf.transpose(inputs))\n\n        brnn_output = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(embedded_input),\n            sequence_length=input_length,\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n        )\n\n        pooled_output = tf.reduce_sum( tf.pack(brnn_output), 0 )\n        pooled_output = pooled_output / tf.expand_dims( tf.to_float(input_length) + 1e-6, 1)\n        final_emb = tf.nn.xw_plus_b(pooled_output, self.W_emb, self.b_emb)\n        final_emb = tf.nn.l2_normalize(final_emb, dim=1, epsilon=1e-7)\n\n        return final_emb", "body": "I want to build a bi-rnn model with tensorflow with LSTM cell, when i try to initialize bidirectional_rnn,\nit gives: `ValueError: Over-sharing: Variable BiRNN_FW/RNN/BasicLSTMCell/Linear/Matrix already exists, disallowed. Did you mean to set reuse=True in VarScope?`\n\n```\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn, rnn_cell\nfrom tensorflow.python.ops.constant_op import constant\nimport numpy as np\n\nclass Model(object):\n    def __init__(self, batch_size, len_word, num_chars, dim_embed, dim_hidden):\n        self.batch_size = batch_size\n        self.dim_embed = dim_embed\n        self.dim_hidden = dim_hidden\n        self.num_chars = num_chars\n        self.len_word = len_word\n\n        with tf.device(\"/cpu:0\"):\n            self.embedding = tf.Variable(tf.random_uniform([num_chars, dim_embed], -0.1, 0.1), name='embedding')\n\n        self.W_emb = tf.Variable(tf.random_uniform([dim_hidden*2, dim_embed], -0.1, 0.1), name='W_emb')\n        self.b_emb = tf.Variable(tf.zeros([dim_embed]), name='b_emb')\n        self.lstm_fw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n        self.lstm_bw_cell = rnn_cell.BasicLSTMCell(dim_hidden)\n\n    def build_model(self):\n        inputs = tf.placeholder(tf.int32, [self.batch_size, self.len_word])\n        input_length = tf.placeholder(tf.int64, [self.batch_size])\n        lstm_state_fw = self.lstm_fw_cell.zero_state(self.batch_size, tf.float32)\n        lstm_state_bw = self.lstm_bw_cell.zero_state(self.batch_size, tf.float32)\n\n        with tf.device(\"/cpu:0\"):\n            embedded_input = tf.nn.embedding_lookup(self.embedding, tf.transpose(inputs))\n\n        brnn_output = rnn.bidirectional_rnn(\n            self.lstm_fw_cell, self.lstm_bw_cell,\n            tf.unpack(embedded_input),\n            sequence_length=input_length,\n            initial_state_fw=lstm_state_fw,\n            initial_state_bw=lstm_state_bw,\n        )\n\n        pooled_output = tf.reduce_sum( tf.pack(brnn_output), 0 )\n        pooled_output = pooled_output / tf.expand_dims( tf.to_float(input_length) + 1e-6, 1)\n        final_emb = tf.nn.xw_plus_b(pooled_output, self.W_emb, self.b_emb)\n        final_emb = tf.nn.l2_normalize(final_emb, dim=1, epsilon=1e-7)\n\n        return final_emb\n```\n"}