{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21653", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21653/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21653/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21653/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21653", "id": 351139262, "node_id": "MDU6SXNzdWUzNTExMzkyNjI=", "number": 21653, "title": "Endless restarting of session when run distribute traning with tensorflow 1.8", "user": {"login": "SharkWater", "id": 37606610, "node_id": "MDQ6VXNlcjM3NjA2NjEw", "avatar_url": "https://avatars0.githubusercontent.com/u/37606610?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SharkWater", "html_url": "https://github.com/SharkWater", "followers_url": "https://api.github.com/users/SharkWater/followers", "following_url": "https://api.github.com/users/SharkWater/following{/other_user}", "gists_url": "https://api.github.com/users/SharkWater/gists{/gist_id}", "starred_url": "https://api.github.com/users/SharkWater/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SharkWater/subscriptions", "organizations_url": "https://api.github.com/users/SharkWater/orgs", "repos_url": "https://api.github.com/users/SharkWater/repos", "events_url": "https://api.github.com/users/SharkWater/events{/privacy}", "received_events_url": "https://api.github.com/users/SharkWater/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-16T10:06:39Z", "updated_at": "2018-11-11T18:40:52Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Red Hat 4.8.5-4</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: Not applicable</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: r1.8</li>\n<li><strong>Python version</strong>: 2.7.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.4</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.9.2</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>: M40 11448MiB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>Cleanup partition error: Unavailable: OS Error\n</code></pre>\n<h3>Descripe problem</h3>\n<p>Training the same model with tensorflow r1.4 is OK. With tensorflow r1.8, the session restarts without stop at the end of training. I have checked the traning log. Some workers went wrong when clearing partition. This error caused session restarting. But at this time, the other workers have alreadly stopped running. Thus, the restarting workers went wrong with master init error and start another session restarting progress.</p>\n<h3>Logs</h3>\n<pre><code>### End of Training\n[2018-08-16 14:54:54.279 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error\n[2018-08-16 14:54:54.282 tf_logging.py:126] [WARNING] An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error\n[2018-08-16 14:54:55.281 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error\n[2018-08-16 14:54:55.333 tf_logging.py:116] [INFO] Graph was finalized.\n[2018-08-16 14:54:55.404 tf_logging.py:116] [INFO] Restoring parameters from hdfs://...\n[2018-08-16 14:54:56.288 193 master.cc:284] [ERROR] Master init: Unavailable: OS Error\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:16.451 196 master.cc:284] [ERROR] Master init: Unavailable: OS Error\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:44.452 195 master.cc:284] [ERROR] Master init: Unavailable: OS Error\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-4\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not applicable\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): r1.8\nPython version: 2.7.5\nBazel version (if compiling from source): 0.5.4\nGCC/Compiler version (if compiling from source): 4.9.2\nCUDA/cuDNN version: 9.0\nGPU model and memory: M40 11448MiB\nExact command to reproduce:\n\nCleanup partition error: Unavailable: OS Error\n\nDescripe problem\nTraining the same model with tensorflow r1.4 is OK. With tensorflow r1.8, the session restarts without stop at the end of training. I have checked the traning log. Some workers went wrong when clearing partition. This error caused session restarting. But at this time, the other workers have alreadly stopped running. Thus, the restarting workers went wrong with master init error and start another session restarting progress.\nLogs\n### End of Training\n[2018-08-16 14:54:54.279 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error\n[2018-08-16 14:54:54.282 tf_logging.py:126] [WARNING] An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error\n[2018-08-16 14:54:55.281 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error\n[2018-08-16 14:54:55.333 tf_logging.py:116] [INFO] Graph was finalized.\n[2018-08-16 14:54:55.404 tf_logging.py:116] [INFO] Restoring parameters from hdfs://...\n[2018-08-16 14:54:56.288 193 master.cc:284] [ERROR] Master init: Unavailable: OS Error\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:16.451 196 master.cc:284] [ERROR] Master init: Unavailable: OS Error\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\n[2018-08-16 14:55:44.452 195 master.cc:284] [ERROR] Master init: Unavailable: OS Error\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-4\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not applicable\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.2\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: M40 11448MiB\r\n- **Exact command to reproduce**:\r\n```\r\nCleanup partition error: Unavailable: OS Error\r\n```\r\n\r\n### Descripe problem\r\nTraining the same model with tensorflow r1.4 is OK. With tensorflow r1.8, the session restarts without stop at the end of training. I have checked the traning log. Some workers went wrong when clearing partition. This error caused session restarting. But at this time, the other workers have alreadly stopped running. Thus, the restarting workers went wrong with master init error and start another session restarting progress.\r\n\r\n### Logs\r\n```\r\n### End of Training\r\n[2018-08-16 14:54:54.279 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error\r\n[2018-08-16 14:54:54.282 tf_logging.py:126] [WARNING] An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error\r\n[2018-08-16 14:54:55.281 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error\r\n[2018-08-16 14:54:55.333 tf_logging.py:116] [INFO] Graph was finalized.\r\n[2018-08-16 14:54:55.404 tf_logging.py:116] [INFO] Restoring parameters from hdfs://...\r\n[2018-08-16 14:54:56.288 193 master.cc:284] [ERROR] Master init: Unavailable: OS Error\r\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\r\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\r\n[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error\r\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\r\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\r\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4\r\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\r\n[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\r\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\r\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\r\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4\r\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\r\n[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\r\n[2018-08-16 14:55:16.451 196 master.cc:284] [ERROR] Master init: Unavailable: OS Error\r\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\r\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\r\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\r\n[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\r\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\r\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3\r\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\r\n[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\r\n[2018-08-16 14:55:44.452 195 master.cc:284] [ERROR] Master init: Unavailable: OS Error\r\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\r\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6\r\n[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7\r\n```\r\n"}