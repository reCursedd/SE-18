{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259842744", "html_url": "https://github.com/tensorflow/tensorflow/issues/5480#issuecomment-259842744", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5480", "id": 259842744, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTg0Mjc0NA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-10T23:52:22Z", "updated_at": "2016-11-10T23:52:22Z", "author_association": "MEMBER", "body_html": "<p>@civilmanxx : Similar to what was described in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"183877607\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5066\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5066/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/5066\">#5066</a> : TensorFlow will allocate all the memory across all the GPUs accessible to it. If you want to run independent TensorFlow programs, say one on each GPU, then as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3645581\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/flx42\">@flx42</a> mentioned, you probably want to do something like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Run myprogram.py on GPU 0</span>\n<span class=\"pl-k\">export</span> CUDA_VISIBLE_DEVICES=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>0<span class=\"pl-pds\">\"</span></span> \nmyprogram.py\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> And another instance on GPU1</span>\n<span class=\"pl-k\">export</span> CUDA_VISIBLE_DEVICES=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>\nmyprogram.py\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> And another instance on GPU2</span>\n<span class=\"pl-k\">export</span> CUDA_VISIBLE_DEVICES=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2<span class=\"pl-pds\">\"</span></span></pre></div>\n<p>Note that the <code>CUDA_VISIBLE_DEVICES</code> environment variable restricts what GPUs the TensorFlow runtime \"sees\", and in the setup above, you want to refer to the GPU as <code>gpu:0</code> in all the instances.</p>\n<p>Let us know if that works out for you, or if I misunderstood.</p>", "body_text": "@civilmanxx : Similar to what was described in #5066 : TensorFlow will allocate all the memory across all the GPUs accessible to it. If you want to run independent TensorFlow programs, say one on each GPU, then as @flx42 mentioned, you probably want to do something like this:\n# Run myprogram.py on GPU 0\nexport CUDA_VISIBLE_DEVICES=\"0\" \nmyprogram.py\n# And another instance on GPU1\nexport CUDA_VISIBLE_DEVICES=\"1\"\nmyprogram.py\n# And another instance on GPU2\nexport CUDA_VISIBLE_DEVICES=\"2\"\nNote that the CUDA_VISIBLE_DEVICES environment variable restricts what GPUs the TensorFlow runtime \"sees\", and in the setup above, you want to refer to the GPU as gpu:0 in all the instances.\nLet us know if that works out for you, or if I misunderstood.", "body": "@civilmanxx : Similar to what was described in #5066 : TensorFlow will allocate all the memory across all the GPUs accessible to it. If you want to run independent TensorFlow programs, say one on each GPU, then as @flx42 mentioned, you probably want to do something like this:\n\n``` sh\n# Run myprogram.py on GPU 0\nexport CUDA_VISIBLE_DEVICES=\"0\" \nmyprogram.py\n# And another instance on GPU1\nexport CUDA_VISIBLE_DEVICES=\"1\"\nmyprogram.py\n# And another instance on GPU2\nexport CUDA_VISIBLE_DEVICES=\"2\"\n```\n\nNote that the `CUDA_VISIBLE_DEVICES` environment variable restricts what GPUs the TensorFlow runtime \"sees\", and in the setup above, you want to refer to the GPU as `gpu:0` in all the instances.\n\nLet us know if that works out for you, or if I misunderstood.\n"}