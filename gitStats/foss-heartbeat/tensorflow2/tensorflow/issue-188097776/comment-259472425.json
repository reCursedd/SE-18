{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259472425", "html_url": "https://github.com/tensorflow/tensorflow/issues/5480#issuecomment-259472425", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5480", "id": 259472425, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTQ3MjQyNQ==", "user": {"login": "civilman628", "id": 8059551, "node_id": "MDQ6VXNlcjgwNTk1NTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/8059551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/civilman628", "html_url": "https://github.com/civilman628", "followers_url": "https://api.github.com/users/civilman628/followers", "following_url": "https://api.github.com/users/civilman628/following{/other_user}", "gists_url": "https://api.github.com/users/civilman628/gists{/gist_id}", "starred_url": "https://api.github.com/users/civilman628/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/civilman628/subscriptions", "organizations_url": "https://api.github.com/users/civilman628/orgs", "repos_url": "https://api.github.com/users/civilman628/repos", "events_url": "https://api.github.com/users/civilman628/events{/privacy}", "received_events_url": "https://api.github.com/users/civilman628/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-09T17:25:30Z", "updated_at": "2016-11-09T17:31:22Z", "author_association": "NONE", "body_html": "<p>let me clarify, out of memory at here is CUDA device out of memory. You mean all apps share GPU memory? each GeForce GTX TITAN X has 12GB GPU memory. Once an app start, it occupies all 4 GPU memory? 12GB * 4 = 48 GB. But my app is very small, just run Inception V1 for a single image feature extraction on 1 GPU (inference phase, but not training phase). 4 Apps do the same thing on 4 GPUs. I do not trigger them in multi thread way, but 4 independent processes and assign to 4 GPU separately.</p>", "body_text": "let me clarify, out of memory at here is CUDA device out of memory. You mean all apps share GPU memory? each GeForce GTX TITAN X has 12GB GPU memory. Once an app start, it occupies all 4 GPU memory? 12GB * 4 = 48 GB. But my app is very small, just run Inception V1 for a single image feature extraction on 1 GPU (inference phase, but not training phase). 4 Apps do the same thing on 4 GPUs. I do not trigger them in multi thread way, but 4 independent processes and assign to 4 GPU separately.", "body": "let me clarify, out of memory at here is CUDA device out of memory. You mean all apps share GPU memory? each GeForce GTX TITAN X has 12GB GPU memory. Once an app start, it occupies all 4 GPU memory? 12GB \\* 4 = 48 GB. But my app is very small, just run Inception V1 for a single image feature extraction on 1 GPU (inference phase, but not training phase). 4 Apps do the same thing on 4 GPUs. I do not trigger them in multi thread way, but 4 independent processes and assign to 4 GPU separately.\n"}