{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/397227152", "html_url": "https://github.com/tensorflow/tensorflow/issues/19969#issuecomment-397227152", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19969", "id": 397227152, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzIyNzE1Mg==", "user": {"login": "krishngithub", "id": 38810591, "node_id": "MDQ6VXNlcjM4ODEwNTkx", "avatar_url": "https://avatars1.githubusercontent.com/u/38810591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krishngithub", "html_url": "https://github.com/krishngithub", "followers_url": "https://api.github.com/users/krishngithub/followers", "following_url": "https://api.github.com/users/krishngithub/following{/other_user}", "gists_url": "https://api.github.com/users/krishngithub/gists{/gist_id}", "starred_url": "https://api.github.com/users/krishngithub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krishngithub/subscriptions", "organizations_url": "https://api.github.com/users/krishngithub/orgs", "repos_url": "https://api.github.com/users/krishngithub/repos", "events_url": "https://api.github.com/users/krishngithub/events{/privacy}", "received_events_url": "https://api.github.com/users/krishngithub/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T09:11:02Z", "updated_at": "2018-06-14T09:11:02Z", "author_association": "NONE", "body_html": "<p>have requirement on similar lines, at the inference time would like to have same(fixed) graph which to be executed for different run time inputs. Currently graph is getting loaded for each request which is over head and taking 1 sec more for every request</p>\n<p>for eg: we have below approx wrapper which will be called in by webserver via POST/GET at runtime.</p>\n<pre><code>service_wrapper(query):\n   _input_fn =Lambda: _populatedataset_fn(query)\n   return model.predict(_input_fn)\n</code></pre>\n<p>now we are looking for a way where we can expose this as a api in a cloud service which should avoid loading graph multiple times for each query request by the client</p>", "body_text": "have requirement on similar lines, at the inference time would like to have same(fixed) graph which to be executed for different run time inputs. Currently graph is getting loaded for each request which is over head and taking 1 sec more for every request\nfor eg: we have below approx wrapper which will be called in by webserver via POST/GET at runtime.\nservice_wrapper(query):\n   _input_fn =Lambda: _populatedataset_fn(query)\n   return model.predict(_input_fn)\n\nnow we are looking for a way where we can expose this as a api in a cloud service which should avoid loading graph multiple times for each query request by the client", "body": "have requirement on similar lines, at the inference time would like to have same(fixed) graph which to be executed for different run time inputs. Currently graph is getting loaded for each request which is over head and taking 1 sec more for every request\r\n\r\nfor eg: we have below approx wrapper which will be called in by webserver via POST/GET at runtime.\r\n```\r\nservice_wrapper(query):\r\n   _input_fn =Lambda: _populatedataset_fn(query)\r\n   return model.predict(_input_fn)\r\n```\r\nnow we are looking for a way where we can expose this as a api in a cloud service which should avoid loading graph multiple times for each query request by the client\r\n\r\n\r\n"}