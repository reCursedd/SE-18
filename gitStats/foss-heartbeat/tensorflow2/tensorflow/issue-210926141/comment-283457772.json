{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/283457772", "html_url": "https://github.com/tensorflow/tensorflow/issues/7952#issuecomment-283457772", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7952", "id": 283457772, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzQ1Nzc3Mg==", "user": {"login": "tianwang95", "id": 7917315, "node_id": "MDQ6VXNlcjc5MTczMTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/7917315?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tianwang95", "html_url": "https://github.com/tianwang95", "followers_url": "https://api.github.com/users/tianwang95/followers", "following_url": "https://api.github.com/users/tianwang95/following{/other_user}", "gists_url": "https://api.github.com/users/tianwang95/gists{/gist_id}", "starred_url": "https://api.github.com/users/tianwang95/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tianwang95/subscriptions", "organizations_url": "https://api.github.com/users/tianwang95/orgs", "repos_url": "https://api.github.com/users/tianwang95/repos", "events_url": "https://api.github.com/users/tianwang95/events{/privacy}", "received_events_url": "https://api.github.com/users/tianwang95/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-01T20:20:11Z", "updated_at": "2017-03-01T20:20:11Z", "author_association": "NONE", "body_html": "<p>The model I used is quite bulky and was originally developed in Keras (2 inputs -&gt; GRU -&gt; concat -&gt; 2 feed forward steps -&gt; sigmoid) so not sure if that will end up being helpful (although I will try and work on a toy model that reproduces the bug).</p>\n<p>However, this is the freeze_graph invocation I used:</p>\n<pre><code>    checkpoint_name = \"saved_checkpoint\"\n    checkpoint_prefix = os.path.join(temp_dir, checkpoint_name)\n    checkpoint_state_name = \"checkpoint_state\"\n    input_graph_name = \"input_graph.pb\"\n    output_graph_name = \"model.pb\"\n    input_graph_path = os.path.join(temp_dir, input_graph_name)\n    input_checkpoint_path = os.path.join(temp_dir, checkpoint_name)\n    output_graph_path = os.path.join(export_dir, output_graph_name)\n\n    #### Save the model to a checkpoint\n    sess = K.get_session()\n    saver = saver_lib.Saver()\n    checkpoint_path = saver.save(\n            sess,\n            checkpoint_prefix,\n            global_step=0,\n            latest_filename=checkpoint_state_name)\n\n    ### Save graph definition\n    tf.train.write_graph(sess.graph.as_graph_def(), temp_dir, input_graph_name)\n\n    ### Freeze graph\n    output_node_names = \"Sigmoid\"\n    restore_op_name = \"save/restore_all\"\n    filename_tensor_name = \"save/Const:0\"\n    input_saver_def_path = \"\"\n    input_binary = False\n    clear_devices = True\n\n    freeze_graph.freeze_graph(input_graph_path,\n                              input_saver_def_path,\n                              input_binary,\n                              checkpoint_path,\n                              output_node_names,\n                              restore_op_name,\n                              filename_tensor_name,\n                              output_graph_path,\n                              clear_devices,\n                              \"\")\n</code></pre>\n<p>I am exporting the graph from Keras, so it is possible that its an issue on their side, although the model does load fine afterwards with:</p>\n<pre><code>#### Test to make sure it has worked\n    graph = load_graph(output_graph_path)\n    q1_input = graph.get_tensor_by_name('import/q1_input:0')\n    q2_input = graph.get_tensor_by_name('import/q2_input:0')\n    y_tensor = graph.get_tensor_by_name('import/Sigmoid:0')\n\n    with tf.Session(graph = graph) as session:\n        acc = 0.0;\n        for point in data.dev_generator():\n            x1 = point[0][0]\n            x2 = point[0][1]\n            y = point[1]\n            y_out = session.run(y_tensor, feed_dict = {\n                    q1_input: x1,\n                    q2_input: x2,\n                })\n            acc += np.sum(y == np.round(y_out)) / 64.0\n        print(\"Final acc: {}\".format(acc / num))\n</code></pre>", "body_text": "The model I used is quite bulky and was originally developed in Keras (2 inputs -> GRU -> concat -> 2 feed forward steps -> sigmoid) so not sure if that will end up being helpful (although I will try and work on a toy model that reproduces the bug).\nHowever, this is the freeze_graph invocation I used:\n    checkpoint_name = \"saved_checkpoint\"\n    checkpoint_prefix = os.path.join(temp_dir, checkpoint_name)\n    checkpoint_state_name = \"checkpoint_state\"\n    input_graph_name = \"input_graph.pb\"\n    output_graph_name = \"model.pb\"\n    input_graph_path = os.path.join(temp_dir, input_graph_name)\n    input_checkpoint_path = os.path.join(temp_dir, checkpoint_name)\n    output_graph_path = os.path.join(export_dir, output_graph_name)\n\n    #### Save the model to a checkpoint\n    sess = K.get_session()\n    saver = saver_lib.Saver()\n    checkpoint_path = saver.save(\n            sess,\n            checkpoint_prefix,\n            global_step=0,\n            latest_filename=checkpoint_state_name)\n\n    ### Save graph definition\n    tf.train.write_graph(sess.graph.as_graph_def(), temp_dir, input_graph_name)\n\n    ### Freeze graph\n    output_node_names = \"Sigmoid\"\n    restore_op_name = \"save/restore_all\"\n    filename_tensor_name = \"save/Const:0\"\n    input_saver_def_path = \"\"\n    input_binary = False\n    clear_devices = True\n\n    freeze_graph.freeze_graph(input_graph_path,\n                              input_saver_def_path,\n                              input_binary,\n                              checkpoint_path,\n                              output_node_names,\n                              restore_op_name,\n                              filename_tensor_name,\n                              output_graph_path,\n                              clear_devices,\n                              \"\")\n\nI am exporting the graph from Keras, so it is possible that its an issue on their side, although the model does load fine afterwards with:\n#### Test to make sure it has worked\n    graph = load_graph(output_graph_path)\n    q1_input = graph.get_tensor_by_name('import/q1_input:0')\n    q2_input = graph.get_tensor_by_name('import/q2_input:0')\n    y_tensor = graph.get_tensor_by_name('import/Sigmoid:0')\n\n    with tf.Session(graph = graph) as session:\n        acc = 0.0;\n        for point in data.dev_generator():\n            x1 = point[0][0]\n            x2 = point[0][1]\n            y = point[1]\n            y_out = session.run(y_tensor, feed_dict = {\n                    q1_input: x1,\n                    q2_input: x2,\n                })\n            acc += np.sum(y == np.round(y_out)) / 64.0\n        print(\"Final acc: {}\".format(acc / num))", "body": "The model I used is quite bulky and was originally developed in Keras (2 inputs -> GRU -> concat -> 2 feed forward steps -> sigmoid) so not sure if that will end up being helpful (although I will try and work on a toy model that reproduces the bug). \r\n\r\nHowever, this is the freeze_graph invocation I used:\r\n\r\n```\r\n    checkpoint_name = \"saved_checkpoint\"\r\n    checkpoint_prefix = os.path.join(temp_dir, checkpoint_name)\r\n    checkpoint_state_name = \"checkpoint_state\"\r\n    input_graph_name = \"input_graph.pb\"\r\n    output_graph_name = \"model.pb\"\r\n    input_graph_path = os.path.join(temp_dir, input_graph_name)\r\n    input_checkpoint_path = os.path.join(temp_dir, checkpoint_name)\r\n    output_graph_path = os.path.join(export_dir, output_graph_name)\r\n\r\n    #### Save the model to a checkpoint\r\n    sess = K.get_session()\r\n    saver = saver_lib.Saver()\r\n    checkpoint_path = saver.save(\r\n            sess,\r\n            checkpoint_prefix,\r\n            global_step=0,\r\n            latest_filename=checkpoint_state_name)\r\n\r\n    ### Save graph definition\r\n    tf.train.write_graph(sess.graph.as_graph_def(), temp_dir, input_graph_name)\r\n\r\n    ### Freeze graph\r\n    output_node_names = \"Sigmoid\"\r\n    restore_op_name = \"save/restore_all\"\r\n    filename_tensor_name = \"save/Const:0\"\r\n    input_saver_def_path = \"\"\r\n    input_binary = False\r\n    clear_devices = True\r\n\r\n    freeze_graph.freeze_graph(input_graph_path,\r\n                              input_saver_def_path,\r\n                              input_binary,\r\n                              checkpoint_path,\r\n                              output_node_names,\r\n                              restore_op_name,\r\n                              filename_tensor_name,\r\n                              output_graph_path,\r\n                              clear_devices,\r\n                              \"\")\r\n```\r\nI am exporting the graph from Keras, so it is possible that its an issue on their side, although the model does load fine afterwards with:\r\n```\r\n#### Test to make sure it has worked\r\n    graph = load_graph(output_graph_path)\r\n    q1_input = graph.get_tensor_by_name('import/q1_input:0')\r\n    q2_input = graph.get_tensor_by_name('import/q2_input:0')\r\n    y_tensor = graph.get_tensor_by_name('import/Sigmoid:0')\r\n\r\n    with tf.Session(graph = graph) as session:\r\n        acc = 0.0;\r\n        for point in data.dev_generator():\r\n            x1 = point[0][0]\r\n            x2 = point[0][1]\r\n            y = point[1]\r\n            y_out = session.run(y_tensor, feed_dict = {\r\n                    q1_input: x1,\r\n                    q2_input: x2,\r\n                })\r\n            acc += np.sum(y == np.round(y_out)) / 64.0\r\n        print(\"Final acc: {}\".format(acc / num))\r\n```"}