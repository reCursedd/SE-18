{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5599", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5599/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5599/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5599/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/5599", "id": 189152805, "node_id": "MDExOlB1bGxSZXF1ZXN0OTM1OTUxMjE=", "number": 5599, "title": "Error in Recurrent Neural Networks tutorial", "user": {"login": "kevin-keraudren", "id": 802153, "node_id": "MDQ6VXNlcjgwMjE1Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/802153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kevin-keraudren", "html_url": "https://github.com/kevin-keraudren", "followers_url": "https://api.github.com/users/kevin-keraudren/followers", "following_url": "https://api.github.com/users/kevin-keraudren/following{/other_user}", "gists_url": "https://api.github.com/users/kevin-keraudren/gists{/gist_id}", "starred_url": "https://api.github.com/users/kevin-keraudren/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kevin-keraudren/subscriptions", "organizations_url": "https://api.github.com/users/kevin-keraudren/orgs", "repos_url": "https://api.github.com/users/kevin-keraudren/repos", "events_url": "https://api.github.com/users/kevin-keraudren/events{/privacy}", "received_events_url": "https://api.github.com/users/kevin-keraudren/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-11-14T16:08:47Z", "updated_at": "2017-04-13T00:25:54Z", "closed_at": "2016-11-17T14:05:10Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/5599", "html_url": "https://github.com/tensorflow/tensorflow/pull/5599", "diff_url": "https://github.com/tensorflow/tensorflow/pull/5599.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/5599.patch"}, "body_html": "<p>I was suprised to see that the two implementations of <code>char-rnn</code> in tensorflow I found on the web fall in the same Python trap:</p>\n<ul>\n<li><a href=\"https://github.com/carpedm20/lstm-char-cnn-tensorflow/blob/master/models/LSTMTDNN.py#L151\">https://github.com/carpedm20/lstm-char-cnn-tensorflow/blob/master/models/LSTMTDNN.py#L151</a></li>\n<li><a href=\"https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py#L25\">https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py#L25</a></li>\n</ul>\n<p>They use <code>MultiRNNCell</code> by copying the pointer to a unique cell instead of creating individual cell instances.</p>\n<p>This error can be illustrated with the following example:</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: a <span class=\"pl-k\">=</span> [[]]<span class=\"pl-k\">*</span><span class=\"pl-c1\">10</span>\n\nIn [<span class=\"pl-c1\">2</span>]: a[<span class=\"pl-c1\">0</span>].append(<span class=\"pl-c1\">3</span>)\n\nIn [<span class=\"pl-c1\">3</span>]: a\nOut[<span class=\"pl-c1\">3</span>]: [[<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>]]\n\nIn [<span class=\"pl-c1\">4</span>]: a <span class=\"pl-k\">=</span> [[] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>)]\n\nIn [<span class=\"pl-c1\">5</span>]: a[<span class=\"pl-c1\">0</span>].append(<span class=\"pl-c1\">3</span>)\n\nIn [<span class=\"pl-c1\">6</span>]: a\nOut[<span class=\"pl-c1\">6</span>]: [[<span class=\"pl-c1\">3</span>], [], [], [], [], [], [], [], [], []]</pre></div>\n<p>In the first case, we create 10 pointers to the same empty list, which is rarely what we want, while in the second case, we create 10 individual empty lists.</p>\n<p>If two independent persons are doing the same mistake, it might be that the documentation is wrong.</p>\n<p>In <a href=\"https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html</a> it is written:</p>\n<pre><code>To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n</code></pre>\n<p>However, the code is stacking pointers to the same LSTM cell. It is creating \"shared weights\" across those cells.</p>\n<p>You can notice that if you go back to the original Lua code cited in the example:<br>\n<a href=\"https://github.com/wojzaremba/lstm/blob/master/main.lua#L136\">https://github.com/wojzaremba/lstm/blob/master/main.lua#L136</a> and <a href=\"https://github.com/wojzaremba/lstm/blob/master/base.lua#L33\">https://github.com/wojzaremba/lstm/blob/master/base.lua#L33</a> , it talks about <code>cloning</code> and <code>avoiding pointers</code>.</p>\n<p>More interestingly, in the README of <a href=\"https://github.com/wojzaremba/lstm\">https://github.com/wojzaremba/lstm</a> , it speaks of achieving <code>115 perplexity for a small model in 1h</code>, which is not what the current code does (I obtained <code>116.983</code>), but is attained with the proposed fix (I got <code>115.063</code>), see <a href=\"https://gist.github.com/kevin-keraudren/f9607e281d9d75fee4000101f7e22a70#file-ptb_word_lm-txt\">https://gist.github.com/kevin-keraudren/f9607e281d9d75fee4000101f7e22a70#file-ptb_word_lm-txt</a> .</p>", "body_text": "I was suprised to see that the two implementations of char-rnn in tensorflow I found on the web fall in the same Python trap:\n\nhttps://github.com/carpedm20/lstm-char-cnn-tensorflow/blob/master/models/LSTMTDNN.py#L151\nhttps://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py#L25\n\nThey use MultiRNNCell by copying the pointer to a unique cell instead of creating individual cell instances.\nThis error can be illustrated with the following example:\nIn [1]: a = [[]]*10\n\nIn [2]: a[0].append(3)\n\nIn [3]: a\nOut[3]: [[3], [3], [3], [3], [3], [3], [3], [3], [3], [3]]\n\nIn [4]: a = [[] for i in range(10)]\n\nIn [5]: a[0].append(3)\n\nIn [6]: a\nOut[6]: [[3], [], [], [], [], [], [], [], [], []]\nIn the first case, we create 10 pointers to the same empty list, which is rarely what we want, while in the second case, we create 10 individual empty lists.\nIf two independent persons are doing the same mistake, it might be that the documentation is wrong.\nIn https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html it is written:\nTo give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n\nHowever, the code is stacking pointers to the same LSTM cell. It is creating \"shared weights\" across those cells.\nYou can notice that if you go back to the original Lua code cited in the example:\nhttps://github.com/wojzaremba/lstm/blob/master/main.lua#L136 and https://github.com/wojzaremba/lstm/blob/master/base.lua#L33 , it talks about cloning and avoiding pointers.\nMore interestingly, in the README of https://github.com/wojzaremba/lstm , it speaks of achieving 115 perplexity for a small model in 1h, which is not what the current code does (I obtained 116.983), but is attained with the proposed fix (I got 115.063), see https://gist.github.com/kevin-keraudren/f9607e281d9d75fee4000101f7e22a70#file-ptb_word_lm-txt .", "body": "I was suprised to see that the two implementations of `char-rnn` in tensorflow I found on the web fall in the same Python trap:\r\n - https://github.com/carpedm20/lstm-char-cnn-tensorflow/blob/master/models/LSTMTDNN.py#L151\r\n - https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py#L25\r\n\r\nThey use `MultiRNNCell` by copying the pointer to a unique cell instead of creating individual cell instances.\r\n\r\nThis error can be illustrated with the following example:\r\n\r\n```python\r\nIn [1]: a = [[]]*10\r\n\r\nIn [2]: a[0].append(3)\r\n\r\nIn [3]: a\r\nOut[3]: [[3], [3], [3], [3], [3], [3], [3], [3], [3], [3]]\r\n\r\nIn [4]: a = [[] for i in range(10)]\r\n\r\nIn [5]: a[0].append(3)\r\n\r\nIn [6]: a\r\nOut[6]: [[3], [], [], [], [], [], [], [], [], []]\r\n```\r\n\r\nIn the first case, we create 10 pointers to the same empty list, which is rarely what we want, while in the second case, we create 10 individual empty lists.\r\n\r\nIf two independent persons are doing the same mistake, it might be that the documentation is wrong.\r\n\r\nIn https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html it is written:\r\n```\r\nTo give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\r\n```\r\n\r\nHowever, the code is stacking pointers to the same LSTM cell. It is creating \"shared weights\" across those cells.\r\n\r\nYou can notice that if you go back to the original Lua code cited in the example:\r\nhttps://github.com/wojzaremba/lstm/blob/master/main.lua#L136 and https://github.com/wojzaremba/lstm/blob/master/base.lua#L33 , it talks about `cloning` and `avoiding pointers`.\r\n\r\nMore interestingly, in the README of https://github.com/wojzaremba/lstm , it speaks of achieving `115 perplexity for a small model in 1h`, which is not what the current code does (I obtained `116.983`), but is attained with the proposed fix (I got `115.063`), see https://gist.github.com/kevin-keraudren/f9607e281d9d75fee4000101f7e22a70#file-ptb_word_lm-txt ."}