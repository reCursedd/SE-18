{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8345", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8345/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8345/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8345/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8345", "id": 213716640, "node_id": "MDU6SXNzdWUyMTM3MTY2NDA=", "number": 8345, "title": "How to Add Adam optimizer metrics to Tensorboard?", "user": {"login": "Prakashvanapalli", "id": 11383400, "node_id": "MDQ6VXNlcjExMzgzNDAw", "avatar_url": "https://avatars0.githubusercontent.com/u/11383400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Prakashvanapalli", "html_url": "https://github.com/Prakashvanapalli", "followers_url": "https://api.github.com/users/Prakashvanapalli/followers", "following_url": "https://api.github.com/users/Prakashvanapalli/following{/other_user}", "gists_url": "https://api.github.com/users/Prakashvanapalli/gists{/gist_id}", "starred_url": "https://api.github.com/users/Prakashvanapalli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Prakashvanapalli/subscriptions", "organizations_url": "https://api.github.com/users/Prakashvanapalli/orgs", "repos_url": "https://api.github.com/users/Prakashvanapalli/repos", "events_url": "https://api.github.com/users/Prakashvanapalli/events{/privacy}", "received_events_url": "https://api.github.com/users/Prakashvanapalli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-13T09:46:35Z", "updated_at": "2017-04-08T07:09:20Z", "closed_at": "2017-04-08T07:09:20Z", "author_association": "NONE", "body_html": "<p>When I trained model for several epochs and want to retrain it again for more epochs. How would Adam optimizer work. will it initialize the time from t =0 or will it save the last time step?</p>\n<p>a) The documentation in tensorflow shows the following calculations. Is there a away I can add these metrics to tensorboard.</p>\n<pre><code>t &lt;- t + 1\nlr_t &lt;- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n\nm_t &lt;- beta1 * m_{t-1} + (1 - beta1) * g\nv_t &lt;- beta2 * v_{t-1} + (1 - beta2) * g * g\nvariable &lt;- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n</code></pre>\n<p>I know this question need to be asked in stackoverflow . but there are no answers for a few questions since a long time <a href=\"http://stackoverflow.com/questions/36990476/getting-the-current-learning-rate-from-a-tf-train-adamoptimizer\" rel=\"nofollow\">question1</a> and <a href=\"http://stackoverflow.com/questions/40752053/how-to-add-learning-rate-to-summaries\" rel=\"nofollow\">question2</a>.</p>\n<p>I am actually getting a problem with error rate when re-training the model from the last checkpoint and I was not sure what exactly is happening with Adam optimizer in this case ?</p>", "body_text": "When I trained model for several epochs and want to retrain it again for more epochs. How would Adam optimizer work. will it initialize the time from t =0 or will it save the last time step?\na) The documentation in tensorflow shows the following calculations. Is there a away I can add these metrics to tensorboard.\nt <- t + 1\nlr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n\nm_t <- beta1 * m_{t-1} + (1 - beta1) * g\nv_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\nvariable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n\nI know this question need to be asked in stackoverflow . but there are no answers for a few questions since a long time question1 and question2.\nI am actually getting a problem with error rate when re-training the model from the last checkpoint and I was not sure what exactly is happening with Adam optimizer in this case ?", "body": "When I trained model for several epochs and want to retrain it again for more epochs. How would Adam optimizer work. will it initialize the time from t =0 or will it save the last time step?  \r\n\r\na) The documentation in tensorflow shows the following calculations. Is there a away I can add these metrics to tensorboard. \r\n\r\n    t <- t + 1\r\n    lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\r\n\r\n    m_t <- beta1 * m_{t-1} + (1 - beta1) * g\r\n    v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\r\n    variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\r\n\r\nI know this question need to be asked in stackoverflow . but there are no answers for a few questions since a long time [question1](http://stackoverflow.com/questions/36990476/getting-the-current-learning-rate-from-a-tf-train-adamoptimizer) and [question2](http://stackoverflow.com/questions/40752053/how-to-add-learning-rate-to-summaries).\r\n\r\nI am actually getting a problem with error rate when re-training the model from the last checkpoint and I was not sure what exactly is happening with Adam optimizer in this case ?"}