{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280653046", "html_url": "https://github.com/tensorflow/tensorflow/issues/7456#issuecomment-280653046", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7456", "id": 280653046, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDY1MzA0Ng==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-17T13:45:26Z", "updated_at": "2017-02-17T13:45:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24216379\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/YiMX\">@YiMX</a> I think Paul is suggesting to do the following, instead of</p>\n<pre><code>with tf.device(\"gpu:1\"):\n  a =   gpu_0_tensor + b\n</code></pre>\n<p>to do this</p>\n<pre><code>with tf.device(\"gpu:1\"):\n  gpu_0_tensor_moved = gpu_0 + 0\n  a = gpu_0_tensor_moved + b\n</code></pre>\n<p>The <code>gpu_0_tensor_moved</code> is a copy of <code>gpu_0_tensor</code> created on <code>gpu1</code>, so that now <code>a+b</code> works on Tensors on the same device. Do this to enough tensors so that every other operation only works on tensors from the same GPU</p>", "body_text": "@YiMX I think Paul is suggesting to do the following, instead of\nwith tf.device(\"gpu:1\"):\n  a =   gpu_0_tensor + b\n\nto do this\nwith tf.device(\"gpu:1\"):\n  gpu_0_tensor_moved = gpu_0 + 0\n  a = gpu_0_tensor_moved + b\n\nThe gpu_0_tensor_moved is a copy of gpu_0_tensor created on gpu1, so that now a+b works on Tensors on the same device. Do this to enough tensors so that every other operation only works on tensors from the same GPU", "body": "@YiMX I think Paul is suggesting to do the following, instead of\r\n\r\n```\r\nwith tf.device(\"gpu:1\"):\r\n  a =   gpu_0_tensor + b\r\n```\r\nto do this\r\n\r\n```\r\nwith tf.device(\"gpu:1\"):\r\n  gpu_0_tensor_moved = gpu_0 + 0\r\n  a = gpu_0_tensor_moved + b\r\n```\r\n\r\nThe `gpu_0_tensor_moved` is a copy of `gpu_0_tensor` created on `gpu1`, so that now `a+b` works on Tensors on the same device. Do this to enough tensors so that every other operation only works on tensors from the same GPU"}