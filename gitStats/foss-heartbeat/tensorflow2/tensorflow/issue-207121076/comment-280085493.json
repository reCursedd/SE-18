{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280085493", "html_url": "https://github.com/tensorflow/tensorflow/issues/7456#issuecomment-280085493", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7456", "id": 280085493, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDA4NTQ5Mw==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-15T17:51:45Z", "updated_at": "2017-02-15T17:51:45Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a></p>\n<p>I have experienced a similar problem that I have not yet been able to fully diagnose.  Briefly, I've experienced NaNs on k80s and p100s that I suspect are due to simultaneous read/write operations on the same memory area resulting in corrupt values.  But I don't really have proof of this.  In my case, it occurs when using GPUDirect to DMA out of a GPU while the GPU is also doing compute.  My best understanding of these devices is that memory operations should be atomic at the cache line level: if DMA reads are asynchronous with writes, the read should get the value prior to the write or after, but never a mixture of before/after bits.  Since I'm seeing NaNs downstream in the gradient computations I guess in principle it's possible that they result from too much asynchrony in the computation (i.e. the computation sees legitimate values mixed from multiple phases, rather than corrupt values), but I'm doubtful.</p>\n<p>My working solution is to make a temporary local copy on the GPU of a value prior to DMA'ing it off device.  This causes the NaNs to go away, at the cost of extra memory use.</p>\n<p>According to the NVIDIA documentation I've seen, the two GPU dies on one k80 card \"should\" act like two separate GPUs, and communication between them still uses DMA via the PCI bus.  So, the differences you see may be due to timing rather than fundamentally different memory access behavior.</p>\n<p>You might try the temporary copy technique to see whether it avoids the problem.  I'd be interested in any further insights you gain into this problem.</p>", "body_text": "@zheng-xq\nI have experienced a similar problem that I have not yet been able to fully diagnose.  Briefly, I've experienced NaNs on k80s and p100s that I suspect are due to simultaneous read/write operations on the same memory area resulting in corrupt values.  But I don't really have proof of this.  In my case, it occurs when using GPUDirect to DMA out of a GPU while the GPU is also doing compute.  My best understanding of these devices is that memory operations should be atomic at the cache line level: if DMA reads are asynchronous with writes, the read should get the value prior to the write or after, but never a mixture of before/after bits.  Since I'm seeing NaNs downstream in the gradient computations I guess in principle it's possible that they result from too much asynchrony in the computation (i.e. the computation sees legitimate values mixed from multiple phases, rather than corrupt values), but I'm doubtful.\nMy working solution is to make a temporary local copy on the GPU of a value prior to DMA'ing it off device.  This causes the NaNs to go away, at the cost of extra memory use.\nAccording to the NVIDIA documentation I've seen, the two GPU dies on one k80 card \"should\" act like two separate GPUs, and communication between them still uses DMA via the PCI bus.  So, the differences you see may be due to timing rather than fundamentally different memory access behavior.\nYou might try the temporary copy technique to see whether it avoids the problem.  I'd be interested in any further insights you gain into this problem.", "body": "@zheng-xq \r\n\r\nI have experienced a similar problem that I have not yet been able to fully diagnose.  Briefly, I've experienced NaNs on k80s and p100s that I suspect are due to simultaneous read/write operations on the same memory area resulting in corrupt values.  But I don't really have proof of this.  In my case, it occurs when using GPUDirect to DMA out of a GPU while the GPU is also doing compute.  My best understanding of these devices is that memory operations should be atomic at the cache line level: if DMA reads are asynchronous with writes, the read should get the value prior to the write or after, but never a mixture of before/after bits.  Since I'm seeing NaNs downstream in the gradient computations I guess in principle it's possible that they result from too much asynchrony in the computation (i.e. the computation sees legitimate values mixed from multiple phases, rather than corrupt values), but I'm doubtful.\r\n\r\nMy working solution is to make a temporary local copy on the GPU of a value prior to DMA'ing it off device.  This causes the NaNs to go away, at the cost of extra memory use. \r\n\r\nAccording to the NVIDIA documentation I've seen, the two GPU dies on one k80 card \"should\" act like two separate GPUs, and communication between them still uses DMA via the PCI bus.  So, the differences you see may be due to timing rather than fundamentally different memory access behavior.\r\n\r\nYou might try the temporary copy technique to see whether it avoids the problem.  I'd be interested in any further insights you gain into this problem."}