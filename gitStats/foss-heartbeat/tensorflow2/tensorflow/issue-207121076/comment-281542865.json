{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281542865", "html_url": "https://github.com/tensorflow/tensorflow/issues/7456#issuecomment-281542865", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7456", "id": 281542865, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTU0Mjg2NQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-22T01:51:46Z", "updated_at": "2017-02-22T01:51:46Z", "author_association": "MEMBER", "body_html": "<p>I think you've made explicit what the original code did, but haven't introduced any new temporary local copies.  What I'm suggesting is intrusive to the code, to the degree that values go back and forth between devices.</p>\n<p>My apologies if I don't understand your example well, I rarely work with TF at this level.  I think 'tower_grads' is located on the CPU, and a separate instance of 'grads' is located on each GPU.<br>\nThe line<br>\n<code>tower_grads.append(grads)</code><br>\nis going to force an inter-device copy from each GPU to the CPU.  Try substituting</p>\n<pre><code>      grads_copy = grads + 0.0\n      tower_grads.append(grads_copy)\n</code></pre>\n<p>I think the average_gradients function is computing entirely on the CPU, if I'm wrong, you'll need some tmp copies in there too.</p>\n<p>Frankly, it doesn't look likely that this is going to solve your problem.  I'd be more hopeful if the source of the inter-device copy were a Var.  Is is possible that opt.compute_gradients() or opt.apply_gradients is hiding some inter-device copies?</p>", "body_text": "I think you've made explicit what the original code did, but haven't introduced any new temporary local copies.  What I'm suggesting is intrusive to the code, to the degree that values go back and forth between devices.\nMy apologies if I don't understand your example well, I rarely work with TF at this level.  I think 'tower_grads' is located on the CPU, and a separate instance of 'grads' is located on each GPU.\nThe line\ntower_grads.append(grads)\nis going to force an inter-device copy from each GPU to the CPU.  Try substituting\n      grads_copy = grads + 0.0\n      tower_grads.append(grads_copy)\n\nI think the average_gradients function is computing entirely on the CPU, if I'm wrong, you'll need some tmp copies in there too.\nFrankly, it doesn't look likely that this is going to solve your problem.  I'd be more hopeful if the source of the inter-device copy were a Var.  Is is possible that opt.compute_gradients() or opt.apply_gradients is hiding some inter-device copies?", "body": "I think you've made explicit what the original code did, but haven't introduced any new temporary local copies.  What I'm suggesting is intrusive to the code, to the degree that values go back and forth between devices.\r\n\r\nMy apologies if I don't understand your example well, I rarely work with TF at this level.  I think 'tower_grads' is located on the CPU, and a separate instance of 'grads' is located on each GPU.\r\nThe line\r\n   ```tower_grads.append(grads)```\r\nis going to force an inter-device copy from each GPU to the CPU.  Try substituting\r\n ```\r\n       grads_copy = grads + 0.0\r\n       tower_grads.append(grads_copy)\r\n```\r\n\r\nI think the average_gradients function is computing entirely on the CPU, if I'm wrong, you'll need some tmp copies in there too.  \r\n\r\nFrankly, it doesn't look likely that this is going to solve your problem.  I'd be more hopeful if the source of the inter-device copy were a Var.  Is is possible that opt.compute_gradients() or opt.apply_gradients is hiding some inter-device copies?"}