{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280030405", "html_url": "https://github.com/tensorflow/tensorflow/issues/7456#issuecomment-280030405", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7456", "id": 280030405, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDAzMDQwNQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-15T14:49:19Z", "updated_at": "2017-02-15T14:49:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>One possibility is that there's something happening with variable sync. IE, if TensorFlow is reading from GPU variable while another op is writing to it at the same time. Normally, in such situation, the variable would be partially updated, but updates do not cross boundaries of individual components. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a> is there an easy way to rule out GPU variable read reading a scalar in the middle of DMA write happening to it?</p>", "body_text": "One possibility is that there's something happening with variable sync. IE, if TensorFlow is reading from GPU variable while another op is writing to it at the same time. Normally, in such situation, the variable would be partially updated, but updates do not cross boundaries of individual components. @poxvoculi is there an easy way to rule out GPU variable read reading a scalar in the middle of DMA write happening to it?", "body": "One possibility is that there's something happening with variable sync. IE, if TensorFlow is reading from GPU variable while another op is writing to it at the same time. Normally, in such situation, the variable would be partially updated, but updates do not cross boundaries of individual components. @poxvoculi is there an easy way to rule out GPU variable read reading a scalar in the middle of DMA write happening to it?"}