{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321453163", "html_url": "https://github.com/tensorflow/tensorflow/issues/7456#issuecomment-321453163", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7456", "id": 321453163, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTQ1MzE2Mw==", "user": {"login": "scenarios", "id": 15104876, "node_id": "MDQ6VXNlcjE1MTA0ODc2", "avatar_url": "https://avatars2.githubusercontent.com/u/15104876?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scenarios", "html_url": "https://github.com/scenarios", "followers_url": "https://api.github.com/users/scenarios/followers", "following_url": "https://api.github.com/users/scenarios/following{/other_user}", "gists_url": "https://api.github.com/users/scenarios/gists{/gist_id}", "starred_url": "https://api.github.com/users/scenarios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scenarios/subscriptions", "organizations_url": "https://api.github.com/users/scenarios/orgs", "repos_url": "https://api.github.com/users/scenarios/repos", "events_url": "https://api.github.com/users/scenarios/events{/privacy}", "received_events_url": "https://api.github.com/users/scenarios/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-10T05:18:02Z", "updated_at": "2017-08-10T05:41:35Z", "author_association": "NONE", "body_html": "<p>Any update? I've been struggling with this problem for a while. I have 8 M40 cards and the problem is very similar with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24216379\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/YiMX\">@YiMX</a>. Everything works well on single card but diverging when using multiple cards together. The most weird part is that when I allocate computation on two different gpus between which the peer to peer access is not supported, everything works well again. For example, it works well on gpu0 with gpu4 or 5 or 6, but fails on gpu0 with gpu1 or 2 or 3.</p>\n<p>The peer to peer matrix<br>\nDMA<br>\n~~0 1 2 3 4 5 6 7<br>\n0:   Y Y Y Y N N N N<br>\n1:   Y Y Y Y N N N N<br>\n2:   Y Y Y Y N N N N<br>\n3:   Y Y Y Y N N N N<br>\n4:   N N N N Y Y Y Y<br>\n5:   N N N N Y Y Y Y<br>\n6:   N N N N Y Y Y Y<br>\n7:   N N N N Y Y Y Y</p>", "body_text": "Any update? I've been struggling with this problem for a while. I have 8 M40 cards and the problem is very similar with @YiMX. Everything works well on single card but diverging when using multiple cards together. The most weird part is that when I allocate computation on two different gpus between which the peer to peer access is not supported, everything works well again. For example, it works well on gpu0 with gpu4 or 5 or 6, but fails on gpu0 with gpu1 or 2 or 3.\nThe peer to peer matrix\nDMA\n~~0 1 2 3 4 5 6 7\n0:   Y Y Y Y N N N N\n1:   Y Y Y Y N N N N\n2:   Y Y Y Y N N N N\n3:   Y Y Y Y N N N N\n4:   N N N N Y Y Y Y\n5:   N N N N Y Y Y Y\n6:   N N N N Y Y Y Y\n7:   N N N N Y Y Y Y", "body": "Any update? I've been struggling with this problem for a while. I have 8 M40 cards and the problem is very similar with @YiMX. Everything works well on single card but diverging when using multiple cards together. The most weird part is that when I allocate computation on two different gpus between which the peer to peer access is not supported, everything works well again. For example, it works well on gpu0 with gpu4 or 5 or 6, but fails on gpu0 with gpu1 or 2 or 3.\r\n\r\nThe peer to peer matrix \r\nDMA  \r\n~~0 1 2 3 4 5 6 7 \r\n    0:   Y Y Y Y N N N N \r\n    1:   Y Y Y Y N N N N \r\n    2:   Y Y Y Y N N N N \r\n    3:   Y Y Y Y N N N N \r\n    4:   N N N N Y Y Y Y \r\n    5:   N N N N Y Y Y Y \r\n    6:   N N N N Y Y Y Y \r\n    7:   N N N N Y Y Y Y \r\n"}