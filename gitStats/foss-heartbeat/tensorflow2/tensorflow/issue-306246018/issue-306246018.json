{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17811", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17811/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17811/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17811/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17811", "id": 306246018, "node_id": "MDU6SXNzdWUzMDYyNDYwMTg=", "number": 17811, "title": "Make feature column input_layer compatible with sequential data", "user": {"login": "philippnormann1337", "id": 2767025, "node_id": "MDQ6VXNlcjI3NjcwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2767025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philippnormann1337", "html_url": "https://github.com/philippnormann1337", "followers_url": "https://api.github.com/users/philippnormann1337/followers", "following_url": "https://api.github.com/users/philippnormann1337/following{/other_user}", "gists_url": "https://api.github.com/users/philippnormann1337/gists{/gist_id}", "starred_url": "https://api.github.com/users/philippnormann1337/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philippnormann1337/subscriptions", "organizations_url": "https://api.github.com/users/philippnormann1337/orgs", "repos_url": "https://api.github.com/users/philippnormann1337/repos", "events_url": "https://api.github.com/users/philippnormann1337/events{/privacy}", "received_events_url": "https://api.github.com/users/philippnormann1337/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2018-03-18T14:41:54Z", "updated_at": "2018-11-13T17:03:58Z", "closed_at": "2018-06-21T12:21:54Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Debian Buster/Sid</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.7.0-rc0</li>\n<li><strong>Python version</strong>: 3.6.5rc1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: See source code below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Feature columns offer easy and reproducible feature encodings. It would be nice if they could be made compatible with sequential data as well. I currently would like to use them for processing inputs in a custom RNN estimator and sadly discovered that the current <a href=\"https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer\" rel=\"nofollow\"><code>input_layer</code></a> API doesn't support sequential inputs. It always returns a Tensor shaped <code>(batch_size, first_layer_dimension)</code>, which makes it unusable in combination with the<a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\" rel=\"nofollow\"> <code>dynamic_rnn</code></a> wrapper which expects inputs of shape <code>(batch_size, max_time, first_layer_dimension)</code></p>\n<h3>Source code / logs</h3>\n<p>My first attempt at a workaround for this shortcoming, was mapping the <code>input_layer</code> function across the sequences of the inputs.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">params</span>, <span class=\"pl-smi\">config</span>):\n    sequence_lengths <span class=\"pl-k\">=</span> features.pop(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>session_length<span class=\"pl-pds\">'</span></span>)\n    rnn_cell, state_size, initializer <span class=\"pl-k\">=</span> params[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn_cell<span class=\"pl-pds\">'</span></span>].values()\n\n    encoded_features <span class=\"pl-k\">=</span> tf.map_fn(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">input_features</span>: tf.feature_column.input_layer(input_features, params[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>feature_columns<span class=\"pl-pds\">'</span></span>]), features, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    encoded_labels <span class=\"pl-k\">=</span> tf.tile(tf.map_fn(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">seqeunce_labels</span>: tf.feature_column.input_layer(seqeunce_labels, params[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>label_columns<span class=\"pl-pds\">'</span></span>]), labels, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32), sequence_lengths)\n\n    cell <span class=\"pl-k\">=</span> rnn_cell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>state_size, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>initializer())\n    outputs, state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(<span class=\"pl-v\">cell</span><span class=\"pl-k\">=</span>cell, <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>encoded_features, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_lengths)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(outputs, <span class=\"pl-v\">num_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n    predictions <span class=\"pl-k\">=</span> tf.nn.softmax(logits)\n    \n    loss <span class=\"pl-k\">=</span> tf.losses.softmax_cross_entropy(encoded_labels, logits)\n    optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>params[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>learning_rate<span class=\"pl-pds\">\"</span></span>])\n    train_op <span class=\"pl-k\">=</span> optimizer.minimize(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_global_step())\n    \n    <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(mode, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions, <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss)</pre></div>\n<p>Due to some kind of frame error this sadly is also not possible. Maybe I am missing something here and any tips on why this is failing would be highly appreciated.</p>\n<div class=\"highlight highlight-source-shell\"><pre>InvalidArgumentError: The node <span class=\"pl-s\"><span class=\"pl-pds\">'</span>group_deps_1<span class=\"pl-pds\">'</span></span> has inputs from different frames. \nThe input <span class=\"pl-s\"><span class=\"pl-pds\">'</span>map/while/input_layer/url_indicator/url_lookup/hash_table/table_init<span class=\"pl-pds\">'</span></span> is <span class=\"pl-k\">in</span> frame <span class=\"pl-s\"><span class=\"pl-pds\">'</span>map/while/while_context<span class=\"pl-pds\">'</span></span>. \nThe input <span class=\"pl-s\"><span class=\"pl-pds\">'</span>map_1/while/input_layer/label_indicator/label_lookup/hash_table/table_init<span class=\"pl-pds\">'</span></span> is <span class=\"pl-k\">in</span> frame <span class=\"pl-s\"><span class=\"pl-pds\">'</span>map_1/while/while_context<span class=\"pl-pds\">'</span></span>.<span class=\"pl-s\"><span class=\"pl-pds\">`</span></span></pre></div>\n<p><strong>In my opinion a native (higher-level) solution to this problem is needed nonetheless!</strong></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Buster/Sid\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.7.0-rc0\nPython version: 3.6.5rc1\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See source code below\n\nDescribe the problem\nFeature columns offer easy and reproducible feature encodings. It would be nice if they could be made compatible with sequential data as well. I currently would like to use them for processing inputs in a custom RNN estimator and sadly discovered that the current input_layer API doesn't support sequential inputs. It always returns a Tensor shaped (batch_size, first_layer_dimension), which makes it unusable in combination with the dynamic_rnn wrapper which expects inputs of shape (batch_size, max_time, first_layer_dimension)\nSource code / logs\nMy first attempt at a workaround for this shortcoming, was mapping the input_layer function across the sequences of the inputs.\ndef model_fn(features, labels, mode, params, config):\n    sequence_lengths = features.pop('session_length')\n    rnn_cell, state_size, initializer = params['rnn_cell'].values()\n\n    encoded_features = tf.map_fn(lambda input_features: tf.feature_column.input_layer(input_features, params['feature_columns']), features, dtype=tf.float32)\n    encoded_labels = tf.tile(tf.map_fn(lambda seqeunce_labels: tf.feature_column.input_layer(seqeunce_labels, params['label_columns']), labels, dtype=tf.float32), sequence_lengths)\n\n    cell = rnn_cell(num_units=state_size, initializer=initializer())\n    outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=encoded_features, dtype=tf.float32, sequence_length=sequence_lengths)\n    logits = tf.contrib.layers.fully_connected(outputs, num_outputs=2, activation_fn=None)\n    predictions = tf.nn.softmax(logits)\n    \n    loss = tf.losses.softmax_cross_entropy(encoded_labels, logits)\n    optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n    \n    return tf.estimator.EstimatorSpec(mode, predictions=predictions, train_op=train_op, loss=loss)\nDue to some kind of frame error this sadly is also not possible. Maybe I am missing something here and any tips on why this is failing would be highly appreciated.\nInvalidArgumentError: The node 'group_deps_1' has inputs from different frames. \nThe input 'map/while/input_layer/url_indicator/url_lookup/hash_table/table_init' is in frame 'map/while/while_context'. \nThe input 'map_1/while/input_layer/label_indicator/label_lookup/hash_table/table_init' is in frame 'map_1/while/while_context'.`\nIn my opinion a native (higher-level) solution to this problem is needed nonetheless!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian Buster/Sid\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0-rc0\r\n- **Python version**: 3.6.5rc1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See source code below\r\n\r\n### Describe the problem\r\n\r\nFeature columns offer easy and reproducible feature encodings. It would be nice if they could be made compatible with sequential data as well. I currently would like to use them for processing inputs in a custom RNN estimator and sadly discovered that the current [`input_layer`](https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer) API doesn't support sequential inputs. It always returns a Tensor shaped `(batch_size, first_layer_dimension)`, which makes it unusable in combination with the[ `dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) wrapper which expects inputs of shape `(batch_size, max_time, first_layer_dimension)`\r\n\r\n### Source code / logs\r\nMy first attempt at a workaround for this shortcoming, was mapping the `input_layer` function across the sequences of the inputs. \r\n```python\r\ndef model_fn(features, labels, mode, params, config):\r\n    sequence_lengths = features.pop('session_length')\r\n    rnn_cell, state_size, initializer = params['rnn_cell'].values()\r\n\r\n    encoded_features = tf.map_fn(lambda input_features: tf.feature_column.input_layer(input_features, params['feature_columns']), features, dtype=tf.float32)\r\n    encoded_labels = tf.tile(tf.map_fn(lambda seqeunce_labels: tf.feature_column.input_layer(seqeunce_labels, params['label_columns']), labels, dtype=tf.float32), sequence_lengths)\r\n\r\n    cell = rnn_cell(num_units=state_size, initializer=initializer())\r\n    outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=encoded_features, dtype=tf.float32, sequence_length=sequence_lengths)\r\n    logits = tf.contrib.layers.fully_connected(outputs, num_outputs=2, activation_fn=None)\r\n    predictions = tf.nn.softmax(logits)\r\n    \r\n    loss = tf.losses.softmax_cross_entropy(encoded_labels, logits)\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n    \r\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions, train_op=train_op, loss=loss)\r\n```\r\nDue to some kind of frame error this sadly is also not possible. Maybe I am missing something here and any tips on why this is failing would be highly appreciated.\r\n```bash\r\nInvalidArgumentError: The node 'group_deps_1' has inputs from different frames. \r\nThe input 'map/while/input_layer/url_indicator/url_lookup/hash_table/table_init' is in frame 'map/while/while_context'. \r\nThe input 'map_1/while/input_layer/label_indicator/label_lookup/hash_table/table_init' is in frame 'map_1/while/while_context'.`\r\n```\r\n**In my opinion a native (higher-level) solution to this problem is needed nonetheless!**"}