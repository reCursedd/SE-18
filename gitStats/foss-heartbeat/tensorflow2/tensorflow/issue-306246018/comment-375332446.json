{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/375332446", "html_url": "https://github.com/tensorflow/tensorflow/issues/17811#issuecomment-375332446", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17811", "id": 375332446, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTMzMjQ0Ng==", "user": {"login": "philippnormann1337", "id": 2767025, "node_id": "MDQ6VXNlcjI3NjcwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2767025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philippnormann1337", "html_url": "https://github.com/philippnormann1337", "followers_url": "https://api.github.com/users/philippnormann1337/followers", "following_url": "https://api.github.com/users/philippnormann1337/following{/other_user}", "gists_url": "https://api.github.com/users/philippnormann1337/gists{/gist_id}", "starred_url": "https://api.github.com/users/philippnormann1337/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philippnormann1337/subscriptions", "organizations_url": "https://api.github.com/users/philippnormann1337/orgs", "repos_url": "https://api.github.com/users/philippnormann1337/repos", "events_url": "https://api.github.com/users/philippnormann1337/events{/privacy}", "received_events_url": "https://api.github.com/users/philippnormann1337/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-22T14:46:45Z", "updated_at": "2018-03-22T20:06:20Z", "author_association": "NONE", "body_html": "<p>I found a way to apply the feature columns to sequences by reshaping them into a non-sequential batch, applying the <code>input_layer</code> function and unpacking it into sequences afterwards.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">unpack_batch</span>(<span class=\"pl-smi\">packed_batch</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">max_len</span>):\n    <span class=\"pl-k\">return</span> {feature_name: tf.reshape(feature_tensor, [batch_size <span class=\"pl-k\">*</span> max_len]) <span class=\"pl-k\">for</span> feature_name, feature_tensor <span class=\"pl-k\">in</span> packed_batch.items()}\n  \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">pack_batch</span>(<span class=\"pl-smi\">unpacked_batch</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">max_len</span>, <span class=\"pl-smi\">first_layer_dimension</span>):\n    <span class=\"pl-k\">return</span> tf.reshape(unpacked_batch, [batch_size, max_len, first_layer_dimension])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">apply_feature_columns_to_sequences</span>(<span class=\"pl-smi\">feature_dict</span>, <span class=\"pl-smi\">feature_columns</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">max_len</span>, <span class=\"pl-smi\">first_layer_dimension</span>):\n    unpacked_feature_dict <span class=\"pl-k\">=</span> unpack_batch(feature_dict, batch_size, max_len)\n    packed_features <span class=\"pl-k\">=</span> pack_batch(tf.feature_column.input_layer(unpacked_feature_dict, feature_columns), batch_size, max_len, first_layer_dimension)\n    <span class=\"pl-k\">return</span> packed_features</pre></div>\n<p>Even though this works for now, I still see the need for a higher level solution to this problem.</p>", "body_text": "I found a way to apply the feature columns to sequences by reshaping them into a non-sequential batch, applying the input_layer function and unpacking it into sequences afterwards.\ndef unpack_batch(packed_batch, batch_size, max_len):\n    return {feature_name: tf.reshape(feature_tensor, [batch_size * max_len]) for feature_name, feature_tensor in packed_batch.items()}\n  \ndef pack_batch(unpacked_batch, batch_size, max_len, first_layer_dimension):\n    return tf.reshape(unpacked_batch, [batch_size, max_len, first_layer_dimension])\n\ndef apply_feature_columns_to_sequences(feature_dict, feature_columns, batch_size, max_len, first_layer_dimension):\n    unpacked_feature_dict = unpack_batch(feature_dict, batch_size, max_len)\n    packed_features = pack_batch(tf.feature_column.input_layer(unpacked_feature_dict, feature_columns), batch_size, max_len, first_layer_dimension)\n    return packed_features\nEven though this works for now, I still see the need for a higher level solution to this problem.", "body": "I found a way to apply the feature columns to sequences by reshaping them into a non-sequential batch, applying the `input_layer` function and unpacking it into sequences afterwards.\r\n```python\r\ndef unpack_batch(packed_batch, batch_size, max_len):\r\n    return {feature_name: tf.reshape(feature_tensor, [batch_size * max_len]) for feature_name, feature_tensor in packed_batch.items()}\r\n  \r\ndef pack_batch(unpacked_batch, batch_size, max_len, first_layer_dimension):\r\n    return tf.reshape(unpacked_batch, [batch_size, max_len, first_layer_dimension])\r\n\r\ndef apply_feature_columns_to_sequences(feature_dict, feature_columns, batch_size, max_len, first_layer_dimension):\r\n    unpacked_feature_dict = unpack_batch(feature_dict, batch_size, max_len)\r\n    packed_features = pack_batch(tf.feature_column.input_layer(unpacked_feature_dict, feature_columns), batch_size, max_len, first_layer_dimension)\r\n    return packed_features\r\n```\r\nEven though this works for now, I still see the need for a higher level solution to this problem."}