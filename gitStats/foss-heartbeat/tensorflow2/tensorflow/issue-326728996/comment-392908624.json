{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/392908624", "html_url": "https://github.com/tensorflow/tensorflow/issues/19571#issuecomment-392908624", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19571", "id": 392908624, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjkwODYyNA==", "user": {"login": "mirekphd", "id": 36706320, "node_id": "MDQ6VXNlcjM2NzA2MzIw", "avatar_url": "https://avatars3.githubusercontent.com/u/36706320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mirekphd", "html_url": "https://github.com/mirekphd", "followers_url": "https://api.github.com/users/mirekphd/followers", "following_url": "https://api.github.com/users/mirekphd/following{/other_user}", "gists_url": "https://api.github.com/users/mirekphd/gists{/gist_id}", "starred_url": "https://api.github.com/users/mirekphd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mirekphd/subscriptions", "organizations_url": "https://api.github.com/users/mirekphd/orgs", "repos_url": "https://api.github.com/users/mirekphd/repos", "events_url": "https://api.github.com/users/mirekphd/events{/privacy}", "received_events_url": "https://api.github.com/users/mirekphd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-29T19:19:52Z", "updated_at": "2018-05-29T19:28:27Z", "author_association": "NONE", "body_html": "<p>It is a duplicate with a twist. I'm not calling for lower utilization of GPU - it is perfectly fine to claim all of it, if there is an option to limit the usage to a certain percentage (per_process_gpu_memory_fraction). What's useful in catboost's approach is the idea to deallocate GPU memory after returning results from the GPU back to CPU. BTW you have already something in common with catboost: you are the only two GPU-enabled frameworks (among the 12 or so I've tested for our containers at work) that give such control over GPU memory usage to the final user - something that even docker is not allowing us to do.</p>\n<p>I noticed that it is customary among deep learning frameworks to create such GPU memory leaks (which it is) - none of them release GPU memory, requiring users to restart python kernel to release memory. Incidentally, none apart from Tensorflow claims nearly all of it, but that's what's so special about you:). In contrast to DNNs, GBDT frameworks tend to release what they no longer need - that applies not only to catboost but also to lightgbm.</p>\n<p>IMO it would be good to contact catboost's GPU guy, Vasily (<a href=\"https://github.com/Noxoomo\">Noxomo</a>) from Yandex team, because catboost manages to not only allocate 95% of GPU memory but also to use 100% of GPU compute power as well (despite sequential nature of boosting itself), which I've seen only in one DNN - the sadly neglected Theano...</p>", "body_text": "It is a duplicate with a twist. I'm not calling for lower utilization of GPU - it is perfectly fine to claim all of it, if there is an option to limit the usage to a certain percentage (per_process_gpu_memory_fraction). What's useful in catboost's approach is the idea to deallocate GPU memory after returning results from the GPU back to CPU. BTW you have already something in common with catboost: you are the only two GPU-enabled frameworks (among the 12 or so I've tested for our containers at work) that give such control over GPU memory usage to the final user - something that even docker is not allowing us to do.\nI noticed that it is customary among deep learning frameworks to create such GPU memory leaks (which it is) - none of them release GPU memory, requiring users to restart python kernel to release memory. Incidentally, none apart from Tensorflow claims nearly all of it, but that's what's so special about you:). In contrast to DNNs, GBDT frameworks tend to release what they no longer need - that applies not only to catboost but also to lightgbm.\nIMO it would be good to contact catboost's GPU guy, Vasily (Noxomo) from Yandex team, because catboost manages to not only allocate 95% of GPU memory but also to use 100% of GPU compute power as well (despite sequential nature of boosting itself), which I've seen only in one DNN - the sadly neglected Theano...", "body": "It is a duplicate with a twist. I'm not calling for lower utilization of GPU - it is perfectly fine to claim all of it, if there is an option to limit the usage to a certain percentage (per_process_gpu_memory_fraction). What's useful in catboost's approach is the idea to deallocate GPU memory after returning results from the GPU back to CPU. BTW you have already something in common with catboost: you are the only two GPU-enabled frameworks (among the 12 or so I've tested for our containers at work) that give such control over GPU memory usage to the final user - something that even docker is not allowing us to do.\r\n\r\nI noticed that it is customary among deep learning frameworks to create such GPU memory leaks (which it is) - none of them release GPU memory, requiring users to restart python kernel to release memory. Incidentally, none apart from Tensorflow claims nearly all of it, but that's what's so special about you:). In contrast to DNNs, GBDT frameworks tend to release what they no longer need - that applies not only to catboost but also to lightgbm. \r\n\r\nIMO it would be good to contact catboost's GPU guy, Vasily ([Noxomo](https://github.com/Noxoomo)) from Yandex team, because catboost manages to not only allocate 95% of GPU memory but also to use 100% of GPU compute power as well (despite sequential nature of boosting itself), which I've seen only in one DNN - the sadly neglected Theano... "}