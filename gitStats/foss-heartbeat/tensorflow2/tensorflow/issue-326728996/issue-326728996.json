{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19571", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19571/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19571/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19571/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19571", "id": 326728996, "node_id": "MDU6SXNzdWUzMjY3Mjg5OTY=", "number": 19571, "title": "Allow full deallocation of GPU memory - like in Catboost", "user": {"login": "mirekphd", "id": 36706320, "node_id": "MDQ6VXNlcjM2NzA2MzIw", "avatar_url": "https://avatars3.githubusercontent.com/u/36706320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mirekphd", "html_url": "https://github.com/mirekphd", "followers_url": "https://api.github.com/users/mirekphd/followers", "following_url": "https://api.github.com/users/mirekphd/following{/other_user}", "gists_url": "https://api.github.com/users/mirekphd/gists{/gist_id}", "starred_url": "https://api.github.com/users/mirekphd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mirekphd/subscriptions", "organizations_url": "https://api.github.com/users/mirekphd/orgs", "repos_url": "https://api.github.com/users/mirekphd/repos", "events_url": "https://api.github.com/users/mirekphd/events{/privacy}", "received_events_url": "https://api.github.com/users/mirekphd/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-05-26T10:47:09Z", "updated_at": "2018-09-28T13:22:41Z", "closed_at": "2018-08-01T20:15:31Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nno (slightly modified)</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\ntensorflow-gpu 1.8.0 </li>\n<li><strong>Python version</strong>:<br>\nPython version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)<br>\n[GCC 7.2.0]</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.0.176 / 7.1.2</li>\n<li><strong>GPU model and memory</strong>:<br>\nGPU[0] GeForce GTX 1080 Ti</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import tensorflow as tf\nimport numpy\nimport matplotlib.pyplot as plt\nimport sklearn.datasets.samples_generator as sample_gen\nrng = numpy.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 500\ndisplay_step = 50\n\n# Training set\nn_features=1\nn_samples=100\n\n# Training Data\n# train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n#                          7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n# train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n#                          2.827,3.465,1.65,2.904,2.42,2.94,1.3])\ntrain_X, train_Y = sample_gen.make_regression(n_samples=n_samples, n_features=n_features, random_state=0)\n# n_samples = train_X.shape[0]\n\n# tf Graph Input\nX = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\n\n# Set model weights\nW = tf.Variable(rng.randn(), name=\"weight\")\nb = tf.Variable(rng.randn(), name=\"bias\")\n\n# Construct a linear model\npred = tf.add(tf.multiply(X, W), b)\n\n# Mean squared error\ncost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n# Gradient descent\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n# Initialize the variables (i.e. assign their default value)\ninit = tf.global_variables_initializer()\n\n# Start training\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Fit all training data\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n\n        #Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \n                \"W=\", sess.run(W), \"b=\", sess.run(b))\n\n    print(\"Optimization Finished!\")\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n    \n    #Graphic display\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()\n    plt.show()\n</code></pre>\n<h3>Describe the problem</h3>\n<p>Same issue as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"286302443\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15880\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/15880/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/15880\">#15880</a> here, with a fully reproducible example using latest TF 1.8 with CUDA 9.0 and cuDNN 7.1 on Ubuntu 16.04. So same old story, but this time I'm giving you a model solution to GPU memory management -  the <a href=\"https://github.com/catboost/catboost\">Catboost library</a> by Yandex.</p>\n<p>I confirm that Tensorflow does not release GPU memory after preallocating most of the available VRAM<br>\n(leaving only a few percent free). This memory should be freed by TF immediately after use for other modeling frameworks to use, so it is a bug that needs to be repaired. To see how it is done, refer to how Catboost manages GPU resources.</p>\n<p>If you are using Jupyter Notebook, restarting python kernel is relatively easy and it \"solves\" the problem, but of course at the cost of losing all your data loaded to CPU memory.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nno (slightly modified)\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\ntensorflow-gpu 1.8.0 \nPython version:\nPython version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)\n[GCC 7.2.0]\nBazel version (if compiling from source):\nN/A\nGCC/Compiler version (if compiling from source):\nN/A\nCUDA/cuDNN version:\n9.0.176 / 7.1.2\nGPU model and memory:\nGPU[0] GeForce GTX 1080 Ti\nExact command to reproduce:\n\nimport tensorflow as tf\nimport numpy\nimport matplotlib.pyplot as plt\nimport sklearn.datasets.samples_generator as sample_gen\nrng = numpy.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 500\ndisplay_step = 50\n\n# Training set\nn_features=1\nn_samples=100\n\n# Training Data\n# train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n#                          7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n# train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n#                          2.827,3.465,1.65,2.904,2.42,2.94,1.3])\ntrain_X, train_Y = sample_gen.make_regression(n_samples=n_samples, n_features=n_features, random_state=0)\n# n_samples = train_X.shape[0]\n\n# tf Graph Input\nX = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\n\n# Set model weights\nW = tf.Variable(rng.randn(), name=\"weight\")\nb = tf.Variable(rng.randn(), name=\"bias\")\n\n# Construct a linear model\npred = tf.add(tf.multiply(X, W), b)\n\n# Mean squared error\ncost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n# Gradient descent\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n# Initialize the variables (i.e. assign their default value)\ninit = tf.global_variables_initializer()\n\n# Start training\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Fit all training data\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n\n        #Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \n                \"W=\", sess.run(W), \"b=\", sess.run(b))\n\n    print(\"Optimization Finished!\")\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n    \n    #Graphic display\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()\n    plt.show()\n\nDescribe the problem\nSame issue as #15880 here, with a fully reproducible example using latest TF 1.8 with CUDA 9.0 and cuDNN 7.1 on Ubuntu 16.04. So same old story, but this time I'm giving you a model solution to GPU memory management -  the Catboost library by Yandex.\nI confirm that Tensorflow does not release GPU memory after preallocating most of the available VRAM\n(leaving only a few percent free). This memory should be freed by TF immediately after use for other modeling frameworks to use, so it is a bug that needs to be repaired. To see how it is done, refer to how Catboost manages GPU resources.\nIf you are using Jupyter Notebook, restarting python kernel is relatively easy and it \"solves\" the problem, but of course at the cost of losing all your data loaded to CPU memory.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno (slightly modified)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\ntensorflow-gpu 1.8.0 <pip>\r\n- **Python version**: \r\nPython version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \r\n[GCC 7.2.0]\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0.176 / 7.1.2\r\n- **GPU model and memory**:\r\nGPU[0] GeForce GTX 1080 Ti\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nimport numpy\r\nimport matplotlib.pyplot as plt\r\nimport sklearn.datasets.samples_generator as sample_gen\r\nrng = numpy.random\r\n\r\n# Parameters\r\nlearning_rate = 0.01\r\ntraining_epochs = 500\r\ndisplay_step = 50\r\n\r\n# Training set\r\nn_features=1\r\nn_samples=100\r\n\r\n# Training Data\r\n# train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\r\n#                          7.042,10.791,5.313,7.997,5.654,9.27,3.1])\r\n# train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\r\n#                          2.827,3.465,1.65,2.904,2.42,2.94,1.3])\r\ntrain_X, train_Y = sample_gen.make_regression(n_samples=n_samples, n_features=n_features, random_state=0)\r\n# n_samples = train_X.shape[0]\r\n\r\n# tf Graph Input\r\nX = tf.placeholder(\"float\")\r\nY = tf.placeholder(\"float\")\r\n\r\n# Set model weights\r\nW = tf.Variable(rng.randn(), name=\"weight\")\r\nb = tf.Variable(rng.randn(), name=\"bias\")\r\n\r\n# Construct a linear model\r\npred = tf.add(tf.multiply(X, W), b)\r\n\r\n# Mean squared error\r\ncost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\r\n# Gradient descent\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\r\n\r\n# Initialize the variables (i.e. assign their default value)\r\ninit = tf.global_variables_initializer()\r\n\r\n# Start training\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    # Fit all training data\r\n    for epoch in range(training_epochs):\r\n        for (x, y) in zip(train_X, train_Y):\r\n            sess.run(optimizer, feed_dict={X: x, Y: y})\r\n\r\n        #Display logs per epoch step\r\n        if (epoch+1) % display_step == 0:\r\n            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\r\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \r\n                \"W=\", sess.run(W), \"b=\", sess.run(b))\r\n\r\n    print(\"Optimization Finished!\")\r\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\r\n    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\r\n    \r\n    #Graphic display\r\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\r\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\r\n    plt.legend()\r\n    plt.show()\r\n    plt.show()\r\n```\r\n### Describe the problem\r\nSame issue as #15880 here, with a fully reproducible example using latest TF 1.8 with CUDA 9.0 and cuDNN 7.1 on Ubuntu 16.04. So same old story, but this time I'm giving you a model solution to GPU memory management -  the [Catboost library](https://github.com/catboost/catboost) by Yandex.\r\n\r\nI confirm that Tensorflow does not release GPU memory after preallocating most of the available VRAM \r\n(leaving only a few percent free). This memory should be freed by TF immediately after use for other modeling frameworks to use, so it is a bug that needs to be repaired. To see how it is done, refer to how Catboost manages GPU resources.\r\n\r\nIf you are using Jupyter Notebook, restarting python kernel is relatively easy and it \"solves\" the problem, but of course at the cost of losing all your data loaded to CPU memory.\r\n\r\n"}