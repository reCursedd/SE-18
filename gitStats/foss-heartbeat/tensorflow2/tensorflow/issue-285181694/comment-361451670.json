{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361451670", "html_url": "https://github.com/tensorflow/tensorflow/issues/15725#issuecomment-361451670", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15725", "id": 361451670, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQ1MTY3MA==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T02:00:04Z", "updated_at": "2018-01-30T02:00:04Z", "author_association": "MEMBER", "body_html": "<p>My testing with AVX vs SSE3 was on K80 and found the following:</p>\n<ul>\n<li>For synthetic data tests that is almost no difference between SSE3 and AVX2 until 8 GPUs and usually more noticeable with real data.</li>\n<li>For single GPU, for the models tested.  VGG16, ResNet50, AlexNet, and InceptionV3 the difference was less than 2% until 8 GPUs.</li>\n<li>The biggest difference was AlexNet with SSE3 being 15-18% slower on 8xK80 on a p2.8xlarge AWS instance.</li>\n</ul>\n<p>The results may be different today with improvements to the data pipeline.  When doing GPU training the CPU is mostly only handling the input pipeline.  The input pipeline can benefit from having code optimized for the CPU but it may not be visible.  In the testing above my hypothesis was that because AlexNet pushes the input pipeline the difference becomes visible.  That theory may not be perfect.  TensorFlow will go AVX by default in 1.6 and likely in nightlies in a day or so.  I will try to remember to report back if I see a bump for SSE3 to AVX for the some nightly tests.</p>", "body_text": "My testing with AVX vs SSE3 was on K80 and found the following:\n\nFor synthetic data tests that is almost no difference between SSE3 and AVX2 until 8 GPUs and usually more noticeable with real data.\nFor single GPU, for the models tested.  VGG16, ResNet50, AlexNet, and InceptionV3 the difference was less than 2% until 8 GPUs.\nThe biggest difference was AlexNet with SSE3 being 15-18% slower on 8xK80 on a p2.8xlarge AWS instance.\n\nThe results may be different today with improvements to the data pipeline.  When doing GPU training the CPU is mostly only handling the input pipeline.  The input pipeline can benefit from having code optimized for the CPU but it may not be visible.  In the testing above my hypothesis was that because AlexNet pushes the input pipeline the difference becomes visible.  That theory may not be perfect.  TensorFlow will go AVX by default in 1.6 and likely in nightlies in a day or so.  I will try to remember to report back if I see a bump for SSE3 to AVX for the some nightly tests.", "body": "My testing with AVX vs SSE3 was on K80 and found the following:\r\n\r\n- For synthetic data tests that is almost no difference between SSE3 and AVX2 until 8 GPUs and usually more noticeable with real data. \r\n- For single GPU, for the models tested.  VGG16, ResNet50, AlexNet, and InceptionV3 the difference was less than 2% until 8 GPUs.\r\n- The biggest difference was AlexNet with SSE3 being 15-18% slower on 8xK80 on a p2.8xlarge AWS instance.  \r\n\r\nThe results may be different today with improvements to the data pipeline.  When doing GPU training the CPU is mostly only handling the input pipeline.  The input pipeline can benefit from having code optimized for the CPU but it may not be visible.  In the testing above my hypothesis was that because AlexNet pushes the input pipeline the difference becomes visible.  That theory may not be perfect.  TensorFlow will go AVX by default in 1.6 and likely in nightlies in a day or so.  I will try to remember to report back if I see a bump for SSE3 to AVX for the some nightly tests. "}