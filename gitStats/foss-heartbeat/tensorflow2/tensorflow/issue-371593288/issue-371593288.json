{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23077", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23077/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23077/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23077/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23077", "id": 371593288, "node_id": "MDU6SXNzdWUzNzE1OTMyODg=", "number": 23077, "title": "Slow training speed and incompatible ops issue in tf.keras.utils.multi_gpu_model", "user": {"login": "kami93", "id": 20102, "node_id": "MDQ6VXNlcjIwMTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/20102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kami93", "html_url": "https://github.com/kami93", "followers_url": "https://api.github.com/users/kami93/followers", "following_url": "https://api.github.com/users/kami93/following{/other_user}", "gists_url": "https://api.github.com/users/kami93/gists{/gist_id}", "starred_url": "https://api.github.com/users/kami93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kami93/subscriptions", "organizations_url": "https://api.github.com/users/kami93/orgs", "repos_url": "https://api.github.com/users/kami93/repos", "events_url": "https://api.github.com/users/kami93/events{/privacy}", "received_events_url": "https://api.github.com/users/kami93/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097546578, "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras", "name": "comp:keras", "color": "0052cc", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-10-18T15:42:23Z", "updated_at": "2018-11-13T16:48:00Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a bug. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See <a href=\"https://github.com/kami93/PredRNN\">https://github.com/kami93/PredRNN</a></li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A</li>\n<li>TensorFlow installed from (source or binary):Source</li>\n<li>TensorFlow version (use command below):v1.11.0</li>\n<li>Python version: Anaconda Python 3.6.6</li>\n<li>Bazel version (if compiling from source):0.17.2</li>\n<li>GCC/Compiler version (if compiling from source): GCC 7.3.0</li>\n<li>CUDA/cuDNN version: CUDA 9.2, cuDNN 7.3</li>\n<li>GPU model and memory: Gefroce 1080Ti * 4 (4-way GPU), 11,178 MiB each.</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nHi. I have built a TensorFlow Keras Sequential model (See <a href=\"https://github.com/kami93/PredRNN/blob/master/predRNN.py\">predRNN.py</a>) which consists of several Keras layers including my custom layer (See <a href=\"https://github.com/kami93/PredRNN/blob/master/keras_custom/layers/STLSTM.py\">keras_custom/layers/STLSTM.py</a>).</p>\n<p>The model by itself runs very well when tf.keras.utils.multi_gpu_model is not applied. The model can be created, compiled, and perform the training (by Model.fit method) without any warning or error.</p>\n<p>However, if tf.keras.utils.multi_gpu_model is applied to replicate the model on several GPUs for multi gpu training, warnings like the followings are raised.</p>\n<pre><code>No node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.\nDevice assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:\n  with tf.device(/gpu:2): &lt;/home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221&gt;\n</code></pre>\n<pre><code>WARNING:tensorflow:Tried to colocate op 'training/Adam/gradients/replica_3/sequential/stlst_m2d/while/convolution_14_grad/ShapeN/Const' (defined at predRNN.py:142) having device '/device:CPU:1' with op 'replica_3/sequential/stlst_m2d/while/convolution_14' (defined at /home/simon/Desktop/git/PredRNN/keras_custom/layers/STLSTM.py:969) which had an incompatible device '/device:GPU:3'.\n</code></pre>\n<p>I think here \"tf.keras.utils.multi_gpu_model\" is not being able to properly assign the convolution ops to other devices which they are initially created for the \"model_creation_device\" (See <a href=\"https://github.com/kami93/PredRNN/blob/master/predRNN.py#L97\">line 97 in predRNN.py</a>).</p>\n<p>Actually, the moodel behaves different with the different selection of \"model_creation_device\".</p>\n<p><strong>Case 1</strong><br>\nIf model_creation_device == '/cpu:0', the CPU usage is almost 100% in every core. For the GPU usages, gpu:0, gpu:1, gpu:2 and gpu:3 are all underutilized below 30%. The training is extremely slow compared to other cases. I think some convolution ops are performed by 'CPU:0' here.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/20102/47162030-33740d00-d32e-11e8-8dc4-b5fd40be6c09.png\"><img src=\"https://user-images.githubusercontent.com/20102/47162030-33740d00-d32e-11e8-8dc4-b5fd40be6c09.png\" alt=\"cpu_usage\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/20102/47163769-a3d05d80-d331-11e8-8e39-d8fa27ec9542.png\"><img src=\"https://user-images.githubusercontent.com/20102/47163769-a3d05d80-d331-11e8-8e39-d8fa27ec9542.png\" alt=\"nvidia-smi_cpu0\" style=\"max-width:100%;\"></a></p>\n<p>##################################################################################</p>\n<p><strong>Case 2</strong><br>\nIf model_creation_device == '/cpu:1', the CPU usage is normal below 30% in every core. However memory shortage warnings such as the followings are raised.</p>\n<pre><code>2018-10-18 23:14:56.079586: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2018-10-18 23:14:56.082410: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n</code></pre>\n<p>For the GPUs usage, only the 'gpu:0' is highly utilized around 100% all the time. Others are mostly underutilized around 30%. However, the training speed is fastest amongst the cases.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/20102/47166363-fc562980-d336-11e8-88d8-1c4b7708a135.png\"><img src=\"https://user-images.githubusercontent.com/20102/47166363-fc562980-d336-11e8-88d8-1c4b7708a135.png\" alt=\"cpu_usage_cpu1\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/20102/47166330-e7799600-d336-11e8-862d-de21e67fc211.png\"><img src=\"https://user-images.githubusercontent.com/20102/47166330-e7799600-d336-11e8-862d-de21e67fc211.png\" alt=\"cpu1\" style=\"max-width:100%;\"></a></p>\n<p>##################################################################################</p>\n<p><strong>Case 3</strong><br>\nIf model_creation_device == '/gpu:0', '/gpu:1', '/gpu:2', or '/gpu:3', the CPU usage is normal below 30% in every core. However, memory shortage warnings are raised.</p>\n<p>The GPUs usage is somewhat strange. Only the \"model_creation_device\" is highly utilized around 100% all the time, and others are mostly underutilized around 30%.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/20102/47165523-1989f880-d335-11e8-9728-424b649c967c.png\"><img src=\"https://user-images.githubusercontent.com/20102/47165523-1989f880-d335-11e8-9728-424b649c967c.png\" alt=\"cpu_usage_gpus\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/20102/47165150-64efd700-d334-11e8-83e1-abc2cf41b02f.png\"><img src=\"https://user-images.githubusercontent.com/20102/47165150-64efd700-d334-11e8-83e1-abc2cf41b02f.png\" alt=\"gpu1\" style=\"max-width:100%;\"></a><br>\nThe training speed is way faster than model_creation_device == '/cpu:0' case. However, slightly slower than model_creation_device == '/cpu:1' case.</p>\n<p><strong>Describe the expected behavior</strong><br>\nMulti GPU training of \"PredRNN\" keras model using \"tf.keras.utils.multi_gpu_model\" works without any warning related ops compatibility with devices. All CPU core's usage is below 30% and GPUs usage are around 100% equally for all GPUs while training.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nRun predRNN.py in <a href=\"https://github.com/kami93/PredRNN\">https://github.com/kami93/PredRNN</a>.</p>\n<p>Modify the \"model_creation_device\" to reproduce issues.</p>\n<p><strong>Other info / logs</strong><br>\nModel source code: <a href=\"https://github.com/kami93/PredRNN\">https://github.com/kami93/PredRNN</a><br>\nlogs when model_creation_device = '/cpu:0'<br>\n<a href=\"https://pastebin.com/esjt8ZLa\" rel=\"nofollow\">https://pastebin.com/esjt8ZLa</a><br>\nlogs when model_creation_device = '/cpu:1'<br>\n<a href=\"https://pastebin.com/yRfm9yW8\" rel=\"nofollow\">https://pastebin.com/yRfm9yW8</a><br>\nlogs when model_creation_device = '/gpu:0'<br>\n<a href=\"https://pastebin.com/CB6Wbzn7\" rel=\"nofollow\">https://pastebin.com/CB6Wbzn7</a><br>\nlogs when model_creation_device = '/gpu:1'<br>\n<a href=\"https://pastebin.com/c917nU53\" rel=\"nofollow\">https://pastebin.com/c917nU53</a><br>\nlogs when model_creation_device = '/gpu:2'<br>\n<a href=\"https://pastebin.com/rbMSPaWN\" rel=\"nofollow\">https://pastebin.com/rbMSPaWN</a><br>\nlogs when model_creation_device = '/gpu:3'<br>\n<a href=\"https://pastebin.com/j3xGDvrw\" rel=\"nofollow\">https://pastebin.com/j3xGDvrw</a></p>", "body_text": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See https://github.com/kami93/PredRNN\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\nTensorFlow installed from (source or binary):Source\nTensorFlow version (use command below):v1.11.0\nPython version: Anaconda Python 3.6.6\nBazel version (if compiling from source):0.17.2\nGCC/Compiler version (if compiling from source): GCC 7.3.0\nCUDA/cuDNN version: CUDA 9.2, cuDNN 7.3\nGPU model and memory: Gefroce 1080Ti * 4 (4-way GPU), 11,178 MiB each.\n\nDescribe the current behavior\nHi. I have built a TensorFlow Keras Sequential model (See predRNN.py) which consists of several Keras layers including my custom layer (See keras_custom/layers/STLSTM.py).\nThe model by itself runs very well when tf.keras.utils.multi_gpu_model is not applied. The model can be created, compiled, and perform the training (by Model.fit method) without any warning or error.\nHowever, if tf.keras.utils.multi_gpu_model is applied to replicate the model on several GPUs for multi gpu training, warnings like the followings are raised.\nNo node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.\nDevice assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:\n  with tf.device(/gpu:2): </home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221>\n\nWARNING:tensorflow:Tried to colocate op 'training/Adam/gradients/replica_3/sequential/stlst_m2d/while/convolution_14_grad/ShapeN/Const' (defined at predRNN.py:142) having device '/device:CPU:1' with op 'replica_3/sequential/stlst_m2d/while/convolution_14' (defined at /home/simon/Desktop/git/PredRNN/keras_custom/layers/STLSTM.py:969) which had an incompatible device '/device:GPU:3'.\n\nI think here \"tf.keras.utils.multi_gpu_model\" is not being able to properly assign the convolution ops to other devices which they are initially created for the \"model_creation_device\" (See line 97 in predRNN.py).\nActually, the moodel behaves different with the different selection of \"model_creation_device\".\nCase 1\nIf model_creation_device == '/cpu:0', the CPU usage is almost 100% in every core. For the GPU usages, gpu:0, gpu:1, gpu:2 and gpu:3 are all underutilized below 30%. The training is extremely slow compared to other cases. I think some convolution ops are performed by 'CPU:0' here.\n\n\n##################################################################################\nCase 2\nIf model_creation_device == '/cpu:1', the CPU usage is normal below 30% in every core. However memory shortage warnings such as the followings are raised.\n2018-10-18 23:14:56.079586: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2018-10-18 23:14:56.082410: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n\nFor the GPUs usage, only the 'gpu:0' is highly utilized around 100% all the time. Others are mostly underutilized around 30%. However, the training speed is fastest amongst the cases.\n\n\n##################################################################################\nCase 3\nIf model_creation_device == '/gpu:0', '/gpu:1', '/gpu:2', or '/gpu:3', the CPU usage is normal below 30% in every core. However, memory shortage warnings are raised.\nThe GPUs usage is somewhat strange. Only the \"model_creation_device\" is highly utilized around 100% all the time, and others are mostly underutilized around 30%.\n\n\nThe training speed is way faster than model_creation_device == '/cpu:0' case. However, slightly slower than model_creation_device == '/cpu:1' case.\nDescribe the expected behavior\nMulti GPU training of \"PredRNN\" keras model using \"tf.keras.utils.multi_gpu_model\" works without any warning related ops compatibility with devices. All CPU core's usage is below 30% and GPUs usage are around 100% equally for all GPUs while training.\nCode to reproduce the issue\nRun predRNN.py in https://github.com/kami93/PredRNN.\nModify the \"model_creation_device\" to reproduce issues.\nOther info / logs\nModel source code: https://github.com/kami93/PredRNN\nlogs when model_creation_device = '/cpu:0'\nhttps://pastebin.com/esjt8ZLa\nlogs when model_creation_device = '/cpu:1'\nhttps://pastebin.com/yRfm9yW8\nlogs when model_creation_device = '/gpu:0'\nhttps://pastebin.com/CB6Wbzn7\nlogs when model_creation_device = '/gpu:1'\nhttps://pastebin.com/c917nU53\nlogs when model_creation_device = '/gpu:2'\nhttps://pastebin.com/rbMSPaWN\nlogs when model_creation_device = '/gpu:3'\nhttps://pastebin.com/j3xGDvrw", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See https://github.com/kami93/PredRNN\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):v1.11.0\r\n- Python version: Anaconda Python 3.6.6\r\n- Bazel version (if compiling from source):0.17.2\r\n- GCC/Compiler version (if compiling from source): GCC 7.3.0\r\n- CUDA/cuDNN version: CUDA 9.2, cuDNN 7.3\r\n- GPU model and memory: Gefroce 1080Ti * 4 (4-way GPU), 11,178 MiB each.\r\n\r\n**Describe the current behavior**\r\nHi. I have built a TensorFlow Keras Sequential model (See [predRNN.py](https://github.com/kami93/PredRNN/blob/master/predRNN.py)) which consists of several Keras layers including my custom layer (See [keras_custom/layers/STLSTM.py](https://github.com/kami93/PredRNN/blob/master/keras_custom/layers/STLSTM.py)).\r\n\r\nThe model by itself runs very well when tf.keras.utils.multi_gpu_model is not applied. The model can be created, compiled, and perform the training (by Model.fit method) without any warning or error.\r\n\r\nHowever, if tf.keras.utils.multi_gpu_model is applied to replicate the model on several GPUs for multi gpu training, warnings like the followings are raised. \r\n\r\n```\r\nNo node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.\r\nDevice assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:\r\n  with tf.device(/gpu:2): </home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221>\r\n```\r\n```\r\nWARNING:tensorflow:Tried to colocate op 'training/Adam/gradients/replica_3/sequential/stlst_m2d/while/convolution_14_grad/ShapeN/Const' (defined at predRNN.py:142) having device '/device:CPU:1' with op 'replica_3/sequential/stlst_m2d/while/convolution_14' (defined at /home/simon/Desktop/git/PredRNN/keras_custom/layers/STLSTM.py:969) which had an incompatible device '/device:GPU:3'.\r\n```\r\nI think here \"tf.keras.utils.multi_gpu_model\" is not being able to properly assign the convolution ops to other devices which they are initially created for the \"model_creation_device\" (See [line 97 in predRNN.py](https://github.com/kami93/PredRNN/blob/master/predRNN.py#L97)).\r\n\r\nActually, the moodel behaves different with the different selection of \"model_creation_device\".\r\n\r\n**Case 1**\r\nIf model_creation_device == '/cpu:0', the CPU usage is almost 100% in every core. For the GPU usages, gpu:0, gpu:1, gpu:2 and gpu:3 are all underutilized below 30%. The training is extremely slow compared to other cases. I think some convolution ops are performed by 'CPU:0' here.\r\n\r\n![cpu_usage](https://user-images.githubusercontent.com/20102/47162030-33740d00-d32e-11e8-8dc4-b5fd40be6c09.png)\r\n![nvidia-smi_cpu0](https://user-images.githubusercontent.com/20102/47163769-a3d05d80-d331-11e8-8e39-d8fa27ec9542.png)\r\n\r\n##################################################################################\r\n\r\n**Case 2**\r\nIf model_creation_device == '/cpu:1', the CPU usage is normal below 30% in every core. However memory shortage warnings such as the followings are raised.\r\n```\r\n2018-10-18 23:14:56.079586: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-10-18 23:14:56.082410: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n```\r\nFor the GPUs usage, only the 'gpu:0' is highly utilized around 100% all the time. Others are mostly underutilized around 30%. However, the training speed is fastest amongst the cases. \r\n\r\n![cpu_usage_cpu1](https://user-images.githubusercontent.com/20102/47166363-fc562980-d336-11e8-88d8-1c4b7708a135.png)\r\n![cpu1](https://user-images.githubusercontent.com/20102/47166330-e7799600-d336-11e8-862d-de21e67fc211.png)\r\n\r\n##################################################################################\r\n\r\n**Case 3**\r\nIf model_creation_device == '/gpu:0', '/gpu:1', '/gpu:2', or '/gpu:3', the CPU usage is normal below 30% in every core. However, memory shortage warnings are raised.\r\n\r\nThe GPUs usage is somewhat strange. Only the \"model_creation_device\" is highly utilized around 100% all the time, and others are mostly underutilized around 30%. \r\n\r\n![cpu_usage_gpus](https://user-images.githubusercontent.com/20102/47165523-1989f880-d335-11e8-9728-424b649c967c.png)\r\n![gpu1](https://user-images.githubusercontent.com/20102/47165150-64efd700-d334-11e8-83e1-abc2cf41b02f.png)\r\nThe training speed is way faster than model_creation_device == '/cpu:0' case. However, slightly slower than model_creation_device == '/cpu:1' case.\r\n\r\n**Describe the expected behavior**\r\nMulti GPU training of \"PredRNN\" keras model using \"tf.keras.utils.multi_gpu_model\" works without any warning related ops compatibility with devices. All CPU core's usage is below 30% and GPUs usage are around 100% equally for all GPUs while training.\r\n\r\n**Code to reproduce the issue**\r\nRun predRNN.py in https://github.com/kami93/PredRNN.\r\n\r\nModify the \"model_creation_device\" to reproduce issues.\r\n\r\n**Other info / logs**\r\nModel source code: https://github.com/kami93/PredRNN\r\nlogs when model_creation_device = '/cpu:0'\r\nhttps://pastebin.com/esjt8ZLa\r\nlogs when model_creation_device = '/cpu:1'\r\nhttps://pastebin.com/yRfm9yW8\r\nlogs when model_creation_device = '/gpu:0'\r\nhttps://pastebin.com/CB6Wbzn7\r\nlogs when model_creation_device = '/gpu:1'\r\nhttps://pastebin.com/c917nU53\r\nlogs when model_creation_device = '/gpu:2'\r\nhttps://pastebin.com/rbMSPaWN\r\nlogs when model_creation_device = '/gpu:3'\r\nhttps://pastebin.com/j3xGDvrw"}