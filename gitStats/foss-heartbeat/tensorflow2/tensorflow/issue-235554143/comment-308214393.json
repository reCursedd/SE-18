{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308214393", "html_url": "https://github.com/tensorflow/tensorflow/issues/10680#issuecomment-308214393", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10680", "id": 308214393, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODIxNDM5Mw==", "user": {"login": "Scitator", "id": 7606451, "node_id": "MDQ6VXNlcjc2MDY0NTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7606451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Scitator", "html_url": "https://github.com/Scitator", "followers_url": "https://api.github.com/users/Scitator/followers", "following_url": "https://api.github.com/users/Scitator/following{/other_user}", "gists_url": "https://api.github.com/users/Scitator/gists{/gist_id}", "starred_url": "https://api.github.com/users/Scitator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Scitator/subscriptions", "organizations_url": "https://api.github.com/users/Scitator/orgs", "repos_url": "https://api.github.com/users/Scitator/repos", "events_url": "https://api.github.com/users/Scitator/events{/privacy}", "received_events_url": "https://api.github.com/users/Scitator/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-13T18:56:20Z", "updated_at": "2017-06-13T18:56:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The problem:<br>\ntypically, <code>generator_input_fn</code> expects that python generator yields numpy array of features with same size for each training example. <strong>But</strong> for some cases, for example, seq2seq models, each training example has different size (just different lens, really). Dynamic shapes broke up current input pipeline:</p>\n<ol>\n<li>firstly, in <code>_enqueue_data</code> function: it use first yielded example for shape specification and remember it. For dynamic shapes it is completely wrong, cause...you know, shape are dynamic. <a href=\"https://github.com/Scitator/tensorflow/blob/a827151b7a6ac152d63e2241a3d7a1d1aaf15aff/tensorflow/python/estimator/inputs/queues/feeding_functions.py#L365\">source</a></li>\n<li>secondly, at feed stage, when it converts list of numpy array to matrix: if shape are equal, all works correctly, but our shapes are not, so -&gt; exception. <a href=\"https://github.com/Scitator/tensorflow/blob/master/tensorflow/python/client/session.py#L1075\">source</a></li>\n</ol>\n<p>Why is it needed?<br>\nPersonally, the main reason why it needed - give simplified input pipeline interface for seq2seq models. It can be done through <code>PaddingFIFOQueue</code> and <code>placeholder_with_default</code>, but for <code>tf.estimator</code> usage we still need some input pipeline function</p>\n<p>What would the feature be? How would it work?<br>\nThe feature is quite simple, it could be an additional flag, or <code>shuffle</code> replacement to allow user to use  <code>PaddingFIFOQueue</code> under the hood for batch preprocessing.</p>\n<p>For now, I clearly understand how to do it for generator pipeline, although for other ones - dont think so. As for me, only generator one can handle dynamic shapes, other one, such as numpy source or pandas daaframe already want data to be same size.</p>", "body_text": "The problem:\ntypically, generator_input_fn expects that python generator yields numpy array of features with same size for each training example. But for some cases, for example, seq2seq models, each training example has different size (just different lens, really). Dynamic shapes broke up current input pipeline:\n\nfirstly, in _enqueue_data function: it use first yielded example for shape specification and remember it. For dynamic shapes it is completely wrong, cause...you know, shape are dynamic. source\nsecondly, at feed stage, when it converts list of numpy array to matrix: if shape are equal, all works correctly, but our shapes are not, so -> exception. source\n\nWhy is it needed?\nPersonally, the main reason why it needed - give simplified input pipeline interface for seq2seq models. It can be done through PaddingFIFOQueue and placeholder_with_default, but for tf.estimator usage we still need some input pipeline function\nWhat would the feature be? How would it work?\nThe feature is quite simple, it could be an additional flag, or shuffle replacement to allow user to use  PaddingFIFOQueue under the hood for batch preprocessing.\nFor now, I clearly understand how to do it for generator pipeline, although for other ones - dont think so. As for me, only generator one can handle dynamic shapes, other one, such as numpy source or pandas daaframe already want data to be same size.", "body": "The problem:\r\ntypically, `generator_input_fn` expects that python generator yields numpy array of features with same size for each training example. **But** for some cases, for example, seq2seq models, each training example has different size (just different lens, really). Dynamic shapes broke up current input pipeline:\r\n1. firstly, in `_enqueue_data` function: it use first yielded example for shape specification and remember it. For dynamic shapes it is completely wrong, cause...you know, shape are dynamic. [source](https://github.com/Scitator/tensorflow/blob/a827151b7a6ac152d63e2241a3d7a1d1aaf15aff/tensorflow/python/estimator/inputs/queues/feeding_functions.py#L365)\r\n2. secondly, at feed stage, when it converts list of numpy array to matrix: if shape are equal, all works correctly, but our shapes are not, so -> exception. [source](https://github.com/Scitator/tensorflow/blob/master/tensorflow/python/client/session.py#L1075)\r\n\r\nWhy is it needed?\r\nPersonally, the main reason why it needed - give simplified input pipeline interface for seq2seq models. It can be done through `PaddingFIFOQueue` and `placeholder_with_default`, but for `tf.estimator` usage we still need some input pipeline function\r\n\r\nWhat would the feature be? How would it work?\r\nThe feature is quite simple, it could be an additional flag, or `shuffle` replacement to allow user to use  `PaddingFIFOQueue` under the hood for batch preprocessing. \r\n\r\nFor now, I clearly understand how to do it for generator pipeline, although for other ones - dont think so. As for me, only generator one can handle dynamic shapes, other one, such as numpy source or pandas daaframe already want data to be same size."}