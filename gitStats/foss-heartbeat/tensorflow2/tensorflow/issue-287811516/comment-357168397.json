{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357168397", "html_url": "https://github.com/tensorflow/tensorflow/issues/16042#issuecomment-357168397", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16042", "id": 357168397, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzE2ODM5Nw==", "user": {"login": "standy66", "id": 1818586, "node_id": "MDQ6VXNlcjE4MTg1ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1818586?v=4", "gravatar_id": "", "url": "https://api.github.com/users/standy66", "html_url": "https://github.com/standy66", "followers_url": "https://api.github.com/users/standy66/followers", "following_url": "https://api.github.com/users/standy66/following{/other_user}", "gists_url": "https://api.github.com/users/standy66/gists{/gist_id}", "starred_url": "https://api.github.com/users/standy66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/standy66/subscriptions", "organizations_url": "https://api.github.com/users/standy66/orgs", "repos_url": "https://api.github.com/users/standy66/repos", "events_url": "https://api.github.com/users/standy66/events{/privacy}", "received_events_url": "https://api.github.com/users/standy66/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T07:58:59Z", "updated_at": "2018-01-12T07:58:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hello, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5117188\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/protoget\">@protoget</a>, thanks for quick reply.</p>\n<p>First, let me introduce a strong case where skip_input is useful. We are following <a href=\"https://arxiv.org/pdf/1512.02595.pdf\" rel=\"nofollow\">DeepSpeech 2</a> paper from Baidu Research. They are using batch normalization for recurrent neural networks similarly to <a href=\"https://arxiv.org/pdf/1510.01378.pdf\" rel=\"nofollow\">Laurent et al., 2015</a>. Here is an excerpt from the paper:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/e51493770a9e9409f83d11e8a4ebff80023fa5df/687474703a2f2f6f6936362e74696e797069632e636f6d2f7377386839692e6a7067\"><img src=\"https://camo.githubusercontent.com/e51493770a9e9409f83d11e8a4ebff80023fa5df/687474703a2f2f6f6936362e74696e797069632e636f6d2f7377386839692e6a7067\" alt=\"\" data-canonical-src=\"http://oi66.tinypic.com/sw8h9i.jpg\" style=\"max-width:100%;\"></a></p>\n<p>And in GRUs and LSTMs, they suggest replacing <code>Wx + b</code> parts in equiations with <code>BN(Wx)</code>. One of the authors of original DeepSpeech 2 paper confirmed that on <a href=\"https://www.reddit.com/r/MachineLearning/comments/4082yn/rnn_batch_normalization_for_grulstm/cyw4ezu/\" rel=\"nofollow\">Reddit</a>. This is easily achievable by stacking dense layer, batch normalization and recurrent layer with <code>input_mode=skip_input</code>.</p>\n<p>As for the answer for your question, yes, I think removing input_mode from cudnn_rnn layers API is ok, but then <code>cudnn_rnn_ops</code> should be accessibe in documentation (as far as I understand now it's not the case with TF 1.5), maybe with this particular case as an example of where <code>skip_input</code> is needed.</p>", "body_text": "Hello, @protoget, thanks for quick reply.\nFirst, let me introduce a strong case where skip_input is useful. We are following DeepSpeech 2 paper from Baidu Research. They are using batch normalization for recurrent neural networks similarly to Laurent et al., 2015. Here is an excerpt from the paper:\n\nAnd in GRUs and LSTMs, they suggest replacing Wx + b parts in equiations with BN(Wx). One of the authors of original DeepSpeech 2 paper confirmed that on Reddit. This is easily achievable by stacking dense layer, batch normalization and recurrent layer with input_mode=skip_input.\nAs for the answer for your question, yes, I think removing input_mode from cudnn_rnn layers API is ok, but then cudnn_rnn_ops should be accessibe in documentation (as far as I understand now it's not the case with TF 1.5), maybe with this particular case as an example of where skip_input is needed.", "body": "Hello, @protoget, thanks for quick reply.\r\n\r\nFirst, let me introduce a strong case where skip_input is useful. We are following [DeepSpeech 2](https://arxiv.org/pdf/1512.02595.pdf) paper from Baidu Research. They are using batch normalization for recurrent neural networks similarly to [Laurent et al., 2015](https://arxiv.org/pdf/1510.01378.pdf). Here is an excerpt from the paper:\r\n\r\n\r\n![](http://oi66.tinypic.com/sw8h9i.jpg)\r\n\r\n\r\nAnd in GRUs and LSTMs, they suggest replacing `Wx + b` parts in equiations with `BN(Wx)`. One of the authors of original DeepSpeech 2 paper confirmed that on [Reddit](https://www.reddit.com/r/MachineLearning/comments/4082yn/rnn_batch_normalization_for_grulstm/cyw4ezu/). This is easily achievable by stacking dense layer, batch normalization and recurrent layer with `input_mode=skip_input`.\r\n\r\n\r\nAs for the answer for your question, yes, I think removing input_mode from cudnn_rnn layers API is ok, but then `cudnn_rnn_ops` should be accessibe in documentation (as far as I understand now it's not the case with TF 1.5), maybe with this particular case as an example of where `skip_input` is needed."}