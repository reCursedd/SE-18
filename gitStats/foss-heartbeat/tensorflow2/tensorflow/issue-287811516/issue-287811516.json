{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16042", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16042/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16042/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16042/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16042", "id": 287811516, "node_id": "MDU6SXNzdWUyODc4MTE1MTY=", "number": 16042, "title": "tf.contrib.cudnn_rnn.CudnnGRU does not work with input_mode='skip_input' in TF1.5", "user": {"login": "standy66", "id": 1818586, "node_id": "MDQ6VXNlcjE4MTg1ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1818586?v=4", "gravatar_id": "", "url": "https://api.github.com/users/standy66", "html_url": "https://github.com/standy66", "followers_url": "https://api.github.com/users/standy66/followers", "following_url": "https://api.github.com/users/standy66/following{/other_user}", "gists_url": "https://api.github.com/users/standy66/gists{/gist_id}", "starred_url": "https://api.github.com/users/standy66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/standy66/subscriptions", "organizations_url": "https://api.github.com/users/standy66/orgs", "repos_url": "https://api.github.com/users/standy66/repos", "events_url": "https://api.github.com/users/standy66/events{/privacy}", "received_events_url": "https://api.github.com/users/standy66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-01-11T14:48:34Z", "updated_at": "2018-11-22T18:53:34Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from</strong>: <code>pip install tensorflow-gpu==1.5.0rc0</code></li>\n<li><strong>TensorFlow version</strong>: v1.3.0-rc1-6090-g622487f 1.5.0-rc0</li>\n<li><strong>Python version</strong>:  3.5</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0, CuDNN 7.0.5</li>\n<li><strong>GPU model and memory</strong>: GTX 1080 8GB</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>We are running tf.contrib.cudnn_rnn.CudnnGRU in our speech recognition setup with input_mode='skip_input' and it crashes the whole process. Here is the assertion error that we are getting:</p>\n<p><code>Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000</code></p>\n<p>Looks like it's crashing here: <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L440\">tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440</a>. It does not crash if input_mode is 'linear_input'.</p>\n<p>Here is the minimal example to reproduce:</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\n\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=100,\n                             input_mode='skip_input', direction='bidirectional')\n# (time, batch_size, num_inputs)\nx = tf.random_normal((100, 16, 100))\ny = layer(x)\n\nwith tf.Session() as sess:\n\tsess.run(tf.global_variables_initializer())\n\tprint(sess.run(y))\n\n</code></pre>\n<p>Logs:</p>\n<pre><code>2018-01-11 17:10:44.858413: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-01-11 17:10:45.067854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\npciBusID: 0000:03:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-01-11 17:10:45.274222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\npciBusID: 0000:04:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-01-11 17:10:45.274776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\n2018-01-11 17:10:45.274804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1 \n2018-01-11 17:10:45.274812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y Y \n2018-01-11 17:10:45.274817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   Y Y \n2018-01-11 17:10:45.274826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-01-11 17:10:45.274832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\n2018-01-11 17:10:46.240862: F tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440] Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000\nAborted (core dumped)\n</code></pre>\n<h3>Thoughts</h3>\n<p>I think the problem is in python wrapper code located at <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\">contrib/cudnn_rnn/python/layers/cudnn_rnn.py</a>.</p>\n<p>Both <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L417\"><code>_canonical_weight_shape(self, layer)</code></a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L443\"><code>_canonical_bias_shape(self, layer)</code></a> don't handle the case when <code>input_mode='skip_input'</code>. The wrapper code doesn't even check if <code>input_size == num_units</code> when <code>input_mode='skip_input'</code> as it should! The issue seems to be easy to fix, but I may be mistaken.</p>\n<p>Here is an example that shows that both 'skip_input' and 'linear_input' get the same set of canonical parameter shapes:</p>\n<pre><code>CudnnGRU = lambda **kwargs: tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=7, direction='bidirectional', **kwargs)\n\nlayer = CudnnGRU(input_mode='skip_input')\nlayer.build((200, 16, 5))\nprint(\"skip_input weights\", layer.canonical_weight_shapes)\nprint(\"skip_input biases\", layer.canonical_bias_shapes)\n\nlayer = CudnnGRU(input_mode='linear_input')\nlayer.build((200, 16, 5))\nprint(\"linear_input_weights\", layer.canonical_weight_shapes)\nprint(\"linear_input_biases\", layer.canonical_bias_shapes)\n</code></pre>\n<p>Example output:</p>\n<pre><code>skip_input weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\nskip_input biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\nlinear_input_weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\nlinear_input_biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\n</code></pre>\n<h3>Workaround</h3>\n<p>In our setup we decided that we will keep allocating opaque_params buffer manually with first querying its size with</p>\n<pre><code># Note: this code may require tf==1.4 to run.\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units=800, input_size=800,\n                                      input_mode='linear_input',\n                                      direction='bidirectional')\nwith tf.Session() as sess:\n    OPAQUE_BUFFER_SIZE = sess.run(layer.params_size())\n    print(OPAQUE_BUFFER_SIZE)\n</code></pre>\n<p>But this is really troublesome because it requires running the intermediate graph to get OPAQUE_BUFFER_SIZE before building the main graph.</p>", "body_text": "System information\n\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from: pip install tensorflow-gpu==1.5.0rc0\nTensorFlow version: v1.3.0-rc1-6090-g622487f 1.5.0-rc0\nPython version:  3.5\nCUDA/cuDNN version: CUDA 9.0, CuDNN 7.0.5\nGPU model and memory: GTX 1080 8GB\n\nDescribe the problem\nWe are running tf.contrib.cudnn_rnn.CudnnGRU in our speech recognition setup with input_mode='skip_input' and it crashes the whole process. Here is the assertion error that we are getting:\nCheck failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000\nLooks like it's crashing here: tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440. It does not crash if input_mode is 'linear_input'.\nHere is the minimal example to reproduce:\nSource code / logs\nimport tensorflow as tf\n\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=100,\n                             input_mode='skip_input', direction='bidirectional')\n# (time, batch_size, num_inputs)\nx = tf.random_normal((100, 16, 100))\ny = layer(x)\n\nwith tf.Session() as sess:\n\tsess.run(tf.global_variables_initializer())\n\tprint(sess.run(y))\n\n\nLogs:\n2018-01-11 17:10:44.858413: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-01-11 17:10:45.067854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\npciBusID: 0000:03:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-01-11 17:10:45.274222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\npciBusID: 0000:04:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-01-11 17:10:45.274776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\n2018-01-11 17:10:45.274804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1 \n2018-01-11 17:10:45.274812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y Y \n2018-01-11 17:10:45.274817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   Y Y \n2018-01-11 17:10:45.274826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-01-11 17:10:45.274832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\n2018-01-11 17:10:46.240862: F tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440] Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000\nAborted (core dumped)\n\nThoughts\nI think the problem is in python wrapper code located at contrib/cudnn_rnn/python/layers/cudnn_rnn.py.\nBoth _canonical_weight_shape(self, layer) and _canonical_bias_shape(self, layer) don't handle the case when input_mode='skip_input'. The wrapper code doesn't even check if input_size == num_units when input_mode='skip_input' as it should! The issue seems to be easy to fix, but I may be mistaken.\nHere is an example that shows that both 'skip_input' and 'linear_input' get the same set of canonical parameter shapes:\nCudnnGRU = lambda **kwargs: tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=7, direction='bidirectional', **kwargs)\n\nlayer = CudnnGRU(input_mode='skip_input')\nlayer.build((200, 16, 5))\nprint(\"skip_input weights\", layer.canonical_weight_shapes)\nprint(\"skip_input biases\", layer.canonical_bias_shapes)\n\nlayer = CudnnGRU(input_mode='linear_input')\nlayer.build((200, 16, 5))\nprint(\"linear_input_weights\", layer.canonical_weight_shapes)\nprint(\"linear_input_biases\", layer.canonical_bias_shapes)\n\nExample output:\nskip_input weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\nskip_input biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\nlinear_input_weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\nlinear_input_biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\n\nWorkaround\nIn our setup we decided that we will keep allocating opaque_params buffer manually with first querying its size with\n# Note: this code may require tf==1.4 to run.\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units=800, input_size=800,\n                                      input_mode='linear_input',\n                                      direction='bidirectional')\nwith tf.Session() as sess:\n    OPAQUE_BUFFER_SIZE = sess.run(layer.params_size())\n    print(OPAQUE_BUFFER_SIZE)\n\nBut this is really troublesome because it requires running the intermediate graph to get OPAQUE_BUFFER_SIZE before building the main graph.", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: `pip install tensorflow-gpu==1.5.0rc0`\r\n- **TensorFlow version**: v1.3.0-rc1-6090-g622487f 1.5.0-rc0\r\n- **Python version**:  3.5\r\n- **CUDA/cuDNN version**: CUDA 9.0, CuDNN 7.0.5\r\n- **GPU model and memory**: GTX 1080 8GB\r\n\r\n### Describe the problem\r\nWe are running tf.contrib.cudnn_rnn.CudnnGRU in our speech recognition setup with input_mode='skip_input' and it crashes the whole process. Here is the assertion error that we are getting:\r\n\r\n`Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000`\r\n\r\nLooks like it's crashing here: [tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L440). It does not crash if input_mode is 'linear_input'.\r\n\r\nHere is the minimal example to reproduce:\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=100,\r\n                             input_mode='skip_input', direction='bidirectional')\r\n# (time, batch_size, num_inputs)\r\nx = tf.random_normal((100, 16, 100))\r\ny = layer(x)\r\n\r\nwith tf.Session() as sess:\r\n\tsess.run(tf.global_variables_initializer())\r\n\tprint(sess.run(y))\r\n\r\n```\r\n\r\nLogs:\r\n\r\n```\r\n2018-01-11 17:10:44.858413: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-11 17:10:45.067854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-01-11 17:10:45.274222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-01-11 17:10:45.274776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\r\n2018-01-11 17:10:45.274804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1 \r\n2018-01-11 17:10:45.274812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y Y \r\n2018-01-11 17:10:45.274817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   Y Y \r\n2018-01-11 17:10:45.274826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-01-11 17:10:45.274832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2018-01-11 17:10:46.240862: F tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440] Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000\r\nAborted (core dumped)\r\n```\r\n\r\n### Thoughts\r\n\r\nI think the problem is in python wrapper code located at [contrib/cudnn_rnn/python/layers/cudnn_rnn.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py).\r\n\r\nBoth [`_canonical_weight_shape(self, layer)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L417) and [`_canonical_bias_shape(self, layer)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L443) don't handle the case when `input_mode='skip_input'`. The wrapper code doesn't even check if `input_size == num_units` when `input_mode='skip_input'` as it should! The issue seems to be easy to fix, but I may be mistaken.\r\n\r\nHere is an example that shows that both 'skip_input' and 'linear_input' get the same set of canonical parameter shapes:\r\n```\r\nCudnnGRU = lambda **kwargs: tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=7, direction='bidirectional', **kwargs)\r\n\r\nlayer = CudnnGRU(input_mode='skip_input')\r\nlayer.build((200, 16, 5))\r\nprint(\"skip_input weights\", layer.canonical_weight_shapes)\r\nprint(\"skip_input biases\", layer.canonical_bias_shapes)\r\n\r\nlayer = CudnnGRU(input_mode='linear_input')\r\nlayer.build((200, 16, 5))\r\nprint(\"linear_input_weights\", layer.canonical_weight_shapes)\r\nprint(\"linear_input_biases\", layer.canonical_bias_shapes)\r\n```\r\n\r\nExample output:\r\n```\r\nskip_input weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\r\nskip_input biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\r\nlinear_input_weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\r\nlinear_input_biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\r\n```\r\n\r\n### Workaround\r\n\r\nIn our setup we decided that we will keep allocating opaque_params buffer manually with first querying its size with \r\n```\r\n# Note: this code may require tf==1.4 to run.\r\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units=800, input_size=800,\r\n                                      input_mode='linear_input',\r\n                                      direction='bidirectional')\r\nwith tf.Session() as sess:\r\n    OPAQUE_BUFFER_SIZE = sess.run(layer.params_size())\r\n    print(OPAQUE_BUFFER_SIZE)\r\n```\r\n\r\nBut this is really troublesome because it requires running the intermediate graph to get OPAQUE_BUFFER_SIZE before building the main graph."}