{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16857", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16857/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16857/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16857/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16857", "id": 295418361, "node_id": "MDU6SXNzdWUyOTU0MTgzNjE=", "number": 16857, "title": "[BUG] seq2seq attention_wrapper use previous alignment?", "user": {"login": "LinJM", "id": 5347113, "node_id": "MDQ6VXNlcjUzNDcxMTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5347113?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LinJM", "html_url": "https://github.com/LinJM", "followers_url": "https://api.github.com/users/LinJM/followers", "following_url": "https://api.github.com/users/LinJM/following{/other_user}", "gists_url": "https://api.github.com/users/LinJM/gists{/gist_id}", "starred_url": "https://api.github.com/users/LinJM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LinJM/subscriptions", "organizations_url": "https://api.github.com/users/LinJM/orgs", "repos_url": "https://api.github.com/users/LinJM/repos", "events_url": "https://api.github.com/users/LinJM/events{/privacy}", "received_events_url": "https://api.github.com/users/LinJM/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-02-08T08:06:37Z", "updated_at": "2018-03-28T06:17:50Z", "closed_at": "2018-03-28T06:17:50Z", "author_association": "NONE", "body_html": "<p>When we use attention model, it's recommended to use previous alignment (attention weight).<br>\nBut I saw the code in the attention_wrapper.py, it ignore the previous alignment.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399</a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/5347113/35961865-202877be-0cea-11e8-8f88-38e638c8e3ad.png\"><img src=\"https://user-images.githubusercontent.com/5347113/35961865-202877be-0cea-11e8-8f88-38e638c8e3ad.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a></p>", "body_text": "When we use attention model, it's recommended to use previous alignment (attention weight).\nBut I saw the code in the attention_wrapper.py, it ignore the previous alignment.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399\n\n@ebrevdo", "body": "When we use attention model, it's recommended to use previous alignment (attention weight).\r\nBut I saw the code in the attention_wrapper.py, it ignore the previous alignment.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399\r\n\r\n![image](https://user-images.githubusercontent.com/5347113/35961865-202877be-0cea-11e8-8f88-38e638c8e3ad.png)\r\n\r\n@ebrevdo "}