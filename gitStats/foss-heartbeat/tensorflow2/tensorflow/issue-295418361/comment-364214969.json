{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364214969", "html_url": "https://github.com/tensorflow/tensorflow/issues/16857#issuecomment-364214969", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16857", "id": 364214969, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDIxNDk2OQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-08T19:04:12Z", "updated_at": "2018-02-08T19:04:12Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Thang will be able to say more; after the ICML deadline.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Thu, Feb 8, 2018 at 12:07 AM, Jeremy ***@***.***&gt; wrote:\n When we use attention model, it's recommended to use previous alignment\n (attention weight).\n Bug I saw the code in the attention_wrapper.py, it ignore the previous\n alignment.\n\n <a href=\"https://github.com/tensorflow/tensorflow/blob/master/\">https://github.com/tensorflow/tensorflow/blob/master/</a>\n tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399\n\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt;\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"295418361\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/16857\" href=\"https://github.com/tensorflow/tensorflow/issues/16857\">#16857</a>&gt;, or mute the\n thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimxuP55wWz8xABMRf02ZxR000bcEsks5tSqs5gaJpZM4R98Z9\">https://github.com/notifications/unsubscribe-auth/ABtimxuP55wWz8xABMRf02ZxR000bcEsks5tSqs5gaJpZM4R98Z9</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thang will be able to say more; after the ICML deadline.\n\u2026\nOn Thu, Feb 8, 2018 at 12:07 AM, Jeremy ***@***.***> wrote:\n When we use attention model, it's recommended to use previous alignment\n (attention weight).\n Bug I saw the code in the attention_wrapper.py, it ignore the previous\n alignment.\n\n https://github.com/tensorflow/tensorflow/blob/master/\n tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399\n\n @ebrevdo <https://github.com/ebrevdo>\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#16857>, or mute the\n thread\n <https://github.com/notifications/unsubscribe-auth/ABtimxuP55wWz8xABMRf02ZxR000bcEsks5tSqs5gaJpZM4R98Z9>\n .", "body": "Thang will be able to say more; after the ICML deadline.\n\nOn Thu, Feb 8, 2018 at 12:07 AM, Jeremy <notifications@github.com> wrote:\n\n> When we use attention model, it's recommended to use previous alignment\n> (attention weight).\n> Bug I saw the code in the attention_wrapper.py, it ignore the previous\n> alignment.\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L399\n>\n> @ebrevdo <https://github.com/ebrevdo>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16857>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxuP55wWz8xABMRf02ZxR000bcEsks5tSqs5gaJpZM4R98Z9>\n> .\n>\n"}