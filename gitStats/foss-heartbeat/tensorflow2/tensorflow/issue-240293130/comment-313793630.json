{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313793630", "html_url": "https://github.com/tensorflow/tensorflow/issues/11255#issuecomment-313793630", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11255", "id": 313793630, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzc5MzYzMA==", "user": {"login": "alexvays", "id": 29877472, "node_id": "MDQ6VXNlcjI5ODc3NDcy", "avatar_url": "https://avatars1.githubusercontent.com/u/29877472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexvays", "html_url": "https://github.com/alexvays", "followers_url": "https://api.github.com/users/alexvays/followers", "following_url": "https://api.github.com/users/alexvays/following{/other_user}", "gists_url": "https://api.github.com/users/alexvays/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexvays/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexvays/subscriptions", "organizations_url": "https://api.github.com/users/alexvays/orgs", "repos_url": "https://api.github.com/users/alexvays/repos", "events_url": "https://api.github.com/users/alexvays/events{/privacy}", "received_events_url": "https://api.github.com/users/alexvays/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T21:09:26Z", "updated_at": "2017-07-07T21:09:26Z", "author_association": "NONE", "body_html": "<p>The specific use case is:</p>\n<ul>\n<li>input data for prediction arrives at random points in time;</li>\n<li>DNNRegressor.predict() is invoked to generate prediction.<br>\nAs it stands, there is approximately 3-4 seconds latency for each prediction when running on Google App Engine flex with model_dir in a Google Storage bucket.  The latency seems to be to a substantial extent due to reloading the model from a gs:// path.   Given that data for prediction arrives in online mode (i.e. it's not available ahead of time), how can input_fn be written to return multiple batches?  The ultimate goal is to minimize latency when generating each prediction, from several seconds down to milliseconds.</li>\n</ul>", "body_text": "The specific use case is:\n\ninput data for prediction arrives at random points in time;\nDNNRegressor.predict() is invoked to generate prediction.\nAs it stands, there is approximately 3-4 seconds latency for each prediction when running on Google App Engine flex with model_dir in a Google Storage bucket.  The latency seems to be to a substantial extent due to reloading the model from a gs:// path.   Given that data for prediction arrives in online mode (i.e. it's not available ahead of time), how can input_fn be written to return multiple batches?  The ultimate goal is to minimize latency when generating each prediction, from several seconds down to milliseconds.", "body": "The specific use case is:\r\n- input data for prediction arrives at random points in time;\r\n- DNNRegressor.predict() is invoked to generate prediction.\r\nAs it stands, there is approximately 3-4 seconds latency for each prediction when running on Google App Engine flex with model_dir in a Google Storage bucket.  The latency seems to be to a substantial extent due to reloading the model from a gs:// path.   Given that data for prediction arrives in online mode (i.e. it's not available ahead of time), how can input_fn be written to return multiple batches?  The ultimate goal is to minimize latency when generating each prediction, from several seconds down to milliseconds."}