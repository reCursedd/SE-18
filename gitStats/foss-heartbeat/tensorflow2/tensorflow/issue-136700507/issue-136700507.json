{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1299", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1299/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1299/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1299/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1299", "id": 136700507, "node_id": "MDU6SXNzdWUxMzY3MDA1MDc=", "number": 1299, "title": "Convolution produces wrong result depending on the batch size.", "user": {"login": "cesarsalgado", "id": 1115209, "node_id": "MDQ6VXNlcjExMTUyMDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1115209?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cesarsalgado", "html_url": "https://github.com/cesarsalgado", "followers_url": "https://api.github.com/users/cesarsalgado/followers", "following_url": "https://api.github.com/users/cesarsalgado/following{/other_user}", "gists_url": "https://api.github.com/users/cesarsalgado/gists{/gist_id}", "starred_url": "https://api.github.com/users/cesarsalgado/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cesarsalgado/subscriptions", "organizations_url": "https://api.github.com/users/cesarsalgado/orgs", "repos_url": "https://api.github.com/users/cesarsalgado/repos", "events_url": "https://api.github.com/users/cesarsalgado/events{/privacy}", "received_events_url": "https://api.github.com/users/cesarsalgado/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-02-26T13:41:11Z", "updated_at": "2016-06-06T21:47:04Z", "closed_at": "2016-06-06T21:47:04Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04 LTS<br>\nGraphics: GeForce GTX 770/PCIe/SSE2<br>\nCuda compute capability: 3.0<br>\ncuda version: 7.0<br>\ncudnn version: 6.5</p>\n<p>installed from sources, commit hash: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/5a30c8f07ebc4817c16399416891bfc95304fda7/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/5a30c8f07ebc4817c16399416891bfc95304fda7\"><tt>5a30c8f</tt></a></p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Download the following numpy array in .npy format: <a href=\"https://www.dropbox.com/s/wljk6r83d0tee14/fail_tensor.npy?dl=0\" rel=\"nofollow\">https://www.dropbox.com/s/wljk6r83d0tee14/fail_tensor.npy?dl=0</a></li>\n<li>Below is shown the code to reproduce the problem. The numpy.float32 array stored in fail_tensor.npy has shape (4, 33, 33, 1). Only the first slice (a[0]) of the array holds the values that causes the problem. The values in the other slice can be any value. The original array with batch_size=4 will cause a problem, however if we use batch_size less than 4 (e.g., a[:3]), then there will be no problem. A convolution with a kernel of ones in the original array will produce a tensor with a negative number even though the input tensor has only non-negative values.</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> scipy.ndimage <span class=\"pl-k\">import</span> convolve\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">set_others</span>):\n    tt <span class=\"pl-k\">=</span> np.load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fail_tensor.npy<span class=\"pl-pds\">'</span></span>)\n    tt <span class=\"pl-k\">=</span> tt[:batch_size]\n    tt[<span class=\"pl-c1\">1</span>:,<span class=\"pl-c1\">...</span>] <span class=\"pl-k\">=</span> set_others\n    t <span class=\"pl-k\">=</span> tf.constant(tt)\n    kernel <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>])\n    r <span class=\"pl-k\">=</span> tf.nn.conv2d(t, kernel, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    <span class=\"pl-c1\">print</span> tt.shape\n    <span class=\"pl-c1\">print</span> tt[<span class=\"pl-c1\">0</span>].min()\n    rr <span class=\"pl-k\">=</span> r.eval(<span class=\"pl-v\">session</span><span class=\"pl-k\">=</span>sess)\n    <span class=\"pl-c1\">print</span> rr[<span class=\"pl-c1\">0</span>].min()\n    scipy_r <span class=\"pl-k\">=</span> convolve(tt[<span class=\"pl-c1\">0</span>,:,:,<span class=\"pl-c1\">0</span>], np.ones((<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>), np.float32), <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>constant<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">print</span> np.allclose(scipy_r, rr[<span class=\"pl-c1\">0</span>,:,:,<span class=\"pl-c1\">0</span>])\n    sess.close()</pre></div>\n<p>3: On IPython:</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">0</span>]: <span class=\"pl-k\">import</span> fail_example\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> with batch_size=3 the result is the same as scipy convolve.</span>\nIn [<span class=\"pl-c1\">1</span>]: fail_example.test(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">set_others</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">0.0</span>\n<span class=\"pl-c1\">4.47035e-08</span>\n<span class=\"pl-c1\">True</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> with batch_size=4 the result ISN'T the same as scipy convolve.</span>\nIn [<span class=\"pl-c1\">2</span>]: fail_example.test(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">set_others</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">0.0</span>\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.410374</span>\n<span class=\"pl-c1\">False</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Setting set_others to 100 doesn't change the result</span>\nIn [<span class=\"pl-c1\">3</span>]: fail_example.test(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">set_others</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>)\n(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">0.0</span>\n<span class=\"pl-c1\">4.47035e-08</span>\n<span class=\"pl-c1\">True</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Setting set_others to 100 doesn't change the result</span>\nIn [<span class=\"pl-c1\">4</span>]: fail_example.test(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">set_others</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>)\n(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">0.0</span>\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.410374</span>\n<span class=\"pl-c1\">False</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> smaller batch_sizes than 4 produces correct result</span>\nIn [<span class=\"pl-c1\">5</span>]: fail_example.test(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">set_others</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">0.0</span>\n<span class=\"pl-c1\">4.47035e-08</span>\n<span class=\"pl-c1\">True</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> smaller batch_sizes than 4 produces correct result</span>\nIn [<span class=\"pl-c1\">6</span>]: fail_example.test(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">set_others</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">0.0</span>\n<span class=\"pl-c1\">4.47035e-08</span>\n<span class=\"pl-c1\">True</span></pre></div>", "body_text": "Environment info\nOperating System: Ubuntu 14.04 LTS\nGraphics: GeForce GTX 770/PCIe/SSE2\nCuda compute capability: 3.0\ncuda version: 7.0\ncudnn version: 6.5\ninstalled from sources, commit hash: 5a30c8f\nSteps to reproduce\n\nDownload the following numpy array in .npy format: https://www.dropbox.com/s/wljk6r83d0tee14/fail_tensor.npy?dl=0\nBelow is shown the code to reproduce the problem. The numpy.float32 array stored in fail_tensor.npy has shape (4, 33, 33, 1). Only the first slice (a[0]) of the array holds the values that causes the problem. The values in the other slice can be any value. The original array with batch_size=4 will cause a problem, however if we use batch_size less than 4 (e.g., a[:3]), then there will be no problem. A convolution with a kernel of ones in the original array will produce a tensor with a negative number even though the input tensor has only non-negative values.\n\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.ndimage import convolve\n\ndef test(batch_size, set_others):\n    tt = np.load('fail_tensor.npy')\n    tt = tt[:batch_size]\n    tt[1:,...] = set_others\n    t = tf.constant(tt)\n    kernel = tf.constant(1.0, shape=[5, 5, 1, 1])\n    r = tf.nn.conv2d(t, kernel, [1, 1, 1, 1], padding='SAME')\n    sess = tf.Session()\n    print tt.shape\n    print tt[0].min()\n    rr = r.eval(session=sess)\n    print rr[0].min()\n    scipy_r = convolve(tt[0,:,:,0], np.ones((5,5), np.float32), mode='constant')\n    print np.allclose(scipy_r, rr[0,:,:,0])\n    sess.close()\n3: On IPython:\nIn [0]: import fail_example\n\n# with batch_size=3 the result is the same as scipy convolve.\nIn [1]: fail_example.test(batch_size=3, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# with batch_size=4 the result ISN'T the same as scipy convolve.\nIn [2]: fail_example.test(batch_size=4, set_others=0)\n(4, 33, 33, 1)\n0.0\n-0.410374\nFalse\n\n# Setting set_others to 100 doesn't change the result\nIn [3]: fail_example.test(batch_size=3, set_others=100)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# Setting set_others to 100 doesn't change the result\nIn [4]: fail_example.test(batch_size=4, set_others=100)\n(4, 33, 33, 1)\n0.0\n-0.410374\nFalse\n\n# smaller batch_sizes than 4 produces correct result\nIn [5]: fail_example.test(batch_size=2, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# smaller batch_sizes than 4 produces correct result\nIn [6]: fail_example.test(batch_size=1, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 LTS\nGraphics: GeForce GTX 770/PCIe/SSE2\nCuda compute capability: 3.0\ncuda version: 7.0\ncudnn version: 6.5\n\ninstalled from sources, commit hash: 5a30c8f07\n### Steps to reproduce\n1. Download the following numpy array in .npy format: https://www.dropbox.com/s/wljk6r83d0tee14/fail_tensor.npy?dl=0\n2. Below is shown the code to reproduce the problem. The numpy.float32 array stored in fail_tensor.npy has shape (4, 33, 33, 1). Only the first slice (a[0]) of the array holds the values that causes the problem. The values in the other slice can be any value. The original array with batch_size=4 will cause a problem, however if we use batch_size less than 4 (e.g., a[:3]), then there will be no problem. A convolution with a kernel of ones in the original array will produce a tensor with a negative number even though the input tensor has only non-negative values.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.ndimage import convolve\n\ndef test(batch_size, set_others):\n    tt = np.load('fail_tensor.npy')\n    tt = tt[:batch_size]\n    tt[1:,...] = set_others\n    t = tf.constant(tt)\n    kernel = tf.constant(1.0, shape=[5, 5, 1, 1])\n    r = tf.nn.conv2d(t, kernel, [1, 1, 1, 1], padding='SAME')\n    sess = tf.Session()\n    print tt.shape\n    print tt[0].min()\n    rr = r.eval(session=sess)\n    print rr[0].min()\n    scipy_r = convolve(tt[0,:,:,0], np.ones((5,5), np.float32), mode='constant')\n    print np.allclose(scipy_r, rr[0,:,:,0])\n    sess.close()\n```\n\n3: On IPython:\n\n``` python\nIn [0]: import fail_example\n\n# with batch_size=3 the result is the same as scipy convolve.\nIn [1]: fail_example.test(batch_size=3, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# with batch_size=4 the result ISN'T the same as scipy convolve.\nIn [2]: fail_example.test(batch_size=4, set_others=0)\n(4, 33, 33, 1)\n0.0\n-0.410374\nFalse\n\n# Setting set_others to 100 doesn't change the result\nIn [3]: fail_example.test(batch_size=3, set_others=100)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# Setting set_others to 100 doesn't change the result\nIn [4]: fail_example.test(batch_size=4, set_others=100)\n(4, 33, 33, 1)\n0.0\n-0.410374\nFalse\n\n# smaller batch_sizes than 4 produces correct result\nIn [5]: fail_example.test(batch_size=2, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n\n# smaller batch_sizes than 4 produces correct result\nIn [6]: fail_example.test(batch_size=1, set_others=0)\n(3, 33, 33, 1)\n0.0\n4.47035e-08\nTrue\n```\n"}