{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/261746089", "html_url": "https://github.com/tensorflow/tensorflow/issues/2922#issuecomment-261746089", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2922", "id": 261746089, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTc0NjA4OQ==", "user": {"login": "ilblackdragon", "id": 175486, "node_id": "MDQ6VXNlcjE3NTQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/175486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilblackdragon", "html_url": "https://github.com/ilblackdragon", "followers_url": "https://api.github.com/users/ilblackdragon/followers", "following_url": "https://api.github.com/users/ilblackdragon/following{/other_user}", "gists_url": "https://api.github.com/users/ilblackdragon/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilblackdragon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilblackdragon/subscriptions", "organizations_url": "https://api.github.com/users/ilblackdragon/orgs", "repos_url": "https://api.github.com/users/ilblackdragon/repos", "events_url": "https://api.github.com/users/ilblackdragon/events{/privacy}", "received_events_url": "https://api.github.com/users/ilblackdragon/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-19T23:11:19Z", "updated_at": "2016-11-19T23:11:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Oh, this should work:</p>\n<pre><code>def get_optimizer():\n    def exp_decay(global_step):\n      return tf.train.exponential_decay(\n          learning_rate=0.00001, global_step=global_step,\n          decay_steps=100, decay_rate=0.97, staircase=True)\n    # use customized decay function in learning_rate\n    return tf.train.AdagradOptimizer(\n         learning_rate=exp_decay(tf.contrib.framework.get_global_step()))\n</code></pre>\n<p>or just:</p>\n<pre><code>def get_optimizer():\n    return tf.train.AdagradOptimizer(\n          learning_rate=tf.train.exponential_decay(\n               learning_rate=0.00001, global_step=tf.contrib.framework.get_global_step(),\n               decay_steps=100, decay_rate=0.97, staircase=True))\n</code></pre>", "body_text": "Oh, this should work:\ndef get_optimizer():\n    def exp_decay(global_step):\n      return tf.train.exponential_decay(\n          learning_rate=0.00001, global_step=global_step,\n          decay_steps=100, decay_rate=0.97, staircase=True)\n    # use customized decay function in learning_rate\n    return tf.train.AdagradOptimizer(\n         learning_rate=exp_decay(tf.contrib.framework.get_global_step()))\n\nor just:\ndef get_optimizer():\n    return tf.train.AdagradOptimizer(\n          learning_rate=tf.train.exponential_decay(\n               learning_rate=0.00001, global_step=tf.contrib.framework.get_global_step(),\n               decay_steps=100, decay_rate=0.97, staircase=True))", "body": "Oh, this should work:\n\n```\ndef get_optimizer():\n    def exp_decay(global_step):\n      return tf.train.exponential_decay(\n          learning_rate=0.00001, global_step=global_step,\n          decay_steps=100, decay_rate=0.97, staircase=True)\n    # use customized decay function in learning_rate\n    return tf.train.AdagradOptimizer(\n         learning_rate=exp_decay(tf.contrib.framework.get_global_step()))\n```\n\nor just:\n\n```\ndef get_optimizer():\n    return tf.train.AdagradOptimizer(\n          learning_rate=tf.train.exponential_decay(\n               learning_rate=0.00001, global_step=tf.contrib.framework.get_global_step(),\n               decay_steps=100, decay_rate=0.97, staircase=True))\n```\n"}