{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23633", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23633/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23633/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23633/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23633", "id": 379196307, "node_id": "MDU6SXNzdWUzNzkxOTYzMDc=", "number": 23633, "title": "Distributed training with TF under BSP does not strictly follow the synchrnous rule", "user": {"login": "Steamgjk", "id": 7100589, "node_id": "MDQ6VXNlcjcxMDA1ODk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7100589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Steamgjk", "html_url": "https://github.com/Steamgjk", "followers_url": "https://api.github.com/users/Steamgjk/followers", "following_url": "https://api.github.com/users/Steamgjk/following{/other_user}", "gists_url": "https://api.github.com/users/Steamgjk/gists{/gist_id}", "starred_url": "https://api.github.com/users/Steamgjk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Steamgjk/subscriptions", "organizations_url": "https://api.github.com/users/Steamgjk/orgs", "repos_url": "https://api.github.com/users/Steamgjk/repos", "events_url": "https://api.github.com/users/Steamgjk/events{/privacy}", "received_events_url": "https://api.github.com/users/Steamgjk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 996845227, "node_id": "MDU6TGFiZWw5OTY4NDUyMjc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:dist-strat", "name": "comp:dist-strat", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "josh11b", "id": 15258583, "node_id": "MDQ6VXNlcjE1MjU4NTgz", "avatar_url": "https://avatars0.githubusercontent.com/u/15258583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/josh11b", "html_url": "https://github.com/josh11b", "followers_url": "https://api.github.com/users/josh11b/followers", "following_url": "https://api.github.com/users/josh11b/following{/other_user}", "gists_url": "https://api.github.com/users/josh11b/gists{/gist_id}", "starred_url": "https://api.github.com/users/josh11b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/josh11b/subscriptions", "organizations_url": "https://api.github.com/users/josh11b/orgs", "repos_url": "https://api.github.com/users/josh11b/repos", "events_url": "https://api.github.com/users/josh11b/events{/privacy}", "received_events_url": "https://api.github.com/users/josh11b/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "josh11b", "id": 15258583, "node_id": "MDQ6VXNlcjE1MjU4NTgz", "avatar_url": "https://avatars0.githubusercontent.com/u/15258583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/josh11b", "html_url": "https://github.com/josh11b", "followers_url": "https://api.github.com/users/josh11b/followers", "following_url": "https://api.github.com/users/josh11b/following{/other_user}", "gists_url": "https://api.github.com/users/josh11b/gists{/gist_id}", "starred_url": "https://api.github.com/users/josh11b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/josh11b/subscriptions", "organizations_url": "https://api.github.com/users/josh11b/orgs", "repos_url": "https://api.github.com/users/josh11b/repos", "events_url": "https://api.github.com/users/josh11b/events{/privacy}", "received_events_url": "https://api.github.com/users/josh11b/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-11-09T15:02:30Z", "updated_at": "2018-11-20T07:43:37Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a bug. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no experiments on mobile devices</li>\n<li>TensorFlow installed from (source or binary): source</li>\n<li>TensorFlow version (use command below): r1.10</li>\n<li>Python version: 2.7</li>\n<li>Bazel version (if compiling from source): 0.16.1</li>\n<li>GCC/Compiler version (if compiling from source): GCC</li>\n<li>CUDA/cuDNN version: 9.0</li>\n<li>GPU model and memory: ??</li>\n</ul>\n<p>You can collect some of this information using our environment capture <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">script</a><br>\nYou can also obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<p><strong>Describe the current behavior</strong><br>\nThe distributed training is conducted with one PS and three workers. The replica_sync is True.  I deliberately slow down one worker (add some sleep in the program of the worker), then I find the other workers can surpass the slow worker by 1~3 iterations<br>\n<strong>Describe the expected behavior</strong><br>\nSince TF follows BSP (it does not support SSP, right?), these workers should be always  under the same iteration.  When slow worker still stays in Iteration 1, the other workers cannot go to Iteration 3 or further.<br>\n<strong>Code to reproduce the issue</strong><br>\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.<br>\nAny Distributed training program should be okay. Just add some sleep in the python code or the rdma.cc file<br>\n<strong>Other info / logs</strong><br>\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.</p>", "body_text": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no experiments on mobile devices\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): r1.10\nPython version: 2.7\nBazel version (if compiling from source): 0.16.1\nGCC/Compiler version (if compiling from source): GCC\nCUDA/cuDNN version: 9.0\nGPU model and memory: ??\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the current behavior\nThe distributed training is conducted with one PS and three workers. The replica_sync is True.  I deliberately slow down one worker (add some sleep in the program of the worker), then I find the other workers can surpass the slow worker by 1~3 iterations\nDescribe the expected behavior\nSince TF follows BSP (it does not support SSP, right?), these workers should be always  under the same iteration.  When slow worker still stays in Iteration 1, the other workers cannot go to Iteration 3 or further.\nCode to reproduce the issue\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\nAny Distributed training program should be okay. Just add some sleep in the python code or the rdma.cc file\nOther info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no experiments on mobile devices\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): r1.10\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): GCC\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: ??\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nThe distributed training is conducted with one PS and three workers. The replica_sync is True.  I deliberately slow down one worker (add some sleep in the program of the worker), then I find the other workers can surpass the slow worker by 1~3 iterations \r\n**Describe the expected behavior**\r\nSince TF follows BSP (it does not support SSP, right?), these workers should be always  under the same iteration.  When slow worker still stays in Iteration 1, the other workers cannot go to Iteration 3 or further. \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nAny Distributed training program should be okay. Just add some sleep in the python code or the rdma.cc file\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n"}