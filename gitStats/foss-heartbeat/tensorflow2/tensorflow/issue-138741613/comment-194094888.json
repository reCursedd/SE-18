{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/194094888", "html_url": "https://github.com/tensorflow/tensorflow/issues/1396#issuecomment-194094888", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1396", "id": 194094888, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NDA5NDg4OA==", "user": {"login": "shawnLeeZX", "id": 2428233, "node_id": "MDQ6VXNlcjI0MjgyMzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2428233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shawnLeeZX", "html_url": "https://github.com/shawnLeeZX", "followers_url": "https://api.github.com/users/shawnLeeZX/followers", "following_url": "https://api.github.com/users/shawnLeeZX/following{/other_user}", "gists_url": "https://api.github.com/users/shawnLeeZX/gists{/gist_id}", "starred_url": "https://api.github.com/users/shawnLeeZX/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shawnLeeZX/subscriptions", "organizations_url": "https://api.github.com/users/shawnLeeZX/orgs", "repos_url": "https://api.github.com/users/shawnLeeZX/repos", "events_url": "https://api.github.com/users/shawnLeeZX/events{/privacy}", "received_events_url": "https://api.github.com/users/shawnLeeZX/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-09T03:18:45Z", "updated_at": "2016-03-09T03:21:15Z", "author_association": "NONE", "body_html": "<p>First I have to confess I did not notice the OpError class before. Now I could<br>\nget the program log something before it quits. Thanks for pointing it out.</p>\n<p>Then I thought a little why I did not notice this, and think this may could be<br>\na small improvement usability. I will elaborate below.</p>\n<p>The error with more context is:</p>\n<div class=\"highlight highlight-source-shell\"><pre>I0309 09:26:19.824918 21189 survivors.py:282] Step 100: loss = nan lr = 0.50000 (359.3 examples/sec 0.356 sec/batch)\nW tensorflow/core/common_runtime/executor.cc:1102] 0x2c1c670 Compute status: Out of range: Nan <span class=\"pl-k\">in</span> summary histogram for: maxout-relu-cifar10/conv1/HistogramSummary_2\n     [[Node: maxout-relu-cifar10/conv1/HistogramSummary_2 <span class=\"pl-k\">=</span> HistogramSummary[T<span class=\"pl-k\">=</span>DT_FLOAT, _device<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/job:localhost/replica:0/task:0/cpu:0<span class=\"pl-pds\">\"</span></span>](maxout-relu-cifar10/conv1/HistogramSummary_2/tag, maxout-relu-cifar10/conv1/biases/read/_470)]]</pre></div>\n<p>I got the same log whether I catch the error or not. For a typical python<br>\nprogram, when an error is raised, it would print out something like</p>\n<pre><code>Traceback (most recent call last):\n  File \"divide_by_zero.py\", line 1, in &lt;module&gt;\n    1/0\nZeroDivisionError: integer division or modulo by zero\n</code></pre>\n<p>If I got something report like it, I would notice I could have error to<br>\ncatch(it seems a little unreasonble to ask user to read what errors one may get<br>\nbefore coding, since python is a lot about interaction).</p>\n<p>So it would be great something could be done on this.</p>\n<hr>\n<p>Lastly, I have a side question, in what cases I could get a NaN number? How<br>\ncould I know?</p>\n<p>In this case, this happens when the learning rate is too larger. I guess it is<br>\nbecause the activation value goes out of range(or some bug I do not know in the<br>\nconvolution op, though I got similar errors in ReLU op as well. Same large learning rate reason).</p>\n<p>I think I could do a denser logging or summary to know what actually happens,<br>\nbut it would much more convenient when a NaN number is taken as an exception,<br>\nsince no futher ops can do anything about it(I could file another issue about<br>\nthis if the team prefers).</p>", "body_text": "First I have to confess I did not notice the OpError class before. Now I could\nget the program log something before it quits. Thanks for pointing it out.\nThen I thought a little why I did not notice this, and think this may could be\na small improvement usability. I will elaborate below.\nThe error with more context is:\nI0309 09:26:19.824918 21189 survivors.py:282] Step 100: loss = nan lr = 0.50000 (359.3 examples/sec 0.356 sec/batch)\nW tensorflow/core/common_runtime/executor.cc:1102] 0x2c1c670 Compute status: Out of range: Nan in summary histogram for: maxout-relu-cifar10/conv1/HistogramSummary_2\n     [[Node: maxout-relu-cifar10/conv1/HistogramSummary_2 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](maxout-relu-cifar10/conv1/HistogramSummary_2/tag, maxout-relu-cifar10/conv1/biases/read/_470)]]\nI got the same log whether I catch the error or not. For a typical python\nprogram, when an error is raised, it would print out something like\nTraceback (most recent call last):\n  File \"divide_by_zero.py\", line 1, in <module>\n    1/0\nZeroDivisionError: integer division or modulo by zero\n\nIf I got something report like it, I would notice I could have error to\ncatch(it seems a little unreasonble to ask user to read what errors one may get\nbefore coding, since python is a lot about interaction).\nSo it would be great something could be done on this.\n\nLastly, I have a side question, in what cases I could get a NaN number? How\ncould I know?\nIn this case, this happens when the learning rate is too larger. I guess it is\nbecause the activation value goes out of range(or some bug I do not know in the\nconvolution op, though I got similar errors in ReLU op as well. Same large learning rate reason).\nI think I could do a denser logging or summary to know what actually happens,\nbut it would much more convenient when a NaN number is taken as an exception,\nsince no futher ops can do anything about it(I could file another issue about\nthis if the team prefers).", "body": "First I have to confess I did not notice the OpError class before. Now I could\nget the program log something before it quits. Thanks for pointing it out.\n\nThen I thought a little why I did not notice this, and think this may could be\na small improvement usability. I will elaborate below.\n\nThe error with more context is:\n\n``` bash\nI0309 09:26:19.824918 21189 survivors.py:282] Step 100: loss = nan lr = 0.50000 (359.3 examples/sec 0.356 sec/batch)\nW tensorflow/core/common_runtime/executor.cc:1102] 0x2c1c670 Compute status: Out of range: Nan in summary histogram for: maxout-relu-cifar10/conv1/HistogramSummary_2\n     [[Node: maxout-relu-cifar10/conv1/HistogramSummary_2 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](maxout-relu-cifar10/conv1/HistogramSummary_2/tag, maxout-relu-cifar10/conv1/biases/read/_470)]]\n```\n\nI got the same log whether I catch the error or not. For a typical python\nprogram, when an error is raised, it would print out something like\n\n```\nTraceback (most recent call last):\n  File \"divide_by_zero.py\", line 1, in <module>\n    1/0\nZeroDivisionError: integer division or modulo by zero\n```\n\nIf I got something report like it, I would notice I could have error to\ncatch(it seems a little unreasonble to ask user to read what errors one may get\nbefore coding, since python is a lot about interaction).\n\nSo it would be great something could be done on this.\n\n---\n\nLastly, I have a side question, in what cases I could get a NaN number? How\ncould I know?\n\nIn this case, this happens when the learning rate is too larger. I guess it is\nbecause the activation value goes out of range(or some bug I do not know in the\nconvolution op, though I got similar errors in ReLU op as well. Same large learning rate reason).\n\nI think I could do a denser logging or summary to know what actually happens,\nbut it would much more convenient when a NaN number is taken as an exception,\nsince no futher ops can do anything about it(I could file another issue about\nthis if the team prefers).\n"}