{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19272", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19272/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19272/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19272/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19272", "id": 322880502, "node_id": "MDU6SXNzdWUzMjI4ODA1MDI=", "number": 19272, "title": "Gradient Penalty won't execute", "user": {"login": "sarthmit", "id": 19748754, "node_id": "MDQ6VXNlcjE5NzQ4NzU0", "avatar_url": "https://avatars3.githubusercontent.com/u/19748754?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sarthmit", "html_url": "https://github.com/sarthmit", "followers_url": "https://api.github.com/users/sarthmit/followers", "following_url": "https://api.github.com/users/sarthmit/following{/other_user}", "gists_url": "https://api.github.com/users/sarthmit/gists{/gist_id}", "starred_url": "https://api.github.com/users/sarthmit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sarthmit/subscriptions", "organizations_url": "https://api.github.com/users/sarthmit/orgs", "repos_url": "https://api.github.com/users/sarthmit/repos", "events_url": "https://api.github.com/users/sarthmit/events{/privacy}", "received_events_url": "https://api.github.com/users/sarthmit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-05-14T16:01:41Z", "updated_at": "2018-11-14T19:18:24Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (pip install inside conda)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>: 2.7.9</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0 / 7</li>\n<li><strong>GPU model and memory</strong>: Nvidia Titan X</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>I use the following code for Gradient Penalty. The code works fine without gradient penalty but with gradient penalty, the code hangs at the mentioned line (mentioned with -------&gt; ). For some reason, incorporating the gradient in the loss term isn't letting me take the gradient. Is it a bug?</p>\n<pre><code>def dropout(inp):\n        return tf.nn.dropout(inp,0.8)\n\ndef mnist_svhn(X,noise,reuse=False):\n    with tf.variable_scope(\"Encoder/mnist-svhn\",reuse=reuse,initializer=initializer):\n        h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))\n        h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))\n        h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))\n        h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))\n        h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,3,1)))\n        h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))\n        h6 = tf.layers.conv2d(h5,256,1,1)\n\n        h7 = tf.concat([h6,noise],axis=-1)\n\n        h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,4,1)))\n        h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))\n        h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))\n        h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))\n        h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,3,5,1)))\n        h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))\n        h14 = tf.layers.conv2d_transpose(h13,3,1,1,activation=tf.nn.sigmoid)\n\n    print h14\n\ndef svhn_mnist(X,noise,reuse=False):\n    with tf.variable_scope(\"Encoder/svhn-mnist\",reuse=reuse,initializer=initializer):\n        h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))\n        h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))\n        h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))\n        h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))\n        h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,4,1)))\n        h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))\n        h6 = tf.layers.conv2d(h5,256,1,1)\n\n        h7 = tf.concat([h6,noise],axis=-1)\n\n        h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,3,1)))\n        h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))\n        h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))\n        h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))\n        h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,32,5,1)))\n        h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))\n        h14 = tf.layers.conv2d_transpose(h13,1,1,1,activation=tf.nn.sigmoid)\n\n    return h14\n\ndef discriminator(X,y,reuse=False):\n    with tf.variable_scope(\"Discriminator\",reuse=reuse,initializer=initializer):\n        d0 = dropout(lrelu(tf.layers.conv2d(X,32,5,1)))\n        d1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d0,64,4,2))))\n        d2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d1,128,4,1))))\n        d3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d2,256,4,2))))\n        d4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d3,512,4,1))))\n\n        h0 = dropout(lrelu(tf.layers.conv2d(y,32,5,1)))\n        h1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2))))\n        h2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1))))\n        h3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2))))\n        h4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,512,3,1))))\n\n        final = tf.concat([d4,h4],axis=-1)\n\n        f0 = dropout(lrelu(tf.layers.conv2d(final,1024,1,1)))\n        f1 = dropout(lrelu(tf.layers.conv2d(f0,1024,1,1)))\n        f2 = dropout(tf.layers.conv2d(f1,1,1,1))\n\n    return tf.reshape(f2,[-1,1])\n\ndef sig_loss(a,b):\n    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=a,labels=b),axis=1)\n\ndef get_vars(name):\n    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=name)\n\ndef l2_loss(a,b):\n    return tf.reduce_sum(tf.square(a-b),axis=1)\n\nX_mnist = tf.placeholder(tf.float32,[None,28,28,1])\nX_svhn = tf.placeholder(tf.float32,[None,32,32,3])\n\nepsilon = tf.placeholder(tf.float32,[None,1])\n\nz_mnist = tf.placeholder(tf.float32,[None,1,1,32])\nz_svhn = tf.placeholder(tf.float32,[None,1,1,32])\n\nXtr_svhn = mnist_svhn(X_mnist,z_mnist)\nXtr_mnist = svhn_mnist(X_svhn,z_svhn)\n\nsv = discriminator(X_svhn,Xtr_mnist)\nmn = discriminator(Xtr_svhn,X_mnist,True)\n\nG_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)\nD_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)\n\nx_hat = epsilon * X_svhn + (1-epsilon) * Xtr_svhn\ngrad_1 = tf.gradients(discriminator(x_hat,svhn_mnist(x_hat,z_svhn,True),True),x_hat)[0]\nx_hat = epsilon * X_mnist + (1-epsilon) * Xtr_mnist\ngrad_2 = tf.gradients(discriminator(mnist_svhn(x_hat,z_mnist,True),x_hat,True),x_hat)[0]\n\nslopes_1 = tf.sqrt(tf.reduce_sum(tf.square(grad_1),reduction_indices=-1))\nslopes_2 = tf.sqrt(tf.reduce_sum(tf.square(grad_2),reduction_indices=-1))\ngrad_penalty = tf.reduce_mean((slopes_1 - 1.)**2) * 5\ngrad_penalty += tf.reduce_mean((slopes_2 - 1.)**2) * 5\n\ndisc_loss = tf.reduce_mean(sig_loss(sv,tf.ones_like(sv))) + \\\n            tf.reduce_mean(sig_loss(mn,tf.zeros_like(mn))) + grad_penalty\n\ngen_loss = tf.reduce_mean(sig_loss(mn,tf.ones_like(mn))) + \\\n            tf.reduce_mean(sig_loss(sv,tf.zeros_like(sv)))\n\ng_grad = G_opt.compute_gradients(gen_loss,var_list=get_vars(\"Encoder\"),colocate_gradient_with_ops=True)\n-------&gt; d_grad = D_opt.compute_gradients(disc_loss,var_list=get_vars(\"Discriminator\"),colocate_gradient_with_ops=True)\n\nd_step = D_opt.apply_gradients(d_grad)\ng_step = G_opt.apply_gradients(g_grad)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary (pip install inside conda)\nTensorFlow version (use command below): 1.8.0\nPython version: 2.7.9\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0 / 7\nGPU model and memory: Nvidia Titan X\nExact command to reproduce:\n\nI use the following code for Gradient Penalty. The code works fine without gradient penalty but with gradient penalty, the code hangs at the mentioned line (mentioned with -------> ). For some reason, incorporating the gradient in the loss term isn't letting me take the gradient. Is it a bug?\ndef dropout(inp):\n        return tf.nn.dropout(inp,0.8)\n\ndef mnist_svhn(X,noise,reuse=False):\n    with tf.variable_scope(\"Encoder/mnist-svhn\",reuse=reuse,initializer=initializer):\n        h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))\n        h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))\n        h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))\n        h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))\n        h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,3,1)))\n        h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))\n        h6 = tf.layers.conv2d(h5,256,1,1)\n\n        h7 = tf.concat([h6,noise],axis=-1)\n\n        h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,4,1)))\n        h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))\n        h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))\n        h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))\n        h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,3,5,1)))\n        h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))\n        h14 = tf.layers.conv2d_transpose(h13,3,1,1,activation=tf.nn.sigmoid)\n\n    print h14\n\ndef svhn_mnist(X,noise,reuse=False):\n    with tf.variable_scope(\"Encoder/svhn-mnist\",reuse=reuse,initializer=initializer):\n        h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))\n        h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))\n        h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))\n        h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))\n        h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,4,1)))\n        h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))\n        h6 = tf.layers.conv2d(h5,256,1,1)\n\n        h7 = tf.concat([h6,noise],axis=-1)\n\n        h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,3,1)))\n        h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))\n        h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))\n        h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))\n        h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,32,5,1)))\n        h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))\n        h14 = tf.layers.conv2d_transpose(h13,1,1,1,activation=tf.nn.sigmoid)\n\n    return h14\n\ndef discriminator(X,y,reuse=False):\n    with tf.variable_scope(\"Discriminator\",reuse=reuse,initializer=initializer):\n        d0 = dropout(lrelu(tf.layers.conv2d(X,32,5,1)))\n        d1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d0,64,4,2))))\n        d2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d1,128,4,1))))\n        d3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d2,256,4,2))))\n        d4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d3,512,4,1))))\n\n        h0 = dropout(lrelu(tf.layers.conv2d(y,32,5,1)))\n        h1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2))))\n        h2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1))))\n        h3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2))))\n        h4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,512,3,1))))\n\n        final = tf.concat([d4,h4],axis=-1)\n\n        f0 = dropout(lrelu(tf.layers.conv2d(final,1024,1,1)))\n        f1 = dropout(lrelu(tf.layers.conv2d(f0,1024,1,1)))\n        f2 = dropout(tf.layers.conv2d(f1,1,1,1))\n\n    return tf.reshape(f2,[-1,1])\n\ndef sig_loss(a,b):\n    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=a,labels=b),axis=1)\n\ndef get_vars(name):\n    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=name)\n\ndef l2_loss(a,b):\n    return tf.reduce_sum(tf.square(a-b),axis=1)\n\nX_mnist = tf.placeholder(tf.float32,[None,28,28,1])\nX_svhn = tf.placeholder(tf.float32,[None,32,32,3])\n\nepsilon = tf.placeholder(tf.float32,[None,1])\n\nz_mnist = tf.placeholder(tf.float32,[None,1,1,32])\nz_svhn = tf.placeholder(tf.float32,[None,1,1,32])\n\nXtr_svhn = mnist_svhn(X_mnist,z_mnist)\nXtr_mnist = svhn_mnist(X_svhn,z_svhn)\n\nsv = discriminator(X_svhn,Xtr_mnist)\nmn = discriminator(Xtr_svhn,X_mnist,True)\n\nG_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)\nD_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)\n\nx_hat = epsilon * X_svhn + (1-epsilon) * Xtr_svhn\ngrad_1 = tf.gradients(discriminator(x_hat,svhn_mnist(x_hat,z_svhn,True),True),x_hat)[0]\nx_hat = epsilon * X_mnist + (1-epsilon) * Xtr_mnist\ngrad_2 = tf.gradients(discriminator(mnist_svhn(x_hat,z_mnist,True),x_hat,True),x_hat)[0]\n\nslopes_1 = tf.sqrt(tf.reduce_sum(tf.square(grad_1),reduction_indices=-1))\nslopes_2 = tf.sqrt(tf.reduce_sum(tf.square(grad_2),reduction_indices=-1))\ngrad_penalty = tf.reduce_mean((slopes_1 - 1.)**2) * 5\ngrad_penalty += tf.reduce_mean((slopes_2 - 1.)**2) * 5\n\ndisc_loss = tf.reduce_mean(sig_loss(sv,tf.ones_like(sv))) + \\\n            tf.reduce_mean(sig_loss(mn,tf.zeros_like(mn))) + grad_penalty\n\ngen_loss = tf.reduce_mean(sig_loss(mn,tf.ones_like(mn))) + \\\n            tf.reduce_mean(sig_loss(sv,tf.zeros_like(sv)))\n\ng_grad = G_opt.compute_gradients(gen_loss,var_list=get_vars(\"Encoder\"),colocate_gradient_with_ops=True)\n-------> d_grad = D_opt.compute_gradients(disc_loss,var_list=get_vars(\"Discriminator\"),colocate_gradient_with_ops=True)\n\nd_step = D_opt.apply_gradients(d_grad)\ng_step = G_opt.apply_gradients(g_grad)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (pip install inside conda)\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n- **GPU model and memory**: Nvidia Titan X\r\n- **Exact command to reproduce**:\r\n\r\nI use the following code for Gradient Penalty. The code works fine without gradient penalty but with gradient penalty, the code hangs at the mentioned line (mentioned with -------> ). For some reason, incorporating the gradient in the loss term isn't letting me take the gradient. Is it a bug?\r\n\r\n    def dropout(inp):\r\n            return tf.nn.dropout(inp,0.8)\r\n\r\n    def mnist_svhn(X,noise,reuse=False):\r\n        with tf.variable_scope(\"Encoder/mnist-svhn\",reuse=reuse,initializer=initializer):\r\n            h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))\r\n            h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))\r\n            h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))\r\n            h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))\r\n            h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,3,1)))\r\n            h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))\r\n            h6 = tf.layers.conv2d(h5,256,1,1)\r\n\r\n            h7 = tf.concat([h6,noise],axis=-1)\r\n\r\n            h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,4,1)))\r\n            h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))\r\n            h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))\r\n            h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))\r\n            h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,3,5,1)))\r\n            h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))\r\n            h14 = tf.layers.conv2d_transpose(h13,3,1,1,activation=tf.nn.sigmoid)\r\n\r\n        print h14\r\n\r\n    def svhn_mnist(X,noise,reuse=False):\r\n        with tf.variable_scope(\"Encoder/svhn-mnist\",reuse=reuse,initializer=initializer):\r\n            h0 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(X,32,5,1)))\r\n            h1 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2)))\r\n            h2 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1)))\r\n            h3 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2)))\r\n            h4 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,256,4,1)))\r\n            h5 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h4,256,1,1)))\r\n            h6 = tf.layers.conv2d(h5,256,1,1)\r\n\r\n            h7 = tf.concat([h6,noise],axis=-1)\r\n\r\n            h8 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h7,256,3,1)))\r\n            h9 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h8,128,4,2)))\r\n            h10 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h9,64,4,1)))\r\n            h11 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h10,32,4,2)))\r\n            h12 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h11,32,5,1)))\r\n            h13 = lrelu(tf.layers.batch_normalization(tf.layers.conv2d_transpose(h12,32,1,1)))\r\n            h14 = tf.layers.conv2d_transpose(h13,1,1,1,activation=tf.nn.sigmoid)\r\n\r\n        return h14\r\n\r\n    def discriminator(X,y,reuse=False):\r\n        with tf.variable_scope(\"Discriminator\",reuse=reuse,initializer=initializer):\r\n            d0 = dropout(lrelu(tf.layers.conv2d(X,32,5,1)))\r\n            d1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d0,64,4,2))))\r\n            d2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d1,128,4,1))))\r\n            d3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d2,256,4,2))))\r\n            d4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(d3,512,4,1))))\r\n\r\n            h0 = dropout(lrelu(tf.layers.conv2d(y,32,5,1)))\r\n            h1 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h0,64,4,2))))\r\n            h2 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h1,128,4,1))))\r\n            h3 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h2,256,4,2))))\r\n            h4 = dropout(lrelu(tf.layers.batch_normalization(tf.layers.conv2d(h3,512,3,1))))\r\n\r\n            final = tf.concat([d4,h4],axis=-1)\r\n\r\n            f0 = dropout(lrelu(tf.layers.conv2d(final,1024,1,1)))\r\n            f1 = dropout(lrelu(tf.layers.conv2d(f0,1024,1,1)))\r\n            f2 = dropout(tf.layers.conv2d(f1,1,1,1))\r\n\r\n        return tf.reshape(f2,[-1,1])\r\n\r\n    def sig_loss(a,b):\r\n        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=a,labels=b),axis=1)\r\n\r\n    def get_vars(name):\r\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=name)\r\n\r\n    def l2_loss(a,b):\r\n        return tf.reduce_sum(tf.square(a-b),axis=1)\r\n\r\n    X_mnist = tf.placeholder(tf.float32,[None,28,28,1])\r\n    X_svhn = tf.placeholder(tf.float32,[None,32,32,3])\r\n\r\n    epsilon = tf.placeholder(tf.float32,[None,1])\r\n\r\n    z_mnist = tf.placeholder(tf.float32,[None,1,1,32])\r\n    z_svhn = tf.placeholder(tf.float32,[None,1,1,32])\r\n\r\n    Xtr_svhn = mnist_svhn(X_mnist,z_mnist)\r\n    Xtr_mnist = svhn_mnist(X_svhn,z_svhn)\r\n\r\n    sv = discriminator(X_svhn,Xtr_mnist)\r\n    mn = discriminator(Xtr_svhn,X_mnist,True)\r\n\r\n    G_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)\r\n    D_opt = tf.train.AdamOptimizer(learning_rate=1e-4,beta1=0.5)\r\n\r\n    x_hat = epsilon * X_svhn + (1-epsilon) * Xtr_svhn\r\n    grad_1 = tf.gradients(discriminator(x_hat,svhn_mnist(x_hat,z_svhn,True),True),x_hat)[0]\r\n    x_hat = epsilon * X_mnist + (1-epsilon) * Xtr_mnist\r\n    grad_2 = tf.gradients(discriminator(mnist_svhn(x_hat,z_mnist,True),x_hat,True),x_hat)[0]\r\n\r\n    slopes_1 = tf.sqrt(tf.reduce_sum(tf.square(grad_1),reduction_indices=-1))\r\n    slopes_2 = tf.sqrt(tf.reduce_sum(tf.square(grad_2),reduction_indices=-1))\r\n    grad_penalty = tf.reduce_mean((slopes_1 - 1.)**2) * 5\r\n    grad_penalty += tf.reduce_mean((slopes_2 - 1.)**2) * 5\r\n\r\n    disc_loss = tf.reduce_mean(sig_loss(sv,tf.ones_like(sv))) + \\\r\n                tf.reduce_mean(sig_loss(mn,tf.zeros_like(mn))) + grad_penalty\r\n\r\n    gen_loss = tf.reduce_mean(sig_loss(mn,tf.ones_like(mn))) + \\\r\n                tf.reduce_mean(sig_loss(sv,tf.zeros_like(sv)))\r\n\r\n    g_grad = G_opt.compute_gradients(gen_loss,var_list=get_vars(\"Encoder\"),colocate_gradient_with_ops=True)\r\n    -------> d_grad = D_opt.compute_gradients(disc_loss,var_list=get_vars(\"Discriminator\"),colocate_gradient_with_ops=True)\r\n\r\n    d_step = D_opt.apply_gradients(d_grad)\r\n    g_step = G_opt.apply_gradients(g_grad)"}