{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/334048144", "html_url": "https://github.com/tensorflow/tensorflow/issues/11070#issuecomment-334048144", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11070", "id": 334048144, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDA0ODE0NA==", "user": {"login": "andrei-pokrovsky", "id": 11221446, "node_id": "MDQ6VXNlcjExMjIxNDQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/11221446?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrei-pokrovsky", "html_url": "https://github.com/andrei-pokrovsky", "followers_url": "https://api.github.com/users/andrei-pokrovsky/followers", "following_url": "https://api.github.com/users/andrei-pokrovsky/following{/other_user}", "gists_url": "https://api.github.com/users/andrei-pokrovsky/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrei-pokrovsky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrei-pokrovsky/subscriptions", "organizations_url": "https://api.github.com/users/andrei-pokrovsky/orgs", "repos_url": "https://api.github.com/users/andrei-pokrovsky/repos", "events_url": "https://api.github.com/users/andrei-pokrovsky/events{/privacy}", "received_events_url": "https://api.github.com/users/andrei-pokrovsky/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-04T04:41:15Z", "updated_at": "2017-10-04T04:42:32Z", "author_association": "NONE", "body_html": "<p>I hit the same issue - worked around by moving out the actual CUDA kernel into a PyCUDA wrapper and profiling that along with captured launch parameters. I think what's happening is there's some startup overhead in TensorFlow that ends up getting captured by the CUDA profiler unnecessarily (and there's a mountain of it). Another thing to try could be to instrument using the CUDA profiler start/stop API <em>inside</em> tensorflow code, because if you use the API from python it has to be inside a session. Something else you could try is to make py_func ops using Python CUDA profiler start/stop API (an idea I just came up with right now, haven't tried it yet)</p>\n<pre><code># add to shell: export LD_LIBRARY_PATH=/usr/local/cuda-8.0/extras/CUPTI/lib64/\n\n\nimport ctypes\n\n_cudart = ctypes.CDLL('libcudart.so')\n\ndef cu_prof_start():\n    ret = _cudart.cudaProfilerStart()\n    if ret != 0:\n        raise Exception('cudaProfilerStart() returned %d' % ret)\n\n\ndef cu_prof_stop():\n    ret = _cudart.cudaProfilerStop()\n    if ret != 0:\n        raise Exception('cudaProfilerStop() returned %d' % ret)\n</code></pre>", "body_text": "I hit the same issue - worked around by moving out the actual CUDA kernel into a PyCUDA wrapper and profiling that along with captured launch parameters. I think what's happening is there's some startup overhead in TensorFlow that ends up getting captured by the CUDA profiler unnecessarily (and there's a mountain of it). Another thing to try could be to instrument using the CUDA profiler start/stop API inside tensorflow code, because if you use the API from python it has to be inside a session. Something else you could try is to make py_func ops using Python CUDA profiler start/stop API (an idea I just came up with right now, haven't tried it yet)\n# add to shell: export LD_LIBRARY_PATH=/usr/local/cuda-8.0/extras/CUPTI/lib64/\n\n\nimport ctypes\n\n_cudart = ctypes.CDLL('libcudart.so')\n\ndef cu_prof_start():\n    ret = _cudart.cudaProfilerStart()\n    if ret != 0:\n        raise Exception('cudaProfilerStart() returned %d' % ret)\n\n\ndef cu_prof_stop():\n    ret = _cudart.cudaProfilerStop()\n    if ret != 0:\n        raise Exception('cudaProfilerStop() returned %d' % ret)", "body": "I hit the same issue - worked around by moving out the actual CUDA kernel into a PyCUDA wrapper and profiling that along with captured launch parameters. I think what's happening is there's some startup overhead in TensorFlow that ends up getting captured by the CUDA profiler unnecessarily (and there's a mountain of it). Another thing to try could be to instrument using the CUDA profiler start/stop API *inside* tensorflow code, because if you use the API from python it has to be inside a session. Something else you could try is to make py_func ops using Python CUDA profiler start/stop API (an idea I just came up with right now, haven't tried it yet)\r\n\r\n\r\n\r\n```\r\n# add to shell: export LD_LIBRARY_PATH=/usr/local/cuda-8.0/extras/CUPTI/lib64/\r\n\r\n\r\nimport ctypes\r\n\r\n_cudart = ctypes.CDLL('libcudart.so')\r\n\r\ndef cu_prof_start():\r\n    ret = _cudart.cudaProfilerStart()\r\n    if ret != 0:\r\n        raise Exception('cudaProfilerStart() returned %d' % ret)\r\n\r\n\r\ndef cu_prof_stop():\r\n    ret = _cudart.cudaProfilerStop()\r\n    if ret != 0:\r\n        raise Exception('cudaProfilerStop() returned %d' % ret)\r\n```\r\n"}