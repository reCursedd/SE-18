{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12727", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12727/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12727/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12727/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12727", "id": 254297393, "node_id": "MDU6SXNzdWUyNTQyOTczOTM=", "number": 12727, "title": "Is tf.one_hot() w/ GPU not working under windows10?", "user": {"login": "didw", "id": 7973071, "node_id": "MDQ6VXNlcjc5NzMwNzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7973071?v=4", "gravatar_id": "", "url": "https://api.github.com/users/didw", "html_url": "https://github.com/didw", "followers_url": "https://api.github.com/users/didw/followers", "following_url": "https://api.github.com/users/didw/following{/other_user}", "gists_url": "https://api.github.com/users/didw/gists{/gist_id}", "starred_url": "https://api.github.com/users/didw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/didw/subscriptions", "organizations_url": "https://api.github.com/users/didw/orgs", "repos_url": "https://api.github.com/users/didw/repos", "events_url": "https://api.github.com/users/didw/events{/privacy}", "received_events_url": "https://api.github.com/users/didw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-08-31T11:05:17Z", "updated_at": "2017-09-04T03:29:14Z", "closed_at": "2017-09-04T03:04:58Z", "author_association": "NONE", "body_html": "<p>I'm trying to run tensorflow in Windows10 environment.<br>\nWhen I use <code>tf.one_hot</code> function with GPU then it occur error.<br>\nBelow is test code, and it's working find in Ubuntu.</p>\n<h3>Test code</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> sklearn.datasets.samples_generator <span class=\"pl-k\">import</span> make_regression\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nX, y, coef <span class=\"pl-k\">=</span> make_regression(<span class=\"pl-v\">n_samples</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">n_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">noise</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">coef</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> To make all X values positive integer</span>\nX <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">xx</span>: <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: <span class=\"pl-c1\">abs</span>(<span class=\"pl-c1\">int</span>(x<span class=\"pl-k\">*</span><span class=\"pl-c1\">10</span>)), xx)), X))\n\ntf.reset_default_graph()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_model</span>(<span class=\"pl-smi\">x</span>):\n    xs <span class=\"pl-k\">=</span> tf.split(x, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">1</span>)\n    h1s <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>h1_<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">'</span></span><span class=\"pl-k\">%</span>i):\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Seems below line is the problem..</span>\n            xs_temp <span class=\"pl-k\">=</span> tf.reshape(tf.one_hot(xs[i], <span class=\"pl-v\">depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>, <span class=\"pl-v\">on_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">off_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">30</span>])\n            weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.contrib.layers.xavier_initializer())\n            biases <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>biases<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>))\n            xs_temp <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(xs_temp, weights) <span class=\"pl-k\">+</span> biases)\n            h1s.append(xs_temp)\n    h1 <span class=\"pl-k\">=</span> tf.concat(h1s, <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>h2<span class=\"pl-pds\">'</span></span>):\n        weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.contrib.layers.xavier_initializer())\n        biases <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>biases<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>))\n        y <span class=\"pl-k\">=</span> tf.matmul(h1, weights) <span class=\"pl-k\">+</span> biases\n    <span class=\"pl-k\">return</span> y\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n    X_ph <span class=\"pl-k\">=</span>  tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>,<span class=\"pl-c1\">2</span>],<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n    y_ph <span class=\"pl-k\">=</span>  tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    output <span class=\"pl-k\">=</span> build_model(X_ph)\nloss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(y_ph, output)\nupdateModel <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-2</span>).minimize(loss)\n\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto()\nconfig.gpu_options.allow_growth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n    sess.run(init)\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n        sess.run(updateModel, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X_ph: X, y_ph: y})</pre></div>\n<h3>Error</h3>\n<pre><code>InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'h1_1/one_hot': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n\t [[Node: h1_1/one_hot = OneHot[T=DT_FLOAT, TI=DT_INT32, axis=-1, _device=\"/device:GPU:0\"](split:1, h1_1/one_hot/depth, h1_1/one_hot/on_value, h1_1/one_hot/off_value)]]\n</code></pre>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install tensorflow-gpu</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/5.1</li>\n<li><strong>GPU model and memory</strong>: Titan XP (12GB)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>", "body_text": "I'm trying to run tensorflow in Windows10 environment.\nWhen I use tf.one_hot function with GPU then it occur error.\nBelow is test code, and it's working find in Ubuntu.\nTest code\nfrom sklearn.datasets.samples_generator import make_regression\nimport tensorflow as tf\n\nX, y, coef = make_regression(n_samples=100, n_features=2, noise=0.1, coef=True)\n# To make all X values positive integer\nX = list(map(lambda xx: list(map(lambda x: abs(int(x*10)), xx)), X))\n\ntf.reset_default_graph()\ndef build_model(x):\n    xs = tf.split(x, [1,1], 1)\n    h1s = []\n    for i in range(2):\n        with tf.variable_scope('h1_%d'%i):\n            # Seems below line is the problem..\n            xs_temp = tf.reshape(tf.one_hot(xs[i], depth=30, on_value=1.0, off_value=0.0), [-1, 30])\n            weights = tf.get_variable('weight', [30, 2], initializer=tf.contrib.layers.xavier_initializer())\n            biases = tf.get_variable('biases', [2], initializer=tf.constant_initializer(0.0))\n            xs_temp = tf.nn.relu(tf.matmul(xs_temp, weights) + biases)\n            h1s.append(xs_temp)\n    h1 = tf.concat(h1s, 1)\n    with tf.variable_scope('h2'):\n        weights = tf.get_variable('weight', [4, 1], initializer=tf.contrib.layers.xavier_initializer())\n        biases = tf.get_variable('biases', [1], initializer=tf.constant_initializer(0.0))\n        y = tf.matmul(h1, weights) + biases\n    return y\n\nwith tf.device('/gpu:0'):\n    X_ph =  tf.placeholder(shape=[None,2],dtype=tf.int32)\n    y_ph =  tf.placeholder(shape=None,dtype=tf.float32)\n    output = build_model(X_ph)\nloss = tf.losses.mean_squared_error(y_ph, output)\nupdateModel = tf.train.AdamOptimizer(1e-2).minimize(loss)\n\ninit = tf.global_variables_initializer()\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nwith tf.Session(config=config) as sess:\n    sess.run(init)\n    for _ in range(100):\n        sess.run(updateModel, feed_dict={X_ph: X, y_ph: y})\nError\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'h1_1/one_hot': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n\t [[Node: h1_1/one_hot = OneHot[T=DT_FLOAT, TI=DT_INT32, axis=-1, _device=\"/device:GPU:0\"](split:1, h1_1/one_hot/depth, h1_1/one_hot/on_value, h1_1/one_hot/off_value)]]\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): pip install tensorflow-gpu\nTensorFlow version (use command below): 1.3.0\nPython version:  3.6\nBazel version (if compiling from source):\nCUDA/cuDNN version: 8.0/5.1\nGPU model and memory: Titan XP (12GB)\nExact command to reproduce:", "body": "I'm trying to run tensorflow in Windows10 environment.\r\nWhen I use `tf.one_hot` function with GPU then it occur error.\r\nBelow is test code, and it's working find in Ubuntu.\r\n\r\n### Test code\r\n```python\r\nfrom sklearn.datasets.samples_generator import make_regression\r\nimport tensorflow as tf\r\n\r\nX, y, coef = make_regression(n_samples=100, n_features=2, noise=0.1, coef=True)\r\n# To make all X values positive integer\r\nX = list(map(lambda xx: list(map(lambda x: abs(int(x*10)), xx)), X))\r\n\r\ntf.reset_default_graph()\r\ndef build_model(x):\r\n    xs = tf.split(x, [1,1], 1)\r\n    h1s = []\r\n    for i in range(2):\r\n        with tf.variable_scope('h1_%d'%i):\r\n            # Seems below line is the problem..\r\n            xs_temp = tf.reshape(tf.one_hot(xs[i], depth=30, on_value=1.0, off_value=0.0), [-1, 30])\r\n            weights = tf.get_variable('weight', [30, 2], initializer=tf.contrib.layers.xavier_initializer())\r\n            biases = tf.get_variable('biases', [2], initializer=tf.constant_initializer(0.0))\r\n            xs_temp = tf.nn.relu(tf.matmul(xs_temp, weights) + biases)\r\n            h1s.append(xs_temp)\r\n    h1 = tf.concat(h1s, 1)\r\n    with tf.variable_scope('h2'):\r\n        weights = tf.get_variable('weight', [4, 1], initializer=tf.contrib.layers.xavier_initializer())\r\n        biases = tf.get_variable('biases', [1], initializer=tf.constant_initializer(0.0))\r\n        y = tf.matmul(h1, weights) + biases\r\n    return y\r\n\r\nwith tf.device('/gpu:0'):\r\n    X_ph =  tf.placeholder(shape=[None,2],dtype=tf.int32)\r\n    y_ph =  tf.placeholder(shape=None,dtype=tf.float32)\r\n    output = build_model(X_ph)\r\nloss = tf.losses.mean_squared_error(y_ph, output)\r\nupdateModel = tf.train.AdamOptimizer(1e-2).minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    sess.run(init)\r\n    for _ in range(100):\r\n        sess.run(updateModel, feed_dict={X_ph: X, y_ph: y})\r\n```\r\n\r\n### Error \r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'h1_1/one_hot': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: h1_1/one_hot = OneHot[T=DT_FLOAT, TI=DT_INT32, axis=-1, _device=\"/device:GPU:0\"](split:1, h1_1/one_hot/depth, h1_1/one_hot/on_value, h1_1/one_hot/off_value)]]\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Titan XP (12GB)\r\n- **Exact command to reproduce**:\r\n"}