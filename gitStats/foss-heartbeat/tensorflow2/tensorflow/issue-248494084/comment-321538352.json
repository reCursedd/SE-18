{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321538352", "html_url": "https://github.com/tensorflow/tensorflow/issues/12086#issuecomment-321538352", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12086", "id": 321538352, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTUzODM1Mg==", "user": {"login": "rubenvereecken", "id": 5216553, "node_id": "MDQ6VXNlcjUyMTY1NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5216553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubenvereecken", "html_url": "https://github.com/rubenvereecken", "followers_url": "https://api.github.com/users/rubenvereecken/followers", "following_url": "https://api.github.com/users/rubenvereecken/following{/other_user}", "gists_url": "https://api.github.com/users/rubenvereecken/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubenvereecken/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubenvereecken/subscriptions", "organizations_url": "https://api.github.com/users/rubenvereecken/orgs", "repos_url": "https://api.github.com/users/rubenvereecken/repos", "events_url": "https://api.github.com/users/rubenvereecken/events{/privacy}", "received_events_url": "https://api.github.com/users/rubenvereecken/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-10T12:35:11Z", "updated_at": "2017-08-10T12:35:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Never mind, my last attempt didn't work because I messed up variable initialization.</p>\n<p>The way I worked around this is as follows. I keep a <code>first</code> boolean in my state saver which is initialized to <code>True</code> and gets set to <code>False</code> after the first time a batch is encountered.<br>\nI keep a separate set of to-be-learned initial states. The trick is to only use them during the first batch for a sequence. I do this by creating a mask from <code>first</code>, expanding the initial state vector to the correct batch size, masking it, then adding it to the state saver's initial states, effectively replacing all initial zero rows and only those. This will include the initial state variables in the computation graph making them optimizable.</p>\n<pre><code>             c = state_saver.state('lstm_c')\n             first = state_saver.state('first')\n\n             only_first_mask = tf.expand_dims(first, 1) # Mask rows\n             not_first_mask = tf.logical_not(only_first_mask)\n             only_first_mask = tf.cast(only_first_mask, tf.float32)\n             not_first_mask = tf.cast(not_first_mask, tf.float32)\n\n             # learnable initial variable\n             init_c = provider.initial_variables['lstm_c']\n             # B x 1 * 1 x 512 -&gt; 512 x 512 (repeats row)\n             # Repeats the initial variable `batch_size` times\n             init_c = tf.einsum('ij,jk-&gt;ik', tf.ones([batch_size,1]), tf.expand_dims(init_c,0))\n             # Mask the initial variable\n             init_c = only_first_mask * init_c\n             c = not_first_mask * c # Has first=true set to zeros\n             # This now contains entries from `init_c` in the rows corresponding to `first`\n             c = c + init_c\n</code></pre>", "body_text": "Never mind, my last attempt didn't work because I messed up variable initialization.\nThe way I worked around this is as follows. I keep a first boolean in my state saver which is initialized to True and gets set to False after the first time a batch is encountered.\nI keep a separate set of to-be-learned initial states. The trick is to only use them during the first batch for a sequence. I do this by creating a mask from first, expanding the initial state vector to the correct batch size, masking it, then adding it to the state saver's initial states, effectively replacing all initial zero rows and only those. This will include the initial state variables in the computation graph making them optimizable.\n             c = state_saver.state('lstm_c')\n             first = state_saver.state('first')\n\n             only_first_mask = tf.expand_dims(first, 1) # Mask rows\n             not_first_mask = tf.logical_not(only_first_mask)\n             only_first_mask = tf.cast(only_first_mask, tf.float32)\n             not_first_mask = tf.cast(not_first_mask, tf.float32)\n\n             # learnable initial variable\n             init_c = provider.initial_variables['lstm_c']\n             # B x 1 * 1 x 512 -> 512 x 512 (repeats row)\n             # Repeats the initial variable `batch_size` times\n             init_c = tf.einsum('ij,jk->ik', tf.ones([batch_size,1]), tf.expand_dims(init_c,0))\n             # Mask the initial variable\n             init_c = only_first_mask * init_c\n             c = not_first_mask * c # Has first=true set to zeros\n             # This now contains entries from `init_c` in the rows corresponding to `first`\n             c = c + init_c", "body": "Never mind, my last attempt didn't work because I messed up variable initialization.\r\n\r\nThe way I worked around this is as follows. I keep a `first` boolean in my state saver which is initialized to `True` and gets set to `False` after the first time a batch is encountered.\r\nI keep a separate set of to-be-learned initial states. The trick is to only use them during the first batch for a sequence. I do this by creating a mask from `first`, expanding the initial state vector to the correct batch size, masking it, then adding it to the state saver's initial states, effectively replacing all initial zero rows and only those. This will include the initial state variables in the computation graph making them optimizable.\r\n\r\n```\r\n             c = state_saver.state('lstm_c')\r\n             first = state_saver.state('first')\r\n\r\n             only_first_mask = tf.expand_dims(first, 1) # Mask rows\r\n             not_first_mask = tf.logical_not(only_first_mask)\r\n             only_first_mask = tf.cast(only_first_mask, tf.float32)\r\n             not_first_mask = tf.cast(not_first_mask, tf.float32)\r\n\r\n             # learnable initial variable\r\n             init_c = provider.initial_variables['lstm_c']\r\n             # B x 1 * 1 x 512 -> 512 x 512 (repeats row)\r\n             # Repeats the initial variable `batch_size` times\r\n             init_c = tf.einsum('ij,jk->ik', tf.ones([batch_size,1]), tf.expand_dims(init_c,0))\r\n             # Mask the initial variable\r\n             init_c = only_first_mask * init_c\r\n             c = not_first_mask * c # Has first=true set to zeros\r\n             # This now contains entries from `init_c` in the rows corresponding to `first`\r\n             c = c + init_c\r\n```\r\n\r\n\r\n"}