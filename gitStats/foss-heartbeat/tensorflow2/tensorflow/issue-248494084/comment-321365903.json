{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321365903", "html_url": "https://github.com/tensorflow/tensorflow/issues/12086#issuecomment-321365903", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12086", "id": 321365903, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTM2NTkwMw==", "user": {"login": "milah", "id": 2327829, "node_id": "MDQ6VXNlcjIzMjc4Mjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/2327829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/milah", "html_url": "https://github.com/milah", "followers_url": "https://api.github.com/users/milah/followers", "following_url": "https://api.github.com/users/milah/following{/other_user}", "gists_url": "https://api.github.com/users/milah/gists{/gist_id}", "starred_url": "https://api.github.com/users/milah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/milah/subscriptions", "organizations_url": "https://api.github.com/users/milah/orgs", "repos_url": "https://api.github.com/users/milah/repos", "events_url": "https://api.github.com/users/milah/events{/privacy}", "received_events_url": "https://api.github.com/users/milah/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-09T20:03:27Z", "updated_at": "2017-08-09T20:03:27Z", "author_association": "NONE", "body_html": "<p>You are right the initial_states cannot be learned with the SequenceQueueingStateSaver that is constructed in batch_sequences_with_states.</p>\n<p>No gradients can flow through the queue. This is a limitation. The documentation could be a bit clearer but it mentions \"e.g. constants or tensors\" for the initial state.</p>\n<p>The only way around this is not using the SequenceQueueingStateSaver. For instance, how about training the initial state with the dynamic_rnn on small enough (pre-segmented) sequences? Then you can switch to the state_saving_rnn with the SequenceQueueingStateSaver with the pre-trained initial state.</p>", "body_text": "You are right the initial_states cannot be learned with the SequenceQueueingStateSaver that is constructed in batch_sequences_with_states.\nNo gradients can flow through the queue. This is a limitation. The documentation could be a bit clearer but it mentions \"e.g. constants or tensors\" for the initial state.\nThe only way around this is not using the SequenceQueueingStateSaver. For instance, how about training the initial state with the dynamic_rnn on small enough (pre-segmented) sequences? Then you can switch to the state_saving_rnn with the SequenceQueueingStateSaver with the pre-trained initial state.", "body": "You are right the initial_states cannot be learned with the SequenceQueueingStateSaver that is constructed in batch_sequences_with_states.\r\n\r\nNo gradients can flow through the queue. This is a limitation. The documentation could be a bit clearer but it mentions \"e.g. constants or tensors\" for the initial state.\r\n\r\nThe only way around this is not using the SequenceQueueingStateSaver. For instance, how about training the initial state with the dynamic_rnn on small enough (pre-segmented) sequences? Then you can switch to the state_saving_rnn with the SequenceQueueingStateSaver with the pre-trained initial state."}