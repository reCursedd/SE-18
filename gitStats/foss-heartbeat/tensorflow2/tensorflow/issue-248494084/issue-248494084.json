{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12086", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12086/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12086/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12086/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12086", "id": 248494084, "node_id": "MDU6SXNzdWUyNDg0OTQwODQ=", "number": 12086, "title": "Cannot learn initial_states with batch_sequences_with_states", "user": {"login": "rubenvereecken", "id": 5216553, "node_id": "MDQ6VXNlcjUyMTY1NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5216553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubenvereecken", "html_url": "https://github.com/rubenvereecken", "followers_url": "https://api.github.com/users/rubenvereecken/followers", "following_url": "https://api.github.com/users/rubenvereecken/following{/other_user}", "gists_url": "https://api.github.com/users/rubenvereecken/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubenvereecken/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubenvereecken/subscriptions", "organizations_url": "https://api.github.com/users/rubenvereecken/orgs", "repos_url": "https://api.github.com/users/rubenvereecken/repos", "events_url": "https://api.github.com/users/rubenvereecken/events{/privacy}", "received_events_url": "https://api.github.com/users/rubenvereecken/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-08-07T18:32:48Z", "updated_at": "2017-08-28T03:39:21Z", "closed_at": "2017-08-28T03:39:21Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Mint 18</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary (pip)</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.3.0.0rc0</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using <code>tf.contrib.training.batch_sequences_with_states</code>, it seems impossible to learn the <code>initial_states</code> passed to it. I've tried using Variables with <code>trainable=True</code> with several initializers and while my optimizer picks them up through <code>tf.trainable_variables()</code>, their values do not get learned.</p>\n<p>I assume this has something to do with TensorFlow continuing to work with the Tensors, not the Variables? Additionally, the initializer is called each run and in TensorBoard it shows that it does not feed into the optimizer, whereas network weights for example would.</p>\n<p>Are my presumptions correct, and if not, how do I fix this? If this is a TensorFlow limitation, how could I work around it?</p>\n<h3>Source code / logs</h3>\n<pre><code>            initializers = {\n                    'lstm_c': train_init,\n                    'lstm_h': train_init,\n                    'encode_lstm_c': train_init,\n                    'encode_lstm_h': train_init,\n                    'last_out': train_init,\n                    }\n\n            with tf.variable_scope(tf.get_variable_scope(), reuse=not is_training):\n\n                initial_states = { k: tf.get_variable('initial_{}'.format(k), v.shape,\n                        dtype=v.dtype,\n                        initializer=initializers[k],\n                        trainable=is_training and self.learn_initial_states, \\\n                        collections=[attend.GraphKeys.INITIAL_STATES]) \\\n                        for k, v in initial_constants.items() }\n\n\n                # This makes sure we're not dealing with the reference but learned values\n                initial_states = { k: v.initialized_value() for k, v in initial_states.items() }\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 18\nTensorFlow installed from (source or binary): Binary (pip)\nTensorFlow version (use command below): v1.3.0.0rc0\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nUsing tf.contrib.training.batch_sequences_with_states, it seems impossible to learn the initial_states passed to it. I've tried using Variables with trainable=True with several initializers and while my optimizer picks them up through tf.trainable_variables(), their values do not get learned.\nI assume this has something to do with TensorFlow continuing to work with the Tensors, not the Variables? Additionally, the initializer is called each run and in TensorBoard it shows that it does not feed into the optimizer, whereas network weights for example would.\nAre my presumptions correct, and if not, how do I fix this? If this is a TensorFlow limitation, how could I work around it?\nSource code / logs\n            initializers = {\n                    'lstm_c': train_init,\n                    'lstm_h': train_init,\n                    'encode_lstm_c': train_init,\n                    'encode_lstm_h': train_init,\n                    'last_out': train_init,\n                    }\n\n            with tf.variable_scope(tf.get_variable_scope(), reuse=not is_training):\n\n                initial_states = { k: tf.get_variable('initial_{}'.format(k), v.shape,\n                        dtype=v.dtype,\n                        initializer=initializers[k],\n                        trainable=is_training and self.learn_initial_states, \\\n                        collections=[attend.GraphKeys.INITIAL_STATES]) \\\n                        for k, v in initial_constants.items() }\n\n\n                # This makes sure we're not dealing with the reference but learned values\n                initial_states = { k: v.initialized_value() for k, v in initial_states.items() }", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: v1.3.0.0rc0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nUsing `tf.contrib.training.batch_sequences_with_states`, it seems impossible to learn the `initial_states` passed to it. I've tried using Variables with `trainable=True` with several initializers and while my optimizer picks them up through `tf.trainable_variables()`, their values do not get learned.\r\n\r\nI assume this has something to do with TensorFlow continuing to work with the Tensors, not the Variables? Additionally, the initializer is called each run and in TensorBoard it shows that it does not feed into the optimizer, whereas network weights for example would.\r\n\r\nAre my presumptions correct, and if not, how do I fix this? If this is a TensorFlow limitation, how could I work around it?\r\n\r\n### Source code / logs\r\n```\r\n            initializers = {\r\n                    'lstm_c': train_init,\r\n                    'lstm_h': train_init,\r\n                    'encode_lstm_c': train_init,\r\n                    'encode_lstm_h': train_init,\r\n                    'last_out': train_init,\r\n                    }\r\n\r\n            with tf.variable_scope(tf.get_variable_scope(), reuse=not is_training):\r\n\r\n                initial_states = { k: tf.get_variable('initial_{}'.format(k), v.shape,\r\n                        dtype=v.dtype,\r\n                        initializer=initializers[k],\r\n                        trainable=is_training and self.learn_initial_states, \\\r\n                        collections=[attend.GraphKeys.INITIAL_STATES]) \\\r\n                        for k, v in initial_constants.items() }\r\n\r\n\r\n                # This makes sure we're not dealing with the reference but learned values\r\n                initial_states = { k: v.initialized_value() for k, v in initial_states.items() }\r\n```\r\n"}