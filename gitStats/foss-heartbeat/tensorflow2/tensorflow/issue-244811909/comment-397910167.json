{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/397910167", "html_url": "https://github.com/tensorflow/tensorflow/issues/11679#issuecomment-397910167", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11679", "id": 397910167, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzkxMDE2Nw==", "user": {"login": "masonk", "id": 449998, "node_id": "MDQ6VXNlcjQ0OTk5OA==", "avatar_url": "https://avatars0.githubusercontent.com/u/449998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/masonk", "html_url": "https://github.com/masonk", "followers_url": "https://api.github.com/users/masonk/followers", "following_url": "https://api.github.com/users/masonk/following{/other_user}", "gists_url": "https://api.github.com/users/masonk/gists{/gist_id}", "starred_url": "https://api.github.com/users/masonk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/masonk/subscriptions", "organizations_url": "https://api.github.com/users/masonk/orgs", "repos_url": "https://api.github.com/users/masonk/repos", "events_url": "https://api.github.com/users/masonk/events{/privacy}", "received_events_url": "https://api.github.com/users/masonk/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-17T22:15:45Z", "updated_at": "2018-06-18T01:04:54Z", "author_association": "NONE", "body_html": "<p>Your way of saving an iterator handle and then feeding it back in is the only way that I can get to work. Thanks!</p>\n<p>Is this the most efficient way to do this? It seems like having to do an op lookup from a handle for every train step is less efficient than just grafting in your new iterator one time during graph setup.</p>\n<p>What made the most sense to me would have been to save a reinitializable iterator into the metagraph, and then to restore that iterator and call <code>sess.run(reinitializable_iterator.make_initializer(my_new_dataset))</code>. That only needs to happen once per iterator initialization instead of every train step.  But, I was unable to save an reinitializable iterator to the metagraph and then properly restore it. After a few hours of trying, I gave up and switched to feedable.</p>\n<p>I think that attaching a new Dataset iterator to a restored model is the most common use case for metagraphs, so I think that these examples you're giving here should be in the docs. If it is indeed the <em>best</em> way to do this.</p>", "body_text": "Your way of saving an iterator handle and then feeding it back in is the only way that I can get to work. Thanks!\nIs this the most efficient way to do this? It seems like having to do an op lookup from a handle for every train step is less efficient than just grafting in your new iterator one time during graph setup.\nWhat made the most sense to me would have been to save a reinitializable iterator into the metagraph, and then to restore that iterator and call sess.run(reinitializable_iterator.make_initializer(my_new_dataset)). That only needs to happen once per iterator initialization instead of every train step.  But, I was unable to save an reinitializable iterator to the metagraph and then properly restore it. After a few hours of trying, I gave up and switched to feedable.\nI think that attaching a new Dataset iterator to a restored model is the most common use case for metagraphs, so I think that these examples you're giving here should be in the docs. If it is indeed the best way to do this.", "body": "Your way of saving an iterator handle and then feeding it back in is the only way that I can get to work. Thanks!\r\n\r\nIs this the most efficient way to do this? It seems like having to do an op lookup from a handle for every train step is less efficient than just grafting in your new iterator one time during graph setup. \r\n\r\nWhat made the most sense to me would have been to save a reinitializable iterator into the metagraph, and then to restore that iterator and call `sess.run(reinitializable_iterator.make_initializer(my_new_dataset))`. That only needs to happen once per iterator initialization instead of every train step.  But, I was unable to save an reinitializable iterator to the metagraph and then properly restore it. After a few hours of trying, I gave up and switched to feedable.\r\n\r\nI think that attaching a new Dataset iterator to a restored model is the most common use case for metagraphs, so I think that these examples you're giving here should be in the docs. If it is indeed the *best* way to do this.\r\n"}