{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17754", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17754/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17754/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17754/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17754", "id": 305794228, "node_id": "MDU6SXNzdWUzMDU3OTQyMjg=", "number": 17754, "title": "[bug] segmentation fault happens with nested higher order function", "user": {"login": "shengc", "id": 940628, "node_id": "MDQ6VXNlcjk0MDYyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/940628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shengc", "html_url": "https://github.com/shengc", "followers_url": "https://api.github.com/users/shengc/followers", "following_url": "https://api.github.com/users/shengc/following{/other_user}", "gists_url": "https://api.github.com/users/shengc/gists{/gist_id}", "starred_url": "https://api.github.com/users/shengc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shengc/subscriptions", "organizations_url": "https://api.github.com/users/shengc/orgs", "repos_url": "https://api.github.com/users/shengc/repos", "events_url": "https://api.github.com/users/shengc/events{/privacy}", "received_events_url": "https://api.github.com/users/shengc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-03-16T03:53:33Z", "updated_at": "2018-03-21T19:31:50Z", "closed_at": "2018-03-21T16:37:55Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: OSX 10.11.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: VirtualEnv</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A (CPU only)</li>\n<li><strong>GPU model and memory</strong>: N/A (CPU only)</li>\n<li><strong>Exact command to reproduce</strong>: see the following</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>segmentation fault happens when the computation graph contains <code>scan</code> with <code>bidirectional_rnn</code> embedded.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nembed_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nhidden_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nnum_class<span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>\n\nwords <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>words<span class=\"pl-pds\">'</span></span>)\nlength <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>length<span class=\"pl-pds\">'</span></span>)\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, (), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>)\ninit_state <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, hidden_dim <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>initial_state<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">make_rnn_cell</span>(): <span class=\"pl-k\">return</span> tf.nn.rnn_cell.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>hidden_dim)\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>embeddings<span class=\"pl-pds\">'</span></span>):\n    embedding <span class=\"pl-k\">=</span> \\\n        tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>parameter<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">100</span>, embed_dim), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    embedded  <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(embedding, words, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>lookup<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_lstm<span class=\"pl-pds\">'</span></span>):\n    cell_fw <span class=\"pl-k\">=</span> make_rnn_cell()\n    cell_bw <span class=\"pl-k\">=</span> make_rnn_cell()\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step</span>(<span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">inp</span>):\n        data <span class=\"pl-k\">=</span> tf.expand_dims(inp[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        length <span class=\"pl-k\">=</span> tf.expand_dims(inp[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        fw_state <span class=\"pl-k\">=</span> tf.split(state[inp[<span class=\"pl-c1\">1</span>], :], <span class=\"pl-c1\">2</span>)[<span class=\"pl-c1\">0</span>]\n        bw_state <span class=\"pl-k\">=</span> tf.split(state[<span class=\"pl-c1\">0</span>, :], <span class=\"pl-c1\">2</span>)[<span class=\"pl-c1\">1</span>]\nonline training (feeding one training example at a time)\n        fw_state <span class=\"pl-k\">=</span> tf.expand_dims(fw_state, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        bw_state <span class=\"pl-k\">=</span> tf.expand_dims(bw_state, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        (outputs_fw, outputs_bw), _ <span class=\"pl-k\">=</span> \\\n            tf.nn.bidirectional_dynamic_rnn(\n                cell_fw, cell_bw, embedded, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>length,\n                <span class=\"pl-v\">initial_state_fw</span><span class=\"pl-k\">=</span>fw_state, <span class=\"pl-v\">initial_state_bw</span><span class=\"pl-k\">=</span>bw_state, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32\n            )\n        outputs <span class=\"pl-k\">=</span> tf.squeeze(tf.concat([outputs_fw, outputs_bw], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>])\n        <span class=\"pl-k\">return</span> outputs\n    outputs <span class=\"pl-k\">=</span> tf.scan(step, (embedded, length), <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>init_state)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_attention<span class=\"pl-pds\">'</span></span>):\n    hidden <span class=\"pl-k\">=</span> \\\n        tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>hidden_dim <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.tanh)\n    attention <span class=\"pl-k\">=</span> \\\n        tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n    attention <span class=\"pl-k\">=</span> tf.transpose(tf.nn.softmax(tf.transpose(attention, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])), <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\nsentence_embedding <span class=\"pl-k\">=</span> tf.reduce_sum(outputs <span class=\"pl-k\">*</span> attention, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\nsentence_embedding <span class=\"pl-k\">=</span> tf.expand_dims(sentence_embedding, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sentence_lstm<span class=\"pl-pds\">'</span></span>):\n    cell_fw <span class=\"pl-k\">=</span> make_rnn_cell()\n    cell_bw <span class=\"pl-k\">=</span> make_rnn_cell()\n    (outputs_fw, outputs_bw), _ <span class=\"pl-k\">=</span> \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sentence_embedding, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\noutputs <span class=\"pl-k\">=</span> tf.squeeze(tf.concat([outputs_fw, outputs_bw], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>])\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sentence_attention<span class=\"pl-pds\">'</span></span>):\n    hidden <span class=\"pl-k\">=</span> \\\n        tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>hidden_dim <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.tanh)\n    attention <span class=\"pl-k\">=</span> \\\n        tf.layers.dense(hidden, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n    attention <span class=\"pl-k\">=</span> tf.transpose(tf.nn.softmax(tf.transpose(attention)))\noutputs <span class=\"pl-k\">=</span> tf.reduce_sum(outputs <span class=\"pl-k\">*</span> attention, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\noutputs <span class=\"pl-k\">=</span> tf.expand_dims(outputs, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\nlogits <span class=\"pl-k\">=</span> tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>num_class, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\nloss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>tf.log(tf.nn.softmax(logits)[:, labels], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>)\ntraining_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>).minimize(loss)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    words_val <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">100</span>))\n    length_val <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">10</span>))\n    labels_val <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, num_class, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>())\n    init_state_val <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">10</span>, hidden_dim <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>)\n\n    fd <span class=\"pl-k\">=</span> { words : words_val, length : length_val, labels : labels_val, init_state : init_state_val}\n    sess.run(training_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>fd)</pre></div>\n<p>Running with TF 1.6, the above code ended up with the following error:</p>\n<div class=\"highlight highlight-source-shell\"><pre>Segmentation fault: 11</pre></div>\n<p>Additional information,<br>\nI ran the program with <code>dtruss</code>, the last few lines printed on console before it crashed are as follows,</p>\n<div class=\"highlight highlight-source-shell\"><pre>psynch_cvsignal(0x7FDED4BC4A68, 0x90000000A00, 0x900)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4A68, 0x90100000A00, 0x900)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4E68, 0x1F0000002000, 0x1F00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4E68, 0x1F0100002000, 0x1F00)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4E68, 0x200000002100, 0x2000)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4E68, 0x200100002100, 0x2000)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4E68, 0x210000002200, 0x2100)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4E68, 0x210100002200, 0x2100)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4C68, 0x1C0000001D00, 0x1C00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4C68, 0x1C0100001D00, 0x1C00)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4A68, 0xA0000000B00, 0xA00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4A68, 0xA0100000B00, 0xA00)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4D68, 0xD0000000E00, 0xD00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4D68, 0xD0100000E00, 0xD00)\t\t = 0 0\npsynch_cvwait(0x0, 0x0, 0x0)\t\t = 0 0\npsynch_cvwait(0x7FDED4BC4F68, 0x1B0100001C00, 0x1B00)\t\t = 0 0\npsynch_cvwait(0x7FDED4BC5068, 0x30100000400, 0x300)\t\t = 0 0\npsynch_cvwait(0x7FDED4BC5168, 0x20100000300, 0x200)\t\t = 0 0\npsynch_cvwait(0x7FDEDCAEFE28, 0x10100000200, 0x100)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4068, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4168, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4268, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4368, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4568, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4468, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4668, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4768, 0x100000100, 0x0)\t\t = -1 Err#260</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.11.6\nTensorFlow installed from (source or binary): VirtualEnv\nTensorFlow version (use command below): 1.6\nPython version: 3.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A (CPU only)\nGPU model and memory: N/A (CPU only)\nExact command to reproduce: see the following\n\nDescribe the problem\nsegmentation fault happens when the computation graph contains scan with bidirectional_rnn embedded.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\n\nembed_dim = 10\nhidden_dim = 10\nnum_class=10\n\nwords = tf.placeholder(tf.int32, [None, None], name='words')\nlength = tf.placeholder(tf.int32, [None], name='length')\nlabels = tf.placeholder(tf.int32, (), name='labels')\ninit_state = tf.placeholder(tf.float32, [None, hidden_dim * 2], name='initial_state')\n\ndef make_rnn_cell(): return tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\n\nwith tf.variable_scope('embeddings'):\n    embedding = \\\n        tf.get_variable('parameter', shape=(100, embed_dim), dtype=tf.float32, trainable=True)\n    embedded  = tf.nn.embedding_lookup(embedding, words, name='lookup')\nwith tf.variable_scope('words_lstm'):\n    cell_fw = make_rnn_cell()\n    cell_bw = make_rnn_cell()\n    def step(state, inp):\n        data = tf.expand_dims(inp[0], axis=0)\n        length = tf.expand_dims(inp[1], axis=0)\n        fw_state = tf.split(state[inp[1], :], 2)[0]\n        bw_state = tf.split(state[0, :], 2)[1]\nonline training (feeding one training example at a time)\n        fw_state = tf.expand_dims(fw_state, axis=0)\n        bw_state = tf.expand_dims(bw_state, axis=0)\n        (outputs_fw, outputs_bw), _ = \\\n            tf.nn.bidirectional_dynamic_rnn(\n                cell_fw, cell_bw, embedded, sequence_length=length,\n                initial_state_fw=fw_state, initial_state_bw=bw_state, dtype=tf.float32\n            )\n        outputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\n        return outputs\n    outputs = tf.scan(step, (embedded, length), initializer=init_state)\nwith tf.variable_scope('words_attention'):\n    hidden = \\\n        tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\n    attention = \\\n        tf.layers.dense(outputs, units=1, activation=None)\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 2, 1])), perm=[0, 2, 1])\nsentence_embedding = tf.reduce_sum(outputs * attention, axis=1)\nsentence_embedding = tf.expand_dims(sentence_embedding, axis=0)\n\nwith tf.variable_scope('sentence_lstm'):\n    cell_fw = make_rnn_cell()\n    cell_bw = make_rnn_cell()\n    (outputs_fw, outputs_bw), _ = \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sentence_embedding, dtype=tf.float32)\noutputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\nwith tf.variable_scope('sentence_attention'):\n    hidden = \\\n        tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\n    attention = \\\n        tf.layers.dense(hidden, units=1, activation=None)\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention)))\noutputs = tf.reduce_sum(outputs * attention, axis=0)\noutputs = tf.expand_dims(outputs, axis=0)\nlogits = tf.layers.dense(outputs, units=num_class, activation=None)\nloss = -tf.log(tf.nn.softmax(logits)[:, labels], name='loss')\ntraining_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    words_val = np.random.randint(0, 100, size=(10, 100))\n    length_val = np.random.randint(0, 100, size=(10))\n    labels_val = np.random.randint(0, num_class, size=())\n    init_state_val = np.random.randn(10, hidden_dim * 2)\n\n    fd = { words : words_val, length : length_val, labels : labels_val, init_state : init_state_val}\n    sess.run(training_op, feed_dict=fd)\nRunning with TF 1.6, the above code ended up with the following error:\nSegmentation fault: 11\nAdditional information,\nI ran the program with dtruss, the last few lines printed on console before it crashed are as follows,\npsynch_cvsignal(0x7FDED4BC4A68, 0x90000000A00, 0x900)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4A68, 0x90100000A00, 0x900)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4E68, 0x1F0000002000, 0x1F00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4E68, 0x1F0100002000, 0x1F00)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4E68, 0x200000002100, 0x2000)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4E68, 0x200100002100, 0x2000)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4E68, 0x210000002200, 0x2100)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4E68, 0x210100002200, 0x2100)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4C68, 0x1C0000001D00, 0x1C00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4C68, 0x1C0100001D00, 0x1C00)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4A68, 0xA0000000B00, 0xA00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4A68, 0xA0100000B00, 0xA00)\t\t = 0 0\npsynch_cvsignal(0x7FDED4BC4D68, 0xD0000000E00, 0xD00)\t\t = 257 0\npsynch_cvwait(0x7FDED4BC4D68, 0xD0100000E00, 0xD00)\t\t = 0 0\npsynch_cvwait(0x0, 0x0, 0x0)\t\t = 0 0\npsynch_cvwait(0x7FDED4BC4F68, 0x1B0100001C00, 0x1B00)\t\t = 0 0\npsynch_cvwait(0x7FDED4BC5068, 0x30100000400, 0x300)\t\t = 0 0\npsynch_cvwait(0x7FDED4BC5168, 0x20100000300, 0x200)\t\t = 0 0\npsynch_cvwait(0x7FDEDCAEFE28, 0x10100000200, 0x100)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4068, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4168, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4268, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4368, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4568, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4468, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4668, 0x100000100, 0x0)\t\t = -1 Err#260\npsynch_cvwait(0x7FDED4BC4768, 0x100000100, 0x0)\t\t = -1 Err#260", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.11.6\r\n- **TensorFlow installed from (source or binary)**: VirtualEnv\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A (CPU only)\r\n- **GPU model and memory**: N/A (CPU only)\r\n- **Exact command to reproduce**: see the following\r\n\r\n### Describe the problem\r\nsegmentation fault happens when the computation graph contains `scan` with `bidirectional_rnn` embedded. \r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nembed_dim = 10\r\nhidden_dim = 10\r\nnum_class=10\r\n\r\nwords = tf.placeholder(tf.int32, [None, None], name='words')\r\nlength = tf.placeholder(tf.int32, [None], name='length')\r\nlabels = tf.placeholder(tf.int32, (), name='labels')\r\ninit_state = tf.placeholder(tf.float32, [None, hidden_dim * 2], name='initial_state')\r\n\r\ndef make_rnn_cell(): return tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n\r\nwith tf.variable_scope('embeddings'):\r\n    embedding = \\\r\n        tf.get_variable('parameter', shape=(100, embed_dim), dtype=tf.float32, trainable=True)\r\n    embedded  = tf.nn.embedding_lookup(embedding, words, name='lookup')\r\nwith tf.variable_scope('words_lstm'):\r\n    cell_fw = make_rnn_cell()\r\n    cell_bw = make_rnn_cell()\r\n    def step(state, inp):\r\n        data = tf.expand_dims(inp[0], axis=0)\r\n        length = tf.expand_dims(inp[1], axis=0)\r\n        fw_state = tf.split(state[inp[1], :], 2)[0]\r\n        bw_state = tf.split(state[0, :], 2)[1]\r\nonline training (feeding one training example at a time)\r\n        fw_state = tf.expand_dims(fw_state, axis=0)\r\n        bw_state = tf.expand_dims(bw_state, axis=0)\r\n        (outputs_fw, outputs_bw), _ = \\\r\n            tf.nn.bidirectional_dynamic_rnn(\r\n                cell_fw, cell_bw, embedded, sequence_length=length,\r\n                initial_state_fw=fw_state, initial_state_bw=bw_state, dtype=tf.float32\r\n            )\r\n        outputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\r\n        return outputs\r\n    outputs = tf.scan(step, (embedded, length), initializer=init_state)\r\nwith tf.variable_scope('words_attention'):\r\n    hidden = \\\r\n        tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = \\\r\n        tf.layers.dense(outputs, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 2, 1])), perm=[0, 2, 1])\r\nsentence_embedding = tf.reduce_sum(outputs * attention, axis=1)\r\nsentence_embedding = tf.expand_dims(sentence_embedding, axis=0)\r\n\r\nwith tf.variable_scope('sentence_lstm'):\r\n    cell_fw = make_rnn_cell()\r\n    cell_bw = make_rnn_cell()\r\n    (outputs_fw, outputs_bw), _ = \\\r\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sentence_embedding, dtype=tf.float32)\r\noutputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\r\nwith tf.variable_scope('sentence_attention'):\r\n    hidden = \\\r\n        tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = \\\r\n        tf.layers.dense(hidden, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention)))\r\noutputs = tf.reduce_sum(outputs * attention, axis=0)\r\noutputs = tf.expand_dims(outputs, axis=0)\r\nlogits = tf.layers.dense(outputs, units=num_class, activation=None)\r\nloss = -tf.log(tf.nn.softmax(logits)[:, labels], name='loss')\r\ntraining_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    words_val = np.random.randint(0, 100, size=(10, 100))\r\n    length_val = np.random.randint(0, 100, size=(10))\r\n    labels_val = np.random.randint(0, num_class, size=())\r\n    init_state_val = np.random.randn(10, hidden_dim * 2)\r\n\r\n    fd = { words : words_val, length : length_val, labels : labels_val, init_state : init_state_val}\r\n    sess.run(training_op, feed_dict=fd)\r\n```\r\n\r\nRunning with TF 1.6, the above code ended up with the following error:\r\n```bash\r\nSegmentation fault: 11\r\n```\r\n\r\nAdditional information,\r\nI ran the program with `dtruss`, the last few lines printed on console before it crashed are as follows,\r\n```bash\r\npsynch_cvsignal(0x7FDED4BC4A68, 0x90000000A00, 0x900)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4A68, 0x90100000A00, 0x900)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4E68, 0x1F0000002000, 0x1F00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4E68, 0x1F0100002000, 0x1F00)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4E68, 0x200000002100, 0x2000)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4E68, 0x200100002100, 0x2000)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4E68, 0x210000002200, 0x2100)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4E68, 0x210100002200, 0x2100)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4C68, 0x1C0000001D00, 0x1C00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4C68, 0x1C0100001D00, 0x1C00)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4A68, 0xA0000000B00, 0xA00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4A68, 0xA0100000B00, 0xA00)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4D68, 0xD0000000E00, 0xD00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4D68, 0xD0100000E00, 0xD00)\t\t = 0 0\r\npsynch_cvwait(0x0, 0x0, 0x0)\t\t = 0 0\r\npsynch_cvwait(0x7FDED4BC4F68, 0x1B0100001C00, 0x1B00)\t\t = 0 0\r\npsynch_cvwait(0x7FDED4BC5068, 0x30100000400, 0x300)\t\t = 0 0\r\npsynch_cvwait(0x7FDED4BC5168, 0x20100000300, 0x200)\t\t = 0 0\r\npsynch_cvwait(0x7FDEDCAEFE28, 0x10100000200, 0x100)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4068, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4168, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4268, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4368, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4568, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4468, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4668, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4768, 0x100000100, 0x0)\t\t = -1 Err#260\r\n```"}