{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146010638", "pull_request_review_id": 70898748, "id": 146010638, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjAxMDYzOA==", "diff_hunk": "@@ -0,0 +1,150 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#if GOOGLE_CUDA\n+\n+#define EIGEN_USE_GPU\n+\n+#include <complex>\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/util/cuda_kernel_helper.h\"\n+#include \"tensorflow/core/kernels/diag_op.h\"\n+\n+namespace tensorflow {\n+namespace functor {\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename T>\n+__global__ void DiagCudaKernel(const int num_threads,\n+                               const int64 size,\n+                               const T* in,\n+                               T* out) {\n+  CUDA_1D_KERNEL_LOOP(index, num_threads) {\n+    out[(1 + size) * index] = in[index];\n+  }\n+}\n+\n+template <typename T>\n+__global__ void ZeroCudaKernel(const int num_threads,\n+                               T* out) {\n+  CUDA_1D_KERNEL_LOOP(index, num_threads) {\n+    out[index] = T(0);\n+  }\n+}\n+\n+template <typename T>\n+struct DiagFunctor<GPUDevice, T> {\n+  EIGEN_ALWAYS_INLINE Status\n+  operator() (OpKernelContext* context, const int64 size,\n+              const T* in, T* out) {\n+    // CudaLaunchConfig uses an int for virtual_thread_count,\n+    // so this may overflow in extreme cases.\n+    if (size && (size * size / size) != size) {\n+      return errors::Internal(\n+          \"DiagOp got input size too large.\");\n+    }\n+\n+    // Empty tensor couldn't launch the kernel.\n+    if (size == 0) {\n+      return Status::OK();\n+    }\n+    const GPUDevice& device = context->eigen_device<GPUDevice>();\n+\n+    // Set output memory with zero elements.\n+    CudaLaunchConfig zero_config = GetCudaLaunchConfig(size*size, device);\n+    ZeroCudaKernel<<<zero_config.block_count,", "path": "tensorflow/core/kernels/diag_op_gpu.cu.cc", "position": 68, "original_position": 68, "commit_id": "dc052299c591bd1b71f48bf5efa0585b5a96350c", "original_commit_id": "dc052299c591bd1b71f48bf5efa0585b5a96350c", "user": {"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}, "body": "FWIW, I don't see why these kernels need to be separated.  It would be more efficient to only have one kernel launch.\r\n\r\nYou could simply have a check for if you're on the diagonal, then read from the input instead of set to T(0).", "created_at": "2017-10-20T16:35:01Z", "updated_at": "2017-10-20T16:35:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13666#discussion_r146010638", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13666", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146010638"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13666#discussion_r146010638"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13666"}}, "body_html": "<p>FWIW, I don't see why these kernels need to be separated.  It would be more efficient to only have one kernel launch.</p>\n<p>You could simply have a check for if you're on the diagonal, then read from the input instead of set to T(0).</p>", "body_text": "FWIW, I don't see why these kernels need to be separated.  It would be more efficient to only have one kernel launch.\nYou could simply have a check for if you're on the diagonal, then read from the input instead of set to T(0)."}