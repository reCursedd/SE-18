{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/145229062", "pull_request_review_id": 69996662, "id": 145229062, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTIyOTA2Mg==", "diff_hunk": "@@ -160,39 +97,138 @@ class DiagPartOp : public OpKernel {\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, out_shape, &output));\n+    functor::DiagPartFunctor<Device, T> diagPartFunc;\n+    diagPartFunc(context->eigen_device<Device>(),\n+                 out_shape.num_elements(),\n+                 tensor.flat<T>().data(),\n+                 output->flat<T>().data());\n+    return;\n+  }\n+};\n+\n+// Implementation of the functor specialization for CPU.\n+// \n+// According to the diagonal definition,\n+// `output[i1,..., ik, i1,..., ik] = input[i1,..., ik]`,\n+//\n+// Let the rank of input is [s1,..., sk], then any offset of input's\n+// pointer can be represent by coordinate [i1,..., ik],\n+// where `index = i1*(s2*...*sk) + i2*(s3*...*sk) +... + ik`\n+//\n+// Let new_index is the offset of output's pointer with coordinate \n+// [i1,..., ik, i1,..., ik], then we have\n+// `new_index = i1*(s2*...sk*s1*...*sk) + i2*(s3*...*sk*s1*...*sk) +... + \\\n+//              ik*(s1*...*sk) + i1*(s2*...*sk) + i2*(s3*...*sk) +... + ik\n+//            = (i1*(s2*...*sk) + i2*(s3*...*sk) +... + ik) * (1 + s1*...*sk)\n+//            = index * (1 + s1*...*sk)\n+//\n+// Let `size = s1*...*sk`, we finally have `new_index = index * (1 + size)`,\n+// which is the transfer function we use below.\n+// This trick make our implementations clear and easy to be parallel.\n+namespace functor {\n+template <typename T>\n+struct DiagFunctor<CPUDevice, T> {", "path": "tensorflow/core/kernels/diag_op.cc", "position": 213, "original_position": 194, "commit_id": "dc052299c591bd1b71f48bf5efa0585b5a96350c", "original_commit_id": "1784839823d340fc74e7ff45f5019257f4d1bf96", "user": {"login": "cwhipkey", "id": 17578177, "node_id": "MDQ6VXNlcjE3NTc4MTc3", "avatar_url": "https://avatars0.githubusercontent.com/u/17578177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cwhipkey", "html_url": "https://github.com/cwhipkey", "followers_url": "https://api.github.com/users/cwhipkey/followers", "following_url": "https://api.github.com/users/cwhipkey/following{/other_user}", "gists_url": "https://api.github.com/users/cwhipkey/gists{/gist_id}", "starred_url": "https://api.github.com/users/cwhipkey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cwhipkey/subscriptions", "organizations_url": "https://api.github.com/users/cwhipkey/orgs", "repos_url": "https://api.github.com/users/cwhipkey/repos", "events_url": "https://api.github.com/users/cwhipkey/events{/privacy}", "received_events_url": "https://api.github.com/users/cwhipkey/received_events", "type": "User", "site_admin": false}, "body": "as you wrote on the comment - making this use work_sharder.h's Shard function would also be useful - also with the benchmark to check performance.  I think that could be a separate PR after this one if you want.   (afaict, the previous CPU DiagPart kernel was also single-threaded, so this is not a regression in that regard).", "created_at": "2017-10-17T19:19:10Z", "updated_at": "2017-10-18T18:12:19Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13666#discussion_r145229062", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13666", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/145229062"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13666#discussion_r145229062"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13666"}}, "body_html": "<p>as you wrote on the comment - making this use work_sharder.h's Shard function would also be useful - also with the benchmark to check performance.  I think that could be a separate PR after this one if you want.   (afaict, the previous CPU DiagPart kernel was also single-threaded, so this is not a regression in that regard).</p>", "body_text": "as you wrote on the comment - making this use work_sharder.h's Shard function would also be useful - also with the benchmark to check performance.  I think that could be a separate PR after this one if you want.   (afaict, the previous CPU DiagPart kernel was also single-threaded, so this is not a regression in that regard)."}