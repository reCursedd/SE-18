{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416578329", "html_url": "https://github.com/tensorflow/tensorflow/pull/17821#issuecomment-416578329", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17821", "id": 416578329, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjU3ODMyOQ==", "user": {"login": "bshao001", "id": 26190682, "node_id": "MDQ6VXNlcjI2MTkwNjgy", "avatar_url": "https://avatars2.githubusercontent.com/u/26190682?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bshao001", "html_url": "https://github.com/bshao001", "followers_url": "https://api.github.com/users/bshao001/followers", "following_url": "https://api.github.com/users/bshao001/following{/other_user}", "gists_url": "https://api.github.com/users/bshao001/gists{/gist_id}", "starred_url": "https://api.github.com/users/bshao001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bshao001/subscriptions", "organizations_url": "https://api.github.com/users/bshao001/orgs", "repos_url": "https://api.github.com/users/bshao001/repos", "events_url": "https://api.github.com/users/bshao001/events{/privacy}", "received_events_url": "https://api.github.com/users/bshao001/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-28T13:11:03Z", "updated_at": "2018-08-28T13:11:03Z", "author_association": "NONE", "body_html": "<p>I tried the IndyLSTMCell in TensorFlow 1.10. It works with the default activation (tanh), but it does not work with nn_ops.relu. If you try to set the activation to relu, the loss will become NAN.  IndyGRUCell has the same problem.</p>\n<p>The relu activation does work with IndRNNCell, however, when I stack it to 4 or 6 layers, I did not see any improvement of the model capacity.</p>\n<p>The cell was placed inside tf.contrib.rnn.MultiRNNCell to get multiple layers, then tf.nn.dynamic_rnn. I tried seq2seq model (Google's NMT model) based on the new cell type as well.</p>\n<p>I tried this with both GPU version and CPU version.</p>\n<p>Any suggestion to fix/address this issue would be very appreciated. Thanks.</p>", "body_text": "I tried the IndyLSTMCell in TensorFlow 1.10. It works with the default activation (tanh), but it does not work with nn_ops.relu. If you try to set the activation to relu, the loss will become NAN.  IndyGRUCell has the same problem.\nThe relu activation does work with IndRNNCell, however, when I stack it to 4 or 6 layers, I did not see any improvement of the model capacity.\nThe cell was placed inside tf.contrib.rnn.MultiRNNCell to get multiple layers, then tf.nn.dynamic_rnn. I tried seq2seq model (Google's NMT model) based on the new cell type as well.\nI tried this with both GPU version and CPU version.\nAny suggestion to fix/address this issue would be very appreciated. Thanks.", "body": "I tried the IndyLSTMCell in TensorFlow 1.10. It works with the default activation (tanh), but it does not work with nn_ops.relu. If you try to set the activation to relu, the loss will become NAN.  IndyGRUCell has the same problem.\r\n\r\nThe relu activation does work with IndRNNCell, however, when I stack it to 4 or 6 layers, I did not see any improvement of the model capacity. \r\n\r\nThe cell was placed inside tf.contrib.rnn.MultiRNNCell to get multiple layers, then tf.nn.dynamic_rnn. I tried seq2seq model (Google's NMT model) based on the new cell type as well.\r\n\r\nI tried this with both GPU version and CPU version.\r\n\r\nAny suggestion to fix/address this issue would be very appreciated. Thanks."}