{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18007", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18007/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18007/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18007/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18007", "id": 308706446, "node_id": "MDU6SXNzdWUzMDg3MDY0NDY=", "number": 18007, "title": "Computing gradients with tf.cond bug ", "user": {"login": "DavidBegert", "id": 18473206, "node_id": "MDQ6VXNlcjE4NDczMjA2", "avatar_url": "https://avatars3.githubusercontent.com/u/18473206?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidBegert", "html_url": "https://github.com/DavidBegert", "followers_url": "https://api.github.com/users/DavidBegert/followers", "following_url": "https://api.github.com/users/DavidBegert/following{/other_user}", "gists_url": "https://api.github.com/users/DavidBegert/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidBegert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidBegert/subscriptions", "organizations_url": "https://api.github.com/users/DavidBegert/orgs", "repos_url": "https://api.github.com/users/DavidBegert/repos", "events_url": "https://api.github.com/users/DavidBegert/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidBegert/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-03-26T19:35:12Z", "updated_at": "2018-07-24T20:47:44Z", "closed_at": "2018-03-27T22:54:14Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.1</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8.0 , CUDNN 7.0.5</li>\n<li><strong>GPU model and memory</strong>:  GTX 1060 (6 GiB Memory, 6.1 Compute Power)</li>\n<li><strong>Exact command to reproduce</strong>: python test_cond_gradients.py (or whatever you call the file with source code)</li>\n</ul>\n<p>The file test_cond_gradients.py can be found  in the source code below</p>\n<h3>Describe the problem</h3>\n<p>I am using a tf.cond to stop gradients from being computed through one part of the graph - depending on which train cross entropy (TCE) is smaller. According to the time however, even with the tf.cond, gradients is still being computed through both parts of the graph which should not be the case.<br>\nIn the source code, there are three different losses tested. The first and second losses (baseline and loss1) are computed in similar time as shown if you run the script. The third loss tested (loss2) is slower than the first two, even though gradients should only be computed through one part of the graph (depending on the cond condition). <em>The second loss is also using a tf.cond but the true_fn and false_fn passed to it are identical.</em> The third should take about the same time to compute as the first two. This problem is amplified more when the network is more complex and thus back-prop takes more time but I have just created a simple example to run.</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport time\n\ndef run_test():\n    x1 = tf.placeholder(tf.float32, [64,32,32,3])\n    x2 = tf.placeholder(tf.float32, [64,32,32,3])\n\n    y = tf.placeholder(tf.int64, [64])\n\n    logits1 = simple_network(x1)\n    logits2 = simple_network(x2, reuse=True)\n\n    # TCEs\n    train_cross_entropy1 = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits1))\n    train_cross_entropy2 = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits2))\n\n    # Losses\n    loss_baseline = train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2)\n\n    loss1 = tf.cond(train_cross_entropy1 &lt; train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \n                                                                 lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2))\n\n    loss2 = tf.cond(train_cross_entropy1 &lt; train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \n                                                                 lambda: tf.stop_gradient(train_cross_entropy1) + train_cross_entropy2)\n    # Train Step\n    train_step = tf.train.AdamOptimizer()\n\n    apply_gradient_op_baseline = train_step.minimize(loss_baseline)\n\n    apply_gradient_op1 = train_step.minimize(loss1)\n\n    apply_gradient_op2 = train_step.minimize(loss2)\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        ## Baseline\n        baseline_s = time.time()\n        for i in range(1000):\n            sess.run(apply_gradient_op_baseline, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\n        print(\"Baseline time: \", time.time() - baseline_s)\n\n        ## Cond 1\n        cond1_s = time.time()\n        for i in range(1000):\n            sess.run(apply_gradient_op1, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\n        print(\"Cond1 time: \", time.time() - cond1_s)\n\n        ## Cond 2 (Should be about the same time as Cond 1)\n        cond2_s = time.time()\n        for i in range(1000):\n            sess.run(apply_gradient_op2, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\n        print(\"Cond2 time: \", time.time() - cond2_s)   \n\ndef simple_network(Y, reuse=False):\n    with tf.variable_scope(\"simple_network\", reuse=reuse):\n        K1 = tf.get_variable(\"K1\", shape=[3,3,3,16], initializer=tf.initializers.random_normal)\n        Y = tf.nn.conv2d(Y, K1, strides=[1,1,1,1], padding='SAME')\n\n        K2 = tf.get_variable(\"K2\", shape=[3,3,16,64], initializer=tf.initializers.random_normal)\n        Y = tf.nn.conv2d(Y, K2, strides=[1,1,1,1], padding='SAME')\n\n        Y = tf.reduce_mean(Y, [1,2])\n\n        FC_W = tf.get_variable(\"FC_W\", shape=[64, 10], initializer=tf.initializers.random_normal)\n        FC_B = tf.get_variable(\"FC_B\", shape=[10], initializer=tf.initializers.random_normal)\n\n        logits = tf.nn.xw_plus_b(Y, FC_W, FC_B)\n\n        return logits\n\nrun_test()\n</code></pre>\n<p>Thanks for any advice or work-arounds of this bug</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4.1\nPython version: 2.7.12\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: CUDA 8.0 , CUDNN 7.0.5\nGPU model and memory:  GTX 1060 (6 GiB Memory, 6.1 Compute Power)\nExact command to reproduce: python test_cond_gradients.py (or whatever you call the file with source code)\n\nThe file test_cond_gradients.py can be found  in the source code below\nDescribe the problem\nI am using a tf.cond to stop gradients from being computed through one part of the graph - depending on which train cross entropy (TCE) is smaller. According to the time however, even with the tf.cond, gradients is still being computed through both parts of the graph which should not be the case.\nIn the source code, there are three different losses tested. The first and second losses (baseline and loss1) are computed in similar time as shown if you run the script. The third loss tested (loss2) is slower than the first two, even though gradients should only be computed through one part of the graph (depending on the cond condition). The second loss is also using a tf.cond but the true_fn and false_fn passed to it are identical. The third should take about the same time to compute as the first two. This problem is amplified more when the network is more complex and thus back-prop takes more time but I have just created a simple example to run.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\nimport time\n\ndef run_test():\n    x1 = tf.placeholder(tf.float32, [64,32,32,3])\n    x2 = tf.placeholder(tf.float32, [64,32,32,3])\n\n    y = tf.placeholder(tf.int64, [64])\n\n    logits1 = simple_network(x1)\n    logits2 = simple_network(x2, reuse=True)\n\n    # TCEs\n    train_cross_entropy1 = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits1))\n    train_cross_entropy2 = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits2))\n\n    # Losses\n    loss_baseline = train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2)\n\n    loss1 = tf.cond(train_cross_entropy1 < train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \n                                                                 lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2))\n\n    loss2 = tf.cond(train_cross_entropy1 < train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \n                                                                 lambda: tf.stop_gradient(train_cross_entropy1) + train_cross_entropy2)\n    # Train Step\n    train_step = tf.train.AdamOptimizer()\n\n    apply_gradient_op_baseline = train_step.minimize(loss_baseline)\n\n    apply_gradient_op1 = train_step.minimize(loss1)\n\n    apply_gradient_op2 = train_step.minimize(loss2)\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        ## Baseline\n        baseline_s = time.time()\n        for i in range(1000):\n            sess.run(apply_gradient_op_baseline, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\n        print(\"Baseline time: \", time.time() - baseline_s)\n\n        ## Cond 1\n        cond1_s = time.time()\n        for i in range(1000):\n            sess.run(apply_gradient_op1, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\n        print(\"Cond1 time: \", time.time() - cond1_s)\n\n        ## Cond 2 (Should be about the same time as Cond 1)\n        cond2_s = time.time()\n        for i in range(1000):\n            sess.run(apply_gradient_op2, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\n        print(\"Cond2 time: \", time.time() - cond2_s)   \n\ndef simple_network(Y, reuse=False):\n    with tf.variable_scope(\"simple_network\", reuse=reuse):\n        K1 = tf.get_variable(\"K1\", shape=[3,3,3,16], initializer=tf.initializers.random_normal)\n        Y = tf.nn.conv2d(Y, K1, strides=[1,1,1,1], padding='SAME')\n\n        K2 = tf.get_variable(\"K2\", shape=[3,3,16,64], initializer=tf.initializers.random_normal)\n        Y = tf.nn.conv2d(Y, K2, strides=[1,1,1,1], padding='SAME')\n\n        Y = tf.reduce_mean(Y, [1,2])\n\n        FC_W = tf.get_variable(\"FC_W\", shape=[64, 10], initializer=tf.initializers.random_normal)\n        FC_B = tf.get_variable(\"FC_B\", shape=[10], initializer=tf.initializers.random_normal)\n\n        logits = tf.nn.xw_plus_b(Y, FC_W, FC_B)\n\n        return logits\n\nrun_test()\n\nThanks for any advice or work-arounds of this bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0 , CUDNN 7.0.5\r\n- **GPU model and memory**:  GTX 1060 (6 GiB Memory, 6.1 Compute Power)\r\n- **Exact command to reproduce**: python test_cond_gradients.py (or whatever you call the file with source code)\r\n\r\nThe file test_cond_gradients.py can be found  in the source code below\r\n\r\n### Describe the problem\r\nI am using a tf.cond to stop gradients from being computed through one part of the graph - depending on which train cross entropy (TCE) is smaller. According to the time however, even with the tf.cond, gradients is still being computed through both parts of the graph which should not be the case.\r\nIn the source code, there are three different losses tested. The first and second losses (baseline and loss1) are computed in similar time as shown if you run the script. The third loss tested (loss2) is slower than the first two, even though gradients should only be computed through one part of the graph (depending on the cond condition). *The second loss is also using a tf.cond but the true_fn and false_fn passed to it are identical.* The third should take about the same time to compute as the first two. This problem is amplified more when the network is more complex and thus back-prop takes more time but I have just created a simple example to run.  \r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\ndef run_test():\r\n    x1 = tf.placeholder(tf.float32, [64,32,32,3])\r\n    x2 = tf.placeholder(tf.float32, [64,32,32,3])\r\n\r\n    y = tf.placeholder(tf.int64, [64])\r\n\r\n    logits1 = simple_network(x1)\r\n    logits2 = simple_network(x2, reuse=True)\r\n\r\n    # TCEs\r\n    train_cross_entropy1 = tf.reduce_mean(\r\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits1))\r\n    train_cross_entropy2 = tf.reduce_mean(\r\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits2))\r\n\r\n    # Losses\r\n    loss_baseline = train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2)\r\n\r\n    loss1 = tf.cond(train_cross_entropy1 < train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \r\n                                                                 lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2))\r\n\r\n    loss2 = tf.cond(train_cross_entropy1 < train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \r\n                                                                 lambda: tf.stop_gradient(train_cross_entropy1) + train_cross_entropy2)\r\n    # Train Step\r\n    train_step = tf.train.AdamOptimizer()\r\n\r\n    apply_gradient_op_baseline = train_step.minimize(loss_baseline)\r\n\r\n    apply_gradient_op1 = train_step.minimize(loss1)\r\n\r\n    apply_gradient_op2 = train_step.minimize(loss2)\r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        ## Baseline\r\n        baseline_s = time.time()\r\n        for i in range(1000):\r\n            sess.run(apply_gradient_op_baseline, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\r\n        print(\"Baseline time: \", time.time() - baseline_s)\r\n\r\n        ## Cond 1\r\n        cond1_s = time.time()\r\n        for i in range(1000):\r\n            sess.run(apply_gradient_op1, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\r\n        print(\"Cond1 time: \", time.time() - cond1_s)\r\n\r\n        ## Cond 2 (Should be about the same time as Cond 1)\r\n        cond2_s = time.time()\r\n        for i in range(1000):\r\n            sess.run(apply_gradient_op2, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\r\n        print(\"Cond2 time: \", time.time() - cond2_s)   \r\n\r\ndef simple_network(Y, reuse=False):\r\n    with tf.variable_scope(\"simple_network\", reuse=reuse):\r\n        K1 = tf.get_variable(\"K1\", shape=[3,3,3,16], initializer=tf.initializers.random_normal)\r\n        Y = tf.nn.conv2d(Y, K1, strides=[1,1,1,1], padding='SAME')\r\n\r\n        K2 = tf.get_variable(\"K2\", shape=[3,3,16,64], initializer=tf.initializers.random_normal)\r\n        Y = tf.nn.conv2d(Y, K2, strides=[1,1,1,1], padding='SAME')\r\n\r\n        Y = tf.reduce_mean(Y, [1,2])\r\n\r\n        FC_W = tf.get_variable(\"FC_W\", shape=[64, 10], initializer=tf.initializers.random_normal)\r\n        FC_B = tf.get_variable(\"FC_B\", shape=[10], initializer=tf.initializers.random_normal)\r\n\r\n        logits = tf.nn.xw_plus_b(Y, FC_W, FC_B)\r\n\r\n        return logits\r\n\r\nrun_test()\r\n```\r\nThanks for any advice or work-arounds of this bug \r\n"}