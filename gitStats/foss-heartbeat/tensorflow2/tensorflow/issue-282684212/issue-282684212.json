{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15425", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15425/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15425/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15425/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15425", "id": 282684212, "node_id": "MDU6SXNzdWUyODI2ODQyMTI=", "number": 15425, "title": "TypeError: broadcast() takes 1 positional argument but 2 were given", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 21, "created_at": "2017-12-17T10:31:02Z", "updated_at": "2018-04-04T06:39:52Z", "closed_at": "2018-03-21T01:27:50Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:source</li>\n<li><strong>TensorFlow version (use command below)</strong>: b'v1.3.0-rc1-6044-g0b80606' 1.4.0    (2 days ago)</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Source code / logs</h3>\n<p>I ran <code>tensorflow/benchmarks</code>, and got the following error.</p>\n<pre><code>Traceback (most recent call last):\n  File \"tf_cnn_benchmarks.py\", line 47, in &lt;module&gt;\n    tf.app.run()\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 124, in run\n    _sys.exit(main(argv))\n  File \"tf_cnn_benchmarks.py\", line 43, in main\n    bench.run()\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 956, in run\n    return self._benchmark_cnn()\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1046, in _benchmark_cnn\n    self._build_model_single_session())\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1563, in _build_model_single_session                                                                      all_top_5_ops, phase_train)\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1370, in _build_fetches\n    self.variable_mgr.preprocess_device_grads(device_grads))\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\n    tf.add)\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 748, in _build_nccl_hybrid\n    send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\nTypeError: broadcast() takes 1 positional argument but 2 were given\n</code></pre>\n<p>The reason is that the invocation of <code>nccl.broadcast</code> is different from its signature:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/all_reduce/python/all_reduce.py#L748\">tensorflow/tensorflow/contrib/all_reduce/python/all_reduce.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 748\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/17e725c0558581cba19bd6c409698b2c3f88efe5\">17e725c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L748\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"748\"></td>\n          <td id=\"LC748\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> send_op, dst_tensors <span class=\"pl-k\">=</span> nccl.broadcast(level_2_output[w], dst_devices) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/nccl/python/ops/nccl_ops.py#L173-L182\">tensorflow/tensorflow/contrib/nccl/python/ops/nccl_ops.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 173 to 182\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/17e725c0558581cba19bd6c409698b2c3f88efe5\">17e725c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L173\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"173\"></td>\n          <td id=\"LC173\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">def</span> <span class=\"pl-en\">broadcast</span>(<span class=\"pl-smi\">tensor</span>): </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L174\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"174\"></td>\n          <td id=\"LC174\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Returns a tensor that can be efficiently transferred to other devices.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L175\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"175\"></td>\n          <td id=\"LC175\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L176\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"176\"></td>\n          <td id=\"LC176\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  Args:</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L177\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"177\"></td>\n          <td id=\"LC177\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    tensor: The tensor to send; must be assigned to a GPU device.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L178\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"178\"></td>\n          <td id=\"LC178\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L179\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"179\"></td>\n          <td id=\"LC179\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  Returns:</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L180\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"180\"></td>\n          <td id=\"LC180\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    A tensor with the value of `src_tensor`, which can be used as input to</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L181\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"181\"></td>\n          <td id=\"LC181\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">    ops on other GPU devices.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L182\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"182\"></td>\n          <td id=\"LC182\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span> </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nProblems still exists in current HEAD.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below): b'v1.3.0-rc1-6044-g0b80606' 1.4.0    (2 days ago)\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nSource code / logs\nI ran tensorflow/benchmarks, and got the following error.\nTraceback (most recent call last):\n  File \"tf_cnn_benchmarks.py\", line 47, in <module>\n    tf.app.run()\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 124, in run\n    _sys.exit(main(argv))\n  File \"tf_cnn_benchmarks.py\", line 43, in main\n    bench.run()\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 956, in run\n    return self._benchmark_cnn()\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1046, in _benchmark_cnn\n    self._build_model_single_session())\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1563, in _build_model_single_session                                                                      all_top_5_ops, phase_train)\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1370, in _build_fetches\n    self.variable_mgr.preprocess_device_grads(device_grads))\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\n    tf.add)\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 748, in _build_nccl_hybrid\n    send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\nTypeError: broadcast() takes 1 positional argument but 2 were given\n\nThe reason is that the invocation of nccl.broadcast is different from its signature:\n\n  \n    \n      tensorflow/tensorflow/contrib/all_reduce/python/all_reduce.py\n    \n    \n         Line 748\n      in\n      17e725c\n    \n    \n    \n    \n\n        \n          \n           send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices) \n        \n    \n  \n\n\n\n  \n    \n      tensorflow/tensorflow/contrib/nccl/python/ops/nccl_ops.py\n    \n    \n        Lines 173 to 182\n      in\n      17e725c\n    \n    \n    \n    \n\n        \n          \n           def broadcast(tensor): \n        \n\n        \n          \n             \"\"\"Returns a tensor that can be efficiently transferred to other devices. \n        \n\n        \n          \n            \n        \n\n        \n          \n             Args: \n        \n\n        \n          \n               tensor: The tensor to send; must be assigned to a GPU device. \n        \n\n        \n          \n            \n        \n\n        \n          \n             Returns: \n        \n\n        \n          \n               A tensor with the value of `src_tensor`, which can be used as input to \n        \n\n        \n          \n               ops on other GPU devices. \n        \n\n        \n          \n             \"\"\" \n        \n    \n  \n\n\nProblems still exists in current HEAD.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: b'v1.3.0-rc1-6044-g0b80606' 1.4.0    (2 days ago)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Source code / logs\r\nI ran `tensorflow/benchmarks`, and got the following error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_cnn_benchmarks.py\", line 47, in <module>\r\n    tf.app.run()\r\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"tf_cnn_benchmarks.py\", line 43, in main\r\n    bench.run()\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 956, in run\r\n    return self._benchmark_cnn()\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1046, in _benchmark_cnn\r\n    self._build_model_single_session())\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1563, in _build_model_single_session                                                                      all_top_5_ops, phase_train)\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1370, in _build_fetches\r\n    self.variable_mgr.preprocess_device_grads(device_grads))\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\r\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\r\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\r\n  File \"/MYHOME/tf-benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\r\n    tf.add)\r\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\r\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\r\n  File \"/MYHOME/.local/lib/python3.6/site-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 748, in _build_nccl_hybrid\r\n    send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\r\nTypeError: broadcast() takes 1 positional argument but 2 were given\r\n```\r\n\r\nThe reason is that the invocation of `nccl.broadcast` is different from its signature:\r\nhttps://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/all_reduce/python/all_reduce.py#L748\r\nhttps://github.com/tensorflow/tensorflow/blob/17e725c0558581cba19bd6c409698b2c3f88efe5/tensorflow/contrib/nccl/python/ops/nccl_ops.py#L173-L182 \r\nProblems still exists in current HEAD."}