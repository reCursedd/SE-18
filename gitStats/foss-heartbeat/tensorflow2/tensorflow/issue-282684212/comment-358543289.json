{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/358543289", "html_url": "https://github.com/tensorflow/tensorflow/issues/15425#issuecomment-358543289", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15425", "id": 358543289, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODU0MzI4OQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-18T05:38:10Z", "updated_at": "2018-01-18T05:38:10Z", "author_association": "MEMBER", "body_html": "<p>The fix is easy, you can patch your own client now if you're eager.</p>\n<p>In all_reduce.py remove these lines</p>\n<pre><code>  dst_devices = per_worker_devices[w][1:]\n  send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\n  # NOTE: need control dependency to ensure send_op executes\n  with ops.control_dependencies([send_op]):\n    with ops.device(per_worker_devices[w][0]):\n      dst_tensors.insert(0, array_ops.identity(level_2_output[w]))\n      down_values[w] = dst_tensors\n</code></pre>\n<p>and replace with these lines:</p>\n<pre><code>  dst_tensors = []\n  with ops.device(per_worker_devices[w][0]):\n    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\n  for d in per_worker_devices[w]:\n    with ops.device(d):\n      dst_tensors.append(array_ops.identity(broadcast_src))\n  down_values[w] = dst_tensors\n</code></pre>\n<p>I'm not expert at NCCL, but I wouldn't be surprised if it only works on dense tensors of numeric types.  IndexedSlices are a sparse representation, right?</p>", "body_text": "The fix is easy, you can patch your own client now if you're eager.\nIn all_reduce.py remove these lines\n  dst_devices = per_worker_devices[w][1:]\n  send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\n  # NOTE: need control dependency to ensure send_op executes\n  with ops.control_dependencies([send_op]):\n    with ops.device(per_worker_devices[w][0]):\n      dst_tensors.insert(0, array_ops.identity(level_2_output[w]))\n      down_values[w] = dst_tensors\n\nand replace with these lines:\n  dst_tensors = []\n  with ops.device(per_worker_devices[w][0]):\n    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\n  for d in per_worker_devices[w]:\n    with ops.device(d):\n      dst_tensors.append(array_ops.identity(broadcast_src))\n  down_values[w] = dst_tensors\n\nI'm not expert at NCCL, but I wouldn't be surprised if it only works on dense tensors of numeric types.  IndexedSlices are a sparse representation, right?", "body": "The fix is easy, you can patch your own client now if you're eager.\r\n\r\nIn all_reduce.py remove these lines\r\n\r\n      dst_devices = per_worker_devices[w][1:]\r\n      send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\r\n      # NOTE: need control dependency to ensure send_op executes\r\n      with ops.control_dependencies([send_op]):\r\n        with ops.device(per_worker_devices[w][0]):\r\n          dst_tensors.insert(0, array_ops.identity(level_2_output[w]))\r\n          down_values[w] = dst_tensors\r\n\r\nand replace with these lines:\r\n\r\n\r\n      dst_tensors = []\r\n      with ops.device(per_worker_devices[w][0]):\r\n        broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\r\n      for d in per_worker_devices[w]:\r\n        with ops.device(d):\r\n          dst_tensors.append(array_ops.identity(broadcast_src))\r\n      down_values[w] = dst_tensors\r\n\r\nI'm not expert at NCCL, but I wouldn't be surprised if it only works on dense tensors of numeric types.  IndexedSlices are a sparse representation, right?  "}