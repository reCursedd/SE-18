{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/359149450", "html_url": "https://github.com/tensorflow/tensorflow/issues/15425#issuecomment-359149450", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15425", "id": 359149450, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTE0OTQ1MA==", "user": {"login": "Agoniii", "id": 18502794, "node_id": "MDQ6VXNlcjE4NTAyNzk0", "avatar_url": "https://avatars1.githubusercontent.com/u/18502794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Agoniii", "html_url": "https://github.com/Agoniii", "followers_url": "https://api.github.com/users/Agoniii/followers", "following_url": "https://api.github.com/users/Agoniii/following{/other_user}", "gists_url": "https://api.github.com/users/Agoniii/gists{/gist_id}", "starred_url": "https://api.github.com/users/Agoniii/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Agoniii/subscriptions", "organizations_url": "https://api.github.com/users/Agoniii/orgs", "repos_url": "https://api.github.com/users/Agoniii/repos", "events_url": "https://api.github.com/users/Agoniii/events{/privacy}", "received_events_url": "https://api.github.com/users/Agoniii/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-20T06:24:12Z", "updated_at": "2018-01-20T06:24:12Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12320255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anpark\">@anpark</a>  I run tf_cnn_benchmarks with the commands <code>--variable_update=distributed_all_reduce --all_reduce_spec nccl/xring</code>. My problem is almost as same as yours.</p>\n<pre><code>  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1545, in _build_fetches\n    self.variable_mgr.preprocess_device_grads(device_grads))\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\n    tf.add)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 749, in _build_nccl_hybrid\n    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 187, in broadcast\n    return gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py\", line 98, in nccl_broadcast\n    \"NcclBroadcast\", input=input, shape=shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3172, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nUnimplementedError (see above for traceback): This op should be replaced during graph optimization.\n\t [[Node: allreduce_160/NcclBroadcast = NcclBroadcast[T=DT_FLOAT, shape=[1001], _device=\"/job:worker/replica:0/task:0/device:GPU:0\"](allreduce_160/Slice)]]\n\t [[Node: group_deps_3_G19865 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=-2613134839557745882, tensor_name=\"edge_55804_group_deps_3\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\n</code></pre>", "body_text": "@anpark  I run tf_cnn_benchmarks with the commands --variable_update=distributed_all_reduce --all_reduce_spec nccl/xring. My problem is almost as same as yours.\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1545, in _build_fetches\n    self.variable_mgr.preprocess_device_grads(device_grads))\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\n    tf.add)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 749, in _build_nccl_hybrid\n    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 187, in broadcast\n    return gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py\", line 98, in nccl_broadcast\n    \"NcclBroadcast\", input=input, shape=shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3172, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nUnimplementedError (see above for traceback): This op should be replaced during graph optimization.\n\t [[Node: allreduce_160/NcclBroadcast = NcclBroadcast[T=DT_FLOAT, shape=[1001], _device=\"/job:worker/replica:0/task:0/device:GPU:0\"](allreduce_160/Slice)]]\n\t [[Node: group_deps_3_G19865 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=-2613134839557745882, tensor_name=\"edge_55804_group_deps_3\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]", "body": "@anpark  I run tf_cnn_benchmarks with the commands `--variable_update=distributed_all_reduce --all_reduce_spec nccl/xring`. My problem is almost as same as yours.\r\n```\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1545, in _build_fetches\r\n    self.variable_mgr.preprocess_device_grads(device_grads))\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 386, in preprocess_device_grads\r\n    agg_small_grads_max_group=self._agg_small_grads_max_group)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 332, in sum_gradients_all_reduce\r\n    if is_hierarchical else aux_device_groups[group_index], num_shards))\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 236, in sum_grad_and_var_all_reduce\r\n    tf.add)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 780, in build_nccl_then_ring\r\n    return _build_nccl_hybrid(input_tensors, red_op, upper_level_f)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/all_reduce/python/all_reduce.py\", line 749, in _build_nccl_hybrid\r\n    broadcast_src = nccl.broadcast(array_ops.identity(level_2_output[w]))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 187, in broadcast\r\n    return gen_nccl_ops.nccl_broadcast(input=tensor, shape=tensor.shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/nccl/ops/gen_nccl_ops.py\", line 98, in nccl_broadcast\r\n    \"NcclBroadcast\", input=input, shape=shape, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3172, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnimplementedError (see above for traceback): This op should be replaced during graph optimization.\r\n\t [[Node: allreduce_160/NcclBroadcast = NcclBroadcast[T=DT_FLOAT, shape=[1001], _device=\"/job:worker/replica:0/task:0/device:GPU:0\"](allreduce_160/Slice)]]\r\n\t [[Node: group_deps_3_G19865 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=-2613134839557745882, tensor_name=\"edge_55804_group_deps_3\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n```"}