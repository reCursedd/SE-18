{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14752", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14752/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14752/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14752/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14752", "id": 275681910, "node_id": "MDU6SXNzdWUyNzU2ODE5MTA=", "number": 14752, "title": "How to solve the error: tensorflow.python.framework.errors_impl.NotFoundError: Key conv_layer3/bias not found in checkpoint", "user": {"login": "graydove", "id": 26141990, "node_id": "MDQ6VXNlcjI2MTQxOTkw", "avatar_url": "https://avatars3.githubusercontent.com/u/26141990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/graydove", "html_url": "https://github.com/graydove", "followers_url": "https://api.github.com/users/graydove/followers", "following_url": "https://api.github.com/users/graydove/following{/other_user}", "gists_url": "https://api.github.com/users/graydove/gists{/gist_id}", "starred_url": "https://api.github.com/users/graydove/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/graydove/subscriptions", "organizations_url": "https://api.github.com/users/graydove/orgs", "repos_url": "https://api.github.com/users/graydove/repos", "events_url": "https://api.github.com/users/graydove/events{/privacy}", "received_events_url": "https://api.github.com/users/graydove/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-11-21T11:43:53Z", "updated_at": "2018-04-03T01:25:24Z", "closed_at": "2018-04-03T01:25:23Z", "author_association": "NONE", "body_html": "<p>#coding=utf-8<br>\n#tensorflow 1.4<br>\n#python 3.6<br>\nimport os<br>\nimport numpy as np<br>\nimport tensorflow as tf<br>\nfrom PIL import Image</p>\n<p>#\u83b7\u53d6dataset<br>\ndef load_data(dataset_path):<br>\nimg = Image.open(dataset_path)<br>\n# \u5b9a\u4e49\u4e00\u4e2a20 \u00d7 20\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u4e00\u5171\u670940\u4e2a\u4eba\uff0c\u6bcf\u4e2a\u4eba\u90fd10\u5f20\u6837\u672c\u7167\u7247<br>\nimg_ndarray = np.asarray(img, dtype='float64') / 256        #\u56fe\u7247\u7070\u5ea6\u503c\u8f93\u51fa<br>\n#img_ndarray = np.asarray(img, dtype='float32') / 32<br>\n# \u8bb0\u5f55\u8138\u6570\u636e\u77e9\u9635\uff0c57 * 47\u4e3a\u6bcf\u5f20\u8138\u7684\u50cf\u7d20\u77e9\u9635<br>\nfaces = np.empty((400, 57 * 47))        #\u8138\u6570\u636e\u77e9\u9635<br>\n# \u8138\u6570\u636e\u77e9\u9635\u5316\u4e3a\u4e00\u7ef4\u5411\u91cf<br>\nfor row in range(20):<br>\nfor column in range(20):<br>\nfaces[20 * row + column] = np.ndarray.flatten(<br>\nimg_ndarray[row * 57: (row + 1) * 57, column * 47 : (column + 1) * 47]<br>\n)</p>\n<pre><code>label = np.zeros((400, 40))     #\u7a7a\u767d\u7684label\u77e9\u9635\nfor i in range(40):\n    label[i * 10: (i + 1) * 10, i] = 1      #\u521d\u59cb\u5206\u7c7b\u77e9\u9635\n\n# \u5c06\u6570\u636e\u5206\u6210\u8bad\u7ec3\u96c6\uff0c\u9a8c\u8bc1\u96c6\uff0c\u6d4b\u8bd5\u96c6\ntrain_data = np.empty((320, 57 * 47))\ntrain_label = np.zeros((320, 40))\n\nvaild_data = np.empty((40, 57 * 47))\nvaild_label = np.zeros((40, 40))\n\ntest_data = np.empty((40, 57 * 47))\ntest_label = np.zeros((40, 40))\n\n# \u5404\u6570\u636e\u96c6\u521d\u59cb\u5316\nfor i in range(40):\n    train_data[i * 8: i * 8 + 8] = faces[i * 10: i * 10 + 8]\n    train_label[i * 8: i * 8 + 8] = label[i * 10: i * 10 + 8]\n\n    vaild_data[i] = faces[i * 10 + 8]\n    vaild_label[i] = label[i * 10 + 8]\n\n    test_data[i] = faces[i * 10 + 9]\n    test_label[i] = label[i * 10 + 9]\n\ntrain_data = train_data.astype('float32')\nvaild_data = vaild_data.astype('float32')\ntest_data = test_data.astype('float32')\n\nreturn [\n    (train_data, train_label),\n    (vaild_data, vaild_label),\n    (test_data, test_label)\n]\n</code></pre>\n<p>def convolutional_layer(data, kernel_size, bias_size, pooling_size):        #\u6570\u636e\uff0c\u5377\u79ef\u6838\uff0c\u504f\u5dee\uff0c\u6c60<br>\nkernel = tf.get_variable(\"conv\", kernel_size, initializer=tf.random_normal_initializer())<br>\nbias = tf.get_variable('bias', bias_size, initializer=tf.random_normal_initializer())<br>\nconv = tf.nn.conv2d(data, kernel, strides=[1, 1, 1, 1], padding='SAME')     #strides\u6b65\u957f\uff0cpadding\u5377\u79ef\u65b9\u5f0f\uff0c\u8868\u793a\u5377\u79ef\u6838\u53ef\u4ee5\u505c\u7559\u5728\u56fe\u50cf\u8fb9\u7f18<br>\nlinear_output = tf.nn.relu(tf.add(conv, bias))      #\u6fc0\u6d3b\u51fd\u6570<br>\n# pooling: Tensor(\"conv_layer2/MaxPool:0\", shape=(40, 15, 12, 64), dtype=float32)<br>\npooling = tf.nn.max_pool(linear_output, ksize=pooling_size, strides=pooling_size, padding=\"SAME\")#\u6c60\u5316\u51fd\u6570<br>\nreturn pooling</p>\n<p>def linear_layer(data, weights_size, biases_size):<br>\nweights = tf.get_variable(\"weigths\", weights_size, initializer=tf.random_normal_initializer())      #\u5377\u79ef\u6743\u91cd\u77e9\u9635<br>\nbiases = tf.get_variable(\"biases\", biases_size, initializer=tf.random_normal_initializer())<br>\nreturn tf.add(tf.matmul(data, weights), biases)     #f(x) = Wx + b</p>\n<p>def convolutional_neural_network(data):<br>\n# \u6839\u636e\u7c7b\u522b\u4e2a\u6570\u5b9a\u4e49\u6700\u540e\u8f93\u51fa\u5c42\u7684\u795e\u7ecf\u5143<br>\nn_ouput_layer = 40</p>\n<pre><code>kernel_shape1 = [5, 5, 1, 32]       #\u5377\u79ef\u6838\u7684\u5927\u5c0f\nkernel_shape2 = [5, 5, 32, 64]\nkernel_shape3 = [5, 5, 64, 128]\n\nbias_shape1 = [32]      #\u7b2c\u4e00\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\nbias_shape2 = [64]      #\u7b2c\u4e8c\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\nbias_shape3 = [128]\n\nfull_conn_w_shape = [8 * 6 * 128, 1024]        #Softmax Regression\u6a21\u578b\u53c2\u6570\nfull_conn_b_shape = [1024]\n\nout_b_shape = [n_ouput_layer]\nout_w_shape = [1024, n_ouput_layer]  # \u8f93\u51fa\u5c42\u6743\u91cd\u77e9\u9635\n\ndata = tf.reshape(data, [-1, 57, 47, 1])\n\n# \u7ecf\u8fc7\u7b2c\u4e00\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 29, 24, 32]\nwith tf.variable_scope(\"conv_layer1\") as layer1:\n    layer1_output = convolutional_layer(\n        data = data,\n        kernel_size = kernel_shape1,\n        bias_size = bias_shape1,\n        pooling_size = [1, 2, 2, 1]\n    )\n# \u7ecf\u8fc7\u7b2c\u4e8c\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 15, 12, 64]\nwith tf.variable_scope(\"conv_layer2\") as layer2:\n    layer2_output = convolutional_layer(\n        data=layer1_output,\n        kernel_size=kernel_shape2,\n        bias_size=bias_shape2,\n        pooling_size=[1, 2, 2, 1]\n    )\nwith tf.variable_scope(\"conv_layer3\") as layer3:\n    layer3_output = convolutional_layer(\n        data=layer2_output,\n        kernel_size=kernel_shape3,\n        bias_size=bias_shape3,\n        pooling_size=[1, 2, 2, 1]\n    )\nwith tf.variable_scope(\"full_connection\") as full_layer4:\n    # \u8bb2\u5377\u79ef\u5c42\u5f20\u91cf\u6570\u636e\u62c9\u62102-D\u5f20\u91cf\u53ea\u6709\u6709\u4e00\u5217\u7684\u5217\u5411\u91cf\n    layer3_output_flatten = tf.contrib.layers.flatten(layer3_output)\n    layer4_output = tf.nn.relu(\n        linear_layer(\n            data=layer3_output_flatten,\n            weights_size=full_conn_w_shape,\n            biases_size=full_conn_b_shape\n        )\n    )\n\nwith tf.variable_scope(\"output\") as output_layer5:\n    output = linear_layer(\n        data=layer4_output,\n        weights_size=out_w_shape,\n        biases_size=out_b_shape\n    )\nprint(data)\nreturn output\n</code></pre>\n<p>def train_facedata(dataset, model_dir,model_path):</p>\n<pre><code># train_set_x = data[0][0]\n# train_set_y = data[0][1]\n# valid_set_x = data[1][0]\n# valid_set_y = data[1][1]\n# test_set_x = data[2][0]\n# test_set_y = data[2][1]\n# X = tf.placeholder(tf.float32, shape=(None, None), name=\"x-input\")  # \u8f93\u5165\u6570\u636e\n# Y = tf.placeholder(tf.float32, shape=(None, None), name='y-input')  # \u8f93\u5165\u6807\u7b7e\n\nbatch_size = 40\n\n# train_set_x, train_set_y = dataset[0]\n# valid_set_x, valid_set_y = dataset[1]\n# test_set_x, test_set_y = dataset[2]\ntrain_set_x = dataset[0][0]\ntrain_set_y = dataset[0][1]\nvalid_set_x = dataset[1][0]\nvalid_set_y = dataset[1][1]\ntest_set_x = dataset[2][0]\ntest_set_y = dataset[2][1]\n\nX = tf.placeholder(tf.float32, [batch_size, 57 * 47])\nY = tf.placeholder(tf.float32, [batch_size, 40])\n\npredict = convolutional_neural_network(X)\ncost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=Y))\noptimizer = tf.train.AdamOptimizer(1e-2).minimize(cost_func)\n\n# \u7528\u4e8e\u4fdd\u5b58\u8bad\u7ec3\u7684\u6700\u4f73\u6a21\u578b\nsaver = tf.train.Saver()\n#model_dir = './model'\n#model_path = model_dir + '/best.ckpt'\n\nwith tf.Session() as session:\n\n    # \u82e5\u4e0d\u5b58\u5728\u6a21\u578b\u6570\u636e\uff0c\u9700\u8981\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\n    if not os.path.exists(model_path + \".index\"):\n\n        session.run(tf.global_variables_initializer())  ##\n\n        best_loss = float('Inf')\n        for epoch in range(20):\n\n            epoch_loss = 0\n            for i in range((int)(np.shape(train_set_x)[0] / batch_size)):\n                x = train_set_x[i * batch_size: (i + 1) * batch_size]\n                y = train_set_y[i * batch_size: (i + 1) * batch_size]\n                _, cost = session.run([optimizer, cost_func], feed_dict={X: x, Y: y})\n                epoch_loss += cost\n\n            print(epoch, ' : ', epoch_loss)\n            if best_loss &gt; epoch_loss:\n                best_loss = epoch_loss\n                if not os.path.exists(model_dir):\n                    os.mkdir(model_dir)\n                    print(\"create the directory: %s\" % model_dir)\n                save_path = saver.save(session, model_path)\n                print(\"Model saved in file: %s\" % save_path)\n\n    # \u6062\u590d\u6570\u636e\u5e76\u6821\u9a8c\u548c\u6d4b\u8bd5\n    saver.restore(session, model_path)\n    correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))\n    valid_accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n    print('valid set accuracy: ', valid_accuracy.eval({X: valid_set_x, Y: valid_set_y}))\n\n    test_pred = tf.argmax(predict, 1).eval({X: test_set_x})\n    test_true = np.argmax(test_set_y, 1)\n    test_correct = correct.eval({X: test_set_x, Y: test_set_y})\n    incorrect_index = [i for i in range(np.shape(test_correct)[0]) if not test_correct[i]]\n    for i in incorrect_index:\n        print('picture person is %i, but mis-predicted as person %i'\n            %(test_true[i], test_pred[i]))\n</code></pre>\n<p>def main():<br>\ndataset_path = \"olivettifaces.gif\"<br>\ndata = load_data(dataset_path)<br>\nmodel_dir = './model'<br>\nmodel_path = model_dir + '/best.ckpt'<br>\nprint(len(data))<br>\ntrain_facedata(data, model_dir, model_path)</p>\n<p>if <strong>name</strong> == \"<strong>main</strong>\" :<br>\nmain()</p>\n<p><strong>I am sure I have set the bias and the convince of the conv_layer3, but it calls me they are not exist.</strong></p>", "body_text": "#coding=utf-8\n#tensorflow 1.4\n#python 3.6\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\n#\u83b7\u53d6dataset\ndef load_data(dataset_path):\nimg = Image.open(dataset_path)\n# \u5b9a\u4e49\u4e00\u4e2a20 \u00d7 20\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u4e00\u5171\u670940\u4e2a\u4eba\uff0c\u6bcf\u4e2a\u4eba\u90fd10\u5f20\u6837\u672c\u7167\u7247\nimg_ndarray = np.asarray(img, dtype='float64') / 256        #\u56fe\u7247\u7070\u5ea6\u503c\u8f93\u51fa\n#img_ndarray = np.asarray(img, dtype='float32') / 32\n# \u8bb0\u5f55\u8138\u6570\u636e\u77e9\u9635\uff0c57 * 47\u4e3a\u6bcf\u5f20\u8138\u7684\u50cf\u7d20\u77e9\u9635\nfaces = np.empty((400, 57 * 47))        #\u8138\u6570\u636e\u77e9\u9635\n# \u8138\u6570\u636e\u77e9\u9635\u5316\u4e3a\u4e00\u7ef4\u5411\u91cf\nfor row in range(20):\nfor column in range(20):\nfaces[20 * row + column] = np.ndarray.flatten(\nimg_ndarray[row * 57: (row + 1) * 57, column * 47 : (column + 1) * 47]\n)\nlabel = np.zeros((400, 40))     #\u7a7a\u767d\u7684label\u77e9\u9635\nfor i in range(40):\n    label[i * 10: (i + 1) * 10, i] = 1      #\u521d\u59cb\u5206\u7c7b\u77e9\u9635\n\n# \u5c06\u6570\u636e\u5206\u6210\u8bad\u7ec3\u96c6\uff0c\u9a8c\u8bc1\u96c6\uff0c\u6d4b\u8bd5\u96c6\ntrain_data = np.empty((320, 57 * 47))\ntrain_label = np.zeros((320, 40))\n\nvaild_data = np.empty((40, 57 * 47))\nvaild_label = np.zeros((40, 40))\n\ntest_data = np.empty((40, 57 * 47))\ntest_label = np.zeros((40, 40))\n\n# \u5404\u6570\u636e\u96c6\u521d\u59cb\u5316\nfor i in range(40):\n    train_data[i * 8: i * 8 + 8] = faces[i * 10: i * 10 + 8]\n    train_label[i * 8: i * 8 + 8] = label[i * 10: i * 10 + 8]\n\n    vaild_data[i] = faces[i * 10 + 8]\n    vaild_label[i] = label[i * 10 + 8]\n\n    test_data[i] = faces[i * 10 + 9]\n    test_label[i] = label[i * 10 + 9]\n\ntrain_data = train_data.astype('float32')\nvaild_data = vaild_data.astype('float32')\ntest_data = test_data.astype('float32')\n\nreturn [\n    (train_data, train_label),\n    (vaild_data, vaild_label),\n    (test_data, test_label)\n]\n\ndef convolutional_layer(data, kernel_size, bias_size, pooling_size):        #\u6570\u636e\uff0c\u5377\u79ef\u6838\uff0c\u504f\u5dee\uff0c\u6c60\nkernel = tf.get_variable(\"conv\", kernel_size, initializer=tf.random_normal_initializer())\nbias = tf.get_variable('bias', bias_size, initializer=tf.random_normal_initializer())\nconv = tf.nn.conv2d(data, kernel, strides=[1, 1, 1, 1], padding='SAME')     #strides\u6b65\u957f\uff0cpadding\u5377\u79ef\u65b9\u5f0f\uff0c\u8868\u793a\u5377\u79ef\u6838\u53ef\u4ee5\u505c\u7559\u5728\u56fe\u50cf\u8fb9\u7f18\nlinear_output = tf.nn.relu(tf.add(conv, bias))      #\u6fc0\u6d3b\u51fd\u6570\n# pooling: Tensor(\"conv_layer2/MaxPool:0\", shape=(40, 15, 12, 64), dtype=float32)\npooling = tf.nn.max_pool(linear_output, ksize=pooling_size, strides=pooling_size, padding=\"SAME\")#\u6c60\u5316\u51fd\u6570\nreturn pooling\ndef linear_layer(data, weights_size, biases_size):\nweights = tf.get_variable(\"weigths\", weights_size, initializer=tf.random_normal_initializer())      #\u5377\u79ef\u6743\u91cd\u77e9\u9635\nbiases = tf.get_variable(\"biases\", biases_size, initializer=tf.random_normal_initializer())\nreturn tf.add(tf.matmul(data, weights), biases)     #f(x) = Wx + b\ndef convolutional_neural_network(data):\n# \u6839\u636e\u7c7b\u522b\u4e2a\u6570\u5b9a\u4e49\u6700\u540e\u8f93\u51fa\u5c42\u7684\u795e\u7ecf\u5143\nn_ouput_layer = 40\nkernel_shape1 = [5, 5, 1, 32]       #\u5377\u79ef\u6838\u7684\u5927\u5c0f\nkernel_shape2 = [5, 5, 32, 64]\nkernel_shape3 = [5, 5, 64, 128]\n\nbias_shape1 = [32]      #\u7b2c\u4e00\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\nbias_shape2 = [64]      #\u7b2c\u4e8c\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\nbias_shape3 = [128]\n\nfull_conn_w_shape = [8 * 6 * 128, 1024]        #Softmax Regression\u6a21\u578b\u53c2\u6570\nfull_conn_b_shape = [1024]\n\nout_b_shape = [n_ouput_layer]\nout_w_shape = [1024, n_ouput_layer]  # \u8f93\u51fa\u5c42\u6743\u91cd\u77e9\u9635\n\ndata = tf.reshape(data, [-1, 57, 47, 1])\n\n# \u7ecf\u8fc7\u7b2c\u4e00\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 29, 24, 32]\nwith tf.variable_scope(\"conv_layer1\") as layer1:\n    layer1_output = convolutional_layer(\n        data = data,\n        kernel_size = kernel_shape1,\n        bias_size = bias_shape1,\n        pooling_size = [1, 2, 2, 1]\n    )\n# \u7ecf\u8fc7\u7b2c\u4e8c\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 15, 12, 64]\nwith tf.variable_scope(\"conv_layer2\") as layer2:\n    layer2_output = convolutional_layer(\n        data=layer1_output,\n        kernel_size=kernel_shape2,\n        bias_size=bias_shape2,\n        pooling_size=[1, 2, 2, 1]\n    )\nwith tf.variable_scope(\"conv_layer3\") as layer3:\n    layer3_output = convolutional_layer(\n        data=layer2_output,\n        kernel_size=kernel_shape3,\n        bias_size=bias_shape3,\n        pooling_size=[1, 2, 2, 1]\n    )\nwith tf.variable_scope(\"full_connection\") as full_layer4:\n    # \u8bb2\u5377\u79ef\u5c42\u5f20\u91cf\u6570\u636e\u62c9\u62102-D\u5f20\u91cf\u53ea\u6709\u6709\u4e00\u5217\u7684\u5217\u5411\u91cf\n    layer3_output_flatten = tf.contrib.layers.flatten(layer3_output)\n    layer4_output = tf.nn.relu(\n        linear_layer(\n            data=layer3_output_flatten,\n            weights_size=full_conn_w_shape,\n            biases_size=full_conn_b_shape\n        )\n    )\n\nwith tf.variable_scope(\"output\") as output_layer5:\n    output = linear_layer(\n        data=layer4_output,\n        weights_size=out_w_shape,\n        biases_size=out_b_shape\n    )\nprint(data)\nreturn output\n\ndef train_facedata(dataset, model_dir,model_path):\n# train_set_x = data[0][0]\n# train_set_y = data[0][1]\n# valid_set_x = data[1][0]\n# valid_set_y = data[1][1]\n# test_set_x = data[2][0]\n# test_set_y = data[2][1]\n# X = tf.placeholder(tf.float32, shape=(None, None), name=\"x-input\")  # \u8f93\u5165\u6570\u636e\n# Y = tf.placeholder(tf.float32, shape=(None, None), name='y-input')  # \u8f93\u5165\u6807\u7b7e\n\nbatch_size = 40\n\n# train_set_x, train_set_y = dataset[0]\n# valid_set_x, valid_set_y = dataset[1]\n# test_set_x, test_set_y = dataset[2]\ntrain_set_x = dataset[0][0]\ntrain_set_y = dataset[0][1]\nvalid_set_x = dataset[1][0]\nvalid_set_y = dataset[1][1]\ntest_set_x = dataset[2][0]\ntest_set_y = dataset[2][1]\n\nX = tf.placeholder(tf.float32, [batch_size, 57 * 47])\nY = tf.placeholder(tf.float32, [batch_size, 40])\n\npredict = convolutional_neural_network(X)\ncost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=Y))\noptimizer = tf.train.AdamOptimizer(1e-2).minimize(cost_func)\n\n# \u7528\u4e8e\u4fdd\u5b58\u8bad\u7ec3\u7684\u6700\u4f73\u6a21\u578b\nsaver = tf.train.Saver()\n#model_dir = './model'\n#model_path = model_dir + '/best.ckpt'\n\nwith tf.Session() as session:\n\n    # \u82e5\u4e0d\u5b58\u5728\u6a21\u578b\u6570\u636e\uff0c\u9700\u8981\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\n    if not os.path.exists(model_path + \".index\"):\n\n        session.run(tf.global_variables_initializer())  ##\n\n        best_loss = float('Inf')\n        for epoch in range(20):\n\n            epoch_loss = 0\n            for i in range((int)(np.shape(train_set_x)[0] / batch_size)):\n                x = train_set_x[i * batch_size: (i + 1) * batch_size]\n                y = train_set_y[i * batch_size: (i + 1) * batch_size]\n                _, cost = session.run([optimizer, cost_func], feed_dict={X: x, Y: y})\n                epoch_loss += cost\n\n            print(epoch, ' : ', epoch_loss)\n            if best_loss > epoch_loss:\n                best_loss = epoch_loss\n                if not os.path.exists(model_dir):\n                    os.mkdir(model_dir)\n                    print(\"create the directory: %s\" % model_dir)\n                save_path = saver.save(session, model_path)\n                print(\"Model saved in file: %s\" % save_path)\n\n    # \u6062\u590d\u6570\u636e\u5e76\u6821\u9a8c\u548c\u6d4b\u8bd5\n    saver.restore(session, model_path)\n    correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))\n    valid_accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n    print('valid set accuracy: ', valid_accuracy.eval({X: valid_set_x, Y: valid_set_y}))\n\n    test_pred = tf.argmax(predict, 1).eval({X: test_set_x})\n    test_true = np.argmax(test_set_y, 1)\n    test_correct = correct.eval({X: test_set_x, Y: test_set_y})\n    incorrect_index = [i for i in range(np.shape(test_correct)[0]) if not test_correct[i]]\n    for i in incorrect_index:\n        print('picture person is %i, but mis-predicted as person %i'\n            %(test_true[i], test_pred[i]))\n\ndef main():\ndataset_path = \"olivettifaces.gif\"\ndata = load_data(dataset_path)\nmodel_dir = './model'\nmodel_path = model_dir + '/best.ckpt'\nprint(len(data))\ntrain_facedata(data, model_dir, model_path)\nif name == \"main\" :\nmain()\nI am sure I have set the bias and the convince of the conv_layer3, but it calls me they are not exist.", "body": "\r\n#coding=utf-8\r\n#tensorflow 1.4\r\n#python 3.6\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\n\r\n#\u83b7\u53d6dataset\r\ndef load_data(dataset_path):\r\n    img = Image.open(dataset_path)\r\n    # \u5b9a\u4e49\u4e00\u4e2a20 \u00d7 20\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u4e00\u5171\u670940\u4e2a\u4eba\uff0c\u6bcf\u4e2a\u4eba\u90fd10\u5f20\u6837\u672c\u7167\u7247\r\n    img_ndarray = np.asarray(img, dtype='float64') / 256        #\u56fe\u7247\u7070\u5ea6\u503c\u8f93\u51fa\r\n    #img_ndarray = np.asarray(img, dtype='float32') / 32\r\n    # \u8bb0\u5f55\u8138\u6570\u636e\u77e9\u9635\uff0c57 * 47\u4e3a\u6bcf\u5f20\u8138\u7684\u50cf\u7d20\u77e9\u9635\r\n    faces = np.empty((400, 57 * 47))        #\u8138\u6570\u636e\u77e9\u9635\r\n    # \u8138\u6570\u636e\u77e9\u9635\u5316\u4e3a\u4e00\u7ef4\u5411\u91cf\r\n    for row in range(20):\r\n        for column in range(20):\r\n            faces[20 * row + column] = np.ndarray.flatten(\r\n                img_ndarray[row * 57: (row + 1) * 57, column * 47 : (column + 1) * 47]\r\n                )\r\n\r\n    label = np.zeros((400, 40))     #\u7a7a\u767d\u7684label\u77e9\u9635\r\n    for i in range(40):\r\n        label[i * 10: (i + 1) * 10, i] = 1      #\u521d\u59cb\u5206\u7c7b\u77e9\u9635\r\n\r\n    # \u5c06\u6570\u636e\u5206\u6210\u8bad\u7ec3\u96c6\uff0c\u9a8c\u8bc1\u96c6\uff0c\u6d4b\u8bd5\u96c6\r\n    train_data = np.empty((320, 57 * 47))\r\n    train_label = np.zeros((320, 40))\r\n\r\n    vaild_data = np.empty((40, 57 * 47))\r\n    vaild_label = np.zeros((40, 40))\r\n\r\n    test_data = np.empty((40, 57 * 47))\r\n    test_label = np.zeros((40, 40))\r\n\r\n    # \u5404\u6570\u636e\u96c6\u521d\u59cb\u5316\r\n    for i in range(40):\r\n        train_data[i * 8: i * 8 + 8] = faces[i * 10: i * 10 + 8]\r\n        train_label[i * 8: i * 8 + 8] = label[i * 10: i * 10 + 8]\r\n\r\n        vaild_data[i] = faces[i * 10 + 8]\r\n        vaild_label[i] = label[i * 10 + 8]\r\n\r\n        test_data[i] = faces[i * 10 + 9]\r\n        test_label[i] = label[i * 10 + 9]\r\n\r\n    train_data = train_data.astype('float32')\r\n    vaild_data = vaild_data.astype('float32')\r\n    test_data = test_data.astype('float32')\r\n\r\n    return [\r\n        (train_data, train_label),\r\n        (vaild_data, vaild_label),\r\n        (test_data, test_label)\r\n    ]\r\n\r\ndef convolutional_layer(data, kernel_size, bias_size, pooling_size):        #\u6570\u636e\uff0c\u5377\u79ef\u6838\uff0c\u504f\u5dee\uff0c\u6c60\r\n    kernel = tf.get_variable(\"conv\", kernel_size, initializer=tf.random_normal_initializer())\r\n    bias = tf.get_variable('bias', bias_size, initializer=tf.random_normal_initializer())\r\n    conv = tf.nn.conv2d(data, kernel, strides=[1, 1, 1, 1], padding='SAME')     #strides\u6b65\u957f\uff0cpadding\u5377\u79ef\u65b9\u5f0f\uff0c\u8868\u793a\u5377\u79ef\u6838\u53ef\u4ee5\u505c\u7559\u5728\u56fe\u50cf\u8fb9\u7f18\r\n    linear_output = tf.nn.relu(tf.add(conv, bias))      #\u6fc0\u6d3b\u51fd\u6570\r\n    # pooling: Tensor(\"conv_layer2/MaxPool:0\", shape=(40, 15, 12, 64), dtype=float32)\r\n    pooling = tf.nn.max_pool(linear_output, ksize=pooling_size, strides=pooling_size, padding=\"SAME\")#\u6c60\u5316\u51fd\u6570\r\n    return pooling\r\n\r\ndef linear_layer(data, weights_size, biases_size):\r\n    weights = tf.get_variable(\"weigths\", weights_size, initializer=tf.random_normal_initializer())      #\u5377\u79ef\u6743\u91cd\u77e9\u9635\r\n    biases = tf.get_variable(\"biases\", biases_size, initializer=tf.random_normal_initializer())\r\n    return tf.add(tf.matmul(data, weights), biases)     #f(x) = Wx + b\r\n\r\ndef convolutional_neural_network(data):\r\n    # \u6839\u636e\u7c7b\u522b\u4e2a\u6570\u5b9a\u4e49\u6700\u540e\u8f93\u51fa\u5c42\u7684\u795e\u7ecf\u5143\r\n    n_ouput_layer = 40\r\n\r\n    kernel_shape1 = [5, 5, 1, 32]       #\u5377\u79ef\u6838\u7684\u5927\u5c0f\r\n    kernel_shape2 = [5, 5, 32, 64]\r\n    kernel_shape3 = [5, 5, 64, 128]\r\n\r\n    bias_shape1 = [32]      #\u7b2c\u4e00\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\r\n    bias_shape2 = [64]      #\u7b2c\u4e8c\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\r\n    bias_shape3 = [128]\r\n\r\n    full_conn_w_shape = [8 * 6 * 128, 1024]        #Softmax Regression\u6a21\u578b\u53c2\u6570\r\n    full_conn_b_shape = [1024]\r\n\r\n    out_b_shape = [n_ouput_layer]\r\n    out_w_shape = [1024, n_ouput_layer]  # \u8f93\u51fa\u5c42\u6743\u91cd\u77e9\u9635\r\n\r\n    data = tf.reshape(data, [-1, 57, 47, 1])\r\n\r\n    # \u7ecf\u8fc7\u7b2c\u4e00\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 29, 24, 32]\r\n    with tf.variable_scope(\"conv_layer1\") as layer1:\r\n        layer1_output = convolutional_layer(\r\n            data = data,\r\n            kernel_size = kernel_shape1,\r\n            bias_size = bias_shape1,\r\n            pooling_size = [1, 2, 2, 1]\r\n        )\r\n    # \u7ecf\u8fc7\u7b2c\u4e8c\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 15, 12, 64]\r\n    with tf.variable_scope(\"conv_layer2\") as layer2:\r\n        layer2_output = convolutional_layer(\r\n            data=layer1_output,\r\n            kernel_size=kernel_shape2,\r\n            bias_size=bias_shape2,\r\n            pooling_size=[1, 2, 2, 1]\r\n        )\r\n    with tf.variable_scope(\"conv_layer3\") as layer3:\r\n        layer3_output = convolutional_layer(\r\n            data=layer2_output,\r\n            kernel_size=kernel_shape3,\r\n            bias_size=bias_shape3,\r\n            pooling_size=[1, 2, 2, 1]\r\n        )\r\n    with tf.variable_scope(\"full_connection\") as full_layer4:\r\n        # \u8bb2\u5377\u79ef\u5c42\u5f20\u91cf\u6570\u636e\u62c9\u62102-D\u5f20\u91cf\u53ea\u6709\u6709\u4e00\u5217\u7684\u5217\u5411\u91cf\r\n        layer3_output_flatten = tf.contrib.layers.flatten(layer3_output)\r\n        layer4_output = tf.nn.relu(\r\n            linear_layer(\r\n                data=layer3_output_flatten,\r\n                weights_size=full_conn_w_shape,\r\n                biases_size=full_conn_b_shape\r\n            )\r\n        )\r\n\r\n    with tf.variable_scope(\"output\") as output_layer5:\r\n        output = linear_layer(\r\n            data=layer4_output,\r\n            weights_size=out_w_shape,\r\n            biases_size=out_b_shape\r\n        )\r\n    print(data)\r\n    return output\r\n\r\ndef train_facedata(dataset, model_dir,model_path):\r\n\r\n    # train_set_x = data[0][0]\r\n    # train_set_y = data[0][1]\r\n    # valid_set_x = data[1][0]\r\n    # valid_set_y = data[1][1]\r\n    # test_set_x = data[2][0]\r\n    # test_set_y = data[2][1]\r\n    # X = tf.placeholder(tf.float32, shape=(None, None), name=\"x-input\")  # \u8f93\u5165\u6570\u636e\r\n    # Y = tf.placeholder(tf.float32, shape=(None, None), name='y-input')  # \u8f93\u5165\u6807\u7b7e\r\n\r\n    batch_size = 40\r\n\r\n    # train_set_x, train_set_y = dataset[0]\r\n    # valid_set_x, valid_set_y = dataset[1]\r\n    # test_set_x, test_set_y = dataset[2]\r\n    train_set_x = dataset[0][0]\r\n    train_set_y = dataset[0][1]\r\n    valid_set_x = dataset[1][0]\r\n    valid_set_y = dataset[1][1]\r\n    test_set_x = dataset[2][0]\r\n    test_set_y = dataset[2][1]\r\n\r\n    X = tf.placeholder(tf.float32, [batch_size, 57 * 47])\r\n    Y = tf.placeholder(tf.float32, [batch_size, 40])\r\n\r\n    predict = convolutional_neural_network(X)\r\n    cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=Y))\r\n    optimizer = tf.train.AdamOptimizer(1e-2).minimize(cost_func)\r\n\r\n    # \u7528\u4e8e\u4fdd\u5b58\u8bad\u7ec3\u7684\u6700\u4f73\u6a21\u578b\r\n    saver = tf.train.Saver()\r\n    #model_dir = './model'\r\n    #model_path = model_dir + '/best.ckpt'\r\n\r\n    with tf.Session() as session:\r\n\r\n        # \u82e5\u4e0d\u5b58\u5728\u6a21\u578b\u6570\u636e\uff0c\u9700\u8981\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\r\n        if not os.path.exists(model_path + \".index\"):\r\n\r\n            session.run(tf.global_variables_initializer())  ##\r\n\r\n            best_loss = float('Inf')\r\n            for epoch in range(20):\r\n\r\n                epoch_loss = 0\r\n                for i in range((int)(np.shape(train_set_x)[0] / batch_size)):\r\n                    x = train_set_x[i * batch_size: (i + 1) * batch_size]\r\n                    y = train_set_y[i * batch_size: (i + 1) * batch_size]\r\n                    _, cost = session.run([optimizer, cost_func], feed_dict={X: x, Y: y})\r\n                    epoch_loss += cost\r\n\r\n                print(epoch, ' : ', epoch_loss)\r\n                if best_loss > epoch_loss:\r\n                    best_loss = epoch_loss\r\n                    if not os.path.exists(model_dir):\r\n                        os.mkdir(model_dir)\r\n                        print(\"create the directory: %s\" % model_dir)\r\n                    save_path = saver.save(session, model_path)\r\n                    print(\"Model saved in file: %s\" % save_path)\r\n\r\n        # \u6062\u590d\u6570\u636e\u5e76\u6821\u9a8c\u548c\u6d4b\u8bd5\r\n        saver.restore(session, model_path)\r\n        correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))\r\n        valid_accuracy = tf.reduce_mean(tf.cast(correct,'float'))\r\n        print('valid set accuracy: ', valid_accuracy.eval({X: valid_set_x, Y: valid_set_y}))\r\n\r\n        test_pred = tf.argmax(predict, 1).eval({X: test_set_x})\r\n        test_true = np.argmax(test_set_y, 1)\r\n        test_correct = correct.eval({X: test_set_x, Y: test_set_y})\r\n        incorrect_index = [i for i in range(np.shape(test_correct)[0]) if not test_correct[i]]\r\n        for i in incorrect_index:\r\n            print('picture person is %i, but mis-predicted as person %i'\r\n                %(test_true[i], test_pred[i]))\r\n\r\n\r\ndef main():\r\n    dataset_path = \"olivettifaces.gif\"\r\n    data = load_data(dataset_path)\r\n    model_dir = './model'\r\n    model_path = model_dir + '/best.ckpt'\r\n    print(len(data))\r\n    train_facedata(data, model_dir, model_path)\r\n\r\nif __name__ == \"__main__\" :\r\n    main()\r\n\r\n**I am sure I have set the bias and the convince of the conv_layer3, but it calls me they are not exist.**"}