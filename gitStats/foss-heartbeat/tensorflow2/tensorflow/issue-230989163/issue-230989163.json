{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10155", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10155/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10155/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10155/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10155", "id": 230989163, "node_id": "MDU6SXNzdWUyMzA5ODkxNjM=", "number": 10155, "title": "Estimator should be able to partially load checkpoints", "user": {"login": "andreas-eberle", "id": 9267365, "node_id": "MDQ6VXNlcjkyNjczNjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/9267365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andreas-eberle", "html_url": "https://github.com/andreas-eberle", "followers_url": "https://api.github.com/users/andreas-eberle/followers", "following_url": "https://api.github.com/users/andreas-eberle/following{/other_user}", "gists_url": "https://api.github.com/users/andreas-eberle/gists{/gist_id}", "starred_url": "https://api.github.com/users/andreas-eberle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andreas-eberle/subscriptions", "organizations_url": "https://api.github.com/users/andreas-eberle/orgs", "repos_url": "https://api.github.com/users/andreas-eberle/repos", "events_url": "https://api.github.com/users/andreas-eberle/events{/privacy}", "received_events_url": "https://api.github.com/users/andreas-eberle/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 26, "created_at": "2017-05-24T10:14:33Z", "updated_at": "2018-11-14T13:17:12Z", "closed_at": "2018-02-06T15:53:44Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Describe the problem</h3>\n<p>When training neural networks and experimenting with different architectures or simply adapting a model to a new number of classes, it is crucial to be able to reuse an existing trained model as far as possible. For example, if I want to use the inception-v4 architecture and train it on 700 instead of 1000 classes, I need to be able to load all layers but the logit ones.</p>\n<p>Unfortunately, this is not possible (at least I wasn't able to find a way) with the Estimator API. Whenever the size of a variable in my model changes or I add or remove a variable, the Estimator cannot load an existing checkpoint any more. This is a major drawback making the Estimator basically unusable for developing a new architecture or adapting an existing one by iteratively adapting the model.</p>\n<h3>Requested features</h3>\n<ul>\n<li>It should be possible to tell the Estimator that it's ok if some variables aren't found in the checkpoint. Those should simply be initialized as if no checkpoint would be loaded.</li>\n<li>It should be possible to specify scopes that should not be loaded from the checkpoint or to specify a flag that says something like \"just don't load variables that have a different shape / that you can't load\".</li>\n<li>Be able to load an existing checkpoint from a different path than the Estimator's <code>model_dir</code> when there is no checkpoint in the <code>model_dir</code> yet. This is helpful to start training from a different checkpoint without manully having to copy those model's checkpoints into the new <code>model_dir</code></li>\n</ul>\n<h3>Inspiration</h3>\n<p>This request has been inspired by the parameters you can specify to the <a href=\"https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py\">train_image_classifier.py</a> script from the tensorflow-models/slim directory. There you have the parameters <code>--checkpoint_exclude_scopes</code>, <code>--ignore_missing_vars</code> and <code>--checkpoint_path</code>.</p>\n<p>Of course, one could say it's possible to implement this manually. But I think these are basic functionalities for everyone doing a bit more deeplearning than only the tutorial. That's why I think this should be part of the otherwise easy to use Estimator API.</p>", "body_text": "Describe the problem\nWhen training neural networks and experimenting with different architectures or simply adapting a model to a new number of classes, it is crucial to be able to reuse an existing trained model as far as possible. For example, if I want to use the inception-v4 architecture and train it on 700 instead of 1000 classes, I need to be able to load all layers but the logit ones.\nUnfortunately, this is not possible (at least I wasn't able to find a way) with the Estimator API. Whenever the size of a variable in my model changes or I add or remove a variable, the Estimator cannot load an existing checkpoint any more. This is a major drawback making the Estimator basically unusable for developing a new architecture or adapting an existing one by iteratively adapting the model.\nRequested features\n\nIt should be possible to tell the Estimator that it's ok if some variables aren't found in the checkpoint. Those should simply be initialized as if no checkpoint would be loaded.\nIt should be possible to specify scopes that should not be loaded from the checkpoint or to specify a flag that says something like \"just don't load variables that have a different shape / that you can't load\".\nBe able to load an existing checkpoint from a different path than the Estimator's model_dir when there is no checkpoint in the model_dir yet. This is helpful to start training from a different checkpoint without manully having to copy those model's checkpoints into the new model_dir\n\nInspiration\nThis request has been inspired by the parameters you can specify to the train_image_classifier.py script from the tensorflow-models/slim directory. There you have the parameters --checkpoint_exclude_scopes, --ignore_missing_vars and --checkpoint_path.\nOf course, one could say it's possible to implement this manually. But I think these are basic functionalities for everyone doing a bit more deeplearning than only the tutorial. That's why I think this should be part of the otherwise easy to use Estimator API.", "body": "### Describe the problem\r\nWhen training neural networks and experimenting with different architectures or simply adapting a model to a new number of classes, it is crucial to be able to reuse an existing trained model as far as possible. For example, if I want to use the inception-v4 architecture and train it on 700 instead of 1000 classes, I need to be able to load all layers but the logit ones.\r\n\r\nUnfortunately, this is not possible (at least I wasn't able to find a way) with the Estimator API. Whenever the size of a variable in my model changes or I add or remove a variable, the Estimator cannot load an existing checkpoint any more. This is a major drawback making the Estimator basically unusable for developing a new architecture or adapting an existing one by iteratively adapting the model.\r\n\r\n### Requested features\r\n* It should be possible to tell the Estimator that it's ok if some variables aren't found in the checkpoint. Those should simply be initialized as if no checkpoint would be loaded.\r\n* It should be possible to specify scopes that should not be loaded from the checkpoint or to specify a flag that says something like \"just don't load variables that have a different shape / that you can't load\".\r\n* Be able to load an existing checkpoint from a different path than the Estimator's `model_dir` when there is no checkpoint in the `model_dir` yet. This is helpful to start training from a different checkpoint without manully having to copy those model's checkpoints into the new `model_dir`\r\n\r\n### Inspiration\r\nThis request has been inspired by the parameters you can specify to the [train_image_classifier.py](https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py) script from the tensorflow-models/slim directory. There you have the parameters `--checkpoint_exclude_scopes`, `--ignore_missing_vars` and `--checkpoint_path`.\r\n\r\nOf course, one could say it's possible to implement this manually. But I think these are basic functionalities for everyone doing a bit more deeplearning than only the tutorial. That's why I think this should be part of the otherwise easy to use Estimator API. "}