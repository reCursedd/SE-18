{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/138800360", "pull_request_review_id": 62643367, "id": 138800360, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzODgwMDM2MA==", "diff_hunk": "@@ -687,6 +687,22 @@ Status MeanGrad(const Scope& scope, const Operation& op,\n }\n REGISTER_GRADIENT_OP(\"Mean\", MeanGrad);\n \n+Status ErfGrad(const Scope& scope, const Operation& op,\n+               const std::vector<Output>& grad_inputs,\n+               std::vector<Output>* grad_outputs) {\n+  auto grad = grad_inputs[0];\n+  auto two_over_root_pi = Cast(scope, Const(scope, 2 / std::sqrt(M_PI)),\n+                               grad.type());\n+  Scope grad_scope = scope.WithControlDependencies(grad);\n+  auto x = ConjugateHelper(grad_scope, op.input(0));\n+  auto dx = Mul(scope,", "path": "tensorflow/cc/gradients/math_grad.cc", "position": null, "original_position": 12, "commit_id": "dbceda103f61ca2c1431483d8f5c664c3af1da7b", "original_commit_id": "12e3316e123ef60e8c2cca64dcb68a91b3243100", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "body": "Hi, @suharshs . \r\n\r\n`grade_scope` is used to keep consistent with its corresponding implementation in python.\r\n\r\n```python\r\ndef _ErfGrad(op, grad):\r\n  \"\"\"Returns grad * 2/sqrt(pi) * exp(-x**2).\"\"\"\r\n  x = op.inputs[0]\r\n  two_over_root_pi = constant_op.constant(2 / np.sqrt(np.pi), dtype=grad.dtype)\r\n  with ops.control_dependencies([grad]):\r\n    x = math_ops.conj(x)\r\n    return grad * two_over_root_pi * math_ops.exp(-math_ops.square(x))\r\n```\r\n\r\nHowever, in my opinion, `with ops.control_dependencies([grad])` seems useless here. Moreover, I found a lot of gradients remove the dependency as well, take AtanhGrad for example\r\n\r\n```python\r\n@ops.RegisterGradient(\"Atanh\")\r\ndef _AtanhGrad(op, grad):\r\n  \"\"\"Returns grad * 1/ (1 - x^2).\"\"\"\r\n  x = op.inputs[0]\r\n  with ops.control_dependencies([grad]):\r\n    x = math_ops.conj(x)\r\n    x2 = math_ops.square(x)\r\n    one = constant_op.constant(1, dtype=grad.dtype)\r\n    inv = math_ops.reciprocal(math_ops.subtract(one, x2))\r\n    return grad * inv\r\n```\r\n\r\n```c++\r\nStatus AtanhGrad(const Scope& scope, const Operation& op,\r\n                 const std::vector<Output>& grad_inputs,\r\n                 std::vector<Output>* grad_outputs) {\r\n  // y = atanh(x)\r\n  // dy/dx = 1 / (1 - x^2)\r\n  auto one = Cast(scope, Const(scope, 1.0), op.input(0).type());\r\n  auto dydx = Reciprocal(scope, Sub(scope, one, Square(scope, op.input(0))));\r\n  // grad(x) = grad(y) * conj(dy/dx)\r\n  grad_outputs->push_back(\r\n      Mul(scope, grad_inputs[0], ConjugateHelper(scope, dydx)));\r\n  return scope.status();\r\n}\r\n```\r\n\r\nAbove all, I prefer to eliminating the useless dependency. How do you think? Thanks.", "created_at": "2017-09-14T05:33:25Z", "updated_at": "2017-10-05T02:07:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12872#discussion_r138800360", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12872", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/138800360"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12872#discussion_r138800360"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12872"}}, "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1450614\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suharshs\">@suharshs</a> .</p>\n<p><code>grade_scope</code> is used to keep consistent with its corresponding implementation in python.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_ErfGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Returns grad * 2/sqrt(pi) * exp(-x**2).<span class=\"pl-pds\">\"\"\"</span></span>\n  x <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n  two_over_root_pi <span class=\"pl-k\">=</span> constant_op.constant(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">/</span> np.sqrt(np.pi), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>grad.dtype)\n  <span class=\"pl-k\">with</span> ops.control_dependencies([grad]):\n    x <span class=\"pl-k\">=</span> math_ops.conj(x)\n    <span class=\"pl-k\">return</span> grad <span class=\"pl-k\">*</span> two_over_root_pi <span class=\"pl-k\">*</span> math_ops.exp(<span class=\"pl-k\">-</span>math_ops.square(x))</pre></div>\n<p>However, in my opinion, <code>with ops.control_dependencies([grad])</code> seems useless here. Moreover, I found a lot of gradients remove the dependency as well, take AtanhGrad for example</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Atanh<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_AtanhGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Returns grad * 1/ (1 - x^2).<span class=\"pl-pds\">\"\"\"</span></span>\n  x <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n  <span class=\"pl-k\">with</span> ops.control_dependencies([grad]):\n    x <span class=\"pl-k\">=</span> math_ops.conj(x)\n    x2 <span class=\"pl-k\">=</span> math_ops.square(x)\n    one <span class=\"pl-k\">=</span> constant_op.constant(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>grad.dtype)\n    inv <span class=\"pl-k\">=</span> math_ops.reciprocal(math_ops.subtract(one, x2))\n    <span class=\"pl-k\">return</span> grad <span class=\"pl-k\">*</span> inv</pre></div>\n<div class=\"highlight highlight-source-c++\"><pre>Status <span class=\"pl-en\">AtanhGrad</span>(<span class=\"pl-k\">const</span> Scope&amp; scope, <span class=\"pl-k\">const</span> Operation&amp; op,\n                 <span class=\"pl-k\">const</span> std::vector&lt;Output&gt;&amp; grad_inputs,\n                 std::vector&lt;Output&gt;* grad_outputs) {\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> y = atanh(x)</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> dy/dx = 1 / (1 - x^2)</span>\n  <span class=\"pl-k\">auto</span> one = <span class=\"pl-c1\">Cast</span>(scope, <span class=\"pl-c1\">Const</span>(scope, <span class=\"pl-c1\">1.0</span>), op.<span class=\"pl-c1\">input</span>(<span class=\"pl-c1\">0</span>).<span class=\"pl-c1\">type</span>());\n  <span class=\"pl-k\">auto</span> dydx = <span class=\"pl-c1\">Reciprocal</span>(scope, <span class=\"pl-c1\">Sub</span>(scope, one, <span class=\"pl-c1\">Square</span>(scope, op.<span class=\"pl-c1\">input</span>(<span class=\"pl-c1\">0</span>))));\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> grad(x) = grad(y) * conj(dy/dx)</span>\n  grad_outputs-&gt;<span class=\"pl-c1\">push_back</span>(\n      <span class=\"pl-c1\">Mul</span>(scope, grad_inputs[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">ConjugateHelper</span>(scope, dydx)));\n  <span class=\"pl-k\">return</span> scope.<span class=\"pl-c1\">status</span>();\n}</pre></div>\n<p>Above all, I prefer to eliminating the useless dependency. How do you think? Thanks.</p>", "body_text": "Hi, @suharshs .\ngrade_scope is used to keep consistent with its corresponding implementation in python.\ndef _ErfGrad(op, grad):\n  \"\"\"Returns grad * 2/sqrt(pi) * exp(-x**2).\"\"\"\n  x = op.inputs[0]\n  two_over_root_pi = constant_op.constant(2 / np.sqrt(np.pi), dtype=grad.dtype)\n  with ops.control_dependencies([grad]):\n    x = math_ops.conj(x)\n    return grad * two_over_root_pi * math_ops.exp(-math_ops.square(x))\nHowever, in my opinion, with ops.control_dependencies([grad]) seems useless here. Moreover, I found a lot of gradients remove the dependency as well, take AtanhGrad for example\n@ops.RegisterGradient(\"Atanh\")\ndef _AtanhGrad(op, grad):\n  \"\"\"Returns grad * 1/ (1 - x^2).\"\"\"\n  x = op.inputs[0]\n  with ops.control_dependencies([grad]):\n    x = math_ops.conj(x)\n    x2 = math_ops.square(x)\n    one = constant_op.constant(1, dtype=grad.dtype)\n    inv = math_ops.reciprocal(math_ops.subtract(one, x2))\n    return grad * inv\nStatus AtanhGrad(const Scope& scope, const Operation& op,\n                 const std::vector<Output>& grad_inputs,\n                 std::vector<Output>* grad_outputs) {\n  // y = atanh(x)\n  // dy/dx = 1 / (1 - x^2)\n  auto one = Cast(scope, Const(scope, 1.0), op.input(0).type());\n  auto dydx = Reciprocal(scope, Sub(scope, one, Square(scope, op.input(0))));\n  // grad(x) = grad(y) * conj(dy/dx)\n  grad_outputs->push_back(\n      Mul(scope, grad_inputs[0], ConjugateHelper(scope, dydx)));\n  return scope.status();\n}\nAbove all, I prefer to eliminating the useless dependency. How do you think? Thanks.", "in_reply_to_id": 138470872}