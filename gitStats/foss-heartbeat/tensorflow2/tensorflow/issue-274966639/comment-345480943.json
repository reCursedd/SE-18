{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345480943", "html_url": "https://github.com/tensorflow/tensorflow/issues/14663#issuecomment-345480943", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14663", "id": 345480943, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTQ4MDk0Mw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-19T00:03:52Z", "updated_at": "2017-11-19T00:03:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm pretty sure that it's impossible to get deterministic results from using an RNG op (like <code>tf.random_uniform()</code>) inside a <code>tf.while_loop()</code> with <code>parallel_iterations &gt; 1</code>, unless there is a data or control dependency between the RNG ops in successive iterations. I can think of two conceptual workarounds, but neither of them is likely to work out of the box with the <code>tf.rnn.rnn_cell</code> APIs:</p>\n<ol>\n<li>Compute the sequence of dropout masks outside the loop using a single <code>tf.random_uniform()</code> call, and access the appropriate slice inside the loop.</li>\n<li>Somehow add control dependencies from the input loop variables (and hence the output values of the previous iteration) to the <code>tf.random_uniform()</code> inside <code>DropoutWrapper</code>. Technically you only need the control dependency between the <code>tf.random_uniform()</code> ops, and the more precise you can make it, the higher the parallelism you'll achieve. (For example, you could add the random mask as a loop variable, purely to make it possible to add a control dependency on it.)</li>\n</ol>\n<p>There may be simpler ways to do it, but those are the only approaches I can think of.</p>\n<p>/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2533174\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ekelsen\">@ekelsen</a> for deterministic execution/reproducibility and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> for <code>tf.nn.dynamic_rnn()</code>.</p>", "body_text": "I'm pretty sure that it's impossible to get deterministic results from using an RNG op (like tf.random_uniform()) inside a tf.while_loop() with parallel_iterations > 1, unless there is a data or control dependency between the RNG ops in successive iterations. I can think of two conceptual workarounds, but neither of them is likely to work out of the box with the tf.rnn.rnn_cell APIs:\n\nCompute the sequence of dropout masks outside the loop using a single tf.random_uniform() call, and access the appropriate slice inside the loop.\nSomehow add control dependencies from the input loop variables (and hence the output values of the previous iteration) to the tf.random_uniform() inside DropoutWrapper. Technically you only need the control dependency between the tf.random_uniform() ops, and the more precise you can make it, the higher the parallelism you'll achieve. (For example, you could add the random mask as a loop variable, purely to make it possible to add a control dependency on it.)\n\nThere may be simpler ways to do it, but those are the only approaches I can think of.\n/cc @ekelsen for deterministic execution/reproducibility and @ebrevdo for tf.nn.dynamic_rnn().", "body": "I'm pretty sure that it's impossible to get deterministic results from using an RNG op (like `tf.random_uniform()`) inside a `tf.while_loop()` with `parallel_iterations > 1`, unless there is a data or control dependency between the RNG ops in successive iterations. I can think of two conceptual workarounds, but neither of them is likely to work out of the box with the `tf.rnn.rnn_cell` APIs:\r\n\r\n1. Compute the sequence of dropout masks outside the loop using a single `tf.random_uniform()` call, and access the appropriate slice inside the loop.\r\n2. Somehow add control dependencies from the input loop variables (and hence the output values of the previous iteration) to the `tf.random_uniform()` inside `DropoutWrapper`. Technically you only need the control dependency between the `tf.random_uniform()` ops, and the more precise you can make it, the higher the parallelism you'll achieve. (For example, you could add the random mask as a loop variable, purely to make it possible to add a control dependency on it.)\r\n\r\nThere may be simpler ways to do it, but those are the only approaches I can think of.\r\n\r\n/cc @ekelsen for deterministic execution/reproducibility and @ebrevdo for `tf.nn.dynamic_rnn()`."}