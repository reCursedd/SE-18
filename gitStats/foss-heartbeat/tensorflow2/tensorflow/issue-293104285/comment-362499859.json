{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362499859", "html_url": "https://github.com/tensorflow/tensorflow/issues/16620#issuecomment-362499859", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16620", "id": 362499859, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjQ5OTg1OQ==", "user": {"login": "ymcasky", "id": 6229000, "node_id": "MDQ6VXNlcjYyMjkwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/6229000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymcasky", "html_url": "https://github.com/ymcasky", "followers_url": "https://api.github.com/users/ymcasky/followers", "following_url": "https://api.github.com/users/ymcasky/following{/other_user}", "gists_url": "https://api.github.com/users/ymcasky/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymcasky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymcasky/subscriptions", "organizations_url": "https://api.github.com/users/ymcasky/orgs", "repos_url": "https://api.github.com/users/ymcasky/repos", "events_url": "https://api.github.com/users/ymcasky/events{/privacy}", "received_events_url": "https://api.github.com/users/ymcasky/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-02T06:36:48Z", "updated_at": "2018-02-02T06:36:48Z", "author_association": "NONE", "body_html": "<p>Thanks for your kindly reply.<br>\nIt can train properly.<br>\nBut even I change epochs from 1 to 100.<br>\nThe acc still around 0.1...<br>\nI also want to feed batch by batch instead of whole 55000 data.<br>\nDo you have other suggestion? Thanks!</p>", "body_text": "Thanks for your kindly reply.\nIt can train properly.\nBut even I change epochs from 1 to 100.\nThe acc still around 0.1...\nI also want to feed batch by batch instead of whole 55000 data.\nDo you have other suggestion? Thanks!", "body": "Thanks for your kindly reply.\r\nIt can train properly.\r\nBut even I change epochs from 1 to 100.\r\nThe acc still around 0.1...\r\nI also want to feed batch by batch instead of whole 55000 data.\r\nDo you have other suggestion? Thanks!"}