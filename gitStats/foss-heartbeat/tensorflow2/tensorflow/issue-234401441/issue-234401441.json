{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10519", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10519/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10519/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10519/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10519", "id": 234401441, "node_id": "MDU6SXNzdWUyMzQ0MDE0NDE=", "number": 10519, "title": "tf.contrib.data: tf-slim training pipeline gets stuck", "user": {"login": "ddtm", "id": 3285481, "node_id": "MDQ6VXNlcjMyODU0ODE=", "avatar_url": "https://avatars1.githubusercontent.com/u/3285481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ddtm", "html_url": "https://github.com/ddtm", "followers_url": "https://api.github.com/users/ddtm/followers", "following_url": "https://api.github.com/users/ddtm/following{/other_user}", "gists_url": "https://api.github.com/users/ddtm/gists{/gist_id}", "starred_url": "https://api.github.com/users/ddtm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ddtm/subscriptions", "organizations_url": "https://api.github.com/users/ddtm/orgs", "repos_url": "https://api.github.com/users/ddtm/repos", "events_url": "https://api.github.com/users/ddtm/events{/privacy}", "received_events_url": "https://api.github.com/users/ddtm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-06-08T03:26:09Z", "updated_at": "2017-06-22T00:34:51Z", "closed_at": "2017-06-22T00:34:51Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux leto28 3.16.0-4-amd64 1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux<br>\nVERSION_ID=\"8\"<br>\nVERSION=\"8 (jessie)\"</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBinary</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\ntf.VERSION = 1.2.0-rc2<br>\ntf.GIT_VERSION = v1.2.0-rc1-24-gce1d6ec<br>\ntf.COMPILER_VERSION = v1.2.0-rc1-24-gce1d6ec</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\nNone</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\n8.0/5.1</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nTITAN X (Pascal), 12189MiB</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\n<code>python ./mwe.py</code></p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I recently ported my dataset handling to the new dataset API from <code>tf.contrib.data</code>. Now it seems that the <code>tf-slim</code> training pipeline stalls if I request just 1 or 2 CPUs for my job (it used to work just fine with the dataset API provided by <code>tf-slim</code>). I does work if I grab 4 CPUs. I tried to come up with a MWE (see below). The interesting thing is that it is not getting stuck if I remove one of the <code>tf.summary.scalar</code>s or <code>.map()</code> at line 39. I suspect this issue is related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"232828152\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/10369\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/10369/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/10369\">#10369</a>.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.data <span class=\"pl-k\">as</span> tcd\n<span class=\"pl-k\">import</span> tensorflow.contrib.slim <span class=\"pl-k\">as</span> slim\n\n<span class=\"pl-k\">from</span> tensorflow.contrib.data.python.ops.dataset_ops <span class=\"pl-k\">import</span> _get_file_names\n\n<span class=\"pl-c1\">DATASET_DIR</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/path_to_the_dataset<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-c1\">FILE_PATTERN</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes_<span class=\"pl-c1\">{}</span>_*.tfrecord<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-c1\">IMAGE_SHAPE</span> <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">48</span>, <span class=\"pl-c1\">48</span>, <span class=\"pl-c1\">3</span>]\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_parse_function</span>(<span class=\"pl-smi\">example_proto</span>):\n    features <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>image/encoded<span class=\"pl-pds\">\"</span></span>: tf.FixedLenFeature(\n            (), tf.string, <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>),\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>image/annotation/color<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature(\n            (), tf.int64, <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>),\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>image/annotation/shape<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature(\n            (), tf.int64, <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>),\n    }\n    parsed_features <span class=\"pl-k\">=</span> tf.parse_single_example(example_proto, features)\n    image_decoded <span class=\"pl-k\">=</span> tf.image.decode_image(parsed_features[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>image/encoded<span class=\"pl-pds\">\"</span></span>])\n    color <span class=\"pl-k\">=</span> parsed_features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image/annotation/color<span class=\"pl-pds\">'</span></span>]\n    shape <span class=\"pl-k\">=</span> parsed_features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image/annotation/shape<span class=\"pl-pds\">'</span></span>]\n\n    <span class=\"pl-k\">return</span> image_decoded, color, shape\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_batch</span>(<span class=\"pl-smi\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>, <span class=\"pl-smi\">group_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-smi\">split_name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>):\n    file_pattern <span class=\"pl-k\">=</span> os.path.join(\n        <span class=\"pl-c1\">DATASET_DIR</span>, <span class=\"pl-c1\">FILE_PATTERN</span>.format(split_name))\n\n    file_names <span class=\"pl-k\">=</span> _get_file_names(file_pattern, <span class=\"pl-v\">randomize_input</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    dataset <span class=\"pl-k\">=</span> tcd.TFRecordDataset(file_names)\n    dataset <span class=\"pl-k\">=</span> dataset.map(_parse_function)\n\n    dataset <span class=\"pl-k\">=</span> dataset.map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">image</span>, <span class=\"pl-smi\">color</span>, <span class=\"pl-smi\">shape</span>: image)\n    dataset <span class=\"pl-k\">=</span> dataset.shuffle(<span class=\"pl-v\">buffer_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10000</span>)\n    dataset <span class=\"pl-k\">=</span> dataset.repeat().batch(group_size <span class=\"pl-k\">*</span> batch_size)\n\n    iterator <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator()\n    images <span class=\"pl-k\">=</span> iterator.get_next()\n\n    images <span class=\"pl-k\">=</span> tf.split(images, group_size, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    images <span class=\"pl-k\">=</span> [tf.reshape(x, [batch_size] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">IMAGE_SHAPE</span>) <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> images]\n\n    <span class=\"pl-k\">return</span> images\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-k\">with</span> tf.Graph().as_default():\n        x_1, x_2, x_3 <span class=\"pl-k\">=</span> get_batch(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>,\n                                  <span class=\"pl-v\">group_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n\n        val <span class=\"pl-k\">=</span> tf.reduce_sum(tf.add_n([x_1, x_2, x_3]))\n        val <span class=\"pl-k\">=</span> tf.Print(val, [tf.constant(<span class=\"pl-c1\">0</span>)], <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>I'm alive! <span class=\"pl-pds\">\"</span></span>)\n\n        global_step <span class=\"pl-k\">=</span> slim.get_or_create_global_step()\n        <span class=\"pl-k\">with</span> tf.control_dependencies([val]):\n            update_global_step_op <span class=\"pl-k\">=</span> tf.assign_add(global_step, <span class=\"pl-c1\">1</span>)\n\n        train_op <span class=\"pl-k\">=</span> update_global_step_op\n\n        tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Summary 1<span class=\"pl-pds\">'</span></span>, val)\n        tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Summary 2<span class=\"pl-pds\">'</span></span>, train_op)\n\n        logdir <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>mwe_logdir<span class=\"pl-pds\">'</span></span>\n        slim.learning.train(\n            <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op,\n            <span class=\"pl-v\">logdir</span><span class=\"pl-k\">=</span>logdir,\n            <span class=\"pl-v\">number_of_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000000</span>)</pre></div>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux leto28 3.16.0-4-amd64 1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\nVERSION_ID=\"8\"\nVERSION=\"8 (jessie)\"\n\n\nTensorFlow installed from (source or binary):\nBinary\n\n\nTensorFlow version (use command below):\ntf.VERSION = 1.2.0-rc2\ntf.GIT_VERSION = v1.2.0-rc1-24-gce1d6ec\ntf.COMPILER_VERSION = v1.2.0-rc1-24-gce1d6ec\n\n\nBazel version (if compiling from source):\nNone\n\n\nCUDA/cuDNN version:\n8.0/5.1\n\n\nGPU model and memory:\nTITAN X (Pascal), 12189MiB\n\n\nExact command to reproduce:\npython ./mwe.py\n\n\nDescribe the problem\nI recently ported my dataset handling to the new dataset API from tf.contrib.data. Now it seems that the tf-slim training pipeline stalls if I request just 1 or 2 CPUs for my job (it used to work just fine with the dataset API provided by tf-slim). I does work if I grab 4 CPUs. I tried to come up with a MWE (see below). The interesting thing is that it is not getting stuck if I remove one of the tf.summary.scalars or .map() at line 39. I suspect this issue is related to #10369.\nSource code / logs\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.data as tcd\nimport tensorflow.contrib.slim as slim\n\nfrom tensorflow.contrib.data.python.ops.dataset_ops import _get_file_names\n\nDATASET_DIR = '/path_to_the_dataset'\nFILE_PATTERN = 'shapes_{}_*.tfrecord'\nIMAGE_SHAPE = [48, 48, 3]\n\n\ndef _parse_function(example_proto):\n    features = {\n        \"image/encoded\": tf.FixedLenFeature(\n            (), tf.string, default_value=\"\"),\n        'image/annotation/color': tf.FixedLenFeature(\n            (), tf.int64, default_value=0),\n        'image/annotation/shape': tf.FixedLenFeature(\n            (), tf.int64, default_value=0),\n    }\n    parsed_features = tf.parse_single_example(example_proto, features)\n    image_decoded = tf.image.decode_image(parsed_features[\"image/encoded\"])\n    color = parsed_features['image/annotation/color']\n    shape = parsed_features['image/annotation/shape']\n\n    return image_decoded, color, shape\n\n\ndef get_batch(batch_size=32, group_size=3, split_name='train'):\n    file_pattern = os.path.join(\n        DATASET_DIR, FILE_PATTERN.format(split_name))\n\n    file_names = _get_file_names(file_pattern, randomize_input=True)\n\n    dataset = tcd.TFRecordDataset(file_names)\n    dataset = dataset.map(_parse_function)\n\n    dataset = dataset.map(lambda image, color, shape: image)\n    dataset = dataset.shuffle(buffer_size=10000)\n    dataset = dataset.repeat().batch(group_size * batch_size)\n\n    iterator = dataset.make_one_shot_iterator()\n    images = iterator.get_next()\n\n    images = tf.split(images, group_size, axis=0)\n    images = [tf.reshape(x, [batch_size] + IMAGE_SHAPE) for x in images]\n\n    return images\n\n\nif __name__ == \"__main__\":\n    with tf.Graph().as_default():\n        x_1, x_2, x_3 = get_batch(batch_size=32,\n                                  group_size=3)\n\n        val = tf.reduce_sum(tf.add_n([x_1, x_2, x_3]))\n        val = tf.Print(val, [tf.constant(0)], \"I'm alive! \")\n\n        global_step = slim.get_or_create_global_step()\n        with tf.control_dependencies([val]):\n            update_global_step_op = tf.assign_add(global_step, 1)\n\n        train_op = update_global_step_op\n\n        tf.summary.scalar('Summary 1', val)\n        tf.summary.scalar('Summary 2', train_op)\n\n        logdir = 'mwe_logdir'\n        slim.learning.train(\n            train_op=train_op,\n            logdir=logdir,\n            number_of_steps=1000000)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux leto28 3.16.0-4-amd64 1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.2.0-rc2\r\ntf.GIT_VERSION = v1.2.0-rc1-24-gce1d6ec\r\ntf.COMPILER_VERSION = v1.2.0-rc1-24-gce1d6ec\r\n\r\n- **Bazel version (if compiling from source)**:\r\nNone\r\n\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1\r\n\r\n- **GPU model and memory**:\r\nTITAN X (Pascal), 12189MiB\r\n\r\n- **Exact command to reproduce**:\r\n`python ./mwe.py`\r\n\r\n### Describe the problem\r\nI recently ported my dataset handling to the new dataset API from `tf.contrib.data`. Now it seems that the `tf-slim` training pipeline stalls if I request just 1 or 2 CPUs for my job (it used to work just fine with the dataset API provided by `tf-slim`). I does work if I grab 4 CPUs. I tried to come up with a MWE (see below). The interesting thing is that it is not getting stuck if I remove one of the `tf.summary.scalar`s or `.map()` at line 39. I suspect this issue is related to #10369.\r\n\r\n### Source code / logs\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.data as tcd\r\nimport tensorflow.contrib.slim as slim\r\n\r\nfrom tensorflow.contrib.data.python.ops.dataset_ops import _get_file_names\r\n\r\nDATASET_DIR = '/path_to_the_dataset'\r\nFILE_PATTERN = 'shapes_{}_*.tfrecord'\r\nIMAGE_SHAPE = [48, 48, 3]\r\n\r\n\r\ndef _parse_function(example_proto):\r\n    features = {\r\n        \"image/encoded\": tf.FixedLenFeature(\r\n            (), tf.string, default_value=\"\"),\r\n        'image/annotation/color': tf.FixedLenFeature(\r\n            (), tf.int64, default_value=0),\r\n        'image/annotation/shape': tf.FixedLenFeature(\r\n            (), tf.int64, default_value=0),\r\n    }\r\n    parsed_features = tf.parse_single_example(example_proto, features)\r\n    image_decoded = tf.image.decode_image(parsed_features[\"image/encoded\"])\r\n    color = parsed_features['image/annotation/color']\r\n    shape = parsed_features['image/annotation/shape']\r\n\r\n    return image_decoded, color, shape\r\n\r\n\r\ndef get_batch(batch_size=32, group_size=3, split_name='train'):\r\n    file_pattern = os.path.join(\r\n        DATASET_DIR, FILE_PATTERN.format(split_name))\r\n\r\n    file_names = _get_file_names(file_pattern, randomize_input=True)\r\n\r\n    dataset = tcd.TFRecordDataset(file_names)\r\n    dataset = dataset.map(_parse_function)\r\n\r\n    dataset = dataset.map(lambda image, color, shape: image)\r\n    dataset = dataset.shuffle(buffer_size=10000)\r\n    dataset = dataset.repeat().batch(group_size * batch_size)\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    images = iterator.get_next()\r\n\r\n    images = tf.split(images, group_size, axis=0)\r\n    images = [tf.reshape(x, [batch_size] + IMAGE_SHAPE) for x in images]\r\n\r\n    return images\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    with tf.Graph().as_default():\r\n        x_1, x_2, x_3 = get_batch(batch_size=32,\r\n                                  group_size=3)\r\n\r\n        val = tf.reduce_sum(tf.add_n([x_1, x_2, x_3]))\r\n        val = tf.Print(val, [tf.constant(0)], \"I'm alive! \")\r\n\r\n        global_step = slim.get_or_create_global_step()\r\n        with tf.control_dependencies([val]):\r\n            update_global_step_op = tf.assign_add(global_step, 1)\r\n\r\n        train_op = update_global_step_op\r\n\r\n        tf.summary.scalar('Summary 1', val)\r\n        tf.summary.scalar('Summary 2', train_op)\r\n\r\n        logdir = 'mwe_logdir'\r\n        slim.learning.train(\r\n            train_op=train_op,\r\n            logdir=logdir,\r\n            number_of_steps=1000000)\r\n```\r\n"}