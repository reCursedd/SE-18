{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/306998852", "html_url": "https://github.com/tensorflow/tensorflow/issues/10519#issuecomment-306998852", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10519", "id": 306998852, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjk5ODg1Mg==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-08T05:00:43Z", "updated_at": "2017-06-08T05:00:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for reporting this... it definitely looks like a bug. I think I've tracked it down to the <code>\"OneShotIterator\"</code> op, which internally blocks on this line while a function executes to build the dataset:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/91cb809bd6bc885458c583d46f4a322c30fa12cf/tensorflow/core/kernels/iterator_ops.cc#L248\">tensorflow/tensorflow/core/kernels/iterator_ops.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 248\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/91cb809bd6bc885458c583d46f4a322c30fa12cf\">91cb809</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L248\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"248\"></td>\n          <td id=\"LC248\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> n.<span class=\"pl-c1\">WaitForNotification</span>(); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>That will block one of the inter-op thread pool threads for the (typically short) execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool: when you have only 1 CPU, the system will deadlock as soon as you hit that line (because no more thread pool threads are available to run the function that will unblock it). When you have 2 CPUs, it can work, <em>but</em> <code>slim.learning.train()</code> uses <code>tf.train.Supervisor</code>, which asynchronously runs a background thread... that runs the same <code>\"OneShotIterator\"</code> op. To ensure that the op only initializes once, the initialization runs under a lock, acquired here:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/91cb809bd6bc885458c583d46f4a322c30fa12cf/tensorflow/core/kernels/iterator_ops.cc#L194\">tensorflow/tensorflow/core/kernels/iterator_ops.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 194\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/91cb809bd6bc885458c583d46f4a322c30fa12cf\">91cb809</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L194\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"194\"></td>\n          <td id=\"LC194\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> mutex_lock <span class=\"pl-smi\">l</span>(mu_); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>The problem is probably starting to become clear: two concurrent executions of the same <code>\"OneShotIterator\"</code> kernel will potentially block two inter-op thread pool threads, leading to deadlock in a 2-CPU system, because there are no more threads available to run the function that will unblock them.</p>\n<p>Anyway, mea culpa, and thanks again for finding the bug. I'll be working on a fix, although it might not make it into the final 1.2 release. In the mean time, there are a couple of workarounds:</p>\n<ol>\n<li>\n<p>Increase the number of threads to more than 2 in the inter-op thread pool. You can do this by passing <code>session_config=tf.ConfigProto(inter_op_parallelism_threads=3)</code> to <code>slim.learning.train()</code>.</p>\n</li>\n<li>\n<p>Use <code>dataset.make_initializable_iterator()</code> instead of <code>dataset.make_one_shot_iterator()</code>. This comes with the additional requirement that you have to run <code>iterator.initializer</code>, which is not completely trivial with <code>slim.learning.train()</code> because you don't have access to the <code>tf.Session</code>. One possibility is to pass <code>local_init_op=tf.group(tf.local_variables_initializer(), tf.tables_initializer(), iterator.initializer)</code> to <code>slim.learning.train()</code>.</p>\n</li>\n</ol>", "body_text": "Thanks for reporting this... it definitely looks like a bug. I think I've tracked it down to the \"OneShotIterator\" op, which internally blocks on this line while a function executes to build the dataset:\n\n  \n    \n      tensorflow/tensorflow/core/kernels/iterator_ops.cc\n    \n    \n         Line 248\n      in\n      91cb809\n    \n    \n    \n    \n\n        \n          \n           n.WaitForNotification(); \n        \n    \n  \n\n\nThat will block one of the inter-op thread pool threads for the (typically short) execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool: when you have only 1 CPU, the system will deadlock as soon as you hit that line (because no more thread pool threads are available to run the function that will unblock it). When you have 2 CPUs, it can work, but slim.learning.train() uses tf.train.Supervisor, which asynchronously runs a background thread... that runs the same \"OneShotIterator\" op. To ensure that the op only initializes once, the initialization runs under a lock, acquired here:\n\n  \n    \n      tensorflow/tensorflow/core/kernels/iterator_ops.cc\n    \n    \n         Line 194\n      in\n      91cb809\n    \n    \n    \n    \n\n        \n          \n           mutex_lock l(mu_); \n        \n    \n  \n\n\nThe problem is probably starting to become clear: two concurrent executions of the same \"OneShotIterator\" kernel will potentially block two inter-op thread pool threads, leading to deadlock in a 2-CPU system, because there are no more threads available to run the function that will unblock them.\nAnyway, mea culpa, and thanks again for finding the bug. I'll be working on a fix, although it might not make it into the final 1.2 release. In the mean time, there are a couple of workarounds:\n\n\nIncrease the number of threads to more than 2 in the inter-op thread pool. You can do this by passing session_config=tf.ConfigProto(inter_op_parallelism_threads=3) to slim.learning.train().\n\n\nUse dataset.make_initializable_iterator() instead of dataset.make_one_shot_iterator(). This comes with the additional requirement that you have to run iterator.initializer, which is not completely trivial with slim.learning.train() because you don't have access to the tf.Session. One possibility is to pass local_init_op=tf.group(tf.local_variables_initializer(), tf.tables_initializer(), iterator.initializer) to slim.learning.train().", "body": "Thanks for reporting this... it definitely looks like a bug. I think I've tracked it down to the `\"OneShotIterator\"` op, which internally blocks on this line while a function executes to build the dataset:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/91cb809bd6bc885458c583d46f4a322c30fa12cf/tensorflow/core/kernels/iterator_ops.cc#L248\r\n\r\nThat will block one of the inter-op thread pool threads for the (typically short) execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool: when you have only 1 CPU, the system will deadlock as soon as you hit that line (because no more thread pool threads are available to run the function that will unblock it). When you have 2 CPUs, it can work, *but* `slim.learning.train()` uses `tf.train.Supervisor`, which asynchronously runs a background thread... that runs the same `\"OneShotIterator\"` op. To ensure that the op only initializes once, the initialization runs under a lock, acquired here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/91cb809bd6bc885458c583d46f4a322c30fa12cf/tensorflow/core/kernels/iterator_ops.cc#L194\r\n\r\nThe problem is probably starting to become clear: two concurrent executions of the same `\"OneShotIterator\"` kernel will potentially block two inter-op thread pool threads, leading to deadlock in a 2-CPU system, because there are no more threads available to run the function that will unblock them.\r\n\r\nAnyway, mea culpa, and thanks again for finding the bug. I'll be working on a fix, although it might not make it into the final 1.2 release. In the mean time, there are a couple of workarounds:\r\n\r\n1. Increase the number of threads to more than 2 in the inter-op thread pool. You can do this by passing `session_config=tf.ConfigProto(inter_op_parallelism_threads=3)` to `slim.learning.train()`.\r\n\r\n2. Use `dataset.make_initializable_iterator()` instead of `dataset.make_one_shot_iterator()`. This comes with the additional requirement that you have to run `iterator.initializer`, which is not completely trivial with `slim.learning.train()` because you don't have access to the `tf.Session`. One possibility is to pass `local_init_op=tf.group(tf.local_variables_initializer(), tf.tables_initializer(), iterator.initializer)` to `slim.learning.train()`. "}