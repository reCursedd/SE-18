{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328072948", "html_url": "https://github.com/tensorflow/tensorflow/issues/4589#issuecomment-328072948", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4589", "id": 328072948, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODA3Mjk0OA==", "user": {"login": "Sanghoon94", "id": 11880170, "node_id": "MDQ6VXNlcjExODgwMTcw", "avatar_url": "https://avatars3.githubusercontent.com/u/11880170?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sanghoon94", "html_url": "https://github.com/Sanghoon94", "followers_url": "https://api.github.com/users/Sanghoon94/followers", "following_url": "https://api.github.com/users/Sanghoon94/following{/other_user}", "gists_url": "https://api.github.com/users/Sanghoon94/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sanghoon94/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sanghoon94/subscriptions", "organizations_url": "https://api.github.com/users/Sanghoon94/orgs", "repos_url": "https://api.github.com/users/Sanghoon94/repos", "events_url": "https://api.github.com/users/Sanghoon94/events{/privacy}", "received_events_url": "https://api.github.com/users/Sanghoon94/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-08T10:54:16Z", "updated_at": "2017-09-08T10:54:16Z", "author_association": "NONE", "body_html": "<p>I am trying to build a seq2seq model using fp16.<br>\nUsing the same network architecture, fp32 model trains fine, but the model using fp16 suffers from CUDA_ERROR_OUT_OF_MEMORY.<br>\nWhy is lower precision holds up larger memory? Is there any solution for this problem?</p>\n<p>I am currently using python3, tf 1.0, cuda 8.0, cudnn 5.1</p>", "body_text": "I am trying to build a seq2seq model using fp16.\nUsing the same network architecture, fp32 model trains fine, but the model using fp16 suffers from CUDA_ERROR_OUT_OF_MEMORY.\nWhy is lower precision holds up larger memory? Is there any solution for this problem?\nI am currently using python3, tf 1.0, cuda 8.0, cudnn 5.1", "body": "I am trying to build a seq2seq model using fp16.\r\nUsing the same network architecture, fp32 model trains fine, but the model using fp16 suffers from CUDA_ERROR_OUT_OF_MEMORY.\r\nWhy is lower precision holds up larger memory? Is there any solution for this problem?\r\n\r\nI am currently using python3, tf 1.0, cuda 8.0, cudnn 5.1"}