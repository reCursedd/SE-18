{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300079250", "html_url": "https://github.com/tensorflow/tensorflow/issues/4589#issuecomment-300079250", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4589", "id": 300079250, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDA3OTI1MA==", "user": {"login": "PhilJd", "id": 16101605, "node_id": "MDQ6VXNlcjE2MTAxNjA1", "avatar_url": "https://avatars2.githubusercontent.com/u/16101605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilJd", "html_url": "https://github.com/PhilJd", "followers_url": "https://api.github.com/users/PhilJd/followers", "following_url": "https://api.github.com/users/PhilJd/following{/other_user}", "gists_url": "https://api.github.com/users/PhilJd/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilJd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilJd/subscriptions", "organizations_url": "https://api.github.com/users/PhilJd/orgs", "repos_url": "https://api.github.com/users/PhilJd/repos", "events_url": "https://api.github.com/users/PhilJd/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilJd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-09T07:04:07Z", "updated_at": "2017-05-09T07:19:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm also very interested in this as I'm currently developing a net optimized for inference speed. Would you accept pull requests for this? I guess it's mainly about what is described in this <a href=\"https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1805\">ToDo</a> - find out which devices and CUDA versions support native float computation and then set the cudnn descriptor accordingly.</p>\n<p><a href=\"https://devblogs.nvidia.com/parallelforall/mixed-precision-programming-cuda-8/\" rel=\"nofollow\">Nvidia's blog</a> (table 2) states that this is the case for cuda &gt;= 7.5 and cudnn &gt;= 5.1.</p>\n<p>Edit:<br>\nAnother example, <a href=\"https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/stream_executor/cuda/cuda_blas.cc#L1757\">BLAS</a> in this case.</p>", "body_text": "I'm also very interested in this as I'm currently developing a net optimized for inference speed. Would you accept pull requests for this? I guess it's mainly about what is described in this ToDo - find out which devices and CUDA versions support native float computation and then set the cudnn descriptor accordingly.\nNvidia's blog (table 2) states that this is the case for cuda >= 7.5 and cudnn >= 5.1.\nEdit:\nAnother example, BLAS in this case.", "body": "I'm also very interested in this as I'm currently developing a net optimized for inference speed. Would you accept pull requests for this? I guess it's mainly about what is described in this [ToDo](https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1805) - find out which devices and CUDA versions support native float computation and then set the cudnn descriptor accordingly.\r\n\r\n[Nvidia's blog](https://devblogs.nvidia.com/parallelforall/mixed-precision-programming-cuda-8/) (table 2) states that this is the case for cuda >= 7.5 and cudnn >= 5.1.\r\n\r\nEdit:\r\nAnother example, [BLAS](https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/stream_executor/cuda/cuda_blas.cc#L1757) in this case."}