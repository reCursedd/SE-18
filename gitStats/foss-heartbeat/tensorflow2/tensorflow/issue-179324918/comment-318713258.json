{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318713258", "html_url": "https://github.com/tensorflow/tensorflow/issues/4589#issuecomment-318713258", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4589", "id": 318713258, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODcxMzI1OA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-28T17:22:18Z", "updated_at": "2017-07-28T17:22:18Z", "author_association": "MEMBER", "body_html": "<p>We're currently working on adding float16 ops to more TensorFlow ops, such as <code>tf.nn.fused_batch_norm</code>, as well as increasing the performance when using float16 ops. Although float16 ops use float32 internally for computations to avoid numerical precision issues, using float16 ops will result in increased performance over their float32 counterparts when run on P100 GPUs due to the reduced memory traffic.  Additionally, we plan on adding float16 support to <a href=\"https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks\">tf_cnn_benchmarks</a>.</p>\n<p>I'll keep this bug updated as more progress is made.</p>", "body_text": "We're currently working on adding float16 ops to more TensorFlow ops, such as tf.nn.fused_batch_norm, as well as increasing the performance when using float16 ops. Although float16 ops use float32 internally for computations to avoid numerical precision issues, using float16 ops will result in increased performance over their float32 counterparts when run on P100 GPUs due to the reduced memory traffic.  Additionally, we plan on adding float16 support to tf_cnn_benchmarks.\nI'll keep this bug updated as more progress is made.", "body": "We're currently working on adding float16 ops to more TensorFlow ops, such as `tf.nn.fused_batch_norm`, as well as increasing the performance when using float16 ops. Although float16 ops use float32 internally for computations to avoid numerical precision issues, using float16 ops will result in increased performance over their float32 counterparts when run on P100 GPUs due to the reduced memory traffic.  Additionally, we plan on adding float16 support to [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks). \r\n\r\nI'll keep this bug updated as more progress is made."}