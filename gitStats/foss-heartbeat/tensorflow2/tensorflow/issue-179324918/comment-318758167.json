{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318758167", "html_url": "https://github.com/tensorflow/tensorflow/issues/4589#issuecomment-318758167", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4589", "id": 318758167, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODc1ODE2Nw==", "user": {"login": "snarb", "id": 4518007, "node_id": "MDQ6VXNlcjQ1MTgwMDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/4518007?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snarb", "html_url": "https://github.com/snarb", "followers_url": "https://api.github.com/users/snarb/followers", "following_url": "https://api.github.com/users/snarb/following{/other_user}", "gists_url": "https://api.github.com/users/snarb/gists{/gist_id}", "starred_url": "https://api.github.com/users/snarb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snarb/subscriptions", "organizations_url": "https://api.github.com/users/snarb/orgs", "repos_url": "https://api.github.com/users/snarb/repos", "events_url": "https://api.github.com/users/snarb/events{/privacy}", "received_events_url": "https://api.github.com/users/snarb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-28T20:47:52Z", "updated_at": "2017-07-28T20:47:52Z", "author_association": "NONE", "body_html": "<p>News say that Intel will also add native  fp16 support for next Intel Xeon Phi processor (Knights Mill is commonly suggested) this year. There are few research that shows that in many cases fp16 is enouph for deep learning. Maybe not in all, but in many. Mb will be beter to allow the user make choice what presision is enouph? For example for my RL task I definitly know that fp16 presision  is  enough. And Cafee training acuracy for fp32 and fp16 is almost the same with much better speed for my task.</p>", "body_text": "News say that Intel will also add native  fp16 support for next Intel Xeon Phi processor (Knights Mill is commonly suggested) this year. There are few research that shows that in many cases fp16 is enouph for deep learning. Maybe not in all, but in many. Mb will be beter to allow the user make choice what presision is enouph? For example for my RL task I definitly know that fp16 presision  is  enough. And Cafee training acuracy for fp32 and fp16 is almost the same with much better speed for my task.", "body": "News say that Intel will also add native  fp16 support for next Intel Xeon Phi processor (Knights Mill is commonly suggested) this year. There are few research that shows that in many cases fp16 is enouph for deep learning. Maybe not in all, but in many. Mb will be beter to allow the user make choice what presision is enouph? For example for my RL task I definitly know that fp16 presision  is  enough. And Cafee training acuracy for fp32 and fp16 is almost the same with much better speed for my task. "}