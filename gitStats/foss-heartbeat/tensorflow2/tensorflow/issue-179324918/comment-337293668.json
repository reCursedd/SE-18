{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/337293668", "html_url": "https://github.com/tensorflow/tensorflow/issues/4589#issuecomment-337293668", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4589", "id": 337293668, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzI5MzY2OA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-17T16:47:28Z", "updated_at": "2017-10-17T16:47:28Z", "author_association": "MEMBER", "body_html": "<p>I don't think much has changed in 1.3.1. But on the master branch, the status is the following.</p>\n<p><a href=\"https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks\">tf_cnn_benchmarks</a> has support for fp16 by using the flag fp16. It currently does an unnecessary costly cast <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/convnet_builder.py#L445\">here</a>, which I will remove once I verify doing so does not affect convergence. The cast is unnecessary since <code>tf.nn.fused_batch_norm</code> now supports fp16 on the GPU (and soon, on the CPU once <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"261778287\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13388\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/13388/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/13388\">#13388</a> is submitted).</p>\n<p>By default, fp16 convolutions still do internal computations in fp32 (on pre-Volta hardware), but they can do fp16 internal computations by setting the newly added environmental variable <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2136\">TF_FP16_CONV_USE_FP32_COMPUTE</a> to 0. As I stated before, fp16 matmuls also do compute in fp32, unless <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/matmul_autotune.cc#L44\">TF_FP16_MATMUL_USE_FP32_COMPUTE</a> is set to 0.</p>", "body_text": "I don't think much has changed in 1.3.1. But on the master branch, the status is the following.\ntf_cnn_benchmarks has support for fp16 by using the flag fp16. It currently does an unnecessary costly cast here, which I will remove once I verify doing so does not affect convergence. The cast is unnecessary since tf.nn.fused_batch_norm now supports fp16 on the GPU (and soon, on the CPU once #13388 is submitted).\nBy default, fp16 convolutions still do internal computations in fp32 (on pre-Volta hardware), but they can do fp16 internal computations by setting the newly added environmental variable TF_FP16_CONV_USE_FP32_COMPUTE to 0. As I stated before, fp16 matmuls also do compute in fp32, unless TF_FP16_MATMUL_USE_FP32_COMPUTE is set to 0.", "body": "I don't think much has changed in 1.3.1. But on the master branch, the status is the following.\r\n\r\n[tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) has support for fp16 by using the flag fp16. It currently does an unnecessary costly cast [here](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/convnet_builder.py#L445), which I will remove once I verify doing so does not affect convergence. The cast is unnecessary since `tf.nn.fused_batch_norm` now supports fp16 on the GPU (and soon, on the CPU once #13388 is submitted).\r\n\r\nBy default, fp16 convolutions still do internal computations in fp32 (on pre-Volta hardware), but they can do fp16 internal computations by setting the newly added environmental variable [TF_FP16_CONV_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2136) to 0. As I stated before, fp16 matmuls also do compute in fp32, unless [TF_FP16_MATMUL_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/matmul_autotune.cc#L44) is set to 0."}