{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300549419", "html_url": "https://github.com/tensorflow/tensorflow/issues/4589#issuecomment-300549419", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4589", "id": 300549419, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDU0OTQxOQ==", "user": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-10T17:08:59Z", "updated_at": "2017-05-10T17:08:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We support native fp16 in most cases. However, we still compute convolutions and matrix multiplications using fp32 floats to avoid numerical stability issues.</p>\n<p>In the case of the multiplications of a N by K matrix with a K by N matrix, we could use native fp16 provided that K is small. When K is large the reduced precision of fp16 introduces enough noise in the computation to cause issues such as reduced inference accuracy. Similarly, in the case of convolution, we could also use native fp16 provided that both the convolution window and the input depth are small.</p>\n<p>The trick here is going to be to figure out what thresholds are acceptable.</p>", "body_text": "We support native fp16 in most cases. However, we still compute convolutions and matrix multiplications using fp32 floats to avoid numerical stability issues.\nIn the case of the multiplications of a N by K matrix with a K by N matrix, we could use native fp16 provided that K is small. When K is large the reduced precision of fp16 introduces enough noise in the computation to cause issues such as reduced inference accuracy. Similarly, in the case of convolution, we could also use native fp16 provided that both the convolution window and the input depth are small.\nThe trick here is going to be to figure out what thresholds are acceptable.", "body": "We support native fp16 in most cases. However, we still compute convolutions and matrix multiplications using fp32 floats to avoid numerical stability issues.\r\n\r\nIn the case of the multiplications of a N by K matrix with a K by N matrix, we could use native fp16 provided that K is small. When K is large the reduced precision of fp16 introduces enough noise in the computation to cause issues such as reduced inference accuracy. Similarly, in the case of convolution, we could also use native fp16 provided that both the convolution window and the input depth are small. \r\n\r\nThe trick here is going to be to figure out what thresholds are acceptable."}