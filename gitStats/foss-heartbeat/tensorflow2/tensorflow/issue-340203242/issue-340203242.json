{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20694", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20694/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20694/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20694/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20694", "id": 340203242, "node_id": "MDU6SXNzdWUzNDAyMDMyNDI=", "number": 20694, "title": "tf.contrib.quantize bug: errors within weights/activations quantization", "user": {"login": "fgr1986", "id": 10627849, "node_id": "MDQ6VXNlcjEwNjI3ODQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/10627849?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fgr1986", "html_url": "https://github.com/fgr1986", "followers_url": "https://api.github.com/users/fgr1986/followers", "following_url": "https://api.github.com/users/fgr1986/following{/other_user}", "gists_url": "https://api.github.com/users/fgr1986/gists{/gist_id}", "starred_url": "https://api.github.com/users/fgr1986/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fgr1986/subscriptions", "organizations_url": "https://api.github.com/users/fgr1986/orgs", "repos_url": "https://api.github.com/users/fgr1986/repos", "events_url": "https://api.github.com/users/fgr1986/events{/privacy}", "received_events_url": "https://api.github.com/users/fgr1986/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-07-11T11:34:06Z", "updated_at": "2018-08-27T22:35:50Z", "closed_at": "2018-08-27T22:35:50Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes, provided</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: -</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: -</li>\n<li><strong>CUDA/cuDNN version</strong>: -</li>\n<li><strong>GPU model and memory</strong>: -</li>\n<li><strong>Exact command to reproduce</strong>: python quantization_test.py (file provided)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><strong>tf.contrib.quantize</strong> (experimental and non-experimental) automatic quantization produce close to but not quantized results as when you manually introduce fakequant operations.<br>\nWeights and/or activations are close to the quantized values, but they are not correct.</p>\n<h3>Source code / logs</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nD_SIZE = 2048\nBATCH_SIZE = 128\nLR = 1e-2\nEPOCHS = 2000\n\nSHOW_INTERVAL = 20\nOUT_CH = 8\n\n\ndef create_dummy_model(is_training=True):\n    # prepare dataset\n    np_features = np.arange(-int(D_SIZE) / 2, int(D_SIZE / 2))\n    features_arr = tf.constant(np_features, dtype=tf.float32)\n    np_labels = np.zeros((D_SIZE, OUT_CH), dtype=float)\n    print(np_labels.shape)\n    for i in range(OUT_CH):\n        np_labels[:, i] = 2 * (i + 1) * np_features\n    print('np_features:')\n    print(np_features[0:8])\n    print('np_labels:')\n    print(np_labels[0:8])\n\n    labels_arr = tf.constant(np_labels, dtype=tf.float32)\n    trn_data = tf.data.Dataset.zip(\n        (tf.data.Dataset.from_tensor_slices(features_arr),\n         tf.data.Dataset.from_tensor_slices(labels_arr))\n    )\n    trn_data = trn_data.shuffle(D_SIZE)\n    trn_data = trn_data.batch(BATCH_SIZE)\n    trn_data = trn_data.repeat(1)\n    trn_data = trn_data.prefetch(1)\n\n    iterator = tf.data.Iterator.from_structure(trn_data.output_types,\n                                               trn_data.output_shapes)\n    features, labels = iterator.get_next()\n    data_init_op = iterator.make_initializer(trn_data)\n\n    # global step\n    global_step = tf.train.get_or_create_global_step()\n\n    # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\n    input = tf.reshape(features, (BATCH_SIZE, 1))\n    flow = tf.identity(input, name='input_tensor')\n    # batch_size x INPUT_CH\n    flow = tf.reshape(flow, (-1, 1))\n    flow = tf.layers.dense(flow, units=OUT_CH, name='dense')\n    flow = tf.reshape(flow, (-1, OUT_CH), name='out')\n\n    # Build forward pass of model.\n    # Make the loss op\n    with tf.variable_scope('loss'):\n        tf.logging.debug('[loss] size of out: %s',\n                         flow.get_shape().as_list())\n        power = tf.pow(labels - flow, 2)\n        power = tf.reduce_sum(power, 1)\n        # power = tf.reduce_sum(power)\n        loss = tf.reduce_mean(power, name='m_loss')\n    tf.summary.scalar('loss', loss)\n\n    # Optimizer\n    if is_training:\n        opt_op = tf.train.AdamOptimizer(LR).minimize(loss, global_step)\n        # opt_op = tf.train.GradientDescentOptimizer(1e-4).minimize(loss, global_step)\n    else:\n        opt_op = False\n    # Merge all the summaries\n    merged = tf.summary.merge_all()\n\n    return {\n        'iterator': iterator,\n        'features': features,\n        'labels': labels,\n        'data_init_op': data_init_op,\n        'global_step': global_step,\n        'flow': flow,\n        'loss': loss,\n        'opt_op': opt_op,\n        'merged': merged\n    }\n\n\ndef quantize_tensor(np_array, n_bits):\n    \"\"\"Quantizes an np_array\"\"\"\n    min_wt = np.min(np_array)\n    max_wt = np.max(np_array)\n    # clap values if required\n    if min_wt &lt; -2**(n_bits-1):\n        min_wt = -2**(n_bits-1)\n    if max_wt &gt; (2**(n_bits-1) - 1) :\n        max_wt = (2**(n_bits-1) - 1)\n    # find number of integer bits to represent this range\n    int_bits = int(np.ceil(np.log2(max(abs(min_wt), abs(max_wt)))))\n    print('int_bits: ', int_bits)\n    # remaining bits are fractional bits (1-bit for sign)\n    frac_bits = n_bits -1 - int_bits\n    # floating point weights are scaled and rounded to [-128,127], which are used in\n    # the fixed-point operations on the actual hardware (i.e., microcontroller)\n    quantized = np.round(np_array * (2**frac_bits))\n    # To quantify the impact of quantized weights, scale them back to\n    # original range to run inference using quantized weights\n    return quantized / (2**frac_bits)\n\n\ndef train_quantization(quantize=True, w_b=3, a_b=3):\n\n    g_tr = tf.Graph()\n    with g_tr.as_default():\n\n        model = create_dummy_model(True)\n\n        # Call the training rewrite which rewrites the graph in-place with\n        # FakeQuantization nodes and folds batchnorm for training. It is\n        # often needed to fine tune a floating point model for quantization\n        # with this training tool. When training from scratch, quant_delay\n        # can be used to activate quantization after training to converge\n        # with the float graph, effectively fine-tuning the model.\n        if quantize:\n            quant_delay = int((D_SIZE / BATCH_SIZE) * EPOCHS * 3 / 4)\n            print('quant_delay: ', quant_delay)\n            print('w_b: ', w_b)\n            print('a_b: ', a_b)\n            tf.contrib.quantize.experimental_create_training_graph(weight_bits=w_b,\n                                                                   activation_bits=a_b,\n                                                                   quant_delay=quant_delay)\n\n        with tf.Session(graph=g_tr) as sess:\n            # global variables Initializing\n            sess.run(tf.global_variables_initializer())\n            # local variables Initializing\n            sess.run(tf.local_variables_initializer())\n\n            if quantize:\n                train_writer = tf.summary.FileWriter(\n                    './tmp/train_quantized_w' + str(w_b) + '_a_' + str(a_b),\n                    sess.graph)\n            else:\n                train_writer = tf.summary.FileWriter('./tmp/train_normal',\n                                                     sess.graph)\n\n            epoch_counter = 0\n            nn_in = None\n            nn_out = None\n            nn_loss = None\n            for epoch_counter in range(EPOCHS):\n\n                # re-initialize the dataset iterator\n                sess.run(model['data_init_op'])\n\n                try:\n                    while True:\n                        _, nn_in, nn_out, nn_labels, nn_loss, gs, summary = sess.run(\n                            [model['opt_op'],\n                             model['features'],\n                             model['flow'],\n                             model['labels'],\n                             model['loss'],\n                             model['global_step'],\n                             model['merged']])\n\n                        # tensorboard and  statistics\n                        train_writer.add_summary(summary, gs)\n                except tf.errors.OutOfRangeError:\n                    # do nothing\n                    if epoch_counter % SHOW_INTERVAL == 0:\n                        print(epoch_counter,  '/', EPOCHS,\n                              ': loss ', nn_loss,\n                              '. Global step: ', gs)\n\n            weights = sess.run(tf.get_default_graph().get_tensor_by_name(\n                'dense/kernel:0'))\n            saver = tf.train.Saver()\n            if quantize:\n                trained_model_path = './tmp/quantized_w' + \\\n                    str(w_b) + '_a_' + str(a_b) + '.ckpt'\n            else:\n                trained_model_path = './tmp/normal.ckpt'\n\n            saver.save(sess, trained_model_path)\n\n        # debug\n        print('nn_out shape: ', nn_out.shape)\n        print('nn_labels shape: ', nn_labels.shape)\n        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(nn_out)))\n        print('weights shape: ', weights.shape)\n        print('weights: ', weights)\n        print('weights different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(weights)))\n\n        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n        plt.plot(weights[0, :], label='weights',\n                 marker='o', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n        print('[debug] exit training')\n        return trained_model_path\n\n\ndef inference_quantization(trained_model_path, quantize=True, w_b=3, a_b=3):\n\n    #################################\n    # Reset default graph\n    tf.reset_default_graph()\n    #################################\n\n    g_inf = tf.Graph()\n    with g_inf.as_default():\n\n        model = create_dummy_model(False)\n\n        with tf.Session(graph=g_inf) as sess:\n            # Call the eval rewrite which rewrites the graph in-place with\n            # FakeQuantization nodes and fold batchnorm for eval.\n            if quantize:\n                print('w_b: ', w_b)\n                print('a_b: ', a_b)\n                tf.contrib.quantize.experimental_create_eval_graph(\n                    weight_bits=w_b,\n                    activation_bits=a_b)\n\n            saver = tf.train.Saver()\n            saver.restore(sess, trained_model_path)\n            # frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n            #       sess, sess.graph_def, [\"out\"])\n            print('[debug] restored')\n\n            # re-initialize the iterator, but this time with training data\n\n            sess.run(model['data_init_op'])\n\n            nn_in, nn_out, nn_labels, nn_loss = sess.run([\n                model['features'],\n                model['flow'],\n                model['labels'],\n                model['loss']])\n            weights = sess.run(tf.get_default_graph(\n            ).get_tensor_by_name('dense/kernel:0'))\n\n        # debug\n        print('nn_out shape: ', nn_out.shape)\n        print('nn_labels shape: ', nn_labels.shape)\n        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(nn_out)))\n        print('weights shape: ', weights.shape)\n        # print('weights: ', weights)\n        print('weights different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(weights)))\n        print('weights: ', weights)\n        q_weights = quantize_tensor(weights, w_b)\n        print('q_weights: ', q_weights)\n        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n        plt.plot(weights[0, :], label='weights',\n                 marker='o', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n\ndef test_quantization():\n\n    # train_quantization(False)\n    trained_model_path = train_quantization(quantize=True, w_b=2, a_b=2)\n    inference_quantization(\n        trained_model_path=trained_model_path, quantize=True, w_b=2, a_b=2)\n\ntest_quantization()\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, provided\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.8.0\nPython version:  3.6\nBazel version (if compiling from source): -\nGCC/Compiler version (if compiling from source): -\nCUDA/cuDNN version: -\nGPU model and memory: -\nExact command to reproduce: python quantization_test.py (file provided)\n\nDescribe the problem\ntf.contrib.quantize (experimental and non-experimental) automatic quantization produce close to but not quantized results as when you manually introduce fakequant operations.\nWeights and/or activations are close to the quantized values, but they are not correct.\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nD_SIZE = 2048\nBATCH_SIZE = 128\nLR = 1e-2\nEPOCHS = 2000\n\nSHOW_INTERVAL = 20\nOUT_CH = 8\n\n\ndef create_dummy_model(is_training=True):\n    # prepare dataset\n    np_features = np.arange(-int(D_SIZE) / 2, int(D_SIZE / 2))\n    features_arr = tf.constant(np_features, dtype=tf.float32)\n    np_labels = np.zeros((D_SIZE, OUT_CH), dtype=float)\n    print(np_labels.shape)\n    for i in range(OUT_CH):\n        np_labels[:, i] = 2 * (i + 1) * np_features\n    print('np_features:')\n    print(np_features[0:8])\n    print('np_labels:')\n    print(np_labels[0:8])\n\n    labels_arr = tf.constant(np_labels, dtype=tf.float32)\n    trn_data = tf.data.Dataset.zip(\n        (tf.data.Dataset.from_tensor_slices(features_arr),\n         tf.data.Dataset.from_tensor_slices(labels_arr))\n    )\n    trn_data = trn_data.shuffle(D_SIZE)\n    trn_data = trn_data.batch(BATCH_SIZE)\n    trn_data = trn_data.repeat(1)\n    trn_data = trn_data.prefetch(1)\n\n    iterator = tf.data.Iterator.from_structure(trn_data.output_types,\n                                               trn_data.output_shapes)\n    features, labels = iterator.get_next()\n    data_init_op = iterator.make_initializer(trn_data)\n\n    # global step\n    global_step = tf.train.get_or_create_global_step()\n\n    # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\n    input = tf.reshape(features, (BATCH_SIZE, 1))\n    flow = tf.identity(input, name='input_tensor')\n    # batch_size x INPUT_CH\n    flow = tf.reshape(flow, (-1, 1))\n    flow = tf.layers.dense(flow, units=OUT_CH, name='dense')\n    flow = tf.reshape(flow, (-1, OUT_CH), name='out')\n\n    # Build forward pass of model.\n    # Make the loss op\n    with tf.variable_scope('loss'):\n        tf.logging.debug('[loss] size of out: %s',\n                         flow.get_shape().as_list())\n        power = tf.pow(labels - flow, 2)\n        power = tf.reduce_sum(power, 1)\n        # power = tf.reduce_sum(power)\n        loss = tf.reduce_mean(power, name='m_loss')\n    tf.summary.scalar('loss', loss)\n\n    # Optimizer\n    if is_training:\n        opt_op = tf.train.AdamOptimizer(LR).minimize(loss, global_step)\n        # opt_op = tf.train.GradientDescentOptimizer(1e-4).minimize(loss, global_step)\n    else:\n        opt_op = False\n    # Merge all the summaries\n    merged = tf.summary.merge_all()\n\n    return {\n        'iterator': iterator,\n        'features': features,\n        'labels': labels,\n        'data_init_op': data_init_op,\n        'global_step': global_step,\n        'flow': flow,\n        'loss': loss,\n        'opt_op': opt_op,\n        'merged': merged\n    }\n\n\ndef quantize_tensor(np_array, n_bits):\n    \"\"\"Quantizes an np_array\"\"\"\n    min_wt = np.min(np_array)\n    max_wt = np.max(np_array)\n    # clap values if required\n    if min_wt < -2**(n_bits-1):\n        min_wt = -2**(n_bits-1)\n    if max_wt > (2**(n_bits-1) - 1) :\n        max_wt = (2**(n_bits-1) - 1)\n    # find number of integer bits to represent this range\n    int_bits = int(np.ceil(np.log2(max(abs(min_wt), abs(max_wt)))))\n    print('int_bits: ', int_bits)\n    # remaining bits are fractional bits (1-bit for sign)\n    frac_bits = n_bits -1 - int_bits\n    # floating point weights are scaled and rounded to [-128,127], which are used in\n    # the fixed-point operations on the actual hardware (i.e., microcontroller)\n    quantized = np.round(np_array * (2**frac_bits))\n    # To quantify the impact of quantized weights, scale them back to\n    # original range to run inference using quantized weights\n    return quantized / (2**frac_bits)\n\n\ndef train_quantization(quantize=True, w_b=3, a_b=3):\n\n    g_tr = tf.Graph()\n    with g_tr.as_default():\n\n        model = create_dummy_model(True)\n\n        # Call the training rewrite which rewrites the graph in-place with\n        # FakeQuantization nodes and folds batchnorm for training. It is\n        # often needed to fine tune a floating point model for quantization\n        # with this training tool. When training from scratch, quant_delay\n        # can be used to activate quantization after training to converge\n        # with the float graph, effectively fine-tuning the model.\n        if quantize:\n            quant_delay = int((D_SIZE / BATCH_SIZE) * EPOCHS * 3 / 4)\n            print('quant_delay: ', quant_delay)\n            print('w_b: ', w_b)\n            print('a_b: ', a_b)\n            tf.contrib.quantize.experimental_create_training_graph(weight_bits=w_b,\n                                                                   activation_bits=a_b,\n                                                                   quant_delay=quant_delay)\n\n        with tf.Session(graph=g_tr) as sess:\n            # global variables Initializing\n            sess.run(tf.global_variables_initializer())\n            # local variables Initializing\n            sess.run(tf.local_variables_initializer())\n\n            if quantize:\n                train_writer = tf.summary.FileWriter(\n                    './tmp/train_quantized_w' + str(w_b) + '_a_' + str(a_b),\n                    sess.graph)\n            else:\n                train_writer = tf.summary.FileWriter('./tmp/train_normal',\n                                                     sess.graph)\n\n            epoch_counter = 0\n            nn_in = None\n            nn_out = None\n            nn_loss = None\n            for epoch_counter in range(EPOCHS):\n\n                # re-initialize the dataset iterator\n                sess.run(model['data_init_op'])\n\n                try:\n                    while True:\n                        _, nn_in, nn_out, nn_labels, nn_loss, gs, summary = sess.run(\n                            [model['opt_op'],\n                             model['features'],\n                             model['flow'],\n                             model['labels'],\n                             model['loss'],\n                             model['global_step'],\n                             model['merged']])\n\n                        # tensorboard and  statistics\n                        train_writer.add_summary(summary, gs)\n                except tf.errors.OutOfRangeError:\n                    # do nothing\n                    if epoch_counter % SHOW_INTERVAL == 0:\n                        print(epoch_counter,  '/', EPOCHS,\n                              ': loss ', nn_loss,\n                              '. Global step: ', gs)\n\n            weights = sess.run(tf.get_default_graph().get_tensor_by_name(\n                'dense/kernel:0'))\n            saver = tf.train.Saver()\n            if quantize:\n                trained_model_path = './tmp/quantized_w' + \\\n                    str(w_b) + '_a_' + str(a_b) + '.ckpt'\n            else:\n                trained_model_path = './tmp/normal.ckpt'\n\n            saver.save(sess, trained_model_path)\n\n        # debug\n        print('nn_out shape: ', nn_out.shape)\n        print('nn_labels shape: ', nn_labels.shape)\n        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(nn_out)))\n        print('weights shape: ', weights.shape)\n        print('weights: ', weights)\n        print('weights different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(weights)))\n\n        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n        plt.plot(weights[0, :], label='weights',\n                 marker='o', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n        print('[debug] exit training')\n        return trained_model_path\n\n\ndef inference_quantization(trained_model_path, quantize=True, w_b=3, a_b=3):\n\n    #################################\n    # Reset default graph\n    tf.reset_default_graph()\n    #################################\n\n    g_inf = tf.Graph()\n    with g_inf.as_default():\n\n        model = create_dummy_model(False)\n\n        with tf.Session(graph=g_inf) as sess:\n            # Call the eval rewrite which rewrites the graph in-place with\n            # FakeQuantization nodes and fold batchnorm for eval.\n            if quantize:\n                print('w_b: ', w_b)\n                print('a_b: ', a_b)\n                tf.contrib.quantize.experimental_create_eval_graph(\n                    weight_bits=w_b,\n                    activation_bits=a_b)\n\n            saver = tf.train.Saver()\n            saver.restore(sess, trained_model_path)\n            # frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n            #       sess, sess.graph_def, [\"out\"])\n            print('[debug] restored')\n\n            # re-initialize the iterator, but this time with training data\n\n            sess.run(model['data_init_op'])\n\n            nn_in, nn_out, nn_labels, nn_loss = sess.run([\n                model['features'],\n                model['flow'],\n                model['labels'],\n                model['loss']])\n            weights = sess.run(tf.get_default_graph(\n            ).get_tensor_by_name('dense/kernel:0'))\n\n        # debug\n        print('nn_out shape: ', nn_out.shape)\n        print('nn_labels shape: ', nn_labels.shape)\n        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(nn_out)))\n        print('weights shape: ', weights.shape)\n        # print('weights: ', weights)\n        print('weights different values in OUT_CH (', OUT_CH, ') elements:',\n              len(np.unique(weights)))\n        print('weights: ', weights)\n        q_weights = quantize_tensor(weights, w_b)\n        print('q_weights: ', q_weights)\n        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',\n                 marker='o', markersize=5)\n        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',\n                 marker='*', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n        plt.plot(weights[0, :], label='weights',\n                 marker='o', markersize=5)\n        plt.legend(loc='lower right')\n        plt.show()\n\n\ndef test_quantization():\n\n    # train_quantization(False)\n    trained_model_path = train_quantization(quantize=True, w_b=2, a_b=2)\n    inference_quantization(\n        trained_model_path=trained_model_path, quantize=True, w_b=2, a_b=2)\n\ntest_quantization()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, provided\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: - \r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: python quantization_test.py (file provided)\r\n\r\n### Describe the problem\r\n**tf.contrib.quantize** (experimental and non-experimental) automatic quantization produce close to but not quantized results as when you manually introduce fakequant operations.\r\nWeights and/or activations are close to the quantized values, but they are not correct.\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nD_SIZE = 2048\r\nBATCH_SIZE = 128\r\nLR = 1e-2\r\nEPOCHS = 2000\r\n\r\nSHOW_INTERVAL = 20\r\nOUT_CH = 8\r\n\r\n\r\ndef create_dummy_model(is_training=True):\r\n    # prepare dataset\r\n    np_features = np.arange(-int(D_SIZE) / 2, int(D_SIZE / 2))\r\n    features_arr = tf.constant(np_features, dtype=tf.float32)\r\n    np_labels = np.zeros((D_SIZE, OUT_CH), dtype=float)\r\n    print(np_labels.shape)\r\n    for i in range(OUT_CH):\r\n        np_labels[:, i] = 2 * (i + 1) * np_features\r\n    print('np_features:')\r\n    print(np_features[0:8])\r\n    print('np_labels:')\r\n    print(np_labels[0:8])\r\n\r\n    labels_arr = tf.constant(np_labels, dtype=tf.float32)\r\n    trn_data = tf.data.Dataset.zip(\r\n        (tf.data.Dataset.from_tensor_slices(features_arr),\r\n         tf.data.Dataset.from_tensor_slices(labels_arr))\r\n    )\r\n    trn_data = trn_data.shuffle(D_SIZE)\r\n    trn_data = trn_data.batch(BATCH_SIZE)\r\n    trn_data = trn_data.repeat(1)\r\n    trn_data = trn_data.prefetch(1)\r\n\r\n    iterator = tf.data.Iterator.from_structure(trn_data.output_types,\r\n                                               trn_data.output_shapes)\r\n    features, labels = iterator.get_next()\r\n    data_init_op = iterator.make_initializer(trn_data)\r\n\r\n    # global step\r\n    global_step = tf.train.get_or_create_global_step()\r\n\r\n    # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\r\n    input = tf.reshape(features, (BATCH_SIZE, 1))\r\n    flow = tf.identity(input, name='input_tensor')\r\n    # batch_size x INPUT_CH\r\n    flow = tf.reshape(flow, (-1, 1))\r\n    flow = tf.layers.dense(flow, units=OUT_CH, name='dense')\r\n    flow = tf.reshape(flow, (-1, OUT_CH), name='out')\r\n\r\n    # Build forward pass of model.\r\n    # Make the loss op\r\n    with tf.variable_scope('loss'):\r\n        tf.logging.debug('[loss] size of out: %s',\r\n                         flow.get_shape().as_list())\r\n        power = tf.pow(labels - flow, 2)\r\n        power = tf.reduce_sum(power, 1)\r\n        # power = tf.reduce_sum(power)\r\n        loss = tf.reduce_mean(power, name='m_loss')\r\n    tf.summary.scalar('loss', loss)\r\n\r\n    # Optimizer\r\n    if is_training:\r\n        opt_op = tf.train.AdamOptimizer(LR).minimize(loss, global_step)\r\n        # opt_op = tf.train.GradientDescentOptimizer(1e-4).minimize(loss, global_step)\r\n    else:\r\n        opt_op = False\r\n    # Merge all the summaries\r\n    merged = tf.summary.merge_all()\r\n\r\n    return {\r\n        'iterator': iterator,\r\n        'features': features,\r\n        'labels': labels,\r\n        'data_init_op': data_init_op,\r\n        'global_step': global_step,\r\n        'flow': flow,\r\n        'loss': loss,\r\n        'opt_op': opt_op,\r\n        'merged': merged\r\n    }\r\n\r\n\r\ndef quantize_tensor(np_array, n_bits):\r\n    \"\"\"Quantizes an np_array\"\"\"\r\n    min_wt = np.min(np_array)\r\n    max_wt = np.max(np_array)\r\n    # clap values if required\r\n    if min_wt < -2**(n_bits-1):\r\n        min_wt = -2**(n_bits-1)\r\n    if max_wt > (2**(n_bits-1) - 1) :\r\n        max_wt = (2**(n_bits-1) - 1)\r\n    # find number of integer bits to represent this range\r\n    int_bits = int(np.ceil(np.log2(max(abs(min_wt), abs(max_wt)))))\r\n    print('int_bits: ', int_bits)\r\n    # remaining bits are fractional bits (1-bit for sign)\r\n    frac_bits = n_bits -1 - int_bits\r\n    # floating point weights are scaled and rounded to [-128,127], which are used in\r\n    # the fixed-point operations on the actual hardware (i.e., microcontroller)\r\n    quantized = np.round(np_array * (2**frac_bits))\r\n    # To quantify the impact of quantized weights, scale them back to\r\n    # original range to run inference using quantized weights\r\n    return quantized / (2**frac_bits)\r\n\r\n\r\ndef train_quantization(quantize=True, w_b=3, a_b=3):\r\n\r\n    g_tr = tf.Graph()\r\n    with g_tr.as_default():\r\n\r\n        model = create_dummy_model(True)\r\n\r\n        # Call the training rewrite which rewrites the graph in-place with\r\n        # FakeQuantization nodes and folds batchnorm for training. It is\r\n        # often needed to fine tune a floating point model for quantization\r\n        # with this training tool. When training from scratch, quant_delay\r\n        # can be used to activate quantization after training to converge\r\n        # with the float graph, effectively fine-tuning the model.\r\n        if quantize:\r\n            quant_delay = int((D_SIZE / BATCH_SIZE) * EPOCHS * 3 / 4)\r\n            print('quant_delay: ', quant_delay)\r\n            print('w_b: ', w_b)\r\n            print('a_b: ', a_b)\r\n            tf.contrib.quantize.experimental_create_training_graph(weight_bits=w_b,\r\n                                                                   activation_bits=a_b,\r\n                                                                   quant_delay=quant_delay)\r\n\r\n        with tf.Session(graph=g_tr) as sess:\r\n            # global variables Initializing\r\n            sess.run(tf.global_variables_initializer())\r\n            # local variables Initializing\r\n            sess.run(tf.local_variables_initializer())\r\n\r\n            if quantize:\r\n                train_writer = tf.summary.FileWriter(\r\n                    './tmp/train_quantized_w' + str(w_b) + '_a_' + str(a_b),\r\n                    sess.graph)\r\n            else:\r\n                train_writer = tf.summary.FileWriter('./tmp/train_normal',\r\n                                                     sess.graph)\r\n\r\n            epoch_counter = 0\r\n            nn_in = None\r\n            nn_out = None\r\n            nn_loss = None\r\n            for epoch_counter in range(EPOCHS):\r\n\r\n                # re-initialize the dataset iterator\r\n                sess.run(model['data_init_op'])\r\n\r\n                try:\r\n                    while True:\r\n                        _, nn_in, nn_out, nn_labels, nn_loss, gs, summary = sess.run(\r\n                            [model['opt_op'],\r\n                             model['features'],\r\n                             model['flow'],\r\n                             model['labels'],\r\n                             model['loss'],\r\n                             model['global_step'],\r\n                             model['merged']])\r\n\r\n                        # tensorboard and  statistics\r\n                        train_writer.add_summary(summary, gs)\r\n                except tf.errors.OutOfRangeError:\r\n                    # do nothing\r\n                    if epoch_counter % SHOW_INTERVAL == 0:\r\n                        print(epoch_counter,  '/', EPOCHS,\r\n                              ': loss ', nn_loss,\r\n                              '. Global step: ', gs)\r\n\r\n            weights = sess.run(tf.get_default_graph().get_tensor_by_name(\r\n                'dense/kernel:0'))\r\n            saver = tf.train.Saver()\r\n            if quantize:\r\n                trained_model_path = './tmp/quantized_w' + \\\r\n                    str(w_b) + '_a_' + str(a_b) + '.ckpt'\r\n            else:\r\n                trained_model_path = './tmp/normal.ckpt'\r\n\r\n            saver.save(sess, trained_model_path)\r\n\r\n        # debug\r\n        print('nn_out shape: ', nn_out.shape)\r\n        print('nn_labels shape: ', nn_labels.shape)\r\n        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',\r\n              len(np.unique(nn_out)))\r\n        print('weights shape: ', weights.shape)\r\n        print('weights: ', weights)\r\n        print('weights different values in OUT_CH (', OUT_CH, ') elements:',\r\n              len(np.unique(weights)))\r\n\r\n        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',\r\n                 marker='o', markersize=5)\r\n        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',\r\n                 marker='*', markersize=5)\r\n        plt.legend(loc='lower right')\r\n        plt.show()\r\n        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',\r\n                 marker='o', markersize=5)\r\n        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',\r\n                 marker='*', markersize=5)\r\n        plt.legend(loc='lower right')\r\n        plt.show()\r\n\r\n        plt.plot(weights[0, :], label='weights',\r\n                 marker='o', markersize=5)\r\n        plt.legend(loc='lower right')\r\n        plt.show()\r\n\r\n        print('[debug] exit training')\r\n        return trained_model_path\r\n\r\n\r\ndef inference_quantization(trained_model_path, quantize=True, w_b=3, a_b=3):\r\n\r\n    #################################\r\n    # Reset default graph\r\n    tf.reset_default_graph()\r\n    #################################\r\n\r\n    g_inf = tf.Graph()\r\n    with g_inf.as_default():\r\n\r\n        model = create_dummy_model(False)\r\n\r\n        with tf.Session(graph=g_inf) as sess:\r\n            # Call the eval rewrite which rewrites the graph in-place with\r\n            # FakeQuantization nodes and fold batchnorm for eval.\r\n            if quantize:\r\n                print('w_b: ', w_b)\r\n                print('a_b: ', a_b)\r\n                tf.contrib.quantize.experimental_create_eval_graph(\r\n                    weight_bits=w_b,\r\n                    activation_bits=a_b)\r\n\r\n            saver = tf.train.Saver()\r\n            saver.restore(sess, trained_model_path)\r\n            # frozen_graphdef = tf.graph_util.convert_variables_to_constants(\r\n            #       sess, sess.graph_def, [\"out\"])\r\n            print('[debug] restored')\r\n\r\n            # re-initialize the iterator, but this time with training data\r\n\r\n            sess.run(model['data_init_op'])\r\n\r\n            nn_in, nn_out, nn_labels, nn_loss = sess.run([\r\n                model['features'],\r\n                model['flow'],\r\n                model['labels'],\r\n                model['loss']])\r\n            weights = sess.run(tf.get_default_graph(\r\n            ).get_tensor_by_name('dense/kernel:0'))\r\n\r\n        # debug\r\n        print('nn_out shape: ', nn_out.shape)\r\n        print('nn_labels shape: ', nn_labels.shape)\r\n        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',\r\n              len(np.unique(nn_out)))\r\n        print('weights shape: ', weights.shape)\r\n        # print('weights: ', weights)\r\n        print('weights different values in OUT_CH (', OUT_CH, ') elements:',\r\n              len(np.unique(weights)))\r\n        print('weights: ', weights)\r\n        q_weights = quantize_tensor(weights, w_b)\r\n        print('q_weights: ', q_weights)\r\n        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',\r\n                 marker='o', markersize=5)\r\n        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',\r\n                 marker='*', markersize=5)\r\n        plt.legend(loc='lower right')\r\n        plt.show()\r\n        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',\r\n                 marker='o', markersize=5)\r\n        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',\r\n                 marker='*', markersize=5)\r\n        plt.legend(loc='lower right')\r\n        plt.show()\r\n\r\n        plt.plot(weights[0, :], label='weights',\r\n                 marker='o', markersize=5)\r\n        plt.legend(loc='lower right')\r\n        plt.show()\r\n\r\n\r\ndef test_quantization():\r\n\r\n    # train_quantization(False)\r\n    trained_model_path = train_quantization(quantize=True, w_b=2, a_b=2)\r\n    inference_quantization(\r\n        trained_model_path=trained_model_path, quantize=True, w_b=2, a_b=2)\r\n\r\ntest_quantization()\r\n```\r\n\r\n"}