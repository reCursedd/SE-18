{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/414033604", "html_url": "https://github.com/tensorflow/tensorflow/issues/20694#issuecomment-414033604", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20694", "id": 414033604, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDAzMzYwNA==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-18T05:27:19Z", "updated_at": "2018-08-18T05:27:19Z", "author_association": "MEMBER", "body_html": "<p>This is because the FakeQuant operations do more than just quantize. First its very important that floating point 0.0 maps directly to a quantized value, otherwise padding errors can propagate. This nudging can be seen here: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fake_quant_ops_functor.h#L41\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fake_quant_ops_functor.h#L41</a></p>\n<p>Also we restrict the weight values to being quantized from [1,255] instead of [0,255] to enable a ARM Neon optimization for our kernels.</p>\n<p>Thus, this is working as intended, please do reopen and correct me if I am misunderstanding or mistaken.</p>", "body_text": "This is because the FakeQuant operations do more than just quantize. First its very important that floating point 0.0 maps directly to a quantized value, otherwise padding errors can propagate. This nudging can be seen here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fake_quant_ops_functor.h#L41\nAlso we restrict the weight values to being quantized from [1,255] instead of [0,255] to enable a ARM Neon optimization for our kernels.\nThus, this is working as intended, please do reopen and correct me if I am misunderstanding or mistaken.", "body": "This is because the FakeQuant operations do more than just quantize. First its very important that floating point 0.0 maps directly to a quantized value, otherwise padding errors can propagate. This nudging can be seen here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fake_quant_ops_functor.h#L41\r\n\r\nAlso we restrict the weight values to being quantized from [1,255] instead of [0,255] to enable a ARM Neon optimization for our kernels.\r\n\r\nThus, this is working as intended, please do reopen and correct me if I am misunderstanding or mistaken."}