{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244952424", "html_url": "https://github.com/tensorflow/tensorflow/pull/4198#issuecomment-244952424", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4198", "id": 244952424, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDk1MjQyNA==", "user": {"login": "vincentvanhoucke", "id": 15737127, "node_id": "MDQ6VXNlcjE1NzM3MTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/15737127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentvanhoucke", "html_url": "https://github.com/vincentvanhoucke", "followers_url": "https://api.github.com/users/vincentvanhoucke/followers", "following_url": "https://api.github.com/users/vincentvanhoucke/following{/other_user}", "gists_url": "https://api.github.com/users/vincentvanhoucke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentvanhoucke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentvanhoucke/subscriptions", "organizations_url": "https://api.github.com/users/vincentvanhoucke/orgs", "repos_url": "https://api.github.com/users/vincentvanhoucke/repos", "events_url": "https://api.github.com/users/vincentvanhoucke/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentvanhoucke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-06T13:38:12Z", "updated_at": "2016-09-06T13:38:12Z", "author_association": "MEMBER", "body_html": "<p>Sorry, I had skimmed through your change too quickly. The issue here is different: if you use two separate kernels to compute <code>x-shift</code> and <code>square()</code> instead of the fused <code>squared_difference()</code>, you end up with two large tensors that need to be preserved through the forward <em>and</em> backward pass: 1) the <code>x</code> tensor itself needed for the backward pass through <code>m_ss</code> 2) <code>x-shift</code>, which is needed for the backward pass through <code>square()</code>. So, while your change is a bit more efficient computationally, it also effectively doubles the amount of memory that's required to keep the activations around, and that's a killer for large models.</p>", "body_text": "Sorry, I had skimmed through your change too quickly. The issue here is different: if you use two separate kernels to compute x-shift and square() instead of the fused squared_difference(), you end up with two large tensors that need to be preserved through the forward and backward pass: 1) the x tensor itself needed for the backward pass through m_ss 2) x-shift, which is needed for the backward pass through square(). So, while your change is a bit more efficient computationally, it also effectively doubles the amount of memory that's required to keep the activations around, and that's a killer for large models.", "body": "Sorry, I had skimmed through your change too quickly. The issue here is different: if you use two separate kernels to compute `x-shift` and `square()` instead of the fused `squared_difference()`, you end up with two large tensors that need to be preserved through the forward _and_ backward pass: 1) the `x` tensor itself needed for the backward pass through `m_ss` 2) `x-shift`, which is needed for the backward pass through `square()`. So, while your change is a bit more efficient computationally, it also effectively doubles the amount of memory that's required to keep the activations around, and that's a killer for large models.\n"}