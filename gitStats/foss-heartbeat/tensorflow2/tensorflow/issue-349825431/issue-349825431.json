{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21557", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21557/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21557/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21557/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21557", "id": 349825431, "node_id": "MDU6SXNzdWUzNDk4MjU0MzE=", "number": 21557, "title": "In DNNClassifier evaluation results, the 'loss' is not always = 'average_loss' * batch_size", "user": {"login": "ageron", "id": 76661, "node_id": "MDQ6VXNlcjc2NjYx", "avatar_url": "https://avatars3.githubusercontent.com/u/76661?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ageron", "html_url": "https://github.com/ageron", "followers_url": "https://api.github.com/users/ageron/followers", "following_url": "https://api.github.com/users/ageron/following{/other_user}", "gists_url": "https://api.github.com/users/ageron/gists{/gist_id}", "starred_url": "https://api.github.com/users/ageron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ageron/subscriptions", "organizations_url": "https://api.github.com/users/ageron/orgs", "repos_url": "https://api.github.com/users/ageron/repos", "events_url": "https://api.github.com/users/ageron/events{/privacy}", "received_events_url": "https://api.github.com/users/ageron/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-12T16:31:14Z", "updated_at": "2018-08-21T08:00:52Z", "closed_at": "2018-08-20T21:49:50Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: MacOSX 10.13.6</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: yes</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.10.0-rc1-19-g656e7a2b34 1.10.0</li>\n<li><strong>Python version</strong>: Python 3.6.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The evaluation results of a <code>DNNClassifier</code> include a <code>'loss'</code> and an <code>'average_loss</code>'.  It seems that the <code>'average_loss'</code> is correct (same as when I compute it manually), while the <code>'loss'</code> is weird.<br>\nIt seems that it's roughly equal to the <code>'average_loss'</code> times the batch size, but not always. This seems like a bug to me.</p>\n<p>I wrote a test (see the code below), and here is its output with different batch sizes (notice that the loss/average_loss ratio is often equal or close to the batch size, but not always, and sometimes it is exactly one off, or simply far from it):</p>\n<pre><code>Batch size: 1 loss: 0.75871724 average_loss: 0.75871724 Ratio: 1.0\nBatch size: 11 loss: 8.33756 average_loss: 0.7587179 Ratio: 10.989012\nBatch size: 21 loss: 15.806628 average_loss: 0.75871813 Ratio: 20.833334\nBatch size: 31 loss: 22.991457 average_loss: 0.7587181 Ratio: 30.30303\nBatch size: 41 loss: 30.348719 average_loss: 0.75871795 Ratio: 40.0\nBatch size: 51 loss: 37.935898 average_loss: 0.75871795 Ratio: 50.0\nBatch size: 61 loss: 44.63047 average_loss: 0.758718 Ratio: 58.82353\nBatch size: 71 loss: 50.581203 average_loss: 0.7587181 Ratio: 66.666664\nBatch size: 81 loss: 58.36292 average_loss: 0.75871795 Ratio: 76.92307\nBatch size: 91 loss: 68.974365 average_loss: 0.758718 Ratio: 90.90909\nBatch size: 101 loss: 75.8718 average_loss: 0.758718 Ratio: 100.0\nBatch size: 111 loss: 75.871796 average_loss: 0.75871795 Ratio: 100.0\nBatch size: 121 loss: 84.302 average_loss: 0.758718 Ratio: 111.111115\nBatch size: 131 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\nBatch size: 141 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\n</code></pre>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division, print_function, unicode_literals\n\n<span class=\"pl-k\">import</span> logging\nlogging.basicConfig(<span class=\"pl-v\">level</span><span class=\"pl-k\">=</span>logging.<span class=\"pl-c1\">WARNING</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> or else it's too verbose</span>\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>tf.enable_eager_execution()  # same exact output if you activate eager execution</span>\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> make the code reproducible (see https://youtu.be/Ys8ofBeR2kA ;-) )</span>\nnp.random.seed(<span class=\"pl-c1\">1234</span>)\nconfig <span class=\"pl-k\">=</span> tf.estimator.RunConfig(<span class=\"pl-v\">tf_random_seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1234</span>)\n\nnum_col <span class=\"pl-k\">=</span> tf.feature_column.numeric_column(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">10</span>])\ndnn_clf <span class=\"pl-k\">=</span> tf.estimator.DNNClassifier(<span class=\"pl-v\">hidden_units</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">100</span>], <span class=\"pl-v\">n_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,\n                                     <span class=\"pl-v\">feature_columns</span><span class=\"pl-k\">=</span>[num_col], <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> some random data</span>\nX <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">10</span>)\ny <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">3</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train the model</span>\ninput_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n    <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X<span class=\"pl-pds\">\"</span></span>: X}, <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>y, <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\ndnn_clf.train(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>input_fn)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> evaluate the model with different batch sizes</span>\n<span class=\"pl-k\">for</span> batch_size <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">151</span>, <span class=\"pl-c1\">10</span>):\n    input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n        <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X<span class=\"pl-pds\">\"</span></span>: X}, <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>y, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    eval_results <span class=\"pl-k\">=</span> dnn_clf.evaluate(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>input_fn)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Batch size:<span class=\"pl-pds\">\"</span></span>, batch_size, <span class=\"pl-v\">end</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span> <span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss:<span class=\"pl-pds\">\"</span></span>, eval_results[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss<span class=\"pl-pds\">\"</span></span>], <span class=\"pl-v\">end</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span> <span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>average_loss:<span class=\"pl-pds\">\"</span></span>, eval_results[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>average_loss<span class=\"pl-pds\">\"</span></span>], <span class=\"pl-v\">end</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span> <span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Ratio:<span class=\"pl-pds\">\"</span></span>, eval_results[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">/</span> eval_results[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>average_loss<span class=\"pl-pds\">\"</span></span>])</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.13.6\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): yes\nTensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34 1.10.0\nPython version: Python 3.6.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:\n\nDescribe the problem\nThe evaluation results of a DNNClassifier include a 'loss' and an 'average_loss'.  It seems that the 'average_loss' is correct (same as when I compute it manually), while the 'loss' is weird.\nIt seems that it's roughly equal to the 'average_loss' times the batch size, but not always. This seems like a bug to me.\nI wrote a test (see the code below), and here is its output with different batch sizes (notice that the loss/average_loss ratio is often equal or close to the batch size, but not always, and sometimes it is exactly one off, or simply far from it):\nBatch size: 1 loss: 0.75871724 average_loss: 0.75871724 Ratio: 1.0\nBatch size: 11 loss: 8.33756 average_loss: 0.7587179 Ratio: 10.989012\nBatch size: 21 loss: 15.806628 average_loss: 0.75871813 Ratio: 20.833334\nBatch size: 31 loss: 22.991457 average_loss: 0.7587181 Ratio: 30.30303\nBatch size: 41 loss: 30.348719 average_loss: 0.75871795 Ratio: 40.0\nBatch size: 51 loss: 37.935898 average_loss: 0.75871795 Ratio: 50.0\nBatch size: 61 loss: 44.63047 average_loss: 0.758718 Ratio: 58.82353\nBatch size: 71 loss: 50.581203 average_loss: 0.7587181 Ratio: 66.666664\nBatch size: 81 loss: 58.36292 average_loss: 0.75871795 Ratio: 76.92307\nBatch size: 91 loss: 68.974365 average_loss: 0.758718 Ratio: 90.90909\nBatch size: 101 loss: 75.8718 average_loss: 0.758718 Ratio: 100.0\nBatch size: 111 loss: 75.871796 average_loss: 0.75871795 Ratio: 100.0\nBatch size: 121 loss: 84.302 average_loss: 0.758718 Ratio: 111.111115\nBatch size: 131 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\nBatch size: 141 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\n\nSource code / logs\nfrom __future__ import division, print_function, unicode_literals\n\nimport logging\nlogging.basicConfig(level=logging.WARNING) # or else it's too verbose\n\nimport tensorflow as tf\n#tf.enable_eager_execution()  # same exact output if you activate eager execution\nimport numpy as np\n\n# make the code reproducible (see https://youtu.be/Ys8ofBeR2kA ;-) )\nnp.random.seed(1234)\nconfig = tf.estimator.RunConfig(tf_random_seed=1234)\n\nnum_col = tf.feature_column.numeric_column(\"X\", shape=[10])\ndnn_clf = tf.estimator.DNNClassifier(hidden_units=[200, 100], n_classes=3,\n                                     feature_columns=[num_col], config=config)\n\n# some random data\nX = np.random.randn(1000, 10)\ny = np.random.randint(3, size=1000)\n\n# train the model\ninput_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"X\": X}, y=y, num_epochs=10, batch_size=32, shuffle=False)\ndnn_clf.train(input_fn=input_fn)\n\n# evaluate the model with different batch sizes\nfor batch_size in range(1, 151, 10):\n    input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={\"X\": X}, y=y, batch_size=batch_size, shuffle=False)\n    eval_results = dnn_clf.evaluate(input_fn=input_fn)\n    print(\"Batch size:\", batch_size, end=\" \")\n    print(\"loss:\", eval_results[\"loss\"], end=\" \")\n    print(\"average_loss:\", eval_results[\"average_loss\"], end=\" \")\n    print(\"Ratio:\", eval_results[\"loss\"] / eval_results[\"average_loss\"])", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: yes\r\n- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0\r\n- **Python version**: Python 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThe evaluation results of a `DNNClassifier` include a `'loss'` and an `'average_loss`'.  It seems that the `'average_loss'` is correct (same as when I compute it manually), while the `'loss'` is weird. \r\n It seems that it's roughly equal to the `'average_loss'` times the batch size, but not always. This seems like a bug to me.\r\n\r\nI wrote a test (see the code below), and here is its output with different batch sizes (notice that the loss/average_loss ratio is often equal or close to the batch size, but not always, and sometimes it is exactly one off, or simply far from it):\r\n\r\n```\r\nBatch size: 1 loss: 0.75871724 average_loss: 0.75871724 Ratio: 1.0\r\nBatch size: 11 loss: 8.33756 average_loss: 0.7587179 Ratio: 10.989012\r\nBatch size: 21 loss: 15.806628 average_loss: 0.75871813 Ratio: 20.833334\r\nBatch size: 31 loss: 22.991457 average_loss: 0.7587181 Ratio: 30.30303\r\nBatch size: 41 loss: 30.348719 average_loss: 0.75871795 Ratio: 40.0\r\nBatch size: 51 loss: 37.935898 average_loss: 0.75871795 Ratio: 50.0\r\nBatch size: 61 loss: 44.63047 average_loss: 0.758718 Ratio: 58.82353\r\nBatch size: 71 loss: 50.581203 average_loss: 0.7587181 Ratio: 66.666664\r\nBatch size: 81 loss: 58.36292 average_loss: 0.75871795 Ratio: 76.92307\r\nBatch size: 91 loss: 68.974365 average_loss: 0.758718 Ratio: 90.90909\r\nBatch size: 101 loss: 75.8718 average_loss: 0.758718 Ratio: 100.0\r\nBatch size: 111 loss: 75.871796 average_loss: 0.75871795 Ratio: 100.0\r\nBatch size: 121 loss: 84.302 average_loss: 0.758718 Ratio: 111.111115\r\nBatch size: 131 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\r\nBatch size: 141 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0\r\n```\r\n\r\n### Source code / logs\r\n```python\r\nfrom __future__ import division, print_function, unicode_literals\r\n\r\nimport logging\r\nlogging.basicConfig(level=logging.WARNING) # or else it's too verbose\r\n\r\nimport tensorflow as tf\r\n#tf.enable_eager_execution()  # same exact output if you activate eager execution\r\nimport numpy as np\r\n\r\n# make the code reproducible (see https://youtu.be/Ys8ofBeR2kA ;-) )\r\nnp.random.seed(1234)\r\nconfig = tf.estimator.RunConfig(tf_random_seed=1234)\r\n\r\nnum_col = tf.feature_column.numeric_column(\"X\", shape=[10])\r\ndnn_clf = tf.estimator.DNNClassifier(hidden_units=[200, 100], n_classes=3,\r\n                                     feature_columns=[num_col], config=config)\r\n\r\n# some random data\r\nX = np.random.randn(1000, 10)\r\ny = np.random.randint(3, size=1000)\r\n\r\n# train the model\r\ninput_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"X\": X}, y=y, num_epochs=10, batch_size=32, shuffle=False)\r\ndnn_clf.train(input_fn=input_fn)\r\n\r\n# evaluate the model with different batch sizes\r\nfor batch_size in range(1, 151, 10):\r\n    input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"X\": X}, y=y, batch_size=batch_size, shuffle=False)\r\n    eval_results = dnn_clf.evaluate(input_fn=input_fn)\r\n    print(\"Batch size:\", batch_size, end=\" \")\r\n    print(\"loss:\", eval_results[\"loss\"], end=\" \")\r\n    print(\"average_loss:\", eval_results[\"average_loss\"], end=\" \")\r\n    print(\"Ratio:\", eval_results[\"loss\"] / eval_results[\"average_loss\"])\r\n```\r\n"}