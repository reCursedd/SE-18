{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18835", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18835/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18835/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18835/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18835", "id": 317307106, "node_id": "MDU6SXNzdWUzMTczMDcxMDY=", "number": 18835, "title": "Issue with building the Hexagon HVX DSP toolchain with TF", "user": {"login": "amirjamez", "id": 8922398, "node_id": "MDQ6VXNlcjg5MjIzOTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/8922398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amirjamez", "html_url": "https://github.com/amirjamez", "followers_url": "https://api.github.com/users/amirjamez/followers", "following_url": "https://api.github.com/users/amirjamez/following{/other_user}", "gists_url": "https://api.github.com/users/amirjamez/gists{/gist_id}", "starred_url": "https://api.github.com/users/amirjamez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amirjamez/subscriptions", "organizations_url": "https://api.github.com/users/amirjamez/orgs", "repos_url": "https://api.github.com/users/amirjamez/repos", "events_url": "https://api.github.com/users/amirjamez/events{/privacy}", "received_events_url": "https://api.github.com/users/amirjamez/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 26, "created_at": "2018-04-24T16:39:38Z", "updated_at": "2018-08-16T18:27:29Z", "closed_at": "2018-08-16T18:27:28Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: NO</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8r</li>\n<li><strong>Python version</strong>:  2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.8.4</li>\n<li><strong>CUDA/cuDNN version</strong>: 8</li>\n<li><strong>GPU model and memory</strong>: GTX 1060</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20085789\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/satok16\">@satok16</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17151892\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tensorflower-gardener\">@tensorflower-gardener</a> , Could you please clarify the different version-combination of tools to use in an HVX-TF toolchain. It can save many people's effort to find and match these together. You have already mentioned in <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx\">HVX_TF</a>, that for <code>Qualcomm SDK 3.0</code>, the compatible nnlib version is <a href=\"https://source.codeaurora.org/quic/hexagon_nn/nnlib/commit/?id=721b2d58f0f4e2d5b182f41e6b7c4db5356bf0fb\" rel=\"nofollow\">Aug-2017-commit</a>. However, some features are missing there and we need to use the latest version with <code>SDK 3.3.3</code>, however, these libraries (<code>nnlib</code>, <code>libcontroller</code>, <code>TF-(sub)makefiles</code>) don't match, thus it has become a messy process to figure out which is related to what version.</p>\n<p><strong>1)</strong> A sample error compiling libcontroller after compiling nnlib with latest SDK (3.3.3):</p>\n<pre><code>src_impl/hexagon_controller.c: In function 'hexagon_controller_AppendNode':\nsrc_impl/hexagon_controller.c:484:70: error: 'hexagon_nn_output' has no member named 'max_size'\n     pos += snprintf(&amp;output_param_buf[pos], 500, \"(%d), \", outputs[i].max_size);\n                                                                      ^\nmake[1]: *** [android_Release/hexagon_controller.o] Error 1\nmake[1]: Leaving directory `~/Qualcomm/Hexagon_SDK/3.3.3/examples/common/hexagon_controller'\nmake: *** [tree] Error 2\n\n</code></pre>\n<p><strong>2)</strong> If I use your suggested commit and SDK 3.0, this is the output I am having:</p>\n<pre><code>native : hexagon_graph_execution_test.cc:129 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\nnative : hexagon_graph_execution_test.cc:135 header size = 54\nnative : hexagon_graph_execution_test.cc:137 image size = 40\nnative : hexagon_graph_execution_test.cc:139 width = 299\nnative : hexagon_graph_execution_test.cc:141 height = -299\nnative : hexagon_graph_execution_test.cc:286 Ioading image finished.\nnative : hexagon_graph_execution_test.cc:185 Loading image finished.\nnative : hexagon_graph_execution_test.cc:189 Copy data to tensor.\nnative : hexagon_graph_execution_test.cc:307 Run graph\nInit hexagon with max attributes (Controller version = 101)\nnative : hexagon_control_wrapper.cc:104 Add input: Mul, 0\nnative : hexagon_control_wrapper.cc:127 Allocate inout buffer\nnative : hexagon_control_wrapper.cc:304 Setup graph completed\nPrepare failed! returned 0xffffffff\n\nNN Id = -755923072\nExecute graph!\nExecution failed!\nexecute got err: -1\n\nNN Id = -755923072\nExecution failed\nNN Id = -755923072\nFailed to read data.\nnative : hexagon_graph_execution_test.cc:313 Output byte size = 4032\nnative : hexagon_graph_execution_test.cc:314 Output shape = [1,1008]\nnative : graph_transfer_utils.cc:47 === Dump ranking ===\nnative : graph_transfer_utils.cc:50 0: 1000, dumbbell, 0\nnative : graph_transfer_utils.cc:50 1: 999, carbonara, 0\nnative : graph_transfer_utils.cc:50 2: 998, stole, 0\nnative : graph_transfer_utils.cc:50 3: 997, rubber eraser, 0\nnative : graph_transfer_utils.cc:50 4: 996, coffee mug, 0\nnative : graph_transfer_utils.cc:50 5: 995, flagpole, 0\nnative : graph_transfer_utils.cc:50 6: 994, parallel bars, 0\nnative : graph_transfer_utils.cc:50 7: 993, cheeseburger, 0\nnative : graph_transfer_utils.cc:50 8: 992, bubble, 0\nnative : graph_transfer_utils.cc:50 9: 991, beaker, 0\nFinalize hexagon\n[       OK ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5848 ms)\n[----------] 1 test from GraphTransferer (5848 ms total)\n\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (5849 ms total)\n[  PASSED  ] 1 test.\n\n  YOU HAVE 5 DISABLED TESTS\n</code></pre>\n<p><strong>3)</strong>  Also in the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx\">toturial</a>, you did not mention anything about downloading/using an inception-v3 frozen-quantized model. If one follows the building each library from the source, where is the part related to use the [<code>tensorflow_inception_v3_stripped_optimized_quantized.pb</code>] ? Since, when I test with my custom quantized_frozen inception-v3 model, (renaming it to be the same as the original 2016 file (<code>tensorflow_inception_v3_stripped_optimized_quantized.pb</code>), I have the error output as:</p>\n<pre><code>...\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\nt1(loading image time)=0.026770\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\nAborted\n\n</code></pre>\n<p>Thanks.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.8r\nPython version:  2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source): 4.8.4\nCUDA/cuDNN version: 8\nGPU model and memory: GTX 1060\nExact command to reproduce:\n\nDescribe the problem\n@satok16 @tensorflower-gardener , Could you please clarify the different version-combination of tools to use in an HVX-TF toolchain. It can save many people's effort to find and match these together. You have already mentioned in HVX_TF, that for Qualcomm SDK 3.0, the compatible nnlib version is Aug-2017-commit. However, some features are missing there and we need to use the latest version with SDK 3.3.3, however, these libraries (nnlib, libcontroller, TF-(sub)makefiles) don't match, thus it has become a messy process to figure out which is related to what version.\n1) A sample error compiling libcontroller after compiling nnlib with latest SDK (3.3.3):\nsrc_impl/hexagon_controller.c: In function 'hexagon_controller_AppendNode':\nsrc_impl/hexagon_controller.c:484:70: error: 'hexagon_nn_output' has no member named 'max_size'\n     pos += snprintf(&output_param_buf[pos], 500, \"(%d), \", outputs[i].max_size);\n                                                                      ^\nmake[1]: *** [android_Release/hexagon_controller.o] Error 1\nmake[1]: Leaving directory `~/Qualcomm/Hexagon_SDK/3.3.3/examples/common/hexagon_controller'\nmake: *** [tree] Error 2\n\n\n2) If I use your suggested commit and SDK 3.0, this is the output I am having:\nnative : hexagon_graph_execution_test.cc:129 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\nnative : hexagon_graph_execution_test.cc:135 header size = 54\nnative : hexagon_graph_execution_test.cc:137 image size = 40\nnative : hexagon_graph_execution_test.cc:139 width = 299\nnative : hexagon_graph_execution_test.cc:141 height = -299\nnative : hexagon_graph_execution_test.cc:286 Ioading image finished.\nnative : hexagon_graph_execution_test.cc:185 Loading image finished.\nnative : hexagon_graph_execution_test.cc:189 Copy data to tensor.\nnative : hexagon_graph_execution_test.cc:307 Run graph\nInit hexagon with max attributes (Controller version = 101)\nnative : hexagon_control_wrapper.cc:104 Add input: Mul, 0\nnative : hexagon_control_wrapper.cc:127 Allocate inout buffer\nnative : hexagon_control_wrapper.cc:304 Setup graph completed\nPrepare failed! returned 0xffffffff\n\nNN Id = -755923072\nExecute graph!\nExecution failed!\nexecute got err: -1\n\nNN Id = -755923072\nExecution failed\nNN Id = -755923072\nFailed to read data.\nnative : hexagon_graph_execution_test.cc:313 Output byte size = 4032\nnative : hexagon_graph_execution_test.cc:314 Output shape = [1,1008]\nnative : graph_transfer_utils.cc:47 === Dump ranking ===\nnative : graph_transfer_utils.cc:50 0: 1000, dumbbell, 0\nnative : graph_transfer_utils.cc:50 1: 999, carbonara, 0\nnative : graph_transfer_utils.cc:50 2: 998, stole, 0\nnative : graph_transfer_utils.cc:50 3: 997, rubber eraser, 0\nnative : graph_transfer_utils.cc:50 4: 996, coffee mug, 0\nnative : graph_transfer_utils.cc:50 5: 995, flagpole, 0\nnative : graph_transfer_utils.cc:50 6: 994, parallel bars, 0\nnative : graph_transfer_utils.cc:50 7: 993, cheeseburger, 0\nnative : graph_transfer_utils.cc:50 8: 992, bubble, 0\nnative : graph_transfer_utils.cc:50 9: 991, beaker, 0\nFinalize hexagon\n[       OK ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5848 ms)\n[----------] 1 test from GraphTransferer (5848 ms total)\n\n[----------] Global test environment tear-down\n[==========] 1 test from 1 test case ran. (5849 ms total)\n[  PASSED  ] 1 test.\n\n  YOU HAVE 5 DISABLED TESTS\n\n3)  Also in the toturial, you did not mention anything about downloading/using an inception-v3 frozen-quantized model. If one follows the building each library from the source, where is the part related to use the [tensorflow_inception_v3_stripped_optimized_quantized.pb] ? Since, when I test with my custom quantized_frozen inception-v3 model, (renaming it to be the same as the original 2016 file (tensorflow_inception_v3_stripped_optimized_quantized.pb), I have the error output as:\n...\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\nt1(loading image time)=0.026770\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\nAborted\n\n\nThanks.", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8r\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: GTX 1060\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n@satok16 @tensorflower-gardener , Could you please clarify the different version-combination of tools to use in an HVX-TF toolchain. It can save many people's effort to find and match these together. You have already mentioned in [HVX_TF](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx), that for `Qualcomm SDK 3.0`, the compatible nnlib version is [Aug-2017-commit](https://source.codeaurora.org/quic/hexagon_nn/nnlib/commit/?id=721b2d58f0f4e2d5b182f41e6b7c4db5356bf0fb). However, some features are missing there and we need to use the latest version with `SDK 3.3.3`, however, these libraries (`nnlib`, `libcontroller`, `TF-(sub)makefiles`) don't match, thus it has become a messy process to figure out which is related to what version. \r\n\r\n**1)** A sample error compiling libcontroller after compiling nnlib with latest SDK (3.3.3):\r\n\r\n```\r\nsrc_impl/hexagon_controller.c: In function 'hexagon_controller_AppendNode':\r\nsrc_impl/hexagon_controller.c:484:70: error: 'hexagon_nn_output' has no member named 'max_size'\r\n     pos += snprintf(&output_param_buf[pos], 500, \"(%d), \", outputs[i].max_size);\r\n                                                                      ^\r\nmake[1]: *** [android_Release/hexagon_controller.o] Error 1\r\nmake[1]: Leaving directory `~/Qualcomm/Hexagon_SDK/3.3.3/examples/common/hexagon_controller'\r\nmake: *** [tree] Error 2\r\n\r\n```\r\n**2)** If I use your suggested commit and SDK 3.0, this is the output I am having:\r\n\r\n```\r\nnative : hexagon_graph_execution_test.cc:129 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:135 header size = 54\r\nnative : hexagon_graph_execution_test.cc:137 image size = 40\r\nnative : hexagon_graph_execution_test.cc:139 width = 299\r\nnative : hexagon_graph_execution_test.cc:141 height = -299\r\nnative : hexagon_graph_execution_test.cc:286 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:185 Loading image finished.\r\nnative : hexagon_graph_execution_test.cc:189 Copy data to tensor.\r\nnative : hexagon_graph_execution_test.cc:307 Run graph\r\nInit hexagon with max attributes (Controller version = 101)\r\nnative : hexagon_control_wrapper.cc:104 Add input: Mul, 0\r\nnative : hexagon_control_wrapper.cc:127 Allocate inout buffer\r\nnative : hexagon_control_wrapper.cc:304 Setup graph completed\r\nPrepare failed! returned 0xffffffff\r\n\r\nNN Id = -755923072\r\nExecute graph!\r\nExecution failed!\r\nexecute got err: -1\r\n\r\nNN Id = -755923072\r\nExecution failed\r\nNN Id = -755923072\r\nFailed to read data.\r\nnative : hexagon_graph_execution_test.cc:313 Output byte size = 4032\r\nnative : hexagon_graph_execution_test.cc:314 Output shape = [1,1008]\r\nnative : graph_transfer_utils.cc:47 === Dump ranking ===\r\nnative : graph_transfer_utils.cc:50 0: 1000, dumbbell, 0\r\nnative : graph_transfer_utils.cc:50 1: 999, carbonara, 0\r\nnative : graph_transfer_utils.cc:50 2: 998, stole, 0\r\nnative : graph_transfer_utils.cc:50 3: 997, rubber eraser, 0\r\nnative : graph_transfer_utils.cc:50 4: 996, coffee mug, 0\r\nnative : graph_transfer_utils.cc:50 5: 995, flagpole, 0\r\nnative : graph_transfer_utils.cc:50 6: 994, parallel bars, 0\r\nnative : graph_transfer_utils.cc:50 7: 993, cheeseburger, 0\r\nnative : graph_transfer_utils.cc:50 8: 992, bubble, 0\r\nnative : graph_transfer_utils.cc:50 9: 991, beaker, 0\r\nFinalize hexagon\r\n[       OK ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5848 ms)\r\n[----------] 1 test from GraphTransferer (5848 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (5849 ms total)\r\n[  PASSED  ] 1 test.\r\n\r\n  YOU HAVE 5 DISABLED TESTS\r\n```\r\n\r\n**3)**  Also in the [toturial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx), you did not mention anything about downloading/using an inception-v3 frozen-quantized model. If one follows the building each library from the source, where is the part related to use the [`tensorflow_inception_v3_stripped_optimized_quantized.pb`] ? Since, when I test with my custom quantized_frozen inception-v3 model, (renaming it to be the same as the original 2016 file (`tensorflow_inception_v3_stripped_optimized_quantized.pb`), I have the error output as:\r\n\r\n```\r\n...\r\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\r\nt1(loading image time)=0.026770\r\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\r\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\r\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\r\nAborted\r\n\r\n```\r\nThanks.\r\n\r\n\r\n\r\n"}