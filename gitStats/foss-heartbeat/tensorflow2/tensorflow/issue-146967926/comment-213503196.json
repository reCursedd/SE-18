{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/213503196", "html_url": "https://github.com/tensorflow/tensorflow/issues/1825#issuecomment-213503196", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1825", "id": 213503196, "node_id": "MDEyOklzc3VlQ29tbWVudDIxMzUwMzE5Ng==", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-22T16:42:23Z", "updated_at": "2016-04-22T16:42:23Z", "author_association": "MEMBER", "body_html": "<p>Some of the functionality of 2) and 3) is covered by the SparseTensor class and the functions<br>\ntf.sparse_merge, and tf.sparse_to_dense.</p>\n<p>The op pair tf.diag / tf.diag_part  (and batch_ extensions) do perform the unpacking/packing similar to what you suggest in 3), but here the potential space savings are much larger. I suppose batch_matrix_band_part could have been written that way (with a companion unpacking op), but I thought the current form, which simply zeros out the remaining part of the matrix, more convenient and much faster than having to write a packing followed by an unpacking op to achieve the same.</p>\n<p>As always, feel free to send a PR if you think it would be useful.</p>", "body_text": "Some of the functionality of 2) and 3) is covered by the SparseTensor class and the functions\ntf.sparse_merge, and tf.sparse_to_dense.\nThe op pair tf.diag / tf.diag_part  (and batch_ extensions) do perform the unpacking/packing similar to what you suggest in 3), but here the potential space savings are much larger. I suppose batch_matrix_band_part could have been written that way (with a companion unpacking op), but I thought the current form, which simply zeros out the remaining part of the matrix, more convenient and much faster than having to write a packing followed by an unpacking op to achieve the same.\nAs always, feel free to send a PR if you think it would be useful.", "body": "Some of the functionality of 2) and 3) is covered by the SparseTensor class and the functions\ntf.sparse_merge, and tf.sparse_to_dense. \n\nThe op pair tf.diag / tf.diag_part  (and batch_ extensions) do perform the unpacking/packing similar to what you suggest in 3), but here the potential space savings are much larger. I suppose batch_matrix_band_part could have been written that way (with a companion unpacking op), but I thought the current form, which simply zeros out the remaining part of the matrix, more convenient and much faster than having to write a packing followed by an unpacking op to achieve the same.\n\nAs always, feel free to send a PR if you think it would be useful.\n"}