{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17297", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17297/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17297/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17297/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17297", "id": 300621819, "node_id": "MDU6SXNzdWUzMDA2MjE4MTk=", "number": 17297, "title": "Expectations of MonitoredTrainingSession for saving and restoring (fixes inside)", "user": {"login": "izaid", "id": 482179, "node_id": "MDQ6VXNlcjQ4MjE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/482179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/izaid", "html_url": "https://github.com/izaid", "followers_url": "https://api.github.com/users/izaid/followers", "following_url": "https://api.github.com/users/izaid/following{/other_user}", "gists_url": "https://api.github.com/users/izaid/gists{/gist_id}", "starred_url": "https://api.github.com/users/izaid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/izaid/subscriptions", "organizations_url": "https://api.github.com/users/izaid/orgs", "repos_url": "https://api.github.com/users/izaid/repos", "events_url": "https://api.github.com/users/izaid/events{/privacy}", "received_events_url": "https://api.github.com/users/izaid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-02-27T13:06:16Z", "updated_at": "2018-03-07T01:20:23Z", "closed_at": "2018-03-07T00:53:58Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n</ul>\n<p>No, anything discussed here is valid with the cifar10 model script -- <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.p\">https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.p</a></p>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n</ul>\n<p>Ubuntu 16.04, but should be platform independent</p>\n<ul>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n</ul>\n<p>Binary</p>\n<ul>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n</ul>\n<p>1.5</p>\n<ul>\n<li><strong>Python version</strong>:</li>\n</ul>\n<p>3.5</p>\n<ul>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n</ul>\n<p>N / A</p>\n<ul>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n</ul>\n<p>N / A</p>\n<ul>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n</ul>\n<p>7</p>\n<ul>\n<li>\n<p><strong>GPU model and memory</strong>:</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><code>MonitoredTrainingSession</code> is the simplest and, to my knowledge, often recommended way to run a training session that can easily be saved and restored. I think there is some confusion here that could be cleared up by fixing two things:</p>\n<ol>\n<li><code>MonitoredTrainingSession</code> does not restore the global step.</li>\n</ol>\n<p>If one creates a step with <code>tf.train.get_or_create_global_step()</code>, and even if this is passed into the optimizer, <code>MonitoredTrainingSession</code> will not restore the correct value in a new session. Instead, it starts it again from 0. This has been mentioned several times, here are some references:</p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"193432276\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6081\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6081/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6081\">#6081</a></li>\n<li><a href=\"https://stackoverflow.com/questions/36113090/how-to-get-the-global-step-when-restoring-checkpoints-in-tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/36113090/how-to-get-the-global-step-when-restoring-checkpoints-in-tensorflow</a></li>\n<li><a href=\"https://stackoverflow.com/questions/47932738/tensorflow-restoring-model-in-a-monitoredsession?rq=1\" rel=\"nofollow\">https://stackoverflow.com/questions/47932738/tensorflow-restoring-model-in-a-monitoredsession?rq=1</a></li>\n</ul>\n<p>The only solution that I've found works is to extract the step from the checkpoint filename manually, and add an operation to set it on restore, via something like</p>\n<p><code>step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])</code></p>\n<p>It would be great if this could be fixed so global steps are properly restored as well.</p>\n<ol start=\"2\">\n<li>Instead of <code>checkpoint_dir</code> as an argument, <code>MonitoredTrainingSession</code> should probably have a <code>save_checkpoint_dir</code> and a <code>restore_checkpoint_dir</code>.</li>\n</ol>\n<p>The reason why this is important is that TensorBoard does not support appending to summary files. This means that a restored session, to my knowledge, can only be viewed in TensorBoard if a new directory is used to write the new event files instead of the directory it was restored from. If that is not done, TensorBoard simply strips the events away and does not display them. It took me time to understand this was what was happening.</p>\n<p>Unfortunately, <code>MonitoredTrainingSession</code> does not support a different save directory at the moment, so the way to make <code>MonitoredTrainingSession</code> work with TensorBoard on restore is to create your own summary hooks. But this defeats part of what <code>MonitoredTrainingSession</code> is supposed to do for you.</p>\n<p>I believe this would be fixed with different save and restore directory, as specified above. A comment about how a different save directory is needed for TensorBoard visualization would also be helpful in the <code>MonitoredTrainingSession</code> documentation.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nNo, anything discussed here is valid with the cifar10 model script -- https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.p\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\nUbuntu 16.04, but should be platform independent\n\nTensorFlow installed from (source or binary):\n\nBinary\n\nTensorFlow version (use command below):\n\n1.5\n\nPython version:\n\n3.5\n\nBazel version (if compiling from source):\n\nN / A\n\nGCC/Compiler version (if compiling from source):\n\nN / A\n\nCUDA/cuDNN version:\n\n7\n\n\nGPU model and memory:\n\n\nExact command to reproduce:\n\n\nDescribe the problem\nMonitoredTrainingSession is the simplest and, to my knowledge, often recommended way to run a training session that can easily be saved and restored. I think there is some confusion here that could be cleared up by fixing two things:\n\nMonitoredTrainingSession does not restore the global step.\n\nIf one creates a step with tf.train.get_or_create_global_step(), and even if this is passed into the optimizer, MonitoredTrainingSession will not restore the correct value in a new session. Instead, it starts it again from 0. This has been mentioned several times, here are some references:\n\n#6081\nhttps://stackoverflow.com/questions/36113090/how-to-get-the-global-step-when-restoring-checkpoints-in-tensorflow\nhttps://stackoverflow.com/questions/47932738/tensorflow-restoring-model-in-a-monitoredsession?rq=1\n\nThe only solution that I've found works is to extract the step from the checkpoint filename manually, and add an operation to set it on restore, via something like\nstep = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])\nIt would be great if this could be fixed so global steps are properly restored as well.\n\nInstead of checkpoint_dir as an argument, MonitoredTrainingSession should probably have a save_checkpoint_dir and a restore_checkpoint_dir.\n\nThe reason why this is important is that TensorBoard does not support appending to summary files. This means that a restored session, to my knowledge, can only be viewed in TensorBoard if a new directory is used to write the new event files instead of the directory it was restored from. If that is not done, TensorBoard simply strips the events away and does not display them. It took me time to understand this was what was happening.\nUnfortunately, MonitoredTrainingSession does not support a different save directory at the moment, so the way to make MonitoredTrainingSession work with TensorBoard on restore is to create your own summary hooks. But this defeats part of what MonitoredTrainingSession is supposed to do for you.\nI believe this would be fixed with different save and restore directory, as specified above. A comment about how a different save directory is needed for TensorBoard visualization would also be helpful in the MonitoredTrainingSession documentation.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nNo, anything discussed here is valid with the cifar10 model script -- https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.p\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nUbuntu 16.04, but should be platform independent\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n1.5\r\n\r\n- **Python version**: \r\n\r\n3.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nN / A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nN / A\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\n7\r\n\r\n- **GPU model and memory**:\r\n\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n`MonitoredTrainingSession` is the simplest and, to my knowledge, often recommended way to run a training session that can easily be saved and restored. I think there is some confusion here that could be cleared up by fixing two things:\r\n\r\n1) `MonitoredTrainingSession` does not restore the global step.\r\n\r\nIf one creates a step with `tf.train.get_or_create_global_step()`, and even if this is passed into the optimizer, `MonitoredTrainingSession` will not restore the correct value in a new session. Instead, it starts it again from 0. This has been mentioned several times, here are some references:\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/6081\r\n- https://stackoverflow.com/questions/36113090/how-to-get-the-global-step-when-restoring-checkpoints-in-tensorflow\r\n- https://stackoverflow.com/questions/47932738/tensorflow-restoring-model-in-a-monitoredsession?rq=1\r\n\r\nThe only solution that I've found works is to extract the step from the checkpoint filename manually, and add an operation to set it on restore, via something like\r\n\r\n`step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])`\r\n\r\nIt would be great if this could be fixed so global steps are properly restored as well.\r\n\r\n2) Instead of `checkpoint_dir` as an argument, `MonitoredTrainingSession` should probably have a `save_checkpoint_dir` and a `restore_checkpoint_dir`.\r\n\r\nThe reason why this is important is that TensorBoard does not support appending to summary files. This means that a restored session, to my knowledge, can only be viewed in TensorBoard if a new directory is used to write the new event files instead of the directory it was restored from. If that is not done, TensorBoard simply strips the events away and does not display them. It took me time to understand this was what was happening.\r\n\r\nUnfortunately, `MonitoredTrainingSession` does not support a different save directory at the moment, so the way to make `MonitoredTrainingSession` work with TensorBoard on restore is to create your own summary hooks. But this defeats part of what `MonitoredTrainingSession` is supposed to do for you.\r\n\r\nI believe this would be fixed with different save and restore directory, as specified above. A comment about how a different save directory is needed for TensorBoard visualization would also be helpful in the `MonitoredTrainingSession` documentation."}