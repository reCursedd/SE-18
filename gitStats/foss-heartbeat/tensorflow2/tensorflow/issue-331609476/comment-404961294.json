{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404961294", "html_url": "https://github.com/tensorflow/tensorflow/issues/19944#issuecomment-404961294", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19944", "id": 404961294, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDk2MTI5NA==", "user": {"login": "tcmxx", "id": 6486152, "node_id": "MDQ6VXNlcjY0ODYxNTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/6486152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tcmxx", "html_url": "https://github.com/tcmxx", "followers_url": "https://api.github.com/users/tcmxx/followers", "following_url": "https://api.github.com/users/tcmxx/following{/other_user}", "gists_url": "https://api.github.com/users/tcmxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/tcmxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tcmxx/subscriptions", "organizations_url": "https://api.github.com/users/tcmxx/orgs", "repos_url": "https://api.github.com/users/tcmxx/repos", "events_url": "https://api.github.com/users/tcmxx/events{/privacy}", "received_events_url": "https://api.github.com/users/tcmxx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-13T21:41:00Z", "updated_at": "2018-07-13T21:41:00Z", "author_association": "NONE", "body_html": "<p>Just for anyone who is interested, I made a very simplified version to be added to array_grad.cc based on python implementation....sanity checks/optimizations are removed, not working for all cases....</p>\n<pre><code>\nstd::vector&lt;Output&gt; ExtractInputShapes(const Scope&amp; scope, const std::vector&lt;Output&gt;&amp; inputs){\n  //Extract the shapes of a set of input tensors.\n  bool fully_known = true;\n  std::vector&lt;Output&gt; sizes;\n  for(int i = 0; i &lt; inputs.size();++i){\n    auto input_shape = Shape(scope, inputs[i]);\n    //removed because I dont konw how to check the type in c++...Assume that fully_know is always true..\n    //if(dynamic_cast&lt;Const*&gt;((&amp;input_shape.op())) == nullptr || dynamic_cast&lt;Tensor*&gt;((&amp;input_shape)) == nullptr){\n    /*if(dynamic_cast&lt;Tensor*&gt;((&amp;input_shape)) == nullptr){\n      fully_known = false;\n      break;\n    }*/\n    sizes.push_back(input_shape);\n  }\n\n  if(fully_known){\n    return sizes;\n  }else{\n    auto result = ShapeN(scope, InputList(inputs)).output;\n    return result;\n  }\n\n}\n\nStatus ConcatGradHelper(const Scope&amp; scope, const Operation&amp; op,\n                 const std::vector&lt;Output&gt;&amp; grad_inputs,\n                 std::vector&lt;Output&gt;* grad_outputs,\n               const int start_value_index, const int end_value_index, const int dim_index) {\n  // Degenerate concatenation, just return grad.\n  if(op.num_inputs() == 2){\n    if(end_value_index &gt; dim_index){\n      grad_outputs-&gt;push_back(NoGradient());\n    }\n    for (int i = 0; i &lt; grad_inputs.size();++i){\n      grad_outputs-&gt;push_back(grad_inputs[i]);\n    }\n    if(end_value_index &lt;= dim_index){\n      grad_outputs-&gt;push_back(NoGradient());\n    }\n    return scope.status();\n  }\n\n  auto concat_dim = op.input(dim_index);\n  std::vector&lt;Output&gt; input_values;\n  for(int i = 0; i &lt; end_value_index - start_value_index;++i){\n    input_values.push_back(op.input(i+start_value_index));\n  }\n  std::vector&lt;Output&gt; out_grads;\n\n  //No support for sparse or eagerly executing\n  auto non_neg_concat_dim = FloorMod(scope, concat_dim, Rank(scope, input_values[0]));\n  auto sizes = ExtractInputShapes(scope, input_values);\n\n\n  //temperary use this implementation only...the python implementation uese two different ones for different input size for optimization....\n  auto stackedSized= Stack(scope,sizes,Stack::Axis(1));\n  std::vector&lt;Output&gt; sliceBegin = {non_neg_concat_dim.z, Const(scope, 0)};\n  auto sliceBeginStacked = Stack(scope,InputList(sliceBegin),Stack::Axis(0));\n  auto slicedSizes = Slice(scope,stackedSized, sliceBeginStacked,{1, -1});\n  auto squeezedSizes = Squeeze(scope,slicedSizes);\n  out_grads = SplitV(scope,grad_inputs[0],squeezedSizes,  non_neg_concat_dim.z, sizes.size()).output;\n\n  if(end_value_index &gt; dim_index){\n    grad_outputs-&gt;push_back(NoGradient());\n  }\n  for (int i = 0; i &lt; out_grads.size();++i){\n    grad_outputs-&gt;push_back(out_grads[i]);\n  }\n  if(end_value_index &lt;= dim_index){\n    grad_outputs-&gt;push_back(NoGradient());\n  }\n  return scope.status();\n}\n\nStatus ConcatGrad(const Scope&amp; scope, const Operation&amp; op,\n                    const std::vector&lt;Output&gt;&amp; grad_inputs,\n                    std::vector&lt;Output&gt;* grad_outputs){\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,1,op.num_inputs(),0);\n}\nREGISTER_GRADIENT_OP(\"Concat\", ConcatGrad);\n\nStatus ConcatGradV2(const Scope&amp; scope, const Operation&amp; op,\n                    const std::vector&lt;Output&gt;&amp; grad_inputs,\n                    std::vector&lt;Output&gt;* grad_outputs){\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,0,op.num_inputs()-1,op.num_inputs()-1);\n}\nREGISTER_GRADIENT_OP(\"ConcatV2\", ConcatGradV2);\n</code></pre>", "body_text": "Just for anyone who is interested, I made a very simplified version to be added to array_grad.cc based on python implementation....sanity checks/optimizations are removed, not working for all cases....\n\nstd::vector<Output> ExtractInputShapes(const Scope& scope, const std::vector<Output>& inputs){\n  //Extract the shapes of a set of input tensors.\n  bool fully_known = true;\n  std::vector<Output> sizes;\n  for(int i = 0; i < inputs.size();++i){\n    auto input_shape = Shape(scope, inputs[i]);\n    //removed because I dont konw how to check the type in c++...Assume that fully_know is always true..\n    //if(dynamic_cast<Const*>((&input_shape.op())) == nullptr || dynamic_cast<Tensor*>((&input_shape)) == nullptr){\n    /*if(dynamic_cast<Tensor*>((&input_shape)) == nullptr){\n      fully_known = false;\n      break;\n    }*/\n    sizes.push_back(input_shape);\n  }\n\n  if(fully_known){\n    return sizes;\n  }else{\n    auto result = ShapeN(scope, InputList(inputs)).output;\n    return result;\n  }\n\n}\n\nStatus ConcatGradHelper(const Scope& scope, const Operation& op,\n                 const std::vector<Output>& grad_inputs,\n                 std::vector<Output>* grad_outputs,\n               const int start_value_index, const int end_value_index, const int dim_index) {\n  // Degenerate concatenation, just return grad.\n  if(op.num_inputs() == 2){\n    if(end_value_index > dim_index){\n      grad_outputs->push_back(NoGradient());\n    }\n    for (int i = 0; i < grad_inputs.size();++i){\n      grad_outputs->push_back(grad_inputs[i]);\n    }\n    if(end_value_index <= dim_index){\n      grad_outputs->push_back(NoGradient());\n    }\n    return scope.status();\n  }\n\n  auto concat_dim = op.input(dim_index);\n  std::vector<Output> input_values;\n  for(int i = 0; i < end_value_index - start_value_index;++i){\n    input_values.push_back(op.input(i+start_value_index));\n  }\n  std::vector<Output> out_grads;\n\n  //No support for sparse or eagerly executing\n  auto non_neg_concat_dim = FloorMod(scope, concat_dim, Rank(scope, input_values[0]));\n  auto sizes = ExtractInputShapes(scope, input_values);\n\n\n  //temperary use this implementation only...the python implementation uese two different ones for different input size for optimization....\n  auto stackedSized= Stack(scope,sizes,Stack::Axis(1));\n  std::vector<Output> sliceBegin = {non_neg_concat_dim.z, Const(scope, 0)};\n  auto sliceBeginStacked = Stack(scope,InputList(sliceBegin),Stack::Axis(0));\n  auto slicedSizes = Slice(scope,stackedSized, sliceBeginStacked,{1, -1});\n  auto squeezedSizes = Squeeze(scope,slicedSizes);\n  out_grads = SplitV(scope,grad_inputs[0],squeezedSizes,  non_neg_concat_dim.z, sizes.size()).output;\n\n  if(end_value_index > dim_index){\n    grad_outputs->push_back(NoGradient());\n  }\n  for (int i = 0; i < out_grads.size();++i){\n    grad_outputs->push_back(out_grads[i]);\n  }\n  if(end_value_index <= dim_index){\n    grad_outputs->push_back(NoGradient());\n  }\n  return scope.status();\n}\n\nStatus ConcatGrad(const Scope& scope, const Operation& op,\n                    const std::vector<Output>& grad_inputs,\n                    std::vector<Output>* grad_outputs){\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,1,op.num_inputs(),0);\n}\nREGISTER_GRADIENT_OP(\"Concat\", ConcatGrad);\n\nStatus ConcatGradV2(const Scope& scope, const Operation& op,\n                    const std::vector<Output>& grad_inputs,\n                    std::vector<Output>* grad_outputs){\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,0,op.num_inputs()-1,op.num_inputs()-1);\n}\nREGISTER_GRADIENT_OP(\"ConcatV2\", ConcatGradV2);", "body": "Just for anyone who is interested, I made a very simplified version to be added to array_grad.cc based on python implementation....sanity checks/optimizations are removed, not working for all cases....\r\n\r\n\r\n```\r\n\r\nstd::vector<Output> ExtractInputShapes(const Scope& scope, const std::vector<Output>& inputs){\r\n  //Extract the shapes of a set of input tensors.\r\n  bool fully_known = true;\r\n  std::vector<Output> sizes;\r\n  for(int i = 0; i < inputs.size();++i){\r\n    auto input_shape = Shape(scope, inputs[i]);\r\n    //removed because I dont konw how to check the type in c++...Assume that fully_know is always true..\r\n    //if(dynamic_cast<Const*>((&input_shape.op())) == nullptr || dynamic_cast<Tensor*>((&input_shape)) == nullptr){\r\n    /*if(dynamic_cast<Tensor*>((&input_shape)) == nullptr){\r\n      fully_known = false;\r\n      break;\r\n    }*/\r\n    sizes.push_back(input_shape);\r\n  }\r\n\r\n  if(fully_known){\r\n    return sizes;\r\n  }else{\r\n    auto result = ShapeN(scope, InputList(inputs)).output;\r\n    return result;\r\n  }\r\n\r\n}\r\n\r\nStatus ConcatGradHelper(const Scope& scope, const Operation& op,\r\n                 const std::vector<Output>& grad_inputs,\r\n                 std::vector<Output>* grad_outputs,\r\n               const int start_value_index, const int end_value_index, const int dim_index) {\r\n  // Degenerate concatenation, just return grad.\r\n  if(op.num_inputs() == 2){\r\n    if(end_value_index > dim_index){\r\n      grad_outputs->push_back(NoGradient());\r\n    }\r\n    for (int i = 0; i < grad_inputs.size();++i){\r\n      grad_outputs->push_back(grad_inputs[i]);\r\n    }\r\n    if(end_value_index <= dim_index){\r\n      grad_outputs->push_back(NoGradient());\r\n    }\r\n    return scope.status();\r\n  }\r\n\r\n  auto concat_dim = op.input(dim_index);\r\n  std::vector<Output> input_values;\r\n  for(int i = 0; i < end_value_index - start_value_index;++i){\r\n    input_values.push_back(op.input(i+start_value_index));\r\n  }\r\n  std::vector<Output> out_grads;\r\n\r\n  //No support for sparse or eagerly executing\r\n  auto non_neg_concat_dim = FloorMod(scope, concat_dim, Rank(scope, input_values[0]));\r\n  auto sizes = ExtractInputShapes(scope, input_values);\r\n\r\n\r\n  //temperary use this implementation only...the python implementation uese two different ones for different input size for optimization....\r\n  auto stackedSized= Stack(scope,sizes,Stack::Axis(1));\r\n  std::vector<Output> sliceBegin = {non_neg_concat_dim.z, Const(scope, 0)};\r\n  auto sliceBeginStacked = Stack(scope,InputList(sliceBegin),Stack::Axis(0));\r\n  auto slicedSizes = Slice(scope,stackedSized, sliceBeginStacked,{1, -1});\r\n  auto squeezedSizes = Squeeze(scope,slicedSizes);\r\n  out_grads = SplitV(scope,grad_inputs[0],squeezedSizes,  non_neg_concat_dim.z, sizes.size()).output;\r\n\r\n  if(end_value_index > dim_index){\r\n    grad_outputs->push_back(NoGradient());\r\n  }\r\n  for (int i = 0; i < out_grads.size();++i){\r\n    grad_outputs->push_back(out_grads[i]);\r\n  }\r\n  if(end_value_index <= dim_index){\r\n    grad_outputs->push_back(NoGradient());\r\n  }\r\n  return scope.status();\r\n}\r\n\r\nStatus ConcatGrad(const Scope& scope, const Operation& op,\r\n                    const std::vector<Output>& grad_inputs,\r\n                    std::vector<Output>* grad_outputs){\r\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,1,op.num_inputs(),0);\r\n}\r\nREGISTER_GRADIENT_OP(\"Concat\", ConcatGrad);\r\n\r\nStatus ConcatGradV2(const Scope& scope, const Operation& op,\r\n                    const std::vector<Output>& grad_inputs,\r\n                    std::vector<Output>* grad_outputs){\r\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,0,op.num_inputs()-1,op.num_inputs()-1);\r\n}\r\nREGISTER_GRADIENT_OP(\"ConcatV2\", ConcatGradV2);\r\n```"}