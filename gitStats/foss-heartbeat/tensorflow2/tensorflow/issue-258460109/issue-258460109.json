{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13124", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13124/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13124/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13124/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13124", "id": 258460109, "node_id": "MDU6SXNzdWUyNTg0NjAxMDk=", "number": 13124, "title": "tensorflow.python.debug.cli.offline_analyzer failed to read debug data from HDFS filesys", "user": {"login": "luchensk", "id": 28526467, "node_id": "MDQ6VXNlcjI4NTI2NDY3", "avatar_url": "https://avatars1.githubusercontent.com/u/28526467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luchensk", "html_url": "https://github.com/luchensk", "followers_url": "https://api.github.com/users/luchensk/followers", "following_url": "https://api.github.com/users/luchensk/following{/other_user}", "gists_url": "https://api.github.com/users/luchensk/gists{/gist_id}", "starred_url": "https://api.github.com/users/luchensk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luchensk/subscriptions", "organizations_url": "https://api.github.com/users/luchensk/orgs", "repos_url": "https://api.github.com/users/luchensk/repos", "events_url": "https://api.github.com/users/luchensk/events{/privacy}", "received_events_url": "https://api.github.com/users/luchensk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-09-18T12:21:45Z", "updated_at": "2018-02-06T18:52:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04.2</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0 from master branch</li>\n<li><strong>Python version</strong>: Python 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: null</li>\n<li><strong>GPU model and memory</strong>: null</li>\n<li><strong>Exact command to reproduce</strong>: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://&lt;debug_data_dir&gt;</li>\n</ul>\n<h3>Issue description</h3>\n<p>I saved debug data by <code>DumpingDebugHook</code> into hdfs filesys and then it failed to read the data by <code>python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://&lt;debug_data_path&gt;</code> with the error \"not a valid DFS filename\" as Invalid argument. But it works well for the local filesys by the same way.</p>\n<h4>Error info:</h4>\n<pre><code># python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\ntfdbg offline: FLAGS.dump_dir = hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n^@hdfsOpenFile(/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:\njava.lang.IllegalArgumentException: Pathname /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 from /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 is not a valid DFS filename.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:197)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 78, in &lt;module&gt;\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 41, in main\n    FLAGS.dump_dir, validate=FLAGS.validate_graph)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 692, in __init__\n    self._load_all_device_dumps(partition_graphs, validate)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 714, in _load_all_device_dumps\n    self._load_partition_graphs(partition_graphs, validate)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 983, in _load_partition_graphs\n    self._dump_graph_file_paths[device_name])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 145, in _load_graph_def_from_event_file\n    event.ParseFromString(f.read())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 119, in read\n    self._preread_check()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 79, in _preread_check\n    compat.as_bytes(self.__name), 1024 * 512, status)\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    c_api.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244; Invalid argument\n</code></pre>\n<p>In fact, it was able to read the hdfs dir info, as below:</p>\n<pre><code>$hdfs dfs -ls hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0\nFound 7 items\n-rw-r--r--   3 root supergroup     141732 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/gradients\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_1\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_2\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_out\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/loss\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/train\n</code></pre>\n<p>After that, I tried to change the name of the dir as above from <code>_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0</code> to <code>_tfdbg_device</code>, and then it was able to load the dir into CLI UI for debug, but there was nothing about debug info to show, as below:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/28526467/30541641-dc4329d2-9cae-11e7-8e07-c84dd1a3a7a3.png\"><img src=\"https://user-images.githubusercontent.com/28526467/30541641-dc4329d2-9cae-11e7-8e07-c84dd1a3a7a3.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.2\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3.0 from master branch\nPython version: Python 2.7.12\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: null\nGPU model and memory: null\nExact command to reproduce: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>\n\nIssue description\nI saved debug data by DumpingDebugHook into hdfs filesys and then it failed to read the data by python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path> with the error \"not a valid DFS filename\" as Invalid argument. But it works well for the local filesys by the same way.\nError info:\n# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\ntfdbg offline: FLAGS.dump_dir = hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n^@hdfsOpenFile(/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:\njava.lang.IllegalArgumentException: Pathname /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 from /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 is not a valid DFS filename.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:197)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 78, in <module>\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 41, in main\n    FLAGS.dump_dir, validate=FLAGS.validate_graph)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 692, in __init__\n    self._load_all_device_dumps(partition_graphs, validate)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 714, in _load_all_device_dumps\n    self._load_partition_graphs(partition_graphs, validate)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 983, in _load_partition_graphs\n    self._dump_graph_file_paths[device_name])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 145, in _load_graph_def_from_event_file\n    event.ParseFromString(f.read())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 119, in read\n    self._preread_check()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 79, in _preread_check\n    compat.as_bytes(self.__name), 1024 * 512, status)\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    c_api.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244; Invalid argument\n\nIn fact, it was able to read the hdfs dir info, as below:\n$hdfs dfs -ls hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0\nFound 7 items\n-rw-r--r--   3 root supergroup     141732 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/gradients\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_1\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_2\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_out\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/loss\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/train\n\nAfter that, I tried to change the name of the dir as above from _tfdbg_device_,job_localhost,replica_0,task_0,cpu_0 to _tfdbg_device, and then it was able to load the dir into CLI UI for debug, but there was nothing about debug info to show, as below:", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0 from master branch\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: null\r\n- **GPU model and memory**: null\r\n- **Exact command to reproduce**: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>\r\n\r\n### Issue description\r\nI saved debug data by `DumpingDebugHook` into hdfs filesys and then it failed to read the data by `python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path>` with the error \"not a valid DFS filename\" as Invalid argument. But it works well for the local filesys by the same way.\r\n\r\n#### Error info:\r\n```\r\n# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\r\ntfdbg offline: FLAGS.dump_dir = hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\n^@hdfsOpenFile(/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:\r\njava.lang.IllegalArgumentException: Pathname /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 from /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 is not a valid DFS filename.\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:197)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 78, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 41, in main\r\n    FLAGS.dump_dir, validate=FLAGS.validate_graph)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 692, in __init__\r\n    self._load_all_device_dumps(partition_graphs, validate)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 714, in _load_all_device_dumps\r\n    self._load_partition_graphs(partition_graphs, validate)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 983, in _load_partition_graphs\r\n    self._dump_graph_file_paths[device_name])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 145, in _load_graph_def_from_event_file\r\n    event.ParseFromString(f.read())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 119, in read\r\n    self._preread_check()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 79, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244; Invalid argument\r\n```\r\n\r\nIn fact, it was able to read the hdfs dir info, as below:\r\n\r\n```\r\n$hdfs dfs -ls hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0\r\nFound 7 items\r\n-rw-r--r--   3 root supergroup     141732 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/gradients\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_1\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_2\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_out\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/loss\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/train\r\n```\r\n\r\nAfter that, I tried to change the name of the dir as above from `_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0` to `_tfdbg_device`, and then it was able to load the dir into CLI UI for debug, but there was nothing about debug info to show, as below:\r\n\r\n![image](https://user-images.githubusercontent.com/28526467/30541641-dc4329d2-9cae-11e7-8e07-c84dd1a3a7a3.png)\r\n"}