{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23438", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23438/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23438/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23438/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23438", "id": 376590080, "node_id": "MDU6SXNzdWUzNzY1OTAwODA=", "number": 23438, "title": "LSTM Cell Training Error", "user": {"login": "ilhan90", "id": 29962152, "node_id": "MDQ6VXNlcjI5OTYyMTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/29962152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilhan90", "html_url": "https://github.com/ilhan90", "followers_url": "https://api.github.com/users/ilhan90/followers", "following_url": "https://api.github.com/users/ilhan90/following{/other_user}", "gists_url": "https://api.github.com/users/ilhan90/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilhan90/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilhan90/subscriptions", "organizations_url": "https://api.github.com/users/ilhan90/orgs", "repos_url": "https://api.github.com/users/ilhan90/repos", "events_url": "https://api.github.com/users/ilhan90/events{/privacy}", "received_events_url": "https://api.github.com/users/ilhan90/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-01T22:07:50Z", "updated_at": "2018-11-07T17:32:21Z", "closed_at": "2018-11-07T17:27:25Z", "author_association": "NONE", "body_html": "<p><em>I have a problem with multi layer lstm training. The accuracy and output model slowly changing when calling \"sess.run(logits, feed_dict={X:...})\" during train process. The problem doesn't happens when I decreased number of layers or neurons or dataset size.</em></p><em>\n<p><strong>System information</strong></p>\n<ul>\n<li>Google colab selecting gpu</li>\n<li>Python version 3</li>\n</ul>\n<pre><code>n_epochs = 6\nbatch_size = 150\n\nn_layers = 3\nn_neurons = 300\nn_outputs = 2\nlearning_rate = 0.001\n\n\nreset_graph()\n\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.int32, [None])\ndevices = [\"/gpu:0\"] # replace with [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"]\nlstm_cells = [DeviceCellWrapper(dev,tf.contrib.rnn.LSTMCell(num_units=n_neurons))\n       for layer in range(n_layers)\n       for dev in devices]\nmulti_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)      \noutputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\ntop_layer_h_state = states[-1][1]\nlogits = tf.layers.dense(top_layer_h_state, n_outputs, name=\"softmax\")\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\nloss = tf.reduce_mean(xentropy, name=\"loss\")\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(loss)\ncorrect = tf.nn.in_top_k(logits, y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\ninit = tf.global_variables_initializer()\n\n\nsaver = tf.train.Saver()\n\n\nwith tf.Session() as sess:\n    init.run()\n    maxValidationPoint = float('-inf')\n\n    for epoch in range(n_epochs):\n        trainDataIndexInPtr = 0\n        while(True):\n            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n            if(trainDataIndexInPtr == 0):\n                break\n\n        acc_train = accuracy.eval(feed_dict={X: X_train.reshape((len(X_train), n_steps, n_inputs)), y: y_train.reshape((len(y_train)))})\n        acc_validation = accuracy.eval(feed_dict={X: X_validation, y: y_validation})\n\n        y_pred = sess.run(logits, feed_dict={X: X_validation})\n        y_pred2 = sess.run(logits, feed_dict={X: X_test})\n\n        print(\"Train accuracy =\", \"%.7f\" % acc_train, \"Validation accuracy =\", \"%.7f\" % acc_validation)\n\n        if(acc_validation &gt; maxAcc_validation ):\n            maxAcc_validation = acc_validation\n            saver.save(sess, \"./Model\")\n</code></pre>\n<h3>Output:</h3>\n<p>Train accuracy = 0.6400015 Validation accuracy = 0.6296040<br>\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070<br>\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685<br>\nTrain accuracy = 0.6643262 Validation accuracy = 0.6265539<br>\nTrain accuracy = 0.6804933 Validation accuracy = 0.6232543<br>\nTrain accuracy = 0.7023602 Validation accuracy = 0.6231584</p>\n<h3>if I throw away \"y_pred = sess.run(logits, feed_dict={X: X_validation})\" and \"y_pred2 = sess.run(logits, feed_dict={X: X_test})\", Output:</h3>\n<p>Train accuracy = 0.6400015 Validation accuracy = 0.6296040<br>\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070<br>\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685<br>\nTrain accuracy = 0.6643139 Validation accuracy = 0.6265347<br>\n<strong>Train accuracy = 0.6794249 Validation accuracy = 0.6231584<br>\nTrain accuracy = 0.7062445 Validation accuracy = 0.6132405</strong></p></em>", "body_text": "I have a problem with multi layer lstm training. The accuracy and output model slowly changing when calling \"sess.run(logits, feed_dict={X:...})\" during train process. The problem doesn't happens when I decreased number of layers or neurons or dataset size.\nSystem information\n\nGoogle colab selecting gpu\nPython version 3\n\nn_epochs = 6\nbatch_size = 150\n\nn_layers = 3\nn_neurons = 300\nn_outputs = 2\nlearning_rate = 0.001\n\n\nreset_graph()\n\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.int32, [None])\ndevices = [\"/gpu:0\"] # replace with [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"]\nlstm_cells = [DeviceCellWrapper(dev,tf.contrib.rnn.LSTMCell(num_units=n_neurons))\n       for layer in range(n_layers)\n       for dev in devices]\nmulti_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)      \noutputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\ntop_layer_h_state = states[-1][1]\nlogits = tf.layers.dense(top_layer_h_state, n_outputs, name=\"softmax\")\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\nloss = tf.reduce_mean(xentropy, name=\"loss\")\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(loss)\ncorrect = tf.nn.in_top_k(logits, y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\ninit = tf.global_variables_initializer()\n\n\nsaver = tf.train.Saver()\n\n\nwith tf.Session() as sess:\n    init.run()\n    maxValidationPoint = float('-inf')\n\n    for epoch in range(n_epochs):\n        trainDataIndexInPtr = 0\n        while(True):\n            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n            if(trainDataIndexInPtr == 0):\n                break\n\n        acc_train = accuracy.eval(feed_dict={X: X_train.reshape((len(X_train), n_steps, n_inputs)), y: y_train.reshape((len(y_train)))})\n        acc_validation = accuracy.eval(feed_dict={X: X_validation, y: y_validation})\n\n        y_pred = sess.run(logits, feed_dict={X: X_validation})\n        y_pred2 = sess.run(logits, feed_dict={X: X_test})\n\n        print(\"Train accuracy =\", \"%.7f\" % acc_train, \"Validation accuracy =\", \"%.7f\" % acc_validation)\n\n        if(acc_validation > maxAcc_validation ):\n            maxAcc_validation = acc_validation\n            saver.save(sess, \"./Model\")\n\nOutput:\nTrain accuracy = 0.6400015 Validation accuracy = 0.6296040\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685\nTrain accuracy = 0.6643262 Validation accuracy = 0.6265539\nTrain accuracy = 0.6804933 Validation accuracy = 0.6232543\nTrain accuracy = 0.7023602 Validation accuracy = 0.6231584\nif I throw away \"y_pred = sess.run(logits, feed_dict={X: X_validation})\" and \"y_pred2 = sess.run(logits, feed_dict={X: X_test})\", Output:\nTrain accuracy = 0.6400015 Validation accuracy = 0.6296040\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685\nTrain accuracy = 0.6643139 Validation accuracy = 0.6265347\nTrain accuracy = 0.6794249 Validation accuracy = 0.6231584\nTrain accuracy = 0.7062445 Validation accuracy = 0.6132405", "body": "<em>I have a problem with multi layer lstm training. The accuracy and output model slowly changing when calling \"sess.run(logits, feed_dict={X:...})\" during train process. The problem doesn't happens when I decreased number of layers or neurons or dataset size.\r\n\r\n**System information**\r\n- Google colab selecting gpu\r\n- Python version 3\r\n\r\n\r\n\r\n\r\n```\r\nn_epochs = 6\r\nbatch_size = 150\r\n\r\nn_layers = 3\r\nn_neurons = 300\r\nn_outputs = 2\r\nlearning_rate = 0.001\r\n\r\n\r\nreset_graph()\r\n\r\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\r\ny = tf.placeholder(tf.int32, [None])\r\ndevices = [\"/gpu:0\"] # replace with [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"]\r\nlstm_cells = [DeviceCellWrapper(dev,tf.contrib.rnn.LSTMCell(num_units=n_neurons))\r\n       for layer in range(n_layers)\r\n       for dev in devices]\r\nmulti_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)      \r\noutputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\r\ntop_layer_h_state = states[-1][1]\r\nlogits = tf.layers.dense(top_layer_h_state, n_outputs, name=\"softmax\")\r\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\r\nloss = tf.reduce_mean(xentropy, name=\"loss\")\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\ntraining_op = optimizer.minimize(loss)\r\ncorrect = tf.nn.in_top_k(logits, y, 1)\r\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n\r\nsaver = tf.train.Saver()\r\n\r\n\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    maxValidationPoint = float('-inf')\r\n\r\n    for epoch in range(n_epochs):\r\n        trainDataIndexInPtr = 0\r\n        while(True):\r\n            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\r\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\r\n            if(trainDataIndexInPtr == 0):\r\n                break\r\n\r\n        acc_train = accuracy.eval(feed_dict={X: X_train.reshape((len(X_train), n_steps, n_inputs)), y: y_train.reshape((len(y_train)))})\r\n        acc_validation = accuracy.eval(feed_dict={X: X_validation, y: y_validation})\r\n\r\n        y_pred = sess.run(logits, feed_dict={X: X_validation})\r\n        y_pred2 = sess.run(logits, feed_dict={X: X_test})\r\n\r\n        print(\"Train accuracy =\", \"%.7f\" % acc_train, \"Validation accuracy =\", \"%.7f\" % acc_validation)\r\n\r\n        if(acc_validation > maxAcc_validation ):\r\n            maxAcc_validation = acc_validation\r\n            saver.save(sess, \"./Model\")\r\n```\r\n\r\n### Output:\r\n\r\nTrain accuracy = 0.6400015 Validation accuracy = 0.6296040\r\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070\r\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685\r\nTrain accuracy = 0.6643262 Validation accuracy = 0.6265539\r\nTrain accuracy = 0.6804933 Validation accuracy = 0.6232543\r\nTrain accuracy = 0.7023602 Validation accuracy = 0.6231584\r\n\r\n### if I throw away \"y_pred = sess.run(logits, feed_dict={X: X_validation})\" and \"y_pred2 = sess.run(logits, feed_dict={X: X_test})\", Output:\r\n\r\nTrain accuracy = 0.6400015 Validation accuracy = 0.6296040\r\nTrain accuracy = 0.6409340 Validation accuracy = 0.6253070\r\nTrain accuracy = 0.6552114 Validation accuracy = 0.6393685\r\nTrain accuracy = 0.6643139 Validation accuracy = 0.6265347\r\n**Train accuracy = 0.6794249 Validation accuracy = 0.6231584\r\nTrain accuracy = 0.7062445 Validation accuracy = 0.6132405**\r\n"}