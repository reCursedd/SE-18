{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6117", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6117/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6117/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6117/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6117", "id": 193734095, "node_id": "MDU6SXNzdWUxOTM3MzQwOTU=", "number": 6117, "title": "distributed tensorflow failed to save variable larger than 2G", "user": {"login": "bluekingsong", "id": 2830940, "node_id": "MDQ6VXNlcjI4MzA5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/2830940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bluekingsong", "html_url": "https://github.com/bluekingsong", "followers_url": "https://api.github.com/users/bluekingsong/followers", "following_url": "https://api.github.com/users/bluekingsong/following{/other_user}", "gists_url": "https://api.github.com/users/bluekingsong/gists{/gist_id}", "starred_url": "https://api.github.com/users/bluekingsong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bluekingsong/subscriptions", "organizations_url": "https://api.github.com/users/bluekingsong/orgs", "repos_url": "https://api.github.com/users/bluekingsong/repos", "events_url": "https://api.github.com/users/bluekingsong/events{/privacy}", "received_events_url": "https://api.github.com/users/bluekingsong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-12-06T10:22:54Z", "updated_at": "2016-12-12T12:01:02Z", "closed_at": "2016-12-07T06:42:59Z", "author_association": "NONE", "body_html": "<p>when use distributed tensorflow saver to save larger variables(more than 2G),  it get stuck and won't save successfully.<br>\ntensorflow version: rc0.11<br>\nbazel version: 0.3.<br>\nminimal reproducible example:</p>\n<h2>file1: test_saver.py</h2>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom datetime import datetime\n\nimport numpy\nimport tensorflow as tf\n\nimport sys\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\ntf.app.flags.DEFINE_string('job_name', '', 'One of \"ps\", \"worker\"')\n\ndef run_training(target, cluster_spec):\n  with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d/%s\" % (0, 'cpu:0'),\n        cluster=cluster_spec)):\n    with tf.name_scope('test'):\n        test_weight1 = tf.get_variable(\"test_weight1\", [24500000, 22],  # bigger then 2G\n                    initializer=tf.random_normal_initializer())\n        test_weight2 = tf.get_variable(\"test_weight2\", [24500, 22], # smaller then 2G\n                    initializer=tf.random_normal_initializer())\n\n    saver1 = tf.train.Saver([test_weight1 ], write_version=tf.train.SaverDef.V2)\n#    saver2 = tf.train.Saver([test_weight2], write_version=tf.train.SaverDef.V2)\n\n    # The op for initializing the variables.\n    init_op = tf.group(tf.initialize_all_variables(),\n                       tf.initialize_local_variables())\n    sv = tf.train.Supervisor(is_chief=True,\n                             logdir=\"log\",\n                             init_op=init_op,\n                             summary_op=None,\n                             saver=saver1,\n#                             saver=saver2,\n                             )\n    # Get a session.\n    sess = sv.prepare_or_wait_for_session(target)\n\n    # Start the queue runners.\n    queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\n    sv.start_queue_runners(sess, queue_runners)\n\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    step = 1\n    print('start to train.')\n    sys.stdout.flush()\n    while not coord.should_stop():\n      w1 = sess.run([test_weight2])\n      print('start to save checkpoint, time=%s'%datetime.now())\n      sys.stdout.flush()\n      checkpoint_path = 'model.ckpt'\n      saver1.save(sess, checkpoint_path, global_step=1)    # saver1 get stuck with high cpu, please refer to line 29 and 39\n      #saver2.save(sess, checkpoint_path, global_step=1)   # saver2 can save smoothly, please refer to line 30 and 40\n      print('after save checkpoint, time=%s'%datetime.now())\n      sys.stdout.flush()\n      step += 1\n    sess.close()\n\ndef main(_):\n  assert FLAGS.job_name in ['ps', 'worker'], 'job_name must be ps or worker'\n\n  ps_hosts = ['127.0.0.1:3721',]\n  worker_hosts = ['127.0.0.1:3722',]\n  cluster_spec = tf.train.ClusterSpec({'ps': ps_hosts, 'worker': worker_hosts})\n  server = tf.train.Server(\n      {'ps': ps_hosts,\n       'worker': worker_hosts},\n      job_name=FLAGS.job_name,\n      task_index=0)   # test for one ps and one worker\n\n  if FLAGS.job_name == 'ps':\n    # `ps` jobs wait for incoming connections from the workers.\n    server.join()\n  else:\n    run_training(server.target, cluster_spec)\n\nif __name__ == '__main__':\n  tf.app.run()\n</code></pre>\n<hr>\n<h2>file2: run_test_saver.sh</h2>\n<pre><code>#!/bin/bash\n\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=ps &gt; ps.out&amp;\n\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=worker &gt; worker.out &amp;\n</code></pre>\n<hr>\n<p>PS:  both saver1 and saver2 can work correctly when use standalone tensorflow(i.e  without parameter server)</p>", "body_text": "when use distributed tensorflow saver to save larger variables(more than 2G),  it get stuck and won't save successfully.\ntensorflow version: rc0.11\nbazel version: 0.3.\nminimal reproducible example:\nfile1: test_saver.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom datetime import datetime\n\nimport numpy\nimport tensorflow as tf\n\nimport sys\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\ntf.app.flags.DEFINE_string('job_name', '', 'One of \"ps\", \"worker\"')\n\ndef run_training(target, cluster_spec):\n  with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d/%s\" % (0, 'cpu:0'),\n        cluster=cluster_spec)):\n    with tf.name_scope('test'):\n        test_weight1 = tf.get_variable(\"test_weight1\", [24500000, 22],  # bigger then 2G\n                    initializer=tf.random_normal_initializer())\n        test_weight2 = tf.get_variable(\"test_weight2\", [24500, 22], # smaller then 2G\n                    initializer=tf.random_normal_initializer())\n\n    saver1 = tf.train.Saver([test_weight1 ], write_version=tf.train.SaverDef.V2)\n#    saver2 = tf.train.Saver([test_weight2], write_version=tf.train.SaverDef.V2)\n\n    # The op for initializing the variables.\n    init_op = tf.group(tf.initialize_all_variables(),\n                       tf.initialize_local_variables())\n    sv = tf.train.Supervisor(is_chief=True,\n                             logdir=\"log\",\n                             init_op=init_op,\n                             summary_op=None,\n                             saver=saver1,\n#                             saver=saver2,\n                             )\n    # Get a session.\n    sess = sv.prepare_or_wait_for_session(target)\n\n    # Start the queue runners.\n    queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\n    sv.start_queue_runners(sess, queue_runners)\n\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    step = 1\n    print('start to train.')\n    sys.stdout.flush()\n    while not coord.should_stop():\n      w1 = sess.run([test_weight2])\n      print('start to save checkpoint, time=%s'%datetime.now())\n      sys.stdout.flush()\n      checkpoint_path = 'model.ckpt'\n      saver1.save(sess, checkpoint_path, global_step=1)    # saver1 get stuck with high cpu, please refer to line 29 and 39\n      #saver2.save(sess, checkpoint_path, global_step=1)   # saver2 can save smoothly, please refer to line 30 and 40\n      print('after save checkpoint, time=%s'%datetime.now())\n      sys.stdout.flush()\n      step += 1\n    sess.close()\n\ndef main(_):\n  assert FLAGS.job_name in ['ps', 'worker'], 'job_name must be ps or worker'\n\n  ps_hosts = ['127.0.0.1:3721',]\n  worker_hosts = ['127.0.0.1:3722',]\n  cluster_spec = tf.train.ClusterSpec({'ps': ps_hosts, 'worker': worker_hosts})\n  server = tf.train.Server(\n      {'ps': ps_hosts,\n       'worker': worker_hosts},\n      job_name=FLAGS.job_name,\n      task_index=0)   # test for one ps and one worker\n\n  if FLAGS.job_name == 'ps':\n    # `ps` jobs wait for incoming connections from the workers.\n    server.join()\n  else:\n    run_training(server.target, cluster_spec)\n\nif __name__ == '__main__':\n  tf.app.run()\n\n\nfile2: run_test_saver.sh\n#!/bin/bash\n\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=ps > ps.out&\n\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=worker > worker.out &\n\n\nPS:  both saver1 and saver2 can work correctly when use standalone tensorflow(i.e  without parameter server)", "body": "  when use distributed tensorflow saver to save larger variables(more than 2G),  it get stuck and won't save successfully.\r\n   tensorflow version: rc0.11\r\n   bazel version: 0.3.\r\n  minimal reproducible example:\r\n\r\nfile1: test_saver.py\r\n------------------------------------------\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport time\r\nfrom datetime import datetime\r\n\r\nimport numpy\r\nimport tensorflow as tf\r\n\r\nimport sys\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\ntf.app.flags.DEFINE_string('job_name', '', 'One of \"ps\", \"worker\"')\r\n\r\ndef run_training(target, cluster_spec):\r\n  with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d/%s\" % (0, 'cpu:0'),\r\n        cluster=cluster_spec)):\r\n    with tf.name_scope('test'):\r\n        test_weight1 = tf.get_variable(\"test_weight1\", [24500000, 22],  # bigger then 2G\r\n                    initializer=tf.random_normal_initializer())\r\n        test_weight2 = tf.get_variable(\"test_weight2\", [24500, 22], # smaller then 2G\r\n                    initializer=tf.random_normal_initializer())\r\n\r\n    saver1 = tf.train.Saver([test_weight1 ], write_version=tf.train.SaverDef.V2)\r\n#    saver2 = tf.train.Saver([test_weight2], write_version=tf.train.SaverDef.V2)\r\n\r\n    # The op for initializing the variables.\r\n    init_op = tf.group(tf.initialize_all_variables(),\r\n                       tf.initialize_local_variables())\r\n    sv = tf.train.Supervisor(is_chief=True,\r\n                             logdir=\"log\",\r\n                             init_op=init_op,\r\n                             summary_op=None,\r\n                             saver=saver1,\r\n#                             saver=saver2,\r\n                             )\r\n    # Get a session.\r\n    sess = sv.prepare_or_wait_for_session(target)\r\n\r\n    # Start the queue runners.\r\n    queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\r\n    sv.start_queue_runners(sess, queue_runners)\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    step = 1\r\n    print('start to train.')\r\n    sys.stdout.flush()\r\n    while not coord.should_stop():\r\n      w1 = sess.run([test_weight2])\r\n      print('start to save checkpoint, time=%s'%datetime.now())\r\n      sys.stdout.flush()\r\n      checkpoint_path = 'model.ckpt'\r\n      saver1.save(sess, checkpoint_path, global_step=1)    # saver1 get stuck with high cpu, please refer to line 29 and 39\r\n      #saver2.save(sess, checkpoint_path, global_step=1)   # saver2 can save smoothly, please refer to line 30 and 40\r\n      print('after save checkpoint, time=%s'%datetime.now())\r\n      sys.stdout.flush()\r\n      step += 1\r\n    sess.close()\r\n\r\ndef main(_):\r\n  assert FLAGS.job_name in ['ps', 'worker'], 'job_name must be ps or worker'\r\n\r\n  ps_hosts = ['127.0.0.1:3721',]\r\n  worker_hosts = ['127.0.0.1:3722',]\r\n  cluster_spec = tf.train.ClusterSpec({'ps': ps_hosts, 'worker': worker_hosts})\r\n  server = tf.train.Server(\r\n      {'ps': ps_hosts,\r\n       'worker': worker_hosts},\r\n      job_name=FLAGS.job_name,\r\n      task_index=0)   # test for one ps and one worker\r\n\r\n  if FLAGS.job_name == 'ps':\r\n    # `ps` jobs wait for incoming connections from the workers.\r\n    server.join()\r\n  else:\r\n    run_training(server.target, cluster_spec)\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n-------------------------------------------------------------------------------------\r\nfile2: run_test_saver.sh\r\n-------------------------------------------------------------------------------------\r\n```\r\n#!/bin/bash\r\n\r\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=ps > ps.out&\r\n\r\nCUDA_VISIBLE_DEVICES='' nohup python test_saver.py --job_name=worker > worker.out &\r\n```\r\n-------------------------------------------------------------------------------------\r\n\r\n  \r\nPS:  both saver1 and saver2 can work correctly when use standalone tensorflow(i.e  without parameter server)"}