{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321071426", "html_url": "https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-321071426", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12052", "id": 321071426, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTA3MTQyNg==", "user": {"login": "cancan101", "id": 51059, "node_id": "MDQ6VXNlcjUxMDU5", "avatar_url": "https://avatars1.githubusercontent.com/u/51059?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cancan101", "html_url": "https://github.com/cancan101", "followers_url": "https://api.github.com/users/cancan101/followers", "following_url": "https://api.github.com/users/cancan101/following{/other_user}", "gists_url": "https://api.github.com/users/cancan101/gists{/gist_id}", "starred_url": "https://api.github.com/users/cancan101/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cancan101/subscriptions", "organizations_url": "https://api.github.com/users/cancan101/orgs", "repos_url": "https://api.github.com/users/cancan101/repos", "events_url": "https://api.github.com/users/cancan101/events{/privacy}", "received_events_url": "https://api.github.com/users/cancan101/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-08T20:25:22Z", "updated_at": "2017-08-08T20:25:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here are the full set of features in cuDNN 7:</p>\n<blockquote>\n<p>Key Features and Enhancements<br>\nThis cuDNN release includes the following key features and enhancements.<br>\nTensor Cores<br>\nVersion 7.0.1 of cuDNN is the first to support the Tensor Core operations in its<br>\nimplementation. Tensor Cores provide highly optimized matrix multiplication<br>\nbuilding blocks that do not have an equivalent numerical behavior in the traditional<br>\ninstructions, therefore, its numerical behavior is slightly different.<br>\ncudnnSetConvolutionMathType, cudnnSetRNNMatrixMathType, and<br>\ncudnnMathType_t<br>\nThe cudnnSetConvolutionMathType and cudnnSetRNNMatrixMathType<br>\nfunctions enable you to choose whether or not to use Tensor Core operations in<br>\nthe convolution and RNN layers respectively by setting the math mode to either<br>\nCUDNN_TENSOR_OP_MATH or CUDNN_DEFAULT_MATH.<br>\nTensor Core operations perform parallel floating point accumulation of multiple<br>\nfloating point products.<br>\nSetting the math mode to CUDNN_TENSOR_OP_MATH indicates that the library will use<br>\nTensor Core operations.<br>\nThe default is CUDNN_DEFAULT_MATH. This default indicates that the Tensor Core<br>\noperations will be avoided by the library. The default mode is a serialized operation<br>\nwhereas, the Tensor Core is a parallelized operation, therefore, the two might result<br>\nin slightly different numerical results due to the different sequencing of operations.<br>\nThe library falls back to the default math mode when Tensor Core operations are<br>\nnot supported or not permitted.<br>\ncudnnSetConvolutionGroupCount<br>\nA new interface that allows applications to perform convolution groups in the<br>\nconvolution layers in a single API call.<br>\ncudnnCTCLoss<br>\ncudnnCTCLoss provides a GPU implementation of the Connectionist Temporal<br>\nClassification (CTC) loss function for RNNs. The CTC loss function is used for<br>\nphoneme recognition in speech and handwriting recognition.<br>\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT<br>\nThe CUDNN_BATCHNORM_SPATIAL_PERSISTENT function is a new batch<br>\nnormalization mode for cudnnBatchNormalizationForwardTraining<br>\nand cudnnBatchNormalizationBackward. This mode is similar to<br>\nCUDNN_BATCHNORM_SPATIAL, however, it can be faster for some tasks.<br>\ncudnnQueryRuntimeError<br>\nThe cudnnQueryRuntimeError function reports error codes written by GPU<br>\nkernels when executing cudnnBatchNormalizationForwardTraining<br>\nand cudnnBatchNormalizationBackward with the<br>\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT mode.<br>\ncudnnGetConvolutionForwardAlgorithm_v7<br>\nThis new API returns all algorithms sorted by expected performance<br>\n(using internal heuristics). These algorithms are output similarly to<br>\ncudnnFindConvolutionForwardAlgorithm.<br>\ncudnnGetConvolutionBackwardDataAlgorithm_v7<br>\nThis new API returns all algorithms sorted by expected performance<br>\n(using internal heuristics). These algorithms are output similarly to<br>\ncudnnFindConvolutionBackwardAlgorithm.<br>\ncudnnGetConvolutionBackwardFilterAlgorithm_v7<br>\nThis new API returns all algorithms sorted by expected performance<br>\n(using internal heuristics). These algorithms are output similarly to<br>\ncudnnFindConvolutionBackwardFilterAlgorithm.<br>\nCUDNN_REDUCE_TENSOR_MUL_NO_ZEROS<br>\nThe MUL_NO_ZEROS function is a multiplication reduction that ignores zeros in the<br>\ndata.<br>\nCUDNN_OP_TENSOR_NOT<br>\nThe OP_TENSOR_NOT function is a unary operation that takes the negative of<br>\n(alpha*A).<br>\ncudnnGetDropoutDescriptor<br>\nThe cudnnGetDropoutDescriptor function allows applications to get dropout<br>\nvalues.</p>\n</blockquote>", "body_text": "Here are the full set of features in cuDNN 7:\n\nKey Features and Enhancements\nThis cuDNN release includes the following key features and enhancements.\nTensor Cores\nVersion 7.0.1 of cuDNN is the first to support the Tensor Core operations in its\nimplementation. Tensor Cores provide highly optimized matrix multiplication\nbuilding blocks that do not have an equivalent numerical behavior in the traditional\ninstructions, therefore, its numerical behavior is slightly different.\ncudnnSetConvolutionMathType, cudnnSetRNNMatrixMathType, and\ncudnnMathType_t\nThe cudnnSetConvolutionMathType and cudnnSetRNNMatrixMathType\nfunctions enable you to choose whether or not to use Tensor Core operations in\nthe convolution and RNN layers respectively by setting the math mode to either\nCUDNN_TENSOR_OP_MATH or CUDNN_DEFAULT_MATH.\nTensor Core operations perform parallel floating point accumulation of multiple\nfloating point products.\nSetting the math mode to CUDNN_TENSOR_OP_MATH indicates that the library will use\nTensor Core operations.\nThe default is CUDNN_DEFAULT_MATH. This default indicates that the Tensor Core\noperations will be avoided by the library. The default mode is a serialized operation\nwhereas, the Tensor Core is a parallelized operation, therefore, the two might result\nin slightly different numerical results due to the different sequencing of operations.\nThe library falls back to the default math mode when Tensor Core operations are\nnot supported or not permitted.\ncudnnSetConvolutionGroupCount\nA new interface that allows applications to perform convolution groups in the\nconvolution layers in a single API call.\ncudnnCTCLoss\ncudnnCTCLoss provides a GPU implementation of the Connectionist Temporal\nClassification (CTC) loss function for RNNs. The CTC loss function is used for\nphoneme recognition in speech and handwriting recognition.\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT\nThe CUDNN_BATCHNORM_SPATIAL_PERSISTENT function is a new batch\nnormalization mode for cudnnBatchNormalizationForwardTraining\nand cudnnBatchNormalizationBackward. This mode is similar to\nCUDNN_BATCHNORM_SPATIAL, however, it can be faster for some tasks.\ncudnnQueryRuntimeError\nThe cudnnQueryRuntimeError function reports error codes written by GPU\nkernels when executing cudnnBatchNormalizationForwardTraining\nand cudnnBatchNormalizationBackward with the\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT mode.\ncudnnGetConvolutionForwardAlgorithm_v7\nThis new API returns all algorithms sorted by expected performance\n(using internal heuristics). These algorithms are output similarly to\ncudnnFindConvolutionForwardAlgorithm.\ncudnnGetConvolutionBackwardDataAlgorithm_v7\nThis new API returns all algorithms sorted by expected performance\n(using internal heuristics). These algorithms are output similarly to\ncudnnFindConvolutionBackwardAlgorithm.\ncudnnGetConvolutionBackwardFilterAlgorithm_v7\nThis new API returns all algorithms sorted by expected performance\n(using internal heuristics). These algorithms are output similarly to\ncudnnFindConvolutionBackwardFilterAlgorithm.\nCUDNN_REDUCE_TENSOR_MUL_NO_ZEROS\nThe MUL_NO_ZEROS function is a multiplication reduction that ignores zeros in the\ndata.\nCUDNN_OP_TENSOR_NOT\nThe OP_TENSOR_NOT function is a unary operation that takes the negative of\n(alpha*A).\ncudnnGetDropoutDescriptor\nThe cudnnGetDropoutDescriptor function allows applications to get dropout\nvalues.", "body": "Here are the full set of features in cuDNN 7:\r\n>Key Features and Enhancements\r\nThis cuDNN release includes the following key features and enhancements.\r\nTensor Cores\r\nVersion 7.0.1 of cuDNN is the first to support the Tensor Core operations in its\r\nimplementation. Tensor Cores provide highly optimized matrix multiplication\r\nbuilding blocks that do not have an equivalent numerical behavior in the traditional\r\ninstructions, therefore, its numerical behavior is slightly different.\r\ncudnnSetConvolutionMathType, cudnnSetRNNMatrixMathType, and\r\ncudnnMathType_t\r\nThe cudnnSetConvolutionMathType and cudnnSetRNNMatrixMathType\r\nfunctions enable you to choose whether or not to use Tensor Core operations in\r\nthe convolution and RNN layers respectively by setting the math mode to either\r\nCUDNN_TENSOR_OP_MATH or CUDNN_DEFAULT_MATH.\r\nTensor Core operations perform parallel floating point accumulation of multiple\r\nfloating point products.\r\nSetting the math mode to CUDNN_TENSOR_OP_MATH indicates that the library will use\r\nTensor Core operations.\r\nThe default is CUDNN_DEFAULT_MATH. This default indicates that the Tensor Core\r\noperations will be avoided by the library. The default mode is a serialized operation\r\nwhereas, the Tensor Core is a parallelized operation, therefore, the two might result\r\nin slightly different numerical results due to the different sequencing of operations.\r\nThe library falls back to the default math mode when Tensor Core operations are\r\nnot supported or not permitted.\r\ncudnnSetConvolutionGroupCount\r\nA new interface that allows applications to perform convolution groups in the\r\nconvolution layers in a single API call.\r\ncudnnCTCLoss\r\ncudnnCTCLoss provides a GPU implementation of the Connectionist Temporal\r\nClassification (CTC) loss function for RNNs. The CTC loss function is used for\r\nphoneme recognition in speech and handwriting recognition.\r\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT\r\nThe CUDNN_BATCHNORM_SPATIAL_PERSISTENT function is a new batch\r\nnormalization mode for cudnnBatchNormalizationForwardTraining\r\nand cudnnBatchNormalizationBackward. This mode is similar to\r\nCUDNN_BATCHNORM_SPATIAL, however, it can be faster for some tasks.\r\ncudnnQueryRuntimeError\r\nThe cudnnQueryRuntimeError function reports error codes written by GPU\r\nkernels when executing cudnnBatchNormalizationForwardTraining\r\nand cudnnBatchNormalizationBackward with the\r\nCUDNN_BATCHNORM_SPATIAL_PERSISTENT mode.\r\ncudnnGetConvolutionForwardAlgorithm_v7\r\nThis new API returns all algorithms sorted by expected performance\r\n(using internal heuristics). These algorithms are output similarly to\r\ncudnnFindConvolutionForwardAlgorithm.\r\ncudnnGetConvolutionBackwardDataAlgorithm_v7\r\nThis new API returns all algorithms sorted by expected performance\r\n(using internal heuristics). These algorithms are output similarly to\r\ncudnnFindConvolutionBackwardAlgorithm.\r\ncudnnGetConvolutionBackwardFilterAlgorithm_v7\r\nThis new API returns all algorithms sorted by expected performance\r\n(using internal heuristics). These algorithms are output similarly to\r\ncudnnFindConvolutionBackwardFilterAlgorithm.\r\nCUDNN_REDUCE_TENSOR_MUL_NO_ZEROS\r\nThe MUL_NO_ZEROS function is a multiplication reduction that ignores zeros in the\r\ndata.\r\nCUDNN_OP_TENSOR_NOT\r\nThe OP_TENSOR_NOT function is a unary operation that takes the negative of\r\n(alpha*A).\r\ncudnnGetDropoutDescriptor\r\nThe cudnnGetDropoutDescriptor function allows applications to get dropout\r\nvalues."}