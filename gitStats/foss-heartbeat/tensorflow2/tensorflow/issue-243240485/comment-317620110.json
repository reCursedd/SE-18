{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317620110", "html_url": "https://github.com/tensorflow/tensorflow/issues/11534#issuecomment-317620110", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11534", "id": 317620110, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzYyMDExMA==", "user": {"login": "khaotik", "id": 6271084, "node_id": "MDQ6VXNlcjYyNzEwODQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/6271084?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khaotik", "html_url": "https://github.com/khaotik", "followers_url": "https://api.github.com/users/khaotik/followers", "following_url": "https://api.github.com/users/khaotik/following{/other_user}", "gists_url": "https://api.github.com/users/khaotik/gists{/gist_id}", "starred_url": "https://api.github.com/users/khaotik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khaotik/subscriptions", "organizations_url": "https://api.github.com/users/khaotik/orgs", "repos_url": "https://api.github.com/users/khaotik/repos", "events_url": "https://api.github.com/users/khaotik/events{/privacy}", "received_events_url": "https://api.github.com/users/khaotik/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-25T03:42:04Z", "updated_at": "2017-07-25T03:42:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> Yes, I encountered this during GAN training.</p>\n<p>Basically, it looks like:</p>\n<pre><code>num_class = ...\nnum_sample = ...\nbatch_size = ...\nlabels = tf.placeholder('int32', [1, num_sample])\nlogits = tf.placeholder('float32', [batch_size, num_sample, num_class])\n</code></pre>\n<p>In my use case, the model is about multi-speaker audio separation, each data point comes with multiple single-speaker samples. So <code>num_samples</code> can't be merged with <code>batch_size</code>.</p>\n<p>We wanted GAN discriminator to do multi-class classification besides true/false. So we use softmax rather than sigmoid.<br>\nHere <code>labels</code> is a constant <code>int</code> tensor across batches. The broadcast situation arises here.</p>\n<p>It's really not a bottleneck in our model, but I'm probably just getting OCD about optimization. This situation may also arise in other separation / segmentation tasks, I think.</p>", "body_text": "@aselle Yes, I encountered this during GAN training.\nBasically, it looks like:\nnum_class = ...\nnum_sample = ...\nbatch_size = ...\nlabels = tf.placeholder('int32', [1, num_sample])\nlogits = tf.placeholder('float32', [batch_size, num_sample, num_class])\n\nIn my use case, the model is about multi-speaker audio separation, each data point comes with multiple single-speaker samples. So num_samples can't be merged with batch_size.\nWe wanted GAN discriminator to do multi-class classification besides true/false. So we use softmax rather than sigmoid.\nHere labels is a constant int tensor across batches. The broadcast situation arises here.\nIt's really not a bottleneck in our model, but I'm probably just getting OCD about optimization. This situation may also arise in other separation / segmentation tasks, I think.", "body": "@aselle Yes, I encountered this during GAN training.\r\n\r\nBasically, it looks like:\r\n\r\n```\r\nnum_class = ...\r\nnum_sample = ...\r\nbatch_size = ...\r\nlabels = tf.placeholder('int32', [1, num_sample])\r\nlogits = tf.placeholder('float32', [batch_size, num_sample, num_class])\r\n```\r\n\r\nIn my use case, the model is about multi-speaker audio separation, each data point comes with multiple single-speaker samples. So `num_samples` can't be merged with `batch_size`.\r\n\r\nWe wanted GAN discriminator to do multi-class classification besides true/false. So we use softmax rather than sigmoid.\r\nHere `labels` is a constant `int` tensor across batches. The broadcast situation arises here.\r\n\r\nIt's really not a bottleneck in our model, but I'm probably just getting OCD about optimization. This situation may also arise in other separation / segmentation tasks, I think."}