{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2322", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2322/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2322/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2322/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2322", "id": 154246443, "node_id": "MDU6SXNzdWUxNTQyNDY0NDM=", "number": 2322, "title": "InvalidArgumentError: Cannot assign a device to node when distributed running", "user": {"login": "ZhuFengdaaa", "id": 9649227, "node_id": "MDQ6VXNlcjk2NDkyMjc=", "avatar_url": "https://avatars0.githubusercontent.com/u/9649227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhuFengdaaa", "html_url": "https://github.com/ZhuFengdaaa", "followers_url": "https://api.github.com/users/ZhuFengdaaa/followers", "following_url": "https://api.github.com/users/ZhuFengdaaa/following{/other_user}", "gists_url": "https://api.github.com/users/ZhuFengdaaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhuFengdaaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhuFengdaaa/subscriptions", "organizations_url": "https://api.github.com/users/ZhuFengdaaa/orgs", "repos_url": "https://api.github.com/users/ZhuFengdaaa/repos", "events_url": "https://api.github.com/users/ZhuFengdaaa/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhuFengdaaa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-05-11T13:41:05Z", "updated_at": "2016-05-13T08:34:26Z", "closed_at": "2016-05-13T08:34:26Z", "author_association": "NONE", "body_html": "<p>I tried running <a href=\"https://github.com/tensorflow/models/tree/master/inception\">this script</a> on two machine, start script as follows, Note that each machine has 4 GPUs.</p>\n<pre><code># machine 10.10.12.28\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=0 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=1 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=3 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\n#machine 10.10.12.29\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=4 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=5 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=6 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=7 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n</code></pre>\n<p>error log as follows:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 264, in train\n    sess = sv.prepare_or_wait_for_session(target, config=sess_config)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 681, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 282, in wait_for_session\n    sess.run([self._local_init_op])\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 355, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 606, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 695, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 715, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_837/shape_and_slice': Could not satisfy explicit device specification '/job:ps/task:1/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:ps/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:0/gpu:1, /job:worker/replica:0/task:0/gpu:2, /job:worker/replica:0/task:0/gpu:3, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0, /job:worker/replica:0/task:1/gpu:1, /job:worker/replica:0/task:1/gpu:2, /job:worker/replica:0/task:1/gpu:3, /job:worker/replica:0/task:2/cpu:0, /job:worker/replica:0/task:2/gpu:0, /job:worker/replica:0/task:2/gpu:1, /job:worker/replica:0/task:2/gpu:2, /job:worker/replica:0/task:2/gpu:3, /job:worker/replica:0/task:4/cpu:0, /job:worker/replica:0/task:4/gpu:0, /job:worker/replica:0/task:4/gpu:1, /job:worker/replica:0/task:4/gpu:2, /job:worker/replica:0/task:4/gpu:3, /job:worker/replica:0/task:5/cpu:0, /job:worker/replica:0/task:5/gpu:0, /job:worker/replica:0/task:5/gpu:1, /job:worker/replica:0/task:5/gpu:2, /job:worker/replica:0/task:5/gpu:3, /job:worker/replica:0/task:6/cpu:0, /job:worker/replica:0/task:6/gpu:0, /job:worker/replica:0/task:6/gpu:1, /job:worker/replica:0/task:6/gpu:2, /job:worker/replica:0/task:6/gpu:3, /job:worker/replica:0/task:7/cpu:0, /job:worker/replica:0/task:7/gpu:0, /job:worker/replica:0/task:7/gpu:1, /job:worker/replica:0/task:7/gpu:2, /job:worker/replica:0/task:7/gpu:3\n     [[Node: save/restore_slice_837/shape_and_slice = Const[dtype=DT_STRING, value=Tensor&lt;type: string shape: [] values: &gt;, _device=\"/job:ps/task:1/device:CPU:0\"]()]]\nCaused by op u'save/restore_slice_837/shape_and_slice', defined at:\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 237, in train\n    saver = tf.train.Saver()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 201, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 444, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 566, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2177, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1161, in __init__\n    self._traceback = _extract_stack()\n</code></pre>", "body_text": "I tried running this script on two machine, start script as follows, Note that each machine has 4 GPUs.\n# machine 10.10.12.28\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=0 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=1 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=3 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n#machine 10.10.12.29\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=4 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=5 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=6 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=7 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nerror log as follows:\nTraceback (most recent call last):\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 264, in train\n    sess = sv.prepare_or_wait_for_session(target, config=sess_config)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 681, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 282, in wait_for_session\n    sess.run([self._local_init_op])\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 355, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 606, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 695, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 715, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_837/shape_and_slice': Could not satisfy explicit device specification '/job:ps/task:1/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:ps/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:0/gpu:1, /job:worker/replica:0/task:0/gpu:2, /job:worker/replica:0/task:0/gpu:3, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0, /job:worker/replica:0/task:1/gpu:1, /job:worker/replica:0/task:1/gpu:2, /job:worker/replica:0/task:1/gpu:3, /job:worker/replica:0/task:2/cpu:0, /job:worker/replica:0/task:2/gpu:0, /job:worker/replica:0/task:2/gpu:1, /job:worker/replica:0/task:2/gpu:2, /job:worker/replica:0/task:2/gpu:3, /job:worker/replica:0/task:4/cpu:0, /job:worker/replica:0/task:4/gpu:0, /job:worker/replica:0/task:4/gpu:1, /job:worker/replica:0/task:4/gpu:2, /job:worker/replica:0/task:4/gpu:3, /job:worker/replica:0/task:5/cpu:0, /job:worker/replica:0/task:5/gpu:0, /job:worker/replica:0/task:5/gpu:1, /job:worker/replica:0/task:5/gpu:2, /job:worker/replica:0/task:5/gpu:3, /job:worker/replica:0/task:6/cpu:0, /job:worker/replica:0/task:6/gpu:0, /job:worker/replica:0/task:6/gpu:1, /job:worker/replica:0/task:6/gpu:2, /job:worker/replica:0/task:6/gpu:3, /job:worker/replica:0/task:7/cpu:0, /job:worker/replica:0/task:7/gpu:0, /job:worker/replica:0/task:7/gpu:1, /job:worker/replica:0/task:7/gpu:2, /job:worker/replica:0/task:7/gpu:3\n     [[Node: save/restore_slice_837/shape_and_slice = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: >, _device=\"/job:ps/task:1/device:CPU:0\"]()]]\nCaused by op u'save/restore_slice_837/shape_and_slice', defined at:\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 237, in train\n    saver = tf.train.Saver()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 201, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 444, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 566, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2177, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1161, in __init__\n    self._traceback = _extract_stack()", "body": "I tried running [this script](https://github.com/tensorflow/models/tree/master/inception) on two machine, start script as follows, Note that each machine has 4 GPUs. \n\n```\n# machine 10.10.12.28\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=0 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=1 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=3 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n#machine 10.10.12.29\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=4 \\\n--gpu_device_id=0 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=5 \\\n--gpu_device_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=6 \\\n--gpu_device_id=2 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=7 \\\n--gpu_device_id=3 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nCUDA_VISIBLE_DEVICES='' ~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n-task_id=1 \\\n--ps_hosts='10.10.102.28:2220,10.10.102.29:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.29:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n```\n\nerror log as follows: \n\n```\nTraceback (most recent call last):\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 264, in train\n    sess = sv.prepare_or_wait_for_session(target, config=sess_config)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 681, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 282, in wait_for_session\n    sess.run([self._local_init_op])\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 355, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 606, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 695, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 715, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'save/restore_slice_837/shape_and_slice': Could not satisfy explicit device specification '/job:ps/task:1/device:CPU:0' because no devices matching that specification are registered in this process; available devices: /job:ps/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/cpu:0, /job:worker/replica:0/task:0/gpu:0, /job:worker/replica:0/task:0/gpu:1, /job:worker/replica:0/task:0/gpu:2, /job:worker/replica:0/task:0/gpu:3, /job:worker/replica:0/task:1/cpu:0, /job:worker/replica:0/task:1/gpu:0, /job:worker/replica:0/task:1/gpu:1, /job:worker/replica:0/task:1/gpu:2, /job:worker/replica:0/task:1/gpu:3, /job:worker/replica:0/task:2/cpu:0, /job:worker/replica:0/task:2/gpu:0, /job:worker/replica:0/task:2/gpu:1, /job:worker/replica:0/task:2/gpu:2, /job:worker/replica:0/task:2/gpu:3, /job:worker/replica:0/task:4/cpu:0, /job:worker/replica:0/task:4/gpu:0, /job:worker/replica:0/task:4/gpu:1, /job:worker/replica:0/task:4/gpu:2, /job:worker/replica:0/task:4/gpu:3, /job:worker/replica:0/task:5/cpu:0, /job:worker/replica:0/task:5/gpu:0, /job:worker/replica:0/task:5/gpu:1, /job:worker/replica:0/task:5/gpu:2, /job:worker/replica:0/task:5/gpu:3, /job:worker/replica:0/task:6/cpu:0, /job:worker/replica:0/task:6/gpu:0, /job:worker/replica:0/task:6/gpu:1, /job:worker/replica:0/task:6/gpu:2, /job:worker/replica:0/task:6/gpu:3, /job:worker/replica:0/task:7/cpu:0, /job:worker/replica:0/task:7/gpu:0, /job:worker/replica:0/task:7/gpu:1, /job:worker/replica:0/task:7/gpu:2, /job:worker/replica:0/task:7/gpu:3\n     [[Node: save/restore_slice_837/shape_and_slice = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: >, _device=\"/job:ps/task:1/device:CPU:0\"]()]]\nCaused by op u'save/restore_slice_837/shape_and_slice', defined at:\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 65, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py\", line 61, in main\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n  File \"/home/zhufengda/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py\", line 237, in train\n    saver = tf.train.Saver()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 201, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 444, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 566, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2177, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1161, in __init__\n    self._traceback = _extract_stack()\n```\n"}