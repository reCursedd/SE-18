{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/394822930", "html_url": "https://github.com/tensorflow/tensorflow/issues/19605#issuecomment-394822930", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19605", "id": 394822930, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDgyMjkzMA==", "user": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-05T19:00:06Z", "updated_at": "2018-06-05T19:00:06Z", "author_association": "MEMBER", "body_html": "<p>For multi-node, I suggest you take a look at <a href=\"https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks\">tf_cnn_benchmarks</a> and its treatment of <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L439\">distributed workers</a> . Tensor2tensor also has <a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/docs/distributed_training.md\">documentation on how they handle distributed training</a>. Is that what you had in mind?</p>", "body_text": "For multi-node, I suggest you take a look at tf_cnn_benchmarks and its treatment of distributed workers . Tensor2tensor also has documentation on how they handle distributed training. Is that what you had in mind?", "body": "For multi-node, I suggest you take a look at [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) and its treatment of [distributed workers](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L439) . Tensor2tensor also has [documentation on how they handle distributed training](https://github.com/tensorflow/tensor2tensor/blob/master/docs/distributed_training.md). Is that what you had in mind?"}