{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/339015154", "html_url": "https://github.com/tensorflow/tensorflow/pull/13852#issuecomment-339015154", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13852", "id": 339015154, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTAxNTE1NA==", "user": {"login": "xiejw", "id": 1184671, "node_id": "MDQ6VXNlcjExODQ2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1184671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiejw", "html_url": "https://github.com/xiejw", "followers_url": "https://api.github.com/users/xiejw/followers", "following_url": "https://api.github.com/users/xiejw/following{/other_user}", "gists_url": "https://api.github.com/users/xiejw/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiejw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiejw/subscriptions", "organizations_url": "https://api.github.com/users/xiejw/orgs", "repos_url": "https://api.github.com/users/xiejw/repos", "events_url": "https://api.github.com/users/xiejw/events{/privacy}", "received_events_url": "https://api.github.com/users/xiejw/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-24T14:45:30Z", "updated_at": "2017-10-24T14:45:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>All TF_CONFIGs look good to me.</p>\n<p>I think the issue is you are calling the Experiment improperly.</p>\n<p>Your code runs on all jobs (master, worker and ps). The same code should behave differently. The code runs on master handling the eval and export in addtion to training. The code on worrk only trains the model. ps handles the data sharing.</p>\n<p>In order to achieve that, Experiment class has multiple methods (called schedule). The \"schedule\" specifies the code to run.</p>\n<p>For master, the schedule should be train_and_evaluate. For worker, the schedule should be train.  User does not need to set the schedule. tf.contrib.learn.learn_runner handles that for your automatically.</p>\n<p>See you see the example in the docstring [1]. The schedule argument should be absent for most cases, as the default is good enough. So, the simplest way is</p>\n<pre><code>def _create_my_experiment(run_config, hparams):\n    # You can change a subset of the run_config properties as\n    #   run_config = run_config.replace(save_checkpoints_steps=500)\n    return tf.contrib.learn.Experiment(\n      estimator=my_estimator(config=run_config, hparams=hparams),\n      train_input_fn=my_train_input,\n      eval_input_fn=my_eval_input)\n\nlearn_runner.run(\n      experiment_fn=_create_my_experiment,\n      run_config=run_config_lib.RunConfig(model_dir=\"some/output/dir\"))\n</code></pre>\n<p>[1] <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_runner.py#L119\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_runner.py#L119</a></p>", "body_text": "All TF_CONFIGs look good to me.\nI think the issue is you are calling the Experiment improperly.\nYour code runs on all jobs (master, worker and ps). The same code should behave differently. The code runs on master handling the eval and export in addtion to training. The code on worrk only trains the model. ps handles the data sharing.\nIn order to achieve that, Experiment class has multiple methods (called schedule). The \"schedule\" specifies the code to run.\nFor master, the schedule should be train_and_evaluate. For worker, the schedule should be train.  User does not need to set the schedule. tf.contrib.learn.learn_runner handles that for your automatically.\nSee you see the example in the docstring [1]. The schedule argument should be absent for most cases, as the default is good enough. So, the simplest way is\ndef _create_my_experiment(run_config, hparams):\n    # You can change a subset of the run_config properties as\n    #   run_config = run_config.replace(save_checkpoints_steps=500)\n    return tf.contrib.learn.Experiment(\n      estimator=my_estimator(config=run_config, hparams=hparams),\n      train_input_fn=my_train_input,\n      eval_input_fn=my_eval_input)\n\nlearn_runner.run(\n      experiment_fn=_create_my_experiment,\n      run_config=run_config_lib.RunConfig(model_dir=\"some/output/dir\"))\n\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_runner.py#L119", "body": "All TF_CONFIGs look good to me. \r\n\r\nI think the issue is you are calling the Experiment improperly. \r\n\r\nYour code runs on all jobs (master, worker and ps). The same code should behave differently. The code runs on master handling the eval and export in addtion to training. The code on worrk only trains the model. ps handles the data sharing. \r\n\r\nIn order to achieve that, Experiment class has multiple methods (called schedule). The \"schedule\" specifies the code to run. \r\n\r\nFor master, the schedule should be train_and_evaluate. For worker, the schedule should be train.  User does not need to set the schedule. tf.contrib.learn.learn_runner handles that for your automatically. \r\n\r\nSee you see the example in the docstring [1]. The schedule argument should be absent for most cases, as the default is good enough. So, the simplest way is\r\n\r\n    def _create_my_experiment(run_config, hparams):\r\n        # You can change a subset of the run_config properties as\r\n        #   run_config = run_config.replace(save_checkpoints_steps=500)\r\n        return tf.contrib.learn.Experiment(\r\n          estimator=my_estimator(config=run_config, hparams=hparams),\r\n          train_input_fn=my_train_input,\r\n          eval_input_fn=my_eval_input)\r\n\r\n    learn_runner.run(\r\n          experiment_fn=_create_my_experiment,\r\n          run_config=run_config_lib.RunConfig(model_dir=\"some/output/dir\"))\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_runner.py#L119\r\n\r\n"}