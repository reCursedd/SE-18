{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/277771401", "html_url": "https://github.com/tensorflow/tensorflow/issues/7224#issuecomment-277771401", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7224", "id": 277771401, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Nzc3MTQwMQ==", "user": {"login": "hycis", "id": 3508361, "node_id": "MDQ6VXNlcjM1MDgzNjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3508361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hycis", "html_url": "https://github.com/hycis", "followers_url": "https://api.github.com/users/hycis/followers", "following_url": "https://api.github.com/users/hycis/following{/other_user}", "gists_url": "https://api.github.com/users/hycis/gists{/gist_id}", "starred_url": "https://api.github.com/users/hycis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hycis/subscriptions", "organizations_url": "https://api.github.com/users/hycis/orgs", "repos_url": "https://api.github.com/users/hycis/repos", "events_url": "https://api.github.com/users/hycis/events{/privacy}", "received_events_url": "https://api.github.com/users/hycis/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-06T18:34:58Z", "updated_at": "2017-02-06T19:07:47Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> So in this case, how do we create a dropout mask on an axis (dropout the entire dimension) for batch training of variable batch size, for example in a sequence of text embeddings of dimension <code>(batchsize, seqlen, embed_dim)</code> where we want to randomly dropout some of the word embedding in the sequence, which will require  <code>noise_shape=[-1, seqlen, 1]</code>, then how we do it with the current dropout? This is a very valid question that we probably need to find a solution.</p>", "body_text": "@girving So in this case, how do we create a dropout mask on an axis (dropout the entire dimension) for batch training of variable batch size, for example in a sequence of text embeddings of dimension (batchsize, seqlen, embed_dim) where we want to randomly dropout some of the word embedding in the sequence, which will require  noise_shape=[-1, seqlen, 1], then how we do it with the current dropout? This is a very valid question that we probably need to find a solution.", "body": "@girving So in this case, how do we create a dropout mask on an axis (dropout the entire dimension) for batch training of variable batch size, for example in a sequence of text embeddings of dimension `(batchsize, seqlen, embed_dim)` where we want to randomly dropout some of the word embedding in the sequence, which will require  `noise_shape=[-1, seqlen, 1]`, then how we do it with the current dropout? This is a very valid question that we probably need to find a solution."}