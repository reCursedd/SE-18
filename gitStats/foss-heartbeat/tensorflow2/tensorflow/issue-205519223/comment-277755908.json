{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/277755908", "html_url": "https://github.com/tensorflow/tensorflow/issues/7283#issuecomment-277755908", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7283", "id": 277755908, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Nzc1NTkwOA==", "user": {"login": "pitercius", "id": 20433727, "node_id": "MDQ6VXNlcjIwNDMzNzI3", "avatar_url": "https://avatars2.githubusercontent.com/u/20433727?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pitercius", "html_url": "https://github.com/pitercius", "followers_url": "https://api.github.com/users/pitercius/followers", "following_url": "https://api.github.com/users/pitercius/following{/other_user}", "gists_url": "https://api.github.com/users/pitercius/gists{/gist_id}", "starred_url": "https://api.github.com/users/pitercius/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pitercius/subscriptions", "organizations_url": "https://api.github.com/users/pitercius/orgs", "repos_url": "https://api.github.com/users/pitercius/repos", "events_url": "https://api.github.com/users/pitercius/events{/privacy}", "received_events_url": "https://api.github.com/users/pitercius/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-06T17:40:44Z", "updated_at": "2017-02-06T17:49:20Z", "author_association": "NONE", "body_html": "<p>It should work but it doesn\u00b4t:</p>\n<hr>\n<p>AttributeError                            Traceback (most recent call last)<br>\n in ()<br>\n1 import tensorflow as tf<br>\n----&gt; 2 print(tf.contrib.learn)</p>\n<p><strong>AttributeError: 'module' object has no attribute 'learn'</strong></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a> , the failing part of the code is:</p>\n<pre><code>####################################################################################################\n## Train\n####################################################################################################\nimport tensorflow as tf\n# print(tf.contrib.learn)\nimport numpy as np\nimport os\nimport time\nimport datetime\n# from tensorflow.contrib import learn\n\n# Parameters\n# ==================================================\n\n# Data loading params\ntf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\ntf.flags.DEFINE_string(\"data_file\", \"./DAT_data_file.txt\", \"Data source for the data.\")\ntf.flags.DEFINE_string(\"target_file\", \"./DAT_target_file.txt\", \"Data source for the target.\")\n\n# Model Hyperparameters\ntf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\ntf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\ntf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\ntf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\ntf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n\n# Training parameters\ntf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\ntf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\ntf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\ntf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\ntf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n# Misc Parameters\ntf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\ntf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\nprint(\"\\nParameters:\")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(\"{}={}\".format(attr.upper(), value))\nprint(\"\")\n\n\n# Data Preparation\n# ==================================================\n\n# Load data\nprint(\"Loading data...\")\nx_text, y = load_data_and_labels(FLAGS.data_file, FLAGS.target_file)\n\n# Build vocabulary\nmax_document_length = max([len(x.split(\" \")) for x in x_text])\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\nx = np.array(list(vocab_processor.fit_transform(x_text)))\n</code></pre>\n<p>In fact, the code is essentially identical to <a href=\"https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py\">https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py</a>  (where dependencies, like load_data_and_labels, are)</p>\n<p>If anything else is needed, don\u00b4t hesitate to ask</p>", "body_text": "It should work but it doesn\u00b4t:\n\nAttributeError                            Traceback (most recent call last)\n in ()\n1 import tensorflow as tf\n----> 2 print(tf.contrib.learn)\nAttributeError: 'module' object has no attribute 'learn'\n@martinwicke , the failing part of the code is:\n####################################################################################################\n## Train\n####################################################################################################\nimport tensorflow as tf\n# print(tf.contrib.learn)\nimport numpy as np\nimport os\nimport time\nimport datetime\n# from tensorflow.contrib import learn\n\n# Parameters\n# ==================================================\n\n# Data loading params\ntf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\ntf.flags.DEFINE_string(\"data_file\", \"./DAT_data_file.txt\", \"Data source for the data.\")\ntf.flags.DEFINE_string(\"target_file\", \"./DAT_target_file.txt\", \"Data source for the target.\")\n\n# Model Hyperparameters\ntf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\ntf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\ntf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\ntf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\ntf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n\n# Training parameters\ntf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\ntf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\ntf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\ntf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\ntf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n# Misc Parameters\ntf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\ntf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\nprint(\"\\nParameters:\")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(\"{}={}\".format(attr.upper(), value))\nprint(\"\")\n\n\n# Data Preparation\n# ==================================================\n\n# Load data\nprint(\"Loading data...\")\nx_text, y = load_data_and_labels(FLAGS.data_file, FLAGS.target_file)\n\n# Build vocabulary\nmax_document_length = max([len(x.split(\" \")) for x in x_text])\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\nx = np.array(list(vocab_processor.fit_transform(x_text)))\n\nIn fact, the code is essentially identical to https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py  (where dependencies, like load_data_and_labels, are)\nIf anything else is needed, don\u00b4t hesitate to ask", "body": "It should work but it doesn\u00b4t:\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-efeec648fb66> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 print(tf.contrib.learn)\r\n\r\n**AttributeError: 'module' object has no attribute 'learn'**\r\n\r\n@martinwicke , the failing part of the code is:\r\n\r\n```\r\n####################################################################################################\r\n## Train\r\n####################################################################################################\r\nimport tensorflow as tf\r\n# print(tf.contrib.learn)\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport datetime\r\n# from tensorflow.contrib import learn\r\n\r\n# Parameters\r\n# ==================================================\r\n\r\n# Data loading params\r\ntf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\r\ntf.flags.DEFINE_string(\"data_file\", \"./DAT_data_file.txt\", \"Data source for the data.\")\r\ntf.flags.DEFINE_string(\"target_file\", \"./DAT_target_file.txt\", \"Data source for the target.\")\r\n\r\n# Model Hyperparameters\r\ntf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\r\ntf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\r\ntf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\r\ntf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\r\ntf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\r\n\r\n# Training parameters\r\ntf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\r\ntf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\r\ntf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\r\ntf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\r\ntf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\r\n# Misc Parameters\r\ntf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\r\ntf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\r\n\r\nFLAGS = tf.flags.FLAGS\r\nFLAGS._parse_flags()\r\nprint(\"\\nParameters:\")\r\nfor attr, value in sorted(FLAGS.__flags.items()):\r\n    print(\"{}={}\".format(attr.upper(), value))\r\nprint(\"\")\r\n\r\n\r\n# Data Preparation\r\n# ==================================================\r\n\r\n# Load data\r\nprint(\"Loading data...\")\r\nx_text, y = load_data_and_labels(FLAGS.data_file, FLAGS.target_file)\r\n\r\n# Build vocabulary\r\nmax_document_length = max([len(x.split(\" \")) for x in x_text])\r\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\r\nx = np.array(list(vocab_processor.fit_transform(x_text)))\r\n```\r\n\r\nIn fact, the code is essentially identical to https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py  (where dependencies, like load_data_and_labels, are)\r\n\r\nIf anything else is needed, don\u00b4t hesitate to ask"}