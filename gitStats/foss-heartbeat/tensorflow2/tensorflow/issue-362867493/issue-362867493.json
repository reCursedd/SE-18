{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22465", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22465/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22465/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22465/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22465", "id": 362867493, "node_id": "MDU6SXNzdWUzNjI4Njc0OTM=", "number": 22465, "title": "[Feature Request]: tf.data.Dataset.map parallelism autotune enhancement", "user": {"login": "fanshiqing", "id": 3927162, "node_id": "MDQ6VXNlcjM5MjcxNjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3927162?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fanshiqing", "html_url": "https://github.com/fanshiqing", "followers_url": "https://api.github.com/users/fanshiqing/followers", "following_url": "https://api.github.com/users/fanshiqing/following{/other_user}", "gists_url": "https://api.github.com/users/fanshiqing/gists{/gist_id}", "starred_url": "https://api.github.com/users/fanshiqing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fanshiqing/subscriptions", "organizations_url": "https://api.github.com/users/fanshiqing/orgs", "repos_url": "https://api.github.com/users/fanshiqing/repos", "events_url": "https://api.github.com/users/fanshiqing/events{/privacy}", "received_events_url": "https://api.github.com/users/fanshiqing/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-09-22T15:48:37Z", "updated_at": "2018-10-14T02:41:34Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Cent OS</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 2.7.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>: NA</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Recently this <a href=\"https://github.com/tensorflow/tensorflow/commit/c8a0dfc741736a59f8fd1776b71f38619d66da56#diff-df634c8243713c0afd2e05c1689412e2\">PR</a> which leverages the underlying  performance model to find the optimal values for the parallelism knobs, this seems intuitive and really useful for me. While after carefully going through the relevant codebase, there are two problems that have been bothering me.</p>\n<p>(1) Currently dataset.map's num_parallel_calls autotune (as well as <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/prefetch_autotuner.cc#L36-L55\">dataset.prefetch buffer size autotune</a>) only works in the scenario when <code>kAutotune</code> guard is set and then start a one-way increment. On the one hand, in actual production environment, careless setting of parameters like <code>map.num_parllel_calls</code> or <code>prefetch.buffer_size</code> is somewhat common, which may result in inefficient  or even wore resource utilization.  On the other hand, even a carefully-tuned parameter which leverages underlying system info and works good in environment may turn out to be incompatible when we transfer into another environment. Therefore it's nature that our AUTOTUNE logic can support our auto_tune params's autotune based on runtime system info <strong>as well as user's initial params passed in</strong>, in which case the autotune_params' adjustment should be two-way.</p>\n<p>(2) At the specific code implementation level, this PR leverages <code>port::NumSchedulableCPUs()</code> to get the number of cores available in the process, and calculate the idealized acceleration brought by parallelism's increment(e.g. <a href=\"https://github.com/tensorflow/tensorflow/blob/c8a0dfc741736a59f8fd1776b71f38619d66da56/tensorflow/core/framework/model.cc#L214-L221\">PARALLEL_MAP</a>). This should be tuned carefully as in complicated distributed environment the increase of map parallelism perhaps results in invalid scheduling of resources, and eventually <strong>brings in negative optimization</strong>.</p>\n<p>For the above two points I have figured out some workaround, and the first draft of the code is also being developed. Any comments from you side is hight welcome and much appreciated. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1072079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jsimsa\">@jsimsa</a></p>\n<p>Thanks</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Cent OS\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4\nPython version: 2.7.5\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: NA\n\nDescribe the problem\nRecently this PR which leverages the underlying  performance model to find the optimal values for the parallelism knobs, this seems intuitive and really useful for me. While after carefully going through the relevant codebase, there are two problems that have been bothering me.\n(1) Currently dataset.map's num_parallel_calls autotune (as well as dataset.prefetch buffer size autotune) only works in the scenario when kAutotune guard is set and then start a one-way increment. On the one hand, in actual production environment, careless setting of parameters like map.num_parllel_calls or prefetch.buffer_size is somewhat common, which may result in inefficient  or even wore resource utilization.  On the other hand, even a carefully-tuned parameter which leverages underlying system info and works good in environment may turn out to be incompatible when we transfer into another environment. Therefore it's nature that our AUTOTUNE logic can support our auto_tune params's autotune based on runtime system info as well as user's initial params passed in, in which case the autotune_params' adjustment should be two-way.\n(2) At the specific code implementation level, this PR leverages port::NumSchedulableCPUs() to get the number of cores available in the process, and calculate the idealized acceleration brought by parallelism's increment(e.g. PARALLEL_MAP). This should be tuned carefully as in complicated distributed environment the increase of map parallelism perhaps results in invalid scheduling of resources, and eventually brings in negative optimization.\nFor the above two points I have figured out some workaround, and the first draft of the code is also being developed. Any comments from you side is hight welcome and much appreciated. @jsimsa\nThanks", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n\r\n### Describe the problem\r\nRecently this [PR](https://github.com/tensorflow/tensorflow/commit/c8a0dfc741736a59f8fd1776b71f38619d66da56#diff-df634c8243713c0afd2e05c1689412e2) which leverages the underlying  performance model to find the optimal values for the parallelism knobs, this seems intuitive and really useful for me. While after carefully going through the relevant codebase, there are two problems that have been bothering me.\r\n\r\n(1) Currently dataset.map's num_parallel_calls autotune (as well as [dataset.prefetch buffer size autotune](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/prefetch_autotuner.cc#L36-L55)) only works in the scenario when `kAutotune` guard is set and then start a one-way increment. On the one hand, in actual production environment, careless setting of parameters like `map.num_parllel_calls` or `prefetch.buffer_size` is somewhat common, which may result in inefficient  or even wore resource utilization.  On the other hand, even a carefully-tuned parameter which leverages underlying system info and works good in environment may turn out to be incompatible when we transfer into another environment. Therefore it's nature that our AUTOTUNE logic can support our auto_tune params's autotune based on runtime system info **as well as user's initial params passed in**, in which case the autotune_params' adjustment should be two-way.\r\n\r\n(2) At the specific code implementation level, this PR leverages `port::NumSchedulableCPUs()` to get the number of cores available in the process, and calculate the idealized acceleration brought by parallelism's increment(e.g. [PARALLEL_MAP](https://github.com/tensorflow/tensorflow/blob/c8a0dfc741736a59f8fd1776b71f38619d66da56/tensorflow/core/framework/model.cc#L214-L221)). This should be tuned carefully as in complicated distributed environment the increase of map parallelism perhaps results in invalid scheduling of resources, and eventually **brings in negative optimization**.\r\n\r\nFor the above two points I have figured out some workaround, and the first draft of the code is also being developed. Any comments from you side is hight welcome and much appreciated. @jsimsa \r\n\r\nThanks\r\n\r\n\r\n"}