{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5277", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5277/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5277/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5277/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5277", "id": 186090103, "node_id": "MDU6SXNzdWUxODYwOTAxMDM=", "number": 5277, "title": "Eigen implemented CPU op is 10 times slower than OpenMP", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2016-10-29T17:23:34Z", "updated_at": "2018-02-02T23:43:25Z", "closed_at": "2018-02-02T23:43:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I implemented a phase CPU operator consisting of four loop levels. I couldn't find any Eigen tensor docs at that stage so I use OpenMP to trivially parallelise the outer loops. I recently found the Eigen tensor documentation so I thought I'd take advantage of it and get all the multithreading/AVX/SSE goodies for free!</p>\n<p>Unfortunately the Eigen version is about 10 times slower!</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 16.04</p>\n<p>Installed version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>$ ls -l /usr/local/cuda-8.0/lib64/libcud*\n-rw-r--r-- 1 root root 558720 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -&gt; libcudart.so.8.0\nlrwxrwxrwx 1 root root     19 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44\n-rw-r--r-- 1 root root 415432 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root 775162 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\n</code></pre>\n<pre><code>$ ls -l /usr/local/cudnn-5.1-cuda-8.0/lib64/lib*\nlrwxrwxrwx 1 root root       13 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so -&gt; libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5 -&gt; libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn_static.a\n</code></pre>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed: python 2.7 linux GPU nightly</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<pre><code>$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so locally\n0.11.0rc1\n</code></pre>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)</li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<ul>\n<li><a href=\"https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/phase_op_cpu.h\">Source code</a>.</li>\n<li><a href=\"https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/Makefile\">Makefile</a>. In terms of optimisations I'm using <strong>-O2</strong> and <strong>-fopenmp</strong></li>\n<li><a href=\"https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/test_phase.py\">Test Case</a> and timing code.</li>\n</ul>\n<h4>OpenMP</h4>\n<ul>\n<li>Timings</li>\n</ul>\n<pre><code>Tensorflow custom GPU time 0.342187\nTensorflow expression GPU time 0.270267\nTensorflow CPU time 0.417076\nNumpy CPU time 2.542890\n</code></pre>\n<ul>\n<li>Code</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> Compute the complex phase</span>\n#<span class=\"pl-k\">pragma</span> omp parallel for\n<span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> src=<span class=\"pl-c1\">0</span>; src&lt;nsrc; ++src)\n{\n    FT l = <span class=\"pl-c1\">lm</span>(src,<span class=\"pl-c1\">0</span>);\n    FT m = <span class=\"pl-c1\">lm</span>(src,<span class=\"pl-c1\">1</span>);\n    FT n = <span class=\"pl-c1\">std::sqrt</span>(<span class=\"pl-c1\">1.0</span> - l*l - m*m) - <span class=\"pl-c1\">1.0</span>;\n\n    <span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> <span class=\"pl-c1\">time</span>=<span class=\"pl-c1\">0</span>; <span class=\"pl-c1\">time</span>&lt;ntime; ++<span class=\"pl-c1\">time</span>)\n    {\n        <span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> antenna=<span class=\"pl-c1\">0</span>; antenna&lt;na; ++antenna)\n        {\n            FT u = <span class=\"pl-c1\">uvw</span>(<span class=\"pl-c1\">time</span>,antenna,<span class=\"pl-c1\">0</span>);\n            FT v = <span class=\"pl-c1\">uvw</span>(<span class=\"pl-c1\">time</span>,antenna,<span class=\"pl-c1\">1</span>);\n            FT w = <span class=\"pl-c1\">uvw</span>(<span class=\"pl-c1\">time</span>,antenna,<span class=\"pl-c1\">2</span>);\n\n            FT real_phase_base = minus_two_pi_over_c*(l*u + m*v + n*w);\n\n            <span class=\"pl-k\">for</span>(<span class=\"pl-k\">int</span> chan=<span class=\"pl-c1\">0</span>; chan&lt;nchan; ++chan)\n            {\n                <span class=\"pl-c\"><span class=\"pl-c\">//</span> Our real phase input to the exponential function is purely imaginary so we can</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">//</span> can elide a call to std::exp&lt;complex&lt;FT&gt;&gt; and just compute the cos and sin</span>\n                FT real_phase = real_phase_base*<span class=\"pl-c1\">frequency</span>(chan);\n                <span class=\"pl-c1\">complex_phase</span>(src,<span class=\"pl-c1\">time</span>,antenna,chan) = { <span class=\"pl-c1\">std::cos</span>(real_phase), <span class=\"pl-c1\">std::sin</span>(real_phase) };\n            }\n        }\n    }\n}</pre></div>\n<h4>Eigen</h4>\n<ul>\n<li>Timings</li>\n</ul>\n<pre><code>Tensorflow custom GPU time 0.344653\nTensorflow expression GPU time 0.275525\nTensorflow CPU time 9.616667\nNumpy CPU time 2.505482\n</code></pre>\n<ul>\n<li>Code</li>\n</ul>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> Doing it this way might give us SIMD's and threading automatically...</span>\n<span class=\"pl-k\">const</span> CPUDevice &amp; device = context-&gt;eigen_device&lt;CPUDevice&gt;();\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Shapes for reshaping and broadcasting</span>\nEigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">4</span>&gt;   <span class=\"pl-en\">lm_shape</span>(nsrc, <span class=\"pl-c1\">1</span>,     <span class=\"pl-c1\">1</span>,  <span class=\"pl-c1\">1</span>    );\nEigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">4</span>&gt;  <span class=\"pl-en\">uvw_shape</span>(<span class=\"pl-c1\">1</span>,    ntime, na, <span class=\"pl-c1\">1</span>    );\nEigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">4</span>&gt; <span class=\"pl-en\">freq_shape</span>(<span class=\"pl-c1\">1</span>,    <span class=\"pl-c1\">1</span>,     <span class=\"pl-c1\">1</span>,  nchan);\n\n<span class=\"pl-k\">auto</span> l = lm.slice(\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">2</span>&gt;(<span class=\"pl-c1\">0</span>,    <span class=\"pl-c1\">0</span>),\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">2</span>&gt;(nsrc, <span class=\"pl-c1\">1</span>))\n    .reshape(lm_shape);\n<span class=\"pl-k\">auto</span> m = lm.slice(\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">2</span>&gt;(<span class=\"pl-c1\">0</span>,    <span class=\"pl-c1\">1</span>),\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">2</span>&gt;(nsrc, <span class=\"pl-c1\">1</span>))\n    .reshape(lm_shape);\n\n<span class=\"pl-k\">auto</span> u = uvw.slice(\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">3</span>&gt;(<span class=\"pl-c1\">0</span>,     <span class=\"pl-c1\">0</span>,  <span class=\"pl-c1\">0</span>),\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">3</span>&gt;(ntime, na, <span class=\"pl-c1\">1</span>))\n    .reshape(uvw_shape);\n\n<span class=\"pl-k\">auto</span> v = uvw.slice(\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">3</span>&gt;(<span class=\"pl-c1\">0</span>,     <span class=\"pl-c1\">0</span>,  <span class=\"pl-c1\">1</span>),\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">3</span>&gt;(ntime, na, <span class=\"pl-c1\">1</span>))\n    .reshape(uvw_shape);\n\n<span class=\"pl-k\">auto</span> w = uvw.slice(\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">3</span>&gt;(<span class=\"pl-c1\">0</span>,     <span class=\"pl-c1\">0</span>,  <span class=\"pl-c1\">2</span>),\n        Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">3</span>&gt;(ntime, na, <span class=\"pl-c1\">1</span>))\n    .reshape(uvw_shape);\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Compute n</span>\n<span class=\"pl-k\">auto</span> n = (l.constant(<span class=\"pl-c1\">1.0</span>) - l*l - m*m).sqrt() - l.constant(<span class=\"pl-c1\">1.0</span>);\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Compute the real phase</span>\n<span class=\"pl-k\">auto</span> real_phase = (\n    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +\n    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +\n    n.broadcast(uvw_shape)*w.broadcast(lm_shape))\n        .broadcast(freq_shape);\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Reshape and broadcast frequency to match real_phase</span>\n<span class=\"pl-k\">auto</span> f = frequency.reshape(freq_shape).broadcast(\n    Eigen::DSizes&lt;<span class=\"pl-k\">int</span>, <span class=\"pl-c1\">4</span>&gt;(nsrc, ntime, na, <span class=\"pl-c1\">1</span>));\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Calculate the phase</span>\n<span class=\"pl-k\">auto</span> phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);\n<span class=\"pl-k\">auto</span> sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op&lt;FT&gt;());\n<span class=\"pl-k\">auto</span> cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op&lt;FT&gt;());\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Now evaluate the complex phase on the device</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> by combining the cosine and sine of the phase</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> to form a complex number</span>\ncomplex_phase.device(device) = cosp.binaryExpr(\n    sinp, make_complex_functor&lt;FT&gt;());</pre></div>\n<h3>What other attempted solutions have you tried?</h3>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>", "body_text": "I implemented a phase CPU operator consisting of four loop levels. I couldn't find any Eigen tensor docs at that stage so I use OpenMP to trivially parallelise the outer loops. I recently found the Eigen tensor documentation so I thought I'd take advantage of it and get all the multithreading/AVX/SSE goodies for free!\nUnfortunately the Eigen version is about 10 times slower!\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nEnvironment info\nOperating System: Ubuntu 16.04\nInstalled version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\n-rw-r--r-- 1 root root 558720 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root     19 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rw-r--r-- 1 root root 415432 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root 775162 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\n\n$ ls -l /usr/local/cudnn-5.1-cuda-8.0/lib64/lib*\nlrwxrwxrwx 1 root root       13 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: python 2.7 linux GPU nightly\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so locally\n0.11.0rc1\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nSource code.\nMakefile. In terms of optimisations I'm using -O2 and -fopenmp\nTest Case and timing code.\n\nOpenMP\n\nTimings\n\nTensorflow custom GPU time 0.342187\nTensorflow expression GPU time 0.270267\nTensorflow CPU time 0.417076\nNumpy CPU time 2.542890\n\n\nCode\n\n// Compute the complex phase\n#pragma omp parallel for\nfor(int src=0; src<nsrc; ++src)\n{\n    FT l = lm(src,0);\n    FT m = lm(src,1);\n    FT n = std::sqrt(1.0 - l*l - m*m) - 1.0;\n\n    for(int time=0; time<ntime; ++time)\n    {\n        for(int antenna=0; antenna<na; ++antenna)\n        {\n            FT u = uvw(time,antenna,0);\n            FT v = uvw(time,antenna,1);\n            FT w = uvw(time,antenna,2);\n\n            FT real_phase_base = minus_two_pi_over_c*(l*u + m*v + n*w);\n\n            for(int chan=0; chan<nchan; ++chan)\n            {\n                // Our real phase input to the exponential function is purely imaginary so we can\n                // can elide a call to std::exp<complex<FT>> and just compute the cos and sin\n                FT real_phase = real_phase_base*frequency(chan);\n                complex_phase(src,time,antenna,chan) = { std::cos(real_phase), std::sin(real_phase) };\n            }\n        }\n    }\n}\nEigen\n\nTimings\n\nTensorflow custom GPU time 0.344653\nTensorflow expression GPU time 0.275525\nTensorflow CPU time 9.616667\nNumpy CPU time 2.505482\n\n\nCode\n\n// Doing it this way might give us SIMD's and threading automatically...\nconst CPUDevice & device = context->eigen_device<CPUDevice>();\n\n// Shapes for reshaping and broadcasting\nEigen::DSizes<int, 4>   lm_shape(nsrc, 1,     1,  1    );\nEigen::DSizes<int, 4>  uvw_shape(1,    ntime, na, 1    );\nEigen::DSizes<int, 4> freq_shape(1,    1,     1,  nchan);\n\nauto l = lm.slice(\n        Eigen::DSizes<int, 2>(0,    0),\n        Eigen::DSizes<int, 2>(nsrc, 1))\n    .reshape(lm_shape);\nauto m = lm.slice(\n        Eigen::DSizes<int, 2>(0,    1),\n        Eigen::DSizes<int, 2>(nsrc, 1))\n    .reshape(lm_shape);\n\nauto u = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  0),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\nauto v = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  1),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\nauto w = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  2),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\n// Compute n\nauto n = (l.constant(1.0) - l*l - m*m).sqrt() - l.constant(1.0);\n\n// Compute the real phase\nauto real_phase = (\n    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +\n    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +\n    n.broadcast(uvw_shape)*w.broadcast(lm_shape))\n        .broadcast(freq_shape);\n\n// Reshape and broadcast frequency to match real_phase\nauto f = frequency.reshape(freq_shape).broadcast(\n    Eigen::DSizes<int, 4>(nsrc, ntime, na, 1));\n\n// Calculate the phase\nauto phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);\nauto sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op<FT>());\nauto cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op<FT>());\n\n// Now evaluate the complex phase on the device\n// by combining the cosine and sine of the phase\n// to form a complex number\ncomplex_phase.device(device) = cosp.binaryExpr(\n    sinp, make_complex_functor<FT>());\nWhat other attempted solutions have you tried?\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).", "body": "I implemented a phase CPU operator consisting of four loop levels. I couldn't find any Eigen tensor docs at that stage so I use OpenMP to trivially parallelise the outer loops. I recently found the Eigen tensor documentation so I thought I'd take advantage of it and get all the multithreading/AVX/SSE goodies for free!\n\nUnfortunately the Eigen version is about 10 times slower!\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\n-rw-r--r-- 1 root root 558720 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root     19 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rw-r--r-- 1 root root 415432 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root 775162 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\n```\n\n```\n$ ls -l /usr/local/cudnn-5.1-cuda-8.0/lib64/lib*\nlrwxrwxrwx 1 root root       13 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed: python 2.7 linux GPU nightly\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so locally\n0.11.0rc1\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n- [Source code](https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/phase_op_cpu.h).\n- [Makefile](https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/Makefile). In terms of optimisations I'm using **-O2** and **-fopenmp**\n- [Test Case](https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/test_phase.py) and timing code.\n#### OpenMP\n- Timings\n\n```\nTensorflow custom GPU time 0.342187\nTensorflow expression GPU time 0.270267\nTensorflow CPU time 0.417076\nNumpy CPU time 2.542890\n```\n- Code\n\n``` cpp\n// Compute the complex phase\n#pragma omp parallel for\nfor(int src=0; src<nsrc; ++src)\n{\n    FT l = lm(src,0);\n    FT m = lm(src,1);\n    FT n = std::sqrt(1.0 - l*l - m*m) - 1.0;\n\n    for(int time=0; time<ntime; ++time)\n    {\n        for(int antenna=0; antenna<na; ++antenna)\n        {\n            FT u = uvw(time,antenna,0);\n            FT v = uvw(time,antenna,1);\n            FT w = uvw(time,antenna,2);\n\n            FT real_phase_base = minus_two_pi_over_c*(l*u + m*v + n*w);\n\n            for(int chan=0; chan<nchan; ++chan)\n            {\n                // Our real phase input to the exponential function is purely imaginary so we can\n                // can elide a call to std::exp<complex<FT>> and just compute the cos and sin\n                FT real_phase = real_phase_base*frequency(chan);\n                complex_phase(src,time,antenna,chan) = { std::cos(real_phase), std::sin(real_phase) };\n            }\n        }\n    }\n}\n```\n#### Eigen\n- Timings\n\n```\nTensorflow custom GPU time 0.344653\nTensorflow expression GPU time 0.275525\nTensorflow CPU time 9.616667\nNumpy CPU time 2.505482\n```\n- Code\n\n``` cpp\n// Doing it this way might give us SIMD's and threading automatically...\nconst CPUDevice & device = context->eigen_device<CPUDevice>();\n\n// Shapes for reshaping and broadcasting\nEigen::DSizes<int, 4>   lm_shape(nsrc, 1,     1,  1    );\nEigen::DSizes<int, 4>  uvw_shape(1,    ntime, na, 1    );\nEigen::DSizes<int, 4> freq_shape(1,    1,     1,  nchan);\n\nauto l = lm.slice(\n        Eigen::DSizes<int, 2>(0,    0),\n        Eigen::DSizes<int, 2>(nsrc, 1))\n    .reshape(lm_shape);\nauto m = lm.slice(\n        Eigen::DSizes<int, 2>(0,    1),\n        Eigen::DSizes<int, 2>(nsrc, 1))\n    .reshape(lm_shape);\n\nauto u = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  0),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\nauto v = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  1),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\nauto w = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  2),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\n// Compute n\nauto n = (l.constant(1.0) - l*l - m*m).sqrt() - l.constant(1.0);\n\n// Compute the real phase\nauto real_phase = (\n    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +\n    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +\n    n.broadcast(uvw_shape)*w.broadcast(lm_shape))\n        .broadcast(freq_shape);\n\n// Reshape and broadcast frequency to match real_phase\nauto f = frequency.reshape(freq_shape).broadcast(\n    Eigen::DSizes<int, 4>(nsrc, ntime, na, 1));\n\n// Calculate the phase\nauto phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);\nauto sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op<FT>());\nauto cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op<FT>());\n\n// Now evaluate the complex phase on the device\n// by combining the cosine and sine of the phase\n// to form a complex number\ncomplex_phase.device(device) = cosp.binaryExpr(\n    sinp, make_complex_functor<FT>());\n```\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n"}