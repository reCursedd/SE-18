{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4857", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4857/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4857/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4857/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4857", "id": 181894187, "node_id": "MDU6SXNzdWUxODE4OTQxODc=", "number": 4857, "title": "tf.extract_image_patches Trying stride only on row", "user": {"login": "ZijiaLewisLu", "id": 16264284, "node_id": "MDQ6VXNlcjE2MjY0Mjg0", "avatar_url": "https://avatars1.githubusercontent.com/u/16264284?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZijiaLewisLu", "html_url": "https://github.com/ZijiaLewisLu", "followers_url": "https://api.github.com/users/ZijiaLewisLu/followers", "following_url": "https://api.github.com/users/ZijiaLewisLu/following{/other_user}", "gists_url": "https://api.github.com/users/ZijiaLewisLu/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZijiaLewisLu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZijiaLewisLu/subscriptions", "organizations_url": "https://api.github.com/users/ZijiaLewisLu/orgs", "repos_url": "https://api.github.com/users/ZijiaLewisLu/repos", "events_url": "https://api.github.com/users/ZijiaLewisLu/events{/privacy}", "received_events_url": "https://api.github.com/users/ZijiaLewisLu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-10-09T15:54:15Z", "updated_at": "2018-07-03T21:44:05Z", "closed_at": "2016-10-17T01:56:37Z", "author_association": "NONE", "body_html": "<p>Hi, Tried to use tf.extract_image_patche() of a tensor [N, sequence_length, embeding_size, 1] to do n-gram with patch size [1,sequence_length-#gram+1, embedding_size, 1]. Thus I set the stride to be [1,1,0,1]. However this leds to the error: <code>ZeroDivisionError: integer division or modulo by zero</code></p>\n<p>Is there a way to only stride on one dim and avoid this error?</p>\n<p>below is the skeleton of my code and error message:</p>\n<pre lang=\"(python)\"><code>self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\nself.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\nslic = tf.(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n</code></pre>\n<pre><code>---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n/home/zijia/nlp/A2/train.py in &lt;module&gt;()\n     85             embedding_size=FLAGS.embedding_dim,\n     86             n_gram=FLAGS.n_gram,\n---&gt; 87             l2_reg_lambda=FLAGS.l2_reg_lambda)\n     88\n     89         # Define Training procedure\n\n/home/zijia/nlp/A2/text_cnn.py in __init__(self, sequence_length, num_classes, vocab_size, embedding_size, n_gram, l2_reg_lambda)\n     32                 grams = [ self.embedded_chars_expanded ]\n     33                 for i in range(2,n_gram+1):\n---&gt; 34                     slic = tf.extract_image_patches(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n     35                     print slic.get_shape()\n     36                     slic = tf.reduce_sum( slic, 3 )\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in extract_image_patches(images, ksizes, strides, rates, padding, name)\n    918   result = _op_def_lib.apply_op(\"ExtractImagePatches\", images=images,\n    919                                 ksizes=ksizes, strides=strides, rates=rates,\n--&gt; 920                                 padding=padding, name=name)\n    921   return result\n    922\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    701           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    702                            input_types=input_types, attrs=attr_protos,\n--&gt; 703                            op_def=op_def)\n    704           outputs = op.outputs\n    705           return _Restructure(ops.convert_n_to_tensor(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2317                     original_op=self._default_original_op, op_def=op_def)\n   2318     if compute_shapes:\n-&gt; 2319       set_shapes_for_outputs(ret)\n   2320     self._add_op(ret)\n   2321     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1709       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1710                          % op.type)\n-&gt; 1711   shapes = shape_func(op)\n   1712   if shapes is None:\n   1713     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _ExtractImagePatchesShape(op)\n   2290                                                             ksize_c_eff,\n   2291                                                             stride_r, stride_c,\n-&gt; 2292                                                             padding)\n   2293\n   2294   out_depth = None if in_depth is None else ksize_r * ksize_c * int(in_depth)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get2d_conv_output_size(input_height, input_width, filter_height, filter_width, row_stride, col_stride, padding_type)\n    182   return get_conv_output_size((input_height, input_width),\n    183                               (filter_height, filter_width),\n--&gt; 184                               (row_stride, col_stride), padding_type)\n    185\n    186\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get_conv_output_size(input_size, filter_size, strides, padding_type)\n    159     output_size = [\n    160         _valid(in_dim, k_dim, s_dim)\n--&gt; 161         for in_dim, k_dim, s_dim in zip(input_size, filter_size, strides)\n    162     ]\n    163   elif padding_type == b\"SAME\":\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _valid(in_dim, k_dim, s_dim)\n    153     def _valid(in_dim, k_dim, s_dim):\n    154       if in_dim is not None and k_dim is not None:\n--&gt; 155         return (in_dim - k_dim + s_dim) // s_dim\n    156       else:\n    157         return None\n\nZeroDivisionError: integer division or modulo by zero\n</code></pre>", "body_text": "Hi, Tried to use tf.extract_image_patche() of a tensor [N, sequence_length, embeding_size, 1] to do n-gram with patch size [1,sequence_length-#gram+1, embedding_size, 1]. Thus I set the stride to be [1,1,0,1]. However this leds to the error: ZeroDivisionError: integer division or modulo by zero\nIs there a way to only stride on one dim and avoid this error?\nbelow is the skeleton of my code and error message:\nself.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\nself.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\nslic = tf.(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n/home/zijia/nlp/A2/train.py in <module>()\n     85             embedding_size=FLAGS.embedding_dim,\n     86             n_gram=FLAGS.n_gram,\n---> 87             l2_reg_lambda=FLAGS.l2_reg_lambda)\n     88\n     89         # Define Training procedure\n\n/home/zijia/nlp/A2/text_cnn.py in __init__(self, sequence_length, num_classes, vocab_size, embedding_size, n_gram, l2_reg_lambda)\n     32                 grams = [ self.embedded_chars_expanded ]\n     33                 for i in range(2,n_gram+1):\n---> 34                     slic = tf.extract_image_patches(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n     35                     print slic.get_shape()\n     36                     slic = tf.reduce_sum( slic, 3 )\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in extract_image_patches(images, ksizes, strides, rates, padding, name)\n    918   result = _op_def_lib.apply_op(\"ExtractImagePatches\", images=images,\n    919                                 ksizes=ksizes, strides=strides, rates=rates,\n--> 920                                 padding=padding, name=name)\n    921   return result\n    922\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    701           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    702                            input_types=input_types, attrs=attr_protos,\n--> 703                            op_def=op_def)\n    704           outputs = op.outputs\n    705           return _Restructure(ops.convert_n_to_tensor(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2317                     original_op=self._default_original_op, op_def=op_def)\n   2318     if compute_shapes:\n-> 2319       set_shapes_for_outputs(ret)\n   2320     self._add_op(ret)\n   2321     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1709       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1710                          % op.type)\n-> 1711   shapes = shape_func(op)\n   1712   if shapes is None:\n   1713     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _ExtractImagePatchesShape(op)\n   2290                                                             ksize_c_eff,\n   2291                                                             stride_r, stride_c,\n-> 2292                                                             padding)\n   2293\n   2294   out_depth = None if in_depth is None else ksize_r * ksize_c * int(in_depth)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get2d_conv_output_size(input_height, input_width, filter_height, filter_width, row_stride, col_stride, padding_type)\n    182   return get_conv_output_size((input_height, input_width),\n    183                               (filter_height, filter_width),\n--> 184                               (row_stride, col_stride), padding_type)\n    185\n    186\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get_conv_output_size(input_size, filter_size, strides, padding_type)\n    159     output_size = [\n    160         _valid(in_dim, k_dim, s_dim)\n--> 161         for in_dim, k_dim, s_dim in zip(input_size, filter_size, strides)\n    162     ]\n    163   elif padding_type == b\"SAME\":\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _valid(in_dim, k_dim, s_dim)\n    153     def _valid(in_dim, k_dim, s_dim):\n    154       if in_dim is not None and k_dim is not None:\n--> 155         return (in_dim - k_dim + s_dim) // s_dim\n    156       else:\n    157         return None\n\nZeroDivisionError: integer division or modulo by zero", "body": "Hi, Tried to use tf.extract_image_patche() of a tensor [N, sequence_length, embeding_size, 1] to do n-gram with patch size [1,sequence_length-#gram+1, embedding_size, 1]. Thus I set the stride to be [1,1,0,1]. However this leds to the error: `ZeroDivisionError: integer division or modulo by zero`\n\nIs there a way to only stride on one dim and avoid this error?\n\nbelow is the skeleton of my code and error message:\n\n``` (python)\nself.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\nself.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\nslic = tf.(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n```\n\n```\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n/home/zijia/nlp/A2/train.py in <module>()\n     85             embedding_size=FLAGS.embedding_dim,\n     86             n_gram=FLAGS.n_gram,\n---> 87             l2_reg_lambda=FLAGS.l2_reg_lambda)\n     88\n     89         # Define Training procedure\n\n/home/zijia/nlp/A2/text_cnn.py in __init__(self, sequence_length, num_classes, vocab_size, embedding_size, n_gram, l2_reg_lambda)\n     32                 grams = [ self.embedded_chars_expanded ]\n     33                 for i in range(2,n_gram+1):\n---> 34                     slic = tf.extract_image_patches(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n     35                     print slic.get_shape()\n     36                     slic = tf.reduce_sum( slic, 3 )\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in extract_image_patches(images, ksizes, strides, rates, padding, name)\n    918   result = _op_def_lib.apply_op(\"ExtractImagePatches\", images=images,\n    919                                 ksizes=ksizes, strides=strides, rates=rates,\n--> 920                                 padding=padding, name=name)\n    921   return result\n    922\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    701           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    702                            input_types=input_types, attrs=attr_protos,\n--> 703                            op_def=op_def)\n    704           outputs = op.outputs\n    705           return _Restructure(ops.convert_n_to_tensor(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2317                     original_op=self._default_original_op, op_def=op_def)\n   2318     if compute_shapes:\n-> 2319       set_shapes_for_outputs(ret)\n   2320     self._add_op(ret)\n   2321     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1709       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1710                          % op.type)\n-> 1711   shapes = shape_func(op)\n   1712   if shapes is None:\n   1713     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _ExtractImagePatchesShape(op)\n   2290                                                             ksize_c_eff,\n   2291                                                             stride_r, stride_c,\n-> 2292                                                             padding)\n   2293\n   2294   out_depth = None if in_depth is None else ksize_r * ksize_c * int(in_depth)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get2d_conv_output_size(input_height, input_width, filter_height, filter_width, row_stride, col_stride, padding_type)\n    182   return get_conv_output_size((input_height, input_width),\n    183                               (filter_height, filter_width),\n--> 184                               (row_stride, col_stride), padding_type)\n    185\n    186\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get_conv_output_size(input_size, filter_size, strides, padding_type)\n    159     output_size = [\n    160         _valid(in_dim, k_dim, s_dim)\n--> 161         for in_dim, k_dim, s_dim in zip(input_size, filter_size, strides)\n    162     ]\n    163   elif padding_type == b\"SAME\":\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _valid(in_dim, k_dim, s_dim)\n    153     def _valid(in_dim, k_dim, s_dim):\n    154       if in_dim is not None and k_dim is not None:\n--> 155         return (in_dim - k_dim + s_dim) // s_dim\n    156       else:\n    157         return None\n\nZeroDivisionError: integer division or modulo by zero\n```\n"}