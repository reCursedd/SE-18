{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/355218060", "html_url": "https://github.com/tensorflow/tensorflow/issues/15821#issuecomment-355218060", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15821", "id": 355218060, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTIxODA2MA==", "user": {"login": "jppgks", "id": 11156808, "node_id": "MDQ6VXNlcjExMTU2ODA4", "avatar_url": "https://avatars3.githubusercontent.com/u/11156808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jppgks", "html_url": "https://github.com/jppgks", "followers_url": "https://api.github.com/users/jppgks/followers", "following_url": "https://api.github.com/users/jppgks/following{/other_user}", "gists_url": "https://api.github.com/users/jppgks/gists{/gist_id}", "starred_url": "https://api.github.com/users/jppgks/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jppgks/subscriptions", "organizations_url": "https://api.github.com/users/jppgks/orgs", "repos_url": "https://api.github.com/users/jppgks/repos", "events_url": "https://api.github.com/users/jppgks/events{/privacy}", "received_events_url": "https://api.github.com/users/jppgks/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-04T07:44:20Z", "updated_at": "2018-01-04T07:44:20Z", "author_association": "NONE", "body_html": "<p>Currently, I'm able to create a GAN using eager execution, without making use of TFGAN - <em>see <a href=\"https://github.com/jppgks/shenanigan/blob/master/1-generating-samples-from-1D-gaussian.ipynb\">notebook on generating samples from 1-D Gaussian</a> for context</em>:</p>\n<ol>\n<li>Enable eager</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\ntfe.enable_eager_execution()</pre></div>\n<ol start=\"2\">\n<li>Define the model</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">MLP</span>(<span class=\"pl-e\">tfe</span>.<span class=\"pl-e\">Network</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">MLP</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.layer1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.track_layer(tf.layers.Dense(<span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.tanh))\n    <span class=\"pl-c1\">self</span>.layer2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.track_layer(tf.layers.Dense(<span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.tanh))\n    \n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Actually runs the model.<span class=\"pl-pds\">\"\"\"</span></span>\n    result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layer1(<span class=\"pl-c1\">input</span>)\n    result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layer2(result)\n    <span class=\"pl-k\">return</span> result</pre></div>\n<ol start=\"3\">\n<li>Create generator and discriminator</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>G<span class=\"pl-pds\">\"</span></span>):\n  G <span class=\"pl-k\">=</span> MLP()\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>D<span class=\"pl-pds\">\"</span></span>):\n  D <span class=\"pl-k\">=</span> MLP()</pre></div>\n<ol start=\"4\">\n<li>Define optimizers, ...</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> For the generator</span>\nbatch_g <span class=\"pl-k\">=</span> tfe.Variable(<span class=\"pl-c1\">0</span>)\nlearning_rate <span class=\"pl-k\">=</span> tf.train.exponential_decay(\n  <span class=\"pl-c1\">0.001</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Base learning rate.</span>\n  batch_g,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Current index into the dataset.</span>\n  <span class=\"pl-c1\">TRAIN_ITERS</span> <span class=\"pl-k\">//</span> <span class=\"pl-c1\">4</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Decay step - this decays 4 times throughout training process.</span>\n  <span class=\"pl-c1\">0.95</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Decay rate.</span>\n  <span class=\"pl-v\">staircase</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nopt_g<span class=\"pl-k\">=</span>tf.train.MomentumOptimizer(learning_rate, <span class=\"pl-c1\">0.6</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> For the discriminator</span>\nbatch <span class=\"pl-k\">=</span> tfe.Variable(<span class=\"pl-c1\">0</span>)\nlearning_rate <span class=\"pl-k\">=</span> tf.train.exponential_decay(\n  <span class=\"pl-c1\">0.001</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Base learning rate.</span>\n  batch,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Current index into the dataset.</span>\n  <span class=\"pl-c1\">TRAIN_ITERS</span> <span class=\"pl-k\">//</span> <span class=\"pl-c1\">4</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Decay step - this decays 4 times throughout training process.</span>\n  <span class=\"pl-c1\">0.95</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Decay rate.</span>\n  <span class=\"pl-v\">staircase</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ndiscriminator_optimizer<span class=\"pl-k\">=</span>tf.train.MomentumOptimizer(learning_rate, <span class=\"pl-c1\">0.6</span>)</pre></div>\n<ol start=\"5\">\n<li>...and value functions</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Used when taking tf.log(), so we don't end up doing tf.log(0)</span>\n<span class=\"pl-c1\">TINY</span> <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">1e-8</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float64)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">value_function</span>(<span class=\"pl-smi\">D</span>, <span class=\"pl-smi\">G</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">z</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> D(x), x sampled from data distribution</span>\n  p_norm_data <span class=\"pl-k\">=</span> D(x) <span class=\"pl-c\"><span class=\"pl-c\">#</span> output likelihood of being normally distributed</span>\n  D1 <span class=\"pl-k\">=</span> tf.maximum(tf.minimum(p_norm_data, <span class=\"pl-c1\">.99</span>), <span class=\"pl-c1\">0.01</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> clamp as a probability</span>\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> D(x), x sampled from model distribution as G(z)</span>\n  generated_samples <span class=\"pl-k\">=</span> G(z)\n  generated_samples <span class=\"pl-k\">=</span> tf.multiply(<span class=\"pl-c1\">5.0</span>, generated_samples)\n  p_norm_gen <span class=\"pl-k\">=</span> D(generated_samples)\n  D2 <span class=\"pl-k\">=</span> tf.maximum(tf.minimum(p_norm_gen, <span class=\"pl-c1\">.99</span>), <span class=\"pl-c1\">0.01</span>)\n  \n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> v(\\theta^g, \\theta^d)</span>\n  loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.log(D1 <span class=\"pl-k\">+</span> <span class=\"pl-c1\">TINY</span>) <span class=\"pl-k\">+</span> tf.log(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> D2 <span class=\"pl-k\">+</span> <span class=\"pl-c1\">TINY</span>))\n  <span class=\"pl-k\">return</span> loss\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">neg_value_function</span>(<span class=\"pl-smi\">D</span>, <span class=\"pl-smi\">G</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">z</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Because our optimizers will minimize the loss function passed in,</span>\n<span class=\"pl-s\">  we define the _negative_ value function.</span>\n<span class=\"pl-s\">  </span>\n<span class=\"pl-s\">  Instead of maximizing the value function, </span>\n<span class=\"pl-s\">  the discriminator will minimize this _negative_ value function.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">'''</span></span>\n  <span class=\"pl-k\">return</span> <span class=\"pl-k\">-</span>value_function(D, G, x, z)</pre></div>\n<ol start=\"6\">\n<li>Train</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Algorithm 1 of Goodfellow et al. (2014)</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Number of discriminator updates per epoch</span>\nk <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Setup log of loss throughout training</span>\nhistd, histg <span class=\"pl-k\">=</span> np.zeros(<span class=\"pl-c1\">TRAIN_ITERS</span>), np.zeros(<span class=\"pl-c1\">TRAIN_ITERS</span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">TRAIN_ITERS</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Train discriminator.</span>\n  <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(k):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># 1) Sample data,</span>\n    x <span class=\"pl-k\">=</span> sample_data(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>M)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># 2) Sample noise prior,</span>\n    z <span class=\"pl-k\">=</span> sample_noise_prior(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>M)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># 3) Compute all partial derivatives of the loss function,</span>\n    histd[i], grads_and_vars <span class=\"pl-k\">=</span> tfe.implicit_value_and_gradients(neg_value_function)(D, G, x, z)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># 4) Optimize discriminator, and not generator (!).</span>\n    discriminator_grads_and_vars <span class=\"pl-k\">=</span> [gv <span class=\"pl-k\">for</span> gv <span class=\"pl-k\">in</span> grads_and_vars <span class=\"pl-k\">if</span> gv[<span class=\"pl-c1\">1</span>].name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>D/<span class=\"pl-pds\">'</span></span>)]\n    discriminator_optimizer.apply_gradients(discriminator_grads_and_vars, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>batch)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Train generator.</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span># 1) Sample noise prior,</span>\n  z <span class=\"pl-k\">=</span> sample_noise_prior(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>M)\n  \n  <span class=\"pl-c\"><span class=\"pl-c\">#</span># 2) Compute all partial derivatives of the loss function</span>\n  histg[i], grads_and_vars <span class=\"pl-k\">=</span> tfe.implicit_value_and_gradients(value_function)(D, G, x, z)\n  \n  <span class=\"pl-c\"><span class=\"pl-c\">#</span># 3) Optimize generator, and not discriminator (!)</span>\n  generator_grads_and_vars <span class=\"pl-k\">=</span> [gv <span class=\"pl-k\">for</span> gv <span class=\"pl-k\">in</span> grads_and_vars <span class=\"pl-k\">if</span> gv[<span class=\"pl-c1\">1</span>].name.startswith(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>G/<span class=\"pl-pds\">'</span></span>)]\n  opt_g.apply_gradients(generator_grads_and_vars, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>batch)</pre></div>", "body_text": "Currently, I'm able to create a GAN using eager execution, without making use of TFGAN - see notebook on generating samples from 1-D Gaussian for context:\n\nEnable eager\n\nimport tensorflow.contrib.eager as tfe\ntfe.enable_eager_execution()\n\nDefine the model\n\nclass MLP(tfe.Network):\n  def __init__(self):\n    super(MLP, self).__init__()\n    self.layer1 = self.track_layer(tf.layers.Dense(units=30, activation=tf.nn.tanh))\n    self.layer2 = self.track_layer(tf.layers.Dense(units=1, activation=tf.nn.tanh))\n    \n  def call(self, input):\n    \"\"\"Actually runs the model.\"\"\"\n    result = self.layer1(input)\n    result = self.layer2(result)\n    return result\n\nCreate generator and discriminator\n\nwith tf.variable_scope(\"G\"):\n  G = MLP()\nwith tf.variable_scope(\"D\"):\n  D = MLP()\n\nDefine optimizers, ...\n\n# For the generator\nbatch_g = tfe.Variable(0)\nlearning_rate = tf.train.exponential_decay(\n  0.001,  # Base learning rate.\n  batch_g,  # Current index into the dataset.\n  TRAIN_ITERS // 4,  # Decay step - this decays 4 times throughout training process.\n  0.95,  # Decay rate.\n  staircase=True)\nopt_g=tf.train.MomentumOptimizer(learning_rate, 0.6)\n\n# For the discriminator\nbatch = tfe.Variable(0)\nlearning_rate = tf.train.exponential_decay(\n  0.001,  # Base learning rate.\n  batch,  # Current index into the dataset.\n  TRAIN_ITERS // 4,  # Decay step - this decays 4 times throughout training process.\n  0.95,  # Decay rate.\n  staircase=True)\ndiscriminator_optimizer=tf.train.MomentumOptimizer(learning_rate, 0.6)\n\n...and value functions\n\n# Used when taking tf.log(), so we don't end up doing tf.log(0)\nTINY = tf.constant(1e-8, dtype=tf.float64)\n\ndef value_function(D, G, x, z):\n  # D(x), x sampled from data distribution\n  p_norm_data = D(x) # output likelihood of being normally distributed\n  D1 = tf.maximum(tf.minimum(p_norm_data, .99), 0.01) # clamp as a probability\n\n  # D(x), x sampled from model distribution as G(z)\n  generated_samples = G(z)\n  generated_samples = tf.multiply(5.0, generated_samples)\n  p_norm_gen = D(generated_samples)\n  D2 = tf.maximum(tf.minimum(p_norm_gen, .99), 0.01)\n  \n  # v(\\theta^g, \\theta^d)\n  loss = tf.reduce_mean(tf.log(D1 + TINY) + tf.log(1 - D2 + TINY))\n  return loss\n\ndef neg_value_function(D, G, x, z):\n  '''Because our optimizers will minimize the loss function passed in,\n  we define the _negative_ value function.\n  \n  Instead of maximizing the value function, \n  the discriminator will minimize this _negative_ value function.\n  '''\n  return -value_function(D, G, x, z)\n\nTrain\n\n# Algorithm 1 of Goodfellow et al. (2014)\n\n# Number of discriminator updates per epoch\nk = 5\n# Setup log of loss throughout training\nhistd, histg = np.zeros(TRAIN_ITERS), np.zeros(TRAIN_ITERS)\n\nfor i in range(TRAIN_ITERS):\n  # Train discriminator.\n  for j in range(k):\n    ## 1) Sample data,\n    x = sample_data(batch_size=M)\n    \n    ## 2) Sample noise prior,\n    z = sample_noise_prior(batch_size=M)\n\n    ## 3) Compute all partial derivatives of the loss function,\n    histd[i], grads_and_vars = tfe.implicit_value_and_gradients(neg_value_function)(D, G, x, z)\n    \n    ## 4) Optimize discriminator, and not generator (!).\n    discriminator_grads_and_vars = [gv for gv in grads_and_vars if gv[1].name.startswith('D/')]\n    discriminator_optimizer.apply_gradients(discriminator_grads_and_vars, global_step=batch)\n\n  # Train generator.\n  ## 1) Sample noise prior,\n  z = sample_noise_prior(batch_size=M)\n  \n  ## 2) Compute all partial derivatives of the loss function\n  histg[i], grads_and_vars = tfe.implicit_value_and_gradients(value_function)(D, G, x, z)\n  \n  ## 3) Optimize generator, and not discriminator (!)\n  generator_grads_and_vars = [gv for gv in grads_and_vars if gv[1].name.startswith('G/')]\n  opt_g.apply_gradients(generator_grads_and_vars, global_step=batch)", "body": "Currently, I'm able to create a GAN using eager execution, without making use of TFGAN - _see [notebook on generating samples from 1-D Gaussian](https://github.com/jppgks/shenanigan/blob/master/1-generating-samples-from-1D-gaussian.ipynb) for context_:\r\n\r\n1. Enable eager\r\n\r\n```python\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n```\r\n\r\n2. Define the model\r\n\r\n```python\r\nclass MLP(tfe.Network):\r\n  def __init__(self):\r\n    super(MLP, self).__init__()\r\n    self.layer1 = self.track_layer(tf.layers.Dense(units=30, activation=tf.nn.tanh))\r\n    self.layer2 = self.track_layer(tf.layers.Dense(units=1, activation=tf.nn.tanh))\r\n    \r\n  def call(self, input):\r\n    \"\"\"Actually runs the model.\"\"\"\r\n    result = self.layer1(input)\r\n    result = self.layer2(result)\r\n    return result\r\n```\r\n\r\n3. Create generator and discriminator\r\n\r\n```python\r\nwith tf.variable_scope(\"G\"):\r\n  G = MLP()\r\nwith tf.variable_scope(\"D\"):\r\n  D = MLP()\r\n```\r\n\r\n4. Define optimizers, ...\r\n\r\n```python\r\n# For the generator\r\nbatch_g = tfe.Variable(0)\r\nlearning_rate = tf.train.exponential_decay(\r\n  0.001,  # Base learning rate.\r\n  batch_g,  # Current index into the dataset.\r\n  TRAIN_ITERS // 4,  # Decay step - this decays 4 times throughout training process.\r\n  0.95,  # Decay rate.\r\n  staircase=True)\r\nopt_g=tf.train.MomentumOptimizer(learning_rate, 0.6)\r\n\r\n# For the discriminator\r\nbatch = tfe.Variable(0)\r\nlearning_rate = tf.train.exponential_decay(\r\n  0.001,  # Base learning rate.\r\n  batch,  # Current index into the dataset.\r\n  TRAIN_ITERS // 4,  # Decay step - this decays 4 times throughout training process.\r\n  0.95,  # Decay rate.\r\n  staircase=True)\r\ndiscriminator_optimizer=tf.train.MomentumOptimizer(learning_rate, 0.6)\r\n```\r\n\r\n5. ...and value functions\r\n\r\n```python\r\n# Used when taking tf.log(), so we don't end up doing tf.log(0)\r\nTINY = tf.constant(1e-8, dtype=tf.float64)\r\n\r\ndef value_function(D, G, x, z):\r\n  # D(x), x sampled from data distribution\r\n  p_norm_data = D(x) # output likelihood of being normally distributed\r\n  D1 = tf.maximum(tf.minimum(p_norm_data, .99), 0.01) # clamp as a probability\r\n\r\n  # D(x), x sampled from model distribution as G(z)\r\n  generated_samples = G(z)\r\n  generated_samples = tf.multiply(5.0, generated_samples)\r\n  p_norm_gen = D(generated_samples)\r\n  D2 = tf.maximum(tf.minimum(p_norm_gen, .99), 0.01)\r\n  \r\n  # v(\\theta^g, \\theta^d)\r\n  loss = tf.reduce_mean(tf.log(D1 + TINY) + tf.log(1 - D2 + TINY))\r\n  return loss\r\n\r\ndef neg_value_function(D, G, x, z):\r\n  '''Because our optimizers will minimize the loss function passed in,\r\n  we define the _negative_ value function.\r\n  \r\n  Instead of maximizing the value function, \r\n  the discriminator will minimize this _negative_ value function.\r\n  '''\r\n  return -value_function(D, G, x, z)\r\n```\r\n\r\n6. Train\r\n\r\n```python\r\n# Algorithm 1 of Goodfellow et al. (2014)\r\n\r\n# Number of discriminator updates per epoch\r\nk = 5\r\n# Setup log of loss throughout training\r\nhistd, histg = np.zeros(TRAIN_ITERS), np.zeros(TRAIN_ITERS)\r\n\r\nfor i in range(TRAIN_ITERS):\r\n  # Train discriminator.\r\n  for j in range(k):\r\n    ## 1) Sample data,\r\n    x = sample_data(batch_size=M)\r\n    \r\n    ## 2) Sample noise prior,\r\n    z = sample_noise_prior(batch_size=M)\r\n\r\n    ## 3) Compute all partial derivatives of the loss function,\r\n    histd[i], grads_and_vars = tfe.implicit_value_and_gradients(neg_value_function)(D, G, x, z)\r\n    \r\n    ## 4) Optimize discriminator, and not generator (!).\r\n    discriminator_grads_and_vars = [gv for gv in grads_and_vars if gv[1].name.startswith('D/')]\r\n    discriminator_optimizer.apply_gradients(discriminator_grads_and_vars, global_step=batch)\r\n\r\n  # Train generator.\r\n  ## 1) Sample noise prior,\r\n  z = sample_noise_prior(batch_size=M)\r\n  \r\n  ## 2) Compute all partial derivatives of the loss function\r\n  histg[i], grads_and_vars = tfe.implicit_value_and_gradients(value_function)(D, G, x, z)\r\n  \r\n  ## 3) Optimize generator, and not discriminator (!)\r\n  generator_grads_and_vars = [gv for gv in grads_and_vars if gv[1].name.startswith('G/')]\r\n  opt_g.apply_gradients(generator_grads_and_vars, global_step=batch)\r\n```"}