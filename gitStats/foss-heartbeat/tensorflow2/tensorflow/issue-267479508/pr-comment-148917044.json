{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/148917044", "pull_request_review_id": 74240685, "id": 148917044, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0ODkxNzA0NA==", "diff_hunk": "@@ -0,0 +1,376 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// The algorithm for dynamic partition has the following steps:\n+// 1. Let N be the size of partitions. We initialize a new vector indices_in\n+//    with the values 0, 1, 2, ..., N-1.\n+// 2. We apply cub::DeviceRadixSort::SortPairs to the key - value pairs given\n+//    by partitions and indices_in. This will result in two new vectors\n+//    partitions_out and indices_out, with partitions_out sorted.\n+// 3. The first dimension of outputs[i] is equal to the length of the interval\n+//    of i-values in partitions_out. We determine it in two steps:\n+//    - compute the starting and ending point of each interval,\n+//    - subtract the starting and ending points to find the length.\n+//    The result is placed in partition_count.", "path": "tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc", "position": 26, "original_position": 26, "commit_id": "1ec44ca2fe337eccc043fe41171316e8b05a8c8a", "original_commit_id": "1ec44ca2fe337eccc043fe41171316e8b05a8c8a", "user": {"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}, "body": "Just looking at this again and realized there is a faster / less custom code way of determining the partition counts:  use reduce_by_key from cub: http://nvlabs.github.io/cub/structcub_1_1_device_reduce.html#a303ae673ac32825f95912b4bfff8bef1\r\n\r\nThe sorted partitions would be the keys and a constant_iterator of 1 would be the values.  It should do less passes over the memory than the current implementation.  Not critical though, so do it if you find it interesting.", "created_at": "2017-11-04T00:30:14Z", "updated_at": "2017-11-04T00:30:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13905#discussion_r148917044", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13905", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/148917044"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13905#discussion_r148917044"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13905"}}, "body_html": "<p>Just looking at this again and realized there is a faster / less custom code way of determining the partition counts:  use reduce_by_key from cub: <a href=\"http://nvlabs.github.io/cub/structcub_1_1_device_reduce.html#a303ae673ac32825f95912b4bfff8bef1\" rel=\"nofollow\">http://nvlabs.github.io/cub/structcub_1_1_device_reduce.html#a303ae673ac32825f95912b4bfff8bef1</a></p>\n<p>The sorted partitions would be the keys and a constant_iterator of 1 would be the values.  It should do less passes over the memory than the current implementation.  Not critical though, so do it if you find it interesting.</p>", "body_text": "Just looking at this again and realized there is a faster / less custom code way of determining the partition counts:  use reduce_by_key from cub: http://nvlabs.github.io/cub/structcub_1_1_device_reduce.html#a303ae673ac32825f95912b4bfff8bef1\nThe sorted partitions would be the keys and a constant_iterator of 1 would be the values.  It should do less passes over the memory than the current implementation.  Not critical though, so do it if you find it interesting."}