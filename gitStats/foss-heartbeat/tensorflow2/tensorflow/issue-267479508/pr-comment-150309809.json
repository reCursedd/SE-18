{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/150309809", "pull_request_review_id": 75847582, "id": 150309809, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDMwOTgwOQ==", "diff_hunk": "@@ -0,0 +1,376 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// The algorithm for dynamic partition has the following steps:\n+// 1. Let N be the size of partitions. We initialize a new vector indices_in\n+//    with the values 0, 1, 2, ..., N-1.\n+// 2. We apply cub::DeviceRadixSort::SortPairs to the key - value pairs given\n+//    by partitions and indices_in. This will result in two new vectors\n+//    partitions_out and indices_out, with partitions_out sorted.\n+// 3. The first dimension of outputs[i] is equal to the length of the interval\n+//    of i-values in partitions_out. We determine it in two steps:\n+//    - compute the starting and ending point of each interval,\n+//    - subtract the starting and ending points to find the length.\n+//    The result is placed in partition_count.", "path": "tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc", "position": 26, "original_position": 26, "commit_id": "1ec44ca2fe337eccc043fe41171316e8b05a8c8a", "original_commit_id": "1ec44ca2fe337eccc043fe41171316e8b05a8c8a", "user": {"login": "codrut3", "id": 10788581, "node_id": "MDQ6VXNlcjEwNzg4NTgx", "avatar_url": "https://avatars1.githubusercontent.com/u/10788581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/codrut3", "html_url": "https://github.com/codrut3", "followers_url": "https://api.github.com/users/codrut3/followers", "following_url": "https://api.github.com/users/codrut3/following{/other_user}", "gists_url": "https://api.github.com/users/codrut3/gists{/gist_id}", "starred_url": "https://api.github.com/users/codrut3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/codrut3/subscriptions", "organizations_url": "https://api.github.com/users/codrut3/orgs", "repos_url": "https://api.github.com/users/codrut3/repos", "events_url": "https://api.github.com/users/codrut3/events{/privacy}", "received_events_url": "https://api.github.com/users/codrut3/received_events", "type": "User", "site_admin": false}, "body": "Actually, I think it's not that easy. The problem is that there could be wrong indices in partitions, which are larger than num_partitions. cub::ReduceByKey does not check the input for correctness and assumes the output array d_unique_out is large enough to contain all the distinct keys. But this need not be the case if the input is wrong. This would result in a seg fault or some other memory corruption. On the other hand, my custom kernel simply ignores out-of-bounds indices.\r\n It would be possible to do a check first, but then I would have to wait for the check to complete, bring the result to the CPU, and decide whether I should continue the computation. I think it's easier to keep the version which just ignores the out-of-bound indices.", "created_at": "2017-11-10T18:47:06Z", "updated_at": "2017-11-10T18:47:06Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13905#discussion_r150309809", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13905", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/150309809"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13905#discussion_r150309809"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13905"}}, "body_html": "<p>Actually, I think it's not that easy. The problem is that there could be wrong indices in partitions, which are larger than num_partitions. cub::ReduceByKey does not check the input for correctness and assumes the output array d_unique_out is large enough to contain all the distinct keys. But this need not be the case if the input is wrong. This would result in a seg fault or some other memory corruption. On the other hand, my custom kernel simply ignores out-of-bounds indices.<br>\nIt would be possible to do a check first, but then I would have to wait for the check to complete, bring the result to the CPU, and decide whether I should continue the computation. I think it's easier to keep the version which just ignores the out-of-bound indices.</p>", "body_text": "Actually, I think it's not that easy. The problem is that there could be wrong indices in partitions, which are larger than num_partitions. cub::ReduceByKey does not check the input for correctness and assumes the output array d_unique_out is large enough to contain all the distinct keys. But this need not be the case if the input is wrong. This would result in a seg fault or some other memory corruption. On the other hand, my custom kernel simply ignores out-of-bounds indices.\nIt would be possible to do a check first, but then I would have to wait for the check to complete, bring the result to the CPU, and decide whether I should continue the computation. I think it's easier to keep the version which just ignores the out-of-bound indices.", "in_reply_to_id": 148917044}