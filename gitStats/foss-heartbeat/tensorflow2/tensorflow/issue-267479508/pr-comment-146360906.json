{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146360906", "pull_request_review_id": 71285997, "id": 146360906, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjM2MDkwNg==", "diff_hunk": "@@ -0,0 +1,331 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#if GOOGLE_CUDA\n+\n+#define EIGEN_USE_GPU\n+\n+#include \"external/cub_archive/cub/device/device_radix_sort.cuh\"\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n+#include \"tensorflow/core/kernels/fill_functor.h\"\n+#include \"tensorflow/core/kernels/gather_functor_gpu.cu.h\"\n+#include \"tensorflow/core/util/cuda_kernel_helper.h\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename T>\n+__global__ void RangeInitKernel(const T start, const T delta, const int32 size,\n+                                T* out) {\n+  CUDA_1D_KERNEL_LOOP(i, size) { out[i] = start + i * delta; }\n+}\n+\n+__global__ void FindEndpointsKernel(const int32* partitions, int32 size,\n+                                    int32 nump, int32* start, int32* end) {\n+  CUDA_1D_KERNEL_LOOP(i, size) {\n+    int32 current = ldg(partitions + i);\n+    if (FastBoundsCheck(current, nump)) {\n+      if (i == 0)\n+        start[current] = i;\n+      else {\n+        int32 before = ldg(partitions + i - 1);\n+        if (before != current) start[current] = i;\n+      }\n+      if (i == size - 1)\n+        end[current] = i + 1;\n+      else {\n+        int32 after = ldg(partitions + i + 1);\n+        if (after != current) end[current] = i + 1;\n+      }\n+    }\n+  }\n+}\n+\n+// We create a local version of subtract, because the tf.subtract kernel\n+// is not defined for int32. We use it to compute the length of an interval\n+// by subtracting the endpoints.\n+__global__ void IntervalLengthKernel(int32* start, int32 size, int32* end) {\n+  CUDA_1D_KERNEL_LOOP(i, size) {\n+    int32 start_point = ldg(start + i);\n+    end[i] = end[i] - start_point;\n+  }\n+}\n+\n+// Initialize out with range start, start + delta, start + 2 * delta, ...\n+// This is needed because tf.range has no GPU implementation.\n+template <typename T>\n+void RangeInit(const GPUDevice& d, const T start, const T delta,\n+               const int32 size, typename TTypes<T>::Flat out) {\n+  CudaLaunchConfig config = GetCudaLaunchConfig(size, d);\n+  RangeInitKernel<\n+      T><<<config.block_count, config.thread_per_block, 0, d.stream()>>>(\n+      start, delta, size, out.data());\n+}\n+\n+// Partitions is a sorted vector of N non-negative integer numbers.\n+// This function computes the starting and ending points of each interval\n+// of values.\n+void ComputeIntervals(const GPUDevice& d, Tensor* partitions, int32 N,\n+                      int32 nump, int32* start_ptr, int32* end_ptr) {\n+  CudaLaunchConfig config = GetCudaLaunchConfig(N, d);\n+  FindEndpointsKernel<<<config.block_count, config.thread_per_block, 0,\n+                        d.stream()>>>(partitions->flat<int32>().data(), N, nump,\n+                                      start_ptr, end_ptr);\n+}\n+\n+// Subtract the ending points of each interval to obtain the interval length.\n+void ComputeItvLength(const GPUDevice& d, int32 num, int32* start_ptr,\n+                      int32* end_ptr) {\n+  CudaLaunchConfig config = GetCudaLaunchConfig(num, d);\n+  IntervalLengthKernel<<<config.block_count, config.thread_per_block, 0,\n+                         d.stream()>>>(start_ptr, num, end_ptr);\n+}\n+\n+template <typename T>\n+void CallGatherKernel(const GPUDevice& d, const T* params, const int32* indices,\n+                      T* out, int64 gather_dim_size, int64 indices_size,\n+                      int64 slice_size, int64 out_size) {\n+  CudaLaunchConfig config = GetCudaLaunchConfig(out_size, d);\n+  GatherOpKernel<\n+      T, int32,\n+      true><<<config.block_count, config.thread_per_block, 0, d.stream()>>>(\n+      params, indices, out, gather_dim_size, indices_size, slice_size,\n+      out_size);\n+}\n+\n+// The current implementation has memory cost on GPU\n+// I + P + max(3N + R, O + N), where:\n+// I - the size of the input\n+// N - the size of the partitions tensor\n+// R - the temporary storage used by cub::RadixSort, about 2N\n+// P - the number of partitions\n+// O - the size of the output\n+// So roughly the cost is I + P + max(5N, O + N).\n+template <typename T>\n+class DynamicPartitionOpGPU : public AsyncOpKernel {\n+ public:\n+  explicit DynamicPartitionOpGPU(OpKernelConstruction* c) : AsyncOpKernel(c) {\n+    OP_REQUIRES_OK(c, c->GetAttr(\"num_partitions\", &num_partitions_));\n+  }\n+\n+  void AllocateTempSpace(OpKernelContext* c, int32 N, Tensor* indices_in,\n+                         Tensor* partitions_out, Tensor* indices_out,\n+                         DoneCallback done) {\n+    int32 M = std::max(N, num_partitions_);\n+    // indices_in will be made slightly larger to accomodate\n+    // later computations.\n+    OP_REQUIRES_OK_ASYNC(\n+        c, c->allocate_temp(DT_INT32, TensorShape({M}), indices_in), done);\n+    OP_REQUIRES_OK_ASYNC(\n+        c, c->allocate_temp(DT_INT32, TensorShape({N}), partitions_out), done);\n+    OP_REQUIRES_OK_ASYNC(\n+        c, c->allocate_temp(DT_INT32, TensorShape({N}), indices_out), done);\n+  }\n+\n+  void AllocateOutputs(OpKernelContext* c, const Tensor* data,\n+                       const Tensor* partitions, const Tensor* partition_count,\n+                       OpOutputList* Tout, DoneCallback done) {\n+    auto e_part_count = partition_count->flat<int32>();\n+    // Allocate output tensors of the right size\n+    OP_REQUIRES_OK_ASYNC(c, c->output_list(\"outputs\", Tout), done);\n+    for (int p = 0; p < num_partitions_; p++) {\n+      TensorShape shape;\n+      shape.AddDim(e_part_count(p));\n+      for (int i = partitions->dims(); i < data->dims(); i++) {\n+        shape.AddDim(data->dim_size(i));\n+      }\n+      Tensor* out;\n+      OP_REQUIRES_OK_ASYNC(c, Tout->allocate(p, shape, &out), done);\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* c, DoneCallback done) {\n+    const Tensor& data = c->input(0);\n+    const Tensor& partitions = c->input(1);\n+\n+    OP_REQUIRES_ASYNC(\n+        c, TensorShapeUtils::StartsWith(data.shape(), partitions.shape()),\n+        errors::InvalidArgument(\"data.shape must start with partitions.shape, \",\n+                                \"got data.shape = \", data.shape().DebugString(),\n+                                \", partitions.shape = \",\n+                                partitions.shape().DebugString()),\n+        done);\n+\n+    Tensor partition_count;\n+    // Prepare for counting.\n+    OP_REQUIRES_OK_ASYNC(\n+        c, c->allocate_temp(DT_INT32, TensorShape({num_partitions_}),\n+                            &partition_count),\n+        done);\n+    Tensor indices_out;\n+    // Count how many times each partition index occurs.\n+    // Also sort the info in partitions and output it in indices_out,\n+    // in preparation for the next step.\n+    this->CountAndSortParts(c, &partitions, &partition_count, &indices_out,\n+                            done);\n+    if (!c->status().ok()) return;\n+\n+    // In order to allocate the output tensor we have to move partition_count\n+    // to CPU.\n+    auto* stream = c->op_device_context()->stream();", "path": "tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc", "position": 230, "original_position": 188, "commit_id": "1ec44ca2fe337eccc043fe41171316e8b05a8c8a", "original_commit_id": "7f43b5cd8055d09b1152faa791050bed033430b9", "user": {"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}, "body": "Please use GetCudaStream to get the stream instead; there are some edge cases with async ops that it handles better.  We ran into these problems with the GPU version of the where op.", "created_at": "2017-10-23T18:55:46Z", "updated_at": "2017-11-02T22:03:25Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13905#discussion_r146360906", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13905", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146360906"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13905#discussion_r146360906"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13905"}}, "body_html": "<p>Please use GetCudaStream to get the stream instead; there are some edge cases with async ops that it handles better.  We ran into these problems with the GPU version of the where op.</p>", "body_text": "Please use GetCudaStream to get the stream instead; there are some edge cases with async ops that it handles better.  We ran into these problems with the GPU version of the where op."}