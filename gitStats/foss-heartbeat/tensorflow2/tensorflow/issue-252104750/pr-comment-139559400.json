{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/139559400", "pull_request_review_id": 63509403, "id": 139559400, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzOTU1OTQwMA==", "diff_hunk": "@@ -26,6 +26,16 @@ limitations under the License.\n #include \"tensorflow/core/platform/stream_executor.h\"\n #include \"tensorflow/core/platform/types.h\"\n \n+#if defined(__CUDACC_VER_MAJOR__) && __CUDACC_VER_MAJOR__ < 9\n+__device__ inline void __syncwarp(unsigned mask=0xFFFFFFFF) {}\n+\n+#define __ballot_sync(mask, predicate)              __ballot(predicate)\n+#define __shfl_sync(mask, val, srcLane, width)      __shfl(val, srcLane, width)\n+#define __shfl_down_sync(mask, val, delta, width)   __shfl_down(val, delta, width)\n+#define __shfl_up_sync(mask, val, delta, width)     __shfl_up(val, delta, width)\n+#define __shfl_xor_sync(mask, val, laneMask, width) __shfl_xor(val, laneMask, width)\n+#endif", "path": "tensorflow/core/util/cuda_kernel_helper.h", "position": 26, "original_position": 12, "commit_id": "ef71383cf2d8d8241d813ac11695a26c34ae3ceb", "original_commit_id": "c4f7d053a667974edda7335585502597f66343c8", "user": {"login": "Artem-B", "id": 526795, "node_id": "MDQ6VXNlcjUyNjc5NQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/526795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Artem-B", "html_url": "https://github.com/Artem-B", "followers_url": "https://api.github.com/users/Artem-B/followers", "following_url": "https://api.github.com/users/Artem-B/following{/other_user}", "gists_url": "https://api.github.com/users/Artem-B/gists{/gist_id}", "starred_url": "https://api.github.com/users/Artem-B/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Artem-B/subscriptions", "organizations_url": "https://api.github.com/users/Artem-B/orgs", "repos_url": "https://api.github.com/users/Artem-B/repos", "events_url": "https://api.github.com/users/Artem-B/events{/privacy}", "received_events_url": "https://api.github.com/users/Artem-B/received_events", "type": "User", "site_admin": false}, "body": "IMO this is incorrect. _sync variants provide guarantee of thread synchronization if called from diverged context, which you can not do  with non-sync variants or on pre-Volta hardware as there's no way to guarantee that the call happens in converged context in non-trivial code.\r\n\r\nHypothetical example:\r\n```\r\nif (threadIdx.x < 16) {\r\n  // stuff\r\n  __shfl_sync(0xFFFFFFFF,...);\r\n  // more stuff that depends on shfl result\r\n} else {\r\n  // stuff\r\n  __shfl_sync(0xFFFFFFFF,...);\r\n  // more stuff that depends on shfl result\r\n}\r\n```\r\nOn volta, I believe __shfl_sync() in both branches would sync up and shuffle the data among all 32 threads in the warp. \r\nOn pre-volta h/w each of the branches would get data only from half of the threads as each branch will be executed in diverged context.\r\n\r\nIMO, compatibility should be provided in the opposite direction -- define old API using their new *_sync counterparts if we're compiling using CUDA-9.\r\n\r\n", "created_at": "2017-09-18T22:45:13Z", "updated_at": "2017-09-18T23:53:51Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12502#discussion_r139559400", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12502", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/139559400"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12502#discussion_r139559400"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12502"}}, "body_html": "<p>IMO this is incorrect. _sync variants provide guarantee of thread synchronization if called from diverged context, which you can not do  with non-sync variants or on pre-Volta hardware as there's no way to guarantee that the call happens in converged context in non-trivial code.</p>\n<p>Hypothetical example:</p>\n<pre><code>if (threadIdx.x &lt; 16) {\n  // stuff\n  __shfl_sync(0xFFFFFFFF,...);\n  // more stuff that depends on shfl result\n} else {\n  // stuff\n  __shfl_sync(0xFFFFFFFF,...);\n  // more stuff that depends on shfl result\n}\n</code></pre>\n<p>On volta, I believe __shfl_sync() in both branches would sync up and shuffle the data among all 32 threads in the warp.<br>\nOn pre-volta h/w each of the branches would get data only from half of the threads as each branch will be executed in diverged context.</p>\n<p>IMO, compatibility should be provided in the opposite direction -- define old API using their new *_sync counterparts if we're compiling using CUDA-9.</p>", "body_text": "IMO this is incorrect. _sync variants provide guarantee of thread synchronization if called from diverged context, which you can not do  with non-sync variants or on pre-Volta hardware as there's no way to guarantee that the call happens in converged context in non-trivial code.\nHypothetical example:\nif (threadIdx.x < 16) {\n  // stuff\n  __shfl_sync(0xFFFFFFFF,...);\n  // more stuff that depends on shfl result\n} else {\n  // stuff\n  __shfl_sync(0xFFFFFFFF,...);\n  // more stuff that depends on shfl result\n}\n\nOn volta, I believe __shfl_sync() in both branches would sync up and shuffle the data among all 32 threads in the warp.\nOn pre-volta h/w each of the branches would get data only from half of the threads as each branch will be executed in diverged context.\nIMO, compatibility should be provided in the opposite direction -- define old API using their new *_sync counterparts if we're compiling using CUDA-9.", "in_reply_to_id": 134872721}