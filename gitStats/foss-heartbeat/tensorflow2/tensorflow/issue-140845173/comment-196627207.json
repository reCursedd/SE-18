{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/196627207", "html_url": "https://github.com/tensorflow/tensorflow/issues/1502#issuecomment-196627207", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1502", "id": 196627207, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NjYyNzIwNw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-15T02:52:18Z", "updated_at": "2016-03-15T02:52:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It looks like two commits were inadvertently squashed. Here's the intended description for the batch norm parts of that commit:</p>\n<blockquote>\n<p>General batchnorm op, suitable to both global normalization for convolutions and per-depth normalization of fully-connected layers. It also supports arbitrary layouts and orderings of the axes.</p>\n<p>Nothing deep in these essentially 4 lines of code, but note:</p>\n<ul>\n<li>The ops are ordered in a way that makes the computation much more efficient (thx Sergey). None of the existing wrappers that didn't use the fused kernel did that, and as a result were almost 2x slower according to the benchmark.</li>\n<li>The benchmark confirms that this implementation is competitive with the handcrafted kernel.</li>\n<li>I confirmed on inception that the step time or accuracy are not affected.</li>\n<li>Added documentation and tests showing how the op can be used for arbitrary layouts.</li>\n<li>The change in API compared with batch_norm_with_global_normalization makes it possible to not have a scale tensor when it's not used, which caused some confusion and unnecessary additional saved variables in the past.</li>\n</ul>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15737127\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vincentvanhoucke\">@vincentvanhoucke</a> is working on the related issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"133980206\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1122\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1122/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1122\">#1122</a> and might have more to add.</p>", "body_text": "It looks like two commits were inadvertently squashed. Here's the intended description for the batch norm parts of that commit:\n\nGeneral batchnorm op, suitable to both global normalization for convolutions and per-depth normalization of fully-connected layers. It also supports arbitrary layouts and orderings of the axes.\nNothing deep in these essentially 4 lines of code, but note:\n\nThe ops are ordered in a way that makes the computation much more efficient (thx Sergey). None of the existing wrappers that didn't use the fused kernel did that, and as a result were almost 2x slower according to the benchmark.\nThe benchmark confirms that this implementation is competitive with the handcrafted kernel.\nI confirmed on inception that the step time or accuracy are not affected.\nAdded documentation and tests showing how the op can be used for arbitrary layouts.\nThe change in API compared with batch_norm_with_global_normalization makes it possible to not have a scale tensor when it's not used, which caused some confusion and unnecessary additional saved variables in the past.\n\n\n@vincentvanhoucke is working on the related issue #1122 and might have more to add.", "body": "It looks like two commits were inadvertently squashed. Here's the intended description for the batch norm parts of that commit:\n\n> General batchnorm op, suitable to both global normalization for convolutions and per-depth normalization of fully-connected layers. It also supports arbitrary layouts and orderings of the axes.\n> \n> Nothing deep in these essentially 4 lines of code, but note:\n> - The ops are ordered in a way that makes the computation much more efficient (thx Sergey). None of the existing wrappers that didn't use the fused kernel did that, and as a result were almost 2x slower according to the benchmark.\n> - The benchmark confirms that this implementation is competitive with the handcrafted kernel.\n> - I confirmed on inception that the step time or accuracy are not affected.\n> - Added documentation and tests showing how the op can be used for arbitrary layouts.\n> - The change in API compared with batch_norm_with_global_normalization makes it possible to not have a scale tensor when it's not used, which caused some confusion and unnecessary additional saved variables in the past.\n\n@vincentvanhoucke is working on the related issue https://github.com/tensorflow/tensorflow/issues/1122 and might have more to add.\n"}