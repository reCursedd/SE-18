{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/403683233", "html_url": "https://github.com/tensorflow/tensorflow/issues/18583#issuecomment-403683233", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18583", "id": 403683233, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzY4MzIzMw==", "user": {"login": "tobegit3hub", "id": 2715000, "node_id": "MDQ6VXNlcjI3MTUwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2715000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobegit3hub", "html_url": "https://github.com/tobegit3hub", "followers_url": "https://api.github.com/users/tobegit3hub/followers", "following_url": "https://api.github.com/users/tobegit3hub/following{/other_user}", "gists_url": "https://api.github.com/users/tobegit3hub/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobegit3hub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobegit3hub/subscriptions", "organizations_url": "https://api.github.com/users/tobegit3hub/orgs", "repos_url": "https://api.github.com/users/tobegit3hub/repos", "events_url": "https://api.github.com/users/tobegit3hub/events{/privacy}", "received_events_url": "https://api.github.com/users/tobegit3hub/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-10T02:53:40Z", "updated_at": "2018-07-10T02:53:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I get this error when adding <code>shuffle()</code> for the <code>dataset</code> object. It is easy to re-produce. We need one image file which could be named <code>1.jpg</code> and one csv file named <code>train.csv</code>. The content of <code>train.csv</code> is like this.</p>\n<pre><code>1.jpg,1\n</code></pre>\n<p>Then we can run the following script which throw <code>UnimplementedError</code> when saving the checkpoint. Notice that it is normal if we don't use <code>shuffle()</code> for the dataset.</p>\n<pre><code>#!/usr/bin/env python\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pandas as pd\nimport tensorflow as tf\n\n\ndef _decode_image_file(filename, label):\n  image_string = tf.read_file(filename)\n  features = tf.image.decode_jpeg(image_string, channels=1)\n  features = tf.image.resize_images(features, [28, 28])\n  return features, label\n\n\ndef main():\n  train_csv_file = \"./train.csv\"\n  feature_size = 28 * 28 * 1\n  label_size = 10\n  epoch_number = 10\n  batch_size = 1\n\n  train_image_list_placeholder = tf.placeholder(tf.string, [None])\n  train_label_list_placeholder = tf.placeholder(tf.int64, [None])\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n          (train_image_list_placeholder, train_label_list_placeholder))\n  train_dataset = train_dataset.repeat(epoch_number).shuffle(1000).map(_decode_image_file).batch(batch_size)\n  train_iterator = train_dataset.make_initializable_iterator()\n  train_features_op, train_label_op = train_iterator.get_next()\n  train_features_op = tf.cast(train_features_op, tf.float32)\n\n  train_dataframe = pd.read_csv(\n      train_csv_file,\n      delimiter=\",\",\n      header=None,\n      dtype={\"label\": \"int64\"}).sample(frac=1.0)\n  train_image_list = train_dataframe[0].tolist()\n  train_label_list = train_dataframe[1].tolist()\n\n  global_step = tf.Variable(\n      0, name=\"global_step\", dtype=tf.int64, trainable=False)\n  input = tf.reshape(train_features_op, (-1, feature_size))\n  weights = tf.get_variable(\n          \"weight\", [feature_size, label_size],\n          dtype=tf.float32,\n          initializer=tf.zeros_initializer)\n  bias = tf.get_variable(\"bias\", [label_size], dtype=tf.float32, initializer=tf.zeros_initializer)\n  logits = tf.matmul(input, weights) + bias\n  loss = tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=train_label_op))\n  train_op = tf.train.GradientDescentOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n  saveable = tf.contrib.data.make_saveable_from_iterator(train_iterator)\n  tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\n  saver = tf.train.Saver()\n\n  with tf.Session() as sess:\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(\n        train_iterator.initializer,\n        feed_dict={\n            train_image_list_placeholder: train_image_list,\n            train_label_list_placeholder: train_label_list\n        })\n\n    try:\n      _, loss_value, global_step_value = sess.run([train_op, loss, global_step])\n      saver.save(sess, \"checkpoint/checkpoint.ckpt\", global_step=global_step_value)\n    except tf.errors.OutOfRangeError:\n      print(\"End of data\")\n\n\nif __name__ == \"__main__\":\n  main()\n</code></pre>", "body_text": "I get this error when adding shuffle() for the dataset object. It is easy to re-produce. We need one image file which could be named 1.jpg and one csv file named train.csv. The content of train.csv is like this.\n1.jpg,1\n\nThen we can run the following script which throw UnimplementedError when saving the checkpoint. Notice that it is normal if we don't use shuffle() for the dataset.\n#!/usr/bin/env python\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pandas as pd\nimport tensorflow as tf\n\n\ndef _decode_image_file(filename, label):\n  image_string = tf.read_file(filename)\n  features = tf.image.decode_jpeg(image_string, channels=1)\n  features = tf.image.resize_images(features, [28, 28])\n  return features, label\n\n\ndef main():\n  train_csv_file = \"./train.csv\"\n  feature_size = 28 * 28 * 1\n  label_size = 10\n  epoch_number = 10\n  batch_size = 1\n\n  train_image_list_placeholder = tf.placeholder(tf.string, [None])\n  train_label_list_placeholder = tf.placeholder(tf.int64, [None])\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n          (train_image_list_placeholder, train_label_list_placeholder))\n  train_dataset = train_dataset.repeat(epoch_number).shuffle(1000).map(_decode_image_file).batch(batch_size)\n  train_iterator = train_dataset.make_initializable_iterator()\n  train_features_op, train_label_op = train_iterator.get_next()\n  train_features_op = tf.cast(train_features_op, tf.float32)\n\n  train_dataframe = pd.read_csv(\n      train_csv_file,\n      delimiter=\",\",\n      header=None,\n      dtype={\"label\": \"int64\"}).sample(frac=1.0)\n  train_image_list = train_dataframe[0].tolist()\n  train_label_list = train_dataframe[1].tolist()\n\n  global_step = tf.Variable(\n      0, name=\"global_step\", dtype=tf.int64, trainable=False)\n  input = tf.reshape(train_features_op, (-1, feature_size))\n  weights = tf.get_variable(\n          \"weight\", [feature_size, label_size],\n          dtype=tf.float32,\n          initializer=tf.zeros_initializer)\n  bias = tf.get_variable(\"bias\", [label_size], dtype=tf.float32, initializer=tf.zeros_initializer)\n  logits = tf.matmul(input, weights) + bias\n  loss = tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=train_label_op))\n  train_op = tf.train.GradientDescentOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n  saveable = tf.contrib.data.make_saveable_from_iterator(train_iterator)\n  tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\n  saver = tf.train.Saver()\n\n  with tf.Session() as sess:\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(\n        train_iterator.initializer,\n        feed_dict={\n            train_image_list_placeholder: train_image_list,\n            train_label_list_placeholder: train_label_list\n        })\n\n    try:\n      _, loss_value, global_step_value = sess.run([train_op, loss, global_step])\n      saver.save(sess, \"checkpoint/checkpoint.ckpt\", global_step=global_step_value)\n    except tf.errors.OutOfRangeError:\n      print(\"End of data\")\n\n\nif __name__ == \"__main__\":\n  main()", "body": "I get this error when adding `shuffle()` for the `dataset` object. It is easy to re-produce. We need one image file which could be named `1.jpg` and one csv file named `train.csv`. The content of `train.csv` is like this.\r\n\r\n```\r\n1.jpg,1\r\n```\r\n\r\nThen we can run the following script which throw `UnimplementedError` when saving the checkpoint. Notice that it is normal if we don't use `shuffle()` for the dataset.\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\n\r\ndef _decode_image_file(filename, label):\r\n  image_string = tf.read_file(filename)\r\n  features = tf.image.decode_jpeg(image_string, channels=1)\r\n  features = tf.image.resize_images(features, [28, 28])\r\n  return features, label\r\n\r\n\r\ndef main():\r\n  train_csv_file = \"./train.csv\"\r\n  feature_size = 28 * 28 * 1\r\n  label_size = 10\r\n  epoch_number = 10\r\n  batch_size = 1\r\n\r\n  train_image_list_placeholder = tf.placeholder(tf.string, [None])\r\n  train_label_list_placeholder = tf.placeholder(tf.int64, [None])\r\n  train_dataset = tf.data.Dataset.from_tensor_slices(\r\n          (train_image_list_placeholder, train_label_list_placeholder))\r\n  train_dataset = train_dataset.repeat(epoch_number).shuffle(1000).map(_decode_image_file).batch(batch_size)\r\n  train_iterator = train_dataset.make_initializable_iterator()\r\n  train_features_op, train_label_op = train_iterator.get_next()\r\n  train_features_op = tf.cast(train_features_op, tf.float32)\r\n\r\n  train_dataframe = pd.read_csv(\r\n      train_csv_file,\r\n      delimiter=\",\",\r\n      header=None,\r\n      dtype={\"label\": \"int64\"}).sample(frac=1.0)\r\n  train_image_list = train_dataframe[0].tolist()\r\n  train_label_list = train_dataframe[1].tolist()\r\n\r\n  global_step = tf.Variable(\r\n      0, name=\"global_step\", dtype=tf.int64, trainable=False)\r\n  input = tf.reshape(train_features_op, (-1, feature_size))\r\n  weights = tf.get_variable(\r\n          \"weight\", [feature_size, label_size],\r\n          dtype=tf.float32,\r\n          initializer=tf.zeros_initializer)\r\n  bias = tf.get_variable(\"bias\", [label_size], dtype=tf.float32, initializer=tf.zeros_initializer)\r\n  logits = tf.matmul(input, weights) + bias\r\n  loss = tf.reduce_mean(\r\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n          logits=logits, labels=train_label_op))\r\n  train_op = tf.train.GradientDescentOptimizer(0.01).minimize(\r\n          loss, global_step=global_step)\r\n\r\n  saveable = tf.contrib.data.make_saveable_from_iterator(train_iterator)\r\n  tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\r\n  saver = tf.train.Saver()\r\n\r\n  with tf.Session() as sess:\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(\r\n        train_iterator.initializer,\r\n        feed_dict={\r\n            train_image_list_placeholder: train_image_list,\r\n            train_label_list_placeholder: train_label_list\r\n        })\r\n\r\n    try:\r\n      _, loss_value, global_step_value = sess.run([train_op, loss, global_step])\r\n      saver.save(sess, \"checkpoint/checkpoint.ckpt\", global_step=global_step_value)\r\n    except tf.errors.OutOfRangeError:\r\n      print(\"End of data\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  main()\r\n```"}