{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/349086516", "html_url": "https://github.com/tensorflow/tensorflow/issues/15057#issuecomment-349086516", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15057", "id": 349086516, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTA4NjUxNg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-04T19:58:21Z", "updated_at": "2017-12-04T19:58:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Timing a single call to session.run is not realistic; and you're spending a lot of time on overhead, especially in the GPU case.  If you really want to know how long it takes to perform inference per session.run() call, you need to warm up the system and then time just the session.run() call:</p>\n<p>What you should be doing is calling between 1-10 session.run() prediction tasks and throwing away their time (this is the \"warm-up\"), then running predict() ~ 10 to 100 or 1000 times, and averaging the resulting timings.  This will give you more consistent and realistic numbers.</p>", "body_text": "Timing a single call to session.run is not realistic; and you're spending a lot of time on overhead, especially in the GPU case.  If you really want to know how long it takes to perform inference per session.run() call, you need to warm up the system and then time just the session.run() call:\nWhat you should be doing is calling between 1-10 session.run() prediction tasks and throwing away their time (this is the \"warm-up\"), then running predict() ~ 10 to 100 or 1000 times, and averaging the resulting timings.  This will give you more consistent and realistic numbers.", "body": "Timing a single call to session.run is not realistic; and you're spending a lot of time on overhead, especially in the GPU case.  If you really want to know how long it takes to perform inference per session.run() call, you need to warm up the system and then time just the session.run() call:\r\n\r\nWhat you should be doing is calling between 1-10 session.run() prediction tasks and throwing away their time (this is the \"warm-up\"), then running predict() ~ 10 to 100 or 1000 times, and averaging the resulting timings.  This will give you more consistent and realistic numbers."}