{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/393498967", "html_url": "https://github.com/tensorflow/tensorflow/issues/19617#issuecomment-393498967", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19617", "id": 393498967, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzQ5ODk2Nw==", "user": {"login": "jinxin0924", "id": 8096033, "node_id": "MDQ6VXNlcjgwOTYwMzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8096033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinxin0924", "html_url": "https://github.com/jinxin0924", "followers_url": "https://api.github.com/users/jinxin0924/followers", "following_url": "https://api.github.com/users/jinxin0924/following{/other_user}", "gists_url": "https://api.github.com/users/jinxin0924/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinxin0924/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinxin0924/subscriptions", "organizations_url": "https://api.github.com/users/jinxin0924/orgs", "repos_url": "https://api.github.com/users/jinxin0924/repos", "events_url": "https://api.github.com/users/jinxin0924/events{/privacy}", "received_events_url": "https://api.github.com/users/jinxin0924/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-31T11:20:55Z", "updated_at": "2018-05-31T11:30:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9065977\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yunyin\">@yunyin</a>, yes, different workers share the same local_step. It is a bug. You can modify model_average_optimizer.py line 91 like this:</p>\n<pre><code>    else:\n      kwargs['trainable'] = trainable\n      kwargs['collections'] = collections\n      if ops.GraphKeys.LOCAL_VARIABLES in collections:\n        with ops.device(self._worker_device):\n          return getter(name, *args, **kwargs)\n      else:\n        return getter(name, *args, **kwargs)\n</code></pre>\n<p>In my experiments, I always create model average optimizer out of the ma_custom scope, so I did not find these bugs. Sorry for that. Again, thanks a lot for tying this opt!</p>", "body_text": "@yunyin, yes, different workers share the same local_step. It is a bug. You can modify model_average_optimizer.py line 91 like this:\n    else:\n      kwargs['trainable'] = trainable\n      kwargs['collections'] = collections\n      if ops.GraphKeys.LOCAL_VARIABLES in collections:\n        with ops.device(self._worker_device):\n          return getter(name, *args, **kwargs)\n      else:\n        return getter(name, *args, **kwargs)\n\nIn my experiments, I always create model average optimizer out of the ma_custom scope, so I did not find these bugs. Sorry for that. Again, thanks a lot for tying this opt!", "body": "@yunyin, yes, different workers share the same local_step. It is a bug. You can modify model_average_optimizer.py line 91 like this:\r\n```\r\n    else:\r\n      kwargs['trainable'] = trainable\r\n      kwargs['collections'] = collections\r\n      if ops.GraphKeys.LOCAL_VARIABLES in collections:\r\n        with ops.device(self._worker_device):\r\n          return getter(name, *args, **kwargs)\r\n      else:\r\n        return getter(name, *args, **kwargs)\r\n```\r\nIn my experiments, I always create model average optimizer out of the ma_custom scope, so I did not find these bugs. Sorry for that. Again, thanks a lot for tying this opt! "}