{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/232803315", "html_url": "https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-232803315", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009", "id": 232803315, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMjgwMzMxNQ==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-14T21:51:27Z", "updated_at": "2016-07-14T21:51:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've noticed memcpy's being mentioned in this and <a href=\"https://github.com/tensorflow/tensorflow/issues/2919\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2919/hovercard\">other</a> issues related to performance. I'm guessing memcpy is used to avoid thread-safety issues, in particular to support operations that take references to tensors such as <strong>assign</strong>.</p>\n<p>Has any thought been given to introducing Copy-On-Write functionality to tensors within the graph? Numpy handles <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flags.html#numpy.ndarray.flags\" rel=\"nofollow\">this</a> but obviously without the tricky thread safety issue.</p>\n<p>I ask because it seems wasteful in terms of I/O and memory to spend a memcpy feeding a numpy array onto a FIFOQueue and another memcpy (within a graph) copying the resultant tensor off the FIFOQueue. Ideally it would be nice to use FIFOQueue as a a pure synchronisation construct ala. go channels, python Queues rather a sychronised buffer. Clearly this needs to happen in cases where tensors cross device boundaries (GPU's/Network interfaces) but might there be scope for performance gains here?</p>\n<p>I also notice that memcpy's happen within slicing operations. Could this be replaced with a Eigen::TensorMap (homologous to a numpy view) for e.g.?</p>", "body_text": "I've noticed memcpy's being mentioned in this and other issues related to performance. I'm guessing memcpy is used to avoid thread-safety issues, in particular to support operations that take references to tensors such as assign.\nHas any thought been given to introducing Copy-On-Write functionality to tensors within the graph? Numpy handles this but obviously without the tricky thread safety issue.\nI ask because it seems wasteful in terms of I/O and memory to spend a memcpy feeding a numpy array onto a FIFOQueue and another memcpy (within a graph) copying the resultant tensor off the FIFOQueue. Ideally it would be nice to use FIFOQueue as a a pure synchronisation construct ala. go channels, python Queues rather a sychronised buffer. Clearly this needs to happen in cases where tensors cross device boundaries (GPU's/Network interfaces) but might there be scope for performance gains here?\nI also notice that memcpy's happen within slicing operations. Could this be replaced with a Eigen::TensorMap (homologous to a numpy view) for e.g.?", "body": "I've noticed memcpy's being mentioned in this and [other](https://github.com/tensorflow/tensorflow/issues/2919) issues related to performance. I'm guessing memcpy is used to avoid thread-safety issues, in particular to support operations that take references to tensors such as **assign**.\n\nHas any thought been given to introducing Copy-On-Write functionality to tensors within the graph? Numpy handles [this](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flags.html#numpy.ndarray.flags) but obviously without the tricky thread safety issue.\n\nI ask because it seems wasteful in terms of I/O and memory to spend a memcpy feeding a numpy array onto a FIFOQueue and another memcpy (within a graph) copying the resultant tensor off the FIFOQueue. Ideally it would be nice to use FIFOQueue as a a pure synchronisation construct ala. go channels, python Queues rather a sychronised buffer. Clearly this needs to happen in cases where tensors cross device boundaries (GPU's/Network interfaces) but might there be scope for performance gains here?\n\nI also notice that memcpy's happen within slicing operations. Could this be replaced with a Eigen::TensorMap (homologous to a numpy view) for e.g.?\n"}