{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/228867287", "html_url": "https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-228867287", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009", "id": 228867287, "node_id": "MDEyOklzc3VlQ29tbWVudDIyODg2NzI4Nw==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-27T20:36:08Z", "updated_at": "2016-06-27T20:53:36Z", "author_association": "MEMBER", "body_html": "<p>I spent some time looking into what's going on here...</p>\n<p>As I mentioned earlier, your input batch of  [64, 30, 49, 512] * tf.float32 equates to 192MB...</p>\n<p>First, I established a baseline cost for memcpying a tensor of the size you are feeding.  A native C++ program calling memcpy in a tight loop between buffers of this size takes around <strong>22ms</strong> per iteration (single threaded).  The TensorFlow enqueue/dequeue operations actually use the <em>Eigen</em> library to copy out slices of the tensors and this is likely to be <strong>less</strong> efficient than a flat memcpy since it needs to handle general shapes of n-D arrays.   Theoretically Eigen might be able to do this in parallel, but doesn't appear to be in this case (from looking at the CPU usage).</p>\n<p>Because you are feeding in numpy arrays, and retrieving a large result, you incur a copy in each direction within the session.Run() call,  <a href=\"https://github.com/tensorflow/tensorflow/blob/dfb71ea206eb9f61e5d97c9727caa1a6449e39cb/tensorflow/python/client/tf_session_helper.cc#L445\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/dfb71ea206eb9f61e5d97c9727caa1a6449e39cb/tensorflow/python/client/tf_session_helper.cc#L525\">here</a> for TensorFlow 0.9.</p>\n<p>Note - this code path is slightly different in various versions of TensorFlow.  Also, if you ever move to the distributed runtime these 192MB tensors will get serialized as ProtoBufs and this is <em>very</em> expensive.</p>\n<p>Also, as <a class=\"user-mention\" data-hovercard-type=\"organization\" data-hovercard-url=\"/orgs/MMRY/hovercard\" href=\"https://github.com/MMRY\">@MMRY</a> says, some of the CPU time related to enqueue operations is most likely being accounted to the <code>DequeueMany</code> op due to <a href=\"https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/core/kernels/queue_base.cc#L336\">this</a> code - once the queue is full, enqueue ops are blocked and wait in a list.  They can then get executed the next time a dequeue succeeds.</p>\n<p>Note that a queue with max size of 1000 elements is about <strong>3GB</strong>, so depending on your machine config you may be causing a lot of virtual memory (and allocator) pressure.  This <em>isn't</em> happening on my machine, but you may want to check by running <code>htop</code> or <code>vmstat</code> while executing your program.  A smaller queue may be more sensible.</p>\n<p>In general, you may be better off using one of the TensorFlow input ops which reads image data directly from the file sytem, and then do any preprocessing as part of the graph.  (as opposed to preprocessing in Python and feeding in the raw tensor data).</p>\n<p>For reference, I've attached a <a href=\"http://goog-perftools.sourceforge.net/doc/cpu_profiler.html\" rel=\"nofollow\">pprof</a> profile of your code (with a few relatively insignificant changes).  You can see that the bulk of the cycles are spent in <code>TF_Run_wrapper_helper</code> (doing memcpys of the numpy arrays), and under <code>QueueBase::FlushUnlocked</code> doing both the enqueue and dequeue copy ops via <em>Eigen</em> (one of which ends up turning into a <code>memcpy</code>)</p>\n<p><strong>EDIT:</strong>  You will see that there also seems to be a decent amount of time spent in libcuda (for which we have no symbols).  This is most likely due to the memory being used for host to device transfers not being \"pinned\".  This appears to cause the Nvidia driver to throw out the anchors and either copy the data to a DMA'able buffer or pin the relevant pages. Net result is that those transfers also take about the same time as a memcpy and consume a lot of CPU.  TensorFlow has heuristics which attempt to allocate tensors in pinned memory when they need to be DMA'd.  It may be the case that they are not working well here. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a> may know more?</p>\n<p>Hope this helps ....<br>\nPaul</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/11547801/16394032/140cbc64-3c68-11e6-8fe3-6335de4ab229.png\"><img src=\"https://cloud.githubusercontent.com/assets/11547801/16394032/140cbc64-3c68-11e6-8fe3-6335de4ab229.png\" alt=\"profile\" style=\"max-width:100%;\"></a></p>", "body_text": "I spent some time looking into what's going on here...\nAs I mentioned earlier, your input batch of  [64, 30, 49, 512] * tf.float32 equates to 192MB...\nFirst, I established a baseline cost for memcpying a tensor of the size you are feeding.  A native C++ program calling memcpy in a tight loop between buffers of this size takes around 22ms per iteration (single threaded).  The TensorFlow enqueue/dequeue operations actually use the Eigen library to copy out slices of the tensors and this is likely to be less efficient than a flat memcpy since it needs to handle general shapes of n-D arrays.   Theoretically Eigen might be able to do this in parallel, but doesn't appear to be in this case (from looking at the CPU usage).\nBecause you are feeding in numpy arrays, and retrieving a large result, you incur a copy in each direction within the session.Run() call,  here and here for TensorFlow 0.9.\nNote - this code path is slightly different in various versions of TensorFlow.  Also, if you ever move to the distributed runtime these 192MB tensors will get serialized as ProtoBufs and this is very expensive.\nAlso, as @MMRY says, some of the CPU time related to enqueue operations is most likely being accounted to the DequeueMany op due to this code - once the queue is full, enqueue ops are blocked and wait in a list.  They can then get executed the next time a dequeue succeeds.\nNote that a queue with max size of 1000 elements is about 3GB, so depending on your machine config you may be causing a lot of virtual memory (and allocator) pressure.  This isn't happening on my machine, but you may want to check by running htop or vmstat while executing your program.  A smaller queue may be more sensible.\nIn general, you may be better off using one of the TensorFlow input ops which reads image data directly from the file sytem, and then do any preprocessing as part of the graph.  (as opposed to preprocessing in Python and feeding in the raw tensor data).\nFor reference, I've attached a pprof profile of your code (with a few relatively insignificant changes).  You can see that the bulk of the cycles are spent in TF_Run_wrapper_helper (doing memcpys of the numpy arrays), and under QueueBase::FlushUnlocked doing both the enqueue and dequeue copy ops via Eigen (one of which ends up turning into a memcpy)\nEDIT:  You will see that there also seems to be a decent amount of time spent in libcuda (for which we have no symbols).  This is most likely due to the memory being used for host to device transfers not being \"pinned\".  This appears to cause the Nvidia driver to throw out the anchors and either copy the data to a DMA'able buffer or pin the relevant pages. Net result is that those transfers also take about the same time as a memcpy and consume a lot of CPU.  TensorFlow has heuristics which attempt to allocate tensors in pinned memory when they need to be DMA'd.  It may be the case that they are not working well here. @poxvoculi may know more?\nHope this helps ....\nPaul", "body": "I spent some time looking into what's going on here...   \n\nAs I mentioned earlier, your input batch of  [64, 30, 49, 512] \\* tf.float32 equates to 192MB...  \n\nFirst, I established a baseline cost for memcpying a tensor of the size you are feeding.  A native C++ program calling memcpy in a tight loop between buffers of this size takes around **22ms** per iteration (single threaded).  The TensorFlow enqueue/dequeue operations actually use the _Eigen_ library to copy out slices of the tensors and this is likely to be **less** efficient than a flat memcpy since it needs to handle general shapes of n-D arrays.   Theoretically Eigen might be able to do this in parallel, but doesn't appear to be in this case (from looking at the CPU usage).\n\nBecause you are feeding in numpy arrays, and retrieving a large result, you incur a copy in each direction within the session.Run() call,  [here](https://github.com/tensorflow/tensorflow/blob/dfb71ea206eb9f61e5d97c9727caa1a6449e39cb/tensorflow/python/client/tf_session_helper.cc#L445) and [here](https://github.com/tensorflow/tensorflow/blob/dfb71ea206eb9f61e5d97c9727caa1a6449e39cb/tensorflow/python/client/tf_session_helper.cc#L525) for TensorFlow 0.9.  \n\nNote - this code path is slightly different in various versions of TensorFlow.  Also, if you ever move to the distributed runtime these 192MB tensors will get serialized as ProtoBufs and this is _very_ expensive.\n\nAlso, as @mmry says, some of the CPU time related to enqueue operations is most likely being accounted to the `DequeueMany` op due to [this](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/core/kernels/queue_base.cc#L336) code - once the queue is full, enqueue ops are blocked and wait in a list.  They can then get executed the next time a dequeue succeeds.\n\nNote that a queue with max size of 1000 elements is about **3GB**, so depending on your machine config you may be causing a lot of virtual memory (and allocator) pressure.  This _isn't_ happening on my machine, but you may want to check by running `htop` or `vmstat` while executing your program.  A smaller queue may be more sensible.\n\nIn general, you may be better off using one of the TensorFlow input ops which reads image data directly from the file sytem, and then do any preprocessing as part of the graph.  (as opposed to preprocessing in Python and feeding in the raw tensor data).\n\nFor reference, I've attached a [pprof](http://goog-perftools.sourceforge.net/doc/cpu_profiler.html) profile of your code (with a few relatively insignificant changes).  You can see that the bulk of the cycles are spent in `TF_Run_wrapper_helper` (doing memcpys of the numpy arrays), and under `QueueBase::FlushUnlocked` doing both the enqueue and dequeue copy ops via _Eigen_ (one of which ends up turning into a `memcpy`)\n\n**EDIT:**  You will see that there also seems to be a decent amount of time spent in libcuda (for which we have no symbols).  This is most likely due to the memory being used for host to device transfers not being \"pinned\".  This appears to cause the Nvidia driver to throw out the anchors and either copy the data to a DMA'able buffer or pin the relevant pages. Net result is that those transfers also take about the same time as a memcpy and consume a lot of CPU.  TensorFlow has heuristics which attempt to allocate tensors in pinned memory when they need to be DMA'd.  It may be the case that they are not working well here. @poxvoculi may know more?\n\nHope this helps .... \nPaul\n\n![profile](https://cloud.githubusercontent.com/assets/11547801/16394032/140cbc64-3c68-11e6-8fe3-6335de4ab229.png)\n"}