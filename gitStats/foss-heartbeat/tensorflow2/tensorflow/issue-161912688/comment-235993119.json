{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235993119", "html_url": "https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-235993119", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009", "id": 235993119, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTk5MzExOQ==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-28T19:03:15Z", "updated_at": "2016-07-28T19:03:27Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1893429\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rohitgirdhar\">@rohitgirdhar</a> I'm not 100% sure I want to add to a closed issue, commenting on something which may not even be related to the original problem, but...</p>\n<p>From the <code>htop</code> output and your <code>drop_caches</code> observation it sounds to me like there may be some memory pressure caused by virtual address space fragmentation and high system buffer cache churn (reading large training datasets from the file system).  Can you see if your program is causing lots of page faults?  e.g. <a href=\"http://www.cyberciti.biz/faq/linux-command-to-see-major-minor-pagefaults/\" rel=\"nofollow\">like this</a></p>\n<p>Internally at google we almost always use the <a href=\"http://goog-perftools.sourceforge.net/doc/tcmalloc.html\" rel=\"nofollow\">TCMalloc</a> memory allocator which appears to be much better for TensorFlow workloads (and most others at google!).  When running the open source TensorFlow build via a standard Python binary the only way to use TCMalloc would be to inject a different heap implementation into Python using <code>LD_PRELOAD</code>, e.g.<br>\n<code>LD_PRELOAD=\"/usr/lib/libtcmalloc.so\" python myprogram.py</code></p>\n<p>It would be very useful if you could try running with TCMalloc and see if it makes any difference.</p>", "body_text": "@rohitgirdhar I'm not 100% sure I want to add to a closed issue, commenting on something which may not even be related to the original problem, but...\nFrom the htop output and your drop_caches observation it sounds to me like there may be some memory pressure caused by virtual address space fragmentation and high system buffer cache churn (reading large training datasets from the file system).  Can you see if your program is causing lots of page faults?  e.g. like this\nInternally at google we almost always use the TCMalloc memory allocator which appears to be much better for TensorFlow workloads (and most others at google!).  When running the open source TensorFlow build via a standard Python binary the only way to use TCMalloc would be to inject a different heap implementation into Python using LD_PRELOAD, e.g.\nLD_PRELOAD=\"/usr/lib/libtcmalloc.so\" python myprogram.py\nIt would be very useful if you could try running with TCMalloc and see if it makes any difference.", "body": "@rohitgirdhar I'm not 100% sure I want to add to a closed issue, commenting on something which may not even be related to the original problem, but...\n\nFrom the `htop` output and your `drop_caches` observation it sounds to me like there may be some memory pressure caused by virtual address space fragmentation and high system buffer cache churn (reading large training datasets from the file system).  Can you see if your program is causing lots of page faults?  e.g. [like this](http://www.cyberciti.biz/faq/linux-command-to-see-major-minor-pagefaults/)\n\nInternally at google we almost always use the [TCMalloc](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) memory allocator which appears to be much better for TensorFlow workloads (and most others at google!).  When running the open source TensorFlow build via a standard Python binary the only way to use TCMalloc would be to inject a different heap implementation into Python using `LD_PRELOAD`, e.g.\n`LD_PRELOAD=\"/usr/lib/libtcmalloc.so\" python myprogram.py`\n\nIt would be very useful if you could try running with TCMalloc and see if it makes any difference.\n"}