{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235074937", "html_url": "https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-235074937", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009", "id": 235074937, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTA3NDkzNw==", "user": {"login": "rohitgirdhar", "id": 1893429, "node_id": "MDQ6VXNlcjE4OTM0Mjk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1893429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohitgirdhar", "html_url": "https://github.com/rohitgirdhar", "followers_url": "https://api.github.com/users/rohitgirdhar/followers", "following_url": "https://api.github.com/users/rohitgirdhar/following{/other_user}", "gists_url": "https://api.github.com/users/rohitgirdhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohitgirdhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohitgirdhar/subscriptions", "organizations_url": "https://api.github.com/users/rohitgirdhar/orgs", "repos_url": "https://api.github.com/users/rohitgirdhar/repos", "events_url": "https://api.github.com/users/rohitgirdhar/events{/privacy}", "received_events_url": "https://api.github.com/users/rohitgirdhar/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-25T20:30:26Z", "updated_at": "2016-07-25T23:18:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a> Thanks for your detailed analysis! I am facing similar issues (timeline attached). I'm using tensorflow read operations throughout, so I believe my problem is not the numpy&lt;-&gt;TF data copy. However your comment on virtual memory/allocator pressure seems related to my problem. Here's a screenshot from <code>htop</code>, and also the <code>vmstat</code> output</p>\n<pre lang=\"text\"><code>procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 4  0 9744848 1865340 2888112 45542040    1    8   118    50    0    0 21 18 61  0  0\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/1893429/17116146/46c40090-526b-11e6-8071-ef45b0fdc881.png\"><img src=\"https://cloud.githubusercontent.com/assets/1893429/17116146/46c40090-526b-11e6-8071-ef45b0fdc881.png\" alt=\"screen shot 2016-07-25 at 1 24 57 pm\" style=\"max-width:100%;\"></a><br>\nIt seems my virtual memory use is 160G, do you think that might be the problem? I am not really familiar with these system level details, so I would greatly appreciate if you could elaborate on this problem. Thank you!</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/1893429/17116222/a385f1f8-526b-11e6-846f-53ce2140edf1.png\"><img src=\"https://cloud.githubusercontent.com/assets/1893429/17116222/a385f1f8-526b-11e6-846f-53ce2140edf1.png\" alt=\"screen shot 2016-07-25 at 10 55 04 am\" style=\"max-width:100%;\"></a></p>\n<p>[Update]<br>\nI tried running the same model with much lower queue capacity (2*batch_size, where batch_size is 288 and my images are 224x224x3) and min_after_dequeue=10). The virtual memory use on htop is still 160G, and I am again experiencing this random slowdown after 1000 iterations. I'm happy to provide any information/logs you might need to help us fix this problem! Here's my data reading code (just in case)</p>\n<div class=\"highlight highlight-source-python\"><pre>    example_list <span class=\"pl-k\">=</span> [_read_image_from_disk(filename_queue, train) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_readers)]\n    <span class=\"pl-k\">if</span> train:\n        example_batch, label_batch <span class=\"pl-k\">=</span> tf.train.shuffle_batch_join(\n                example_list, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n                <span class=\"pl-v\">capacity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> batch_size,\n                <span class=\"pl-v\">min_after_dequeue</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n    <span class=\"pl-k\">else</span>:\n        example_batch, label_batch <span class=\"pl-k\">=</span> tf.train.batch_join(example_list, batch_size)\n</pre></div>", "body_text": "@prb12 Thanks for your detailed analysis! I am facing similar issues (timeline attached). I'm using tensorflow read operations throughout, so I believe my problem is not the numpy<->TF data copy. However your comment on virtual memory/allocator pressure seems related to my problem. Here's a screenshot from htop, and also the vmstat output\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 4  0 9744848 1865340 2888112 45542040    1    8   118    50    0    0 21 18 61  0  0\n\n\nIt seems my virtual memory use is 160G, do you think that might be the problem? I am not really familiar with these system level details, so I would greatly appreciate if you could elaborate on this problem. Thank you!\n\n[Update]\nI tried running the same model with much lower queue capacity (2*batch_size, where batch_size is 288 and my images are 224x224x3) and min_after_dequeue=10). The virtual memory use on htop is still 160G, and I am again experiencing this random slowdown after 1000 iterations. I'm happy to provide any information/logs you might need to help us fix this problem! Here's my data reading code (just in case)\n    example_list = [_read_image_from_disk(filename_queue, train) for _ in range(num_readers)]\n    if train:\n        example_batch, label_batch = tf.train.shuffle_batch_join(\n                example_list, batch_size=batch_size,\n                capacity=2 * batch_size,\n                min_after_dequeue=10)\n    else:\n        example_batch, label_batch = tf.train.batch_join(example_list, batch_size)", "body": "@prb12 Thanks for your detailed analysis! I am facing similar issues (timeline attached). I'm using tensorflow read operations throughout, so I believe my problem is not the numpy<->TF data copy. However your comment on virtual memory/allocator pressure seems related to my problem. Here's a screenshot from `htop`, and also the `vmstat` output\n\n``` text\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 4  0 9744848 1865340 2888112 45542040    1    8   118    50    0    0 21 18 61  0  0\n```\n\n![screen shot 2016-07-25 at 1 24 57 pm](https://cloud.githubusercontent.com/assets/1893429/17116146/46c40090-526b-11e6-8071-ef45b0fdc881.png)\nIt seems my virtual memory use is 160G, do you think that might be the problem? I am not really familiar with these system level details, so I would greatly appreciate if you could elaborate on this problem. Thank you!\n\n![screen shot 2016-07-25 at 10 55 04 am](https://cloud.githubusercontent.com/assets/1893429/17116222/a385f1f8-526b-11e6-846f-53ce2140edf1.png)\n\n[Update]\nI tried running the same model with much lower queue capacity (2*batch_size, where batch_size is 288 and my images are 224x224x3) and min_after_dequeue=10). The virtual memory use on htop is still 160G, and I am again experiencing this random slowdown after 1000 iterations. I'm happy to provide any information/logs you might need to help us fix this problem! Here's my data reading code (just in case)\n\n``` python\n    example_list = [_read_image_from_disk(filename_queue, train) for _ in range(num_readers)]\n    if train:\n        example_batch, label_batch = tf.train.shuffle_batch_join(\n                example_list, batch_size=batch_size,\n                capacity=2 * batch_size,\n                min_after_dequeue=10)\n    else:\n        example_batch, label_batch = tf.train.batch_join(example_list, batch_size)\n\n```\n"}