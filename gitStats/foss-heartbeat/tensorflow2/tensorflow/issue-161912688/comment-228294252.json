{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/228294252", "html_url": "https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-228294252", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009", "id": 228294252, "node_id": "MDEyOklzc3VlQ29tbWVudDIyODI5NDI1Mg==", "user": {"login": "tomrunia", "id": 5536129, "node_id": "MDQ6VXNlcjU1MzYxMjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5536129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomrunia", "html_url": "https://github.com/tomrunia", "followers_url": "https://api.github.com/users/tomrunia/followers", "following_url": "https://api.github.com/users/tomrunia/following{/other_user}", "gists_url": "https://api.github.com/users/tomrunia/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomrunia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomrunia/subscriptions", "organizations_url": "https://api.github.com/users/tomrunia/orgs", "repos_url": "https://api.github.com/users/tomrunia/repos", "events_url": "https://api.github.com/users/tomrunia/events{/privacy}", "received_events_url": "https://api.github.com/users/tomrunia/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-24T09:00:38Z", "updated_at": "2016-06-24T09:00:38Z", "author_association": "NONE", "body_html": "<p>Oke, I wrote a simple script (see below) in which the threads simple enqueue batches of <code>np.ones([64, 30, 49, 512], dtype=np.float32)</code> to the <code>FIFOQueue</code>. The main loop simple dequeues 64 examples and performs a simple <code>tf.square</code> of the input tensor. Before the main loop starts I make sure that the queue is filled with a sufficient number of examples. Again, the dequeue operation takes a \"long\" time to finish as you can see in the timelines below.</p>\n<p><strong>GPU running on Titan X</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/8240eefac13caa9a467c4e2fba44da26fee3e1ca/687474703a2f2f692e696d6775722e636f6d2f56724b613531472e706e67\"><img src=\"https://camo.githubusercontent.com/8240eefac13caa9a467c4e2fba44da26fee3e1ca/687474703a2f2f692e696d6775722e636f6d2f56724b613531472e706e67\" alt=\"gpu\" data-canonical-src=\"http://i.imgur.com/VrKa51G.png\" style=\"max-width:100%;\"></a></p>\n<p><strong>CPU running on Xeon E5-2640</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/37e75ef1dd991acca2be192ad0362fee608496a3/687474703a2f2f692e696d6775722e636f6d2f566c4736385a582e706e67\"><img src=\"https://camo.githubusercontent.com/37e75ef1dd991acca2be192ad0362fee608496a3/687474703a2f2f692e696d6775722e636f6d2f566c4736385a582e706e67\" alt=\"cpu\" data-canonical-src=\"http://i.imgur.com/VlG68ZX.png\" style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> threading\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.client <span class=\"pl-k\">import</span> timeline\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_queue</span>():\n\n    feature_input <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">49</span>, <span class=\"pl-c1\">512</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>queue_inputs<span class=\"pl-pds\">\"</span></span>)\n    target_intput <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">30</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>queue_targets<span class=\"pl-pds\">\"</span></span>)\n\n    queue <span class=\"pl-k\">=</span> tf.FIFOQueue(\n        <span class=\"pl-v\">capacity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>,\n        <span class=\"pl-v\">dtypes</span><span class=\"pl-k\">=</span>[tf.float32, tf.float32],\n        <span class=\"pl-v\">shapes</span><span class=\"pl-k\">=</span>[[<span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">49</span>, <span class=\"pl-c1\">512</span>], [<span class=\"pl-c1\">30</span>]],\n        <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FIFOQueue<span class=\"pl-pds\">\"</span></span>\n    )\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Enqueue and dequeue operations</span>\n    enqueue_op <span class=\"pl-k\">=</span> queue.enqueue_many([feature_input, target_intput])\n    queue_inputs, queue_targets <span class=\"pl-k\">=</span> queue.dequeue_many(<span class=\"pl-c1\">64</span>)\n\n    queue_size <span class=\"pl-k\">=</span> queue.size()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Very simple training operator, dequeue examples from the FIFOQueue and</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> square the matrix elementwise. Just for testing of course.</span>\n    square_op <span class=\"pl-k\">=</span> tf.square(queue_inputs)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Coordinator for threads</span>\n    coord <span class=\"pl-k\">=</span> tf.train.Coordinator()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize the TensorFlow session</span>\n    gpu_options <span class=\"pl-k\">=</span> tf.GPUOptions(\n        <span class=\"pl-v\">per_process_gpu_memory_fraction</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.75</span>,\n    )\n\n    sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(\n        <span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options,\n        <span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n        <span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>\n    ))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">load_and_enqueue</span>():\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Just feed random stuff to the queue</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>features = np.random.rand(64, 30, 49, 512)</span>\n            features <span class=\"pl-k\">=</span> np.ones([<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">49</span>, <span class=\"pl-c1\">512</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n            targets  <span class=\"pl-k\">=</span> np.ones([<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">30</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Feed example to Tensorflow placeholder</span>\n            feed_dict <span class=\"pl-k\">=</span> {\n                feature_input: features,\n                target_intput: targets\n            }\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Push all the training examples to the queue</span>\n            sess.run(enqueue_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed_dict)\n\n            <span class=\"pl-k\">if</span> coord.should_stop():\n                <span class=\"pl-k\">break</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Start the threads</span>\n    num_threads <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_threads):\n        t <span class=\"pl-k\">=</span> threading.Thread(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>load_and_enqueue)\n        t.setDaemon(<span class=\"pl-c1\">True</span>)\n        t.start()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make sure the queueu is filled with some examples (n = 500)</span>\n    num_samples_in_queue <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">while</span> num_samples_in_queue <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">500</span>:\n        num_samples_in_queue <span class=\"pl-k\">=</span> sess.run(queue_size)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Initializing queue, current size = <span class=\"pl-c1\">%i</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> num_samples_in_queue)\n        time.sleep(<span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize the session</span>\n    init <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n    sess.run(init)\n\n    run_options  <span class=\"pl-k\">=</span> tf.RunOptions(<span class=\"pl-v\">trace_level</span><span class=\"pl-k\">=</span>tf.RunOptions.<span class=\"pl-c1\">FULL_TRACE</span>)\n    run_metadata <span class=\"pl-k\">=</span> tf.RunMetadata()\n\n    <span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Step = <span class=\"pl-c1\">%i</span>, QueueSize = <span class=\"pl-c1\">%i</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (step, sess.run(queue_size)))\n\n        <span class=\"pl-k\">if</span> step <span class=\"pl-k\">==</span> <span class=\"pl-c1\">10</span>:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Perform a step with saving the results as timeline object</span>\n            result <span class=\"pl-k\">=</span> sess.run(square_op, <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>run_options, <span class=\"pl-v\">run_metadata</span><span class=\"pl-k\">=</span>run_metadata)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the Timeline object, and write it to a json</span>\n            tl <span class=\"pl-k\">=</span> timeline.Timeline(run_metadata.step_stats)\n            ctf <span class=\"pl-k\">=</span> tl.generate_chrome_trace_format()\n            <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timeline.json<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>writing to timeline.json<span class=\"pl-pds\">\"</span></span>)\n                f.write(ctf)\n\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Perform a step without saving the results</span>\n            result <span class=\"pl-k\">=</span> sess.run(square_op)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Ask for the threads to stop</span>\n    coord.request_stop()\n    sess.close()\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    test_queue()\n</pre></div>", "body_text": "Oke, I wrote a simple script (see below) in which the threads simple enqueue batches of np.ones([64, 30, 49, 512], dtype=np.float32) to the FIFOQueue. The main loop simple dequeues 64 examples and performs a simple tf.square of the input tensor. Before the main loop starts I make sure that the queue is filled with a sufficient number of examples. Again, the dequeue operation takes a \"long\" time to finish as you can see in the timelines below.\nGPU running on Titan X\n\nCPU running on Xeon E5-2640\n\nimport time\nimport numpy as np\nimport threading\n\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\n\ndef test_queue():\n\n    feature_input = tf.placeholder(tf.float32, shape=[None, 30, 49, 512], name=\"queue_inputs\")\n    target_intput = tf.placeholder(tf.float32, shape=[None, 30], name=\"queue_targets\")\n\n    queue = tf.FIFOQueue(\n        capacity=1000,\n        dtypes=[tf.float32, tf.float32],\n        shapes=[[30, 49, 512], [30]],\n        name=\"FIFOQueue\"\n    )\n\n    # Enqueue and dequeue operations\n    enqueue_op = queue.enqueue_many([feature_input, target_intput])\n    queue_inputs, queue_targets = queue.dequeue_many(64)\n\n    queue_size = queue.size()\n\n    # Very simple training operator, dequeue examples from the FIFOQueue and\n    # square the matrix elementwise. Just for testing of course.\n    square_op = tf.square(queue_inputs)\n\n    # Coordinator for threads\n    coord = tf.train.Coordinator()\n\n    # Initialize the TensorFlow session\n    gpu_options = tf.GPUOptions(\n        per_process_gpu_memory_fraction=0.75,\n    )\n\n    sess = tf.Session(config=tf.ConfigProto(\n        gpu_options=gpu_options,\n        log_device_placement=False,\n        allow_soft_placement=False\n    ))\n\n    def load_and_enqueue():\n        while True:\n            # Just feed random stuff to the queue\n            #features = np.random.rand(64, 30, 49, 512)\n            features = np.ones([64, 30, 49, 512], dtype=np.float32)\n            targets  = np.ones([64, 30], dtype=np.float32)\n\n            # Feed example to Tensorflow placeholder\n            feed_dict = {\n                feature_input: features,\n                target_intput: targets\n            }\n\n            # Push all the training examples to the queue\n            sess.run(enqueue_op, feed_dict=feed_dict)\n\n            if coord.should_stop():\n                break\n\n    # Start the threads\n    num_threads = 4\n    for i in range(num_threads):\n        t = threading.Thread(target=load_and_enqueue)\n        t.setDaemon(True)\n        t.start()\n\n    # Make sure the queueu is filled with some examples (n = 500)\n    num_samples_in_queue = 0\n    while num_samples_in_queue < 500:\n        num_samples_in_queue = sess.run(queue_size)\n        print(\"Initializing queue, current size = %i\" % num_samples_in_queue)\n        time.sleep(1)\n\n    # Initialize the session\n    init = tf.initialize_all_variables()\n    sess.run(init)\n\n    run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n\n    for step in range(1000):\n\n        print(\"Step = %i, QueueSize = %i\" % (step, sess.run(queue_size)))\n\n        if step == 10:\n            # Perform a step with saving the results as timeline object\n            result = sess.run(square_op, options=run_options, run_metadata=run_metadata)\n\n            # Create the Timeline object, and write it to a json\n            tl = timeline.Timeline(run_metadata.step_stats)\n            ctf = tl.generate_chrome_trace_format()\n            with open('timeline.json', 'w') as f:\n                print(\"writing to timeline.json\")\n                f.write(ctf)\n\n        else:\n            # Perform a step without saving the results\n            result = sess.run(square_op)\n\n    # Ask for the threads to stop\n    coord.request_stop()\n    sess.close()\n\n\nif __name__ == \"__main__\":\n    test_queue()", "body": "Oke, I wrote a simple script (see below) in which the threads simple enqueue batches of `np.ones([64, 30, 49, 512], dtype=np.float32)` to the `FIFOQueue`. The main loop simple dequeues 64 examples and performs a simple `tf.square` of the input tensor. Before the main loop starts I make sure that the queue is filled with a sufficient number of examples. Again, the dequeue operation takes a \"long\" time to finish as you can see in the timelines below. \n\n**GPU running on Titan X**\n![gpu](http://i.imgur.com/VrKa51G.png)\n\n**CPU running on Xeon E5-2640**\n![cpu](http://i.imgur.com/VlG68ZX.png)\n\n``` python\nimport time\nimport numpy as np\nimport threading\n\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\n\ndef test_queue():\n\n    feature_input = tf.placeholder(tf.float32, shape=[None, 30, 49, 512], name=\"queue_inputs\")\n    target_intput = tf.placeholder(tf.float32, shape=[None, 30], name=\"queue_targets\")\n\n    queue = tf.FIFOQueue(\n        capacity=1000,\n        dtypes=[tf.float32, tf.float32],\n        shapes=[[30, 49, 512], [30]],\n        name=\"FIFOQueue\"\n    )\n\n    # Enqueue and dequeue operations\n    enqueue_op = queue.enqueue_many([feature_input, target_intput])\n    queue_inputs, queue_targets = queue.dequeue_many(64)\n\n    queue_size = queue.size()\n\n    # Very simple training operator, dequeue examples from the FIFOQueue and\n    # square the matrix elementwise. Just for testing of course.\n    square_op = tf.square(queue_inputs)\n\n    # Coordinator for threads\n    coord = tf.train.Coordinator()\n\n    # Initialize the TensorFlow session\n    gpu_options = tf.GPUOptions(\n        per_process_gpu_memory_fraction=0.75,\n    )\n\n    sess = tf.Session(config=tf.ConfigProto(\n        gpu_options=gpu_options,\n        log_device_placement=False,\n        allow_soft_placement=False\n    ))\n\n    def load_and_enqueue():\n        while True:\n            # Just feed random stuff to the queue\n            #features = np.random.rand(64, 30, 49, 512)\n            features = np.ones([64, 30, 49, 512], dtype=np.float32)\n            targets  = np.ones([64, 30], dtype=np.float32)\n\n            # Feed example to Tensorflow placeholder\n            feed_dict = {\n                feature_input: features,\n                target_intput: targets\n            }\n\n            # Push all the training examples to the queue\n            sess.run(enqueue_op, feed_dict=feed_dict)\n\n            if coord.should_stop():\n                break\n\n    # Start the threads\n    num_threads = 4\n    for i in range(num_threads):\n        t = threading.Thread(target=load_and_enqueue)\n        t.setDaemon(True)\n        t.start()\n\n    # Make sure the queueu is filled with some examples (n = 500)\n    num_samples_in_queue = 0\n    while num_samples_in_queue < 500:\n        num_samples_in_queue = sess.run(queue_size)\n        print(\"Initializing queue, current size = %i\" % num_samples_in_queue)\n        time.sleep(1)\n\n    # Initialize the session\n    init = tf.initialize_all_variables()\n    sess.run(init)\n\n    run_options  = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n\n    for step in range(1000):\n\n        print(\"Step = %i, QueueSize = %i\" % (step, sess.run(queue_size)))\n\n        if step == 10:\n            # Perform a step with saving the results as timeline object\n            result = sess.run(square_op, options=run_options, run_metadata=run_metadata)\n\n            # Create the Timeline object, and write it to a json\n            tl = timeline.Timeline(run_metadata.step_stats)\n            ctf = tl.generate_chrome_trace_format()\n            with open('timeline.json', 'w') as f:\n                print(\"writing to timeline.json\")\n                f.write(ctf)\n\n        else:\n            # Perform a step without saving the results\n            result = sess.run(square_op)\n\n    # Ask for the threads to stop\n    coord.request_stop()\n    sess.close()\n\n\nif __name__ == \"__main__\":\n    test_queue()\n\n```\n"}