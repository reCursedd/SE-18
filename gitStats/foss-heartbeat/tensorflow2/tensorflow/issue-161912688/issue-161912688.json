{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3009", "id": 161912688, "node_id": "MDU6SXNzdWUxNjE5MTI2ODg=", "number": 3009, "title": "FIFOQueue: dequeue many operation very slow?", "user": {"login": "tomrunia", "id": 5536129, "node_id": "MDQ6VXNlcjU1MzYxMjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5536129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomrunia", "html_url": "https://github.com/tomrunia", "followers_url": "https://api.github.com/users/tomrunia/followers", "following_url": "https://api.github.com/users/tomrunia/following{/other_user}", "gists_url": "https://api.github.com/users/tomrunia/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomrunia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomrunia/subscriptions", "organizations_url": "https://api.github.com/users/tomrunia/orgs", "repos_url": "https://api.github.com/users/tomrunia/repos", "events_url": "https://api.github.com/users/tomrunia/events{/privacy}", "received_events_url": "https://api.github.com/users/tomrunia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2016-06-23T12:39:49Z", "updated_at": "2017-11-28T14:18:25Z", "closed_at": "2016-07-18T18:35:59Z", "author_association": "NONE", "body_html": "<p>When training a relatively simple model (1-layer LSTM, 256 units) my Titan X GPU keeps spiking from 0% to 30% GPU utilization. Conclusion: somewhere in the pipeline there is a bottleneck which limits the GPU to be processing the training batches continuously. I use a FIFOQueue to which examples are being fed in one or more separate threads:</p>\n<div class=\"highlight highlight-source-python\"><pre>queue <span class=\"pl-k\">=</span> tf.FIFOQueue(\n     <span class=\"pl-v\">capacity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.config.queue_capacity,\n     <span class=\"pl-v\">dtypes</span><span class=\"pl-k\">=</span>[tf.float32, tf.float32],\n     <span class=\"pl-v\">shapes</span><span class=\"pl-k\">=</span>[[<span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">49</span>, <span class=\"pl-c1\">512</span>], [<span class=\"pl-c1\">30</span>]],\n     <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>FIFOQueue<span class=\"pl-pds\">\"</span></span>\n)</pre></div>\n<p>For the training operation I use <code>queue.dequeue_many</code> to get examples from the queue. As you can see the batch size is 64 examples. So in the end the input tensor is <code>64x30x49x512</code> of type <code>tf.float32</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Model inputs, either use the queue (training) or feed_dict (evaluation)</span>\ninputs, targets <span class=\"pl-k\">=</span> queue.dequeue_many(<span class=\"pl-c1\">64</span>)</pre></div>\n<p>To find out why my code is running \"slow\" (i.e. spiking GPU allocation and no temperature increase) I use the <code>Timeline</code> object (<a href=\"http://stackoverflow.com/questions/34293714/tensorflow-can-i-measure-the-execution-time-of-individual-operations\" rel=\"nofollow\">see here</a>) to measure execution times of individual operations. The results displayed below show the measurements for one training iteration at which point the queue was filled with more than 1000 examples. I have included screenshots for both GPU and CPU-only runs (forced with <code>export CUDA_VISIBLE_DEVICES=-1</code>.</p>\n<p>What strikes me from these results is that it takes a really long time to dequeue examples from the FIFOQueue. What is happening here...something wrong or is the dequeuing operation just very slow? Overall the dequeuing operation and sending the data to the GPU takes up half of the time of a training iteration. No wonder that the GPU utilization is spiking. Any help is welcome optimizing my training pipeline! As I understand correctly the examples are all queued in RAM, is there also a way to queue them ahead on GPU memory so when they are needed they do not have to be moved CPU =&gt; GPU?</p>\n<p>This is tested on TensorFlow v9.0 build from sources about 1.5 week ago.</p>\n<p><strong>GPU running on Titan X</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/00bf7d43b758040151933111f311860a43e1f37d/687474703a2f2f692e696d6775722e636f6d2f65545a4e50444d2e706e673f31\"><img src=\"https://camo.githubusercontent.com/00bf7d43b758040151933111f311860a43e1f37d/687474703a2f2f692e696d6775722e636f6d2f65545a4e50444d2e706e673f31\" alt=\"gpu\" data-canonical-src=\"http://i.imgur.com/eTZNPDM.png?1\" style=\"max-width:100%;\"></a></p>\n<p><strong>CPU running on Xeon CPU E5-2640</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/41ac426ca934b3bea70f53b3a05efbbcce49b4bc/687474703a2f2f692e696d6775722e636f6d2f6d516b4c4838682e706e673f31\"><img src=\"https://camo.githubusercontent.com/41ac426ca934b3bea70f53b3a05efbbcce49b4bc/687474703a2f2f692e696d6775722e636f6d2f6d516b4c4838682e706e673f31\" alt=\"cpu\" data-canonical-src=\"http://i.imgur.com/mQkLH8h.png?1\" style=\"max-width:100%;\"></a></p>", "body_text": "When training a relatively simple model (1-layer LSTM, 256 units) my Titan X GPU keeps spiking from 0% to 30% GPU utilization. Conclusion: somewhere in the pipeline there is a bottleneck which limits the GPU to be processing the training batches continuously. I use a FIFOQueue to which examples are being fed in one or more separate threads:\nqueue = tf.FIFOQueue(\n     capacity=self.config.queue_capacity,\n     dtypes=[tf.float32, tf.float32],\n     shapes=[[30, 49, 512], [30]],\n     name=\"FIFOQueue\"\n)\nFor the training operation I use queue.dequeue_many to get examples from the queue. As you can see the batch size is 64 examples. So in the end the input tensor is 64x30x49x512 of type tf.float32:\n# Model inputs, either use the queue (training) or feed_dict (evaluation)\ninputs, targets = queue.dequeue_many(64)\nTo find out why my code is running \"slow\" (i.e. spiking GPU allocation and no temperature increase) I use the Timeline object (see here) to measure execution times of individual operations. The results displayed below show the measurements for one training iteration at which point the queue was filled with more than 1000 examples. I have included screenshots for both GPU and CPU-only runs (forced with export CUDA_VISIBLE_DEVICES=-1.\nWhat strikes me from these results is that it takes a really long time to dequeue examples from the FIFOQueue. What is happening here...something wrong or is the dequeuing operation just very slow? Overall the dequeuing operation and sending the data to the GPU takes up half of the time of a training iteration. No wonder that the GPU utilization is spiking. Any help is welcome optimizing my training pipeline! As I understand correctly the examples are all queued in RAM, is there also a way to queue them ahead on GPU memory so when they are needed they do not have to be moved CPU => GPU?\nThis is tested on TensorFlow v9.0 build from sources about 1.5 week ago.\nGPU running on Titan X\n\nCPU running on Xeon CPU E5-2640", "body": "When training a relatively simple model (1-layer LSTM, 256 units) my Titan X GPU keeps spiking from 0% to 30% GPU utilization. Conclusion: somewhere in the pipeline there is a bottleneck which limits the GPU to be processing the training batches continuously. I use a FIFOQueue to which examples are being fed in one or more separate threads:\n\n``` python\nqueue = tf.FIFOQueue(\n     capacity=self.config.queue_capacity,\n     dtypes=[tf.float32, tf.float32],\n     shapes=[[30, 49, 512], [30]],\n     name=\"FIFOQueue\"\n)\n```\n\nFor the training operation I use `queue.dequeue_many` to get examples from the queue. As you can see the batch size is 64 examples. So in the end the input tensor is `64x30x49x512` of type `tf.float32`:\n\n``` python\n# Model inputs, either use the queue (training) or feed_dict (evaluation)\ninputs, targets = queue.dequeue_many(64)\n```\n\nTo find out why my code is running \"slow\" (i.e. spiking GPU allocation and no temperature increase) I use the `Timeline` object ([see here](http://stackoverflow.com/questions/34293714/tensorflow-can-i-measure-the-execution-time-of-individual-operations)) to measure execution times of individual operations. The results displayed below show the measurements for one training iteration at which point the queue was filled with more than 1000 examples. I have included screenshots for both GPU and CPU-only runs (forced with `export CUDA_VISIBLE_DEVICES=-1`. \n\nWhat strikes me from these results is that it takes a really long time to dequeue examples from the FIFOQueue. What is happening here...something wrong or is the dequeuing operation just very slow? Overall the dequeuing operation and sending the data to the GPU takes up half of the time of a training iteration. No wonder that the GPU utilization is spiking. Any help is welcome optimizing my training pipeline! As I understand correctly the examples are all queued in RAM, is there also a way to queue them ahead on GPU memory so when they are needed they do not have to be moved CPU => GPU?\n\nThis is tested on TensorFlow v9.0 build from sources about 1.5 week ago.\n\n**GPU running on Titan X**\n![gpu](http://i.imgur.com/eTZNPDM.png?1) \n\n**CPU running on Xeon CPU E5-2640**\n![cpu](http://i.imgur.com/mQkLH8h.png?1)\n"}