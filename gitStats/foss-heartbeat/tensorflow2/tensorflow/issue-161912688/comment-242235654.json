{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242235654", "html_url": "https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-242235654", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3009", "id": 242235654, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjIzNTY1NA==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-24T23:05:34Z", "updated_at": "2016-08-24T23:05:34Z", "author_association": "NONE", "body_html": "<p>Is there any progress on improving Numpy -&gt; GPU throughput with Tensorflow?</p>\n<p>I have a similar benchmark to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5536129\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tomrunia\">@tomrunia</a> of passing a O(100MB) tensor from Numpy to GPU and returning a small slice of the tensor. This would ideally be bottlenecked by PCI-e bandwidth, but runs at ~1/10th of that rate. Additionally, I see very poor scaling when attempting to run this tasks over multiple GPUs (on a single node), while theoretically I should benefit from the additional PCI-e bandwidth. (all stats from Tensorflow 0.9 built from source).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a>'s description is very useful, but I'd like to understand more of what's going on when passing through feed_dict, enqueuing, dequeuing.<br>\nMy understanding:<br>\nWhen a Numpy tensor is passed through a feed_dict it is copied into a host side Eigen tensor.<br>\nWhen a tensor is enqueued, it is added to a queue that lives on the host.<br>\nWhen a tensor is dequeued on the GPU, a cudaMemcpy copies the tensor from host to device. This should run near PCI-e bandwidth.</p>\n<p>Questions about my understanding:<br>\nIs enqueueing a tensor done by reference or with a full copy (and memory allocation)?<br>\nAre the memcpy's on feed_dict passing (and enqueue if there's a memcpy there) performed by a single thread for the full Tensorflow session? Even if the <code>Session.run</code> calls come from multiple Python threads?</p>\n<p>Are there plans to fix this, such as a queue that is resident on the GPU?</p>", "body_text": "Is there any progress on improving Numpy -> GPU throughput with Tensorflow?\nI have a similar benchmark to @tomrunia of passing a O(100MB) tensor from Numpy to GPU and returning a small slice of the tensor. This would ideally be bottlenecked by PCI-e bandwidth, but runs at ~1/10th of that rate. Additionally, I see very poor scaling when attempting to run this tasks over multiple GPUs (on a single node), while theoretically I should benefit from the additional PCI-e bandwidth. (all stats from Tensorflow 0.9 built from source).\n@prb12's description is very useful, but I'd like to understand more of what's going on when passing through feed_dict, enqueuing, dequeuing.\nMy understanding:\nWhen a Numpy tensor is passed through a feed_dict it is copied into a host side Eigen tensor.\nWhen a tensor is enqueued, it is added to a queue that lives on the host.\nWhen a tensor is dequeued on the GPU, a cudaMemcpy copies the tensor from host to device. This should run near PCI-e bandwidth.\nQuestions about my understanding:\nIs enqueueing a tensor done by reference or with a full copy (and memory allocation)?\nAre the memcpy's on feed_dict passing (and enqueue if there's a memcpy there) performed by a single thread for the full Tensorflow session? Even if the Session.run calls come from multiple Python threads?\nAre there plans to fix this, such as a queue that is resident on the GPU?", "body": "Is there any progress on improving Numpy -> GPU throughput with Tensorflow?\n\nI have a similar benchmark to @tomrunia of passing a O(100MB) tensor from Numpy to GPU and returning a small slice of the tensor. This would ideally be bottlenecked by PCI-e bandwidth, but runs at ~1/10th of that rate. Additionally, I see very poor scaling when attempting to run this tasks over multiple GPUs (on a single node), while theoretically I should benefit from the additional PCI-e bandwidth. (all stats from Tensorflow 0.9 built from source).\n\n@prb12's description is very useful, but I'd like to understand more of what's going on when passing through feed_dict, enqueuing, dequeuing.\nMy understanding:\nWhen a Numpy tensor is passed through a feed_dict it is copied into a host side Eigen tensor.\nWhen a tensor is enqueued, it is added to a queue that lives on the host.\nWhen a tensor is dequeued on the GPU, a cudaMemcpy copies the tensor from host to device. This should run near PCI-e bandwidth.\n\nQuestions about my understanding:\nIs enqueueing a tensor done by reference or with a full copy (and memory allocation)?\nAre the memcpy's on feed_dict passing (and enqueue if there's a memcpy there) performed by a single thread for the full Tensorflow session? Even if the `Session.run` calls come from multiple Python threads?\n\nAre there plans to fix this, such as a queue that is resident on the GPU?\n"}