{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16132", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16132/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16132/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16132/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16132", "id": 288645017, "node_id": "MDU6SXNzdWUyODg2NDUwMTc=", "number": 16132, "title": "Bug while printing parameters and gradients", "user": {"login": "nicola-decao", "id": 9703100, "node_id": "MDQ6VXNlcjk3MDMxMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/9703100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nicola-decao", "html_url": "https://github.com/nicola-decao", "followers_url": "https://api.github.com/users/nicola-decao/followers", "following_url": "https://api.github.com/users/nicola-decao/following{/other_user}", "gists_url": "https://api.github.com/users/nicola-decao/gists{/gist_id}", "starred_url": "https://api.github.com/users/nicola-decao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nicola-decao/subscriptions", "organizations_url": "https://api.github.com/users/nicola-decao/orgs", "repos_url": "https://api.github.com/users/nicola-decao/repos", "events_url": "https://api.github.com/users/nicola-decao/events{/privacy}", "received_events_url": "https://api.github.com/users/nicola-decao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-15T16:08:51Z", "updated_at": "2018-01-19T22:26:56Z", "closed_at": "2018-01-19T22:26:56Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (anaconda)</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.4.0-19-ga52c8d9 1.4.1</li>\n<li><strong>Python version</strong>: 3.6.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: using CPU</li>\n<li><strong>GPU model and memory</strong>: using CPU</li>\n<li><strong>Exact command to reproduce</strong>: see below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.</p>\n<h3>Source code / logs</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\ntf.set_random_seed(42)\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets('data/', one_hot=True)\n\nx = tf.placeholder(tf.float32, shape=(None, 784))\ny = tf.placeholder(tf.float32, shape=(None, 10))\n\nW = tf.get_variable('W0', (784, 10))\npred = tf.matmul(x, W)\nloss = tf.reduce_sum((y - pred) ** 2)\ngrads = tf.gradients(loss, W)\ntrain_step = tf.train.AdamOptimizer().minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nprint(sess.run(W))\n\n&gt;&gt;&gt; [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\n  -0.01042821]\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\n  -0.0796528 ]\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\n  -0.04312544]\n ...\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\n   0.06578781]\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\n  -0.02337921]\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\n  -0.02436799]]\n\nfor _ in range(1000):\n    x_mb, y_mb = mnist.train.next_batch(32)\n    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})\n    print('loss: {:2.5}'.format(loss_))\n\n&gt;&gt;&gt; I won't print uselss log here but the loss is decreasing\n\nprint(sess.run(W))\n\n&gt;&gt;&gt; [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\n  -0.01042821]\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\n  -0.0796528 ]\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\n  -0.04312544]\n ...\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\n   0.06578781]\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\n  -0.02337921]\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\n  -0.02436799]]\n\nprint(sess.run(grads, {x: x_mb, y: y_mb}))\n\n&gt;&gt;&gt; [array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary (anaconda)\nTensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1\nPython version: 3.6.4\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: using CPU\nGPU model and memory: using CPU\nExact command to reproduce: see below\n\nDescribe the problem\nThe model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\n\ntf.set_random_seed(42)\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets('data/', one_hot=True)\n\nx = tf.placeholder(tf.float32, shape=(None, 784))\ny = tf.placeholder(tf.float32, shape=(None, 10))\n\nW = tf.get_variable('W0', (784, 10))\npred = tf.matmul(x, W)\nloss = tf.reduce_sum((y - pred) ** 2)\ngrads = tf.gradients(loss, W)\ntrain_step = tf.train.AdamOptimizer().minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nprint(sess.run(W))\n\n>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\n  -0.01042821]\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\n  -0.0796528 ]\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\n  -0.04312544]\n ...\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\n   0.06578781]\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\n  -0.02337921]\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\n  -0.02436799]]\n\nfor _ in range(1000):\n    x_mb, y_mb = mnist.train.next_batch(32)\n    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})\n    print('loss: {:2.5}'.format(loss_))\n\n>>> I won't print uselss log here but the loss is decreasing\n\nprint(sess.run(W))\n\n>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\n  -0.01042821]\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\n  -0.0796528 ]\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\n  -0.04312544]\n ...\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\n   0.06578781]\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\n  -0.02337921]\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\n  -0.02436799]]\n\nprint(sess.run(grads, {x: x_mb, y: y_mb}))\n\n>>> [array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (anaconda)\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: using CPU\r\n- **GPU model and memory**: using CPU\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nThe model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.set_random_seed(42)\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nmnist = input_data.read_data_sets('data/', one_hot=True)\r\n\r\nx = tf.placeholder(tf.float32, shape=(None, 784))\r\ny = tf.placeholder(tf.float32, shape=(None, 10))\r\n\r\nW = tf.get_variable('W0', (784, 10))\r\npred = tf.matmul(x, W)\r\nloss = tf.reduce_sum((y - pred) ** 2)\r\ngrads = tf.gradients(loss, W)\r\ntrain_step = tf.train.AdamOptimizer().minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nprint(sess.run(W))\r\n\r\n>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\r\n  -0.01042821]\r\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\r\n  -0.0796528 ]\r\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\r\n  -0.04312544]\r\n ...\r\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\r\n   0.06578781]\r\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\r\n  -0.02337921]\r\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\r\n  -0.02436799]]\r\n\r\nfor _ in range(1000):\r\n    x_mb, y_mb = mnist.train.next_batch(32)\r\n    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})\r\n    print('loss: {:2.5}'.format(loss_))\r\n\r\n>>> I won't print uselss log here but the loss is decreasing\r\n\r\nprint(sess.run(W))\r\n\r\n>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605\r\n  -0.01042821]\r\n [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129\r\n  -0.0796528 ]\r\n [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736\r\n  -0.04312544]\r\n ...\r\n [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174\r\n   0.06578781]\r\n [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339\r\n  -0.02337921]\r\n [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324\r\n  -0.02436799]]\r\n\r\nprint(sess.run(grads, {x: x_mb, y: y_mb}))\r\n\r\n>>> [array([[0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       ...,\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.],\r\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]\r\n\r\n```\r\n"}