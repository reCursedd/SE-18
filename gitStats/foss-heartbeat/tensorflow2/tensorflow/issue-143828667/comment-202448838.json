{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/202448838", "html_url": "https://github.com/tensorflow/tensorflow/issues/1668#issuecomment-202448838", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1668", "id": 202448838, "node_id": "MDEyOklzc3VlQ29tbWVudDIwMjQ0ODgzOA==", "user": {"login": "keveman", "id": 229914, "node_id": "MDQ6VXNlcjIyOTkxNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/229914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keveman", "html_url": "https://github.com/keveman", "followers_url": "https://api.github.com/users/keveman/followers", "following_url": "https://api.github.com/users/keveman/following{/other_user}", "gists_url": "https://api.github.com/users/keveman/gists{/gist_id}", "starred_url": "https://api.github.com/users/keveman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keveman/subscriptions", "organizations_url": "https://api.github.com/users/keveman/orgs", "repos_url": "https://api.github.com/users/keveman/repos", "events_url": "https://api.github.com/users/keveman/events{/privacy}", "received_events_url": "https://api.github.com/users/keveman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-28T15:44:37Z", "updated_at": "2016-03-28T15:44:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The attention mask is available as a tensor here :<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L522\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L522</a></p>\n<p>It should be easy to fetch it out during a run call and visualize it. You can try posting this to StackOverflow  to see if someone in the general community has done this visualization. I am closing this issue, since we have the required functionality in TensorFlow.</p>", "body_text": "The attention mask is available as a tensor here :\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L522\nIt should be easy to fetch it out during a run call and visualize it. You can try posting this to StackOverflow  to see if someone in the general community has done this visualization. I am closing this issue, since we have the required functionality in TensorFlow.", "body": "The attention mask is available as a tensor here : \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L522\n\nIt should be easy to fetch it out during a run call and visualize it. You can try posting this to StackOverflow  to see if someone in the general community has done this visualization. I am closing this issue, since we have the required functionality in TensorFlow.\n"}