{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/223449591", "html_url": "https://github.com/tensorflow/tensorflow/issues/2627#issuecomment-223449591", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2627", "id": 223449591, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMzQ0OTU5MQ==", "user": {"login": "ibab", "id": 890531, "node_id": "MDQ6VXNlcjg5MDUzMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/890531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ibab", "html_url": "https://github.com/ibab", "followers_url": "https://api.github.com/users/ibab/followers", "following_url": "https://api.github.com/users/ibab/following{/other_user}", "gists_url": "https://api.github.com/users/ibab/gists{/gist_id}", "starred_url": "https://api.github.com/users/ibab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ibab/subscriptions", "organizations_url": "https://api.github.com/users/ibab/orgs", "repos_url": "https://api.github.com/users/ibab/repos", "events_url": "https://api.github.com/users/ibab/events{/privacy}", "received_events_url": "https://api.github.com/users/ibab/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-02T23:11:32Z", "updated_at": "2016-06-02T23:12:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It can be fixed by rewriting it like this</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Complex<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_ComplexGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Returns the real and imaginary components of 'grad', respectively.<span class=\"pl-pds\">\"\"\"</span></span>\n  x <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n  y <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">1</span>]\n  sx <span class=\"pl-k\">=</span> array_ops.shape(x)\n  sy <span class=\"pl-k\">=</span> array_ops.shape(y)\n  rx, ry <span class=\"pl-k\">=</span> gen_array_ops._broadcast_gradient_args(sx, sy)\n  <span class=\"pl-k\">return</span> (array_ops.reshape(math_ops.reduce_sum(math_ops.real(grad), rx), sx),\n          array_ops.reshape(math_ops.reduce_sum(math_ops.imag(grad), ry), sy))</pre></div>\n<p>I've looked through the rest of the <code>math_ops</code>, and couldn't spot another case where the broadcasting has been missed.<br>\nWould be nice to write a bit of code to apply this reshape-reduce trick, though, as it's used very often.</p>\n<p>I'll check whether there are specific tests for this problem and make a PR.</p>\n<p>When implementing the gradient like this, @yshady's example above runs without problems <g-emoji class=\"g-emoji\" alias=\"+1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44d.png\">\ud83d\udc4d</g-emoji></p>", "body_text": "It can be fixed by rewriting it like this\n@ops.RegisterGradient(\"Complex\")\ndef _ComplexGrad(op, grad):\n  \"\"\"Returns the real and imaginary components of 'grad', respectively.\"\"\"\n  x = op.inputs[0]\n  y = op.inputs[1]\n  sx = array_ops.shape(x)\n  sy = array_ops.shape(y)\n  rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  return (array_ops.reshape(math_ops.reduce_sum(math_ops.real(grad), rx), sx),\n          array_ops.reshape(math_ops.reduce_sum(math_ops.imag(grad), ry), sy))\nI've looked through the rest of the math_ops, and couldn't spot another case where the broadcasting has been missed.\nWould be nice to write a bit of code to apply this reshape-reduce trick, though, as it's used very often.\nI'll check whether there are specific tests for this problem and make a PR.\nWhen implementing the gradient like this, @yshady's example above runs without problems \ud83d\udc4d", "body": "It can be fixed by rewriting it like this\n\n``` python\n@ops.RegisterGradient(\"Complex\")\ndef _ComplexGrad(op, grad):\n  \"\"\"Returns the real and imaginary components of 'grad', respectively.\"\"\"\n  x = op.inputs[0]\n  y = op.inputs[1]\n  sx = array_ops.shape(x)\n  sy = array_ops.shape(y)\n  rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  return (array_ops.reshape(math_ops.reduce_sum(math_ops.real(grad), rx), sx),\n          array_ops.reshape(math_ops.reduce_sum(math_ops.imag(grad), ry), sy))\n```\n\nI've looked through the rest of the `math_ops`, and couldn't spot another case where the broadcasting has been missed.\nWould be nice to write a bit of code to apply this reshape-reduce trick, though, as it's used very often.\n\nI'll check whether there are specific tests for this problem and make a PR.\n\nWhen implementing the gradient like this, @yshady's example above runs without problems \ud83d\udc4d \n"}