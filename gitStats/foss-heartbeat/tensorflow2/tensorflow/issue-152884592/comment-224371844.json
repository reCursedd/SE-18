{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/224371844", "html_url": "https://github.com/tensorflow/tensorflow/issues/2210#issuecomment-224371844", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2210", "id": 224371844, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDM3MTg0NA==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-07T18:30:29Z", "updated_at": "2016-06-07T18:30:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3530212\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sjperkins\">@sjperkins</a> is correct. If you want to share the same GPU between TensorFlow and other Cuda library, you have to be very careful. No Cuda Runtime API is safe. Either you call stream-executor, or call Cuda Driver API, and manage the context yourself. Every time when you are done with your Cuda work, restore the context back to the previous state, because that is what stream-executor implicitly expects.</p>", "body_text": "@sjperkins is correct. If you want to share the same GPU between TensorFlow and other Cuda library, you have to be very careful. No Cuda Runtime API is safe. Either you call stream-executor, or call Cuda Driver API, and manage the context yourself. Every time when you are done with your Cuda work, restore the context back to the previous state, because that is what stream-executor implicitly expects.", "body": "@sjperkins is correct. If you want to share the same GPU between TensorFlow and other Cuda library, you have to be very careful. No Cuda Runtime API is safe. Either you call stream-executor, or call Cuda Driver API, and manage the context yourself. Every time when you are done with your Cuda work, restore the context back to the previous state, because that is what stream-executor implicitly expects. \n"}