{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225321744", "html_url": "https://github.com/tensorflow/tensorflow/pull/2767#issuecomment-225321744", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2767", "id": 225321744, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTMyMTc0NA==", "user": {"login": "PhABC", "id": 9306422, "node_id": "MDQ6VXNlcjkzMDY0MjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/9306422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhABC", "html_url": "https://github.com/PhABC", "followers_url": "https://api.github.com/users/PhABC/followers", "following_url": "https://api.github.com/users/PhABC/following{/other_user}", "gists_url": "https://api.github.com/users/PhABC/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhABC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhABC/subscriptions", "organizations_url": "https://api.github.com/users/PhABC/orgs", "repos_url": "https://api.github.com/users/PhABC/repos", "events_url": "https://api.github.com/users/PhABC/events{/privacy}", "received_events_url": "https://api.github.com/users/PhABC/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-10T23:52:19Z", "updated_at": "2016-06-11T00:05:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> Yes, you can use initializer for the RNN cells, I did miss that. On the other hand, my feature does not only allow initializers. Here, initializers can be a list of 2 matrices that are multipled together before going through the transfer function. The first matrix can be used as an logical index matrix (composed of 1 and 0) which allows you to have various kind of connectivity patterns ( if you set the learnability for this matrix to zero obviously). For example, you can now have a sparse weight matrix or only allow self-connections (identity matrix), but other designs could be used. The second matrix is your real weight matrix, that you put as learnable. Hence, when they are multipled, only the weights that have a non zero index in the first matrix are used.</p>\n<p>This is convenient as you do not need to reassign the weights at every iteration, which is the case if you want a self-connection (identity) constraint with the current initializer. With a few hundred thousands iterations and multiple variables, this could have an impact on performance.</p>", "body_text": "@lukaszkaiser Yes, you can use initializer for the RNN cells, I did miss that. On the other hand, my feature does not only allow initializers. Here, initializers can be a list of 2 matrices that are multipled together before going through the transfer function. The first matrix can be used as an logical index matrix (composed of 1 and 0) which allows you to have various kind of connectivity patterns ( if you set the learnability for this matrix to zero obviously). For example, you can now have a sparse weight matrix or only allow self-connections (identity matrix), but other designs could be used. The second matrix is your real weight matrix, that you put as learnable. Hence, when they are multipled, only the weights that have a non zero index in the first matrix are used.\nThis is convenient as you do not need to reassign the weights at every iteration, which is the case if you want a self-connection (identity) constraint with the current initializer. With a few hundred thousands iterations and multiple variables, this could have an impact on performance.", "body": "@lukaszkaiser Yes, you can use initializer for the RNN cells, I did miss that. On the other hand, my feature does not only allow initializers. Here, initializers can be a list of 2 matrices that are multipled together before going through the transfer function. The first matrix can be used as an logical index matrix (composed of 1 and 0) which allows you to have various kind of connectivity patterns ( if you set the learnability for this matrix to zero obviously). For example, you can now have a sparse weight matrix or only allow self-connections (identity matrix), but other designs could be used. The second matrix is your real weight matrix, that you put as learnable. Hence, when they are multipled, only the weights that have a non zero index in the first matrix are used. \n\nThis is convenient as you do not need to reassign the weights at every iteration, which is the case if you want a self-connection (identity) constraint with the current initializer. With a few hundred thousands iterations and multiple variables, this could have an impact on performance.  \n"}