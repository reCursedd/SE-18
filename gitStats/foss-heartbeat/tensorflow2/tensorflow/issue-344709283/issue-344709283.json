{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21146", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21146/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21146/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21146/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/21146", "id": 344709283, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA0MDM4NjY3", "number": 21146, "title": "A faster BatchSelectFunctor for tf.where on CPU.", "user": {"login": "lowintelligence", "id": 10669111, "node_id": "MDQ6VXNlcjEwNjY5MTEx", "avatar_url": "https://avatars0.githubusercontent.com/u/10669111?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lowintelligence", "html_url": "https://github.com/lowintelligence", "followers_url": "https://api.github.com/users/lowintelligence/followers", "following_url": "https://api.github.com/users/lowintelligence/following{/other_user}", "gists_url": "https://api.github.com/users/lowintelligence/gists{/gist_id}", "starred_url": "https://api.github.com/users/lowintelligence/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lowintelligence/subscriptions", "organizations_url": "https://api.github.com/users/lowintelligence/orgs", "repos_url": "https://api.github.com/users/lowintelligence/repos", "events_url": "https://api.github.com/users/lowintelligence/events{/privacy}", "received_events_url": "https://api.github.com/users/lowintelligence/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 987666414, "node_id": "MDU6TGFiZWw5ODc2NjY0MTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/ready%20to%20pull", "name": "ready to pull", "color": "2cd643", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-26T06:19:01Z", "updated_at": "2018-08-09T20:28:03Z", "closed_at": "2018-08-09T20:28:03Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21146", "html_url": "https://github.com/tensorflow/tensorflow/pull/21146", "diff_url": "https://github.com/tensorflow/tensorflow/pull/21146.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/21146.patch"}, "body_html": "<p>Op 'tf.where(c, t, e)' supports that 't' and 'e' are N-D tensors while 'c' is a 1D tensor, which would call BatchSelectFunctor to get the result. But its basic implementation broadcasts 'c' to the same dimension with 't' and 'e', which would get bad efficiency on CPU for large tensors.</p>\n<p>Here a loop-based implementation would be adopted to make this operation faster on CPU. There would be no broadcast during the calculation, thus would effectively improve the performance (seeing the benchmark test 'BatchSelect').</p>\n<p>Benchmark result below was done on a 2 x E5-2682v4 (16c @ 2.5GHz) platform:<br>\ncompiling args: -mfma -mavx2 -mavx -msse4.2 -msse4.1 -O3<br>\n(old version):<br>\nBenchmark: m_1000_n_10_use_gpu_False \t wall_time: 0.000196 s \t Throughput: 0.409 GB/s<br>\nBenchmark: m_1000_n_100_use_gpu_False \t wall_time: 0.000323 s \t Throughput: 2.48 GB/s<br>\nBenchmark: m_1000_n_1000_use_gpu_False \t wall_time: 0.000623 s \t Throughput: 12.8 GB/s<br>\nBenchmark: m_10000_n_10_use_gpu_False \t wall_time: 0.00031 s \t Throughput: 2.58 GB/s<br>\nBenchmark: m_10000_n_100_use_gpu_False \t wall_time: 0.00061 s \t Throughput: 13.1 GB/s<br>\nBenchmark: m_10000_n_1000_use_gpu_False \t wall_time: 0.00364 s \t Throughput: 22 GB/s<br>\nBenchmark: m_100000_n_10_use_gpu_False \t wall_time: 0.000602 s \t Throughput: 13.3 GB/s<br>\nBenchmark: m_100000_n_100_use_gpu_False \t wall_time: 0.00367 s \t Throughput: 21.8 GB/s<br>\nBenchmark: m_100000_n_1000_use_gpu_False \t wall_time: 0.0309 s \t Throughput: 25.9 GB/s</p>\n<p>(new version):<br>\nBenchmark: m_1000_n_10_use_gpu_False \t wall_time: 0.000153 s \t Throughput: 0.524 GB/s<br>\nBenchmark: m_1000_n_100_use_gpu_False \t wall_time: 0.000178 s \t Throughput: 4.49 GB/s<br>\nBenchmark: m_1000_n_1000_use_gpu_False \t wall_time: 0.000327 s \t Throughput: 24.5 GB/s<br>\nBenchmark: m_10000_n_10_use_gpu_False \t wall_time: 0.000203 s \t Throughput: 3.94 GB/s<br>\nBenchmark: m_10000_n_100_use_gpu_False \t wall_time: 0.000344 s \t Throughput: 23.3 GB/s<br>\nBenchmark: m_10000_n_1000_use_gpu_False \t wall_time: 0.00123 s \t Throughput: 64.9 GB/s<br>\nBenchmark: m_100000_n_10_use_gpu_False \t wall_time: 0.000326 s \t Throughput: 24.6 GB/s<br>\nBenchmark: m_100000_n_100_use_gpu_False \t wall_time: 0.0016 s \t Throughput: 50 GB/s<br>\nBenchmark: m_100000_n_1000_use_gpu_False \t wall_time: 0.0166 s \t Throughput: 48.2 GB/s</p>", "body_text": "Op 'tf.where(c, t, e)' supports that 't' and 'e' are N-D tensors while 'c' is a 1D tensor, which would call BatchSelectFunctor to get the result. But its basic implementation broadcasts 'c' to the same dimension with 't' and 'e', which would get bad efficiency on CPU for large tensors.\nHere a loop-based implementation would be adopted to make this operation faster on CPU. There would be no broadcast during the calculation, thus would effectively improve the performance (seeing the benchmark test 'BatchSelect').\nBenchmark result below was done on a 2 x E5-2682v4 (16c @ 2.5GHz) platform:\ncompiling args: -mfma -mavx2 -mavx -msse4.2 -msse4.1 -O3\n(old version):\nBenchmark: m_1000_n_10_use_gpu_False \t wall_time: 0.000196 s \t Throughput: 0.409 GB/s\nBenchmark: m_1000_n_100_use_gpu_False \t wall_time: 0.000323 s \t Throughput: 2.48 GB/s\nBenchmark: m_1000_n_1000_use_gpu_False \t wall_time: 0.000623 s \t Throughput: 12.8 GB/s\nBenchmark: m_10000_n_10_use_gpu_False \t wall_time: 0.00031 s \t Throughput: 2.58 GB/s\nBenchmark: m_10000_n_100_use_gpu_False \t wall_time: 0.00061 s \t Throughput: 13.1 GB/s\nBenchmark: m_10000_n_1000_use_gpu_False \t wall_time: 0.00364 s \t Throughput: 22 GB/s\nBenchmark: m_100000_n_10_use_gpu_False \t wall_time: 0.000602 s \t Throughput: 13.3 GB/s\nBenchmark: m_100000_n_100_use_gpu_False \t wall_time: 0.00367 s \t Throughput: 21.8 GB/s\nBenchmark: m_100000_n_1000_use_gpu_False \t wall_time: 0.0309 s \t Throughput: 25.9 GB/s\n(new version):\nBenchmark: m_1000_n_10_use_gpu_False \t wall_time: 0.000153 s \t Throughput: 0.524 GB/s\nBenchmark: m_1000_n_100_use_gpu_False \t wall_time: 0.000178 s \t Throughput: 4.49 GB/s\nBenchmark: m_1000_n_1000_use_gpu_False \t wall_time: 0.000327 s \t Throughput: 24.5 GB/s\nBenchmark: m_10000_n_10_use_gpu_False \t wall_time: 0.000203 s \t Throughput: 3.94 GB/s\nBenchmark: m_10000_n_100_use_gpu_False \t wall_time: 0.000344 s \t Throughput: 23.3 GB/s\nBenchmark: m_10000_n_1000_use_gpu_False \t wall_time: 0.00123 s \t Throughput: 64.9 GB/s\nBenchmark: m_100000_n_10_use_gpu_False \t wall_time: 0.000326 s \t Throughput: 24.6 GB/s\nBenchmark: m_100000_n_100_use_gpu_False \t wall_time: 0.0016 s \t Throughput: 50 GB/s\nBenchmark: m_100000_n_1000_use_gpu_False \t wall_time: 0.0166 s \t Throughput: 48.2 GB/s", "body": "Op 'tf.where(c, t, e)' supports that 't' and 'e' are N-D tensors while 'c' is a 1D tensor, which would call BatchSelectFunctor to get the result. But its basic implementation broadcasts 'c' to the same dimension with 't' and 'e', which would get bad efficiency on CPU for large tensors.\r\n\r\nHere a loop-based implementation would be adopted to make this operation faster on CPU. There would be no broadcast during the calculation, thus would effectively improve the performance (seeing the benchmark test 'BatchSelect').\r\n\r\nBenchmark result below was done on a 2 x E5-2682v4 (16c @ 2.5GHz) platform:\r\ncompiling args: -mfma -mavx2 -mavx -msse4.2 -msse4.1 -O3\r\n(old version):\r\nBenchmark: m_1000_n_10_use_gpu_False \t wall_time: 0.000196 s \t Throughput: 0.409 GB/s\r\nBenchmark: m_1000_n_100_use_gpu_False \t wall_time: 0.000323 s \t Throughput: 2.48 GB/s\r\nBenchmark: m_1000_n_1000_use_gpu_False \t wall_time: 0.000623 s \t Throughput: 12.8 GB/s\r\nBenchmark: m_10000_n_10_use_gpu_False \t wall_time: 0.00031 s \t Throughput: 2.58 GB/s\r\nBenchmark: m_10000_n_100_use_gpu_False \t wall_time: 0.00061 s \t Throughput: 13.1 GB/s\r\nBenchmark: m_10000_n_1000_use_gpu_False \t wall_time: 0.00364 s \t Throughput: 22 GB/s\r\nBenchmark: m_100000_n_10_use_gpu_False \t wall_time: 0.000602 s \t Throughput: 13.3 GB/s\r\nBenchmark: m_100000_n_100_use_gpu_False \t wall_time: 0.00367 s \t Throughput: 21.8 GB/s\r\nBenchmark: m_100000_n_1000_use_gpu_False \t wall_time: 0.0309 s \t Throughput: 25.9 GB/s\r\n\r\n(new version):\r\nBenchmark: m_1000_n_10_use_gpu_False \t wall_time: 0.000153 s \t Throughput: 0.524 GB/s\r\nBenchmark: m_1000_n_100_use_gpu_False \t wall_time: 0.000178 s \t Throughput: 4.49 GB/s\r\nBenchmark: m_1000_n_1000_use_gpu_False \t wall_time: 0.000327 s \t Throughput: 24.5 GB/s\r\nBenchmark: m_10000_n_10_use_gpu_False \t wall_time: 0.000203 s \t Throughput: 3.94 GB/s\r\nBenchmark: m_10000_n_100_use_gpu_False \t wall_time: 0.000344 s \t Throughput: 23.3 GB/s\r\nBenchmark: m_10000_n_1000_use_gpu_False \t wall_time: 0.00123 s \t Throughput: 64.9 GB/s\r\nBenchmark: m_100000_n_10_use_gpu_False \t wall_time: 0.000326 s \t Throughput: 24.6 GB/s\r\nBenchmark: m_100000_n_100_use_gpu_False \t wall_time: 0.0016 s \t Throughput: 50 GB/s\r\nBenchmark: m_100000_n_1000_use_gpu_False \t wall_time: 0.0166 s \t Throughput: 48.2 GB/s"}