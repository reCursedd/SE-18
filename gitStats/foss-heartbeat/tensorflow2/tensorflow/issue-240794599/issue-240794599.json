{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11308", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11308/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11308/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11308/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11308", "id": 240794599, "node_id": "MDU6SXNzdWUyNDA3OTQ1OTk=", "number": 11308, "title": "Reading data from GCS, processing hangs indefinitely", "user": {"login": "dobbysock1002", "id": 29670478, "node_id": "MDQ6VXNlcjI5NjcwNDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/29670478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dobbysock1002", "html_url": "https://github.com/dobbysock1002", "followers_url": "https://api.github.com/users/dobbysock1002/followers", "following_url": "https://api.github.com/users/dobbysock1002/following{/other_user}", "gists_url": "https://api.github.com/users/dobbysock1002/gists{/gist_id}", "starred_url": "https://api.github.com/users/dobbysock1002/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dobbysock1002/subscriptions", "organizations_url": "https://api.github.com/users/dobbysock1002/orgs", "repos_url": "https://api.github.com/users/dobbysock1002/repos", "events_url": "https://api.github.com/users/dobbysock1002/events{/privacy}", "received_events_url": "https://api.github.com/users/dobbysock1002/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-07-05T22:31:33Z", "updated_at": "2017-12-01T07:08:02Z", "closed_at": "2017-12-01T07:03:57Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Using the ml engine example here: <a href=\"https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer\">https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer</a></li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac OSX 10.11.5</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1.0</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>This code runs fine if I point it to local data. It also runs fine if I point it to the public dataset gs://cloudml-public/census/data/adult.data.csv in the instructions. However, when I make a copy of that public dataset and store it in my own GCS bucket the code just hangs after listing these common warnings:</p>\n<pre><code>2017-07-05 16:14:50.392729: W tensorflow/core/platform/cpu_feature_guard.cc:45] The \nTensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your \nmachine and could speed up CPU computations.\n2017-07-05 16:14:50.392761: W tensorflow/core/platform/cpu_feature_guard.cc:45] The \nTensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-07-05 16:14:50.392767: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-07-05 16:14:50.392772: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-07-05 16:14:50.392776: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n</code></pre>\n<p>I am assuming the issue is permissions related or that the file can't be found but rather than failing or giving an error it just runs indefinitely. I'm not entirely sure if this is happening during the string_input_producer step, the read_up_to step or even as part of the shuffle_batch. It seems like a bug that nothing is causing the code to fail.</p>\n<pre><code>files = tf.concat([\n  tf.train.match_filenames_once(filename)\n  for filename in filenames\n], axis=0)\n\nfilename_queue = tf.train.string_input_producer(\n    files, num_epochs=num_epochs, shuffle=shuffle)\nreader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n\n_, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n# DNNLinearCombinedClassifier expects rank 2 tensors.\nrow_columns = tf.expand_dims(rows, -1)\ncolumns = tf.decode_csv(row_columns, record_defaults=CSV_COLUMN_DEFAULTS)\nfeatures = dict(zip(CSV_COLUMNS, columns))\n\n# Remove unused columns\nfor col in UNUSED_COLUMNS:\n  features.pop(col)\n\nif shuffle:\n  # This operation maintains a buffer of Tensors so that inputs are\n  # well shuffled even between batches.\n  features = tf.train.shuffle_batch(\n      features,\n      batch_size,\n      capacity=batch_size * 10,\n      min_after_dequeue=batch_size*2 + 1,\n      num_threads=multiprocessing.cpu_count(),\n      enqueue_many=True,\n      allow_smaller_final_batch=True\n  )\n</code></pre>", "body_text": "System information\n\nUsing the ml engine example here: https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.11.5\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.1.0\nPython version: 2.7.12\n\nDescribe the problem\nThis code runs fine if I point it to local data. It also runs fine if I point it to the public dataset gs://cloudml-public/census/data/adult.data.csv in the instructions. However, when I make a copy of that public dataset and store it in my own GCS bucket the code just hangs after listing these common warnings:\n2017-07-05 16:14:50.392729: W tensorflow/core/platform/cpu_feature_guard.cc:45] The \nTensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your \nmachine and could speed up CPU computations.\n2017-07-05 16:14:50.392761: W tensorflow/core/platform/cpu_feature_guard.cc:45] The \nTensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-07-05 16:14:50.392767: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-07-05 16:14:50.392772: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-07-05 16:14:50.392776: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n\nI am assuming the issue is permissions related or that the file can't be found but rather than failing or giving an error it just runs indefinitely. I'm not entirely sure if this is happening during the string_input_producer step, the read_up_to step or even as part of the shuffle_batch. It seems like a bug that nothing is causing the code to fail.\nfiles = tf.concat([\n  tf.train.match_filenames_once(filename)\n  for filename in filenames\n], axis=0)\n\nfilename_queue = tf.train.string_input_producer(\n    files, num_epochs=num_epochs, shuffle=shuffle)\nreader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n\n_, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n# DNNLinearCombinedClassifier expects rank 2 tensors.\nrow_columns = tf.expand_dims(rows, -1)\ncolumns = tf.decode_csv(row_columns, record_defaults=CSV_COLUMN_DEFAULTS)\nfeatures = dict(zip(CSV_COLUMNS, columns))\n\n# Remove unused columns\nfor col in UNUSED_COLUMNS:\n  features.pop(col)\n\nif shuffle:\n  # This operation maintains a buffer of Tensors so that inputs are\n  # well shuffled even between batches.\n  features = tf.train.shuffle_batch(\n      features,\n      batch_size,\n      capacity=batch_size * 10,\n      min_after_dequeue=batch_size*2 + 1,\n      num_threads=multiprocessing.cpu_count(),\n      enqueue_many=True,\n      allow_smaller_final_batch=True\n  )", "body": "\r\n### System information\r\n- Using the ml engine example here: https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.11.5\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Python version**: 2.7.12\r\n\r\n### Describe the problem\r\nThis code runs fine if I point it to local data. It also runs fine if I point it to the public dataset gs://cloudml-public/census/data/adult.data.csv in the instructions. However, when I make a copy of that public dataset and store it in my own GCS bucket the code just hangs after listing these common warnings:\r\n```\r\n2017-07-05 16:14:50.392729: W tensorflow/core/platform/cpu_feature_guard.cc:45] The \r\nTensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your \r\nmachine and could speed up CPU computations.\r\n2017-07-05 16:14:50.392761: W tensorflow/core/platform/cpu_feature_guard.cc:45] The \r\nTensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-05 16:14:50.392767: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-05 16:14:50.392772: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-05 16:14:50.392776: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\nI am assuming the issue is permissions related or that the file can't be found but rather than failing or giving an error it just runs indefinitely. I'm not entirely sure if this is happening during the string_input_producer step, the read_up_to step or even as part of the shuffle_batch. It seems like a bug that nothing is causing the code to fail.\r\n\r\n    files = tf.concat([\r\n      tf.train.match_filenames_once(filename)\r\n      for filename in filenames\r\n    ], axis=0)\r\n\r\n    filename_queue = tf.train.string_input_producer(\r\n        files, num_epochs=num_epochs, shuffle=shuffle)\r\n    reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\r\n\r\n    _, rows = reader.read_up_to(filename_queue, num_records=batch_size)\r\n    # DNNLinearCombinedClassifier expects rank 2 tensors.\r\n    row_columns = tf.expand_dims(rows, -1)\r\n    columns = tf.decode_csv(row_columns, record_defaults=CSV_COLUMN_DEFAULTS)\r\n    features = dict(zip(CSV_COLUMNS, columns))\r\n\r\n    # Remove unused columns\r\n    for col in UNUSED_COLUMNS:\r\n      features.pop(col)\r\n\r\n    if shuffle:\r\n      # This operation maintains a buffer of Tensors so that inputs are\r\n      # well shuffled even between batches.\r\n      features = tf.train.shuffle_batch(\r\n          features,\r\n          batch_size,\r\n          capacity=batch_size * 10,\r\n          min_after_dequeue=batch_size*2 + 1,\r\n          num_threads=multiprocessing.cpu_count(),\r\n          enqueue_many=True,\r\n          allow_smaller_final_batch=True\r\n      )\r\n\r\n"}