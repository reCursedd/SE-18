{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5523", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5523/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5523/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5523/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5523", "id": 188585806, "node_id": "MDU6SXNzdWUxODg1ODU4MDY=", "number": 5523, "title": "XLA's Dot should follow broadcast semantics from np.matmul, not np.dot", "user": {"login": "shoyer", "id": 1217238, "node_id": "MDQ6VXNlcjEyMTcyMzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1217238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shoyer", "html_url": "https://github.com/shoyer", "followers_url": "https://api.github.com/users/shoyer/followers", "following_url": "https://api.github.com/users/shoyer/following{/other_user}", "gists_url": "https://api.github.com/users/shoyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/shoyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shoyer/subscriptions", "organizations_url": "https://api.github.com/users/shoyer/orgs", "repos_url": "https://api.github.com/users/shoyer/repos", "events_url": "https://api.github.com/users/shoyer/events{/privacy}", "received_events_url": "https://api.github.com/users/shoyer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "eliben", "id": 1130906, "node_id": "MDQ6VXNlcjExMzA5MDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1130906?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliben", "html_url": "https://github.com/eliben", "followers_url": "https://api.github.com/users/eliben/followers", "following_url": "https://api.github.com/users/eliben/following{/other_user}", "gists_url": "https://api.github.com/users/eliben/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliben/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliben/subscriptions", "organizations_url": "https://api.github.com/users/eliben/orgs", "repos_url": "https://api.github.com/users/eliben/repos", "events_url": "https://api.github.com/users/eliben/events{/privacy}", "received_events_url": "https://api.github.com/users/eliben/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "eliben", "id": 1130906, "node_id": "MDQ6VXNlcjExMzA5MDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1130906?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliben", "html_url": "https://github.com/eliben", "followers_url": "https://api.github.com/users/eliben/followers", "following_url": "https://api.github.com/users/eliben/following{/other_user}", "gists_url": "https://api.github.com/users/eliben/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliben/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliben/subscriptions", "organizations_url": "https://api.github.com/users/eliben/orgs", "repos_url": "https://api.github.com/users/eliben/repos", "events_url": "https://api.github.com/users/eliben/events{/privacy}", "received_events_url": "https://api.github.com/users/eliben/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-11-10T18:40:49Z", "updated_at": "2017-02-09T22:36:15Z", "closed_at": "2016-11-21T18:01:06Z", "author_association": "MEMBER", "body_html": "<p>I notice that the <a href=\"https://www.tensorflow.org/versions/master/resources/xla_prerelease.html#dot\" rel=\"nofollow\">XLA Dot operation</a> copies \"outer-product style\" broadcast semantics from <code>numpy.dot</code>:</p>\n<table>\n<thead>\n<tr>\n<th>Input</th>\n<th>Output</th>\n<th>Semantics</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>array [p x q x r] <code>dot</code> array [s x r x t]</td>\n<td>array [p x q x s x t]</td>\n<td>array dot product (read below)</td>\n</tr>\n</tbody>\n</table>\n<p>In brief, I think this is a mistake. It would be better to follow the \"<a href=\"http://legacy.python.org/dev/peps/pep-0465/#semantics\" rel=\"nofollow\">matmul style</a>\" style broadcasting semantics of Python's <code>@</code> operation and NumPy's <code>matmul</code>.</p>\n<p>matmul's broadcasting is much more general, and in my opinion, also easier to understand. For example, it can do batch matrix-multiplication, but also can still do outer product style broadcasting if you insert dummy dimensions of length 1 (the axes do end up in a different order), e.g.,<br>\nbatch matmul: [p x q x r] <code>matmul</code> [p x r x t] -&gt; [p x q x t]<br>\nouter product matmul: [p x 1 x q x r] <code>matmul</code> [1 x s x r x t] -&gt; [p x s x q x t]</p>\n<p>If we could go back in time as NumPy developers, we assuredly would change <code>dot</code> to work this way (now we cannot, because of backwards compatibility concerns). So it would be nice to change this for XLA before we lock in this behavior.</p>", "body_text": "I notice that the XLA Dot operation copies \"outer-product style\" broadcast semantics from numpy.dot:\n\n\n\nInput\nOutput\nSemantics\n\n\n\n\narray [p x q x r] dot array [s x r x t]\narray [p x q x s x t]\narray dot product (read below)\n\n\n\nIn brief, I think this is a mistake. It would be better to follow the \"matmul style\" style broadcasting semantics of Python's @ operation and NumPy's matmul.\nmatmul's broadcasting is much more general, and in my opinion, also easier to understand. For example, it can do batch matrix-multiplication, but also can still do outer product style broadcasting if you insert dummy dimensions of length 1 (the axes do end up in a different order), e.g.,\nbatch matmul: [p x q x r] matmul [p x r x t] -> [p x q x t]\nouter product matmul: [p x 1 x q x r] matmul [1 x s x r x t] -> [p x s x q x t]\nIf we could go back in time as NumPy developers, we assuredly would change dot to work this way (now we cannot, because of backwards compatibility concerns). So it would be nice to change this for XLA before we lock in this behavior.", "body": "I notice that the [XLA Dot operation](https://www.tensorflow.org/versions/master/resources/xla_prerelease.html#dot) copies \"outer-product style\" broadcast semantics from `numpy.dot`:\r\n\r\n| Input                                     | Output                | Semantics                      |\r\n|-------------------------------------------|-----------------------|--------------------------------|\r\n| array [p x q x r] `dot` array [s x r x t] | array [p x q x s x t] | array dot product (read below) |\r\n \r\nIn brief, I think this is a mistake. It would be better to follow the \"[matmul style](http://legacy.python.org/dev/peps/pep-0465/#semantics)\" style broadcasting semantics of Python's `@` operation and NumPy's `matmul`.\r\n\r\nmatmul's broadcasting is much more general, and in my opinion, also easier to understand. For example, it can do batch matrix-multiplication, but also can still do outer product style broadcasting if you insert dummy dimensions of length 1 (the axes do end up in a different order), e.g.,\r\nbatch matmul: [p x q x r] `matmul` [p x r x t] -> [p x q x t]\r\nouter product matmul: [p x 1 x q x r] `matmul` [1 x s x r x t] -> [p x s x q x t]\r\n\r\nIf we could go back in time as NumPy developers, we assuredly would change `dot` to work this way (now we cannot, because of backwards compatibility concerns). So it would be nice to change this for XLA before we lock in this behavior."}