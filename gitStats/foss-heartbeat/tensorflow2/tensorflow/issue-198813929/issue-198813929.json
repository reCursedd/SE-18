{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6644", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6644/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6644/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6644/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6644", "id": 198813929, "node_id": "MDU6SXNzdWUxOTg4MTM5Mjk=", "number": 6644, "title": "Error: Data loss: file is too short to be an sstable", "user": {"login": "Lldenaurois", "id": 16069990, "node_id": "MDQ6VXNlcjE2MDY5OTkw", "avatar_url": "https://avatars2.githubusercontent.com/u/16069990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lldenaurois", "html_url": "https://github.com/Lldenaurois", "followers_url": "https://api.github.com/users/Lldenaurois/followers", "following_url": "https://api.github.com/users/Lldenaurois/following{/other_user}", "gists_url": "https://api.github.com/users/Lldenaurois/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lldenaurois/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lldenaurois/subscriptions", "organizations_url": "https://api.github.com/users/Lldenaurois/orgs", "repos_url": "https://api.github.com/users/Lldenaurois/repos", "events_url": "https://api.github.com/users/Lldenaurois/events{/privacy}", "received_events_url": "https://api.github.com/users/Lldenaurois/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-01-04T21:21:42Z", "updated_at": "2017-02-13T17:44:20Z", "closed_at": "2017-02-13T17:44:20Z", "author_association": "NONE", "body_html": "<p>Hi there,</p>\n<p>I'm training the Language Model code available here: <a href=\"https://github.com/rafaljozefowicz/lm\">https://github.com/rafaljozefowicz/lm</a></p>\n<p>I am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.</p>\n<p>Currently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.</p>\n<pre><code>W tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\nTraceback (most recent call last):\n  File \"single_lm_train.py\", line 38, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\n    while ckpt_loader.load_checkpoint():\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\n    if load_from_checkpoint(self.saver, self.logdir):\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\n\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\n    saver = tf.train.Saver(model.avg_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\npe_and_slices)]]\n\n</code></pre>", "body_text": "Hi there,\nI'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm\nI am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.\nCurrently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues.\nW tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\nTraceback (most recent call last):\n  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\n    while ckpt_loader.load_checkpoint():\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\n    if load_from_checkpoint(self.saver, self.logdir):\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\n\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"single_lm_train.py\", line 34, in main\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\n    saver = tf.train.Saver(model.avg_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): file is too short to be an sstable\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\npe_and_slices)]]", "body": "Hi there,\r\n\r\nI'm training the Language Model code available here: https://github.com/rafaljozefowicz/lm\r\n\r\nI am getting a \"Data loss\" error, see below for a traceback call. I was able to find the file where the error is raised, but I am having some trouble understanding exactly what is going wrong. It seems like this is related to a checkpoint file need to populate tensorboard, however I would greatly appreciate it if someone could point me in the right direction, so I may resolve this issue.\r\n\r\nCurrently, when I look at TensorBoard, I see a global_step of 0 for the entire training duration, and I'm worried that the model is not running. All of my GPUs are fully utilized, and I have no other issues. \r\n\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:975] Data loss: file is too short to be an sstable\r\nTraceback (most recent call last):\r\n  File \"single_lm_train.py\", line 38, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"single_lm_train.py\", line 34, in main\r\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 111, in run_eval\r\n    while ckpt_loader.load_checkpoint():\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 45, in load_checkpoint\r\n    if load_from_checkpoint(self.saver, self.logdir):\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/common.py\", line 27, in load_from_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1388, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\ntensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable\r\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/shape_and_slices)]]\r\nCaused by op u'save/RestoreV2_15', defined at:  File \"single_lm_train.py\", line 38, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"single_lm_train.py\", line 34, in main\r\n    run_eval(dataset, hps, FLAGS.logdir, FLAGS.mode, FLAGS.eval_steps)\r\n  File \"/shareddata/s5kbjt/NLM_RNN/lm/run_utils.py\", line 98, in run_eval\r\n    saver = tf.train.Saver(model.avg_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nDataLossError (see above for traceback): file is too short to be an sstable\r\n         [[Node: save/RestoreV2_15 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/repl\r\nica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_15/tensor_names, save/RestoreV2_15/sha\r\npe_and_slices)]]\r\n\r\n```"}