{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290062768", "html_url": "https://github.com/tensorflow/tensorflow/issues/2807#issuecomment-290062768", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2807", "id": 290062768, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDA2Mjc2OA==", "user": {"login": "sundw2014", "id": 12440834, "node_id": "MDQ6VXNlcjEyNDQwODM0", "avatar_url": "https://avatars1.githubusercontent.com/u/12440834?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sundw2014", "html_url": "https://github.com/sundw2014", "followers_url": "https://api.github.com/users/sundw2014/followers", "following_url": "https://api.github.com/users/sundw2014/following{/other_user}", "gists_url": "https://api.github.com/users/sundw2014/gists{/gist_id}", "starred_url": "https://api.github.com/users/sundw2014/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sundw2014/subscriptions", "organizations_url": "https://api.github.com/users/sundw2014/orgs", "repos_url": "https://api.github.com/users/sundw2014/repos", "events_url": "https://api.github.com/users/sundw2014/events{/privacy}", "received_events_url": "https://api.github.com/users/sundw2014/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-29T11:33:47Z", "updated_at": "2017-03-29T11:33:47Z", "author_association": "NONE", "body_html": "<p>I had the similar problem. Finally, I found that there are some comments in <code>tensorflow/tensorflow/core/kernels/quantized_conv_ops.cc</code></p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> This means that multiple ops can't be run simultaneously on different</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> threads, because we have a single shared resource. The platforms this is</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> aimed at have intra-op parallelism as their focus though, so it shouldn't</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> be an issue.</span>\n    mutex_lock <span class=\"pl-en\">lock_buffer</span>(im2col_buffer_resource-&gt;mu);\n    core::ScopedUnref <span class=\"pl-en\">unref_buffer</span>(im2col_buffer_resource);\n    T1* im2col_buffer = im2col_buffer_resource-&gt;data;</pre></div>\n<p>So, to get a good performance, you should set <code>inter_op_parallelism_threads=1</code> when you use <code>QuantizedConv2D</code> op.<br>\nBut, even you do that, you will find that although the conv ops get faster because of the use of <code>gemmlowp</code>, quantization will add some new ops like <code>RequantizationRange</code>, <code>Requantize</code> to your model, which are very time-consuming. So, the quantized model will still be slower than the original one.</p>", "body_text": "I had the similar problem. Finally, I found that there are some comments in tensorflow/tensorflow/core/kernels/quantized_conv_ops.cc\n// This means that multiple ops can't be run simultaneously on different\n    // threads, because we have a single shared resource. The platforms this is\n    // aimed at have intra-op parallelism as their focus though, so it shouldn't\n    // be an issue.\n    mutex_lock lock_buffer(im2col_buffer_resource->mu);\n    core::ScopedUnref unref_buffer(im2col_buffer_resource);\n    T1* im2col_buffer = im2col_buffer_resource->data;\nSo, to get a good performance, you should set inter_op_parallelism_threads=1 when you use QuantizedConv2D op.\nBut, even you do that, you will find that although the conv ops get faster because of the use of gemmlowp, quantization will add some new ops like RequantizationRange, Requantize to your model, which are very time-consuming. So, the quantized model will still be slower than the original one.", "body": "I had the similar problem. Finally, I found that there are some comments in ```tensorflow/tensorflow/core/kernels/quantized_conv_ops.cc```\r\n\r\n```C++\r\n// This means that multiple ops can't be run simultaneously on different\r\n    // threads, because we have a single shared resource. The platforms this is\r\n    // aimed at have intra-op parallelism as their focus though, so it shouldn't\r\n    // be an issue.\r\n    mutex_lock lock_buffer(im2col_buffer_resource->mu);\r\n    core::ScopedUnref unref_buffer(im2col_buffer_resource);\r\n    T1* im2col_buffer = im2col_buffer_resource->data;\r\n```\r\nSo, to get a good performance, you should set ```inter_op_parallelism_threads=1``` when you use ```QuantizedConv2D``` op.\r\nBut, even you do that, you will find that although the conv ops get faster because of the use of ```gemmlowp```, quantization will add some new ops like ```RequantizationRange```, ```Requantize``` to your model, which are very time-consuming. So, the quantized model will still be slower than the original one."}