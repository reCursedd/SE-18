{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361409032", "html_url": "https://github.com/tensorflow/tensorflow/issues/2807#issuecomment-361409032", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2807", "id": 361409032, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQwOTAzMg==", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-29T22:29:02Z", "updated_at": "2018-01-29T22:29:02Z", "author_association": "MEMBER", "body_html": "<p>We are focusing our eight-bit efforts on TF Lite (visible at tensorflow/contrib/lite), so we aren't expecting TensorFlow's quantized performance to improve in cases where it's not currently fast. These tend to be on x86 platforms (we're concentrating on ARM performance for mobile), and for models that use ops that we don't have quantized implementations for (which is most models outside a few vision-related ones we've optimized for).</p>\n<p>Since we're not likely to see changes in this area soon, I'm closing this as infeasible. Pull requests or other help in this area would be very welcome of course!</p>", "body_text": "We are focusing our eight-bit efforts on TF Lite (visible at tensorflow/contrib/lite), so we aren't expecting TensorFlow's quantized performance to improve in cases where it's not currently fast. These tend to be on x86 platforms (we're concentrating on ARM performance for mobile), and for models that use ops that we don't have quantized implementations for (which is most models outside a few vision-related ones we've optimized for).\nSince we're not likely to see changes in this area soon, I'm closing this as infeasible. Pull requests or other help in this area would be very welcome of course!", "body": "We are focusing our eight-bit efforts on TF Lite (visible at tensorflow/contrib/lite), so we aren't expecting TensorFlow's quantized performance to improve in cases where it's not currently fast. These tend to be on x86 platforms (we're concentrating on ARM performance for mobile), and for models that use ops that we don't have quantized implementations for (which is most models outside a few vision-related ones we've optimized for).\r\n\r\nSince we're not likely to see changes in this area soon, I'm closing this as infeasible. Pull requests or other help in this area would be very welcome of course!"}