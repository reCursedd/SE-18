{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/424812731", "html_url": "https://github.com/tensorflow/tensorflow/issues/22497#issuecomment-424812731", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22497", "id": 424812731, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDgxMjczMQ==", "user": {"login": "eliorc", "id": 17727283, "node_id": "MDQ6VXNlcjE3NzI3Mjgz", "avatar_url": "https://avatars2.githubusercontent.com/u/17727283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliorc", "html_url": "https://github.com/eliorc", "followers_url": "https://api.github.com/users/eliorc/followers", "following_url": "https://api.github.com/users/eliorc/following{/other_user}", "gists_url": "https://api.github.com/users/eliorc/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliorc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliorc/subscriptions", "organizations_url": "https://api.github.com/users/eliorc/orgs", "repos_url": "https://api.github.com/users/eliorc/repos", "events_url": "https://api.github.com/users/eliorc/events{/privacy}", "received_events_url": "https://api.github.com/users/eliorc/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T18:00:16Z", "updated_at": "2018-09-26T18:00:16Z", "author_association": "NONE", "body_html": "<p>Yep, due to sensitivity I can't post the entire code but this is censored version</p>\n<pre><code>dataset = tf.data.TFRecordDataset(tfrecord_path)\ndataset = dataset.map(...., num_parallel_calls=n_workers)\ndataset = dataset.map(....,  num_parallel_calls=n_workers)\ndataset = dataset.map(...., num_parallel_calls=n_workers)\n\ndataset = dataset.map(...,  num_parallel_calls=n_workers)\n\ndataset = dataset.shuffle(buffer_size=10000)\n\ndataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=general.element_length_func,\n                                                                  bucket_batch_sizes=[batch_size] * (\n                                                                          len(bucket_boundaries) + 1),\n                                                                  # +2 for the below [0] and above [-1]\n                                                                  bucket_boundaries=bucket_boundaries,\n                                                                  padded_shapes={.....}))\n\ndataset = dataset.prefetch(buffer_size=prefetch)\n\nhandle = tf.placeholder(tf.string, shape=[], name='handle')\niterator = tf.data.Iterator.from_string_handle(handle,\n                                                   dataset.output_types,\n                                                   dataset.output_shapes)\n\ntrain_iterator = dataset.make_initializable_iterator()\n\niterator.get_next()\n....\n</code></pre>\n<p>The code runs on a 16 core machine</p>", "body_text": "Yep, due to sensitivity I can't post the entire code but this is censored version\ndataset = tf.data.TFRecordDataset(tfrecord_path)\ndataset = dataset.map(...., num_parallel_calls=n_workers)\ndataset = dataset.map(....,  num_parallel_calls=n_workers)\ndataset = dataset.map(...., num_parallel_calls=n_workers)\n\ndataset = dataset.map(...,  num_parallel_calls=n_workers)\n\ndataset = dataset.shuffle(buffer_size=10000)\n\ndataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=general.element_length_func,\n                                                                  bucket_batch_sizes=[batch_size] * (\n                                                                          len(bucket_boundaries) + 1),\n                                                                  # +2 for the below [0] and above [-1]\n                                                                  bucket_boundaries=bucket_boundaries,\n                                                                  padded_shapes={.....}))\n\ndataset = dataset.prefetch(buffer_size=prefetch)\n\nhandle = tf.placeholder(tf.string, shape=[], name='handle')\niterator = tf.data.Iterator.from_string_handle(handle,\n                                                   dataset.output_types,\n                                                   dataset.output_shapes)\n\ntrain_iterator = dataset.make_initializable_iterator()\n\niterator.get_next()\n....\n\nThe code runs on a 16 core machine", "body": "Yep, due to sensitivity I can't post the entire code but this is censored version\r\n\r\n```\r\ndataset = tf.data.TFRecordDataset(tfrecord_path)\r\ndataset = dataset.map(...., num_parallel_calls=n_workers)\r\ndataset = dataset.map(....,  num_parallel_calls=n_workers)\r\ndataset = dataset.map(...., num_parallel_calls=n_workers)\r\n\r\ndataset = dataset.map(...,  num_parallel_calls=n_workers)\r\n\r\ndataset = dataset.shuffle(buffer_size=10000)\r\n\r\ndataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=general.element_length_func,\r\n                                                                  bucket_batch_sizes=[batch_size] * (\r\n                                                                          len(bucket_boundaries) + 1),\r\n                                                                  # +2 for the below [0] and above [-1]\r\n                                                                  bucket_boundaries=bucket_boundaries,\r\n                                                                  padded_shapes={.....}))\r\n\r\ndataset = dataset.prefetch(buffer_size=prefetch)\r\n\r\nhandle = tf.placeholder(tf.string, shape=[], name='handle')\r\niterator = tf.data.Iterator.from_string_handle(handle,\r\n                                                   dataset.output_types,\r\n                                                   dataset.output_shapes)\r\n\r\ntrain_iterator = dataset.make_initializable_iterator()\r\n\r\niterator.get_next()\r\n....\r\n```\r\n\r\nThe code runs on a 16 core machine"}