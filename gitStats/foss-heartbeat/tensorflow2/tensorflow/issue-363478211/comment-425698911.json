{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/425698911", "html_url": "https://github.com/tensorflow/tensorflow/issues/22497#issuecomment-425698911", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22497", "id": 425698911, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTY5ODkxMQ==", "user": {"login": "eliorc", "id": 17727283, "node_id": "MDQ6VXNlcjE3NzI3Mjgz", "avatar_url": "https://avatars2.githubusercontent.com/u/17727283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliorc", "html_url": "https://github.com/eliorc", "followers_url": "https://api.github.com/users/eliorc/followers", "following_url": "https://api.github.com/users/eliorc/following{/other_user}", "gists_url": "https://api.github.com/users/eliorc/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliorc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliorc/subscriptions", "organizations_url": "https://api.github.com/users/eliorc/orgs", "repos_url": "https://api.github.com/users/eliorc/repos", "events_url": "https://api.github.com/users/eliorc/events{/privacy}", "received_events_url": "https://api.github.com/users/eliorc/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-30T06:43:55Z", "updated_at": "2018-09-30T06:43:55Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a></p>\n<ul>\n<li><em>Is the corrupted record always at the same offset, or does it vary?</em> - I'm using the <code>.shuffle</code> call, so I believe even though the number of the record is different each time I restart the machine, it might be the same one.</li>\n<li><em>Does the error reproduce every time you run on worker 2, or only sometimes?</em> - The error always reproduces on worker 2, if I restart the host machine it will go through the counting process and somewhere during the training it will fail, and then it will always fail upon trying to count</li>\n<li><em>How was the file generated?</em> - Pretty standard code, these are <code>tf.SequenceExample</code>, pretty similar in technique but more complex than code that can be found <a href=\"http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\" rel=\"nofollow\">here</a></li>\n<li><em>Are you using any compression options?</em> - Not that I know of, unless there is some kind of default compression. What I did do is compress the file before transferring it between systems using zip.</li>\n<li><em>What file system are you using?</em> (e.g. Is it a local file, or in GCS, S3, etc.?) - Local filesystems</li>\n</ul>\n<p>P.S. thanks for your great support on the SO platform, it helps a lot</p>", "body_text": "Hi @mrry\n\nIs the corrupted record always at the same offset, or does it vary? - I'm using the .shuffle call, so I believe even though the number of the record is different each time I restart the machine, it might be the same one.\nDoes the error reproduce every time you run on worker 2, or only sometimes? - The error always reproduces on worker 2, if I restart the host machine it will go through the counting process and somewhere during the training it will fail, and then it will always fail upon trying to count\nHow was the file generated? - Pretty standard code, these are tf.SequenceExample, pretty similar in technique but more complex than code that can be found here\nAre you using any compression options? - Not that I know of, unless there is some kind of default compression. What I did do is compress the file before transferring it between systems using zip.\nWhat file system are you using? (e.g. Is it a local file, or in GCS, S3, etc.?) - Local filesystems\n\nP.S. thanks for your great support on the SO platform, it helps a lot", "body": "Hi @mrry \r\n\r\n- _Is the corrupted record always at the same offset, or does it vary?_ - I'm using the `.shuffle` call, so I believe even though the number of the record is different each time I restart the machine, it might be the same one.\r\n- _Does the error reproduce every time you run on worker 2, or only sometimes?_ - The error always reproduces on worker 2, if I restart the host machine it will go through the counting process and somewhere during the training it will fail, and then it will always fail upon trying to count\r\n- _How was the file generated?_ - Pretty standard code, these are `tf.SequenceExample`, pretty similar in technique but more complex than code that can be found [here](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)\r\n- _Are you using any compression options?_ - Not that I know of, unless there is some kind of default compression. What I did do is compress the file before transferring it between systems using zip.\r\n- _What file system are you using?_ (e.g. Is it a local file, or in GCS, S3, etc.?) - Local filesystems\r\n\r\nP.S. thanks for your great support on the SO platform, it helps a lot"}