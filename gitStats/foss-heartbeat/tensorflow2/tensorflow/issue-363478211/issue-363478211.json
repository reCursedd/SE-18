{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22497", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22497/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22497/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22497/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22497", "id": 363478211, "node_id": "MDU6SXNzdWUzNjM0NzgyMTE=", "number": 22497, "title": "DataLoss error on TFRecords - happens on one machine doesn't on other", "user": {"login": "eliorc", "id": 17727283, "node_id": "MDQ6VXNlcjE3NzI3Mjgz", "avatar_url": "https://avatars2.githubusercontent.com/u/17727283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliorc", "html_url": "https://github.com/eliorc", "followers_url": "https://api.github.com/users/eliorc/followers", "following_url": "https://api.github.com/users/eliorc/following{/other_user}", "gists_url": "https://api.github.com/users/eliorc/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliorc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliorc/subscriptions", "organizations_url": "https://api.github.com/users/eliorc/orgs", "repos_url": "https://api.github.com/users/eliorc/repos", "events_url": "https://api.github.com/users/eliorc/events{/privacy}", "received_events_url": "https://api.github.com/users/eliorc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547147, "node_id": "MDU6TGFiZWwxMDk3NTQ3MTQ3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:ops", "name": "comp:ops", "color": "0052cc", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-09-25T09:06:29Z", "updated_at": "2018-10-23T10:29:01Z", "closed_at": "2018-10-23T10:29:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<h4>System 1 (Bug DOESN'T occur)</h4>\n<p>All details are from inside the docker which the codes run in</p>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: True</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04.1, 4.15.0-29-generic</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>: pip install tensorflow_gpu</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>: v1.10.0-0-g656e7a2b34 1.10.0</p>\n</li>\n<li>\n<p><strong>Mobile device</strong>: N/A</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>: N/A</p>\n</li>\n<li>\n<p><strong>Python version</strong>: 3.6.6</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>: CUDA: V9.0.176, cuDNN 7.1.4</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>: GTX 1080 8GB + GTX 980 Ti 6GB</p>\n</li>\n<li>\n<p><strong>Docker details</strong>:<br>\nNVIDIA Docker: 2.0.2<br>\nClient:<br>\nVersion:           17.12.0-ce<br>\nAPI version:       1.35<br>\nGo version:        go1.9.2<br>\nGit commit:        c97c6d6<br>\nBuilt:             Wed Dec 27 20:11:19 2017<br>\nOS/Arch:          linux/amd64<br>\nExperimental:      false<br>\nServer:<br>\nEngine:<br>\nVersion:          17.12.0-ce<br>\nAPI version:      1.35 (minimum version 1.12)<br>\nGo version:       go1.9.2<br>\nGit commit:       c97c6d6<br>\nBuilt:            Wed Dec 27 20:09:53 2017<br>\nOS/Arch:          linux/amd64<br>\nExperimental:     false</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>: See below</p>\n</li>\n</ul>\n<h4>System 2 (Bug DOES occur)</h4>\n<p>All details are from inside the docker which the codes run in</p>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: True</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04.1, 4.15.0-34-generic</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install tensorflow_gpu</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.10.1-0-g4dcfddc5d1 1.10.1</li>\n<li><strong>Mobile device</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>Python version</strong>: 3.6.6</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA: V9.0.176, cuDNN 7.2.1</li>\n<li><strong>GPU model and memory</strong>: GTX 1080 Ti 12GB x 2</li>\n<li><strong>Docker details</strong>:<br>\nNVIDIA Docker: 2.0.3<br>\nClient:<br>\nVersion:           18.06.0-ce<br>\nAPI version:       1.38<br>\nGo version:        go1.10.3<br>\nGit commit:        0ffa825<br>\nBuilt:             Wed Jul 18 19:11:02 2018<br>\nOS/Arch:           linux/amd64<br>\nExperimental:      false<br>\nServer:<br>\nEngine:<br>\nVersion:          18.06.0-ce<br>\nAPI version:      1.38 (minimum version 1.12)<br>\nGo version:       go1.10.3<br>\nGit commit:       0ffa825<br>\nBuilt:            Wed Jul 18 19:09:05 2018<br>\nOS/Arch:          linux/amd64<br>\nExperimental:     false</li>\n<li><strong>Exact command to reproduce</strong>: See below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using the same docker image, which is an altered version of <a href=\"https://github.com/ufoym/deepo/blob/master/docker/Dockerfile.tensorflow-py36-cu90\">this image</a> running the same code over the same TFRecords on one machine (System 2) I get a DataLoss exception (corrupted record) while on the other system (System 1) I don't.<br>\nI have verified using checksum that the files were transferred safely. I have also tried to retransfer the files at least two times. I have also have tried using other sets of TFRecords I created. The problem is the same in all cases, it happens on System 2, but does not happen on System 1.</p>\n<p>The TFRecords were written on System 1 and are over 150GB in size.<br>\nI have also tried rebuilding the image and restarting containers.<br>\nI have also tried restarting the host machine.<br>\nThe code is running on one GPU, I tried using each, all fail.</p>\n<h3>Source code / logs</h3>\n<p>Due to sensitivity of the code I can't share all of it, but I'll paste the relevant lines.</p>\n<p>In the training code I have 3 places where I iterate over the dataset, once when I count the number of records<br>\n<code>train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))</code>.<br>\nSecond time when I feed the training loop</p>\n<pre><code>while True:\n    try:\n        ...\n        _, batch_summary = sess.run([opt, merged], feed_dict={handle: train_handle})\n        ...\n    except tf.errors.OutOfRangeError:\n        ...\n</code></pre>\n<p>And third time very similar code for the validation set.</p>\n<p>After I recreate image + containers and restarting the machine, I will successfully finish the calculation of <code>train_size</code> (full loop over TFRecords), then I would start training, and somewhere in the training loop I would fail with the traceback attached below. If then I run the code again, I will fail on the first call to the TFRecords, with the same traceback - meaning this time it would happen on the <code>train_size</code> calculation. It will now forever fail on this call, until I restart the machine and the scenario repeats.</p>\n<p>As I said, same code on different machine (System 1) never fails.</p>\n<p>Traceback:</p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"train.py\", line 884, in <br>\nmain()<br>\nFile \"train.py\", line 878, in main<br>\ntrain(graph, model_dir, tensorboard_dir, meta, is_recovering=recovering)<br>\nFile \"train.py\", line 846, in train<br>\n'sequential']})<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 877, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run<br>\nrun_metadata)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 6720637495<br>\n[[Node: data/IteratorGetNext = IteratorGetNext<a href=\"data/IteratorFromStringHandle\">output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]</p>\n<p>Caused by op 'data/IteratorGetNext', defined at:<br>\nFile \"train.py\", line 884, in <br>\nmain()<br>\nFile \"train.py\", line 874, in main<br>\ngraph, meta = build_model(available_gpus)<br>\nFile \"train.py\", line 502, in build_model<br>\nreturn build_classifier(available_gpus)<br>\nFile \"train.py\", line 572, in build_classifier<br>\nn_gpus=FLAGS.n_gpus)<br>\nFile \"/opt/code/utils/architecture/data/v4.py\", line 309, in data_prep<br>\n<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 410, in get_next<br>\nname=name)), self._output_types,<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next<br>\noutput_shapes=output_shapes, name=name)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper<br>\nop_def=op_def)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func<br>\nreturn func(*args, **kwargs)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op<br>\nop_def=op_def)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in <strong>init</strong><br>\nself._traceback = tf_stack.extract_stack()</p>\n<p>DataLossError (see above for traceback): corrupted record at 6720637495<br>\n[[Node: data/IteratorGetNext = IteratorGetNext<a href=\"data/IteratorFromStringHandle\">output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]</p>\n</blockquote>", "body_text": "System information\nSystem 1 (Bug DOESN'T occur)\nAll details are from inside the docker which the codes run in\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): True\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1, 4.15.0-29-generic\n\n\nTensorFlow installed from (source or binary): pip install tensorflow_gpu\n\n\nTensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0\n\n\nMobile device: N/A\n\n\nBazel version (if compiling from source): N/A\n\n\nPython version: 3.6.6\n\n\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\n\n\nCUDA/cuDNN version: CUDA: V9.0.176, cuDNN 7.1.4\n\n\nGPU model and memory: GTX 1080 8GB + GTX 980 Ti 6GB\n\n\nDocker details:\nNVIDIA Docker: 2.0.2\nClient:\nVersion:           17.12.0-ce\nAPI version:       1.35\nGo version:        go1.9.2\nGit commit:        c97c6d6\nBuilt:             Wed Dec 27 20:11:19 2017\nOS/Arch:          linux/amd64\nExperimental:      false\nServer:\nEngine:\nVersion:          17.12.0-ce\nAPI version:      1.35 (minimum version 1.12)\nGo version:       go1.9.2\nGit commit:       c97c6d6\nBuilt:            Wed Dec 27 20:09:53 2017\nOS/Arch:          linux/amd64\nExperimental:     false\n\n\nExact command to reproduce: See below\n\n\nSystem 2 (Bug DOES occur)\nAll details are from inside the docker which the codes run in\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): True\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1, 4.15.0-34-generic\nTensorFlow installed from (source or binary): pip install tensorflow_gpu\nTensorFlow version (use command below): v1.10.1-0-g4dcfddc5d1 1.10.1\nMobile device: N/A\nBazel version (if compiling from source): N/A\nPython version: 3.6.6\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCUDA/cuDNN version: CUDA: V9.0.176, cuDNN 7.2.1\nGPU model and memory: GTX 1080 Ti 12GB x 2\nDocker details:\nNVIDIA Docker: 2.0.3\nClient:\nVersion:           18.06.0-ce\nAPI version:       1.38\nGo version:        go1.10.3\nGit commit:        0ffa825\nBuilt:             Wed Jul 18 19:11:02 2018\nOS/Arch:           linux/amd64\nExperimental:      false\nServer:\nEngine:\nVersion:          18.06.0-ce\nAPI version:      1.38 (minimum version 1.12)\nGo version:       go1.10.3\nGit commit:       0ffa825\nBuilt:            Wed Jul 18 19:09:05 2018\nOS/Arch:          linux/amd64\nExperimental:     false\nExact command to reproduce: See below\n\nDescribe the problem\nUsing the same docker image, which is an altered version of this image running the same code over the same TFRecords on one machine (System 2) I get a DataLoss exception (corrupted record) while on the other system (System 1) I don't.\nI have verified using checksum that the files were transferred safely. I have also tried to retransfer the files at least two times. I have also have tried using other sets of TFRecords I created. The problem is the same in all cases, it happens on System 2, but does not happen on System 1.\nThe TFRecords were written on System 1 and are over 150GB in size.\nI have also tried rebuilding the image and restarting containers.\nI have also tried restarting the host machine.\nThe code is running on one GPU, I tried using each, all fail.\nSource code / logs\nDue to sensitivity of the code I can't share all of it, but I'll paste the relevant lines.\nIn the training code I have 3 places where I iterate over the dataset, once when I count the number of records\ntrain_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path'])).\nSecond time when I feed the training loop\nwhile True:\n    try:\n        ...\n        _, batch_summary = sess.run([opt, merged], feed_dict={handle: train_handle})\n        ...\n    except tf.errors.OutOfRangeError:\n        ...\n\nAnd third time very similar code for the validation set.\nAfter I recreate image + containers and restarting the machine, I will successfully finish the calculation of train_size (full loop over TFRecords), then I would start training, and somewhere in the training loop I would fail with the traceback attached below. If then I run the code again, I will fail on the first call to the TFRecords, with the same traceback - meaning this time it would happen on the train_size calculation. It will now forever fail on this call, until I restart the machine and the scenario repeats.\nAs I said, same code on different machine (System 1) never fails.\nTraceback:\n\nTraceback (most recent call last):\nFile \"train.py\", line 884, in \nmain()\nFile \"train.py\", line 878, in main\ntrain(graph, model_dir, tensorboard_dir, meta, is_recovering=recovering)\nFile \"train.py\", line 846, in train\n'sequential']})\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 877, in run\nrun_metadata_ptr)\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\nrun_metadata)\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 6720637495\n[[Node: data/IteratorGetNext = IteratorGetNextoutput_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\nCaused by op 'data/IteratorGetNext', defined at:\nFile \"train.py\", line 884, in \nmain()\nFile \"train.py\", line 874, in main\ngraph, meta = build_model(available_gpus)\nFile \"train.py\", line 502, in build_model\nreturn build_classifier(available_gpus)\nFile \"train.py\", line 572, in build_classifier\nn_gpus=FLAGS.n_gpus)\nFile \"/opt/code/utils/architecture/data/v4.py\", line 309, in data_prep\n\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 410, in get_next\nname=name)), self._output_types,\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\noutput_shapes=output_shapes, name=name)\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\nop_def=op_def)\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\nreturn func(*args, **kwargs)\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\nop_def=op_def)\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in init\nself._traceback = tf_stack.extract_stack()\nDataLossError (see above for traceback): corrupted record at 6720637495\n[[Node: data/IteratorGetNext = IteratorGetNextoutput_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]", "body": "### System information\r\n#### System 1 (Bug DOESN'T occur)\r\nAll details are from inside the docker which the codes run in\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: True\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1, 4.15.0-29-generic\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow_gpu\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Mobile device**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Python version**: 3.6.6\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA: V9.0.176, cuDNN 7.1.4  \r\n- **GPU model and memory**: GTX 1080 8GB + GTX 980 Ti 6GB\r\n- **Docker details**: \r\n  NVIDIA Docker: 2.0.2\r\n    Client:\r\n    Version:           17.12.0-ce\r\n    API version:       1.35\r\n    Go version:        go1.9.2\r\n    Git commit:        c97c6d6\r\n    Built:             Wed Dec 27 20:11:19 2017\r\n    OS/Arch:          linux/amd64\r\n    Experimental:      false\r\n  Server:\r\n    Engine:\r\n    Version:          17.12.0-ce\r\n    API version:      1.35 (minimum version 1.12)\r\n    Go version:       go1.9.2\r\n    Git commit:       c97c6d6\r\n    Built:            Wed Dec 27 20:09:53 2017\r\n    OS/Arch:          linux/amd64\r\n    Experimental:     false\r\n\r\n- **Exact command to reproduce**: See below\r\n\r\n#### System 2 (Bug DOES occur)\r\nAll details are from inside the docker which the codes run in\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: True\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1, 4.15.0-34-generic\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow_gpu\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n- **Mobile device**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Python version**: 3.6.6\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA: V9.0.176, cuDNN 7.2.1  \r\n- **GPU model and memory**: GTX 1080 Ti 12GB x 2\r\n- **Docker details**:\r\n  NVIDIA Docker: 2.0.3\r\n    Client:\r\n    Version:           18.06.0-ce\r\n    API version:       1.38\r\n    Go version:        go1.10.3\r\n    Git commit:        0ffa825\r\n    Built:             Wed Jul 18 19:11:02 2018\r\n    OS/Arch:           linux/amd64\r\n    Experimental:      false\r\n  Server:\r\n    Engine:\r\n    Version:          18.06.0-ce\r\n    API version:      1.38 (minimum version 1.12)\r\n    Go version:       go1.10.3\r\n    Git commit:       0ffa825\r\n    Built:            Wed Jul 18 19:09:05 2018\r\n    OS/Arch:          linux/amd64\r\n    Experimental:     false\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nUsing the same docker image, which is an altered version of [this image](https://github.com/ufoym/deepo/blob/master/docker/Dockerfile.tensorflow-py36-cu90) running the same code over the same TFRecords on one machine (System 2) I get a DataLoss exception (corrupted record) while on the other system (System 1) I don't.\r\nI have verified using checksum that the files were transferred safely. I have also tried to retransfer the files at least two times. I have also have tried using other sets of TFRecords I created. The problem is the same in all cases, it happens on System 2, but does not happen on System 1.\r\n\r\nThe TFRecords were written on System 1 and are over 150GB in size.\r\nI have also tried rebuilding the image and restarting containers.\r\nI have also tried restarting the host machine.\r\nThe code is running on one GPU, I tried using each, all fail.\r\n\r\n### Source code / logs\r\nDue to sensitivity of the code I can't share all of it, but I'll paste the relevant lines.\r\n\r\nIn the training code I have 3 places where I iterate over the dataset, once when I count the number of records\r\n`train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))`. \r\nSecond time when I feed the training loop\r\n\r\n```\r\nwhile True:\r\n    try:\r\n        ...\r\n        _, batch_summary = sess.run([opt, merged], feed_dict={handle: train_handle})\r\n        ...\r\n    except tf.errors.OutOfRangeError:\r\n        ...\r\n```\r\nAnd third time very similar code for the validation set.\r\n\r\nAfter I recreate image + containers and restarting the machine, I will successfully finish the calculation of `train_size` (full loop over TFRecords), then I would start training, and somewhere in the training loop I would fail with the traceback attached below. If then I run the code again, I will fail on the first call to the TFRecords, with the same traceback - meaning this time it would happen on the `train_size` calculation. It will now forever fail on this call, until I restart the machine and the scenario repeats.\r\n\r\nAs I said, same code on different machine (System 1) never fails.\r\n\r\n\r\nTraceback:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"train.py\", line 884, in <module>\r\n>     main()\r\n>   File \"train.py\", line 878, in main\r\n>     train(graph, model_dir, tensorboard_dir, meta, is_recovering=recovering)\r\n>   File \"train.py\", line 846, in train\r\n>     'sequential']})\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 877, in run\r\n>     run_metadata_ptr)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n>     feed_dict_tensor, options, run_metadata)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n>     run_metadata)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 6720637495\r\n> \t [[Node: data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](data/IteratorFromStringHandle)]]\r\n> \r\n> Caused by op 'data/IteratorGetNext', defined at:\r\n>   File \"train.py\", line 884, in <module>\r\n>     main()\r\n>   File \"train.py\", line 874, in main\r\n>     graph, meta = build_model(available_gpus)\r\n>   File \"train.py\", line 502, in build_model\r\n>     return build_classifier(available_gpus)\r\n>   File \"train.py\", line 572, in build_classifier\r\n>     n_gpus=FLAGS.n_gpus)\r\n>   File \"/opt/code/utils/architecture/data/v4.py\", line 309, in data_prep\r\n>     <deleted line for sensitivity issues>\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 410, in get_next\r\n>     name=name)), self._output_types,\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\r\n>     output_shapes=output_shapes, name=name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n>     op_def=op_def)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n>     op_def=op_def)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n>     self._traceback = tf_stack.extract_stack()\r\n> \r\n> DataLossError (see above for traceback): corrupted record at 6720637495\r\n> \t [[Node: data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](data/IteratorFromStringHandle)]]\r\n"}