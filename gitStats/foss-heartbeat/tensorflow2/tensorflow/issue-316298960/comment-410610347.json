{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/410610347", "html_url": "https://github.com/tensorflow/tensorflow/issues/18736#issuecomment-410610347", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18736", "id": 410610347, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDYxMDM0Nw==", "user": {"login": "IvanZhangDoIt", "id": 25627631, "node_id": "MDQ6VXNlcjI1NjI3NjMx", "avatar_url": "https://avatars1.githubusercontent.com/u/25627631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IvanZhangDoIt", "html_url": "https://github.com/IvanZhangDoIt", "followers_url": "https://api.github.com/users/IvanZhangDoIt/followers", "following_url": "https://api.github.com/users/IvanZhangDoIt/following{/other_user}", "gists_url": "https://api.github.com/users/IvanZhangDoIt/gists{/gist_id}", "starred_url": "https://api.github.com/users/IvanZhangDoIt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IvanZhangDoIt/subscriptions", "organizations_url": "https://api.github.com/users/IvanZhangDoIt/orgs", "repos_url": "https://api.github.com/users/IvanZhangDoIt/repos", "events_url": "https://api.github.com/users/IvanZhangDoIt/events{/privacy}", "received_events_url": "https://api.github.com/users/IvanZhangDoIt/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-06T07:11:34Z", "updated_at": "2018-08-06T07:11:34Z", "author_association": "NONE", "body_html": "<p>Tf:1.9 , I have the same issue.    Total: prams: 367.352.  256G mem.  1) When I use tf.keras.layers, it's OK, but I changed to tf.layers to address  BN layer accuracy issue.  this issue pop up 2) Batch size 128 work when using tf.keras layer, still exist when batch size is set to 8    3) from the detail, the clue is on Adam initilization, but it shoud not.</p>\n<p>2018-08-06 06:54:03.420400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 61.04GiB.  Current allocation summary follows.</p>\n<p>2018-08-06 06:54:03.422814: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________<br>\n2018-08-06 06:54:03.423826: W tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at constant_op.cc:75 : Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float<br>\n2018-08-06 06:54:45.899764: E tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float<br>\n[[Node: block_output_fc1/kernel/Adam/Initializer/zeros = Const<a href=\"\">dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [32000000,512] values: [0 0 0]...&gt;, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"</a>]]</p>", "body_text": "Tf:1.9 , I have the same issue.    Total: prams: 367.352.  256G mem.  1) When I use tf.keras.layers, it's OK, but I changed to tf.layers to address  BN layer accuracy issue.  this issue pop up 2) Batch size 128 work when using tf.keras layer, still exist when batch size is set to 8    3) from the detail, the clue is on Adam initilization, but it shoud not.\n2018-08-06 06:54:03.420400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 61.04GiB.  Current allocation summary follows.\n2018-08-06 06:54:03.422814: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________\n2018-08-06 06:54:03.423826: W tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at constant_op.cc:75 : Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float\n2018-08-06 06:54:45.899764: E tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float\n[[Node: block_output_fc1/kernel/Adam/Initializer/zeros = Constdtype=DT_FLOAT, value=Tensor<type: float shape: [32000000,512] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]", "body": "Tf:1.9 , I have the same issue.    Total: prams: 367.352.  256G mem.  1) When I use tf.keras.layers, it's OK, but I changed to tf.layers to address  BN layer accuracy issue.  this issue pop up 2) Batch size 128 work when using tf.keras layer, still exist when batch size is set to 8    3) from the detail, the clue is on Adam initilization, but it shoud not. \r\n\r\n2018-08-06 06:54:03.420400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 61.04GiB.  Current allocation summary follows.\r\n\r\n\r\n2018-08-06 06:54:03.422814: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________\r\n2018-08-06 06:54:03.423826: W tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at constant_op.cc:75 : Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float\r\n2018-08-06 06:54:45.899764: E tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float\r\n [[Node: block_output_fc1/kernel/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [32000000,512] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]"}