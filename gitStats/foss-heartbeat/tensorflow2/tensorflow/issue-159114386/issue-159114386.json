{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2726", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2726/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2726/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2726/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2726", "id": 159114386, "node_id": "MDU6SXNzdWUxNTkxMTQzODY=", "number": 2726, "title": "Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "user": {"login": "hellf", "id": 9377459, "node_id": "MDQ6VXNlcjkzNzc0NTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/9377459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hellf", "html_url": "https://github.com/hellf", "followers_url": "https://api.github.com/users/hellf/followers", "following_url": "https://api.github.com/users/hellf/following{/other_user}", "gists_url": "https://api.github.com/users/hellf/gists{/gist_id}", "starred_url": "https://api.github.com/users/hellf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hellf/subscriptions", "organizations_url": "https://api.github.com/users/hellf/orgs", "repos_url": "https://api.github.com/users/hellf/repos", "events_url": "https://api.github.com/users/hellf/events{/privacy}", "received_events_url": "https://api.github.com/users/hellf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-06-08T09:07:45Z", "updated_at": "2016-08-21T08:00:37Z", "closed_at": "2016-07-11T21:53:30Z", "author_association": "NONE", "body_html": "<p>Operating System: 2 servers(each have 4 GPUs, Titan-x), ubuntu14.04<br>\nCUDA 7.5, cuDNN 4.0.7<br>\nTensorflow has been installed with source file with bazel.</p>\n<p>MNIST example works well on each single server.<br>\n~/tensorflow/tensorflow/models/image/mnist$ python convolutional.py</p>\n<p>To make MNIST example as distributed version, I modified convolutional.py as follows.<br>\nI refer \"Putting it all together: example trainer program\" in<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md</a></p>\n<pre><code>#Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n#This should achieve a test error of 0.7%. Please keep this model as simple and\n#linear as possible, it is meant as a tutorial for simple convolutional models.\n#Run with --self_test on the command line to execute a short self-test.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\nimport time\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\nWORK_DIRECTORY = 'data'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\n  ######################################################################################\n  #Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n  #Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n  ######################################################################################\n\ntf.app.flags.DEFINE_boolean(\"self_test\", False, \"True if running a self test.\")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef maybe_download(filename):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not tf.gfile.Exists(WORK_DIRECTORY):\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\n  filepath = os.path.join(WORK_DIRECTORY, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.Size()\n    print('Successfully downloaded', filename, size, 'bytes.')\n  return filepath\n\n\ndef extract_data(filename, num_images):\n  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n\n  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n  \"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n    return data\n\n\ndef extract_labels(filename, num_images):\n  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_images)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n  return labels\n\n\ndef fake_data(num_images):\n  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n  data = numpy.ndarray(\n      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n      dtype=numpy.float32)\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n  for image in xrange(num_images):\n    label = image % 2\n    data[image, :, :, 0] = label - 0.5\n    labels[image] = label\n  return data, labels\n\n\ndef error_rate(predictions, labels):\n  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n  return 100.0 - (\n      100.0 *\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n      predictions.shape[0])\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  ##########################################################################################\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n  ##########################################################################################\n  if FLAGS.self_test:\n    print('Running self-test.')\n    train_data, train_labels = fake_data(256)\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n    num_epochs = 1\n  else:\n    # Get the data.\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, ...]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_data = train_data[VALIDATION_SIZE:, ...]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    num_epochs = NUM_EPOCHS\n  ##########################################################################################\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n  ##########################################################################################\n      train_size = train_labels.shape[0]\n\n      # This is where training samples and labels are fed to the graph.\n      # These placeholder nodes will be fed a batch of training data at each\n      # training step using the {feed_dict} argument to the Run() call below.\n      train_data_node = tf.placeholder(\n          tf.float32,\n          shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n      train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n      eval_data = tf.placeholder(\n          tf.float32,\n          shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n      # The variables below hold all the trainable weights. They are passed an\n      # initial value which will be assigned when we call:\n      # {tf.initialize_all_variables().run()}\n      conv1_weights = tf.Variable(\n          tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                              stddev=0.1,\n                              seed=SEED))\n      conv1_biases = tf.Variable(tf.zeros([32]))\n      conv2_weights = tf.Variable(\n          tf.truncated_normal([5, 5, 32, 64],\n                              stddev=0.1,\n                              seed=SEED))\n      conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n      fc1_weights = tf.Variable(  # fully connected, depth 512.\n          tf.truncated_normal(\n              [IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n              stddev=0.1,\n              seed=SEED))\n      fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n      fc2_weights = tf.Variable(\n          tf.truncated_normal([512, NUM_LABELS],\n                              stddev=0.1,\n                              seed=SEED))\n      fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n\n      # We will replicate the model structure for the training subgraph, as well\n      # as the evaluation subgraphs, while sharing the trainable parameters.\n      def model(data, train=False):\n        \"\"\"The Model definition.\"\"\"\n        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n        # the same size as the input). Note that {strides} is a 4D array whose\n        # shape matches the data layout: [image index, y, x, depth].\n        conv = tf.nn.conv2d(data,\n                            conv1_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        # Bias and rectified linear non-linearity.\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n        # Max pooling. The kernel size spec {ksize} also follows the layout of\n        # the data. Here we have a pooling window of 2, and a stride of 2.\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        conv = tf.nn.conv2d(pool,\n                            conv2_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n        # fully connected layers.\n        pool_shape = pool.get_shape().as_list()\n        reshape = tf.reshape(\n            pool,\n            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n        # Fully connected layer. Note that the '+' operation automatically\n        # broadcasts the biases.\n        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n        # Add a 50% dropout during training only. Dropout also scales\n        # activations such that no rescaling is needed at evaluation time.\n        if train:\n          hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n        return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n      # Training computation: logits + cross-entropy loss.\n      logits = model(train_data_node, True)\n      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits, train_labels_node))\n\n      # L2 regularization for the fully connected parameters.\n      regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                      tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n      # Add the regularization term to the loss.\n      loss += 5e-4 * regularizers\n\n      # Optimizer: set up a variable that's incremented once per batch and\n      # controls the learning rate decay.\n      batch = tf.Variable(0)\n      # Decay once per epoch, using an exponential schedule starting at 0.01.\n      learning_rate = tf.train.exponential_decay(\n          0.01,                # Base learning rate.\n          batch * BATCH_SIZE,  # Current index into the dataset.\n          train_size,          # Decay step.\n          0.95,                # Decay rate.\n          staircase=True)\n      # Use simple momentum for the optimization.\n      optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                             0.9).minimize(loss,\n                                                           global_step=batch)\n\n      # Predictions for the current training minibatch.\n      train_prediction = tf.nn.softmax(logits)\n\n      # Predictions for the test and validation, which we'll compute less often.\n      eval_prediction = tf.nn.softmax(model(eval_data))\n\n      # Small utility function to evaluate a dataset by feeding batches of data to\n      # {eval_data} and pulling the results from {eval_predictions}.\n      # Saves memory and enables this to run on smaller GPUs.\n      def eval_in_batches(data, sess):\n        \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n        size = data.shape[0]\n        if size &lt; EVAL_BATCH_SIZE:\n          raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n        predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n          end = begin + EVAL_BATCH_SIZE\n          if end &lt;= size:\n            predictions[begin:end, :] = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[begin:end, ...]})\n          else:\n            batch_predictions = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n            predictions[begin:, :] = batch_predictions[begin - size:, :]\n        return predictions\n      # Run all the initializers to prepare the trainable parameters.\n      #saver = tf.train.Saver()#dist\n      summary_op = tf.merge_all_summaries()#dist\n      init_op = tf.initialize_all_variables()#dist\n      #tf.initialize_all_variables().run()\n      print('Initialized!')\n    ###################################################################################\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             #logdir=\"/home/user/tmp\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             #saver=saver,\n                             global_step=batch)#,\n                             #save_model_secs=600)\n    ###################################################################################\n    # Create a local session to run the training.\n    start_time = time.time()\n    #with tf.Session() as sess:\n    #with sv.managed_session(server.target) as sess:\n    with sv.prepare_or_wait_for_session(server.target, config=None) as sess:\n      # Loop through training steps.\n      for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n      #step = 0\n      #while not sv.should_stop() and step &lt; (int(num_epochs * train_size) // BATCH_SIZE):\n        # Compute the offset of the current minibatch in the data.\n        # Note that we could use better randomization across epochs.\n        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n        batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n        # This dictionary maps the batch data (as a numpy array) to the\n        # node in the graph it should be fed to.\n        feed_dict = {train_data_node: batch_data,\n                     train_labels_node: batch_labels}\n        # Run the graph and fetch some of the nodes.\n        _, l, lr, predictions = sess.run(\n            [optimizer, loss, learning_rate, train_prediction],\n            feed_dict=feed_dict)\n        if step % EVAL_FREQUENCY == 0:\n          elapsed_time = time.time() - start_time\n          start_time = time.time()\n          print('Step %d (epoch %.2f), %.1f ms' %\n                (step, float(step) * BATCH_SIZE / train_size,\n                 1000 * elapsed_time / EVAL_FREQUENCY))\n          print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n          print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n          print('Validation error: %.1f%%' % error_rate(\n              eval_in_batches(validation_data, sess), validation_labels))\n          sys.stdout.flush()\n      # Finally print the result!\n      test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n      print('Test error: %.1f%%' % test_error)\n      if FLAGS.self_test:\n        print('test_error', test_error)\n        assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % (\n            test_error,)\n    # Ask for all the services to stop.\n    sv.stop()\n\n\nif __name__ == '__main__':\n  tf.app.run()\n</code></pre>\n<h1>Commands</h1>\n<h2>on server1, terminal1</h2>\n<p>$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=ps --task_index=0</p>\n<h2>on server1, terminal2</h2>\n<p>$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=0</p>\n<h2>on server2, terminal2</h2>\n<p>$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=1</p>\n<h1>Message &amp; Error</h1>\n<h2>Common</h2>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:0b:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:09:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:06:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:05:00.0\nTotal memory: 12.00GiB\nFree memory: 11.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\n</code></pre>\n<h2>on server1, terminal1</h2>\n<pre><code>I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {localhost:2222, user-70:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {user-81:2223, user-70:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 10.14G (10892742656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 9.13G (9803467776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 8.22G (8823120896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n</code></pre>\n<p>...</p>\n<h2>on server1, terminal2</h2>\n<pre><code>I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {localhost:2223, user-70:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:08.411750480   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:09.412362146   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:11.005026028   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:13.191502278   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\nTraceback (most recent call last):\n  File \"convolutional.py\", line 367, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 343, in main\n    feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InternalError: cuDNN launch failure : input shape([64,1,28,28]) filter shape([5,5,1,32])\n         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:worker/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_G6, Variable/read_S53)]]\n         [[Node: add_5_G12 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device_incarnation=-6674272897051056682, tensor_name=\"edge_106_add_5\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D', defined at:\n  File \"convolutional.py\", line 367, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 254, in main\n    logits = model(train_data_node, True)\n  File \"convolutional.py\", line 220, in model\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 295, in conv2d\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n</code></pre>\n<h2>on server2, terminal2</h2>\n<pre><code>I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {user-81:2223, localhost:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:48.351331932   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:49.352058170   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:50.939646814   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\n</code></pre>\n<p>Could anybody help me?</p>\n<h2>Errors on server1, terminal1</h2>\n<pre><code>E tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\n</code></pre>\n<h2>Errors on server1, terminal2</h2>\n<pre><code>E tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n</code></pre>", "body_text": "Operating System: 2 servers(each have 4 GPUs, Titan-x), ubuntu14.04\nCUDA 7.5, cuDNN 4.0.7\nTensorflow has been installed with source file with bazel.\nMNIST example works well on each single server.\n~/tensorflow/tensorflow/models/image/mnist$ python convolutional.py\nTo make MNIST example as distributed version, I modified convolutional.py as follows.\nI refer \"Putting it all together: example trainer program\" in\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md\n#Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n#This should achieve a test error of 0.7%. Please keep this model as simple and\n#linear as possible, it is meant as a tutorial for simple convolutional models.\n#Run with --self_test on the command line to execute a short self-test.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\nimport time\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\nWORK_DIRECTORY = 'data'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\n  ######################################################################################\n  #Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n  #Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n  ######################################################################################\n\ntf.app.flags.DEFINE_boolean(\"self_test\", False, \"True if running a self test.\")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef maybe_download(filename):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not tf.gfile.Exists(WORK_DIRECTORY):\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\n  filepath = os.path.join(WORK_DIRECTORY, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.Size()\n    print('Successfully downloaded', filename, size, 'bytes.')\n  return filepath\n\n\ndef extract_data(filename, num_images):\n  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n\n  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n  \"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n    return data\n\n\ndef extract_labels(filename, num_images):\n  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_images)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n  return labels\n\n\ndef fake_data(num_images):\n  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n  data = numpy.ndarray(\n      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n      dtype=numpy.float32)\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n  for image in xrange(num_images):\n    label = image % 2\n    data[image, :, :, 0] = label - 0.5\n    labels[image] = label\n  return data, labels\n\n\ndef error_rate(predictions, labels):\n  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n  return 100.0 - (\n      100.0 *\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n      predictions.shape[0])\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  ##########################################################################################\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n  ##########################################################################################\n  if FLAGS.self_test:\n    print('Running self-test.')\n    train_data, train_labels = fake_data(256)\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n    num_epochs = 1\n  else:\n    # Get the data.\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, ...]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_data = train_data[VALIDATION_SIZE:, ...]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    num_epochs = NUM_EPOCHS\n  ##########################################################################################\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n  ##########################################################################################\n      train_size = train_labels.shape[0]\n\n      # This is where training samples and labels are fed to the graph.\n      # These placeholder nodes will be fed a batch of training data at each\n      # training step using the {feed_dict} argument to the Run() call below.\n      train_data_node = tf.placeholder(\n          tf.float32,\n          shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n      train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n      eval_data = tf.placeholder(\n          tf.float32,\n          shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n      # The variables below hold all the trainable weights. They are passed an\n      # initial value which will be assigned when we call:\n      # {tf.initialize_all_variables().run()}\n      conv1_weights = tf.Variable(\n          tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                              stddev=0.1,\n                              seed=SEED))\n      conv1_biases = tf.Variable(tf.zeros([32]))\n      conv2_weights = tf.Variable(\n          tf.truncated_normal([5, 5, 32, 64],\n                              stddev=0.1,\n                              seed=SEED))\n      conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n      fc1_weights = tf.Variable(  # fully connected, depth 512.\n          tf.truncated_normal(\n              [IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n              stddev=0.1,\n              seed=SEED))\n      fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n      fc2_weights = tf.Variable(\n          tf.truncated_normal([512, NUM_LABELS],\n                              stddev=0.1,\n                              seed=SEED))\n      fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n\n      # We will replicate the model structure for the training subgraph, as well\n      # as the evaluation subgraphs, while sharing the trainable parameters.\n      def model(data, train=False):\n        \"\"\"The Model definition.\"\"\"\n        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n        # the same size as the input). Note that {strides} is a 4D array whose\n        # shape matches the data layout: [image index, y, x, depth].\n        conv = tf.nn.conv2d(data,\n                            conv1_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        # Bias and rectified linear non-linearity.\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n        # Max pooling. The kernel size spec {ksize} also follows the layout of\n        # the data. Here we have a pooling window of 2, and a stride of 2.\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        conv = tf.nn.conv2d(pool,\n                            conv2_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n        # fully connected layers.\n        pool_shape = pool.get_shape().as_list()\n        reshape = tf.reshape(\n            pool,\n            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n        # Fully connected layer. Note that the '+' operation automatically\n        # broadcasts the biases.\n        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n        # Add a 50% dropout during training only. Dropout also scales\n        # activations such that no rescaling is needed at evaluation time.\n        if train:\n          hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n        return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n      # Training computation: logits + cross-entropy loss.\n      logits = model(train_data_node, True)\n      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits, train_labels_node))\n\n      # L2 regularization for the fully connected parameters.\n      regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                      tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n      # Add the regularization term to the loss.\n      loss += 5e-4 * regularizers\n\n      # Optimizer: set up a variable that's incremented once per batch and\n      # controls the learning rate decay.\n      batch = tf.Variable(0)\n      # Decay once per epoch, using an exponential schedule starting at 0.01.\n      learning_rate = tf.train.exponential_decay(\n          0.01,                # Base learning rate.\n          batch * BATCH_SIZE,  # Current index into the dataset.\n          train_size,          # Decay step.\n          0.95,                # Decay rate.\n          staircase=True)\n      # Use simple momentum for the optimization.\n      optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                             0.9).minimize(loss,\n                                                           global_step=batch)\n\n      # Predictions for the current training minibatch.\n      train_prediction = tf.nn.softmax(logits)\n\n      # Predictions for the test and validation, which we'll compute less often.\n      eval_prediction = tf.nn.softmax(model(eval_data))\n\n      # Small utility function to evaluate a dataset by feeding batches of data to\n      # {eval_data} and pulling the results from {eval_predictions}.\n      # Saves memory and enables this to run on smaller GPUs.\n      def eval_in_batches(data, sess):\n        \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n        size = data.shape[0]\n        if size < EVAL_BATCH_SIZE:\n          raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n        predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n          end = begin + EVAL_BATCH_SIZE\n          if end <= size:\n            predictions[begin:end, :] = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[begin:end, ...]})\n          else:\n            batch_predictions = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n            predictions[begin:, :] = batch_predictions[begin - size:, :]\n        return predictions\n      # Run all the initializers to prepare the trainable parameters.\n      #saver = tf.train.Saver()#dist\n      summary_op = tf.merge_all_summaries()#dist\n      init_op = tf.initialize_all_variables()#dist\n      #tf.initialize_all_variables().run()\n      print('Initialized!')\n    ###################################################################################\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             #logdir=\"/home/user/tmp\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             #saver=saver,\n                             global_step=batch)#,\n                             #save_model_secs=600)\n    ###################################################################################\n    # Create a local session to run the training.\n    start_time = time.time()\n    #with tf.Session() as sess:\n    #with sv.managed_session(server.target) as sess:\n    with sv.prepare_or_wait_for_session(server.target, config=None) as sess:\n      # Loop through training steps.\n      for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n      #step = 0\n      #while not sv.should_stop() and step < (int(num_epochs * train_size) // BATCH_SIZE):\n        # Compute the offset of the current minibatch in the data.\n        # Note that we could use better randomization across epochs.\n        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n        batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n        # This dictionary maps the batch data (as a numpy array) to the\n        # node in the graph it should be fed to.\n        feed_dict = {train_data_node: batch_data,\n                     train_labels_node: batch_labels}\n        # Run the graph and fetch some of the nodes.\n        _, l, lr, predictions = sess.run(\n            [optimizer, loss, learning_rate, train_prediction],\n            feed_dict=feed_dict)\n        if step % EVAL_FREQUENCY == 0:\n          elapsed_time = time.time() - start_time\n          start_time = time.time()\n          print('Step %d (epoch %.2f), %.1f ms' %\n                (step, float(step) * BATCH_SIZE / train_size,\n                 1000 * elapsed_time / EVAL_FREQUENCY))\n          print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n          print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n          print('Validation error: %.1f%%' % error_rate(\n              eval_in_batches(validation_data, sess), validation_labels))\n          sys.stdout.flush()\n      # Finally print the result!\n      test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n      print('Test error: %.1f%%' % test_error)\n      if FLAGS.self_test:\n        print('test_error', test_error)\n        assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % (\n            test_error,)\n    # Ask for all the services to stop.\n    sv.stop()\n\n\nif __name__ == '__main__':\n  tf.app.run()\n\nCommands\non server1, terminal1\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=ps --task_index=0\non server1, terminal2\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=0\non server2, terminal2\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=1\nMessage & Error\nCommon\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:0b:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:09:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:06:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:05:00.0\nTotal memory: 12.00GiB\nFree memory: 11.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\n\non server1, terminal1\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2222, user-70:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {user-81:2223, user-70:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 10.14G (10892742656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 9.13G (9803467776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 8.22G (8823120896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n\n...\non server1, terminal2\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, user-70:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:08.411750480   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:09.412362146   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:11.005026028   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:13.191502278   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\nTraceback (most recent call last):\n  File \"convolutional.py\", line 367, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 343, in main\n    feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InternalError: cuDNN launch failure : input shape([64,1,28,28]) filter shape([5,5,1,32])\n         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:worker/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_G6, Variable/read_S53)]]\n         [[Node: add_5_G12 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device_incarnation=-6674272897051056682, tensor_name=\"edge_106_add_5\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D', defined at:\n  File \"convolutional.py\", line 367, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 254, in main\n    logits = model(train_data_node, True)\n  File \"convolutional.py\", line 220, in model\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 295, in conv2d\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\non server2, terminal2\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {user-81:2223, localhost:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:48.351331932   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:49.352058170   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:50.939646814   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\n\nCould anybody help me?\nErrors on server1, terminal1\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\n\nErrors on server1, terminal2\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY", "body": "Operating System: 2 servers(each have 4 GPUs, Titan-x), ubuntu14.04\nCUDA 7.5, cuDNN 4.0.7\nTensorflow has been installed with source file with bazel.\n\nMNIST example works well on each single server.\n~/tensorflow/tensorflow/models/image/mnist$ python convolutional.py\n\nTo make MNIST example as distributed version, I modified convolutional.py as follows.\nI refer \"Putting it all together: example trainer program\" in \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md\n\n```\n#Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n#This should achieve a test error of 0.7%. Please keep this model as simple and\n#linear as possible, it is meant as a tutorial for simple convolutional models.\n#Run with --self_test on the command line to execute a short self-test.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\nimport time\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\nWORK_DIRECTORY = 'data'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\n  ######################################################################################\n  #Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n  #Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n  ######################################################################################\n\ntf.app.flags.DEFINE_boolean(\"self_test\", False, \"True if running a self test.\")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef maybe_download(filename):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not tf.gfile.Exists(WORK_DIRECTORY):\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\n  filepath = os.path.join(WORK_DIRECTORY, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.Size()\n    print('Successfully downloaded', filename, size, 'bytes.')\n  return filepath\n\n\ndef extract_data(filename, num_images):\n  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n\n  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n  \"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n    return data\n\n\ndef extract_labels(filename, num_images):\n  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_images)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n  return labels\n\n\ndef fake_data(num_images):\n  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n  data = numpy.ndarray(\n      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n      dtype=numpy.float32)\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n  for image in xrange(num_images):\n    label = image % 2\n    data[image, :, :, 0] = label - 0.5\n    labels[image] = label\n  return data, labels\n\n\ndef error_rate(predictions, labels):\n  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n  return 100.0 - (\n      100.0 *\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n      predictions.shape[0])\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  ##########################################################################################\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n  ##########################################################################################\n  if FLAGS.self_test:\n    print('Running self-test.')\n    train_data, train_labels = fake_data(256)\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n    num_epochs = 1\n  else:\n    # Get the data.\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, ...]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_data = train_data[VALIDATION_SIZE:, ...]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    num_epochs = NUM_EPOCHS\n  ##########################################################################################\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n  ##########################################################################################\n      train_size = train_labels.shape[0]\n\n      # This is where training samples and labels are fed to the graph.\n      # These placeholder nodes will be fed a batch of training data at each\n      # training step using the {feed_dict} argument to the Run() call below.\n      train_data_node = tf.placeholder(\n          tf.float32,\n          shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n      train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n      eval_data = tf.placeholder(\n          tf.float32,\n          shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n      # The variables below hold all the trainable weights. They are passed an\n      # initial value which will be assigned when we call:\n      # {tf.initialize_all_variables().run()}\n      conv1_weights = tf.Variable(\n          tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                              stddev=0.1,\n                              seed=SEED))\n      conv1_biases = tf.Variable(tf.zeros([32]))\n      conv2_weights = tf.Variable(\n          tf.truncated_normal([5, 5, 32, 64],\n                              stddev=0.1,\n                              seed=SEED))\n      conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n      fc1_weights = tf.Variable(  # fully connected, depth 512.\n          tf.truncated_normal(\n              [IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n              stddev=0.1,\n              seed=SEED))\n      fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n      fc2_weights = tf.Variable(\n          tf.truncated_normal([512, NUM_LABELS],\n                              stddev=0.1,\n                              seed=SEED))\n      fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n\n      # We will replicate the model structure for the training subgraph, as well\n      # as the evaluation subgraphs, while sharing the trainable parameters.\n      def model(data, train=False):\n        \"\"\"The Model definition.\"\"\"\n        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n        # the same size as the input). Note that {strides} is a 4D array whose\n        # shape matches the data layout: [image index, y, x, depth].\n        conv = tf.nn.conv2d(data,\n                            conv1_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        # Bias and rectified linear non-linearity.\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n        # Max pooling. The kernel size spec {ksize} also follows the layout of\n        # the data. Here we have a pooling window of 2, and a stride of 2.\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        conv = tf.nn.conv2d(pool,\n                            conv2_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n        # fully connected layers.\n        pool_shape = pool.get_shape().as_list()\n        reshape = tf.reshape(\n            pool,\n            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n        # Fully connected layer. Note that the '+' operation automatically\n        # broadcasts the biases.\n        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n        # Add a 50% dropout during training only. Dropout also scales\n        # activations such that no rescaling is needed at evaluation time.\n        if train:\n          hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n        return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n      # Training computation: logits + cross-entropy loss.\n      logits = model(train_data_node, True)\n      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits, train_labels_node))\n\n      # L2 regularization for the fully connected parameters.\n      regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                      tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n      # Add the regularization term to the loss.\n      loss += 5e-4 * regularizers\n\n      # Optimizer: set up a variable that's incremented once per batch and\n      # controls the learning rate decay.\n      batch = tf.Variable(0)\n      # Decay once per epoch, using an exponential schedule starting at 0.01.\n      learning_rate = tf.train.exponential_decay(\n          0.01,                # Base learning rate.\n          batch * BATCH_SIZE,  # Current index into the dataset.\n          train_size,          # Decay step.\n          0.95,                # Decay rate.\n          staircase=True)\n      # Use simple momentum for the optimization.\n      optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                             0.9).minimize(loss,\n                                                           global_step=batch)\n\n      # Predictions for the current training minibatch.\n      train_prediction = tf.nn.softmax(logits)\n\n      # Predictions for the test and validation, which we'll compute less often.\n      eval_prediction = tf.nn.softmax(model(eval_data))\n\n      # Small utility function to evaluate a dataset by feeding batches of data to\n      # {eval_data} and pulling the results from {eval_predictions}.\n      # Saves memory and enables this to run on smaller GPUs.\n      def eval_in_batches(data, sess):\n        \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n        size = data.shape[0]\n        if size < EVAL_BATCH_SIZE:\n          raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n        predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n          end = begin + EVAL_BATCH_SIZE\n          if end <= size:\n            predictions[begin:end, :] = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[begin:end, ...]})\n          else:\n            batch_predictions = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n            predictions[begin:, :] = batch_predictions[begin - size:, :]\n        return predictions\n      # Run all the initializers to prepare the trainable parameters.\n      #saver = tf.train.Saver()#dist\n      summary_op = tf.merge_all_summaries()#dist\n      init_op = tf.initialize_all_variables()#dist\n      #tf.initialize_all_variables().run()\n      print('Initialized!')\n    ###################################################################################\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             #logdir=\"/home/user/tmp\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             #saver=saver,\n                             global_step=batch)#,\n                             #save_model_secs=600)\n    ###################################################################################\n    # Create a local session to run the training.\n    start_time = time.time()\n    #with tf.Session() as sess:\n    #with sv.managed_session(server.target) as sess:\n    with sv.prepare_or_wait_for_session(server.target, config=None) as sess:\n      # Loop through training steps.\n      for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n      #step = 0\n      #while not sv.should_stop() and step < (int(num_epochs * train_size) // BATCH_SIZE):\n        # Compute the offset of the current minibatch in the data.\n        # Note that we could use better randomization across epochs.\n        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n        batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n        # This dictionary maps the batch data (as a numpy array) to the\n        # node in the graph it should be fed to.\n        feed_dict = {train_data_node: batch_data,\n                     train_labels_node: batch_labels}\n        # Run the graph and fetch some of the nodes.\n        _, l, lr, predictions = sess.run(\n            [optimizer, loss, learning_rate, train_prediction],\n            feed_dict=feed_dict)\n        if step % EVAL_FREQUENCY == 0:\n          elapsed_time = time.time() - start_time\n          start_time = time.time()\n          print('Step %d (epoch %.2f), %.1f ms' %\n                (step, float(step) * BATCH_SIZE / train_size,\n                 1000 * elapsed_time / EVAL_FREQUENCY))\n          print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n          print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n          print('Validation error: %.1f%%' % error_rate(\n              eval_in_batches(validation_data, sess), validation_labels))\n          sys.stdout.flush()\n      # Finally print the result!\n      test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n      print('Test error: %.1f%%' % test_error)\n      if FLAGS.self_test:\n        print('test_error', test_error)\n        assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % (\n            test_error,)\n    # Ask for all the services to stop.\n    sv.stop()\n\n\nif __name__ == '__main__':\n  tf.app.run()\n```\n# Commands\n## on server1, terminal1\n\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=ps --task_index=0\n## on server1, terminal2\n\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=0\n## on server2, terminal2\n\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=1\n# Message & Error\n## Common\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:0b:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:09:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:06:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:05:00.0\nTotal memory: 12.00GiB\nFree memory: 11.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\n```\n## on server1, terminal1\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2222, user-70:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {user-81:2223, user-70:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 10.14G (10892742656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 9.13G (9803467776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 8.22G (8823120896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n```\n\n...\n## on server1, terminal2\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, user-70:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:08.411750480   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:09.412362146   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:11.005026028   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:13.191502278   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\nTraceback (most recent call last):\n  File \"convolutional.py\", line 367, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 343, in main\n    feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InternalError: cuDNN launch failure : input shape([64,1,28,28]) filter shape([5,5,1,32])\n         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:worker/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_G6, Variable/read_S53)]]\n         [[Node: add_5_G12 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device_incarnation=-6674272897051056682, tensor_name=\"edge_106_add_5\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D', defined at:\n  File \"convolutional.py\", line 367, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 254, in main\n    logits = model(train_data_node, True)\n  File \"convolutional.py\", line 220, in model\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 295, in conv2d\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n```\n## on server2, terminal2\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {user-81:2223, localhost:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:48.351331932   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:49.352058170   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:50.939646814   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\n```\n\nCould anybody help me?\n## Errors on server1, terminal1\n\n```\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\n```\n## Errors on server1, terminal2\n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n```\n"}