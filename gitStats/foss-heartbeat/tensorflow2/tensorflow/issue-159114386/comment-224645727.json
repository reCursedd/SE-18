{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/224645727", "html_url": "https://github.com/tensorflow/tensorflow/issues/2726#issuecomment-224645727", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2726", "id": 224645727, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDY0NTcyNw==", "user": {"login": "ischlag", "id": 7107156, "node_id": "MDQ6VXNlcjcxMDcxNTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/7107156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ischlag", "html_url": "https://github.com/ischlag", "followers_url": "https://api.github.com/users/ischlag/followers", "following_url": "https://api.github.com/users/ischlag/following{/other_user}", "gists_url": "https://api.github.com/users/ischlag/gists{/gist_id}", "starred_url": "https://api.github.com/users/ischlag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ischlag/subscriptions", "organizations_url": "https://api.github.com/users/ischlag/orgs", "repos_url": "https://api.github.com/users/ischlag/repos", "events_url": "https://api.github.com/users/ischlag/events{/privacy}", "received_events_url": "https://api.github.com/users/ischlag/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-08T16:23:17Z", "updated_at": "2016-06-08T16:26:22Z", "author_association": "NONE", "body_html": "<p>Well, your script is working on my machines. What do you get if you execute the following command:</p>\n<pre><code>(tf-env)worker1:~$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n#define CUDNN_MAJOR      4\n#define CUDNN_MINOR      0\n#define CUDNN_PATCHLEVEL 7\n</code></pre>\n<p>Make sure to run it on all GPU machines.</p>\n<p>I ran your script on 1 paramater server without gpu and 3 workers with each one GTX 960 GPU.</p>\n<p>Parameter Server:</p>\n<pre><code>(tensorflow-cpu)param:~$ python convolutional.py \\\n&gt; --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n&gt; --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n&gt; --job_name=ps --task_index=0\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n</code></pre>\n<p>Worker 1:</p>\n<pre><code>(tf-env)worker1:~$ python convolutional.py \\\n&gt; --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n&gt; --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n&gt; --job_name=worker --task_index=0\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.77GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {localhost:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:30.327518348    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:30.327551292    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328548987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328627152    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:32.725781597    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:33.036280593    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.500345457    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.721717987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:38.816290592    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:39.043082815    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:46.250808712    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.668526778    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nStep 0 (epoch 0.00), 449.4 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 301.8 ms\nMinibatch loss: 3.292, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.5%\nStep 200 (epoch 0.23), 413.2 ms\nMinibatch loss: 3.435, learning rate: 0.010000\nMinibatch error: 7.8%\nValidation error: 3.2%\n</code></pre>\n<p>Worker 2:</p>\n<pre><code>(tf-env)worker2:~$ python convolutional.py \\\n&gt; --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n&gt; --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n&gt; --job_name=worker --task_index=1\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.61GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {pc3-002-l.cs.some-university.ac.uk:2222, localhost:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:46.733183601    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:47.733565779    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:49.099973593    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:51.845976385    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.380873638    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:02:02.158287167    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nStep 0 (epoch 0.00), 592.7 ms\nMinibatch loss: 3.580, learning rate: 0.010000\nMinibatch error: 14.1%\nValidation error: 7.3%\nStep 100 (epoch 0.12), 408.5 ms\nMinibatch loss: 3.029, learning rate: 0.010000\nMinibatch error: 0.0%\nValidation error: 3.2%\nStep 200 (epoch 0.23), 415.5 ms\nMinibatch loss: 2.972, learning rate: 0.010000\nMinibatch error: 1.6%\nValidation error: 2.4%\n</code></pre>\n<p>Worker 3:</p>\n<pre><code>(tf-env)worker3:~$ python convolutional.py \\\n&gt; --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n&gt; --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n&gt; --job_name=worker --task_index=2\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.53GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nStep 0 (epoch 0.00), 337.0 ms\nMinibatch loss: 3.471, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 7.4%\nStep 100 (epoch 0.12), 407.0 ms\nMinibatch loss: 3.057, learning rate: 0.010000\nMinibatch error: 3.1%\nValidation error: 3.1%\nStep 200 (epoch 0.23), 412.3 ms\nMinibatch loss: 3.086, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 2.3%\n</code></pre>\n<p>I'm actually very happy I found this. I currently learning how to use tensorflow in a distributed setting. So thanks for your detailed and excellent example!</p>", "body_text": "Well, your script is working on my machines. What do you get if you execute the following command:\n(tf-env)worker1:~$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n#define CUDNN_MAJOR      4\n#define CUDNN_MINOR      0\n#define CUDNN_PATCHLEVEL 7\n\nMake sure to run it on all GPU machines.\nI ran your script on 1 paramater server without gpu and 3 workers with each one GTX 960 GPU.\nParameter Server:\n(tensorflow-cpu)param:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=ps --task_index=0\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n\nWorker 1:\n(tf-env)worker1:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=0\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.77GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:30.327518348    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:30.327551292    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328548987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328627152    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:32.725781597    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:33.036280593    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.500345457    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.721717987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:38.816290592    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:39.043082815    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:46.250808712    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.668526778    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nStep 0 (epoch 0.00), 449.4 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 301.8 ms\nMinibatch loss: 3.292, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.5%\nStep 200 (epoch 0.23), 413.2 ms\nMinibatch loss: 3.435, learning rate: 0.010000\nMinibatch error: 7.8%\nValidation error: 3.2%\n\nWorker 2:\n(tf-env)worker2:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=1\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.61GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, localhost:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:46.733183601    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:47.733565779    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:49.099973593    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:51.845976385    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.380873638    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:02:02.158287167    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nStep 0 (epoch 0.00), 592.7 ms\nMinibatch loss: 3.580, learning rate: 0.010000\nMinibatch error: 14.1%\nValidation error: 7.3%\nStep 100 (epoch 0.12), 408.5 ms\nMinibatch loss: 3.029, learning rate: 0.010000\nMinibatch error: 0.0%\nValidation error: 3.2%\nStep 200 (epoch 0.23), 415.5 ms\nMinibatch loss: 2.972, learning rate: 0.010000\nMinibatch error: 1.6%\nValidation error: 2.4%\n\nWorker 3:\n(tf-env)worker3:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=2\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.53GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nStep 0 (epoch 0.00), 337.0 ms\nMinibatch loss: 3.471, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 7.4%\nStep 100 (epoch 0.12), 407.0 ms\nMinibatch loss: 3.057, learning rate: 0.010000\nMinibatch error: 3.1%\nValidation error: 3.1%\nStep 200 (epoch 0.23), 412.3 ms\nMinibatch loss: 3.086, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 2.3%\n\nI'm actually very happy I found this. I currently learning how to use tensorflow in a distributed setting. So thanks for your detailed and excellent example!", "body": "Well, your script is working on my machines. What do you get if you execute the following command:\n\n```\n(tf-env)worker1:~$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n#define CUDNN_MAJOR      4\n#define CUDNN_MINOR      0\n#define CUDNN_PATCHLEVEL 7\n```\n\nMake sure to run it on all GPU machines.\n\nI ran your script on 1 paramater server without gpu and 3 workers with each one GTX 960 GPU. \n\nParameter Server:\n\n```\n(tensorflow-cpu)param:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=ps --task_index=0\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n```\n\nWorker 1:\n\n```\n(tf-env)worker1:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=0\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.77GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:30.327518348    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:30.327551292    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328548987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328627152    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:32.725781597    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:33.036280593    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.500345457    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.721717987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:38.816290592    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:39.043082815    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:46.250808712    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.668526778    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nStep 0 (epoch 0.00), 449.4 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 301.8 ms\nMinibatch loss: 3.292, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.5%\nStep 200 (epoch 0.23), 413.2 ms\nMinibatch loss: 3.435, learning rate: 0.010000\nMinibatch error: 7.8%\nValidation error: 3.2%\n```\n\nWorker 2:\n\n```\n(tf-env)worker2:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=1\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.61GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, localhost:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:46.733183601    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:47.733565779    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:49.099973593    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:51.845976385    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.380873638    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:02:02.158287167    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nStep 0 (epoch 0.00), 592.7 ms\nMinibatch loss: 3.580, learning rate: 0.010000\nMinibatch error: 14.1%\nValidation error: 7.3%\nStep 100 (epoch 0.12), 408.5 ms\nMinibatch loss: 3.029, learning rate: 0.010000\nMinibatch error: 0.0%\nValidation error: 3.2%\nStep 200 (epoch 0.23), 415.5 ms\nMinibatch loss: 2.972, learning rate: 0.010000\nMinibatch error: 1.6%\nValidation error: 2.4%\n```\n\nWorker 3:\n\n```\n(tf-env)worker3:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=2\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.53GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nStep 0 (epoch 0.00), 337.0 ms\nMinibatch loss: 3.471, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 7.4%\nStep 100 (epoch 0.12), 407.0 ms\nMinibatch loss: 3.057, learning rate: 0.010000\nMinibatch error: 3.1%\nValidation error: 3.1%\nStep 200 (epoch 0.23), 412.3 ms\nMinibatch loss: 3.086, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 2.3%\n```\n\nI'm actually very happy I found this. I currently learning how to use tensorflow in a distributed setting. So thanks for your detailed and excellent example!\n"}