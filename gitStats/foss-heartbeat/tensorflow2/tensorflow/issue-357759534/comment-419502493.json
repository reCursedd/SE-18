{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419502493", "html_url": "https://github.com/tensorflow/tensorflow/issues/22119#issuecomment-419502493", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22119", "id": 419502493, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTUwMjQ5Mw==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T16:58:38Z", "updated_at": "2018-09-07T16:58:38Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>It runs consistently when I include these flags TF_XLA_FLAGS=\"--parallel_check_failfast=false\"</p>\n</blockquote>\n<p>The parallel_check_op is deleted at HEAD because it was unused.  If it were affecting things, then you should see in your logs <code>failfast on first parallel-check failure</code>.  If you don't see that, I don't think this flag can possibly be changing anything.</p>\n<p>Can you confirm that you have looked carefully through the logs when running with CUDA_LAUNCH_BLOCKING and do not notice anything else suspicious?  Is it possible to post (or send privately) your full log, or if you're not comfortable with that, the log of all lines that start with <code>W</code> or <code>E</code> (i.e., all warnings and errors)?</p>\n<blockquote>\n<p>It seems to be when I have opt.compute_gradients(loss, params) compiled into XLA code that I see a weird 3x speed-up but the loss doesn't decrease</p>\n</blockquote>\n<p>Weird.</p>\n<p>It could totally be a bug in XLA (or cudnn), but it may be pretty hard to debug without the ability to reproduce.</p>", "body_text": "It runs consistently when I include these flags TF_XLA_FLAGS=\"--parallel_check_failfast=false\"\n\nThe parallel_check_op is deleted at HEAD because it was unused.  If it were affecting things, then you should see in your logs failfast on first parallel-check failure.  If you don't see that, I don't think this flag can possibly be changing anything.\nCan you confirm that you have looked carefully through the logs when running with CUDA_LAUNCH_BLOCKING and do not notice anything else suspicious?  Is it possible to post (or send privately) your full log, or if you're not comfortable with that, the log of all lines that start with W or E (i.e., all warnings and errors)?\n\nIt seems to be when I have opt.compute_gradients(loss, params) compiled into XLA code that I see a weird 3x speed-up but the loss doesn't decrease\n\nWeird.\nIt could totally be a bug in XLA (or cudnn), but it may be pretty hard to debug without the ability to reproduce.", "body": "> It runs consistently when I include these flags TF_XLA_FLAGS=\"--parallel_check_failfast=false\"\r\n\r\nThe parallel_check_op is deleted at HEAD because it was unused.  If it were affecting things, then you should see in your logs `failfast on first parallel-check failure`.  If you don't see that, I don't think this flag can possibly be changing anything.\r\n\r\nCan you confirm that you have looked carefully through the logs when running with CUDA_LAUNCH_BLOCKING and do not notice anything else suspicious?  Is it possible to post (or send privately) your full log, or if you're not comfortable with that, the log of all lines that start with `W` or `E` (i.e., all warnings and errors)?\r\n\r\n> It seems to be when I have opt.compute_gradients(loss, params) compiled into XLA code that I see a weird 3x speed-up but the loss doesn't decrease\r\n\r\nWeird.\r\n\r\nIt could totally be a bug in XLA (or cudnn), but it may be pretty hard to debug without the ability to reproduce."}