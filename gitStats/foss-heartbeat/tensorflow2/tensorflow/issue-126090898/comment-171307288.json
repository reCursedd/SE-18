{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/171307288", "html_url": "https://github.com/tensorflow/tensorflow/issues/751#issuecomment-171307288", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/751", "id": 171307288, "node_id": "MDEyOklzc3VlQ29tbWVudDE3MTMwNzI4OA==", "user": {"login": "MarkDaoust", "id": 1414837, "node_id": "MDQ6VXNlcjE0MTQ4Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1414837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkDaoust", "html_url": "https://github.com/MarkDaoust", "followers_url": "https://api.github.com/users/MarkDaoust/followers", "following_url": "https://api.github.com/users/MarkDaoust/following{/other_user}", "gists_url": "https://api.github.com/users/MarkDaoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkDaoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkDaoust/subscriptions", "organizations_url": "https://api.github.com/users/MarkDaoust/orgs", "repos_url": "https://api.github.com/users/MarkDaoust/repos", "events_url": "https://api.github.com/users/MarkDaoust/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkDaoust/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-13T14:30:01Z", "updated_at": "2016-01-13T14:30:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry about that.</p>\n<p>It sounds like you've working on this already.</p>\n<p>You've probably already figured out that maybe one starting point could be moving the <a href=\"https://github.com/tensorflow/tensorflow/blob/eef2aaa3f7931cc3c0e7bbc6757386cfbcf5e3ef/tensorflow/python/training/saver.py#L923-L925\"><code>path.join</code> in <code>saver.latest_checkpoint</code></a> to the <a href=\"https://github.com/tensorflow/tensorflow/blob/eef2aaa3f7931cc3c0e7bbc6757386cfbcf5e3ef/tensorflow/python/training/saver.py#L550\">end of <code>saver.get_checkpoint_state</code></a>, and applying it instead to all the paths in <code>ckp</code> (unless I've missed something else).<br>\nI'd also add asserts to the tests checking that all the file paths returned from <code>save.last_checkpoints</code> and  <code>tf.train.get_checkpoint_state</code> exist (which would have caught this bug).</p>\n<p>Just let me know if you want any of that in a pull request, or if there's anything else I can do.</p>", "body_text": "Sorry about that.\nIt sounds like you've working on this already.\nYou've probably already figured out that maybe one starting point could be moving the path.join in saver.latest_checkpoint to the end of saver.get_checkpoint_state, and applying it instead to all the paths in ckp (unless I've missed something else).\nI'd also add asserts to the tests checking that all the file paths returned from save.last_checkpoints and  tf.train.get_checkpoint_state exist (which would have caught this bug).\nJust let me know if you want any of that in a pull request, or if there's anything else I can do.", "body": "Sorry about that.\n\nIt sounds like you've working on this already.\n\nYou've probably already figured out that maybe one starting point could be moving the [`path.join` in `saver.latest_checkpoint`](https://github.com/tensorflow/tensorflow/blob/eef2aaa3f7931cc3c0e7bbc6757386cfbcf5e3ef/tensorflow/python/training/saver.py#L923-L925) to the [end of `saver.get_checkpoint_state`](https://github.com/tensorflow/tensorflow/blob/eef2aaa3f7931cc3c0e7bbc6757386cfbcf5e3ef/tensorflow/python/training/saver.py#L550), and applying it instead to all the paths in `ckp` (unless I've missed something else). \nI'd also add asserts to the tests checking that all the file paths returned from `save.last_checkpoints` and  `tf.train.get_checkpoint_state` exist (which would have caught this bug).\n\nJust let me know if you want any of that in a pull request, or if there's anything else I can do.\n"}