{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318717719", "html_url": "https://github.com/tensorflow/tensorflow/issues/6722#issuecomment-318717719", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6722", "id": 318717719, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODcxNzcxOQ==", "user": {"login": "Valentin4869", "id": 12462272, "node_id": "MDQ6VXNlcjEyNDYyMjcy", "avatar_url": "https://avatars2.githubusercontent.com/u/12462272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Valentin4869", "html_url": "https://github.com/Valentin4869", "followers_url": "https://api.github.com/users/Valentin4869/followers", "following_url": "https://api.github.com/users/Valentin4869/following{/other_user}", "gists_url": "https://api.github.com/users/Valentin4869/gists{/gist_id}", "starred_url": "https://api.github.com/users/Valentin4869/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Valentin4869/subscriptions", "organizations_url": "https://api.github.com/users/Valentin4869/orgs", "repos_url": "https://api.github.com/users/Valentin4869/repos", "events_url": "https://api.github.com/users/Valentin4869/events{/privacy}", "received_events_url": "https://api.github.com/users/Valentin4869/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-28T17:40:57Z", "updated_at": "2017-07-28T17:41:15Z", "author_association": "NONE", "body_html": "<p>Sorry, I know you closed this, but I've ran into this problem myself today and have been trying to understand what's wrong for 13 hours with no fix. I wonder if you managed to figure out what was wrong? It seems to me that it's a bug with TF.</p>\n<p>I imported some weights from a Caffe network into TF, and for verification I used MATLAB. I also have two OpenCL implementations using FFT convolution and standard convolution. All results excluding TF are consistent, and no matter how much I play around with the padding in TF, it just won't produce the correct results. Plotting the outputs show significant differences, especially in the second layer.  In my case, these errors accumulate and grow through the second convolutional layer and reduces the network's accuracy from 97% to 60%. Of course, it's possible to train the network again in TF to get weights that work, but I don't understand why this bug is not fixed considering how bad it is.</p>", "body_text": "Sorry, I know you closed this, but I've ran into this problem myself today and have been trying to understand what's wrong for 13 hours with no fix. I wonder if you managed to figure out what was wrong? It seems to me that it's a bug with TF.\nI imported some weights from a Caffe network into TF, and for verification I used MATLAB. I also have two OpenCL implementations using FFT convolution and standard convolution. All results excluding TF are consistent, and no matter how much I play around with the padding in TF, it just won't produce the correct results. Plotting the outputs show significant differences, especially in the second layer.  In my case, these errors accumulate and grow through the second convolutional layer and reduces the network's accuracy from 97% to 60%. Of course, it's possible to train the network again in TF to get weights that work, but I don't understand why this bug is not fixed considering how bad it is.", "body": "Sorry, I know you closed this, but I've ran into this problem myself today and have been trying to understand what's wrong for 13 hours with no fix. I wonder if you managed to figure out what was wrong? It seems to me that it's a bug with TF.\r\n\r\nI imported some weights from a Caffe network into TF, and for verification I used MATLAB. I also have two OpenCL implementations using FFT convolution and standard convolution. All results excluding TF are consistent, and no matter how much I play around with the padding in TF, it just won't produce the correct results. Plotting the outputs show significant differences, especially in the second layer.  In my case, these errors accumulate and grow through the second convolutional layer and reduces the network's accuracy from 97% to 60%. Of course, it's possible to train the network again in TF to get weights that work, but I don't understand why this bug is not fixed considering how bad it is. "}