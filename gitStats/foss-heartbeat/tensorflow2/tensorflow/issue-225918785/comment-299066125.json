{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299066125", "html_url": "https://github.com/tensorflow/tensorflow/pull/9622#issuecomment-299066125", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9622", "id": 299066125, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTA2NjEyNQ==", "user": {"login": "qjivy", "id": 24410810, "node_id": "MDQ6VXNlcjI0NDEwODEw", "avatar_url": "https://avatars2.githubusercontent.com/u/24410810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qjivy", "html_url": "https://github.com/qjivy", "followers_url": "https://api.github.com/users/qjivy/followers", "following_url": "https://api.github.com/users/qjivy/following{/other_user}", "gists_url": "https://api.github.com/users/qjivy/gists{/gist_id}", "starred_url": "https://api.github.com/users/qjivy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qjivy/subscriptions", "organizations_url": "https://api.github.com/users/qjivy/orgs", "repos_url": "https://api.github.com/users/qjivy/repos", "events_url": "https://api.github.com/users/qjivy/events{/privacy}", "received_events_url": "https://api.github.com/users/qjivy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-03T23:45:33Z", "updated_at": "2017-05-03T23:45:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here they are.<br>\nTest results are showed in the following table. The performance of the inference of the \u201cquantized both node and weights VGG16\u201d on 4 target phones become better.</p>\n<p>Phone              chip-name               test-condition                        before   after    diff-ratio<br>\nSamsung C7\tSnapdragon625\ton core4<del>7 @2.0GHz\t       3.55\t    2.58\t27%<br>\nSPRD-proto1\tSPRD-SOC1\t        on big core4</del>7 @2.0GHz\t7.05\t    3.36\t52%<br>\nLG Nexus5X6P\tSnapdragon808\ton little core0<del><a href=\"mailto:3@1.44Ghz\">3@1.44Ghz</a>    4.63\t    3.41\t26%<br>\nSPRD-proto2\tSPRD-SOC2\ton core0</del>3 @1.4Ghz\t               3.73\t    3.15\t16%</p>\n<p>Platform and test condition description:<br>\nPhone\t        chip name \t       CPU and DDR configuration<br>\nSamsung C7\tSnapdragon625\t8<em>CA53, not big/little, LPDDR3 933Mhz<br>\nSPRD-proto1\tSPRD-SOC1\t        8</em>CA53,big.LITTLE,LPDDR4 667Mhz<br>\nLG Nexus5X6P\tSnapdragon808\t4<em>CA53,2</em>CA57, big.LITTLE, LPDDR3 933Mhz<br>\nSPRD-proto2\tSPRD-SOC2\t        4*CA7, LPDDR3 667Mhz</p>\n<p>To my knowledge, the FC layers of VGG16 has much more weights matrix and much less FLOPS than the convolution layers, that means the computation/memory access ratio is quite smaller.<br>\nFor the CA53 likewise in-ordered core, stalls would happen every time a load instruction is not get data back. For those out-of-order cores like CA57, although the pipeline has some tolerant of long latency load instruction, due to the limited depth of the re-order queue, the core may still stall to wait the load instructions. If only one core are assigned the whole FC task, the bandwidth usage would be quite low because the single core cannot emit enough DDR requests. If we divide the task into more cores, the requests would be more intensive then the total memory bandwidth would be much better than before. Meanwhile, the latency of the single core stall for load would be amortized between cores.</p>", "body_text": "Here they are.\nTest results are showed in the following table. The performance of the inference of the \u201cquantized both node and weights VGG16\u201d on 4 target phones become better.\nPhone              chip-name               test-condition                        before   after    diff-ratio\nSamsung C7\tSnapdragon625\ton core47 @2.0GHz\t       3.55\t    2.58\t27%\nSPRD-proto1\tSPRD-SOC1\t        on big core47 @2.0GHz\t7.05\t    3.36\t52%\nLG Nexus5X6P\tSnapdragon808\ton little core03@1.44Ghz    4.63\t    3.41\t26%\nSPRD-proto2\tSPRD-SOC2\ton core03 @1.4Ghz\t               3.73\t    3.15\t16%\nPlatform and test condition description:\nPhone\t        chip name \t       CPU and DDR configuration\nSamsung C7\tSnapdragon625\t8CA53, not big/little, LPDDR3 933Mhz\nSPRD-proto1\tSPRD-SOC1\t        8CA53,big.LITTLE,LPDDR4 667Mhz\nLG Nexus5X6P\tSnapdragon808\t4CA53,2CA57, big.LITTLE, LPDDR3 933Mhz\nSPRD-proto2\tSPRD-SOC2\t        4*CA7, LPDDR3 667Mhz\nTo my knowledge, the FC layers of VGG16 has much more weights matrix and much less FLOPS than the convolution layers, that means the computation/memory access ratio is quite smaller.\nFor the CA53 likewise in-ordered core, stalls would happen every time a load instruction is not get data back. For those out-of-order cores like CA57, although the pipeline has some tolerant of long latency load instruction, due to the limited depth of the re-order queue, the core may still stall to wait the load instructions. If only one core are assigned the whole FC task, the bandwidth usage would be quite low because the single core cannot emit enough DDR requests. If we divide the task into more cores, the requests would be more intensive then the total memory bandwidth would be much better than before. Meanwhile, the latency of the single core stall for load would be amortized between cores.", "body": "Here they are.\r\nTest results are showed in the following table. The performance of the inference of the \u201cquantized both node and weights VGG16\u201d on 4 target phones become better. \r\n\r\n\r\nPhone              chip-name               test-condition                        before   after    diff-ratio\r\nSamsung C7\tSnapdragon625\ton core4~7 @2.0GHz\t       3.55\t    2.58\t27%\r\nSPRD-proto1\tSPRD-SOC1\t        on big core4~7 @2.0GHz\t7.05\t    3.36\t52%\r\nLG Nexus5X6P\tSnapdragon808\ton little core0~3@1.44Ghz    4.63\t    3.41\t26%\r\nSPRD-proto2\tSPRD-SOC2\ton core0~3 @1.4Ghz\t               3.73\t    3.15\t16%\r\n\r\nPlatform and test condition description:\r\nPhone\t        chip name \t       CPU and DDR configuration\t                   \r\nSamsung C7\tSnapdragon625\t8*CA53, not big/little, LPDDR3 933Mhz\t\r\nSPRD-proto1\tSPRD-SOC1\t        8*CA53,big.LITTLE,LPDDR4 667Mhz\t\r\nLG Nexus5X6P\tSnapdragon808\t4*CA53,2*CA57, big.LITTLE, LPDDR3 933Mhz\t\r\nSPRD-proto2\tSPRD-SOC2\t        4*CA7, LPDDR3 667Mhz\t\r\n\r\nTo my knowledge, the FC layers of VGG16 has much more weights matrix and much less FLOPS than the convolution layers, that means the computation/memory access ratio is quite smaller. \r\nFor the CA53 likewise in-ordered core, stalls would happen every time a load instruction is not get data back. For those out-of-order cores like CA57, although the pipeline has some tolerant of long latency load instruction, due to the limited depth of the re-order queue, the core may still stall to wait the load instructions. If only one core are assigned the whole FC task, the bandwidth usage would be quite low because the single core cannot emit enough DDR requests. If we divide the task into more cores, the requests would be more intensive then the total memory bandwidth would be much better than before. Meanwhile, the latency of the single core stall for load would be amortized between cores.\r\n\r\n\r\n\r\n"}