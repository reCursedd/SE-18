{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300996721", "html_url": "https://github.com/tensorflow/tensorflow/pull/9622#issuecomment-300996721", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9622", "id": 300996721, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDk5NjcyMQ==", "user": {"login": "qjivy", "id": 24410810, "node_id": "MDQ6VXNlcjI0NDEwODEw", "avatar_url": "https://avatars2.githubusercontent.com/u/24410810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qjivy", "html_url": "https://github.com/qjivy", "followers_url": "https://api.github.com/users/qjivy/followers", "following_url": "https://api.github.com/users/qjivy/following{/other_user}", "gists_url": "https://api.github.com/users/qjivy/gists{/gist_id}", "starred_url": "https://api.github.com/users/qjivy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qjivy/subscriptions", "organizations_url": "https://api.github.com/users/qjivy/orgs", "repos_url": "https://api.github.com/users/qjivy/repos", "events_url": "https://api.github.com/users/qjivy/events{/privacy}", "received_events_url": "https://api.github.com/users/qjivy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-12T06:41:12Z", "updated_at": "2017-05-12T09:56:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Maciekcc\uff0c<br>\nThanks for your reply\uff01<br>\nVGG16 has 3 vector \u00d7matrix layers\uff0c they are FC6/FC7/FC8 layers.<br>\nFC6\uff1a LHS\uff1a 1\u00d725088 RHS\uff1a25088\u00d74096<br>\nFC7\uff1a LHS: 1x4096 RHS:4096x4096<br>\nFC8: LHS: 1x4096 RHS: 4096x1000</p>\n<p>here is my test result on Nexus5X6P, 4 little core of Cortex-A53 @1.44Ghz, for both single thread and 4 thread multi-thread for m always 1, n from 512 to 32768, k from 1024 to 16384. The result shows that parallelization always brings good to these shapes.<br>\n<a href=\"http://paste.ubuntu.com/24559715/\" rel=\"nofollow\">http://paste.ubuntu.com/24559715/</a></p>", "body_text": "Hi Maciekcc\uff0c\nThanks for your reply\uff01\nVGG16 has 3 vector \u00d7matrix layers\uff0c they are FC6/FC7/FC8 layers.\nFC6\uff1a LHS\uff1a 1\u00d725088 RHS\uff1a25088\u00d74096\nFC7\uff1a LHS: 1x4096 RHS:4096x4096\nFC8: LHS: 1x4096 RHS: 4096x1000\nhere is my test result on Nexus5X6P, 4 little core of Cortex-A53 @1.44Ghz, for both single thread and 4 thread multi-thread for m always 1, n from 512 to 32768, k from 1024 to 16384. The result shows that parallelization always brings good to these shapes.\nhttp://paste.ubuntu.com/24559715/", "body": "Hi Maciekcc\uff0c\r\nThanks for your reply\uff01\r\nVGG16 has 3 vector \u00d7matrix layers\uff0c they are FC6/FC7/FC8 layers.\r\nFC6\uff1a LHS\uff1a 1\u00d725088 RHS\uff1a25088\u00d74096\r\nFC7\uff1a LHS: 1x4096 RHS:4096x4096\r\nFC8: LHS: 1x4096 RHS: 4096x1000\r\n\r\nhere is my test result on Nexus5X6P, 4 little core of Cortex-A53 @1.44Ghz, for both single thread and 4 thread multi-thread for m always 1, n from 512 to 32768, k from 1024 to 16384. The result shows that parallelization always brings good to these shapes.\r\nhttp://paste.ubuntu.com/24559715/\r\n "}