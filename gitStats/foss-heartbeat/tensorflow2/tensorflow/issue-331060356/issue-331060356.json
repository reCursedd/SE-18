{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19899", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19899/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19899/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19899/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19899", "id": 331060356, "node_id": "MDU6SXNzdWUzMzEwNjAzNTY=", "number": 19899, "title": "Documentation feature request: Explain Nesterov Accelerated Gradient implementation", "user": {"login": "armanschwarz", "id": 8016089, "node_id": "MDQ6VXNlcjgwMTYwODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/8016089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/armanschwarz", "html_url": "https://github.com/armanschwarz", "followers_url": "https://api.github.com/users/armanschwarz/followers", "following_url": "https://api.github.com/users/armanschwarz/following{/other_user}", "gists_url": "https://api.github.com/users/armanschwarz/gists{/gist_id}", "starred_url": "https://api.github.com/users/armanschwarz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/armanschwarz/subscriptions", "organizations_url": "https://api.github.com/users/armanschwarz/orgs", "repos_url": "https://api.github.com/users/armanschwarz/repos", "events_url": "https://api.github.com/users/armanschwarz/events{/privacy}", "received_events_url": "https://api.github.com/users/armanschwarz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-11T06:31:02Z", "updated_at": "2018-10-11T23:42:05Z", "closed_at": "2018-10-11T23:42:05Z", "author_association": "NONE", "body_html": "<p>I haven't filled out the provided form because this is a feature request for the documentation.</p>\n<p>The documentation for <a href=\"https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer\" rel=\"nofollow\"><code>tf.train.MomentumOptimizer</code></a> offers a <code>use_nesterov</code> parameter on which the documentation says the following:</p>\n<blockquote>\n<p><code>use_nesterov</code>: If True use Nesterov Momentum. See <a href=\"http://proceedings.mlr.press/v28/sutskever13.pdf\" rel=\"nofollow\">Sutskever et al.,<br>\n2013</a>. This<br>\nimplementation always computes gradients at the value of the<br>\nvariable(s) passed to the optimizer. Using Nesterov Momentum makes the<br>\nvariable(s) track the values called <code>theta_t + mu*v_t</code> in the paper.</p>\n</blockquote>\n<p>The problem is that the linked paper outlines the normal NAG algorithm, which requires computation of a gradient at a different value to those provided (namely at the next step). This is not clarified in the documentation and led me to some confusion. I ended up <a href=\"https://stackoverflow.com/questions/50774683/how-is-nesterovs-accelerated-gradient-descent-implemented-in-tensorflow\" rel=\"nofollow\">asking this question on StackOverflow</a> and cobbled together <a href=\"https://stackoverflow.com/a/50774886/1613983\" rel=\"nofollow\">an answer myself</a>, however I think the answer by <code>user1735003</code> is so complete that without too much wrangling it could greatly enhance the documentation:</p>\n<p><a href=\"https://stackoverflow.com/a/50778921/1613983\" rel=\"nofollow\">Answer by <code>user1735003</code></a></p>\n<p>Some clarification to the fact that tensorflow actually implements a modified version of the algorithm which is only correct under certain conditions would have been very helpful and would have saved me some time.</p>", "body_text": "I haven't filled out the provided form because this is a feature request for the documentation.\nThe documentation for tf.train.MomentumOptimizer offers a use_nesterov parameter on which the documentation says the following:\n\nuse_nesterov: If True use Nesterov Momentum. See Sutskever et al.,\n2013. This\nimplementation always computes gradients at the value of the\nvariable(s) passed to the optimizer. Using Nesterov Momentum makes the\nvariable(s) track the values called theta_t + mu*v_t in the paper.\n\nThe problem is that the linked paper outlines the normal NAG algorithm, which requires computation of a gradient at a different value to those provided (namely at the next step). This is not clarified in the documentation and led me to some confusion. I ended up asking this question on StackOverflow and cobbled together an answer myself, however I think the answer by user1735003 is so complete that without too much wrangling it could greatly enhance the documentation:\nAnswer by user1735003\nSome clarification to the fact that tensorflow actually implements a modified version of the algorithm which is only correct under certain conditions would have been very helpful and would have saved me some time.", "body": "I haven't filled out the provided form because this is a feature request for the documentation.\r\n\r\nThe documentation for [`tf.train.MomentumOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer) offers a `use_nesterov` parameter on which the documentation says the following:\r\n\r\n> `use_nesterov`: If True use Nesterov Momentum. See [Sutskever et al.,\r\n> 2013](http://proceedings.mlr.press/v28/sutskever13.pdf). This\r\n> implementation always computes gradients at the value of the\r\n> variable(s) passed to the optimizer. Using Nesterov Momentum makes the\r\n> variable(s) track the values called `theta_t + mu*v_t` in the paper.\r\n\r\nThe problem is that the linked paper outlines the normal NAG algorithm, which requires computation of a gradient at a different value to those provided (namely at the next step). This is not clarified in the documentation and led me to some confusion. I ended up [asking this question on StackOverflow](https://stackoverflow.com/questions/50774683/how-is-nesterovs-accelerated-gradient-descent-implemented-in-tensorflow) and cobbled together [an answer myself](https://stackoverflow.com/a/50774886/1613983), however I think the answer by `user1735003` is so complete that without too much wrangling it could greatly enhance the documentation:\r\n\r\n[Answer by `user1735003`](https://stackoverflow.com/a/50778921/1613983)\r\n\r\nSome clarification to the fact that tensorflow actually implements a modified version of the algorithm which is only correct under certain conditions would have been very helpful and would have saved me some time."}