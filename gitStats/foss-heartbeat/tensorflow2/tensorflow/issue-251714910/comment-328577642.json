{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328577642", "html_url": "https://github.com/tensorflow/tensorflow/issues/12453#issuecomment-328577642", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12453", "id": 328577642, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODU3NzY0Mg==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T16:06:53Z", "updated_at": "2017-09-11T16:06:53Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">Add a separate optimizer to contrib.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Mon, Sep 11, 2017 at 2:22 AM, Ilya Edrenkin ***@***.***&gt; wrote:\n I think special-casing the optimizers is the right answer here. This code\n you wrote can be moved into C++ kernels (added initially in contrib) if you\n see a performance regression.\n TensorFlow distinguishes between not differentiable and differentiable but\n 0 gradient. Trying to further refine this distinction feels like a losing\n battle to me with a static computation graph.\n\n Dear <a class=\"user-mention\" href=\"https://github.com/alextp\">@alextp</a> &lt;<a href=\"https://github.com/alextp\">https://github.com/alextp</a>&gt;: I agree that adding special\n values for nonexisting gradients would cause a mess. Is my understanding\n correct that you are fine with using all-zero-gradient-tensor as a special\n case signifying that this gradient tensor should be ignored?\n If yes, I can add this behaviour to tensorflow. What would be the best way\n to do it?\n\n    1. Add a separate optimizer to contrib?\n    2. Add it as an optional behaviour to the existing optimizers?\n    3. Add it as the default behaviour to the existing optimizers?\n\n Thank you!\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"251714910\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12453\" href=\"https://github.com/tensorflow/tensorflow/issues/12453#issuecomment-328470835\">#12453 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxctbl7V4Nv7rbhMWsypOKa9heREYks5shPvDgaJpZM4O9lnu\">https://github.com/notifications/unsubscribe-auth/AAATxctbl7V4Nv7rbhMWsypOKa9heREYks5shPvDgaJpZM4O9lnu</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "Add a separate optimizer to contrib.\n\u2026\nOn Mon, Sep 11, 2017 at 2:22 AM, Ilya Edrenkin ***@***.***> wrote:\n I think special-casing the optimizers is the right answer here. This code\n you wrote can be moved into C++ kernels (added initially in contrib) if you\n see a performance regression.\n TensorFlow distinguishes between not differentiable and differentiable but\n 0 gradient. Trying to further refine this distinction feels like a losing\n battle to me with a static computation graph.\n\n Dear @alextp <https://github.com/alextp>: I agree that adding special\n values for nonexisting gradients would cause a mess. Is my understanding\n correct that you are fine with using all-zero-gradient-tensor as a special\n case signifying that this gradient tensor should be ignored?\n If yes, I can add this behaviour to tensorflow. What would be the best way\n to do it?\n\n    1. Add a separate optimizer to contrib?\n    2. Add it as an optional behaviour to the existing optimizers?\n    3. Add it as the default behaviour to the existing optimizers?\n\n Thank you!\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#12453 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxctbl7V4Nv7rbhMWsypOKa9heREYks5shPvDgaJpZM4O9lnu>\n .\n\n\n-- \n - Alex", "body": "Add a separate optimizer to contrib.\n\nOn Mon, Sep 11, 2017 at 2:22 AM, Ilya Edrenkin <notifications@github.com>\nwrote:\n\n> I think special-casing the optimizers is the right answer here. This code\n> you wrote can be moved into C++ kernels (added initially in contrib) if you\n> see a performance regression.\n> TensorFlow distinguishes between not differentiable and differentiable but\n> 0 gradient. Trying to further refine this distinction feels like a losing\n> battle to me with a static computation graph.\n>\n> Dear @alextp <https://github.com/alextp>: I agree that adding special\n> values for nonexisting gradients would cause a mess. Is my understanding\n> correct that you are fine with using all-zero-gradient-tensor as a special\n> case signifying that this gradient tensor should be ignored?\n> If yes, I can add this behaviour to tensorflow. What would be the best way\n> to do it?\n>\n>    1. Add a separate optimizer to contrib?\n>    2. Add it as an optional behaviour to the existing optimizers?\n>    3. Add it as the default behaviour to the existing optimizers?\n>\n> Thank you!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12453#issuecomment-328470835>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxctbl7V4Nv7rbhMWsypOKa9heREYks5shPvDgaJpZM4O9lnu>\n> .\n>\n\n\n\n-- \n - Alex\n"}