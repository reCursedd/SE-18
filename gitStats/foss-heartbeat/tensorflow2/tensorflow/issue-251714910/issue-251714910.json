{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12453", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12453/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12453/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12453/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12453", "id": 251714910, "node_id": "MDU6SXNzdWUyNTE3MTQ5MTA=", "number": 12453, "title": "Adaptive optmizers do not work well with multi-head networks when some labels are missing", "user": {"login": "ilya-edrenkin", "id": 13562803, "node_id": "MDQ6VXNlcjEzNTYyODAz", "avatar_url": "https://avatars2.githubusercontent.com/u/13562803?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilya-edrenkin", "html_url": "https://github.com/ilya-edrenkin", "followers_url": "https://api.github.com/users/ilya-edrenkin/followers", "following_url": "https://api.github.com/users/ilya-edrenkin/following{/other_user}", "gists_url": "https://api.github.com/users/ilya-edrenkin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilya-edrenkin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilya-edrenkin/subscriptions", "organizations_url": "https://api.github.com/users/ilya-edrenkin/orgs", "repos_url": "https://api.github.com/users/ilya-edrenkin/repos", "events_url": "https://api.github.com/users/ilya-edrenkin/events{/privacy}", "received_events_url": "https://api.github.com/users/ilya-edrenkin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 19, "created_at": "2017-08-21T16:46:57Z", "updated_at": "2017-11-09T22:42:16Z", "closed_at": "2017-11-09T22:42:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Use case: we have a multi-task network with many outputs. Each example in the dataset has only subset of the labels; non-existing loss components are masked out. A single optimizer is used, because the presence of particular labels in the example stream is statically unknown.</p>\n<p>Problem: masked loss components give zero gradient estimates for the corresponding variables. That breaks adaptive optimizers (Momentum, Adam, ...) because steps are taken and slots are corrupted with the incoming zero gradient estimates. The desired behaviour is: do nothing for variables (and corresponding slots) that were effectively unused in the forward pass (like it is done for the embeddings using the IndexedSlices trick).<br>\nRelated problem: tf.global_norm, tf.clip_by_global_norm are also affected by these stray zero gradient estimates.</p>\n<p>Minimal example/demonstration:</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">14</span>]: feature <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1.0</span>])\nIn [<span class=\"pl-c1\">15</span>]: output <span class=\"pl-k\">=</span> feature <span class=\"pl-k\">*</span> <span class=\"pl-c1\">42.0</span>\nIn [<span class=\"pl-c1\">16</span>]: loss <span class=\"pl-k\">=</span> tf.squared_difference(output, [<span class=\"pl-c1\">0.0</span>])\nIn [<span class=\"pl-c1\">17</span>]: mask <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">False</span>])\nIn [<span class=\"pl-c1\">18</span>]: masked_loss <span class=\"pl-k\">=</span> tf.boolean_mask(loss, mask)\nIn [<span class=\"pl-c1\">19</span>]: masked_loss_grad <span class=\"pl-k\">=</span> tf.gradients(masked_loss, feature)\nIn [<span class=\"pl-c1\">20</span>]: masked_loss_grad\nOut[<span class=\"pl-c1\">20</span>]: [<span class=\"pl-k\">&lt;</span>tf.Tensor <span class=\"pl-s\"><span class=\"pl-pds\">'</span>gradients/mul_3_grad/Reshape:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">1</span>,) dtype=float32<span class=\"pl-k\">&gt;</span>]\nIn [<span class=\"pl-c1\">21</span>]: masked_loss_grad[<span class=\"pl-c1\">0</span>].eval(<span class=\"pl-v\">session</span><span class=\"pl-k\">=</span>session)\nOut[<span class=\"pl-c1\">21</span>]: array([ <span class=\"pl-c1\">0</span>.], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>float32)</pre></div>\n<p>This is technically correct -- the gradient is indeed zero -- but if <code>feature</code> was a Variable, we don't really want to apply this gradient in an adaptive optimizer, because the training example didn't have useful information about the loss function.</p>\n<p>Feature request: better way to deal with that (probably a special value for the gradients that would signify \"nonexisting\" gradients; e.g. tensorflow uses <code>None</code>, when this information is available statically).</p>\n<p>For now here is a simple and ugly workaround:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_is_all_zeros</span>(<span class=\"pl-smi\">grad</span>):\n    all_zeros <span class=\"pl-k\">=</span> tf.equal(tf.count_nonzero(grad), <span class=\"pl-c1\">0</span>)\n    <span class=\"pl-k\">return</span> all_zeros\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">HackedMomentumOptimizer</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">train</span>.<span class=\"pl-e\">MomentumOptimizer</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n        <span class=\"pl-c1\">super</span>(HackedMomentumOptimizer, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(<span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_apply_dense</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad</span>, <span class=\"pl-smi\">var</span>):\n        all_zeros <span class=\"pl-k\">=</span> _is_all_zeros(grad)\n        <span class=\"pl-k\">return</span> tf.cond(all_zeros,\n                       <span class=\"pl-k\">lambda</span>: tf.no_op(),\n                       <span class=\"pl-k\">lambda</span>: <span class=\"pl-c1\">super</span>(HackedMomentumOptimizer, <span class=\"pl-c1\">self</span>).\n\t\t       _apply_dense(grad, var))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_resource_apply_dense</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad</span>, <span class=\"pl-smi\">var</span>):\n        all_zeros <span class=\"pl-k\">=</span> _is_all_zeros(grad)\n        <span class=\"pl-k\">return</span> tf.cond(all_zeros,\n                       <span class=\"pl-k\">lambda</span>: tf.no_op(),\n                       <span class=\"pl-k\">lambda</span>: <span class=\"pl-c1\">super</span>(HackedMomentumOptimizer, <span class=\"pl-c1\">self</span>).\n                       _resource_apply_dense(grad, var))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_apply_sparse</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad</span>, <span class=\"pl-smi\">var</span>):\n        all_zeros <span class=\"pl-k\">=</span> _is_all_zeros(grad)\n        <span class=\"pl-k\">return</span> tf.cond(all_zeros,\n                       <span class=\"pl-k\">lambda</span>: tf.no_op(),\n                       <span class=\"pl-k\">lambda</span>: <span class=\"pl-c1\">super</span>(HackedMomentumOptimizer, <span class=\"pl-c1\">self</span>).\n                       _apply_sparse(grad, var))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_resource_apply_sparse</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad</span>, <span class=\"pl-smi\">var</span>, <span class=\"pl-smi\">indices</span>):\n        all_zeros <span class=\"pl-k\">=</span> _is_all_zeros(grad)\n        <span class=\"pl-k\">return</span> tf.cond(all_zeros,\n                       <span class=\"pl-k\">lambda</span>: tf.no_op(),\n                       <span class=\"pl-k\">lambda</span>: <span class=\"pl-c1\">super</span>(HackedMomentumOptimizer, <span class=\"pl-c1\">self</span>).\n                       _resource_apply_sparse(grad, var, indices))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_clip_gradients</span>(<span class=\"pl-smi\">gradients_variables</span>, <span class=\"pl-smi\">clip_norm</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>.):\n    gradients, variables <span class=\"pl-k\">=</span> six.moves.zip(<span class=\"pl-k\">*</span>gradients_variables)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_replace_nonexisting_grad</span>(<span class=\"pl-smi\">grad</span>):\n        all_zeros <span class=\"pl-k\">=</span> _is_all_zeros(grad)\n        <span class=\"pl-k\">return</span> tf.cond(all_zeros,\n                       <span class=\"pl-k\">lambda</span>: tf.zeros([], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32),\n                       <span class=\"pl-k\">lambda</span>: grad)\n    gradients_filtered <span class=\"pl-k\">=</span> [_replace_nonexisting_grad(g) <span class=\"pl-k\">for</span> g <span class=\"pl-k\">in</span> gradients]\n    fixed_global_norm <span class=\"pl-k\">=</span> tf.global_norm(gradients_filtered)\n    gradients, global_norm <span class=\"pl-k\">=</span> tf.clip_by_global_norm(gradients, clip_norm,\n\t                                            <span class=\"pl-v\">use_norm</span><span class=\"pl-k\">=</span>fixed_global_norm)\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_norm<span class=\"pl-pds\">'</span></span>, global_norm)\n    <span class=\"pl-k\">return</span> six.moves.zip(gradients, variables)</pre></div>", "body_text": "Use case: we have a multi-task network with many outputs. Each example in the dataset has only subset of the labels; non-existing loss components are masked out. A single optimizer is used, because the presence of particular labels in the example stream is statically unknown.\nProblem: masked loss components give zero gradient estimates for the corresponding variables. That breaks adaptive optimizers (Momentum, Adam, ...) because steps are taken and slots are corrupted with the incoming zero gradient estimates. The desired behaviour is: do nothing for variables (and corresponding slots) that were effectively unused in the forward pass (like it is done for the embeddings using the IndexedSlices trick).\nRelated problem: tf.global_norm, tf.clip_by_global_norm are also affected by these stray zero gradient estimates.\nMinimal example/demonstration:\nIn [14]: feature = tf.constant([1.0])\nIn [15]: output = feature * 42.0\nIn [16]: loss = tf.squared_difference(output, [0.0])\nIn [17]: mask = tf.constant([False])\nIn [18]: masked_loss = tf.boolean_mask(loss, mask)\nIn [19]: masked_loss_grad = tf.gradients(masked_loss, feature)\nIn [20]: masked_loss_grad\nOut[20]: [<tf.Tensor 'gradients/mul_3_grad/Reshape:0' shape=(1,) dtype=float32>]\nIn [21]: masked_loss_grad[0].eval(session=session)\nOut[21]: array([ 0.], dtype=float32)\nThis is technically correct -- the gradient is indeed zero -- but if feature was a Variable, we don't really want to apply this gradient in an adaptive optimizer, because the training example didn't have useful information about the loss function.\nFeature request: better way to deal with that (probably a special value for the gradients that would signify \"nonexisting\" gradients; e.g. tensorflow uses None, when this information is available statically).\nFor now here is a simple and ugly workaround:\ndef _is_all_zeros(grad):\n    all_zeros = tf.equal(tf.count_nonzero(grad), 0)\n    return all_zeros\n\nclass HackedMomentumOptimizer(tf.train.MomentumOptimizer):\n    def __init__(self, *args, **kwargs):\n        super(HackedMomentumOptimizer, self).__init__(*args, **kwargs)\n\n    def _apply_dense(self, grad, var):\n        all_zeros = _is_all_zeros(grad)\n        return tf.cond(all_zeros,\n                       lambda: tf.no_op(),\n                       lambda: super(HackedMomentumOptimizer, self).\n\t\t       _apply_dense(grad, var))\n\n    def _resource_apply_dense(self, grad, var):\n        all_zeros = _is_all_zeros(grad)\n        return tf.cond(all_zeros,\n                       lambda: tf.no_op(),\n                       lambda: super(HackedMomentumOptimizer, self).\n                       _resource_apply_dense(grad, var))\n\n    def _apply_sparse(self, grad, var):\n        all_zeros = _is_all_zeros(grad)\n        return tf.cond(all_zeros,\n                       lambda: tf.no_op(),\n                       lambda: super(HackedMomentumOptimizer, self).\n                       _apply_sparse(grad, var))\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        all_zeros = _is_all_zeros(grad)\n        return tf.cond(all_zeros,\n                       lambda: tf.no_op(),\n                       lambda: super(HackedMomentumOptimizer, self).\n                       _resource_apply_sparse(grad, var, indices))\n\n\ndef _clip_gradients(gradients_variables, clip_norm=20.):\n    gradients, variables = six.moves.zip(*gradients_variables)\n\n    def _replace_nonexisting_grad(grad):\n        all_zeros = _is_all_zeros(grad)\n        return tf.cond(all_zeros,\n                       lambda: tf.zeros([], dtype=tf.float32),\n                       lambda: grad)\n    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]\n    fixed_global_norm = tf.global_norm(gradients_filtered)\n    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm,\n\t                                            use_norm=fixed_global_norm)\n    tf.summary.scalar('global_norm', global_norm)\n    return six.moves.zip(gradients, variables)", "body": "Use case: we have a multi-task network with many outputs. Each example in the dataset has only subset of the labels; non-existing loss components are masked out. A single optimizer is used, because the presence of particular labels in the example stream is statically unknown.\r\n\r\nProblem: masked loss components give zero gradient estimates for the corresponding variables. That breaks adaptive optimizers (Momentum, Adam, ...) because steps are taken and slots are corrupted with the incoming zero gradient estimates. The desired behaviour is: do nothing for variables (and corresponding slots) that were effectively unused in the forward pass (like it is done for the embeddings using the IndexedSlices trick).\r\nRelated problem: tf.global_norm, tf.clip_by_global_norm are also affected by these stray zero gradient estimates.\r\n\r\nMinimal example/demonstration:\r\n```python\r\nIn [14]: feature = tf.constant([1.0])\r\nIn [15]: output = feature * 42.0\r\nIn [16]: loss = tf.squared_difference(output, [0.0])\r\nIn [17]: mask = tf.constant([False])\r\nIn [18]: masked_loss = tf.boolean_mask(loss, mask)\r\nIn [19]: masked_loss_grad = tf.gradients(masked_loss, feature)\r\nIn [20]: masked_loss_grad\r\nOut[20]: [<tf.Tensor 'gradients/mul_3_grad/Reshape:0' shape=(1,) dtype=float32>]\r\nIn [21]: masked_loss_grad[0].eval(session=session)\r\nOut[21]: array([ 0.], dtype=float32)\r\n```\r\nThis is technically correct -- the gradient is indeed zero -- but if `feature` was a Variable, we don't really want to apply this gradient in an adaptive optimizer, because the training example didn't have useful information about the loss function.\r\n\r\nFeature request: better way to deal with that (probably a special value for the gradients that would signify \"nonexisting\" gradients; e.g. tensorflow uses `None`, when this information is available statically).\r\n\r\nFor now here is a simple and ugly workaround:\r\n```python\r\ndef _is_all_zeros(grad):\r\n    all_zeros = tf.equal(tf.count_nonzero(grad), 0)\r\n    return all_zeros\r\n\r\nclass HackedMomentumOptimizer(tf.train.MomentumOptimizer):\r\n    def __init__(self, *args, **kwargs):\r\n        super(HackedMomentumOptimizer, self).__init__(*args, **kwargs)\r\n\r\n    def _apply_dense(self, grad, var):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n\t\t       _apply_dense(grad, var))\r\n\r\n    def _resource_apply_dense(self, grad, var):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n                       _resource_apply_dense(grad, var))\r\n\r\n    def _apply_sparse(self, grad, var):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n                       _apply_sparse(grad, var))\r\n\r\n    def _resource_apply_sparse(self, grad, var, indices):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n                       _resource_apply_sparse(grad, var, indices))\r\n\r\n\r\ndef _clip_gradients(gradients_variables, clip_norm=20.):\r\n    gradients, variables = six.moves.zip(*gradients_variables)\r\n\r\n    def _replace_nonexisting_grad(grad):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.zeros([], dtype=tf.float32),\r\n                       lambda: grad)\r\n    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]\r\n    fixed_global_norm = tf.global_norm(gradients_filtered)\r\n    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm,\r\n\t                                            use_norm=fixed_global_norm)\r\n    tf.summary.scalar('global_norm', global_norm)\r\n    return six.moves.zip(gradients, variables)\r\n```"}