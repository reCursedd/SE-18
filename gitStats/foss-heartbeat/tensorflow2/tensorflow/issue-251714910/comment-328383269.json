{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328383269", "html_url": "https://github.com/tensorflow/tensorflow/issues/12453#issuecomment-328383269", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12453", "id": 328383269, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODM4MzI2OQ==", "user": {"login": "MtDersvan", "id": 7069222, "node_id": "MDQ6VXNlcjcwNjkyMjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7069222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MtDersvan", "html_url": "https://github.com/MtDersvan", "followers_url": "https://api.github.com/users/MtDersvan/followers", "following_url": "https://api.github.com/users/MtDersvan/following{/other_user}", "gists_url": "https://api.github.com/users/MtDersvan/gists{/gist_id}", "starred_url": "https://api.github.com/users/MtDersvan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MtDersvan/subscriptions", "organizations_url": "https://api.github.com/users/MtDersvan/orgs", "repos_url": "https://api.github.com/users/MtDersvan/repos", "events_url": "https://api.github.com/users/MtDersvan/events{/privacy}", "received_events_url": "https://api.github.com/users/MtDersvan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-10T23:56:01Z", "updated_at": "2017-09-10T23:56:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Same situation here: a multi-task NN with 5-6 outputs (tasks), where each task has \u2248 80% subset of overall labels of the same dataset.</p>\n<p>What is the best current solution for this problem in TensorFlow?<br>\nWill directly using upper-mentioned <code>HackedMomentumOptimizer</code> for a combined loss work well?<br>\nDo we need to weigh/mask the losses?<br>\nHave you resolved this problem <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13562803\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ilya-edrenkin\">@ilya-edrenkin</a>?</p>\n<p>Currently in production, I use alternate multi-task learning where I have separate Optimizers for each loss and filter tf.contrib.Datasets accordingly available labels.<br>\nThis has it's downsides:</p>\n<ul>\n<li>Optimizers are not being able to effectively communicate (e.g. share internal states - velocity, etc), so they do not know about each other.</li>\n<li>This is wasting a lot of computation as single forwards/backward passes could use information from multiple losses/tasks instead of only one.</li>\n</ul>", "body_text": "Same situation here: a multi-task NN with 5-6 outputs (tasks), where each task has \u2248 80% subset of overall labels of the same dataset.\nWhat is the best current solution for this problem in TensorFlow?\nWill directly using upper-mentioned HackedMomentumOptimizer for a combined loss work well?\nDo we need to weigh/mask the losses?\nHave you resolved this problem @ilya-edrenkin?\nCurrently in production, I use alternate multi-task learning where I have separate Optimizers for each loss and filter tf.contrib.Datasets accordingly available labels.\nThis has it's downsides:\n\nOptimizers are not being able to effectively communicate (e.g. share internal states - velocity, etc), so they do not know about each other.\nThis is wasting a lot of computation as single forwards/backward passes could use information from multiple losses/tasks instead of only one.", "body": "Same situation here: a multi-task NN with 5-6 outputs (tasks), where each task has \u2248 80% subset of overall labels of the same dataset.\r\n\r\nWhat is the best current solution for this problem in TensorFlow?\r\nWill directly using upper-mentioned `HackedMomentumOptimizer` for a combined loss work well?\r\nDo we need to weigh/mask the losses? \r\nHave you resolved this problem @ilya-edrenkin?\r\n\r\nCurrently in production, I use alternate multi-task learning where I have separate Optimizers for each loss and filter tf.contrib.Datasets accordingly available labels.\r\nThis has it's downsides: \r\n- Optimizers are not being able to effectively communicate (e.g. share internal states - velocity, etc), so they do not know about each other.\r\n- This is wasting a lot of computation as single forwards/backward passes could use information from multiple losses/tasks instead of only one."}