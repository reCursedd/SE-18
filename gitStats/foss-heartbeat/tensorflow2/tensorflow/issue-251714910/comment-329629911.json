{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/329629911", "html_url": "https://github.com/tensorflow/tensorflow/issues/12453#issuecomment-329629911", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12453", "id": 329629911, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTYyOTkxMQ==", "user": {"login": "Threynaud", "id": 13488732, "node_id": "MDQ6VXNlcjEzNDg4NzMy", "avatar_url": "https://avatars1.githubusercontent.com/u/13488732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Threynaud", "html_url": "https://github.com/Threynaud", "followers_url": "https://api.github.com/users/Threynaud/followers", "following_url": "https://api.github.com/users/Threynaud/following{/other_user}", "gists_url": "https://api.github.com/users/Threynaud/gists{/gist_id}", "starred_url": "https://api.github.com/users/Threynaud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Threynaud/subscriptions", "organizations_url": "https://api.github.com/users/Threynaud/orgs", "repos_url": "https://api.github.com/users/Threynaud/repos", "events_url": "https://api.github.com/users/Threynaud/events{/privacy}", "received_events_url": "https://api.github.com/users/Threynaud/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-14T22:50:03Z", "updated_at": "2017-09-15T09:19:15Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13562803\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ilya-edrenkin\">@ilya-edrenkin</a>  I am so sorry but I think I am still missing something... The code works like a charm but I still get the erratic behavior related to the missing labels for one of the tasks! I think the problem is due to how I combine my 3 losses!</p>\n<p>What I am doing now is simply summing the 3 losses of the 3 heads of my network and then applying the optimizer one the resulting loss but I realized that it might not be the good way to do it because the HackedOptimizer can't detect when an example has missing labels for one of the task..<br>\nI guess this is where gradient clipping comes handy but I really have no idea how to use this. Also, does this work with bash training?</p>\n<p>This is basically what I'm doing for now:</p>\n<pre><code># Loss definition\ntarget_skills = tf.cast(labels['skills'], tf.float32)\nloss_skills = tf.reduce_mean(\n    tf.losses.sigmoid_cross_entropy(\n        target_skills, logits_skills))\n\ntarget_fos = tf.cast(labels['fos'], tf.float32)\nloss_fos = tf.reduce_mean(\n    tf.losses.sigmoid_cross_entropy(\n        target_fos, logits_fos))\n\ntarget_d1 = tf.cast(labels['domain_lvl1'], tf.float32)\nloss_d1 = tf.reduce_mean(\n    tf.losses.softmax_cross_entropy(\n        target_d1, logits_d1))\n\nloss = loss_d1 + loss_skills + loss_fos\n\ntrain_op = HackedMomentumOptimizer().minimize(\n                loss, global_step=global_step)\n</code></pre>\n<p>I'm sorry for all those questions, I'm quite new with pure tf... :/</p>\n<p>EDIT:<br>\nThis is what I did:</p>\n<pre><code>optimizer = HackedMomentumOptimizer()\ngvs = optimizer.compute_gradients(loss)\ngvs = _clip_gradients(gvs)\n\ntrain_op = optimizer.apply_gradients(\n    gvs, global_step=global_step)\n</code></pre>\n<p>but I get the following <code>AssertionError</code>:</p>\n<pre><code>  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 124, in _model_fn\n    gvs = _clip_gradients(gvs)\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 389, in _clip_gradients\n    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 388, in _replace_nonexisting_grad\n    lambda: grad)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1855, in cond\n    (isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor)))\nAssertionError\n</code></pre>", "body_text": "@ilya-edrenkin  I am so sorry but I think I am still missing something... The code works like a charm but I still get the erratic behavior related to the missing labels for one of the tasks! I think the problem is due to how I combine my 3 losses!\nWhat I am doing now is simply summing the 3 losses of the 3 heads of my network and then applying the optimizer one the resulting loss but I realized that it might not be the good way to do it because the HackedOptimizer can't detect when an example has missing labels for one of the task..\nI guess this is where gradient clipping comes handy but I really have no idea how to use this. Also, does this work with bash training?\nThis is basically what I'm doing for now:\n# Loss definition\ntarget_skills = tf.cast(labels['skills'], tf.float32)\nloss_skills = tf.reduce_mean(\n    tf.losses.sigmoid_cross_entropy(\n        target_skills, logits_skills))\n\ntarget_fos = tf.cast(labels['fos'], tf.float32)\nloss_fos = tf.reduce_mean(\n    tf.losses.sigmoid_cross_entropy(\n        target_fos, logits_fos))\n\ntarget_d1 = tf.cast(labels['domain_lvl1'], tf.float32)\nloss_d1 = tf.reduce_mean(\n    tf.losses.softmax_cross_entropy(\n        target_d1, logits_d1))\n\nloss = loss_d1 + loss_skills + loss_fos\n\ntrain_op = HackedMomentumOptimizer().minimize(\n                loss, global_step=global_step)\n\nI'm sorry for all those questions, I'm quite new with pure tf... :/\nEDIT:\nThis is what I did:\noptimizer = HackedMomentumOptimizer()\ngvs = optimizer.compute_gradients(loss)\ngvs = _clip_gradients(gvs)\n\ntrain_op = optimizer.apply_gradients(\n    gvs, global_step=global_step)\n\nbut I get the following AssertionError:\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 124, in _model_fn\n    gvs = _clip_gradients(gvs)\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 389, in _clip_gradients\n    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 388, in _replace_nonexisting_grad\n    lambda: grad)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1855, in cond\n    (isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor)))\nAssertionError", "body": "@ilya-edrenkin  I am so sorry but I think I am still missing something... The code works like a charm but I still get the erratic behavior related to the missing labels for one of the tasks! I think the problem is due to how I combine my 3 losses!\r\n\r\nWhat I am doing now is simply summing the 3 losses of the 3 heads of my network and then applying the optimizer one the resulting loss but I realized that it might not be the good way to do it because the HackedOptimizer can't detect when an example has missing labels for one of the task..\r\nI guess this is where gradient clipping comes handy but I really have no idea how to use this. Also, does this work with bash training?\r\n\r\nThis is basically what I'm doing for now:\r\n```\r\n# Loss definition\r\ntarget_skills = tf.cast(labels['skills'], tf.float32)\r\nloss_skills = tf.reduce_mean(\r\n    tf.losses.sigmoid_cross_entropy(\r\n        target_skills, logits_skills))\r\n\r\ntarget_fos = tf.cast(labels['fos'], tf.float32)\r\nloss_fos = tf.reduce_mean(\r\n    tf.losses.sigmoid_cross_entropy(\r\n        target_fos, logits_fos))\r\n\r\ntarget_d1 = tf.cast(labels['domain_lvl1'], tf.float32)\r\nloss_d1 = tf.reduce_mean(\r\n    tf.losses.softmax_cross_entropy(\r\n        target_d1, logits_d1))\r\n\r\nloss = loss_d1 + loss_skills + loss_fos\r\n\r\ntrain_op = HackedMomentumOptimizer().minimize(\r\n                loss, global_step=global_step)\r\n```\r\n\r\nI'm sorry for all those questions, I'm quite new with pure tf... :/\r\n\r\nEDIT:\r\nThis is what I did:\r\n```\r\noptimizer = HackedMomentumOptimizer()\r\ngvs = optimizer.compute_gradients(loss)\r\ngvs = _clip_gradients(gvs)\r\n\r\ntrain_op = optimizer.apply_gradients(\r\n    gvs, global_step=global_step)\r\n```\r\n\r\nbut I get the following `AssertionError`:\r\n```\r\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 124, in _model_fn\r\n    gvs = _clip_gradients(gvs)\r\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 389, in _clip_gradients\r\n    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]\r\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 388, in _replace_nonexisting_grad\r\n    lambda: grad)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1855, in cond\r\n    (isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor)))\r\nAssertionError\r\n```"}