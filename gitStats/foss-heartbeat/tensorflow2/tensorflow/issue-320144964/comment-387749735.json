{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387749735", "html_url": "https://github.com/tensorflow/tensorflow/issues/19073#issuecomment-387749735", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19073", "id": 387749735, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Nzc0OTczNQ==", "user": {"login": "ghostplant", "id": 12099308, "node_id": "MDQ6VXNlcjEyMDk5MzA4", "avatar_url": "https://avatars2.githubusercontent.com/u/12099308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghostplant", "html_url": "https://github.com/ghostplant", "followers_url": "https://api.github.com/users/ghostplant/followers", "following_url": "https://api.github.com/users/ghostplant/following{/other_user}", "gists_url": "https://api.github.com/users/ghostplant/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghostplant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghostplant/subscriptions", "organizations_url": "https://api.github.com/users/ghostplant/orgs", "repos_url": "https://api.github.com/users/ghostplant/repos", "events_url": "https://api.github.com/users/ghostplant/events{/privacy}", "received_events_url": "https://api.github.com/users/ghostplant/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T14:04:29Z", "updated_at": "2018-05-09T14:13:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a> Yes, if I do not specify --data-dir, the cpu occupation can be reduced, but it is meaningless when training a dataset. If I reduce the inter-thread parallel number, the cpu can be reduced as well but the model benchmark will be very bad.</p>\n<p>By using 32-cpu inter-thread parallel number, I can get 800 image/sec using 4 P100 GPUs (costing 3130% cpus);</p>\n<p>However, if I use 8-cpu inter-thread parallel number, I can only get 370 image/sec using same 4 P100 GPUs (costing 800% cpus which is decent).</p>", "body_text": "@reedwm Yes, if I do not specify --data-dir, the cpu occupation can be reduced, but it is meaningless when training a dataset. If I reduce the inter-thread parallel number, the cpu can be reduced as well but the model benchmark will be very bad.\nBy using 32-cpu inter-thread parallel number, I can get 800 image/sec using 4 P100 GPUs (costing 3130% cpus);\nHowever, if I use 8-cpu inter-thread parallel number, I can only get 370 image/sec using same 4 P100 GPUs (costing 800% cpus which is decent).", "body": "@reedwm Yes, if I do not specify --data-dir, the cpu occupation can be reduced, but it is meaningless when training a dataset. If I reduce the inter-thread parallel number, the cpu can be reduced as well but the model benchmark will be very bad.\r\n\r\nBy using 32-cpu inter-thread parallel number, I can get 800 image/sec using 4 P100 GPUs (costing 3130% cpus);\r\n\r\nHowever, if I use 8-cpu inter-thread parallel number, I can only get 370 image/sec using same 4 P100 GPUs (costing 800% cpus which is decent)."}