{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19421", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19421/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19421/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19421/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19421", "id": 324730823, "node_id": "MDU6SXNzdWUzMjQ3MzA4MjM=", "number": 19421, "title": "Feature Request:  128-bit floats", "user": {"login": "johnpjust", "id": 31108737, "node_id": "MDQ6VXNlcjMxMTA4NzM3", "avatar_url": "https://avatars3.githubusercontent.com/u/31108737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johnpjust", "html_url": "https://github.com/johnpjust", "followers_url": "https://api.github.com/users/johnpjust/followers", "following_url": "https://api.github.com/users/johnpjust/following{/other_user}", "gists_url": "https://api.github.com/users/johnpjust/gists{/gist_id}", "starred_url": "https://api.github.com/users/johnpjust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johnpjust/subscriptions", "organizations_url": "https://api.github.com/users/johnpjust/orgs", "repos_url": "https://api.github.com/users/johnpjust/repos", "events_url": "https://api.github.com/users/johnpjust/events{/privacy}", "received_events_url": "https://api.github.com/users/johnpjust/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-05-20T18:34:40Z", "updated_at": "2018-11-14T19:18:47Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I have posted a question and answer on <a href=\"https://stackoverflow.com/questions/50438071/cost-function-convergence-in-tensorflow-using-softmax-cross-entropy-with-logits/50438072#50438072\" rel=\"nofollow\">SO</a> showing why this is needed.  Admittedly it is probably not a common case, but I do research and the problems I deal with usually require creative solutions and so I'm generally pushing the boundaries of packages like Tensorflow (which is a wonderful package!).  I ran into this with soft-target classification.</p>\n<blockquote>\n<p>with soft targets, especially ones that aren't close to 1 or zero, cross entropy loss doesn't change significantly as the algorithm improves. Let's say the targets are [0.39019628, 0.44301641, 0.16678731]. Well, using the formula for cross entropy</p>\n</blockquote>\n<p><code>cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))</code></p>\n<blockquote>\n<p>but then using the targets \"y_\" in place of the predicted probabilities \"y\" we arrive at the true entropy value of 1.0266190072458234. If you're predictions are just slightly off of target....lets say they are [0.39511779, 0.44509024, 0.15979198], then the cross entropy is 1.026805558049737.</p>\n</blockquote>\n<p>Basically even with 64-bit floats, the loss function shows evidence that it would continue to train if further significant digits were available.  This is just to support the cost function during the training process since the final trained values would not need such precision, but to get to an optimal convergence I need it.</p>", "body_text": "I have posted a question and answer on SO showing why this is needed.  Admittedly it is probably not a common case, but I do research and the problems I deal with usually require creative solutions and so I'm generally pushing the boundaries of packages like Tensorflow (which is a wonderful package!).  I ran into this with soft-target classification.\n\nwith soft targets, especially ones that aren't close to 1 or zero, cross entropy loss doesn't change significantly as the algorithm improves. Let's say the targets are [0.39019628, 0.44301641, 0.16678731]. Well, using the formula for cross entropy\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n\nbut then using the targets \"y_\" in place of the predicted probabilities \"y\" we arrive at the true entropy value of 1.0266190072458234. If you're predictions are just slightly off of target....lets say they are [0.39511779, 0.44509024, 0.15979198], then the cross entropy is 1.026805558049737.\n\nBasically even with 64-bit floats, the loss function shows evidence that it would continue to train if further significant digits were available.  This is just to support the cost function during the training process since the final trained values would not need such precision, but to get to an optimal convergence I need it.", "body": "I have posted a question and answer on [SO](https://stackoverflow.com/questions/50438071/cost-function-convergence-in-tensorflow-using-softmax-cross-entropy-with-logits/50438072#50438072) showing why this is needed.  Admittedly it is probably not a common case, but I do research and the problems I deal with usually require creative solutions and so I'm generally pushing the boundaries of packages like Tensorflow (which is a wonderful package!).  I ran into this with soft-target classification.  \r\n\r\n> with soft targets, especially ones that aren't close to 1 or zero, cross entropy loss doesn't change significantly as the algorithm improves. Let's say the targets are [0.39019628, 0.44301641, 0.16678731]. Well, using the formula for cross entropy\r\n\r\n`cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))`\r\n\r\n> but then using the targets \"y_\" in place of the predicted probabilities \"y\" we arrive at the true entropy value of 1.0266190072458234. If you're predictions are just slightly off of target....lets say they are [0.39511779, 0.44509024, 0.15979198], then the cross entropy is 1.026805558049737.\r\n\r\nBasically even with 64-bit floats, the loss function shows evidence that it would continue to train if further significant digits were available.  This is just to support the cost function during the training process since the final trained values would not need such precision, but to get to an optimal convergence I need it.\r\n\r\n\r\n"}