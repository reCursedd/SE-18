{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/179485231", "html_url": "https://github.com/tensorflow/tensorflow/issues/975#issuecomment-179485231", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/975", "id": 179485231, "node_id": "MDEyOklzc3VlQ29tbWVudDE3OTQ4NTIzMQ==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-03T21:41:59Z", "updated_at": "2016-02-03T21:41:59Z", "author_association": "MEMBER", "body_html": "<p>This seems a more appropriate issue for tensorflow-discuss...</p>\n<p>The most likely cause of this is that you are maybe calling session.run()  on an op and not fetching any tensor results?</p>\n<p>This has an interesting behavior where all the computation is enqueued on the GPU device, but if you don't ask to copy any results back from the GPU the call to run() can return immediately.</p>\n<p>The second time you call session.run(), enqueueing the GPU ops ends up waiting for the previous work to finish (for some reason).  The steady-state throughput is limited to 7.7/second, but the first measurement is misleading.</p>\n<p>You can verify this hypothesis by either fetching back a small result tensor at the end of each step, or by adding a check_numerics op at the end of the step (which is always executed synchronously).</p>", "body_text": "This seems a more appropriate issue for tensorflow-discuss...\nThe most likely cause of this is that you are maybe calling session.run()  on an op and not fetching any tensor results?\nThis has an interesting behavior where all the computation is enqueued on the GPU device, but if you don't ask to copy any results back from the GPU the call to run() can return immediately.\nThe second time you call session.run(), enqueueing the GPU ops ends up waiting for the previous work to finish (for some reason).  The steady-state throughput is limited to 7.7/second, but the first measurement is misleading.\nYou can verify this hypothesis by either fetching back a small result tensor at the end of each step, or by adding a check_numerics op at the end of the step (which is always executed synchronously).", "body": "This seems a more appropriate issue for tensorflow-discuss...\n\nThe most likely cause of this is that you are maybe calling session.run()  on an op and not fetching any tensor results?\n\nThis has an interesting behavior where all the computation is enqueued on the GPU device, but if you don't ask to copy any results back from the GPU the call to run() can return immediately. \n\nThe second time you call session.run(), enqueueing the GPU ops ends up waiting for the previous work to finish (for some reason).  The steady-state throughput is limited to 7.7/second, but the first measurement is misleading.\n\nYou can verify this hypothesis by either fetching back a small result tensor at the end of each step, or by adding a check_numerics op at the end of the step (which is always executed synchronously).\n"}