{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/542", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/542/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/542/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/542/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/542", "id": 122870115, "node_id": "MDU6SXNzdWUxMjI4NzAxMTU=", "number": 542, "title": "tf.transpose error when trying to transpose matrix of bools (workaround below), also recommend adding tf.repeat", "user": {"login": "hardmaru", "id": 6318110, "node_id": "MDQ6VXNlcjYzMTgxMTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/6318110?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hardmaru", "html_url": "https://github.com/hardmaru", "followers_url": "https://api.github.com/users/hardmaru/followers", "following_url": "https://api.github.com/users/hardmaru/following{/other_user}", "gists_url": "https://api.github.com/users/hardmaru/gists{/gist_id}", "starred_url": "https://api.github.com/users/hardmaru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hardmaru/subscriptions", "organizations_url": "https://api.github.com/users/hardmaru/orgs", "repos_url": "https://api.github.com/users/hardmaru/repos", "events_url": "https://api.github.com/users/hardmaru/events{/privacy}", "received_events_url": "https://api.github.com/users/hardmaru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2015-12-18T03:12:22Z", "updated_at": "2016-03-09T01:54:24Z", "closed_at": "2016-03-09T01:54:24Z", "author_association": "NONE", "body_html": "<p>I'm trying to implement some conditioning flows when training RNNs.  Basically, once an end-of-sequence event has been detected, I reset the RNN state to zeros, and have been able to build in this logic successfully I think with tf.greater() and tf.select()</p>\n<p>I implemented a numpy.repeat(a, repeats) sort of equivalent in tensorflow.  I recommend in the future, this can be also added as a tf.repeat() function repeating a bunch of boolean values across a tensor for control flows.</p>\n<p>Here's my implementation of a repeater (it only works for non-generalised case, for my problem, so I need to generalise it for higher dimensional tensors in the future):</p>\n<pre><code>def tfrepeat(a, repeats):\n    num_row = a.get_shape()[0].value\n    num_col = a.get_shape()[1].value\n    assert(num_col == 1)\n    result = [a for i in range(repeats)]\n    result = tf.concat(0, result)\n    result = tf.reshape(result, [repeats, num_row])\n    result = tf.transpose(result)\n    return result\n</code></pre>\n<p>The issue I have is I needed to transpose the result in the end to have the dimensions line up, and the results are all boolean values, and currently I noticed tf.transpose doesn't transpose a matrix of bool's</p>\n<p>The workaround I have was to apply the functions to real numbers, and afterwards, make the end result into a large bool matrix, although this isn't ideal.</p>\n<p>Workaround:</p>\n<pre><code>eoc_detection = inp[:,eoc_column]\neoc_detection = tf.reshape(eoc_detection, [num_batches, 1])\neoc_detection_state = tfrepeat(eoc_detection, num_state)\neoc_detection_state = tf.greater(eoc_detection_state, tf.zeros_like(eoc_detection_state,dtype=tf.float32))\nnew_state = tf.select(eoc_detection_state, initial_state, new_state)\n</code></pre>\n<p><code>new_state</code> is the lstm state to be fed in next time. If the end of content state is detected in training, we reset it.  This way, I can allow batches of the same length to be trained.</p>", "body_text": "I'm trying to implement some conditioning flows when training RNNs.  Basically, once an end-of-sequence event has been detected, I reset the RNN state to zeros, and have been able to build in this logic successfully I think with tf.greater() and tf.select()\nI implemented a numpy.repeat(a, repeats) sort of equivalent in tensorflow.  I recommend in the future, this can be also added as a tf.repeat() function repeating a bunch of boolean values across a tensor for control flows.\nHere's my implementation of a repeater (it only works for non-generalised case, for my problem, so I need to generalise it for higher dimensional tensors in the future):\ndef tfrepeat(a, repeats):\n    num_row = a.get_shape()[0].value\n    num_col = a.get_shape()[1].value\n    assert(num_col == 1)\n    result = [a for i in range(repeats)]\n    result = tf.concat(0, result)\n    result = tf.reshape(result, [repeats, num_row])\n    result = tf.transpose(result)\n    return result\n\nThe issue I have is I needed to transpose the result in the end to have the dimensions line up, and the results are all boolean values, and currently I noticed tf.transpose doesn't transpose a matrix of bool's\nThe workaround I have was to apply the functions to real numbers, and afterwards, make the end result into a large bool matrix, although this isn't ideal.\nWorkaround:\neoc_detection = inp[:,eoc_column]\neoc_detection = tf.reshape(eoc_detection, [num_batches, 1])\neoc_detection_state = tfrepeat(eoc_detection, num_state)\neoc_detection_state = tf.greater(eoc_detection_state, tf.zeros_like(eoc_detection_state,dtype=tf.float32))\nnew_state = tf.select(eoc_detection_state, initial_state, new_state)\n\nnew_state is the lstm state to be fed in next time. If the end of content state is detected in training, we reset it.  This way, I can allow batches of the same length to be trained.", "body": "I'm trying to implement some conditioning flows when training RNNs.  Basically, once an end-of-sequence event has been detected, I reset the RNN state to zeros, and have been able to build in this logic successfully I think with tf.greater() and tf.select()\n\nI implemented a numpy.repeat(a, repeats) sort of equivalent in tensorflow.  I recommend in the future, this can be also added as a tf.repeat() function repeating a bunch of boolean values across a tensor for control flows.\n\nHere's my implementation of a repeater (it only works for non-generalised case, for my problem, so I need to generalise it for higher dimensional tensors in the future):\n\n```\ndef tfrepeat(a, repeats):\n    num_row = a.get_shape()[0].value\n    num_col = a.get_shape()[1].value\n    assert(num_col == 1)\n    result = [a for i in range(repeats)]\n    result = tf.concat(0, result)\n    result = tf.reshape(result, [repeats, num_row])\n    result = tf.transpose(result)\n    return result\n```\n\nThe issue I have is I needed to transpose the result in the end to have the dimensions line up, and the results are all boolean values, and currently I noticed tf.transpose doesn't transpose a matrix of bool's\n\nThe workaround I have was to apply the functions to real numbers, and afterwards, make the end result into a large bool matrix, although this isn't ideal.\n\nWorkaround:\n\n```\neoc_detection = inp[:,eoc_column]\neoc_detection = tf.reshape(eoc_detection, [num_batches, 1])\neoc_detection_state = tfrepeat(eoc_detection, num_state)\neoc_detection_state = tf.greater(eoc_detection_state, tf.zeros_like(eoc_detection_state,dtype=tf.float32))\nnew_state = tf.select(eoc_detection_state, initial_state, new_state)\n```\n\n`new_state` is the lstm state to be fed in next time. If the end of content state is detected in training, we reset it.  This way, I can allow batches of the same length to be trained.\n"}