{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366524641", "html_url": "https://github.com/tensorflow/tensorflow/issues/17092#issuecomment-366524641", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17092", "id": 366524641, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjUyNDY0MQ==", "user": {"login": "georgh", "id": 1831252, "node_id": "MDQ6VXNlcjE4MzEyNTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1831252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgh", "html_url": "https://github.com/georgh", "followers_url": "https://api.github.com/users/georgh/followers", "following_url": "https://api.github.com/users/georgh/following{/other_user}", "gists_url": "https://api.github.com/users/georgh/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgh/subscriptions", "organizations_url": "https://api.github.com/users/georgh/orgs", "repos_url": "https://api.github.com/users/georgh/repos", "events_url": "https://api.github.com/users/georgh/events{/privacy}", "received_events_url": "https://api.github.com/users/georgh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-18T15:39:10Z", "updated_at": "2018-02-18T15:57:25Z", "author_association": "NONE", "body_html": "<p>Changing the op to</p>\n<pre><code>def op(alpha, Xder, Xdertest, i):\n      with tf.device('/cpu:0'):\n            Xder = tf.identity(Xder, name='copyOfXder_{}'.format(i))\n      cols = size #tf.shape(X)[0]\n      first = tf.matmul(Xder, alpha, transpose_a=True, name=\"first_{}\".format(i))            # cols x num_des x 1  \n      with tf.control_dependencies([first]):\n            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=\"xdt_{}\".format(i))   # cols x num_dim x num_des\n            third = tf.matmul(xdt, first, name=\"third_{}\".format(i))                         # cols x num_dim x 1\n            total = tf.reduce_sum(third, name=\"total_{}\".format(i))                          # single number\n      return total \n</code></pre>\n<p>will result in the expected behavior: xder is copied for every repetition. This allows the execution up to -size 16k as it should.</p>\n<p>After talking with myself for two days now, my problem is mostly solved. But if some of the mods reviews this, I think the issue shouldn't be closed. First the following should be changed:</p>\n<ul>\n<li>report_tensor_allocations_upon_oom should be fixed that it includes memory of placeholders</li>\n<li>a tool like mem_util should be included in tf-core - could be mixed with timeline?</li>\n</ul>\n<p>I am not sure how stable my solution is. Could be that the tf.identity-op is removed by the optimizer in some cases. =&gt; A \"clear memory on GPU\" op could be useful<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> Thanks again for you help. I found your work about memory reduction: <a href=\"https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9\" rel=\"nofollow\">https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9</a><br>\nBut I am not sure how you implement the poor memory strategy. Is there a way to tell tf to forget the result of an op, even though he uses it later on again? Or did you rewrite the operations graph so that it does not include references to the first operations?</p>", "body_text": "Changing the op to\ndef op(alpha, Xder, Xdertest, i):\n      with tf.device('/cpu:0'):\n            Xder = tf.identity(Xder, name='copyOfXder_{}'.format(i))\n      cols = size #tf.shape(X)[0]\n      first = tf.matmul(Xder, alpha, transpose_a=True, name=\"first_{}\".format(i))            # cols x num_des x 1  \n      with tf.control_dependencies([first]):\n            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=\"xdt_{}\".format(i))   # cols x num_dim x num_des\n            third = tf.matmul(xdt, first, name=\"third_{}\".format(i))                         # cols x num_dim x 1\n            total = tf.reduce_sum(third, name=\"total_{}\".format(i))                          # single number\n      return total \n\nwill result in the expected behavior: xder is copied for every repetition. This allows the execution up to -size 16k as it should.\nAfter talking with myself for two days now, my problem is mostly solved. But if some of the mods reviews this, I think the issue shouldn't be closed. First the following should be changed:\n\nreport_tensor_allocations_upon_oom should be fixed that it includes memory of placeholders\na tool like mem_util should be included in tf-core - could be mixed with timeline?\n\nI am not sure how stable my solution is. Could be that the tf.identity-op is removed by the optimizer in some cases. => A \"clear memory on GPU\" op could be useful\n@yaroslavvb Thanks again for you help. I found your work about memory reduction: https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9\nBut I am not sure how you implement the poor memory strategy. Is there a way to tell tf to forget the result of an op, even though he uses it later on again? Or did you rewrite the operations graph so that it does not include references to the first operations?", "body": "Changing the op to \r\n```\r\ndef op(alpha, Xder, Xdertest, i):\r\n      with tf.device('/cpu:0'):\r\n            Xder = tf.identity(Xder, name='copyOfXder_{}'.format(i))\r\n      cols = size #tf.shape(X)[0]\r\n      first = tf.matmul(Xder, alpha, transpose_a=True, name=\"first_{}\".format(i))            # cols x num_des x 1  \r\n      with tf.control_dependencies([first]):\r\n            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=\"xdt_{}\".format(i))   # cols x num_dim x num_des\r\n            third = tf.matmul(xdt, first, name=\"third_{}\".format(i))                         # cols x num_dim x 1\r\n            total = tf.reduce_sum(third, name=\"total_{}\".format(i))                          # single number\r\n      return total \r\n```\r\nwill result in the expected behavior: xder is copied for every repetition. This allows the execution up to -size 16k as it should.\r\n\r\nAfter talking with myself for two days now, my problem is mostly solved. But if some of the mods reviews this, I think the issue shouldn't be closed. First the following should be changed:\r\n* report_tensor_allocations_upon_oom should be fixed that it includes memory of placeholders\r\n* a tool like mem_util should be included in tf-core - could be mixed with timeline?\r\n\r\nI am not sure how stable my solution is. Could be that the tf.identity-op is removed by the optimizer in some cases. => A \"clear memory on GPU\" op could be useful\r\n@yaroslavvb Thanks again for you help. I found your work about memory reduction: https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9\r\nBut I am not sure how you implement the poor memory strategy. Is there a way to tell tf to forget the result of an op, even though he uses it later on again? Or did you rewrite the operations graph so that it does not include references to the first operations?  \r\n"}