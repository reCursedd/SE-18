{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366549994", "html_url": "https://github.com/tensorflow/tensorflow/issues/17092#issuecomment-366549994", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17092", "id": 366549994, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjU0OTk5NA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-18T21:36:46Z", "updated_at": "2018-02-18T21:36:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Some ideas for predictable memory usage:</p>\n<ol>\n<li><code>export TF_CUDNN_USE_AUTOTUNE=0</code> to remove performance tuning that takes (unpredictable) amounts of extra memory.</li>\n<li>Remove other optimizations</li>\n</ol>\n<pre><code>  optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\ngraph_options=tf.GraphOptions(optimizer_options=optimizer_options))\n  config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\n  config.graph_options.place_pruned_graph = True\n  sess= tf.Session(config=config)\n</code></pre>\n<ol start=\"3\">\n<li>TensorFlow will forget outputs of an op as soon as all of its consumers have executed. So you have to connect some of the original consumers of an op to the copy of an op (using graph editor), this will cause outputs to be forgotten/then recomputed</li>\n<li>About placeholder....this op is somewhat special, I'm guessing the memory gets allocated by runtime in a different code path from other ops and may not get captured by <code>mem_util</code>. However, I recall seeing memory allocations from it in the <code>LOG_MEMORY</code> messages</li>\n</ol>", "body_text": "Some ideas for predictable memory usage:\n\nexport TF_CUDNN_USE_AUTOTUNE=0 to remove performance tuning that takes (unpredictable) amounts of extra memory.\nRemove other optimizations\n\n  optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\ngraph_options=tf.GraphOptions(optimizer_options=optimizer_options))\n  config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\n  config.graph_options.place_pruned_graph = True\n  sess= tf.Session(config=config)\n\n\nTensorFlow will forget outputs of an op as soon as all of its consumers have executed. So you have to connect some of the original consumers of an op to the copy of an op (using graph editor), this will cause outputs to be forgotten/then recomputed\nAbout placeholder....this op is somewhat special, I'm guessing the memory gets allocated by runtime in a different code path from other ops and may not get captured by mem_util. However, I recall seeing memory allocations from it in the LOG_MEMORY messages", "body": "Some ideas for predictable memory usage:\r\n1. `export TF_CUDNN_USE_AUTOTUNE=0` to remove performance tuning that takes (unpredictable) amounts of extra memory. \r\n2. Remove other optimizations\r\n```\r\n  optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\r\ngraph_options=tf.GraphOptions(optimizer_options=optimizer_options))\r\n  config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\r\n  config.graph_options.place_pruned_graph = True\r\n  sess= tf.Session(config=config)\r\n```\r\n\r\n3. TensorFlow will forget outputs of an op as soon as all of its consumers have executed. So you have to connect some of the original consumers of an op to the copy of an op (using graph editor), this will cause outputs to be forgotten/then recomputed\r\n4. About placeholder....this op is somewhat special, I'm guessing the memory gets allocated by runtime in a different code path from other ops and may not get captured by `mem_util`. However, I recall seeing memory allocations from it in the `LOG_MEMORY` messages"}