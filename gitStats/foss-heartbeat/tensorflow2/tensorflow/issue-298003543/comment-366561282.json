{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/366561282", "html_url": "https://github.com/tensorflow/tensorflow/issues/17092#issuecomment-366561282", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17092", "id": 366561282, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjU2MTI4Mg==", "user": {"login": "georgh", "id": 1831252, "node_id": "MDQ6VXNlcjE4MzEyNTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1831252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgh", "html_url": "https://github.com/georgh", "followers_url": "https://api.github.com/users/georgh/followers", "following_url": "https://api.github.com/users/georgh/following{/other_user}", "gists_url": "https://api.github.com/users/georgh/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgh/subscriptions", "organizations_url": "https://api.github.com/users/georgh/orgs", "repos_url": "https://api.github.com/users/georgh/repos", "events_url": "https://api.github.com/users/georgh/events{/privacy}", "received_events_url": "https://api.github.com/users/georgh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-19T00:09:32Z", "updated_at": "2018-02-19T00:12:01Z", "author_association": "NONE", "body_html": "<p>Thanks for your reply.<br>\nKnowing about the optimization is probably useful, even though I would like to find a solution with optimization turned on. But I should probably check how much effect the optimization has.</p>\n<p>Placeholder produce some entries in the state_stats protobuf, but as far as I can tell only for the CPU.<br>\nThe gpu-stream creates a lot of mesages instead, but they do not include a free message and do not explicitly denote the placeholder.<br>\nAdding allocation messages for the target of the streams might be a solution for this issue, or at least a starting point.<br>\nThe LOG_MEMORY messages are nice, but I don't think its a good solution to parse them to get basic information about allocation and frees.<br>\nAnd report_tensor_allocations_upon_oom is a flag that should definitely work without the LOG.</p>\n<p>I had a short look in the code for this but the tensorflow codebase is quite huge. A lot of the code for ops seems to be scattered in different files. Is there any tutorial or overview for the internal structure of the tf code?</p>", "body_text": "Thanks for your reply.\nKnowing about the optimization is probably useful, even though I would like to find a solution with optimization turned on. But I should probably check how much effect the optimization has.\nPlaceholder produce some entries in the state_stats protobuf, but as far as I can tell only for the CPU.\nThe gpu-stream creates a lot of mesages instead, but they do not include a free message and do not explicitly denote the placeholder.\nAdding allocation messages for the target of the streams might be a solution for this issue, or at least a starting point.\nThe LOG_MEMORY messages are nice, but I don't think its a good solution to parse them to get basic information about allocation and frees.\nAnd report_tensor_allocations_upon_oom is a flag that should definitely work without the LOG.\nI had a short look in the code for this but the tensorflow codebase is quite huge. A lot of the code for ops seems to be scattered in different files. Is there any tutorial or overview for the internal structure of the tf code?", "body": "Thanks for your reply.\r\nKnowing about the optimization is probably useful, even though I would like to find a solution with optimization turned on. But I should probably check how much effect the optimization has.\r\n\r\nPlaceholder produce some entries in the state_stats protobuf, but as far as I can tell only for the CPU.\r\nThe gpu-stream creates a lot of mesages instead, but they do not include a free message and do not explicitly denote the placeholder.\r\nAdding allocation messages for the target of the streams might be a solution for this issue, or at least a starting point.\r\nThe LOG_MEMORY messages are nice, but I don't think its a good solution to parse them to get basic information about allocation and frees. \r\nAnd report_tensor_allocations_upon_oom is a flag that should definitely work without the LOG.\r\n\r\nI had a short look in the code for this but the tensorflow codebase is quite huge. A lot of the code for ops seems to be scattered in different files. Is there any tutorial or overview for the internal structure of the tf code? \r\n"}