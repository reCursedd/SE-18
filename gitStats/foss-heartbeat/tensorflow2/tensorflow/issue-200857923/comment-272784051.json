{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/272784051", "html_url": "https://github.com/tensorflow/tensorflow/issues/6860#issuecomment-272784051", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6860", "id": 272784051, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Mjc4NDA1MQ==", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-16T06:33:34Z", "updated_at": "2017-02-11T05:50:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for your reply.</p>\n<p>Though, you might have some misunderstanding about word2vec.  I'm familiar with this model, and have published a paper on it.  I think this is a minor bug, not a desired behavior.  For example, this will cause failure of some theoretical analysis, i.e.: <a href=\"https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf\" rel=\"nofollow\">neural-word-embedding-as-implicit-matrix-factorization</a>.</p>\n<p>But it's not too serious, because the effect is to subsample the corpus.  In practice, it still works well for word embedding learning.</p>\n<p>A simple fix is to let the pointer <code>data_index</code> to backtrack a little bit at the end of generate_batch(), see the next-to-last line below:</p>\n<pre><code># Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips &lt;= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [skip_window]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  data_index = (data_index + len(data) - span) % len(data)\n  return batch, labels\n</code></pre>\n<p>FYI.</p>", "body_text": "Thanks for your reply.\nThough, you might have some misunderstanding about word2vec.  I'm familiar with this model, and have published a paper on it.  I think this is a minor bug, not a desired behavior.  For example, this will cause failure of some theoretical analysis, i.e.: neural-word-embedding-as-implicit-matrix-factorization.\nBut it's not too serious, because the effect is to subsample the corpus.  In practice, it still works well for word embedding learning.\nA simple fix is to let the pointer data_index to backtrack a little bit at the end of generate_batch(), see the next-to-last line below:\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [skip_window]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  data_index = (data_index + len(data) - span) % len(data)\n  return batch, labels\n\nFYI.", "body": "Thanks for your reply.\r\n\r\nThough, you might have some misunderstanding about word2vec.  I'm familiar with this model, and have published a paper on it.  I think this is a minor bug, not a desired behavior.  For example, this will cause failure of some theoretical analysis, i.e.: [neural-word-embedding-as-implicit-matrix-factorization](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf).\r\n\r\nBut it's not too serious, because the effect is to subsample the corpus.  In practice, it still works well for word embedding learning.\r\n\r\nA simple fix is to let the pointer `data_index` to backtrack a little bit at the end of generate_batch(), see the next-to-last line below:\r\n```\r\n# Step 3: Function to generate a training batch for the skip-gram model.\r\ndef generate_batch(batch_size, num_skips, skip_window):\r\n  global data_index\r\n  assert batch_size % num_skips == 0\r\n  assert num_skips <= 2 * skip_window\r\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\r\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\r\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\r\n  buffer = collections.deque(maxlen=span)\r\n  for _ in range(span):\r\n    buffer.append(data[data_index])\r\n    data_index = (data_index + 1) % len(data)\r\n  for i in range(batch_size // num_skips):\r\n    target = skip_window  # target label at the center of the buffer\r\n    targets_to_avoid = [skip_window]\r\n    for j in range(num_skips):\r\n      while target in targets_to_avoid:\r\n        target = random.randint(0, span - 1)\r\n      targets_to_avoid.append(target)\r\n      batch[i * num_skips + j] = buffer[skip_window]\r\n      labels[i * num_skips + j, 0] = buffer[target]\r\n    buffer.append(data[data_index])\r\n    data_index = (data_index + 1) % len(data)\r\n  data_index = (data_index + len(data) - span) % len(data)\r\n  return batch, labels\r\n```\r\n\r\nFYI."}